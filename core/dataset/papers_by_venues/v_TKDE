#index 443007
#* A MAC Policy Framework for Multilevel Relational Databases
#@ Xiaolei Qian;Teresa F. Lunt
#t 1996
#c 7
#% 36683
#% 51451
#% 69537
#% 78831
#% 109746
#% 131553
#% 141503
#% 151143
#% 151161
#% 151170
#% 317991
#% 374401
#% 443120
#% 480945
#% 488004
#% 488012
#% 535819
#% 664503
#% 664509
#% 664522
#! We develop a formal framework of MAC policies in multilevel relational databases. We identify the important components of MAC policies and their desirable properties. The framework provides a basis for systematically specifying MAC policies and characterizing their potential mismatches. Based on the framework, we compare and unify the MAC policies and policy components that are proposed in the literature or imposed in existing systems. Our framework could be used to capture and resolve MAC policy mismatches in the trusted interoperation of heterogeneous multilevel relational databases.

#index 443008
#* A Trusted Subject Architecture for Multilevel Secure Object-Oriented Databases
#@ Roshan K. Thomas;Ravi S. Sandhu
#t 1996
#c 7
#% 23959
#% 30566
#% 54075
#% 69534
#% 200351
#% 320629
#% 437154
#% 507381
#% 664475
#! In this paper, we address security in object-oriented database systems for multilevel secure environments. Such an environment consists of users cleared to various security levels, accessing information labeled with varying classifications. Our purpose is three-fold. First, we show how security can be naturally incorporated into the object model of computing so as to form a foundation for building multilevel secure object-oriented database management systems. Next, we show how such an abstract security model can be realized under a cost-effective, viable, and popular security architecture. Finally, we give security arguments based on trusted subjects and a formal proof to demonstrate the confidentiality of our architecture and approach.A notable feature of our solution is the support for secure synchronous write-up operations. This is useful when low level users want to send information to higher level users. In the object-oriented context, this is naturally modeled and efficiently accomplished through write-up messages sent by low level subjects. However, such write-up messages can pose confidentiality leaks (through timing and signaling channels) if the timing of the receipt and processing of the messages is observable to lower level senders. Such covert channels are a formidable obstacle in building high-assurance secure systems. Further, solutions to problems such as these have been known to involve various tradeoffs between confidentiality, integrity, and performance. We present a concurrent computation model that closes such channels while preserving the conflicting goals of confidentiality, integrity, and performance. Finally, we give a confidentiality proof for a trusted subject architecture and implementation and demonstrate that the trusted subject (process) cannot leak information in violation of multilevel security.

#index 443009
#* Correctness Criteria for Multilevel Secure Transactions
#@ Kenneth P. Smith;Barbara T. Blaustein;Sushil Jajodia;LouAnna Notargiacomo
#t 1996
#c 7
#% 9241
#% 151155
#% 308796
#% 403195
#% 488017
#% 664470
#% 664471
#% 664490
#! The benefits of distributed systems and shared database resources are widely recognized, but they often cannot be exploited by users who must protect their data by using label-based access controls. In particular, users of label-based data need to read and write data at different security levels within a single database transaction, which is not currently possible without violating multilevel security constraints. This paper presents a formal model of multilevel transactions which provide this capability. We define four ACIS (atomicity, consistency, isolation, and security) correctness properties of multilevel transactions. While atomicity, consistency and isolation are mutually achievable in standard single-site and distributed transactions, we show that the security requirements of multilevel transactions conflict with some of these goals. This forces trade-offs to be made among the ACIS correctness properties, and we define appropriate partial correctness properties. Due to such trade-offs, an important problem is to design multilevel transaction execution protocols which achieve the greatest possible degree of correctness. These protocols must provide a variety of approaches to making trade-offs according to the differing priorities of various users. We present three transaction execution protocols which achieve a high degree of correctness. These protocols exemplify the correctness trade-offs proven in the paper, and offer realistic implementation options.

#index 443010
#* Inference in MLS Database Systems
#@ Donald G. Marks
#t 1996
#c 7
#% 36683
#% 68601
#% 151145
#% 151148
#% 151149
#% 412588
#% 488015
#% 507378
#! Database systems that contain information of varying degrees of sensitivity pose the threat that some of the Low data may infer High data. This study derives conditions sufficient to identify such inference threats. First, it is reasoned that a database can only control material implications, as specified in formal logic systems. These material implications are found using Knowledge Discovery techniques. Material implications allow reasoning about outside knowledge, and provide the first assurance that outside knowledge does not assist in circumventing the inference controls. Database queries specify the properties of sets of data and are compared to help determine inferences. These queries are grouped into equivalence classes based upon their inference characteristics. A unique graph based model is developed for the equivalence classes that 1) makes such comparisons easy, and 2) allows implementation of an algorithm capable of finding those material implication rules where High data is inferred from Low data. This is the first method that offers assurance and sufficiency arguments that the mechanism is at least strong enough to protect the High data in the database from inference attacks that require Low data.

#index 443011
#* Wizard: A Database Inference Analysis and Detection System
#@ Harry S. Delugach;Thomas H. Hinke
#t 1996
#c 7
#% 2298
#% 32907
#% 36683
#% 53706
#% 151148
#% 454192
#% 454193
#% 465176
#% 488015
#% 488145
#% 664495
#! The database inference problem is a well-known problem in database security and information system security in general. In order to prevent an adversary from inferring classified information from combinations of unclassified information, a database inference analyst must be able to detect and prevent possible inferences. Detecting database inference problems at database design time provides great power in reducing problems over the lifetime of a database. We have developed and constructed a system called Wizard to analyze databases for their inference problems. The system takes as input a database schema, its constituent instances (if available) and additional human-supplied domain information, and provides a set of associations between entities and/or activities that can be grouped by their potential severity of inference vulnerability. A knowledge acquisition process called microanalysis permits semantic knowledge of a database to be incorporated into the analysis using conceptual graphs. These graphs are then analyzed with respect to inference-relevant domains we call facets using tools we have developed. We can determine inference problems within single facets as well as some inference problems between two or more facets. The architecture of the system is meant to be general so that further refinements of inference information subdomains can be easily incorporated into the system.

#index 443012
#* A Temporal Access Control Mechanism for Database Systems
#@ Elisa Bertino;Claudio Bettini;Elena Ferrari;Pierangela Samarati
#t 1996
#c 7
#% 100613
#% 103705
#% 139132
#% 151510
#% 152928
#% 172275
#% 480623
#! This paper presents a discretionary access control model in which authorizations contain temporal intervals of validity. An authorization is automatically revoked when the associated temporal interval expires. The proposed model provides rules for the automatic derivation of new authorizations from those explicitly specified. Both positive and negative authorizations are supported. A formal definition of those concepts is presented in the paper, together with the semantic interpretation of authorizations and derivation rules as clauses of a general logic program. Issues deriving from the presence of negative authorizations are discussed. We also allow negation in rules: it is possible to derive new authorizations on the basis of the absence of other authorizations. The presence of this type of rules may lead to the generation of different sets of authorizations, depending on the evaluation order. An approach is presented, based on establishing an ordering among authorizations and derivation rules, which guarantees a unique set of valid authorizations. Moreover, we give an algorithm detecting whether such an ordering can be established for a given set of authorizations and rules. Administrative operations for adding, removing, or modifying authorizations and derivation rules are presented and efficiency issues related to these operations are also tackled in the paper. A materialization approach is proposed, allowing to efficiently perform access control.

#index 443013
#* An Access Control Model and Its Use in Representing Mental Health Application Access Policy
#@ Vijay Varadharajan;Claudio Calvelli
#t 1996
#c 7
#% 611
#% 36192
#% 134471
#% 146722
#% 664511
#! This paper considers an access control model and proposes extensions to it to deal with authentication and revocation. The model is then applied to represent access control policy in a mental health system.In the first part of the paper, extensions to the Schematic Protection Model (SPM) are presented. The authentication and revocation extensions are independent of one another in the sense that each one affects a different part of the decision algorithm. The extensions comprise a modification of the syntax to be able to represent the new concepts and, more importantly, a modification of the decision algorithm for the safety problem to take these changes into account.We introduce the concept of conditional tickets and use it to provide authentication. Apart from this, we have found this concept to be useful in modeling systems. Hence we have separated this (syntactical) issue from the definition of the new algorithm.The second part considers the access policy for a mental health application. We have used the extensions of SPM to model part of this access policy. Even with our extensions, SPM still remains a monotonic model, where rights can be removed only in very special cases, and this makes it impossible to represent all the aspects of the problem. Other than to serve as an example for the extensions we propose, this paper also helps to separate aspects of this access control policy which are inherently monotonic from parts which are defined in a non-monotonic way, but can still be represented in a monotonic model.

#index 443014
#* Knowledge Processing in Control Systems
#@ Ricardo R. Gudwin;Fernando A. C. Gomide;Márcio L. Andrade Netto;Maurício F. Magalhães
#t 1996
#c 7
#% 10804
#% 34874
#% 37905
#% 40312
#% 69256
#% 95971
#% 95982
#% 95983
#% 288821
#! A real time knowledge processing procedure is proposed for rule-based systems in general and control systems applications in particular. Distinguishing features of the procedure include a mechanism for rule base compression and an inference scheme based on matrix operators. The procedure is also amenable for schedulability analysis to provide response time warranty An application concerning supervisory group control of elevators is also included to show the usefulness of the proposed procedure.

#index 443015
#* Uncertainty Management in Expert Systems Using Fuzzy Petri Nets
#@ Amit Konar;Ajit K. Mandal
#t 1996
#c 7
#% 24547
#% 26348
#% 40312
#% 369148
#% 374605
#% 442657
#% 442719
#% 444670
#% 835738
#! The paper aims at developing new techniques for uncertainty management in expert systems for two generic class of problems using fuzzy Petri net that represents logical connectivity among a set of imprecise propositions. One class of problems addressed in the paper deals with the computation of fuzzy belief of any proposition from the fuzzy beliefs of a set of independent initiating propositions in a given network. The other class of problems is concerned with the computation of steady-state fuzzy beliefs of the propositions embedded in the network, from their initial fuzzy beliefs through a process called belief-revision. During belief-revision, a fuzzy Petri net with cycles may exhibit "limitcycle behavior" of fuzzy beliefs for some propositions in the network. No decisions can be arrived at from a fuzzy Petri net with such behavior. To circumvent this problem, techniques have been developed for the detection and elimination of limitcycles. Further, an algorithm for selecting one evidence from each set of mutually inconsistent evidences, referred to as nonmonotonic reasoning, has also been presented in connection with the problems of belief-revision. Finally, the concepts proposed for solving the problems of belief-revision have been applied successfully for tackling imprecision, uncertainty, and nonmonotonicity of evidences in an illustrative expert system for criminal investigation.

#index 443016
#* Genetic Search: Analysis Using Fitness Moments
#@ M. Srinivas;L. m. Patnaik
#t 1996
#c 7
#% 10658
#% 36406
#% 36407
#% 36408
#% 36411
#% 36413
#% 81925
#% 81937
#% 81938
#% 81954
#% 92717
#% 114994
#% 168999
#% 369236
#% 466380
#! Genetic Algorithms are efficient and robust search methods that are being employed in a plethora of applications with extremely large search spaces. The directed search mechanism employed in Genetic Algorithms performs a simultaneous and balanced, exploration of new regions in the search space and exploitation of already discovered regions.This paper introduces the notion of fitness moments for analyzing the working of Genetic Algorithms (GAs). We show that the fitness moments in any generation may be predicted from those of the initial population. Since a knowledge of the fitness moments allows us to estimate the fitness distribution of strings, this approach provides for a method of characterizing the dynamics of GAs. In particular the average fitness and fitness variance of the population in any generation may be predicted.We introduce the technique of fitness-based disruption of solutions for improving the performance of GAs. Using fitness moments, we demonstrate the advantages of using fitness-based disruption. We also present experimental results comparing the performance of a standard GA and GAs (CDGA and AGA) that incorporate the principle of fitness-based disruption. The experimental evidence clearly demonstrates the power of fitness based disruption.

#index 443017
#* Constructing Efficient Belief Network Structures With Expert Provided Information
#@ Sumit Sarkar;Ishwar Murthy
#t 1996
#c 7
#% 6199
#% 32575
#% 44876
#% 68244
#% 101217
#% 442814
#% 449588
#% 527664
#% 527673
#% 527675
#% 527691
#% 527851
#! We present a technique to construct efficient belief network structures for application areas where large amounts of data are available and information on the ordering of the variables can be obtained from domain experts. We identify classes of networks that are efficient for propagating beliefs. We formulate the problem as one of determining the belief network representation from a given class that best represents the data. We use the I-Divergence measure which is known to have certain desirable properties for evaluating different approximations. We present some theoretical findings that characterize the nature of solutions that are obtained. These theoretical results lead to an efficient solution procedure for finding the best network representation. We also discuss other information that may be reasonably obtained from experts, and show how such information leads to improving the efficiency of the technique to find the best network structure.

#index 443018
#* A Multi-Granularity Locking Model for Concurrency Control in Object-Oriented Database Systems
#@ Suh-Yin Lee;Ruey-Long Liou
#t 1996
#c 7
#% 9241
#% 18606
#% 23959
#% 32903
#% 32971
#% 43164
#% 57952
#% 57956
#% 58918
#% 59114
#% 68197
#% 77984
#% 287028
#% 287304
#% 287749
#% 289399
#% 442721
#% 531907
#! A locking model adopting a multi-granularity approach is proposed for concurrency control in object-oriented database systems. The model is motivated by a desire to provide high concurrency and low locking overhead in accessing objects. Locking in schemas and locking in instances are developed separately and then are integrated. Schema changes and composite objects are also taken into account. A dual queue scheme for efficient scheduling of lock requests is developed. The model consists of a rich set of lock modes, a compatibility matrix, and a locking protocol. Characteristic query examples on single class, class lattice, and composite objects are used to illustrate the comparison between the ORION model and the proposed model. It is shown that our locking model has indeed made some improvements and is suitable for concurrency control in object-oriented databases.

#index 443019
#* The Design and Implementation of the Ariel Active Database Rule System
#@ Eric N. Hanson
#t 1996
#c 7
#% 1797
#% 32893
#% 37972
#% 43208
#% 45257
#% 58361
#% 62020
#% 62022
#% 62026
#% 68091
#% 69316
#% 82448
#% 82861
#% 86944
#% 86945
#% 111368
#% 116044
#% 140056
#% 152910
#% 156121
#% 286178
#% 287647
#% 341233
#% 442705
#% 442706
#% 442732
#% 462649
#% 463117
#% 480621
#% 480765
#% 480768
#! This paper describes the design and implementation of the Ariel DBMS and its tightly-coupled forward-chaining rule system. The query language of Ariel is a subset of POSTQUEL, extended with a new production-rule sublanguage. Ariel supports traditional relational database query and update operations efficiently, using a System R-like query processing strategy. In addition, the Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of selections, joins, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual驴-memory nodes which save storage since they contain only the predicate associated with the memory node instead of copies of data matching the predicate. In addition, the notions of tokens and 驴-memory nodes are generalized to support event and transition conditions. The rule-action executor in Ariel binds the data matching a rule's condition to the action of the rule at rule fire time, and executes the rule action using the query processor.

#index 443020
#* The Effect of Knowledge Representation Schemes on Maintainability of Knowledge-Based Systems
#@ Sunro Lee;Robert M. O'Keefe
#t 1996
#c 7
#% 2078
#% 24861
#% 54053
#% 70226
#% 147772
#% 318948
#% 442708
#! We use an experimental approach to investigate the representational effects of knowledge on maintainability, and compare this with the structural effects of rule sets investigated by Davis [5]. Results show that an object-based system, compared to a structured rule-based system, was easier to maintain in terms of the time to do the maintenance tasks, but not necessarily in terms of accuracy of the alterations. However, in some instances subsumption and redundancies were introduced into the rule-based system, which can cause problems for subsequent maintenance. Subjects perceived the structured rule-base system as more complex than the object-oriented system, and perceived the object structure as more useful than the rule modularization and documentation.

#index 443021
#* A Knowledge-Based Control Architecture with Interactive Reasoning Functions
#@ Gerald A. Sullivan
#t 1996
#c 7
#% 12472
#% 37905
#% 66948
#% 444687
#% 444764
#% 444765
#! A knowledge-based system architecture called IPEX is presented that uses a time distributed, interactive reasoning paradigm for process control applications. Structural features of the system are presented, and it is shown that temporal considerations are included in each of the system's data structures either explicitly or implicitly. Diagnostic planning is discussed, and explanations of the algorithms that formulate and maintain diagnostic/control plans are given. In particular, it is shown that the IPEX system can manage concurrently executing interactive diagnostic plans for multiple problem hypotheses.

#index 443022
#* Rule Revision With Recurrent Neural Networks
#@ Christian W. Omlin;C. Lee Giles
#t 1996
#c 7
#% 70613
#% 105614
#% 111451
#% 126917
#% 132674
#% 132675
#% 143042
#% 169755
#% 170665
#% 184199
#% 204015
#% 212288
#% 404772
#% 442952
#% 1051436
#! Abstract-Recurrent neural networks readily process, recognize and generate temporal sequences. By encoding grammatical strings as temporal sequences, recurrent neural networks can be trained to behave like deterministic sequential finite-state automata. Algorithms have been developed for extracting grammatical rules from trained networks. Using a simple method for inserting prior knowledge (or rules) into recurrent neural networks, we show that recurrent neural networks are able to perform rule revision. Rule revision is performed by comparing the inserted rules with the rules in the finite-state automata extracted from trained networks. The results from training a recurrent neural network to recognize a known non-trivial, randomly generated regular grammar show that not only do the networks preserve correct rules but that they are able to correct through training inserted rules which were initially incorrect. (By incorrect, we mean that the rules were not the ones in the randomly generated grammar.)

#index 443023
#* A Note on "Incomplete Relational Database Models Based on Intervals"
#@ Jui-Shang Chiu;Arbee L.  P. Chen
#t 1996
#c 7
#% 663
#% 452767
#% 463119
#% 556918
#% 836134
#! In [5], a family of relational database models (M-1 to M-5) were proposed to represent unknown values by intervals. Relational operators were extended for evaluating queries on these models. In this note, we stultify the theorems claiming that query evaluation in models M-2, M-3, and M-5 is sound.

#index 443024
#* Editorial: Four Named to Join Editorial Board of IEEE Transactions on Knowledge and Data Engineering
#@ Benjamin W. Wah
#t 1996
#c 7

#index 443025
#* A Guide to the Literature on Learning Probabilistic Networks from Data
#@ Wray Buntine
#t 1996
#c 7
#% 3596
#% 8202
#% 18378
#% 28175
#% 28176
#% 44876
#% 90157
#% 91872
#% 92135
#% 101213
#% 103309
#% 109042
#% 109043
#% 128599
#% 128623
#% 129987
#% 130114
#% 130153
#% 130878
#% 136350
#% 145224
#% 151239
#% 156181
#% 162952
#% 183490
#% 185075
#% 185077
#% 185079
#% 191910
#% 194447
#% 197387
#% 198076
#% 232110
#% 232132
#% 242970
#% 366651
#% 369349
#% 394679
#% 405009
#% 443632
#% 455904
#% 527665
#% 527675
#% 527830
#% 527851
#% 669197
#% 1290045
#% 1650627
#% 1650684
#% 1650852
#% 1650855
#% 1650911
#% 1650934
#% 1650946
#% 1650959
#% 1650960
#% 1650961
#% 1650962
#! This literature review discusses different methods under the general rubric of learning Bayesian networks from data, and includes some overlapping work on more general probabilistic networks. Connections are drawn between the statistical, neural network, and uncertainty communities, and between the different methodological communities, such as Bayesian, description length, and classical statistics. Basic concepts for learning and Bayesian networks are introduced and methods are then reviewed. Methods are discussed for learning parameters of a probabilistic network, for learning the structure, and for learning hidden variables. The presentation avoids formal definitions and theorems, as these are plentiful in the literature, and instead illustrates key concepts with simplified examples.

#index 443026
#* Advances in Feedforward Neural Networks: Demystifying Knowledge Acquiring Black Boxes
#@ Carl G. Looney
#t 1996
#c 7
#% 33917
#% 61477
#% 65443
#% 68820
#% 78889
#% 78945
#% 92071
#% 92148
#% 96692
#% 110958
#% 117076
#% 119387
#% 132941
#% 158037
#% 177728
#% 395890
#% 499581
#! We survey research of recent years on the supervised training of feedforward neural networks. The goal is to expose how the networks work, how to engineer them so they can learn data with less extraneous noise, how to train them efficiently, and how to assure that the training is valid. The scope covers gradient descent and polynomial line search, from backpropagation through conjugate gradients and quasi-Newton methods. There is a consensus among researchers that adaptive step gains (learning rates) can stabilize and accelerate convergence and that a good starting weight set improves both the training speed and the learning quality. The training problem includes both the design of a network function and the fitting of the function to a set of input and output data points by computing a set of coefficient weights. The form of the function can be adjusted by adjoining new neurons and pruning existing ones and setting other parameters such as biases and exponential rates. Our exposition reveals several useful results that are readily implementable.

#index 443027
#* Handling Discovered Structure in Database Systems
#@ John F. Roddick;Noel G. Craske;Thomas J. Richards
#t 1996
#c 7
#% 121
#% 2432
#% 4086
#% 6710
#% 18615
#% 52012
#% 55899
#% 63674
#% 76920
#% 76921
#% 84398
#% 85508
#% 87052
#% 107226
#% 107250
#% 110361
#% 111284
#% 116720
#% 119430
#% 125635
#% 126704
#% 129570
#% 135384
#% 136347
#% 139661
#% 286257
#% 319244
#% 322880
#% 404844
#% 411734
#% 439523
#% 452820
#% 462472
#% 462638
#% 481100
#% 481270
#% 533823
#% 535036
#% 690047
#% 750929
#% 750942
#% 836134
#! Most database systems research assumes that the database schema is determined by a database administrator. With the recent increase in interest in knowledge discovery from databases and the predicted increase in the volume of data expected to be stored it is appropriate to reexamine this assumption and investigate how derived or induced, rather than database administrator supplied, structure can be accommodated and used by database systems. This paper investigates some of the characteristics of inductive learning and knowledge discovery as they pertain to database systems and the constraints that would be imposed on appropriate inductive learning algorithms is discussed. A formal method of defining induced dependencies (both static and temporal) is proposed as the inductive analogue to functional dependencies. The Boswell database system exemplifying some of these characteristics is also briefly discussed.

#index 443028
#* A High-Level Petri Net for Goal-Directed Semantics of Horn Clause Logic
#@ John Jeffrey;Jorge Lobo;Tadao Murata
#t 1996
#c 7
#% 7052
#% 14280
#% 33376
#% 40177
#% 57520
#% 63343
#% 85208
#% 147354
#% 288985
#% 319818
#% 368992
#% 442764
#% 464589
#% 533657
#% 533668
#! A new high-level Petri net (HLPN) model is introduced as a graphical syntax for Horn Clause Logic (HCL) programs. We call these nets: Horn Clause Logic Goal-Directed Nets (HCLGNs). It is shown that there is a bijection between the queried definite programs and the class of HCLGNs. In addition, a visualization of SLD-resolution is realized through the enabling and firing rules and net markings. The correctness of these rules with respect to SLD-resolution is also proven. Using these notions, we model SLD-refutations and failing computations.Through minor modification of the definition of HCLGNs for pure HCL programs and of the enabling and firing rules, it is shown how HCLGNs can be used to model built-in atoms and provide a new AND/OR-parallel execution model. HCLGNs have also been used to: model a subset of Prolog; provide a framework for modeling variations on SLD-resolution, such as SLD-ALG; specify an operational semantics for committed-choice (flat-guarded) concurrent logic languages using FGHC as an example.Recently, several software packages have become available for editing and executing HLPNs. These graphical editors can now play the same role that string editors have played for many years. The simulation capabilities of the HLPN software offer opportunities to perform automated, interactive code walk-throughs and also have potential for providing a framework for visual debugging environments. We note however that HCLGNs differ from the major classes of HLPNs for which software tools have been developed in primarily two ways: 1) the tokens in the markings can have variables; and 2) the firing of a transition may not only update the marking of the adjacent places, but may instantiate variables in tokens in the markings of places that are non-adjacent to the fired transition. Thus, the existing packages can only provide graphical syntax editing and are not appropriate for graphical simulation of HCLGNs. In the paper, we provide an algebraic characterization of HCLGNs that can serve as a design guideline for implementing HCLGNs.

#index 443029
#* Optimization of Materialization Strategies for Derived Data Elements
#@ David Botzer;Opher Etzion
#t 1996
#c 7
#% 13016
#% 32914
#% 59350
#% 62026
#% 77005
#% 107685
#% 111368
#% 111372
#% 116044
#% 140609
#% 158900
#% 277332
#% 286176
#% 427218
#% 442663
#% 480626
#! The research in materialization of derived data elements has dealt so far with the if issue; that is, the question whether to physically store derived data elements. In the active database area, there has been some research on the how issue. In this paper, we deal with the when issue, devising an optimization model to determine the optimal materialization strategy. The decision problem confronted by the optimization model is more complex than "to materialize or not to materialize." The decision problem deals with devising the materialization strategy that consists of a set of interdependent decisions about each derived data element. Each decision relates to two issues:Should the value of a derived data element be persistent?What is the required level of consistency of a derived value with respect to its derivers?For each derived data element, the decision is based on both its local properties (complexity of derivation, update and retrieval frequencies, etc.) and its interdependencies with other derived values. The optimization model is based on a heuristic algorithm that finds a local optimum (which is a global optimum in many cases) in O(N2) and a monitor that obtains feedback about the actual database performance. This optimization model is general and is not specific to any data model. Our experimental results show that a predictor for the optimal solution cannot be obtained in any intuitive or analytic way, due to the complexity of the involved considerations; thus, there is no obvious way to achieve these results without using the optimization model. This fact is a strong motivation for applying such an optimization model. Our experimental results further indicate that the optimization model is useful in the sense that the system performance (with respect to the applications' goal function) is substantially improved compared to any universal materialization policy.

#index 443030
#* Evaluating Aggregate Operations Over Imprecise Data
#@ Arbee L. P. Chen;Jui-Shang Chiu;Frank S. C. Tseng
#t 1996
#c 7
#% 2783
#% 12515
#% 27056
#% 48721
#% 55882
#% 64411
#% 66284
#% 77969
#% 108510
#% 119816
#% 158203
#% 169319
#% 287313
#% 287333
#% 408396
#% 435122
#% 442692
#% 442830
#% 463119
#% 480102
#% 836124
#% 836134
#! Imprecise data in databases were originally denoted as null values, which represent the meaning of "values unknown at present." More generally, a partial value corresponds to a finite set of possible values for an attribute in which exactly one of the values is the "true" value. In this paper, we define a set of extended aggregate operations, namely sum, average, count, maximum, and minimum, which can be applied to an attribute containing partial values. Two types of aggregate operators are considered: scalar aggregates and aggregate functions. We study the properties of the aggregate operations and develop efficient algorithms for count, maximum and minimum. However, for sum and average, we point out that in general it takes exponential time complexity to do the computations.

#index 443031
#* An Object-Oriented Database System Jasmine: Implementation, Application, and Extension
#@ Hiroshi Ishikawa;Yasuo Yamane;Yoshio Izumida;Nobuaki Kawato
#t 1996
#c 7
#% 103
#% 2035
#% 3821
#% 6797
#% 10015
#% 23959
#% 24408
#% 25873
#% 32887
#% 37909
#% 38687
#% 41665
#% 43210
#% 57952
#% 65640
#% 67858
#% 68197
#% 86946
#% 111351
#% 111372
#% 111913
#% 135556
#% 442692
#% 442703
#% 442704
#% 463859
#% 479584
#% 481122
#! New applications such as engineering tasks require complex object modeling, integration of database and programming facilities, and extensibility. We have devised an Object-Oriented DBMS called Jasmine for such advanced applications. This paper describes the implementation, application, and extension of Jasmine in detail. First, we focus on the impact of the design of its Object-Oriented model and language on database implementation technology. We describe what part of traditional relational database technology we extend to handle Object-Oriented features such as object identifiers, complex objects, class hierarchies, and methods. We introduce nested relations to efficiently store and access clustered complex objects. We use hash-based methods to efficiently access nonclustered complex objects. We provide user-defined functions directly evaluated on page buffers to efficiently process method invocation. We devise Object-Oriented optimization of queries including class hierarchies, complex objects, and method invocation. We incorporate dedicated object buffering to allow efficient access to objects through object identifiers. Second, we describe nontrivial applications of Jasmine and discuss the validity of Object-Oriented databases. We focus on a constraint management facility, which can be implemented by taking advantage of the extensibility of Jasmine. The facility includes constraint rules, called design goals, for automatic database population required by engineering applications. Third, we describe a view facility for schema integration also needed by engineering applications in distributed environments. We focus on how we extend Jasmine to implement the facility.

#index 443032
#* VELOS: A New Approach for Efficiently Achieving High Availability in Partitioned Distributed Systems
#@ Peter Triantafillou;David J. Taylor
#t 1996
#c 7
#% 3083
#% 7576
#% 9241
#% 11816
#% 18612
#% 32895
#% 54515
#% 55399
#% 100594
#% 287226
#% 289207
#% 290469
#% 291888
#% 317986
#% 444142
#% 444356
#% 462342
#% 534393
#% 602675
#% 602806
#% 694478
#! This work presents a new protocol, VELOS, for tolerating partitionings in distributed systems with replicated data. Our primary goals were influenced by efficiency and availability constraints. The proposed protocol achieves optimal availability, according to a well known metric, while ensuring one-copy serializability. In addition, however, VELOS is designed to reduce the cost involved in achieving high availability. We have developed mechanisms through which transactions, in the absence of failures, can access replicated data objects and observe shorter delays than related protocols, and impose smaller loads on the network and the servers. Furthermore, VELOS offers high availability without relying on system transactions that must execute to restore availability when failures and recoveries occur. Such system transactions typically access all (replicas of all) data objects and thus introduce significant delays to user transactions and consume large quantities of resources such as network bandwidth and CPU cycles. Thus, we offer our protocol as a proof that high availability can be achieved inexpensively.

#index 443033
#* Speeding Up External Mergesort
#@ LuoQuan Zheng;Per-Åke Larson
#t 1996
#c 7
#% 41684
#% 41824
#% 59967
#% 64730
#% 68010
#% 77937
#% 86928
#% 88336
#% 97833
#% 252608
#% 368123
#% 463126
#% 646782
#! External mergesort is normally implemented so that each run is stored contiguously on disk and blocks of data are read exactly in the order they are needed during merging. We investigate two ideas for improving the performance of external mergesort: interleaved layout and a new reading strategy. Interleaved layout places blocks from different runs in consecutive disk addresses. This is done in the hope that interleaving will reduce seek overhead during merging. The new reading strategy precomputes the order in which data blocks are to be read according to where they are located on disk and when they are needed for merging. Extra buffer space makes it possible to read blocks in an order that reduces seek overhead, instead of reading them exactly in the order they are needed for merging. A detailed simulation model was used to compare the two layout strategies and three reading strategies. The effects of using multiple work disks were also investigated. We found that, in most cases, interleaved layout does not improve performance, but that the new reading strategy consistently performs better than double buffering and forecasting.

#index 443034
#* On-Line Clustering
#@ Athman Bouguettaya
#t 1996
#c 7
#% 47621
#% 59348
#% 102745
#% 102746
#% 114418
#% 243299
#% 696604
#! In this study, we focus on the stability and behavior of three widely used clustering algorithms. Various correlation coefficients are computed to help us understand the sensitivity of object clustering. Surprisingly, the results indicate that there is almost no difference among any clustering approach. Furthermore, all methods appear to be near stable. These findings tend to show that the clustering algorithms are independent of the way objects are inherently clustered.

#index 443035
#* On Coupling Multiple Systems With A Global Buffer
#@ Ming-Syan Chen;Philip S. Yu;Tao-Heng Yang
#t 1996
#c 7
#% 102802
#% 102803
#% 114582
#% 135877
#% 169487
#% 287304
#! In this paper, we conduct a performance study of coupling multiple systems with a global buffer, and present several results obtained from a multiple-system simulator. This simulator has been run against three workloads, and the coupled system behavior with these three different inputs is studied. Several statistics, including those on local and global buffer hits, page writes to the global buffer, cross-invalidations, and castouts are reported. Their relationship to the degree of data skew is explored. Moreover, in addition to the update-caching approach, a design alternative for the use of a global buffer, namely read-caching, is explored. In read-caching, not only updated pages but also pages read by each node are kept in the global buffer, thereby facilitating other nodes' access to the same pages at the cost of a higher global buffer usage. Also investigated is the case of no-caching, i.e., without using a global buffer. Several simulation results are presented and analyzed.

#index 443036
#* Describing Database Objects in a Concept Language Environment
#@ Alessandro Artale;Francesca Cesarini;Giovanni Soda
#t 1996
#c 7
#% 413
#% 58356
#% 64441
#% 79502
#% 101435
#% 102966
#% 107233
#% 108692
#% 108697
#% 116299
#% 117899
#% 462510
#% 464543
#% 556363
#% 556383
#! In this paper, we formally investigate the structural similarities and differences existing between object database models and concept languages establishing a correspondence between the two environments. Object Databases Models deal with two kinds of data: individual objects, which have an identity, and values, which can be basic values or can have complex structures containing both basic values and objects. Concept Languages only deal with individual objects. The correspondence points out the different role played by objects and values in both approaches and defines a way of properly mapping database descriptions into concept language descriptions at both a terminological and assertional level. Once the mapping is achieved, object databases can take advantage of both the algorithms and the results concerning their complexity developed in concept languages.

#index 443037
#* Correction to a Footnote in "Theoretical and Practical Considerations of Uncertainty and Complexity in Automated Knowledge Acquisition"
#@ Xiao-Jia M. Zhou;Tharam S. Dillon
#t 1996
#c 7
#% 92537
#% 442980
#! First Page of the Article

#index 443038
#* Current Approaches to Handling Imperfect Information in Data and Knowledge Bases
#@ Simon Parsons
#t 1996
#c 7
#% 273
#% 663
#% 1146
#% 1715
#% 3034
#% 3035
#% 7689
#% 10324
#% 26793
#% 28052
#% 30838
#% 32879
#% 36534
#% 39263
#% 42003
#% 42480
#% 44876
#% 47717
#% 48720
#% 59340
#% 59918
#% 61038
#% 65346
#% 67866
#% 68244
#% 71809
#% 77650
#% 84656
#% 86371
#% 89160
#% 89846
#% 95339
#% 100152
#% 101207
#% 101210
#% 101217
#% 101238
#% 101241
#% 101242
#% 101248
#% 101259
#% 102767
#% 108510
#% 118746
#% 121990
#% 128612
#% 128626
#% 129198
#% 130112
#% 130150
#% 136360
#% 136554
#% 144840
#% 147677
#% 154985
#% 164547
#% 169329
#% 171996
#% 287007
#% 287313
#% 287333
#% 289333
#% 366687
#% 369148
#% 370348
#% 416017
#% 442863
#% 444968
#% 457177
#% 459572
#% 470026
#% 470044
#% 470340
#% 480102
#% 480789
#% 496558
#% 503345
#% 527518
#% 527675
#% 527848
#% 527851
#% 562033
#% 567872
#% 681574
#! This paper surveys methods for representing and reasoning with imperfect information. It opens with an attempt to classify the different types of imperfection that may pervade data, and a discussion of the sources of such imperfections. The classification is then used as a framework for considering work that explicitly concerns the representation of imperfect information, and related work on how imperfect information may be used as a basis for reasoning. The work that is surveyed is drawn from both the field of databases and the field of artificial intelligence. Both of these areas have long been concerned with the problems caused by imperfect information, and this paper stresses the relationships between the approaches developed in each.

#index 443039
#* Intelligent Query Answering by Knowledge Discovery Techniques
#@ Jiawei Han;Yue Huang;Nick Cercone;Yongjian Fu
#t 1996
#c 7
#% 4797
#% 29252
#% 36683
#% 38696
#% 77960
#% 86936
#% 97632
#% 111378
#% 152934
#% 152935
#% 232147
#% 287631
#% 442678
#% 452747
#% 462336
#! Knowledge discovery facilitates querying database knowledge and intelligent query answering in database systems. In this paper, we investigate the application of discovered knowledge, concept hierarchies, and knowledge discovery tools for intelligent query answering in database systems. A knowledge-rich data model is constructed to incorporate discovered knowledge and knowledge discovery tools. Queries are classified into data queries and knowledge queries. Both types of queries can be answered directly by simple retrieval or intelligently by analyzing the intent of query and providing generalized, neighborhood or associated information using stored or discovered knowledge. Techniques have been developed for intelligent query answering using discovered knowledge and/or knowledge discovery tools, which includes generalization, data summarization, concept clustering, rule discovery, query rewriting, deduction, lazy evaluation, application of multiple-layered databases, etc. Our study shows that knowledge discovery substantially broadens the spectrum of intelligent query answering and may have deep implications on query answering in data- and knowledge-base systems.

#index 443040
#* Objective-Driven Monitoring for Broadband Networks
#@ Subrata Mazumdar;Aurel A. Lazar
#t 1996
#c 7
#% 2187
#% 18401
#% 36227
#% 48044
#% 287631
#% 698023
#! An approach to sensor configuration, installation, and activation for real-time monitoring of broadband networks for managing its performance is presented. An objective-driven measurement strategy for establishing the dynamic and statistical databases of the network is described. Objective driven monitoring allows the activation of sensors for data collection and abstraction based on a set of objectives. The objectives are derived from the quality of service requirements for real-time traffic control and operator submitted queries. The methodology of objective-driven monitoring for selective activation of sensors is implemented as a set of rules in the knowledge base of the monitor.

#index 443041
#* Testing Expert Systems in Process Control
#@ Kai Finke;Matthias Jarke;Roland Soltysiak;Peter Szczurko
#t 1996
#c 7
#% 386
#% 20561
#% 21072
#% 63438
#% 69325
#% 76601
#% 116005
#% 116007
#% 367346
#% 374973
#% 515505
#! Special features of process control expert systems (PCX) make it both necessary and feasible to test them in a comprehensive manner. FAITH, an automated regression testing environment for PCX, integrates five specially adapted testing techniques, exploiting external specifications gained from the process control environment. FAITH also offers a set of metrics which can be used to predict testability during the early phases of PCX development. FAITH has been developed for a major German chemicals company where it is routinely used for certifying PCX, and for guiding design-for-testability.

#index 443042
#* Optimization of Parallel Execution for Multi-Join Queries
#@ Ming-Syan Chen;Philip S. Yu;Kun-Lung Wu
#t 1996
#c 7
#% 32911
#% 36117
#% 58352
#% 60049
#% 67460
#% 77937
#% 83133
#% 83154
#% 83232
#% 114577
#% 115661
#% 116040
#% 116041
#% 116084
#% 119485
#% 126522
#% 152915
#% 172907
#% 186169
#% 201956
#% 286377
#% 286386
#% 318049
#% 339624
#% 339644
#% 339651
#% 339654
#% 339727
#% 339913
#% 411554
#% 435117
#% 442676
#% 442698
#% 442700
#% 442729
#% 442976
#% 444245
#% 452786
#% 462175
#% 462651
#% 463108
#% 473077
#% 480258
#% 480615
#% 480966
#% 481104
#% 533362
#! In this paper, we study the subject of exploiting interoperator parallelism to optimize the execution of multi-join queries. Specifically, we focus on two major issues: 1) scheduling the execution sequence of multiple joins within a query, and 2) determining the number of processors to be allocated for the execution of each join operation obtained in 1). For the first issue, we propose and evaluate by simulation several methods to determine the general join sequences, or bushy trees. Despite their simplicity, the heuristics proposed can lead to the general join sequences that significantly outperform the optimal sequential join sequence. The quality of the join sequences obtained by the proposed heuristics is shown to be fairly close to that of the optimal one. For the second issue, it is shown that the processor allocation for exploiting interoperator parallelism is subject to more constraints驴such as execution dependency and system fragmentation驴than those in the study of intraoperator parallelism for a single join. The concept of synchronous execution time is proposed to alleviate these constraints. Several heuristics to deal with the processor allocation, categorized by bottom-up and top-down approaches, are derived and are evaluated by simulation. The relationship between issues 1) and 2) is explored. Among all the schemes evaluated, the two-step approach proposed, which first applies the join sequence heuristic to build a bushy tree as if under a single processor system, and then, in light of the concept of synchronous execution time, allocates processors to execute each join in the bushy tree in a top-down manner, emerges as the best solution to minimize the query execution time.

#index 443043
#* Parallel Optimization of Large Join Queries with Set Operators and Aggregates in a Parallel Environment Supporting Pipeline
#@ Myra Spiliopoulou;Michael Hatzopoulos;Yannis Cotronis
#t 1996
#c 7
#% 3771
#% 25998
#% 32877
#% 43161
#% 58376
#% 86949
#% 102765
#% 116040
#% 116041
#% 136740
#% 139840
#% 169843
#% 287005
#% 318049
#% 411554
#% 435116
#% 442864
#% 463108
#% 479938
#% 480091
#% 480615
#% 480955
#% 481104
#% 481110
#% 481289
#% 481429
#% 499813
#% 500123
#% 509413
#! We propose a parallel optimizer for queries containing a large number of joins, as well as set operators and aggregate functions. The platform of execution is a shared-disk multiprocessor machine supporting bushy parallelism and pipeline. Our model partitions the query into almost independent subtrees that can be optimized simultaneously and applies an enhanced variation of the iterative improvement technique on those of the subtrees, which contain a large number of joins. This technique is parallelized, too. In order to estimate the cost of the states constructed during optimization of join subtrees, cost formulae are developed that estimate the cost of relational algebra operators when executed across coalescing pipes.

#index 443044
#* Path Signatures: A Way to Speed Up Recursion in Relational Databases
#@ Jukka Teuhola
#t 1996
#c 7
#% 3771
#% 13014
#% 13044
#% 32894
#% 47621
#% 57963
#% 59536
#% 75571
#% 83330
#% 88051
#% 102761
#% 114578
#% 139176
#% 411744
#% 442705
#% 479913
#% 480081
#% 481098
#% 565445
#% 566109
#! Composite objects often involve recursive relationships, so called bills-of-materials, which are cumbersome to handle in relational database systems. The relationships constitute a directed graph, where the successors of a node represent its components, recursively. Instead of the whole transitive closure (all ancestor-descendant pairs), the task is here to retrieve the descendants of any given node. A simple relational solution is suggested, which packs information of the ancestor path of each node into a fixed-length code, called the signature. The code is nonunique, and its purpose is to define a relatively small superset of the descendants, as well as establish a basis for clustering. It supports effective retrieval of the descendants, in terms of both disk accesses and DBMS calls. The method performs best for tree-structured graphs, where the processing time typically decreases by a factor of more than 10, compared to a simple loop of joins. Also general directed graphs, both acyclic and cyclic, can be processed more effectively. The method is implemented on top of a relational system, but advantages can be gained on other platforms, too.

#index 443045
#* A Graph-Based Framework for Multiparadigmatic Visual Access to Databases
#@ Tiziana Catarci;Shi-Kuo Chang;Maria F. Costabile;Stefano Levialdi;Giuseppe Santucci
#t 1996
#c 7
#% 2008
#% 3081
#% 36309
#% 36683
#% 38696
#% 77933
#% 77996
#% 83541
#% 91432
#% 109495
#% 109496
#% 116048
#% 116588
#% 134451
#% 140750
#% 169946
#% 268797
#% 277320
#% 527869
#% 527877
#% 599549
#! We describe an approach for multiparadigmatic visual access to databases, which is proposed to achieve seamless integration of different interaction paradigms. The user is provided with an adaptive interface augmented by a user model, supporting different visual representations of both data and queries. The visual representations are characterized on the basis of the chosen visual formalisms, namely forms, diagrams, and icons. To access different databases, a unified data model, the Graph Model, is used as a common underlying formalism to which databases, expressed in the most popular data models, can be mapped. Graph Model databases are queried through the adaptive interface. The semantics of the query operations is formally defined in terms of graphical primitives. Such a formal approach permits us to define the concept of "atomic query," which is the minimal portion of a query that can be transferred from one interaction paradigm to another and processed by the system. Since certain interaction modalities and visual representations are more suitable for certain user classes, the system can suggest to the user the most appropriate interaction modality as well as the visual representation, according to the user model. Some results on user model construction are presented.

#index 443046
#* A Trie Compaction Algorithm for a Large Set of Keys
#@ Jun-ichi Aoe;Katsushi Morimoto;Masami Shishibori;Ki-Hong Park
#t 1996
#c 7
#% 119
#% 662
#% 3888
#% 24076
#% 24409
#% 36315
#% 66210
#% 99032
#% 105938
#% 131327
#% 139608
#% 252608
#% 287317
#% 295947
#% 319848
#% 320817
#% 321327
#% 326878
#% 374720
#% 374865
#% 407822
#! A trie structure is frequently used for various applications, such as natural language dictionaries, database systems, and compilers. However, the total number of states (and transitions between them) of a trie becomes large so that space cost may not be acceptable for a huge key set. In order to resolve this disadvantage, this paper presents a new scheme, called "two-trie," that enables us to perform efficient retrievals, insertions, and deletions for the key sets. The essential idea is to construct two tries for both front and rear compressions of keys, which is similar to a DAWG (Directed Acyclic Word-Graph). The approach differs from a DAWG in that the two-trie approach presented can uniquely determine information corresponding to keys while a DAWG cannot. For an efficient implementation of the two-trie, two types of data structures are introduced. The theoretical and experimental observations show that the method presented is more practical than existing ones considering the use of dynamic key sets, storing information of keys, and compression of transitions.

#index 443047
#* Principles for Organizing Semantic Relations in Large Knowledge Bases
#@ Larry M. Stephens;Yufeng F. Chen
#t 1996
#c 7
#% 2078
#% 101509
#% 108691
#% 159120
#% 174882
#% 405391
#! This paper defines principles for organizing semantic relations represented by slots in frame-structured knowledge bases. We organize slots based on the knowledge-level semantics of relations and the symbol-level function of slots that implement the representation language. The symbol-level organization of slots depends on the inferencing and expressive capabilities of the knowledge representation system. At the knowledge level, two entirely different organizational schemes are identified: one based on linguistic similarities and differences, and another based on the types of concepts being related.

#index 443048
#* A Deductive Object-Oriented Database System for Situated Inference in Law
#@ Stephen Wong;Satoshi Tojo
#t 1996
#c 7
#% 19638
#% 58354
#% 73005
#% 97127
#% 142470
#% 171710
#! Deductive Object-Oriented Databases and Situation Theory are two important areas of research in the fields of database and of linguistics. AI and Law is a new field attracting both AI researchers and legal practitioners. Our research brings together the former two fields with the aim of designing knowledge applications in the latter. This is achieved through a formal model for legal reasoning, ${\cal SM}$, and a deductive object-oriented database system, ${\cal QUIX}\!O\!{\cal TE}$. The purpose of this paper is to introduce the key features of this formal model, based on situation theory, and to describe how this database system can implement this abstract model for complex legal reasoning applications. Concrete examples from legal precedents are used to illustrate these advanced features.

#index 443049
#* Towards the Correctness and Consistency of Update Semantics in Semantic Database Schema
#@ Joan Peckham;Fred Maryanski;Steven A. Demurjian
#t 1996
#c 7
#% 1869
#% 27043
#% 33003
#% 38696
#% 53706
#% 54046
#% 58361
#% 59348
#% 65658
#% 82850
#% 83315
#% 86944
#% 108500
#% 151164
#% 158051
#% 233619
#% 234905
#% 287631
#% 374401
#% 377755
#% 407822
#% 442727
#% 480762
#% 535016
#% 561901
#% 564413
#% 697626
#! This paper discusses a paradigm and prototype system for the design-time expression, checking, and automatic implementation of the semantics of database updates. Here, enforcement rules are viewed as the implementation of constraints and are specified, checked for consistency, and then finally mapped to object-oriented code during database design. A classification of enforcement rule types is provided as a basis for these design activities, and the general strategy for specification, analysis, and implementation of these rules within a semantic modeling paradigm is discussed. SORAC (semantic, objects, relationships, and constraints), a prototype database design system at the University of Rhode Island, is also described.

#index 443050
#* An Optimal Resource Scheduler for Continuous Display of Structured Video Objects
#@ Martha L. Escobar-Molano;Shahram Gandeharizadeh;Douglas Ierardi
#t 1996
#c 7
#% 1156
#% 124017
#% 152943
#% 201842
#% 442097
#% 444276
#% 460864
#% 481282
#% 511160
#! A structured video consists of a collection of background objects, characters, spatial and temporal constructs, and rendering features. Assuming a platform consisting of a fixed amount of memory and a magnetic disk drive, this study presents a resource scheduler for continuous display of structured video that minimizes both the latency observed by a display and its required amount of memory.

#index 443051
#* GUEST EDITOR'S INTRODUCTION: Special Section on Digital Libraries
#@ Nabil R. Adam;Yelena Yesha
#t 1996
#c 7
#! First Page of the Article

#index 443052
#* Boolean Query Mapping Across Heterogeneous Information Sources
#@ Kevin Chen-Chuan Chang;Hector Garcia-Molina;Andreas Paepcke
#t 1996
#c 7
#% 3582
#% 11929
#% 36683
#% 55490
#% 115462
#% 363236
#% 672354
#% 672631
#% 727574
#! Searching over heterogeneous information sources is difficult because of the nonuniform query languages. Our approach is to allow a user to compose Boolean queries in one rich front-end language. For each user query and target source, we transform the user query into a subsuming query that can be supported by the source but that may return extra documents. The results are then processed by a filter query to yield the correct final result. In this paper we introduce the architecture and associated algorithms for generating the supported subsuming queries and filters. We show that generated subsuming queries return a minimal number of documents; we also discuss how minimal cost filters can be obtained. We have implemented prototype versions of these algorithms and demonstrated them on heterogeneous Boolean systems.

#index 443053
#* A Knowledge-Based Approach for Retrieving Images by Content
#@ Chih-Cheng Hsu;Wesley W. Chu;Ricky K. Taira
#t 1996
#c 7
#% 154336
#% 184249
#% 213443
#% 435139
#% 437409
#% 442800
#% 442902
#% 445709
#% 452795
#% 480950
#% 526851
#! A knowledge-based approach is introduced for retrieving images by content. It supports the answering of conceptual image queries involving similar-to predicates, spatial semantic operators, and references to conceptual terms. Interested objects in the images are represented by contours segmented from images. Image content such as shapes and spatial relationships are derived from object contours according to domain-specific image knowledge. A three-layered model is proposed for integrating image representations, extracted image features, and image semantics. With such a model, images can be retrieved based on the features and content specified in the queries. The knowledge-based query processing is based on a query relaxation technique. The image features are classified by an automatic clustering algorithm and represented by Type Abstraction Hierarchies (TAHs) for knowledge-based query processing. Since the features selected for TAH generation are based on context and user profile, and the TAHs can be generated automatically by a clustering algorithm from the feature database, our proposed image retrieval approach is scalable and context-sensitive. The performance of the proposed knowledge-based query processing is also discussed.

#index 443054
#* Picture Similarity Retrieval Using the 2D Projection Interval Representation
#@ Mohammad Nabil;Anne H. H. Ngu;John Shepherd
#t 1996
#c 7
#% 23998
#% 68781
#% 83962
#% 101930
#% 102772
#% 116335
#% 124680
#% 131454
#% 137799
#% 173637
#% 319244
#% 463414
#% 527010
#% 546590
#! Spatial relationships are important ingredients for expressing constraints in retrieval systems for pictorial or multimedia databases. We have proposed a unified representation for spatial relationships, 2D Projection Interval Relationships (2D-PIR), that integrates both directional and topological relationships. In this paper we develop techniques for similarity retrieval based on the 2D-PIR representation, including a method for dealing with rotated and reflected images.

#index 443055
#* Tries for Approximate String Matching
#@ H. Shang;T. h. Merrettal
#t 1996
#c 7
#% 15033
#% 115467
#% 115472
#% 120648
#% 120649
#% 121270
#% 131061
#% 174226
#% 215300
#% 288578
#% 288885
#% 289010
#% 317975
#% 320454
#% 324015
#% 460866
#% 489950
#% 545961
#! Tries offer text searches with costs which are independent of the size of the document being searched, and so are important for large documents requiring spelling checkers, case insensitivity, and limited approximate regular secondary storage. Approximate searches, in which the search pattern differs from the document by k substitutions, transpositions, insertions or deletions, have hitherto been carried out only at costs linear in the size of the document. We present a trie-based method whose cost is independent of document size. Our experiments show that this new method significantly outperforms the nearest competitor for k = 0 and k = 1, which are arguably the most important cases. The linear cost (in k) of the other methods begins to catch up, for our small files, only at k = 2. For larger files, complexity arguments indicate that tries will outperform the linear methods for larger values of k. Trie indexes combine suffixes and so are compact in storage. When the text itself does not need to be stored, as in a spelling checker, we even obtain negative overhead: 50% compression. We discuss a variety of applications and extensions, including best match (for spelling checkers), case insensitivity, and limited approximate regular expression matching.

#index 443056
#* WISE: A World Wide Web Resource Database System
#@ Budi Yuwono;Dik Lun Lee
#t 1996
#c 7
#% 46803
#% 127586
#% 176501
#% 406493
#% 464229
#! This paper describes the World Wide Web Index and Search Engine (WISE) for Internet resource discovery. The system is designed around a resource database containing meta-information about WWW resources and is automatically built using an indexer robot, a special WWW client agent. The resource database allows users to search for resources based on keywords, and to learn about potentially relevant resources without having to directly access them. Such capabilities can significantly reduce the amount of time that a user needs to spend in order to find the information of his/her interest. We discuss WISE's main components: the resource database, the indexer robot, the search engine, and the user interface, and through the technical discussions, we highlight the research issues involved in the design, the implementation and the evaluation of such a system.

#index 443057
#* An Authorization Model for a Distributed Hypertext System
#@ Pierangela Samarati;Elisa Bertino;Sushil Jajodia
#t 1996
#c 7
#% 23929
#% 41664
#% 41666
#% 56450
#% 91075
#% 128261
#% 137954
#% 161722
#% 164560
#% 185253
#% 186320
#% 442862
#% 443103
#% 533349
#% 664550
#! Digital libraries support quick and efficient access to a large number of information sources that are distributed but interlinked. As the amount of information to be shared grows, the need to restrict access only to specific users or for specific usage will surely arise. The protection of information in digital libraries, however, is difficult because of the peculiarity of the hypertext paradigm which is generally used to represent information in digital libraries, together with the fact that related data in a hypertext are often distributed at different sites. In this paper, we present an authorization model for distributed hypertext systems. Our model supports authorizations at different granularity levels, takes into consideration different types of data and the relationships among them, and allows administrative privileges to be delegated.

#index 443058
#* Extending Existing Dependency Theory to Temporal Databases
#@ Christian S. Jensen;Richard Thomas Snodgrass;Michael D. Soo
#t 1996
#c 7
#% 3673
#% 5191
#% 10245
#% 16028
#% 18615
#% 18772
#% 32915
#% 36683
#% 43028
#% 49596
#% 58368
#% 61814
#% 64150
#% 68201
#% 140617
#% 163440
#% 178813
#% 225003
#% 286860
#% 287221
#% 287754
#% 287762
#% 289351
#% 361445
#% 411570
#% 435109
#% 442711
#% 442781
#% 461863
#% 503375
#% 527791
#% 535040
#% 546596
#% 687518
#% 688969
#% 690047
#% 693457
#% 694730
#! Normal forms play a central role in the design of relational databases. Several normal forms for temporal relational databases have been proposed. These definitions are particular to specific temporal data models, which are numerous and incompatible. This paper attempts to rectify this situation. We define a consistent framework of temporal equivalents of the important conventional database design concepts: functional dependencies, primary keys, and third and Boyce-Codd normal forms. This framework is enabled by making a clear distinction between the logical concept of a temporal relation and its physical representation. As a result, the role played by temporal normal forms during temporal database design closely parallels that of normal forms during conventional database design. These new normal forms apply equally well to all temporal data models that have timeslice operators, including those employing tuple timestamping, backlogs, and attribute value timestamping. As a basis for our research, we conduct a thorough examination of existing proposals for temporal dependencies, keys, and normal forms. To demonstrate the generality of our approach, we outline how normal forms and dependency theory can also be applied to spatial and spatiotemporal databases.

#index 443059
#* The Starburst Active Database Rule System
#@ Jennifer Widom
#t 1996
#c 7
#% 1797
#% 43209
#% 45257
#% 83315
#% 86941
#% 86944
#% 86946
#% 116044
#% 116045
#% 120696
#% 123589
#% 125951
#% 149590
#% 152910
#% 152921
#% 167258
#% 170898
#% 394417
#% 442706
#% 458563
#% 462654
#% 463117
#% 463419
#% 480600
#% 480620
#% 480621
#% 480623
#% 480765
#% 480768
#% 480779
#% 480800
#% 480942
#% 481106
#% 481130
#! This paper describes our development of the Starburst Rule System, an active database rules facility integrated into the Starburst extensible relational database system at the IBM Almaden Research Center. The Starburst rule language is based on arbitrary database state transitions rather than tuple- or statement-level changes, yielding a clear and flexible execution semantics. The rule system has been implemented completely. Its rapid implementation was facilitated by the extensibility features of Starburst, and rule management and rule processing are integrated into all aspects of database processing.

#index 443060
#* A Semiautomatic Method for Assigning Elevation in Contour Maps
#@ Marco A. G. M. Maia;Átila L. F. Xavier
#t 1996
#c 7
#% 234905
#% 373671
#! Assigning contour elevations during digitalization process of topographic charts can be a very time-consuming task and liable to human errors. The method here presented for this purpose is based upon the integration of three elements: the definition of nesting tree, a formal set of rules linking neighboring contour elevations and the concept of ambiguity interval. The information provided by the chart control points and its propagation on the nesting tree for ambiguity interval update allows to automatically evaluate a considerable number of elevations, typically 60%. The remaining curves are assessed with the help of an operator where an strategy is employed to minimize human interaction. At the end of the process, the typical figure describing the number of elevations requested by the system is around 20%.

#index 443061
#* On Satisfiability, Equivalence, and Implication Problems Involving Conjunctive Queries in Database Systems
#@ Sha Guo;Wei Sun;Mark A. Weiss
#t 1996
#c 7
#% 1451
#% 2530
#% 13016
#% 13029
#% 35562
#% 36181
#% 36191
#% 36683
#% 59365
#% 68086
#% 69272
#% 70370
#% 103264
#% 115193
#% 268788
#% 287336
#% 287667
#% 327432
#% 368248
#% 407822
#% 411644
#% 442678
#% 442851
#% 442924
#% 479770
#! Satisfiability, equivalence, and implication problems involving conjunctive queries are important and widely encountered problems in database management systems. These problems need to be efficiently and effectively solved. In this paper, we consider queries which are conjunctions of the inequalities of the form (XopC), (XopY), and/or (XopY + C), where X and Y are two attributes, C is a constant, and op驴 {, 驴}. These types of inequalities are widely used in database systems, since the first type is a selection, the second type is a 驴-join, and the third type is a very popular clause in a deductive database system. The satisfiability, equivalence, and implication problems in the integer domain (for attributes and constants) have been shown to be NP-hard [20], [24]. However, we show that these problems can be solved efficiently in the real domain. The incorporation of the real domain is significant, because the real domain is practically and widely used in a database. Necessary and sufficient conditions and algorithms are presented. A novel concept of the "modulo closure" and a set of sound and complete axioms with respect to the "modulo closure" are also proposed to infer all correct and necessary inequalities from a given query. The proposed axioms generalize Ullman's axioms [27] where queries only consist of 驴-joins.

#index 443062
#* The Strong Partial Transitive-Closure Problem: Algorithms and Performance Evaluation
#@ Ismail H. Toroslu;Ghassan Z. Qadah
#t 1996
#c 7
#% 3771
#% 11797
#% 13014
#% 47620
#% 77979
#% 82355
#% 83158
#% 88051
#% 91007
#% 91062
#% 100605
#% 119824
#% 123129
#% 139176
#% 172952
#% 427195
#% 458599
#% 462646
#% 463107
#% 463440
#% 479933
#% 480284
#! The development of efficient algorithms to process the different forms of transitive-closure (TC) queries within the context of large database systems has recently attracted a large volume of research efforts. In this paper, we present two new algorithms suitable for processing one of these forms, the so called strong partially instantiated transitive closure, in which one of the query's arguments is instantiated to a set of constants and the processing of which yields a set of tuples that draw their values from both of the query's instantiated and uninstantiated arguments. These algorithms avoids the redundant computations and high storage cost found in a number of similar algorithms. Using simulation, this paper compares the performance of the new algorithms with those found in literature and shows clearly the superiority of the new algorithms.

#index 443063
#* An Architectural Framework for CKBS Applications
#@ S. m. Deen
#t 1996
#c 7
#% 52276
#% 52277
#% 52294
#% 52300
#% 52303
#% 52319
#% 442655
#% 524886
#! This paper presents an architectural framework for cooperating knowledge based systems (CKBSs) with parallels drawn from the multiagent systems of DAI. A CKBS is distinguished from a multiagent system by its need to provide a workable approach for real-world distributed applications. The framework proposed considers only interagent activities in what is called transaction-oriented processing. The framework, based largely on well-tested computer science concepts, provides for a multilayered edifice with information transparency, and a multilevel schema to suit different user expertise. It permits dynamic definition of cooperation strategies for different tasks as required, in a high-level language providing relative ease of use. A particular novelty is the interpretation of actions as side-effects of update operations on action tuples transmitted among agents via what are called shadows. This provides the generality needed. Effectiveness, flexibility, and ease of use are some of the key considerations.

#index 443064
#* Customizing Transaction Models and Mechanisms in a Programmable Environment Supporting Reliable Workflow Automation
#@ Dimitrios Georgakopoulos;Mark F. Hornick;Frank Manola
#t 1996
#c 7
#% 9241
#% 32897
#% 36296
#% 55415
#% 67458
#% 68143
#% 77982
#% 83248
#% 83933
#% 86938
#% 86939
#% 112319
#% 122909
#% 122914
#% 172878
#% 185412
#% 287220
#% 287231
#% 403195
#% 435123
#% 442853
#% 461871
#% 463878
#% 480771
#% 481261
#! A Transaction Specification and Management Environment (TSME) is a programmable system that supports implementation-independent specification of application-specific extended transaction models (ETMs) and configuration of transaction management mechanisms (TMMs) to enforce specified ETMs. The TSME can ensure correctness and reliability while allowing the functionality required by workflows and other advanced applications that require access to multiple heterogeneous, autonomous, and/or distributed (HAD) systems. To support ETM specification, the TSME provides a transaction specification language that describes dependencies between transactions. Unlike other ETM specification languages, TSME's dependency descriptors use a common set of primitives, and are enforceable, i.e., can be evaluated at any time during transaction execution to determine whether operations issued violate ETM specifications. To determine whether an ETM can be enforced in a specific HAD system environment, the TSME supports specification of the transactional capabilities of HAD systems, and comparison of these with ETM specifications to determine mismatches. To enforce ETMs that are more restrictive than those supported by the union of the transactional capabilities of HAD systems, the TSME provides a collection of transactional services. These services are programmable and configurable, i.e., they accept instructions that change their behavior as required by an ETM and can be combined in specific ways to create a run-time TMM capable of enforcing the ETM. We discuss the TSME in the context of a distributed object management system. We give ETM specification examples and describe corresponding TMM configurations for a telecommunications application.

#index 443065
#* On the Complexity of Distributed Query Optimization
#@ Chihping Wang;Ming-Syan Chen
#t 1996
#c 7
#% 121
#% 992
#% 998
#% 1752
#% 1758
#% 2252
#% 3900
#% 3926
#% 44054
#% 51376
#% 67460
#% 91618
#% 285936
#% 286916
#% 287031
#% 289282
#% 408396
#% 408504
#% 411687
#% 411737
#% 416019
#% 442666
#% 442667
#% 442851
#% 442852
#% 444223
#% 452786
#% 463697
#% 463863
#% 565451
#% 688958
#! While a significant amount of research efforts has been reported on developing algorithms, based on joins and semijoins, to tackle distributed query processing, there is relatively little progress made toward exploring the complexity of the problems studied. As a result, proving NP-hardness of or devising polynomial-time algorithms for certain distributed query optimization problems has been elaborated upon by many researchers. However, due to its inherent difficulty, the complexity of the majority of problems on distributed query optimization remains unknown. In this paper we generally characterize the distributed query optimization problems and provide a frame work to explore their complexity. As it will be shown, most distributed query optimization problems can be transformed into an optimization problem comprising a set of binary decisions, termed Sum Product Optimization (SPO) problem. We first prove SPO is NP-hard in light of the NP-completeness of a well-known problem, Knapsack (KNAP). Then, using this result as a basis, we prove that five classes of distributed query optimization problems, which cover the majority of distributed query optimization problems previously studied in the literature, are NP-hard by polynomially reducing SPO to each of them. The detail for each problem transformation is derived. We not only prove the conjecture that many prior studies relied upon, but also provide a frame work for future related studies.

#index 443066
#* Editorial: Two Named to Editorial Board of IEEE Transactions on Knowledge and Data Engineering
#@ Benjamin W. Wah
#t 1996
#c 7

#index 443067
#* Editorial: Results of TKDE's 1995 Readership Survey
#@ Benjamin W. Wah
#t 1996
#c 7

#index 443068
#* Curriculum Knowledge Representation and Manipulation in Knowledge-Based Tutoring Systems
#@ Gang Zhou;Jason T. -L. Wang;Peter A. Ng
#t 1996
#c 7
#% 5562
#% 5572
#% 23806
#% 36105
#% 50395
#% 50397
#% 80484
#% 106902
#% 129357
#% 131807
#% 154962
#% 157234
#% 166245
#% 494111
#% 552449
#% 552463
#% 703366
#! A Knowledge-Based Tutoring System (KBTS) is a computer-based instructional system that uses artificial intelligence techniques to help people learn some subjects. We found that the knowledge communication process involving a KBTS and a human student can be decomposed into a series of communication cycles, where each cycle concentrates on one topic and contains four major phases: planning, discussing, evaluating and remedying. The major contributions of this work are the development of a generic architecture for supporting the knowledge communication between a KBTS and a student, and a graphical notation and schema for supporting the curriculum knowledge representation and manipulation during the planning phase of a tutoring process. The curriculum knowledge about a course can help a tutoring system determine the sequences in which the topics will be discussed with the students effectively and diagnose the students' mistakes. The curriculum knowledge base contains the goal structure of the course, prerequisite relations, and multiple ways of organizing topics, among others. As an example, we have focused on developing SQL-TUTOR, a KBTS for the domain of SQL programming. This system has features such as efficient control mechanism, explicit curriculum knowledge representation, and individualized private tutoring. For allowing the students relative freedom to decide how to study the domain knowledge about a subject, the system provides the students with a group of operators to hand-tailor the learning schedules according to their special backgrounds, requests, and interests.

#index 443069
#* An Evidential Reasoning Approach to Attribute Value Conflict Resolution in Database Integration
#@ Ee-Peng Lim;Jaideep Srivastava;Shashi Shekhar
#t 1996
#c 7
#% 6797
#% 28052
#% 38530
#% 55294
#% 74852
#% 126332
#% 158203
#% 412755
#% 442692
#% 442830
#% 452754
#% 463114
#% 463445
#% 482067
#! Resolving domain incompatibility among independently developed databases often involves uncertain information. DeMichiel [1] showed that uncertain information can be generated by the mapping of conflicting attributes to a common domain, based on some domain knowledge. In this paper, we show that uncertain information can also arise when the database integration process requires information not directly represented in the component databases, but can be obtained through some summary of data. We therefore propose an extended relational model based on Dempster-Shafer theory of evidence [2] to incorporate such uncertain knowledge about the source databases. The extended relation uses evidence sets to represent uncertainty in information, which allow probabilities to be attached to subsets of possible domain values. We also develop a full set of extended relational operations over the extended relations. In particular, an extended union operation has been formalized to combine two extended relations using Dempster's rule of combination. The closure and boundedness properties of our proposed extended operations are formulated. We also illustrate the use of extended operations by some query examples.

#index 443070
#* Using Compiled Knowledge to Guide and Focus Abductive Diagnosis
#@ Luca Console;Luigi Portinale;Daniele Theseider Dupré
#t 1996
#c 7
#% 1121
#% 4629
#% 19949
#% 21137
#% 21138
#% 38670
#% 44876
#% 53410
#% 59913
#% 67569
#% 78634
#% 95580
#% 105619
#% 107135
#% 110188
#% 110365
#% 116293
#% 132053
#% 132173
#% 150840
#% 168280
#% 405539
#% 444751
#% 449587
#% 451031
#% 527820
#% 541199
#% 669901
#! Several artificial intelligence architectures and systems based on "deep" models of a domain have been proposed, in particular for the diagnostic task. These systems have several advantages over traditional knowledge based systems, but they have a main limitation in their computational complexity. One of the ways to face this problem is to rely on a knowledge compilation phase, which produces knowledge that can be used more effectively with respect to the original one.In this paper we show how a specific knowledge compilation approach can focus reasoning in abductive diagnosis, and, in particular, can improve the performances of AID, an abductive diagnosis system. The approach aims at focusing the overall diagnostic cycle in two interdependent ways: avoiding the generation of candidate solutions to be discarded a posteriori and integrating the generation of candidate solutions with discrimination among different candidates. Knowledge compilation is used off-line to produce operational (i.e., easily evaluated) conditions that embed the abductive reasoning strategy and are used in addition to the original model, with the goal of ruling out parts of the search space or focusing on parts of it. The conditions are useful to solve most cases using less time for computing the same solutions, yet preserving all the power of the model-based system for dealing with multiple faults and explaining the solutions. Experimental results showing the advantages of the approach are presented.

#index 443071
#* A Logic Programming Framework for Modeling Temporal Objects
#@ F. Nihan Kesim;Marek Sergot
#t 1996
#c 7
#% 7047
#% 7679
#% 23874
#% 23932
#% 30567
#% 58354
#% 64442
#% 83109
#% 89644
#% 116987
#% 124757
#% 126704
#% 135384
#% 149632
#% 163440
#% 168773
#% 178813
#% 181056
#% 277335
#% 442702
#% 458584
#% 480251
#% 480599
#! We present a general approach for modeling temporal aspects of objects in a logic programming framework. Change is formulated in the context of a database which stores explicitly a record of all changes that have occurred to objects and thus (implicitly) all states of objects in the database. A snapshot of the database at any given time is an object-oriented database, in the sense that it supports an object-based data model. An object is viewed as a collection of simple atomic formulas, with support for an explicit notion of object identity, classes and inheritance. The event calculus is a treatment of time and change in first-order classical logic augmented with negation as failure. The paper develops a variant of the event calculus for representing changes to objects, including change in internal state of objects, creation and deletion of objects, and mutation of objects over time. The concluding sections present two natural and straightforward extensions, to deal with versioning of objects and schema evolution, and a sketch of implementation strategies for practical application to temporal object-oriented databases.

#index 443072
#* Computation of Stable Models and Its Integration with Logical Query Processing
#@ Weidong Chen;David Scott Warren
#t 1996
#c 7
#% 33376
#% 55408
#% 64408
#% 91617
#% 95248
#% 99452
#% 103704
#% 103705
#% 123056
#% 125938
#% 130418
#% 137876
#% 146254
#% 147502
#% 154317
#% 169171
#% 176464
#% 181064
#% 190636
#% 268771
#% 268779
#% 289377
#% 442955
#% 515884
#! The well-founded semantics and the stable model semantics capture intuitions of the skeptical and credulous semantics in nonmonotonic reasoning, respectively. They represent the two dominant proposals for the declarative semantics of deductive databases and logic programs. However, neither semantics seems to be suitable for all applications. We have developed an efficient implementation of goal-oriented effective query evaluation under the well-founded semantics. It produces a residual program for subgoals that are relevant to a query, which contains facts for true instances and clauses with body literals for undefined instances. This paper presents a simple method of stable model computation that can be applied to the residual program of a query to derive answers with respect to stable models. The method incorporates both forward and backward chaining to propagate the assumed truth values of ground atoms, and derives multiple stable models through backtracking. Users are able to request that only stable models satisfying certain conditions be computed. A prototype has been developed that provides integrated query evaluation under the well-founded semantics, the stable models, and ordinary Prolog execution. We describe the user interface of the prototype and present some experimental results.

#index 443073
#* Decomposition of Knowledge for Concurrent Processing
#@ Gilbert Babin;Cheng Hsu
#t 1996
#c 7
#% 58361
#% 86946
#% 102800
#% 112279
#% 126340
#% 128875
#% 129167
#% 174003
#% 463123
#! In some environments, it is more difficult for distributed systems to cooperate. In fact, some distributed systems are highly heterogeneous and might not readily cooperate. In order to alleviate these problems, we have developed an environment that preserves the autonomy of the local systems, while enabling distributed processing. This is achieved by 1) modeling the different application systems into a central knowledge base (called a Metadatabase), 2) providing each application system with a local knowledge processor, and 3) distributing the knowledge within these local shells. This paper is concerned with describing the knowledge decomposition process used for its distribution. The decomposition process is used to minimize the needed cooperation among the local knowledge processors, and is accomplished by "serializing" the rule execution process. A rule is decomposed into a ordered set of subrules, each of which is executed in sequence and located in a specific local knowledge processor. The goals of the decomposition algorithm are to minimize the number of subrules produced, hence reducing the time spent in communication, and to assure that the sequential execution of the subrules is "equivalent" to the execution of the original rule.

#index 443074
#* Using Reconfiguration for Efficient Management of Replicated Data
#@ Divyakant Agrawal;Amr El Abbadi
#t 1996
#c 7
#% 870
#% 2027
#% 3083
#% 3765
#% 18612
#% 55399
#% 59265
#% 69274
#% 83137
#% 91630
#% 112202
#% 116068
#% 131549
#% 193433
#% 287303
#% 290470
#% 291888
#% 318403
#% 320902
#% 442839
#% 462022
#% 531907
#% 602675
#! Replicated data management protocols have been proposed that exploit a logically structured set of copies. These protocols have the advantage that they provide limited fault-tolerance at low communication cost. The proposed protocols can be viewed as analogues of the read-one write-all protocol in the context of logical structures. In this paper, we start by generalizing these protocols in two ways for logical structures. First, the quorum based approach is applied to develop protocols that use structured read and write quorums, thus attaining a high degree of data availability for both read and write operations. Next, the reconfiguration or views approach is developed for these structures resulting in protocols that attain high degrees of availability at significantly low communication cost for read operations. In this sense, the proposed protocols have the advantages of the read-one write-all protocol for low cost read operations as well as the majority quorum protocol for high data availability. Finally, we generalize the reconfiguration approach to allow for the dynamic reconfiguration of the database system from one replica management protocol to another. This allows database systems to adapt to an evolving and dynamic application environment.

#index 443075
#* Hybrid Knowledge Bases
#@ James J. Lu;Anil Nerode;V. s. Subrahmanian
#t 1996
#c 7
#% 3812
#% 33376
#% 35562
#% 53385
#% 64407
#% 67902
#% 68091
#% 85086
#% 99137
#% 101949
#% 102748
#% 107145
#% 111912
#% 115193
#% 116303
#% 123108
#% 124785
#% 144840
#% 152605
#% 152980
#% 157425
#% 158909
#% 171049
#% 277342
#% 497460
#% 546499
#% 676547
#! Deductive databases that interact with, and are accessed by, reasoning agents in the real world (such as logic controllers in automated manufacturing, weapons guidance systems, aircraft landing systems, land-vehicle maneuvering systems, and air-traffic control systems) must have the ability to deal with multiple modes of reasoning. Specifically, the types of reasoning we are concerned with include, among others, reasoning about time, reasoning about quantitative relationships that may be expressed in the form of differential equations or optimization problems, and reasoning about numeric modes of uncertainty about the domain which the database seeks to describe. Such databases may need to handle diverse forms of data structures, and frequently they may require use of the assumption-based nonmonotonic representation of knowledge.A hybrid knowledge base is a theoretical framework capturing all the above modes of reasoning. The theory tightly unifies the Constraint Logic Programming Scheme of Jaffar and Lassez [12], the Generalized Annotated Logic Programming Theory of Kifer and Subrahmanian [17], and the Stable Model semantics of Gelfond and Lifschitz [7]. New techniques are introduced which extend both the work on Annotated Logic Programming and the Stable Model semantics. (Proofs are omitted from the paper to ensure readability. Complete details of all results may be found in [24].)

#index 443076
#* Performance Analysis of Long-Lived Transaction Processing Systems with Rollbacks and Aborts
#@ Deron Liang;Satish K. Tripathi
#t 1996
#c 7
#% 1051
#% 1723
#% 2008
#% 9241
#% 32897
#% 118090
#% 288955
#% 319726
#% 466948
#% 480259
#% 597589
#% 835744
#! Increasing the parallelism in transaction processing and maintaining data consistency appear to be two conflicting goals in designing Distributed Database Systems (DDBS). This problem becomes especially difficult if the DDBS is serving long-lived transactions (LLTs). Recently, a special case of LLTs, called sagas, has been introduced that addresses this problem. The DDBS with sagas provides high parallelism to transactions by allowing sagas to release their locks as early as possible. However, it is also subject to overhead due to efforts needed to restore data consistency in case of failures. In this paper, we first conduct a series of simulation studies to compare the performance of LLT systems with saga implementation (or saga systems) and the LLT systems without saga implementation (or nonsaga systems) in a faulty environment. The simulation studies show that the saga systems outperform their nonsaga counterparts under most of conditions including the heavy failure cases. We thus propose an analytical queuing model to further investigate the performance behavior of the saga systems. The motivation of the development of this analytical model is twofold. It assists us to further study quantitatively the performance penalty of the saga implementation due to the failure recovery overhead. Furthermore, the analytical solution can be used by system administrators to fine tune the performance of the saga system. This analytical model captures the primary aspects of the saga system, namely, data locking, resource contention, and failure recovery. Due to the complicated nature of the analytical modeling, we solve the model approximately for various performance metrics using decomposition methods, and validate the accuracy of the analytical results via simulations.

#index 443077
#* Global Committability in Multidatabase Systems
#@ Ahmed K. Elmagarmid;Jin Jing;Won Kim;Omran Bukhres;Aidong Zhang
#t 1996
#c 7
#% 9241
#% 18767
#% 32897
#% 83125
#% 86940
#% 102753
#% 112319
#% 287220
#% 435104
#% 462802
#% 463101
#! In this paper we develop a formal basis for research into the reliability aspects of transaction processing in multidatabase systems. We define a new correctness notion called global committability(GC) for the correct unilateral commit and retry recovery of global transactions in an autonomous MDBS environment. This notion facilitates to ensure the isolation property of global transactions when the retry approach is applied. The formalization work illustrates that conventional serializability (SR) notion and recoverability (RC) notion are not sufficient to specify a correct execution (i.e., isolated execution and recovery) of global transactions when the unilateral commit and the retry recovery are used to ensure the atomicity of global transactions. This work is significant because the unilateral commit and the retry recovery is an attractive complementary means to the undo recovery (whose correct schedule is specified by the conventional RC notion) for advanced transaction applications with characteristics of site autonomy and long-lived execution.

#index 443078
#* A Framework for Product Data
#@ Alison McKay;M. Susan Bloor;Alan de Pennington
#t 1996
#c 7
#% 2020
#% 22948
#% 55617
#% 57996
#% 68197
#% 93865
#% 132682
#% 157103
#% 317978
#% 373678
#% 435764
#% 691657
#% 693183
#! Improved support for engineering using information technology involves the integration of existing, evolving and future product data, and software that processes that data. For this reason, there is increasing interest in the representation of product data in the computer to support computer-aided engineering applications. To avoid duplication and inconsistency, and to support the use of new implementation technology as it emerges, conceptual models of product data are required. Such models are independent of the software and hardware environments in which they are implemented. System architectures to support the integration of applications at implementation time are becoming an accepted part of engineering information systems. We contend that, to use these software support environments effectively, integrated product data is required. Further, it must be possible to extend the integrated product data in a controlled fashion if it is to evolve to support future engineering applications effectively. A framework that is a part of the product data, at the conceptual modeling (rather than the implementation) stage, is shown to help to satisfy these requirements. The framework presented here is a structure for the information content of product data rather than for the implementation of such data. Our research has shown that product data based on the framework can be successfully implemented in a number of different forms; for example, network, relational, and object-oriented databases. This paper describes a framework for electromechanical product data that has been implemented in a structure editor and is currently being used to support a range of engineering applications. We show that the process of product data integration can be improved by using existing integration strategies in conjunction with a framework that provides an overall organization for the data.

#index 443079
#* Alternative Correctness Criteria for Concurrent Execution of Transactions in Multilevel Secure Databases
#@ Vijayalakshmi Atluri;Sushil Jajodia;Elisa Bertino
#t 1996
#c 7
#% 9241
#% 45256
#% 51451
#% 122904
#% 319768
#% 320469
#% 340666
#% 374401
#% 437126
#% 462778
#% 691658
#! This paper investigates issues related to transaction concurrency control in multilevel secure databases. It demonstrates how the conflicts between the correctness requirements and the secrecy requirements can be reconciled by proposing two different solutions. This paper, first, explores the correctness criteria that are weaker than one-copy serializability. Each of these weaker criteria, though not as strict as one-copy serializability, is required to preserve database consistency in some meaningful way, and moreover, its implementation does not require the scheduler to be trusted. It proposes three different, increasingly stricter notions of serializability驴level-wise serializability, one-item read serializability and pair-wise serializability驴that can serve as substitutes for one-copy serializability. This paper, then, investigates secure concurrency control protocols that generate one-copy serializable histories and presents a multiversion timestamping protocol that has several very desirable properties: It is secure, produces multiversion histories that are equivalent to serial one-copy histories in which transactions are placed in a timestamp order, eliminates starvation, and can be implemented using single-level untrusted schedulers.

#index 443080
#* Storage Allocation Policies for Time-Dependent Multimedia Data
#@ Huang-Jen Chen;Thomas D. C. Little
#t 1996
#c 7
#% 6801
#% 38248
#% 57049
#% 83129
#% 91633
#% 107702
#% 111876
#% 113939
#% 114572
#% 151340
#% 151343
#% 159079
#% 452790
#% 452791
#% 460864
#% 480281
#% 519989
#% 520139
#% 661680
#! Multimedia computing requires support for heterogeneous data types with differing storage, communication, and delivery requirements. Continuous media data types such as audio and video impose delivery requirements that are not satisfied by conventional physical storage organizations. In this paper, we describe a physical organization for multimedia data based on the need to support the delivery of multiple playout sessions from a single rotating-disk storage device. Our model relates disk characteristics to the different media recording and playback rates and derives their storage pattern. This storage organization guarantees that as long as a multimedia delivery process is running, starvation will never occur. Furthermore, we derive bandwidth and buffer constraints for disk access and present an approach to minimize latencies for non-continuous media stored on the same device. The analysis and numerical results indicate the feasibility of using conventional rotating magnetic disk storage devices to support multiple sessions for on-demand video applications.

#index 443081
#* Editorial: Introducing the New Editor-in-Chief of IEEE Transactions on Knowledge and Data Engineering
#@ Benjamin W. Wah
#t 1996
#c 7

#index 443082
#* Data Mining: An Overview from a Database Perspective
#@ Ming-Syan Chen;Jiawei Han;Philip S. Yu
#t 1996
#c 7
#% 36672
#% 86950
#% 90661
#% 102772
#% 116203
#% 136350
#% 140617
#% 152934
#% 172386
#% 172949
#% 173771
#% 186340
#% 199515
#% 199537
#% 199538
#% 201893
#% 201894
#% 201928
#% 210160
#% 210164
#% 210173
#% 210182
#% 232102
#% 232106
#% 232108
#% 232111
#% 232113
#% 232117
#% 232119
#% 232126
#% 232146
#% 232147
#% 232170
#% 240188
#% 394246
#% 394333
#% 412588
#% 449588
#% 452747
#% 452821
#% 452822
#% 459008
#% 460862
#% 463257
#% 463742
#% 463903
#% 464196
#% 464204
#% 480940
#% 481281
#% 481290
#% 481588
#% 481604
#% 481608
#% 481609
#% 481754
#% 481758
#% 482702
#% 527021
#% 527022
#% 566123
#% 631914
#! Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information providing services, such as data warehousing and on-line services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided, and to increase the business opportunities. In response to such a demand, this article is to provide a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided, and a comparative study of such techniques is presented.

#index 443083
#* Finding Aggregate Proximity Relationships and Commonalities in Spatial Data Mining
#@ Edwin M. Knorr;Raymond T. Ng
#t 1996
#c 7
#% 2115
#% 26133
#% 32913
#% 68091
#% 84654
#% 86950
#% 103743
#% 152934
#% 166244
#% 406493
#% 427199
#% 443083
#% 463742
#% 480093
#% 480940
#% 480964
#% 481281
#! In this paper, we study two spatial knowledge discovery problems involving proximity relationships between clusters and features. The first problem is: Given a cluster of points, how can we efficiently find features (represented as polygons) that are closest to the majority of points in the cluster? We measure proximity in an aggregate sense due to the nonuniform distribution of points in a cluster (e.g., houses on a map), and the different shapes and sizes of features (e.g., natural or man-made geographic features). The second problem is: Given n clusters of points, how can we extract the aggregate proximity commonalities (i.e., features) that apply to most, if not all, of the n clusters? Regarding the first problem, the main contribution of the paper is the development of Algorithm CRH which uses geometric approximations (i.e., circles, rectangles, and convex hulls) to filter and select features. Highly scalable and incremental, Algorithm CRH can examine over 50,000 features and their spatial relationships with a given cluster in approximately one second of CPU time. Regarding the second problem, the key contribution is the development of Algorithm GenCom that makes use of concept generalization to effectively derive many meaningful commonalities that cannot be found otherwise.

#index 443084
#* A Metapattern-Based Automated Discovery Loop for Integrated Data Mining-Unsupervised Learning of Relational Patterns
#@ Wei-Min Shen;Bing Leng
#t 1996
#c 7
#% 55408
#% 90214
#% 124708
#% 131422
#% 170416
#% 170418
#% 232143
#% 232146
#% 232147
#% 363500
#% 408638
#% 449508
#% 451052
#% 1272165
#! Metapattern (also known as metaquery) is a new approach for integrated data mining systems. Different from a typical "tool-box" like integration, where components must be picked and chosen by users without much help, metapatterns provide a common representation for intercomponent communication as well as a human interface for hypothesis development and search control. One weakness of this approach, however, is that the task of generating fruitful metapatterns is still a heavy burden for human users. In this paper, we describe a metapattern generator and an integrated discovery loop that can automatically generate metapatterns. Experiments in both artificial and real-world databases have shown that this new system goes beyond the existing machine learning technologies, and can discover relational patterns without requiring humans to prelabel the data as positive or negative examples for some given target concepts. With this technology, future data mining systems could discover high-quality, human comprehensible knowledge in a much more efficient and focused manner, and data mining could be managed easily by both expert and less expert users.

#index 443085
#* Efficient Mining of Association Rules in Distributed Databases
#@ David W. Cheung;Vincent T. Ng;Ada W. Fu;Yongjian Fu
#t 1996
#c 7
#% 36683
#% 152934
#% 172386
#% 187192
#% 199538
#% 201894
#% 232102
#% 412588
#% 443091
#% 452747
#% 452821
#% 460862
#% 464204
#% 480940
#% 481281
#% 481290
#% 481588
#% 481754
#% 481758
#% 497479
#! Many sequential algorithms have been proposed for mining of association rules. However, very little work has been done in mining association rules in distributed databases. A direct application of sequential algorithms to distributed databases is not effective, because it requires a large amount of communication overhead. In this study, an efficient algorithm, DMA, is proposed. It generates a small number of candidate sets and requires only O(n) messages for support count exchange for each candidate set, where n is the number of sites in a distributed database. The algorithm has been implemented on an experimental test bed and its performance is studied. The results show that DMA has superior performance when comparing with the direct application of a popular sequential algorithm in distributed databases.

#index 443086
#* Visualization Techniques for Mining Large Databases: A Comparison
#@ Daniel A. Keim;Hans-Peter Kriegel
#t 1996
#c 7
#% 1211
#% 28144
#% 46803
#% 68659
#% 80330
#% 96296
#% 102726
#% 127860
#% 154334
#% 172811
#% 201483
#% 201894
#% 201975
#% 238794
#% 252608
#% 364386
#% 434411
#% 435966
#% 436116
#% 442712
#% 452747
#% 452819
#% 463257
#% 463742
#% 463880
#% 481281
#% 481290
#% 488932
#% 540244
#% 619523
#% 641075
#% 726016
#% 726017
#% 726032
#% 726069
#% 727085
#! Visual data mining techniques have proven to be of high value in exploratory data analysis, and they also have a high potential for mining large databases. In this article, we describe and evaluate a new visualization-based approach to mining large databases. The basic idea of our visual data mining techniques is to represent as many data items as possible on the screen at the same time by mapping each data value to a pixel of the screen and arranging the pixels adequately. The major goal of this article is to evaluate our visual data mining techniques and to compare them to other well-known visualization techniques for multidimensional data: the parallel coordinate and stick figure visualization techniques. For the evaluation of visual data mining techniques, in the first place the perception of properties of the data counts, and only in the second place the CPU time and the number of secondary storage accesses are important. In addition to testing the visualization techniques using real data, we developed a testing environment for database visualizations similar to the benchmark approach used for comparing the performance of database systems. The testing environment allows the generation of test data sets with predefined data characteristics which are important for comparing the perceptual abilities of visual data mining techniques.

#index 443087
#* Extraction and Applications of Statistical Relationships in Relational Databases
#@ Wen-Chi Hou
#t 1996
#c 7
#% 33922
#% 51647
#% 77650
#% 135874
#% 152934
#% 463114
#% 463119
#% 480940
#% 480964
#! In this paper, we discuss modeling and extraction of statistical relationships among attributes. Different methods are used for extraction of different types of relationships. A complete methodology for extraction is developed by integrating widely accepted statistical methods. Statistical relationships manifest embedded relationships in data and thus lend themselves naturally to estimating unknown attribute values and detecting unlikely values. We will carefully examine these applications and evaluate the usefulness of statistical relationships in these applications using a real-life database.

#index 443088
#* An Efficient Inductive Learning Method for Object-Oriented Database Using Attribute Entropy
#@ Yueh-Min Huang;Shian-Hua Lin
#t 1996
#c 7
#% 21992
#% 25443
#% 49241
#% 57951
#% 277335
#% 442703
#% 442765
#% 449529
#% 449568
#% 449588
#% 452747
#% 462171
#% 462638
#% 463077
#! The data-driven characteristic of Version Space works efficiently in memory even if the training set is enormous. However, the concept hierarchy of each attribute used to generalize/specialize the hypothesis of S/G-set is processed sequentially and instance-by-instance, which degrades its performance. As for ID3, the decision tree is generated from the order of attributes according to their entropies to reduce the number of attributes in some of the tree paths. Unlike Version Space, ID3 generates an extremely complex decision tree when the training set is enormous. Therefore, we propose a method, AGE, taking advantages of Version Space and ID3 to learn rules from object-oriented databases (OODB) with the least number of learning features according to the entropy. By simulations, we found the performance of our learning algorithm is better than both Version Space and ID3. Furthermore, AGE's time complexity and space complexity are both linear to the number of training instances.

#index 443089
#* Knowledge Discovery in Deductive Databases with Large Deduction Results: The First Step
#@ Chien-Le Goh;Masahiko Tsukamoto;Shojiro Nishio
#t 1996
#c 7
#% 11797
#% 13014
#% 452747
#% 480964
#% 481290
#! Deductive databases have the ability to deduce new facts from a set of facts using a set of rules. They are also useful in the integration of artificial intelligence and database. However, when recursive rules are involved, the amount of deduced facts can become too large to be practically stored, viewed or analyzed. This seriously hinders the usefulness of deductive databases. In order to overcome this problem, we propose four methods to discover characteristic rules from large amount of deduction results without actually having to store all the deduction results. This paper presents the first step in the application of knowledge discovery techniques to deductive databases with large deduction results.

#index 443090
#* Effective Data Mining Using Neural Networks
#@ Hongjun Lu;Rudy Setiono;Huan Liu
#t 1996
#c 7
#% 92542
#% 136350
#% 169767
#% 191910
#% 232218
#% 452821
#% 566123
#% 836002
#! Classification is one of the data mining problems receiving great attention recently in the database community. This paper presents an approach to discover symbolic classification rules using neural networks. Neural networks have not been thought suited for data mining because how the classifications were made is not explicitly stated as symbolic rules that are suitable for verification or interpretation by humans. With the proposed approach, concise symbolic rules with high accuracy can be extracted from a neural network. The network is first trained to achieve the required accuracy rate. Redundant connections of the network are then removed by a network pruning algorithm. The activation values of the hidden units in the network are analyzed, and classification rules are generated using the result of this analysis. The effectiveness of the proposed approach is clearly demonstrated by the experimental results on a set of standard data mining test problems.

#index 443091
#* Parallel Mining of Association Rules
#@ Rakesh Agrawal;John C. Shafer
#t 1996
#c 7
#% 152934
#% 199538
#% 201894
#% 443091
#% 463883
#% 481290
#% 481588
#% 481754
#% 481758
#! We consider the problem of mining association rules on a shared-nothing multiprocessor. We present three algorithms that explore a spectrum of trade-offs between computation, communication, memory usage, synchronization, and the use of problem-specific information. The best algorithm exhibits near perfect scaleup behavior, yet requires only minimal overhead compared to the current best serial algorithm.

#index 443092
#* What Makes Patterns Interesting in Knowledge Discovery Systems
#@ Avi Silberschatz;Alexander Tuzhilin
#t 1996
#c 7
#% 36683
#% 152934
#% 172386
#% 405391
#% 452822
#! One of the central problems in the field of knowledge discovery is the development of good measures of interestingness of discovered patterns. Such measures of interestingness are divided into objective measures驴those that depend only on the structure of a pattern and the underlying data used in the discovery process, and the subjective measures驴those that also depend on the class of users who examine the pattern. The focus of this paper is on studying subjective measures of interestingness. These measures are classified into actionable and unexpected, and the relationship between them is examined. The unexpected measure of interestingness is defined in terms of the belief system that the user has. Interestingness of a pattern is expressed in terms of how it affects the belief system. The paper also discusses how this unexpected measure of interestingness can be used in the discovery process.

#index 443093
#* Concurrency Control in B-Trees with Batch Updates
#@ Kerttu Pollari-Malmi;Eljas Soisalon-Soininen;Tatu Ylönen
#t 1996
#c 7
#% 5381
#% 6716
#% 23890
#% 36118
#% 83183
#% 86532
#% 116086
#% 116087
#% 286929
#% 291189
#% 317933
#% 318437
#% 321250
#% 403195
#% 458517
#% 458528
#% 462030
#! In many applications it is important to be able to insert (or delete) keys into a B-tree as a batch, instead of inserting them one by one. Moreover, it may be necessary to search the B-tree concurrently with the batch update. An example of this is a text database system supporting a newspaper house: All words (keys) of each article obtained from a news agency should be inserted at once, and, at the same time, searches for documents containing certain words should be allowed as efficiently as possible. We have designed two solutions to the problem of concurrent batch update. The first solution驴implemented in our commercial text database system驴optimizes the speed of the batch update, and the second solution optimizes the average search time during the batch update.

#index 443094
#* Performance Analysis of Dynamic Finite Versioning Schemes: Storage Cost vs. Obsolescence
#@ Arif Merchant;Kun-Lung Wu;Philip S. Yu;Ming-Syan Chen
#t 1996
#c 7
#% 117
#% 3951
#% 8194
#% 9241
#% 54582
#% 58379
#% 102804
#% 116055
#% 118657
#% 234905
#% 286377
#% 287230
#% 287366
#% 318392
#% 374261
#% 411741
#% 463253
#% 463261
#% 463429
#% 835744
#! Dynamic finite versioning (DFV) schemes are an effective approach to concurrent transaction and query processing, where a finite number of consistent, but maybe slightly out-of-date, logical snapshots of the database can be dynamically derived for query access. In DFV, the storage overhead for keeping additional versions of changed data to support the logical snapshots and the amount of obsolescence faced by queries are two major performance issues. In this paper, we analyze the performance of DFV, with emphasis on the trade-offs between the storage cost and obsolescence. We develop analytical models based on a renewal-process approximation to evaluate the performance of DFV using M驴 2 snapshots. Asymptotic closed-form results for high query arrival rates are given for the case of two snapshots. Simulation is used to validate the analytical models and to evaluate the trade-offs between various strategies for advancing snapshots when M 2. The results show that 1) the analytical models match closely with simulation; 2) both the storage cost and obsolescence are sensitive to the snapshot-advancing strategies, especially for M 2 snapshots; and 3) generally speaking, increasing the number of snapshots demonstrates a trade-off between storage overhead and query obsolescence. For cases with skewed access or low update rates, a moderate increase in the number of snapshots beyond two can substantially reduce the obsolescence, while the storage overhead may increase only slightly, or even decrease in some cases. Such a reduction in obsolescence is more significant as the coefficient of variation of the query length distribution becomes larger. Moreover, for very low update rates, a large number of snapshots can be used to reduce the obsolescence to almost zero without increasing the storage overhead.

#index 443095
#* Description and Identification of Distributed Fragments of Recursive Relations
#@ Sakti Pramanik;Sungwon Jung
#t 1996
#c 7
#% 13014
#% 32905
#% 47127
#% 58365
#% 77932
#% 77979
#% 83159
#% 86933
#% 88051
#% 91007
#% 139176
#% 340642
#% 408396
#% 411734
#% 452753
#% 462646
#% 462971
#% 463409
#% 463413
#% 463440
#% 480284
#! In a distributed environment, it is advantageous to fragment a relation and store the fragments at various sites. In this paper, based on the concept of lattice structures, we develop a framework to study the fragmentation problems of distributed recursive relations. Two of the fragmentation problems are how to describe and identify fragments. Description and identification methods previously suggested are more suitable in parallel environments than in distributed databases. We propose a method to describe and identify fragments based on lattice structures. Finding lattice descriptions of fragments is shown to be an NP-complete problem. We analyze the performance of lattice approach both theoretically and experimentally. This is done by creating a database of recursive relations. The empirical analysis shows that our proposed algorithms give near-optimal solutions.

#index 443096
#* Editorial: Acknowledging TKDE's Fine Past, and Looking to an Even Better Future
#@ Farokh B. Bastani
#t 1997
#c 7

#index 443097
#* Announcement: Acknowledgment of Reviewers for 1995 and 1996
#@  IEEE Transactions on Knowledge and Data Engineering Staff
#t 1997
#c 7

#index 443098
#* Rule-Based System Validation through Automatic Identification of Equivalence Classes
#@ Gary W. Rosenwald;Chen-Ching Liu
#t 1997
#c 7
#% 25470
#% 367346
#% 452761
#% 666681
#! The proposed validation method identifies equivalence classes for fundamental tasks in a rule-based system to help identify incorrect knowledge and rule inconsistencies. The proposed method validates the entire input domain of a pre-existing rule-based system by using symbolic execution to determine the conditions under which tasks may be instantiated.

#index 443099
#* Capture, Integration, and Analysis of Digital System Requirements with Conceptual Graphs
#@ Walling R. Cyre
#t 1997
#c 7
#% 399
#% 2298
#% 11702
#% 32201
#% 48285
#% 65173
#% 137717
#% 317871
#% 404691
#% 408675
#% 442243
#% 462719
#% 465181
#% 507100
#% 747355
#% 837637
#! Initial requirements for new digital systems and products that are generally expressed in a variety of notations including diagrams and natural language can be automatically translated to a common knowledge representation for integration, for consistency and completeness analysis, and for further automatic synthesis. In this paper, block diagrams, flowcharts, timing diagrams, and English as used in specifying digital systems requirements are considered as examples of source notations for system requirements. The knowledge representation selected for this work is a form of semantic networks called conceptual graphs. For each source notation, a basis set of semantic primitives in terms of conceptual graphs is given, together with an algorithm for automatically generating conceptual structures from the notation. The automatic generation of conceptual structures from English presumes a restricted sublanguage of English and feedback to the author for verification of the interpretation. Mechanisms for integrating the separate conceptual structures generated from individual requirements expressions using schemata are discussed, and methods are illustrated for consistency and completeness analysis.

#index 443100
#* Semantics, Consistency, and Query Processing of Empirical Deductive Databases
#@ Raymond T. Ng
#t 1997
#c 7
#% 3034
#% 7689
#% 28052
#% 33376
#% 44876
#% 47960
#% 66101
#% 67902
#% 101612
#% 102767
#% 123108
#% 124785
#% 128636
#% 130112
#% 130150
#% 144840
#% 183609
#% 412588
#% 480964
#! In recent years, there has been a growing interest in reasoning with uncertainty in logic programming and deductive databases. However, most frameworks proposed, thus far, are either nonprobabilistic in nature or based on subjective probabilities. In this paper, we address the problem of incorporating empirical probabilities驴that is, probabilities obtained from statistical findings驴in deductive databases. To this end, we develop a formal model-theoretic basis for such databases. We also present a sound and complete algorithm for checking the consistency of such databases. Moreover, we develop consistency-preserving ways to optimize the algorithm for practical usage. Finally, we show how query answering for empirical deductive databases can be carried out.

#index 443101
#* The Role of Polymorphic Reuse Mechanisms in Schema Evolution in an Object-Oriented Database
#@ Ling Liu;Roberto Zicari;Walter Hürsch;Karl J. Lieberherr
#t 1997
#c 7
#% 413
#% 3523
#% 23959
#% 30567
#% 32903
#% 57612
#% 77999
#% 86321
#% 86332
#% 125607
#% 126307
#% 140611
#% 145939
#% 154425
#% 157763
#% 181430
#% 361313
#% 452780
#% 458608
#% 463097
#% 463754
#% 480796
#% 481445
#% 518034
#% 555016
#! A seamless approach to the incremental design and reuse of object-oriented methods and query specifications is presented. We argue for avoiding or minimizing the effort required for manually reprogramming methods and queries due to schema modifications, and demonstrate how the role of polymorphic reuse mechanisms is exploited for enhancing the adaptiveness of database programs against schema evolution in an object-oriented database. The salient features of our approach are the use of propagation patterns and a mechanism for propagation pattern refinement. Propagation patterns are employed as an interesting specification formalism for modeling operational requirements. They encourage the reuse of operational specifications against the structural modification of an object-oriented schema. Propagation pattern refinement is suited for the specification of reusable operational modules. It promotes the reusability of propagation patterns toward the operational requirement changes. This approach has a formal basis and emphasizes structural derivation of specifications. The main innovations are in raising the level of abstraction for behavioral schema design, and for making possible the derivation of operational semantics from structural specifications. As a result, both the modularity and reusability of object-oriented schemas are increased.

#index 443102
#* Indexing for Multiversion Locking: Alternatives and Performance Evaluation
#@ Paul M. Bober;Michael J. Carey
#t 1997
#c 7
#% 10392
#% 58371
#% 58379
#% 116055
#% 286929
#% 287366
#% 318392
#% 320902
#% 411701
#% 411741
#% 442706
#% 462480
#% 463261
#% 479596
#% 480096
#% 480941
#% 531907
#% 602792
#% 690419
#% 690966
#! Multiversion two-phase locking (MV2PL) provides on-line serializable queries without introducing the long blocking delays that can occur with conventional two-phase locking (2PL). MV2PL requires indexing structures, however, that are capable of supporting multiple versions of data. In this paper, we present several options for extending single-version indexing schemes for use with MV2PL. These basic approaches are largely orthogonal to the underlying indexing structure (e.g., hashing or B+ trees). The options considered differ in where they place version selection information (i.e., references to individual versions); this information is placed either with the data or with the index entries of one or more of the indices. We also present the results from a performance study that show that placing the version selection information with the data is usually the best option, since it keeps the indices smaller and thus enables a larger fraction of the index pages to remain cached in the buffer pool.

#index 443103
#* An Extended Authorization Model for Relational Databases
#@ Elisa Bertino;Pierangela Samarati;Sushil Jajodia
#t 1997
#c 7
#% 664
#% 42881
#% 56647
#% 68141
#% 91075
#% 172275
#% 286901
#% 287493
#% 287647
#% 287670
#% 319549
#% 458598
#% 481124
#! We propose two extensions to the authorization model for relational databases defined originally by Griffiths and Wade. The first extension concerns a new type of revoke operation, called noncascading revoke operation. The original model contains a single, cascading revoke operation, meaning that when a privilege is revoked from a user, a recursive revocation takes place that deletes all authorizations granted by this user that do not have other supporting authorizations. The new type of revocation avoids the recursive revocation of authorizations. The second extension concerns negative authorization which permits specification of explicit denial for a user to access an object under a particular mode. We also address the management of views and groups with respect to the proposed extensions.

#index 443104
#* The Expressive Power of Temporal Relational Query Languages
#@ Abdullah Uz Tansel;Erkan Tin
#t 1997
#c 7
#% 18615
#% 18772
#% 36683
#% 43028
#% 49596
#% 55295
#% 58368
#% 83108
#% 85465
#% 93076
#% 111284
#% 157158
#% 199807
#% 286257
#% 361445
#% 442711
#% 443135
#% 452751
#% 461861
#% 461863
#! We consider the representation of temporal data based on tuple and attribute timestamping. We identify the requirements in modeling temporal data and elaborate on their implications in the expressive power of temporal query languages. We introduce a temporal relational data model where N1NF relations and attribute timestamping are used and one level of nesting is allowed. For this model, a nested relational tuple calculus (NTC) is defined. We follow a comparative approach in evaluating the expressive power of temporal query languages, using NTC as a metric and comparing it with the existing temporal query languages. We prove that NTC subsumes the expressive power of these query languages. We also demonstrate how various temporal relational models can be obtained from our temporal relations by NTC and give equivalent NTC expressions for their languages. Furthermore, we show the equivalence of intervals and temporal elements (sets) as timestamps in our model.

#index 443105
#* CCAM: A Connectivity-Clustered Access Method for Networks and Network Computations
#@ Shashi Shekhar;Duen-Ren Liu
#t 1997
#c 7
#% 47621
#% 58364
#% 64431
#% 68091
#% 86951
#% 102745
#% 113581
#% 139176
#% 152902
#% 169089
#% 285932
#% 408396
#% 411694
#% 415957
#% 427199
#% 442768
#% 442858
#% 462617
#% 462956
#% 463107
#% 463251
#% 463413
#% 463421
#% 463583
#% 480093
#% 481433
#% 481455
#% 605157
#! Current Spatial Database Management Systems (SDBMS) provide efficient access methods and operators for point and range queries over collections of spatial points, line segments, and polygons. However, it is not clear if existing spatial access methods can efficiently support network computations which traverse line-segments in a spatial network based on connectivity rather than geographic proximity. The expected I/O cost for many network operations can be reduced by maximizing the Weighted Connectivity Residue Ratio (WCRR), i.e., the chance that a pair of connected nodes that are more likely to be accessed together are allocated to a common page of the file. CCAM is an access method for general networks that uses connectivity clustering. CCAM supports the operations of insert, delete, create, and find as well as the new operations, get-A-successor and get-successors, which retrieve one or all successors of a node to facilitate aggregate computations on networks. The nodes of the network are assigned to disk pages via a graph partitioning approach to maximize the WCRR. CCAM includes methods for static clustering, as well as dynamic incremental reclustering, to maintain high WCRR in the face of updates, without incurring high overheads. We also describe possible modifications to improve the WCRR that can be achieved by existing spatial access methods. Experiments with network computations on the Minneapolis road map show that CCAM outperforms existing access methods, even though the proposed modifications also substantially improve the performance of existing spatial access methods.

#index 443106
#* Object-Based Semantic Real-Time Concurrency Control with Bounded Imprecision
#@ Lisa Cingiser DiPippo;Victor Fay Wolfe
#t 1997
#c 7
#% 459
#% 13426
#% 33003
#% 38791
#% 46607
#% 67458
#% 103739
#% 123096
#% 158051
#% 172217
#% 287220
#% 287231
#% 437029
#% 443004
#% 463253
#% 561901
#% 622828
#% 701987
#! This paper describes a concurrency control technique for real-time object-oriented databases that supports logical consistency and temporal consistency, as well as bounded imprecision that results from their trade-offs. The concurrency control technique uses a semantic locking mechanism within each object and user-defined conditional compatibility over the methods of the object. The semantics can specify when to sacrifice precise logical consistency to meet temporal consistency requirements. It can also specify accumulation and bounding of any resulting logical imprecision. We show that this technique, under certain general restrictions, can preserve global correctness and bound imprecision by proving it can guarantee a form of epsilon serializability specialized for object-oriented databases.

#index 443107
#* Optimal Bucket Allocation Design of k-ary MKH Files for Partial Match Retrieval
#@ C. y. Chen;H. f. Lin;C. c. Chang;R. c. t. Lee
#t 1997
#c 7
#% 1191
#% 1736
#% 4377
#% 30073
#% 43179
#% 45480
#% 76239
#% 112180
#% 120112
#% 135559
#% 144625
#% 222159
#% 286962
#% 287302
#% 287305
#% 289177
#% 321305
#% 411686
#% 479936
#! This paper first shows that the bucket allocation problem of an MKH (Multiple Key Hashing) file for partial match retrieval can be reduced to that of a smaller sized subfile, called the remainder of the file. And it is pointed out that the remainder type MKH file is the hardest MKH file for which to design an optimal allocation scheme. We then particularly concentrate on the allocation of an important remainder type MKH file; namely, the k-ary MKH file. We present various sufficient conditions on the number of available disks and the number of attributes for a k-ary MKH file to have a perfectly optimal allocation among the disks for partial match queries. Based upon these perfectly optimal allocations, we further present a heuristic method, called the CH (Cyclic Hashing) method, to produce near optimal allocations for the general k-ary MKH files. Finally, a comparison, by experiment, between the performances of the proposed method and an "ideal" perfectly optimal method, shows that the CH method is indeed satisfactorily good for the general k-ary MKH files.

#index 443108
#* Toward the Notion of a Knowledge Repository for Financial Risk Management
#@ Michel Benaroch
#t 1997
#c 7
#% 47148
#% 57958
#% 110011
#% 115331
#% 158762
#% 159120
#% 197909
#% 489195
#! An approach for designing a knowledge repository for risk management is presented. Since varied representations are used to capture the diverse types of knowledge involved in this domain, the atomic knowledge units stored in the repository are considered to be domain model (K) and inference method (M) pairs, (K, M) pairs, which address subtasks in the domain. Such (K, M) pairs are semantically uniform. Conceptually, this allows to view the repository as though it were a shared "database" of (K, M) pairs that has two key features. It serves stand-alone systems by enabling them to apply stored (K, M) pairs and share the generated results, where the applied pairs can be associated with any subtask that is part of entire application tasks. Additionally, it avoids capturing redundant subtask-specific Ks to the extent possible by dynamically deriving them from deep principled Ks it stores.

#index 443109
#* Learning to Predict: INC2.5
#@ Mirsad Hadzikadic;Benjamin F. Bohren
#t 1997
#c 7
#% 65440
#% 449569
#% 451050
#% 451051
#% 451052
#! This paper discusses INC2.5, an incremental concept formation system. The goal of INC2.5 is to form a hierarchy of concept descriptions based on previously seen instances which will be used to predict the classification of a new instance description. Each subtree of the hierarchy consists of instances which are similar to each other. The further from the root, the greater the similarity is between the instances within the same groupings. The ability to classify instances based on their description has many applications. For example, in the medical field doctors are required daily to diagnose patients; in other words, classify patients according to their symptoms. INC2.5 has been successfully applied to several domains, including breast cancer, general trauma, congressional voting records, and the monk's problems.

#index 443110
#* Alternating Hashing for Expansible Files
#@ Ye-In Chang;Chien-I Lee
#t 1997
#c 7
#% 512
#% 1387
#% 38226
#% 287020
#% 287317
#% 295947
#% 319055
#% 442791
#% 482041
#% 681091
#! In this paper, we propose a generalized approach for designing a class of dynamic hashing schemes which require no index and have the growth of a file at a rate of$${\textstyle{{n\,+\,1} \over n}}$$per full expansion, where n is the number of pages of the file, as compared to a rate of two in linear hashing. Based on this generalized approach, we derive a new dynamic hashing scheme called alternating hashing, in which, when a split occurs in page k, the data records in page k will be redistributed to page k and page (k + 1), or page k and page (k驴 1), according to whether the value of level d is even or odd, respectively. (Note that a level is defined as the number of full expansions happened so far.) From our performance analysis, given a fixed load control, the proposed scheme can achieve nearly 97% storage utilization as compared to 78% storage utilization by using linear hashing.

#index 443111
#* Ordering and Selecting Production Rules for Constraint Maintenance: Complexity and Heuristic Solution
#@ Piero Fraternali;Stefano Paraboschi
#t 1997
#c 7
#% 241
#% 83315
#% 102314
#% 137618
#% 167258
#% 205241
#% 408396
#% 459266
#! We present a technique for analyzing the run time behavior of integrity constraints repair actions, i.e., active database rules specifically designed to correct violations of database integrity. When constraints become violated due to an incorrect user transaction, rule computation is started to restore the database to a correct state. Since repair actions may be numerous and may conflict with each other, an automated support to the analysis of their run-time behavior is necessary. The proposed technique helps the rule base administrator define a repair rule selection strategy such that the computation terminates for every input transaction, the final database state satisfies all the constraints, and the user's preferences among different ways to restore integrity are taken into account. In addition, it can be used by the rule designer to spot "dangerous" rules that may be subject to redesign. This problem is formulated as an optimization problem on directed hypergraphs, which we demonstrate to be NP-hard and solve by means of a heuristic algorithm.

#index 443112
#* Database Management with Sequence Trees and Tokens
#@ Robert C. Goldstein;Christian Wagner
#t 1997
#c 7
#% 3901
#% 36684
#% 317933
#% 374562
#% 408466
#% 482049
#% 603136
#% 750960
#% 837662
#% 852964
#! An approach to organizing storage in database systems is presented that, under a wide range of conditions, saves both storage space and processing time. Text values in a database are replaced by short, fixed-length, rank-preserving numeric tokens. The actual values are stored in separate, nonredundant storage. Database operations that depend only on the relative magnitude of data values can be performed directly on the tokens. Tokenization is shown to improve database performance most in situations where there are a lot of ad hoc queries and a low volume of database insertions relative to other operations.

#index 443113
#* Sharing Metainformation to Guide Cooperative Search Among Heterogeneous Reusable Agents
#@ Susan E. Lander;Victor R. Lesser
#t 1997
#c 7
#% 52274
#% 68342
#% 68343
#% 68344
#% 109163
#% 110011
#% 160142
#% 172394
#% 200693
#% 214028
#% 317964
#% 676637
#! A reusable agent is a self-contained computational system that implements some specific expertise and that can be embedded into diverse applications requiring that expertise. Systems composed of heterogeneous reusable agents are potentially highly adaptable, maintainable, and affordable, assuming that integration issues such as information sharing, coordination, and conflict management can be effectively addressed. In this article, we investigate sharing metalevel search information to improve system performance, specifically with respect to how sharing affects the quality of solutions and the runtime efficiency of a reusable-agent system. We first give a formal description of shareable metainformation in systems where agents have private knowledge and databases and where agents are specifically intended to be reusable. We then present and analyze experimental results from a mechanical design system for steam condensers that demonstrate performance improvements related to information sharing and assimilation. Finally, we discuss the practical benefits and limitations of information sharing in application systems comprising heterogeneous reusable agents. Issues of pragmatic interest include determining what types of information can realistically be shared and determining when the costs of sharing outweigh the benefits.

#index 443114
#* Knowledge Conceptualization Tool
#@ Hiroko Fujihara;Dick B. Simmons;Newton C. Ellis;Robert E. Shannon
#t 1997
#c 7
#% 1952
#% 3596
#% 44876
#% 52005
#% 58212
#% 59299
#% 59302
#% 64612
#% 67565
#% 100976
#% 120110
#% 129985
#% 228105
#% 321635
#% 375017
#% 406493
#% 449587
#% 449588
#% 451052
#! Knowledge acquisition is one of the most important and problematic aspects of developing knowledge-based systems. Many automated tools have been introduced in the past, however, manual techniques are still heavily used. Interviewing is one of the most commonly used manual techniques for a knowledge acquisition process, and few automated support tools exist to help knowledge engineers enhance their performance. This paper presents a knowledge conceptualization tool (KCT) in which the knowledge engineer can effectively retrieve, structure, and formalize knowledge components, so that the resulting knowledge base is accurate and complete. The KCT uses information retrieval technique to facilitate conceptualization, which is one of the human intensive activities of knowledge acquisition. Two information retrieval techniques employing best-match strategies are used: vector space model and probabilistic ranking principle model. A prototype of the KCT was implemented to demonstrate the concept. The results from KCT are compared with the outputs from a manual knowledge acquisition process in terms of amount of information retrieved and the process time spent. An analysis of the results shows that the process time to retrieve knowledge components (e.g., facts, rules, protocols, and uncertainty) of KCT is about half that of the manual process, and the number of knowledge components retrieved from knowledge acquisition activities is four times more than that retrieved through a manual process. Furthermore, KCT captured every knowledge component that the knowledge engineer manually captured. KCT demonstrates the effectiveness of the knowledge acquisition process model proposed in this paper.

#index 443115
#* Using Directed Hypergraphs to Verify Rule-Based Expert Systems
#@ Mysore Ramaswamy;Sumit Sarkar;Ye-Sho Chen
#t 1997
#c 7
#% 2079
#% 20561
#% 27363
#% 37580
#% 60463
#% 60914
#% 76601
#% 83041
#% 126610
#% 140401
#% 162639
#% 364282
#% 377175
#% 452775
#! Rule-based representation techniques have become popular for storage and manipulation of domain knowledge in expert systems. It is important that systems using such a representation are verified for accuracy before implementation. In recent years, graphical techniques have been found to provide a good framework for the detection of errors that may appear in a rule base [1], [16], [17], [19], [23]. In this work we present a graphical representation scheme that: 1) captures complex dependencies across clauses in a rule base in a compact yet intuitively clear manner and 2) is easily automated to detect structural errors in a rigorous fashion. Our technique uses a directed hypergraph to accurately detect the different types of structural errors that appear in a rule base. The technique allows rules to be represented in a manner that clearly identifies complex dependencies across compound clauses. Subsequently, the verification procedure can detect errors in an accurate fashion by using simple operations on the adjacency matrix of the directed hypergraph. The technique is shown to have a computational complexity that is comparable to that of other graphical techniques. The graphical representation coupled with the associated matrix operations illustrate how directed hypergraphs are a very appropriate representation technique for the verification task.

#index 443116
#* A Form-Based Natural Language Front-End to a CIM Database
#@ Nabil R. Adam;Aryya Gangopadhyay
#t 1997
#c 7
#% 11960
#% 11963
#% 16799
#% 21141
#% 32891
#% 45495
#% 46186
#% 114515
#% 115412
#% 147084
#% 287464
#% 287860
#% 320176
#% 322955
#% 369625
#% 404691
#% 442677
#% 442757
#% 597793
#% 742115
#% 747609
#% 748317
#! This paper presents a methodology for developing a user interface that combines fourth generation interface tools (SQL forms) with a natural language processor for a database management system. The natural language processor consists of an index, a lexicon, and a parser. The index is used to uniquely identify each form in the system through a conceptual representation of its purpose. The form fields specify database or nondatabase fields whose values are either entered by the user (user-defined) or are derived by the form (system-defined) in response to user input. A set of grammar rules are associated with each form. The lexicon consists of all words recognized by the system, their grammatical categories, roots, their associations (if any) with database objects and forms. The parser scans a natural language query to identify a form in a bottom-up fashion. The information requested in the user query is determined in a top-down manner by parsing through the grammar rules associated with the identified form. Extragrammatical inputs with limited deviations from the grammar rules are supported. Combining a natural language processor with SQL forms allows processing data modification tasks without violating any database integrity constraint, having duplicate records, or entering invalid data. A prototype natural language interface is described as a front-end to an ORACLE database for a computer integrated manufacturing system.

#index 443117
#* Incremental Computation of Set Difference Views
#@ Lars Bækgaard;Leo Mark
#t 1997
#c 7
#% 1932
#% 3947
#% 13015
#% 13016
#% 18614
#% 32914
#% 36683
#% 78005
#% 83330
#% 86945
#% 86946
#% 98469
#% 102752
#% 116044
#% 136740
#% 152928
#% 189634
#% 201898
#% 201928
#% 201929
#% 201930
#% 217179
#% 287672
#% 318049
#% 321468
#% 322880
#% 341233
#% 442767
#% 442781
#% 442971
#% 462789
#% 463897
#% 464056
#% 479945
#% 480623
#! Views can be computed by means of recomputation or they can be computed incrementally. Incremental view computation outperforms view recomputation in many situations. This is, in particular, true in distributed database environments like data warehousing environments. We present and analyze two algorithms for the incremental computation of relational set difference views. Set differences occur naturally in the definition of the set-division operator and in rewritten nested queries. The I/O efficiency of the incremental algorithms is compared to each other and to efficient recomputation algorithms. Our cost analysis strongly indicates that incremental computation of set difference views outperforms recomputation of set difference views in many situations.

#index 443118
#* Divergence Control Algorithms for Epsilon Serializability
#@ Kun-Lung Wu;Philip S. Yu;Calton Pu
#t 1997
#c 7
#% 1326
#% 9241
#% 27057
#% 43206
#% 77005
#% 77982
#% 102804
#% 115569
#% 122914
#% 137753
#% 176491
#% 184219
#% 286836
#% 286967
#% 289207
#% 317987
#% 442786
#% 443004
#% 461847
#% 463253
#% 463575
#% 510873
#! This paper presents divergence control methods for epsilon serializability (ESR) in centralized databases. ESR alleviates the strictness of serializability (SR) in transaction processing by allowing for limited inconsistency. The bounded inconsistency is automatically maintained by divergence control (DC) methods in a way similar to SR is maintained by concurrency control (CC) mechanisms. However, DC for ESR allows more concurrency than CC for SR. We first demonstrate the feasibility of ESR by showing the design of three representative DC methods: two-phase locking, timestamp ordering and optimistic approaches. DC methods are designed by systematically enhancing CC algorithms in two stages: extension and relaxation. In the extension stage, a CC algorithm is analyzed to locate the places where it identifies non-SR conflicts of database operations. In the relaxation stage, the non-SR conflicts are relaxed to allow for controlled inconsistency. We then demonstrate the applicability of ESR by presenting the design of DC methods using other most known inconsistency specifications, such as absolute value, age and total number of nonserializably read data items. In addition, we present a performance study using an optimistic divergence control algorithm as an example to show that a substantial improvement in concurrency can be achieved in ESR by allowing for a small amount of inconsistency.

#index 443119
#* A Data/Knowledge Paradigm for the Modeling and Design of Operations Support Systems
#@ Vijay K. Vaishnavi;Gary C. Buchanan;William L. Kuechler Jr.
#t 1997
#c 7
#% 1797
#% 2078
#% 2079
#% 2080
#% 3246
#% 5201
#% 7425
#% 10019
#% 16498
#% 25117
#% 26722
#% 37972
#% 38696
#% 39428
#% 54046
#% 57962
#% 58361
#% 73228
#% 82786
#% 86349
#% 86944
#% 95982
#% 97623
#% 102788
#% 105949
#% 115188
#% 115728
#% 116192
#% 128937
#% 131844
#% 136731
#% 148916
#% 157392
#% 159256
#% 185244
#% 185246
#% 286368
#% 294629
#% 363096
#% 395537
#% 405608
#% 442705
#% 442930
#% 444758
#% 462633
#% 595480
#! This paper develops the Smart Object paradigm and its instantiation, which provide a new conceptualization for the modeling, design, and development of an important but little researched class of information systems, Operations Support Systems (OSS). OSS is our term for systems which provide interactive support for the management of large, complex operations environments, such as manufacturing plants, military operations, and large power generation facilities. The most salient feature of OSS' is their dynamic nature. The number and kind of elements composing the system as well as the mode of control of those elements change frequently in response to the environment. The abstraction of control and the ease with which complex dynamic control behavior can be modeled and simulated is one of the important aspects of the paradigm. The framework for the Smart Object paradigm is the fusion of object-oriented design models with declarative knowledge representation and active inferencing from AI models. Additional defining concepts from data/knowledge models, semantic data models, active databases, and frame based systems, are added to the synthesis as justified by their contribution to the ability to naturally model OSS at a high level of abstraction. The model assists in declaratively representing domain data/knowledge and its structure, and task or process knowledge, in addition to modeling multilevel control and interobject coordination.

#index 443120
#* A Semantic Framework of the Multilevel Secure Relational Model
#@ Xiaolei Qian;Teresa F. Lunt
#t 1997
#c 7
#% 36683
#% 51451
#% 69537
#% 78831
#% 102749
#% 131553
#% 141503
#% 151161
#% 176496
#% 317991
#% 374401
#% 480945
#% 664503
#! A multilevel relational database represents information in a multilevel state of the world, which is the knowledge of the truth value of a statement with respect to a level in a security lattice. We develop a semantic framework of the multilevel secure relational model with tuple-level labeling, which formalizes the notion of validity in multilevel relational databases. We also identify the multilevel security properties that precisely characterize the validity of multilevel relational databases, which can be maintained efficiently. Finally, we give an update semantics of the multilevel secure relational model that preserves both integrity and secrecy.

#index 443121
#* Text Compression for Dynamic Document Databases
#@ Alistair Moffat;Justin Zobel;Neil Sharman
#t 1997
#c 7
#% 1921
#% 3244
#% 9767
#% 38374
#% 57849
#% 68073
#% 143011
#% 162462
#% 179166
#% 193743
#% 197695
#% 213786
#% 394709
#! For compression of text databases, semi-static word-based methods provide good performance in terms of both speed and disk space, but two problems arise. First, the memory requirements for the compression model during decoding can be unacceptably high. Second, the need to handle document insertions means that the collection must be periodically recompressed if compression efficiency is to be maintained on dynamic collections. Here we show that with careful management the impact of both of these drawbacks can be kept small. Experiments with a word-based model and over 500 Mb of text show that excellent compression rates can be retained even in the presence of severe memory limitations on the decoder, and after significant expansion in the amount of stored text.

#index 443122
#* Block-Oriented Compression Techniques for Large Statistical Databases
#@ Wee Keong Ng;Chinya V. Ravishankar
#t 1997
#c 7
#% 2247
#% 2248
#% 68236
#% 193923
#% 287349
#% 320901
#% 377755
#% 408396
#% 463895
#% 480075
#% 503544
#% 852964
#% 852968
#! Disk I/O has long been a performance bottleneck for very large databases. Database compression can be used to reduce disk I/O bandwidth requirements for large data transfers. In this paper, we explore the compression of large statistical databases and propose techniques for organizing the compressed data such that standard database operations such as retrievals, inserts, deletes and modifications are supported. We examine the applicability and performance of three methods. Two of these are adaptations of existing methods, but the third, called Tuple Differential Coding (TDC) [16], is a new method that allows conventional access mechanisms to be used with the compressed data to provide efficient access. We demonstrate how the performance of queries that involve large data transfers can be improved with these database compression techniques.

#index 443123
#* Experience with Rule Induction and k-Nearest Neighbor Methods for Interface Agents that Learn
#@ Terry R. Payne;Peter Edwards;Claire L. Green
#t 1997
#c 7
#% 5182
#% 18566
#% 114519
#% 140588
#% 152778
#% 159114
#% 406493
#% 449566
#! Interface agents are being developed to assist users with a variety of tasks. To perform effectively, such agents need knowledge of user preferences. An agent architecture has been developed which observes a user performing tasks, and identifies features which can be used as training data by a learning algorithm. Using the learned profile, an agent can give advice to the user on dealing with new situations. The architecture has been applied to two different information filtering domains: classifying incoming mail messages (Magi) and identifying interesting USENet news articles (UNA). This paper describes the architecture and examines the results of experimentation with different learning algorithms and different feature extraction strategies within these domains.

#index 443124
#* Generalized Version Space Learning Algorithm for Noisy and Uncertain Data
#@ Tzung-Pei Hong;Shian-Shyong Tseng
#t 1997
#c 7
#% 2432
#% 3893
#% 42992
#% 42994
#% 44625
#% 90137
#% 90157
#% 131430
#% 136350
#% 178515
#% 449531
#% 449566
#% 459664
#% 688946
#% 835738
#! This paper generalizes the learning strategy of version space to manage noisy and uncertain training data. A new learning algorithm is proposed that consists of two main phases: searching and pruning. The searching phase generates and collects possible candidates into a large set; the pruning phase then prunes this set according to various criteria to find a maximally consistent version space. When the training instances cannot completely be classified, the proposed learning algorithm can make a trade-off between including positive training instances and excluding negative ones according to the requirements of different application domains. Furthermore, suitable pruning parameters are chosen according to a given time limit, so the algorithm can also make a trade-off between time complexity and accuracy. The proposed learning algorithm is then a flexible and efficient induction method that makes the version space learning strategy more practical.

#index 443125
#* Reusing Analogous Components
#@ Betty H. C. Cheng;Jun-Jang Jeng
#t 1997
#c 7
#% 1840
#% 90770
#% 91217
#% 115589
#% 115626
#% 120801
#% 238970
#% 445865
#% 445888
#% 460088
#! Using formal specifications to represent software components facilitates the determination of reusability because they more precisely characterize the functionality of the software, and the well-defined syntax makes processing amenable to automation. This paper presents an approach, based on formal methods, to the search, retrieval, and modification of reusable software components. From a two-tiered hierarchy of reusable software components, the existing components that are analogous to the query specification are retrieved from the hierarchy. The specification for an analogous retrieved component is compared to the query specification to determine what changes need to be applied to the corresponding program component in order to make it satisfy the query specification.

#index 443126
#* Dismountable Media Management in Tertiary Storage Systems
#@ Daniel A. Ford
#t 1997
#c 7
#! We describe an adaptive algorithm for tertiary storage media mount management. This predictive longest next reference (LNR) mount management algorithm attempts to approximate the optimal page replacement policy, OPT [1], [2].

#index 443127
#* Data on Air: Organization and Access
#@ T. Imielinski;S. Viswanathan;B. r. Badrinath
#t 1997
#c 7
#% 32884
#% 124011
#% 169835
#% 172874
#% 172876
#% 201897
#% 242695
#! Organizing massive amount of data on wireless communication networks in order to provide fast and low power access to users equipped with palmtops, is a new challenge to the data management and telecommunication communities. Solutions must take under consideration the physical restrictions of low network bandwidth and limited battery life of palmtops. This paper proposes algorithms for multiplexing clustering and nonclustering indexes along with data on wireless networks. The power consumption and the latency for obtaining the required data are considered as the two basic performance criteria for all algorithms. First, this paper describes two algorithms namely, (1, m) Indexing and Distributed Indexing, for multiplexing data and its clustering index. Second, an algorithm called Nonclustered Indexing is described for allocating static data and its corresponding nonclustered index. Then, the Nonclustered indexing algorithm is generalized to the case of multiple indexes. Finally, the proposed algorithms are analytically demonstrated to lead to significant improvement of battery life while retaining a low latency.

#index 443128
#* Analysis of the n-Dimensional Quadtree Decomposition for Arbitrary Hyperrectangles
#@ Christos Faloutsos;H. v. Jagadish;Yannis Manolopoulos
#t 1997
#c 7
#% 13041
#% 42091
#% 45766
#% 64431
#% 86951
#% 102759
#% 117775
#% 118213
#% 321455
#% 407995
#% 445701
#% 462956
#% 463597
#% 480610
#% 565447
#! We give a closed-form expression for the average number of n-dimensional quadtree nodes ("pieces" or "blocks") required by an n-dimensional hyperrectangle aligned with the axes. Our formula includes as special cases the formulae of previous efforts for two-dimensional spaces [8]. It also agrees with theoretical and empirical results that the number of blocks depends on the hypersurface of the hyperrectangle and not on its hypervolume. The practical use of the derived formula is that it allows the estimation of the space requirements of the n-dimensional quadtree decomposition. Quadtrees are used extensively in two-dimensional spaces (geographic information systems and spatial databases in general), as well in higher dimensionality spaces (as oct-trees for three-dimensional spaces, e.g., in graphics, robotics, and three-dimensional medical images [2]). Our formula permits the estimation of the space requirements for data hyperrectangles when stored in an index structure like a (n-dimensional) quadtree, as well as the estimation of the search time for query hyperrectangles, for the so-called linear quadtrees [17]. A theoretical contribution of the paper is the observation that the number of blocks is a piece-wise linear function of the sides of the hyperrectangle.

#index 443129
#* Optimal Design of Multiple Hash Tables for Concurrency Control
#@ Ming-Syan Chen;Philip S. Yu
#t 1997
#c 7
#% 102803
#% 135877
#% 137753
#% 287304
#% 443035
#% 480927
#! In this paper, we propose the approach of using multiple hash tables for lock requests with different data access patterns to minimize the number of false contentions in a data sharing environment. We first derive some theoretical results on using multiple hash tables. Then, in light of these derivations, a two-step procedure to design multiple hash tables is developed. In the first step, data items are partitioned into a given number of groups. Each group of data items is associated with the use of a hash table in such a way that lock requests to data items in the same group will be hashed into the same hash table. In the second step, given an aggregate hash table size, the hash table size for each individual data group is optimally determined so as to minimize the number of false contentions. Some design examples and remarks on the proposed method are given. It is observed from real database systems that different data sets usually have their distinct data access patterns, thus resulting in an environment where this approach can offer significant performance improvement.

#index 443130
#* An Efficient Multiversion Access Structure
#@ Peter J. Varman;Rakesh M. Verma
#t 1997
#c 7
#% 1749
#% 10392
#% 16028
#% 18615
#% 32915
#% 54711
#% 56081
#% 58371
#% 64150
#% 83105
#% 86953
#% 99440
#% 102759
#% 102809
#% 163440
#% 182672
#% 286256
#% 286257
#% 427199
#% 427206
#% 452782
#% 462480
#% 463749
#% 480096
#% 526866
#! An efficient multiversion access structure for a transaction-time database is presented. Our method requires optimal storage and query times for several important queries and logarithmic update times. Three version operations驴inserts, updates, and deletes驴are allowed on the current database, while queries are allowed on any version, present or past. The following query operations are performed in optimal query time: key range search, key history search, and time range view. The key-range query retrieves all records having keys in a specified key range at a specified time; the key history query retrieves all records with a given key in a specified time range; and the time range view query retrieves all records that were current during a specified time interval. Special cases of these queries include the key search query, which retrieves a particular version of a record, and the snapshot query which reconstructs the database at some past time. To the best of our knowledge no previous multiversion access structure simultaneously supports all these query and version operations within these time and space bounds. The bounds on query operations are worst case per operation, while those for storage space and version operations are (worst-case) amortized over a sequence of version operations. Simulation results show that good storage utilization and query performance is obtained.

#index 443131
#* Efficient Bulk-Loading of Gridfiles
#@ Scott T. Leutenegger;David M. Nicol
#t 1997
#c 7
#% 152948
#% 153215
#% 176911
#% 285932
#% 286835
#% 408504
#% 458615
#! This paper considers the problem of bulk-loading large data sets for the gridfile multiattribute indexing technique. We propose a rectilinear partitioning algorithm that heuristically seeks to minimize the size of the gridfile needed to ensure no bucket overflows. Empirical studies on both synthetic data sets and on data sets drawn from computational fluid dynamics applications demonstrate that our algorithm is very efficient, and is able to handle large data sets. In addition, we present an algorithm for bulk-loading data sets too large to fit in main memory. Utilizing a sort of the entire data set it creates a gridfile without incurring any overflows.

#index 443132
#* A Performance Comparison of Locking Methods with Limited Wait Depth
#@ Alexander Thomasian
#t 1997
#c 7
#% 1326
#% 1723
#% 2544
#% 9241
#% 27057
#% 28618
#% 35080
#% 37276
#% 40350
#% 69477
#% 76977
#% 101820
#% 114710
#% 117902
#% 118658
#% 152596
#% 157155
#% 194938
#% 212198
#% 287470
#% 318960
#% 403195
#% 404765
#% 442083
#% 442906
#% 442951
#% 444340
#% 462634
#% 674394
#% 835744
#! A number of recent studies have proposed lock conflict resolution methods to improve the performance of standard locking, i.e., strict two-phase locking with the general waiting method. This paper is primarily concerned with the performance of wait depth limited methods with respect to each other and some other methods. The methods considered include the general waiting, wound-wait, and no-waiting methods, symmetric and asymmetric versions of cautious waiting and running priority methods, the wait depth limited (WDL) method, and a modified version of it. In spite of the availability of analytic solutions for most of wait depth limited methods, for reasons given in the paper, the performance comparison is based on simulation results. The contributions of this study are as follows: 1) modeling assumptions, i.e., a careful definition of transaction restart options; 2) new results concerning the relative performance of wait depth limited methods, which show that a) the running priority method outperforms cautious waiting and may even outperform the WDL method in a system with limited hardware resource, b) WDL outperforms other methods in high lock contention, high capacity systems, and c) modified WDL has a performance comparable to WDL, but incurs less overhead in selecting the abort victim; and 3) contrary to common belief, Tay's Effective Database Size Paradigm for dealing with shared and exclusive locks and/or skewed database accesses in standard locking is applicable to some wait depth limited methods and provides acceptably accurate approximations in others驴as long as locking modes for restarted transactions are not resampled.

#index 443133
#* Similarity Searching in Medical Image Databases
#@ Euripides G. M. Petrakis;Christos Faloutsos
#t 1997
#c 7
#% 6806
#% 13041
#% 23998
#% 54003
#% 64431
#% 68781
#% 78243
#% 83962
#% 86950
#% 86951
#% 102772
#% 124441
#% 124680
#% 150477
#% 166094
#% 169940
#% 172908
#% 201876
#% 286237
#% 321455
#% 407995
#% 427199
#% 442826
#% 452795
#% 480093
#% 480950
#% 481455
#! We propose a method to handle approximate searching by image content in medical image databases. Image content is represented by attributed relational graphs holding features of objects and relationships between objects. The method relies on the assumption that a fixed number of "labeled" or "expected" objects (e.g., "heart," "lungs," etc.) are common in all images of a given application domain in addition to a variable number of "unexpected" or "unlabeled" objects (e.g., "tumor," "hematoma," etc.). The method can answer queries by example, such as "find all X-rays that are similar to Smith's X-ray." The stored images are mapped to points in a multidimensional space and are indexed using state-of-the-art database methods (R-trees). The proposed method has several desirable properties:Database search is approximate, so that all images up to a prespecified degree of similarity (tolerance) are retrieved.It has no "false dismissals" (i.e., all images qualifying query selection criteria are retrieved).It is much faster than sequential scanning for searching in the main memory and on the disk (i.e., by up to an order of magnitude), thus scaling-up well for large databases.

#index 443134
#* Default Logic as a Query Language
#@ Marco Cadoli;Thomas Eiter;Georg Gottlob
#t 1997
#c 7
#% 28120
#% 36683
#% 64407
#% 78634
#% 93766
#% 94456
#% 95643
#% 101647
#% 101922
#% 101956
#% 103704
#% 103705
#% 107149
#% 134571
#% 137875
#% 147546
#% 164405
#% 166793
#% 181220
#% 231883
#% 268771
#% 268779
#% 277320
#% 383293
#% 384978
#% 399235
#! Research in nonmonotonic reasoning has focused largely on the idea of representing knowledge about the world via rules that are generally true but can be defeated. Even if relational databases are nowadays the main tool for storing very large sets of data, the approach of using nonmonotonic AI formalisms as relational database query languages has been investigated to a much smaller extent. In this work, we propose a novel application of Reiter's default logic by introducing a default query language (DQL) for finite relational databases, which is based on default rules. The main result of this paper is that DQL is as expressive as SO驴驴, the existential-universal fragment of second-order logic. This result is not only of theoretical importance: We exhibit queries驴which are useful in practice驴that can be expressed with DQL and cannot with other query languages based on nonmonotonic logics such as DATALOG with negation under the stable model semantics. In particular, we show that DQL is well-suited for diagnostic reasoning.

#index 443135
#* Temporal Relational Data Model
#@ Abdullah Uz Tansel
#t 1997
#c 7
#% 4695
#% 10245
#% 13044
#% 18615
#% 18772
#% 27056
#% 32915
#% 36683
#% 43028
#% 43188
#% 43191
#% 49596
#% 55295
#% 55882
#% 58368
#% 83526
#% 111284
#% 114712
#% 157158
#% 286256
#% 286257
#% 287664
#% 291858
#% 322880
#% 361445
#% 416030
#% 442711
#% 443104
#% 452751
#% 461861
#% 461863
#% 462787
#% 463254
#% 480962
#% 487752
#% 568176
#% 690047
#! This paper incorporates a temporal dimension to nested relations. It combines research in temporal databases and nested relations for managing the temporal data in nontraditional database applications. A temporal data value is represented as a temporal atom; a temporal atom consists of two parts: a temporal set and a value. The temporal atom asserts that the value is valid over the time duration represented by its temporal set. The data model allows relations with arbitrary levels of nesting and can represent the histories of objects and their relationships. Temporal relational algebra and calculus languages are formulated and their equivalence is proved. Temporal relational algebra includes operations to manipulate temporal data and to restructure nested temporal relations. Additionally, we define operations to generate a power set of a relation, a set membership test, and a set inclusion test, which are all derived from the other operations of temporal relational algebra. To obtain a concise representation of temporal data (temporal reduction), collapsed versions of the set-theoretic operations are defined. Procedures to express collapsed operations by the regular operations of temporal relational algebra are included. The paper also develops procedures to completely flatten a nested temporal relation into an equivalent 1NF relation and back to its original form, thus providing a basis for the semantics of the collapsed operations by the traditional operations on 1NF relations.

#index 443136
#* SQL Extension for Interval Data
#@ Nikos A. Lorentzos;Yannis G. Mitsopoulos
#t 1997
#c 7
#% 4695
#% 13044
#% 18615
#% 25215
#% 27056
#% 32915
#% 43028
#% 85465
#% 140617
#% 153621
#% 157158
#% 163440
#% 192560
#% 322880
#% 361445
#% 442711
#% 442919
#% 461863
#% 564093
#! IXSQL, an extension to SQL, is proposed for the management of interval data. IXSQL is syntactically and semantically upwards consistent with SQL2. Its specification has been based both on theoretical results and actual user requirements for the management of temporal data, a special case of interval data. Design decisions and implementation issues are also discussed.

#index 443137
#* On Modeling Cost Functions for Object-Oriented Databases
#@ Elisa Bertino;Paola Foscoli
#t 1997
#c 7
#% 57952
#% 58354
#% 58356
#% 77649
#% 77996
#% 83537
#% 86954
#% 102770
#% 102784
#% 152917
#% 172902
#% 206597
#% 268799
#% 395684
#% 411554
#% 442665
#% 442703
#% 442721
#% 442808
#% 442941
#% 462171
#% 481276
#% 481417
#% 481449
#! In this paper, we present a set of parameters able to exactly model topologies of object references in object-oriented databases. These parameters are important since they are used to model query execution strategy costs for optimization. The model we present considers also the cases of multivalued attributes and null references. Moreover, a set of derived parameters are introduced and their mathematical derivations are shown. These derived parameters are important, since they allow selectivity of nested predicates to be estimated. Moreover, they are used in estimating storage, access, and update costs for a number of access structures specifically tailored to efficiently support object-oriented queries.

#index 443138
#* An Improved Algorithm for the Incremental Recomputation of Active Relational Expressions
#@ Timothy Griffin;Leonid Libkin;Howard Trickey
#t 1997
#c 7
#% 201929
#% 442767
#! In [1], Qian and Wiederhold presented an algorithm for the incremental recomputation of relational algebra expressions that was claimed to preserve a certain minimality condition. This condition guarantees that the incremental change sets do not contain any unnecessary tuples; so, redundant computations are not performed. We show that, in fact, their algorithm violates this condition. We present an improved algorithm that does preserve this notion of minimality.

#index 443139
#* Cell Suppression Methodology: The Importance of Suppressing Marginal Totals
#@ P. c. Chu
#t 1997
#c 7
#% 51982
#% 64152
#% 67453
#% 100610
#% 152588
#% 374401
#% 442730
#! Safeguarding confidential information is of paramount concern to government agencies in publishing statistical data. Given a set of sensitive cells, the problem is to identify a set of complementary cells to suppress so as to mask the values of the sensitive cells. All of the existing cell suppression methods fail to consider the relationships among cell values and the representation of these relationships in marginal totals. That marginal totals may contain potent information has not been appreciated. This paper employs the theory of nominal data analysis to demonstrate that the disclosure of marginal totals can be very risky. It recommends adding a front-end test to the existing methods. The goal is to identify a list of sensitive marginal totals that have to be suppressed. This increases the sophistication of cell suppression methodology by providing an extra layer of protection.

#index 443140
#* Information Flow Control in Object-Oriented Systems
#@ Pierangela Samarati;Elisa Bertino;Alessandro Ciampichetti;Sushil Jajodia
#t 1997
#c 7
#% 151509
#% 164560
#% 320629
#% 488014
#% 790869
#! In this paper, we describe a high assurance discretionary access control model for object-oriented systems. The model not only ensures protection against Trojan horses leaking information, but provides the flexibility of discretionary access control at the same time. The basic idea of our approach is to check all information flows among objects in the system in order to block possible illegal flows. An illegal flow arises when information is transmitted from one object to another object in violation of the security policy. The interaction modes among objects are taken into account in determining illegal flows. We consider three different interaction modes that are standard interaction modes found in the open distributed processing models. The paper presents formal definitions and proof of correctness of our flow control algorithm.

#index 443141
#* A Deductive Environment for Dealing with Objects and Nonmonotonic Reasoning
#@ Nicola Leone;Pasquale Rullo;Antonella Mecchia;Giuseppe Rossi
#t 1997
#c 7
#% 7679
#% 11797
#% 23902
#% 29253
#% 32976
#% 36683
#% 43211
#% 50078
#% 53385
#% 53388
#% 58354
#% 58356
#% 64442
#% 77167
#% 77168
#% 86937
#% 86941
#% 99135
#% 102789
#% 105243
#% 158909
#% 172020
#% 268765
#% 277335
#% 277342
#% 442702
#% 442817
#% 479938
#% 480952
#% 563606
#% 598376
#! The BQM驴an acronym that stands for Bottom-up Query machine, the role played by our system in the framework of the KIWIS system [2]驴system extends deductive database technology with knowledge structuring capabilities to provide an advanced environment for the development of data and knowledge-based applications. The system relies on a knowledge representation language that combines the declarativeness of logic programming with the notions of object, inheritance with exceptions, and message passing. Exceptions are supported by allowing rules with negated heads. The use of exceptions inside the inheritance mechanism makes the language inherently nonmonotonic. The paper contains a comprehensive description of both the language and the implementation principles of the BQM system. It begins by providing a model-theoretic semantics of the language based on the notion of least model. A fixpoint semantics, providing a constructive definition of the least model, is given as well. Then, a number of implementation techniques for efficient query evaluation are described. Such techniques significantly extend "traditional" deductive database query evaluation strategies to deal with monotonic reasoning. A description of the architecture of the current prototype of the BQM system is also given.

#index 443142
#* Implementation of Tabled Evaluation with Delaying in Prolog
#@ R. Ramesh;Weidong Chen
#t 1997
#c 7
#% 8850
#% 33376
#% 66097
#% 101623
#% 103705
#% 124131
#% 137872
#% 137876
#% 169273
#% 172951
#% 205234
#% 442702
#% 480952
#! Unlike SLD resolution as implemented in Prolog, tabled evaluation with delaying guarantees termination for function-free logic programs, avoids repeated computation of identical subqueries, and handles recursion through negation. It is often used in query processing and nonmonotonic reasoning where termination is required. This paper presents a new technique for incorporating tabled evaluation into existing Prolog systems. It requires neither time consuming modifications of a Prolog engine nor metainterpretation that can enormously slow down program execution. Instead, using a program transformation approach, the technique allows effective use of the advanced Prolog technology. The transformed program uses tabling primitives implemented externally in C that provide direct control over the search strategies. This brings efficiency as well as portability across Prolog systems. Experiences with a prototype implementation indicate that the approach results in a flexible and pragmatic method for query processing and nonmonotonic reasoning on top of Prolog. Performance measurements show that the method is efficient for practical applications.

#index 443143
#* An Empirical Study of Domain Knowledge and Its Benefits to Substructure Discovery
#@ Surnjani Djoko;Diane J. Cook;Lawrence B. Holder
#t 1997
#c 7
#% 61792
#% 68813
#% 107031
#% 196798
#% 369349
#% 449588
#% 451052
#! Discovering repetitive, interesting, and functional substructures in a structural database improves the ability to interpret and compress the data. However, scientists working with a database in their area of expertise often search for predetermined types of structures or for structures exhibiting characteristics specific to the domain. This paper presents a method for guiding the discovery process with domain-specific knowledge. In this paper, the SUBDUE discovery system is used to evaluate the benefits of using domain knowledge to guide the discovery process. Domain knowledge is incorporated into SUBDUE following a single general methodology to guide the discovery process. Results show that domain-specific knowledge improves the search for substructures that are useful to the domain and leads to greater compression of the data. To illustrate these benefits, examples and experiments from the computer programming, computer-aided design circuit, and artificially generated domains are presented.

#index 443144
#* Programming with Logical Queries, Bulk Updates, and Hypothetical Reasoning
#@ Weidong Chen
#t 1997
#c 7
#% 958
#% 47955
#% 53385
#% 53388
#% 53393
#% 58574
#% 77165
#% 83328
#% 90860
#% 95618
#% 101646
#% 102547
#% 103705
#% 120697
#% 137876
#% 146250
#% 154317
#% 182424
#% 190636
#% 277345
#% 289377
#% 374945
#% 404922
#% 458518
#% 480616
#% 481130
#% 490315
#! This paper presents a language of update programs that integrates logical queries, bulk updates and hypothetical reasoning in a seamless manner. There is no syntactic or semantic distinction between queries and updates. Update programs extend logic programs with negation in both syntax and semantics. They allow bulk updates in which an arbitrary update is applied simultaneously for all answers of an arbitrary query. Hypothetical reasoning is naturally supported by testing the success or failure of an update. We describe an alternating fixpoint semantics of update programs and show that it can express all nondeterministic database transformations.

#index 443145
#* A Transparent Schema-Evolution System Based on Object-Oriented View Technology
#@ Young-Gook Ra;Elke A. Rundensteiner
#t 1997
#c 7
#% 23960
#% 32903
#% 32970
#% 86321
#% 102780
#% 111361
#% 115009
#% 125607
#% 140611
#% 146201
#% 153301
#% 169812
#% 172327
#% 442820
#% 442856
#% 458608
#% 463896
#% 480251
#% 480773
#% 480958
#% 534878
#% 581786
#% 614575
#% 647199
#% 733510
#! When a database is shared by many users, updates to the database schema are almost always prohibited because there is a risk of making existing application programs obsolete when they run against the modified schema. This paper addresses the problem by integrating schema evolution with view facilities. When new requirements necessitate schema updates for a particular user, then the user specifies schema changes to his personal view, rather than to the shared-base schema. Our view schema-evolution approach then computes a new view schema that reflects the semantics of the desired schema change, and replaces the old view with the new one. We show that our system provides the means for schema change without affecting other views (and thus without affecting existing application programs). The persistent data is shared by different views of the schema, i.e., both old as well as newly developed applications can continue to interoperate. This paper describes a solution approach of realizing the evolution mechanism as a working system, which as its key feature requires the underlying object-oriented view system to support capacity-augmenting views. In this paper, we present algorithms that implement the complete set of typical schema-evolution operations as view definitions. Lastly, we describe the transparent schema-evolution system (TSE) that we have built on top of GemStone, including our solution for supporting capacity-augmenting view mechanisms.

#index 443146
#* Intelligent Critic System for Architectural Design
#@ Hon Wai Chun;Edmund Ming-Kit Lai
#t 1997
#c 7
#% 4176
#% 73518
#% 79437
#% 83859
#% 103909
#% 110790
#% 123871
#% 130155
#% 157392
#% 319244
#% 363118
#% 366657
#% 669539
#% 756675
#! This paper describes an intelligent computer-aided architectural design system (ICAAD) called ICADS. ICADS encapsulates different types of design knowledge into independent "critic" modules. Each "critic" module possesses expertise in evaluating an architect's work in different areas of architectural design and can offer expert advice when needed. This research focuses on the representation of spatial information encoded in architectural floor plans and the representation of expert design knowledge. Described in this paper is our research in designing and developing two particular "critic" modules. The first module, FPDX, checks a residential apartment floor plan, verifies that the plan meets a set of government regulations, and offers suggestions for floor plan changes if regulations are not met. The second module, IDX, analyzes room and furniture layout according to a set of interior design guidelines and offers ideas on how furniture should be moved if the placement does not follow good design principles.

#index 443147
#* Multiprocessor Document Allocation: A Genetic Algorithm Approach
#@ Ophir Frieder;Hava T. Siegelmann
#t 1997
#c 7
#% 36279
#% 86533
#% 86534
#% 194250
#% 224702
#% 340645
#% 369236
#% 408396
#% 533475
#! We formally define the Multiprocessor Document Allocation Problem (MDAP) and prove it to be computationally intractable (NP Complete). Once it is shown that MDAP is NP Complete, we describe a document allocation algorithm based on genetic algorithms. This algorithm assumes that the documents are clustered using any one of the many clustering techniques. We later show that our allocation algorithm probabilistically converges to a good solution. For a behavioral evaluation, we present sample experimental results.

#index 443148
#* Feature Selection via Discretization
#@ Huan Liu;Rudy Setiono
#t 1997
#c 7
#% 99397
#% 136350
#% 167633
#! Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant and/or redundant attributes. Chi2 is a simple and general algorithm that uses the 驴2 statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data. It achieves feature selection via discretization. It can handle mixed attributes, work with multiclass data, and remove irrelevant and redundant attributes.

#index 443149
#* Establishing the Relevancy of the Bookkeeping Libraries to the Functional Testing of Computer Implementations
#@ Stamatis Vassiliadis;George Triantafyllos;Walid Kobrosly
#t 1997
#c 7
#% 782
#% 4172
#% 5658
#% 44049
#% 57419
#% 188615
#% 442913
#% 603840
#! In this paper, we address issues related to the definition of "faults," "errors," and "failures" and their separability, and attribution to the different development processes of computing systems. In particular, we deal with historical databases, which presumably contain certain data (i.e., test failure data) and describe the methodology that can be used to analyze the database and obtain the pertinent information. The validation method may be of particular importance, especially when information from the database needs to be extrapolated for a purpose other than the one for which the database was developed. Our methodology was used to evaluate the historical data collected during the development of the IBM 4381 and 9370 family of computers, and to extrapolate the faults found during the function testing.

#index 443150
#* The NUMA with Clusters of Processors for Parallel Join
#@ Sakti Pramanik;Walid R. Tout
#t 1997
#c 7
#% 43203
#% 60049
#% 83133
#% 115661
#% 152924
#% 340668
#% 442700
#% 463121
#% 463427
#% 463428
#% 480761
#% 480966
#% 482070
#% 678980
#! Recently, a number of hybrid systems have been proposed to combine the advantages of shared nothing and shared everything concepts for computing relational join operations. Most of these proposed systems, however, presented a few analytical results and have produced limited or no implementations on actual multiprocessors. In this paper, we present a parallel join algorithm with load-balancing for a hybrid system that combines both shared-nothing and shared-everything architectures. We derive an analytical model for the join algorithm on this architecture and validate it using both hardware/software simulations and actual experimentations. We study the performance of the join on the hybrid system for a wide range of system parameter values. We conclude that the hybrid system outperforms both shared-nothing and shared-everything architectures.

#index 443151
#* On a Pattern-Oriented Model for Intrusion Detection
#@ Shiuh-Pyng Shieh;Virgil D. Gligor
#t 1997
#c 7
#% 18527
#% 18528
#% 183319
#% 668883
#! Operational security problems, which are often the result of access authorization misuse, can lead to intrusion in secure computer systems. We motivate the need for pattern-oriented intrusion detection, and present a model that tracks both data and privilege flows within secure systems to detect context-dependent intrusions caused by operational security problems. The model allows the uniform representation of various types of intrusion patterns, such as those caused by unintended use of foreign programs and input data, imprudent choice of default privileges, and use of weak protection mechanisms. As with all pattern-oriented models, this model cannot be used to detect new, unanticipated intrusion patterns that could be detected by statistical models. For this reason, we expect that this model will complement, not replace, statistical models for intrusion detection.

#index 443152
#* Authorization and Revocation in Object-Oriented Databases
#@ Ivo Majetic;Ernst L. Leiss
#t 1997
#c 7
#% 6037
#% 12364
#% 90679
#% 116185
#% 116218
#% 287493
#% 374405
#% 408140
#% 437055
#% 442862
#! Few studies of object-oriented databases deal with their security, a fundamental aspect of systems with complex data structures. Most authorization systems give users who own resources only some basic control over them; here, we provide users with more direct control over their resources by associating with each grant propagation numbers. Propagation numbers govern the grantability and exercisability of the privileges. Of particular interest in our study of authorization in an o-o environment is the combination of inheritance and granting of privileges. Diverse policies are discussed and implemented in a test-bed system.

#index 443153
#* GUEST EDITOR'S INTRODUCTION: Research Surveys on Building Systems with Knowledge and Data Engineering Techniques
#@ David L. Spooner
#t 1997
#c 7
#% 442911
#! First Page of the Article

#index 443154
#* Sequential Decision Models for Expert System Optimization
#@ Vijay S. Mookerjee;Michael V. Mannino
#t 1997
#c 7
#% 241
#% 1017
#% 4868
#% 21138
#% 25646
#% 25655
#% 34262
#% 36817
#% 42994
#% 54049
#% 77704
#% 92554
#% 101223
#% 103914
#% 130034
#% 130205
#% 141070
#% 288449
#% 318028
#% 320272
#% 320807
#% 321547
#% 322186
#% 322467
#% 322602
#% 443423
#% 444865
#% 449531
#% 449588
#% 454157
#% 573222
#% 573648
#% 840577
#% 1272369
#% 1650660
#% 1650768
#% 1784148
#! Sequential decision models are an important element of expert system optimization when the cost or time to collect inputs is significant and inputs are not known until the system operates. Many expert systems in business, engineering, and medicine have benefited from sequential decision technology. In this survey, we unify the disparate literature on sequential decision models to improve comprehensibility and accessibility. We separate formulation of sequential decision models from solution techniques. For model formulation, we classify sequential decision models by objective (cost minimization versus value maximization) knowledge source (rules, data, belief network, etc.), and optimized form (decision tree, path, input order). A wide variety of sequential decision models are discussed in this taxonomy. For solution techniques, we demonstrate how search methods and heuristics are influenced by economic objective, knowledge source, and optimized form. We discuss open research problems to stimulate additional research and development.

#index 443155
#* Elicitation of Knowledge from Multiple Experts Using Network Inference
#@ Robert Rush;William A. Wallace
#t 1997
#c 7
#% 5166
#% 34262
#% 55186
#% 87502
#% 130056
#% 198000
#% 208154
#% 442753
#% 444890
#% 452828
#! Eliciting knowledge from multiple experts usually entails the use of groups, and thus is subject to the problems inherent in group dynamics. We present a technique for multiple expert knowledge acquisition that does not rely upon the use of groups and can take advantage of technological advances in communications and computing, i.e., the Internet. The approach uses influence diagrams to represent the individual expert's understanding of the problem situation and develops a Multiple Expert Influence Diagram (MEID), a composite representation of the experts' knowledge. Following a review of present methods for multiple expert knowledge elicitation, we formally define the MEID, describe its manner of construction, and discuss its interpretation. We continue with a review of the issues to be faced in implementation of the technique, and give an illustrative example. Finally, we emphasize the need to provide users of decision aids with defensible measures of the quality of the rules produced by these aids. The MEID-approach is intended to serve as a first step in this direction.

#index 443156
#* Transaction Processing in Multilevel Secure Databases with Kernelized Architecture: Challenges and Solutions
#@ Vijayalakshmi Atluri;Sushil Jajodia;Elisa Bertino
#t 1997
#c 7
#% 4619
#% 9241
#% 51451
#% 69534
#% 99814
#% 151157
#% 159735
#% 214434
#% 241801
#% 317991
#% 319768
#% 320187
#% 320469
#% 374401
#% 403195
#% 443079
#% 443189
#% 444507
#% 488006
#% 488017
#% 488147
#% 488150
#% 488181
#% 507235
#% 527411
#% 664471
#! Multilevel security poses many challenging problems for transaction processing. The challenges are due to the conflicting requirements imposed by confidentiality, integrity, and availability驴the three components of security. We identify these requirements on transaction processing in Multilevel Secure (MLS) database management systems (DBMSs) and survey the efforts of a number of researchers to meet these requirements. While our emphasis is primarily on centralized systems based on kernelized architecture, we briefly overview the research in the distributed MLS DBMSs as well.

#index 443157
#* R-MINI: An Iterative Approach for Generating Minimal Rules from Examples
#@ Se June Hong
#t 1997
#c 7
#% 4086
#% 26125
#% 61792
#% 66937
#% 369349
#% 407292
#% 442814
#% 443158
#% 449566
#! Generating classification rules or decision trees from examples has been a subject of intense study in the pattern recognition community, the statistics community, and the machine-learning community of the artificial intelligence area. We pursue a point of view that minimality of rules is important, perhaps above all other considerations (biases) that come into play in generating rules. We present a new minimal rule-generation algorithm called R-MINI (Rule-MINI) that is an adaptation of a well-established heuristic-switching-function-minimization technique, MINI. The main mechanism that reduces the number of rules is repeated application of generalization and specialization operations to the rule set while maintaining completeness and consistency. R-MINI results on some benchmark cases are also presented.

#index 443158
#* Use of Contextual Information for Feature Ranking and Discretization
#@ Se June Hong
#t 1997
#c 7
#% 3873
#% 42994
#% 66937
#% 136350
#% 418022
#% 443157
#% 444931
#% 449566
#% 496608
#% 550897
#! Deriving classification rules or decision trees from examples is an important problem. When there are too many features, discarding weak features before the derivation process is highly desirable. When there are numeric features, they need to be discretized for the rule generation. We present a new approach to these problems. Traditional techniques make use of feature merits based on either the information theoretic, or the statistical correlation between each feature and the class. We instead assign merits to features by finding each feature's "obligation" to the class discrimination in the context of other features. The merits are then used to rank the features, select a feature subset, and discretize the numeric variables. Experience with benchmark example sets demonstrates that the new approach is a powerful alternative to the traditional methods. This paper concludes by posing some new technical issues that arise from this approach.

#index 443159
#* SHAPES: A Novel Approach for Learning Search Heuristics in Under-Constrained Optimization Problems
#@ Khanh P. V. Doan;Kit Po Wong
#t 1997
#c 7
#% 42002
#% 57896
#% 92135
#% 106661
#% 114994
#% 129972
#% 370528
#% 449586
#% 449587
#% 451031
#% 451046
#! Although much research in machine learning has been carried out on acquiring knowledge for problem-solving in many problem domains, little effort has been focused on learning search-control knowledge for solving optimization problems. This paper reports on the development of SHAPES, a system that learns heuristic search guidance for solving optimization problems in intractable, under-constrained domains based on the Explanation-Based Learning (EBL) framework. The system embodies two new and novel approaches to machine learning. First, it makes use of explanations of varying levels of approximation as a mean for verifying heuristic-based decisions, allowing heuristic estimates to be revised and corrected during problem-solving. The provision of such a revision mechanism is particularly important when working in intractable and under-constrained domains, where heuristics tend to be highly over-generalized, and hence at times will give rise to incorrect results. Second, it employs a new linear and quadratic programming-based weight-assignment algorithm formulated to direct search toward optimal solutions under best-first search. The algorithm offers a direct method for assigning rule strengths and, in so doing, avoids the need to address the credit-assignment problem faced by other iterative weight-adjustment methods.

#index 443160
#* Modified Fractal Signature (MFS): A New Approach to Document Analysis for Automatic Knowledge Acquisition
#@ Yuan Y. Tang;Hong Ma;Dihua Xi;Xiaogang Mao;Ching Y. Suen
#t 1997
#c 7
#% 12418
#% 38596
#% 442650
#% 442652
#% 442840
#! One of the key technologies related to knowledge and data engineering is the acquisition of knowledge and data in the development and utilization of information system and the strategies to capture new knowledge and data. Actually, millions of documents, including technical reports, government files, newspapers, books, magazines, letters, bank checks, etc., have to be processed every day, and knowledge has to be acquired from them. This paper presents a new approach to document analysis for automatic knowledge acquisition. The traditional approaches have two major disadvantages: (1) They are not effective for processing documents with high geometrical complexity. Specially, the top-down approach can process only the simple documents which have specific format or contain some a priori information. (2) The top-down approach needs to split large components into small ones iteratively, while the bottom-up approach needs to merge small components into large ones iteratively. They are time consuming. This new approach is based on modified fractal signature. It can overcome the above weaknesses.

#index 443161
#* Integrating Calendar Dates and Qualitative Temporal Constraints in the Treatment of Periodic Events
#@ Paolo Terenziani
#t 1997
#c 7
#% 399
#% 33376
#% 41884
#% 43191
#% 49596
#% 53385
#% 65912
#% 66122
#% 79502
#% 82720
#% 84513
#% 100613
#% 135873
#% 166497
#% 268796
#% 277326
#% 319244
#% 361445
#% 463738
#% 464550
#% 481255
#% 527793
#% 674973
#% 744412
#! The paper describes a framework for representing and reasoning with periodic events. In particular, it proposes a temporal formalism which deals with both 1) quantitative information concerning the frame of time (e.g., between 1990 and 1993) and the user-defined calendar-dates (e.g., on the first Mondays of April) in which periodic events are located and 2) the qualitative relations between periodic events (e.g., Sam visits the branch office X01 before going to his office). The meaning of the temporal specifications in our formalism is described in logical terms. The paper defines the basic operations of inversion, intersection and composition of temporal specifications. These operations are correct (with respect to the logical definition of the specifications) and do not lose information. Finally, the paper also describes a correct algorithm which takes advantage of these operations for performing temporal reasoning, and analyses its complexity. An application of the temporal framework to the scheduling in an office is shown in an example.

#index 443162
#* Transactions and Updates in Deductive Databases
#@ Danilo Montesi;Elisa Bertino;Maurizio Martelli
#t 1997
#c 7
#% 1797
#% 14720
#% 23898
#% 23901
#% 33376
#% 35562
#% 36179
#% 36683
#% 47955
#% 47974
#% 53393
#% 53400
#% 55408
#% 62024
#% 90860
#% 102789
#% 153261
#% 169697
#% 206597
#% 213979
#% 277344
#% 318050
#% 374945
#% 384978
#% 403195
#% 416007
#% 435130
#% 442661
#% 462177
#% 480616
#% 480952
#% 490306
#! In this paper, we develop a new approach that provides a smooth integration of extensional updates and declarative query languages for deductive databases. The approach is based on a declarative specification of updates in rule bodies. Updates are not executed as soon are evaluated. Instead, they are collected and then applied to the database when the query evaluation is completed. We call this approach nonimmediate update semantics. We provide a top-down and equivalent bottom-up semantics which reflect the corresponding computation models. We also package set of updates into transactions and we provide a formal semantics for transactions. Then, in order to handle complex transactions, we extend the transaction language with control constructors still preserving formal semantics and semantics equivalence.

#index 443163
#* Independently Updated Views
#@ Uday R. Kulkarni;Richard G. Ramirez
#t 1997
#c 7
#% 470
#% 13015
#% 13016
#% 32903
#% 32914
#% 40633
#% 71570
#% 102775
#% 114577
#% 116203
#% 157864
#% 287672
#% 287793
#% 320113
#% 341233
#% 411675
#% 411717
#% 442663
#% 442704
#% 462162
#% 462321
#% 463283
#% 479585
#% 480251
#% 480623
#% 565456
#! Independently updated views (IUVs) are updated versions of a relational view. Unlike traditional views, updating an IUV changes only that IUV and not the underlying database. Updates to an IUV are stored in a differential table (DT) and incorporated when the IUV is accessed. There is no restriction on the type of query used to define the IUV. Applications of IUVs include alternative scenarios for decision support, local updating of distributed snapshots, and CAD versioning. IUVs allow versions to be defined on dynamic databases. This may cause inconsistencies because both the IUV and the base tables on which the IUV is defined can be updated independently of one another. We introduce "overlapping updates" that cause such inconsistencies and present algorithms for their detection and resolution using timestamps. These algorithms do not impose additional overhead on the database and require only the modification of the DT. A performance analysis indicates that the cost of querying IUVs is reasonable for a wide range of queries, views, and access strategies. We also consider a materialized implementation of IUVs. Materialized IUVs do not require a DT, relying instead on additional system columns for operation codes and timestamps. Results suggest a mix of implementation strategies for different types of IUV applications, depending on the frequency and nature of queries on the IUVs, the IUV sizes, and how actively the base tables are updated.

#index 443164
#* Using a Hash-Based Method with Transaction Trimming for Mining Association Rules
#@ Jong Soo Park;Ming-Syan Chen;Philip S. Yu
#t 1997
#c 7
#% 152934
#% 172892
#% 252608
#% 322885
#% 449588
#% 460862
#% 463257
#% 463883
#% 463903
#% 480940
#% 480964
#% 481281
#% 481290
#% 481588
#! In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. Mining association rules means that, given a database of sales transactions, to discover all associations among items such that the presence of some items in a transaction will imply the presence of other items in the same transaction. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items that appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first, and then, identifying驴within this candidate set驴those itemsets that meet the large itemset requirement. Generally, this is done iteratively for each large k-itemset in increasing order of k, where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate sets in early iterations is usually the dominating factor for the overall data-mining performance. To address this issue, we develop an effective algorithm for the candidate set generation. It is a hash-based algorithm and is especially effective for the generation of candidate set for large 2-itemsets. Explicitly, the number of candidate 2-itemsets generated by the proposed algorithm is, in orders of magnitude, smaller than that by previous methods驴thus resolving the performance bottleneck. Note that the generation of smaller candidate sets enables us to effectively trim the transaction database size at a much earlier stage of the iterations, thereby reducing the computational cost for later iterations significantly. The advantage of the proposed algorithm also provides us the opportunity of reducing the amount of disk I/O required. Extensive simulation study is conducted to evaluate performance of the proposed algorithm.

#index 443165
#* Probabilistic Model and Optimal Reorganization of B+-Tree with Physical Clustering
#@ June S. Park;V. Sridhar
#t 1997
#c 7
#% 8826
#% 43030
#% 55451
#% 71607
#% 317933
#% 377455
#% 383830
#% 442669
#% 461846
#% 835744
#! We consider a variant of B+-tree called the B+-tree with physical clustering (BC), which is exemplified by VSAM key-sequenced datasets. In this file, logically contiguous data buckets are physically clustered in a storage unit called a region in order to enhance the efficiency of sequential processing. As a consequence, records in a bucket are redistributed over time according to two different split algorithms: bucket split and region split. A stochastic model of the disorganization of BC files is constructed using order statistics. Database administrators of on-line transaction systems often need to reorganize BC files before a region split occurs. An efficient algorithm for determining the optimal reorganization time of BC files is developed. Analytic results are validated by simulation. In passing, we discuss guidelines for load-time configuration of BC files.

#index 443166
#* Boolean Similarity Measures for Resource Discovery
#@ Shih-Hao Li;Peter B. Danzig
#t 1997
#c 7
#% 4489
#% 36360
#% 86380
#% 124010
#% 144223
#% 158911
#% 167200
#% 169816
#% 375017
#% 437133
#% 448743
#% 672582
#! As the number of Internet servers increases rapidly, it becomes difficult to determine the relevant servers when searching for information. We develop a new method to rank Internet servers for Boolean queries. Our method reduces time and space complexity from exponential to polynomial in the number of Boolean terms. We contrast it with other known methods and describe its implementation.

#index 443167
#* Editor-In-Chief's Editorial: Seven Join TKDE Editorial Board
#@ Farokh B. Bastani
#t 1997
#c 7

#index 443168
#* ANNOUNCEMENT: Acknowledgment of 1997 TKDE Reviewers
#@  IEEE Transactions on Knowledge and Data Engineering Staff
#t 1997
#c 7

#index 443169
#* 1997 Index: IEEE Transactions on Knowledge and Data Engineering, Vol. 9
#@  IEEE Transactions on Knowledge and Data Engineering Staff
#t 1997
#c 7

#index 443170
#* QUEM: An Achievement Test for Knowledge-Based Systems
#@ Caroline C. Hayes;Michael I. Parzen
#t 1997
#c 7
#% 114353
#% 125044
#% 125068
#% 125076
#% 142755
#% 153479
#% 153486
#% 441027
#! This paper describes the QUality and Experience Metric (QUEM), a method for estimating the skill level of a knowledge-based system based on the quality of the solutions it produces. It allows one to assess how many years of experience the system would be judged to have if it were a human by providing a quantitative measure of the system's overall competence. QUEM can be viewed as a type of achievement or job-placement test administered to knowledge-based systems to help system designers determine how the system should be used and by what level of user. To apply QUEM, a set of subjects, experienced judges, and problems must be identified. The subjects should have a broad range of experience levels. Subjects and the knowledge-based system are asked to solve the problems; and judges are asked to rank order all solutions驴from worst quality to best. The data from the subjects is used to construct a skill-function relating experience to solution quality, and confidence bands showing the variability in performance. The system's quality ranking is then plugged into the skill function to produce an estimate of the system's experience level. QUEM can be used to gauge the experience level of an individual system, to compare two systems, or to compare a system to its intended users. This represents an important advance in providing quantitative measures of overall performance that can be applied to a broad range of systems.

#index 443171
#* Generalized Production Rules as a Basis for Integrating Active and Deductive Databases
#@ Luigi Palopoli;Riccardo Torlone
#t 1997
#c 7
#% 23952
#% 33376
#% 36683
#% 43209
#% 55408
#% 58361
#% 62021
#% 73005
#% 77679
#% 83315
#% 84985
#% 86941
#% 86942
#% 86944
#% 86946
#% 101646
#% 116045
#% 140056
#% 170063
#% 170898
#% 404772
#% 442824
#% 452763
#% 462177
#% 463568
#% 480942
#% 566369
#! We address the problem of providing a homogeneous framework for integrating, in a database environment, active rules, which allow the specification of actions to be executed whenever certain events take place, and deductive rules, which allow the specification of deductions in a logic programming style. Actually, it is widely recognized that both kinds of rules enhance the capabilities of database systems since they provide very natural mechanisms for the management of various important activities (e.g., knowledge representation, complex data manipulation, integrity constraint enforcement, view maintenance). However, in spite of their strong relationship, little work has been done on the unification of these powerful paradigms. In this paper, we present a rule-based language with an event-driven semantics that allows programmers to express both active and deductive computations. The language is based on a new notion of production rules whose effect is both a change of state and an answer to a query. By using several examples, we show that this simple language schema allows us to uniformly define different computations on data, including complex data manipulations, deductive evaluations, and active rule processing. We define the semantics of the language and then describe the architecture of a preliminary implementation of the language. Finally, we report about application and experience of use of the language.

#index 443172
#* High-Order Pattern Discovery from Discrete-Valued Data
#@ Andrew K. C. Wong;Yang Wang
#t 1997
#c 7
#% 9898
#% 10181
#% 27759
#% 35065
#% 44876
#% 57793
#% 84511
#% 86535
#% 374912
#% 442814
#% 443880
#% 449566
#% 449588
#% 451052
#% 480940
#% 651280
#% 695909
#! To uncover qualitative and quantitative patterns in a data set is a challenging task for research in the area of machine learning and data analysis. Due to the complexity of real-world data, high-order (polythetic) patterns or event associations, in addition to first-order class-dependent relationships, have to be acquired. Once the patterns of different orders are found, they should be represented in a form appropriate for further analysis and interpretation. In this paper, we propose a novel method to discover qualitative and quantitative patterns (or event associations) inherent in a data set. It uses the adjusted residual analysis in statistics to test the significance of the occurrence of a pattern candidate against its expectation. To avoid exhaustive search of all possible combinations of primary events, techniques of eliminating the impossible pattern candidates are developed. The detected patterns of different orders are then represented in an attributed hypergraph which is lucid for pattern interpretation and analysis. Test results on artificial and real-world data are discussed toward the end of the paper.

#index 443173
#* Implication and Referential Constraints: A New Formal Reasoning
#@ Xubo Zhang;Z. Meral Ozsoyoglu
#t 1997
#c 7
#% 2602
#% 32891
#% 36181
#% 36683
#% 69272
#% 77960
#% 122398
#% 123118
#% 164969
#% 198473
#% 268764
#% 268788
#% 287336
#% 289266
#% 368248
#% 384112
#% 408396
#% 415941
#% 415980
#% 415981
#% 462033
#% 462336
#% 462799
#% 463100
#% 464381
#% 489072
#% 563759
#% 599549
#! In this paper, we address the issue of reasoning with two classes of commonly used semantic integrity constraints in database and knowledge-base systems: implication constraints and referential constraints. We first consider a central problem in this respect, the IRC-refuting problem, which is to decide whether a conjunctive query always produces an empty relation on (finite) database instances satisfying a given set of implication and referential constraints. Since the general problem is undecidable, we only consider acyclic referential constraints. Under this assumption, we prove that the IRC-refuting problem is decidable, and give a novel necessary and sufficient condition for it. Under the same assumption, we also study several other problems encountered in semantic query optimization, such as the semantics-based query containment problem, redundant join problem, and redundant selection-condition problem, and show that they are polynomially equivalent or reducible to the IRC-refuting problem. Moreover, we give results on reducing the complexity for some special cases of the IRC-refuting problem.

#index 443174
#* Case-Based Reasoning Systems: From Automation to Decision-Aiding and Stimulation
#@ Soumitra Dutta;Berend Wierenga;Arco Dalebout
#t 1997
#c 7
#% 2148
#% 32570
#% 45606
#% 48699
#% 55397
#% 55921
#% 90116
#% 94899
#% 97310
#% 98223
#% 103909
#% 103914
#% 114546
#% 120806
#% 144213
#% 168280
#% 230027
#% 248881
#% 364830
#% 366694
#% 377200
#% 404691
#% 405727
#% 408020
#% 444841
#% 461691
#% 696239
#% 772391
#! Over the past decade, case-based reasoning (CBR) has emerged as a major research area within the artificial intelligence research field due to both its widespread usage by humans and its appeal as a methodology for building intelligent systems. Conventional CBR systems have been largely designed as automated problem-solvers for producing a solution to a given problem by adapting the solution to a similar, previously solved problem. Such systems have had limited success in real-world applications. More recently, there has been a search for new paradigms and directions for increasing the utility of CBR systems for decision support. This paper focuses on the synergism between the research areas of CBR and decision support systems (DSSs). A conceptual framework for DSSs is presented and used to develop a taxonomy of three different types of CBR systems: 1) conventional, 2) decision-aiding, and 3) stimulative. The major characteristics of each type of CBR system are explained with a particular focus on decision-aiding and stimulative CBR systems. The research implications of the evolution in the design of CBR systems from automation toward decision-aiding and stimulation are also explored.

#index 443175
#* Strategic Reasoning Under Trade-Offs between Action Costs and Advantages
#@ Silvano Mussi
#t 1997
#c 7
#% 1715
#% 25112
#% 48701
#% 61225
#% 74819
#% 76462
#% 90681
#% 99418
#% 164554
#% 179919
#% 446007
#% 452773
#! This paper proposes a model for making the most opportune choice among a number of alternatives in situations involving trade-offs and uncertainty. Alternative actions are evaluated on the basis of strategic attributes, such as cost and effectiveness, and the choice is dynamically determined. The issue is addressed by proposing a set of rules (trade-off rules) representing strategic knowledge specifically oriented to solve trade-off problems. Trade-off rules are encoded in conditional sentences such as: "if P then Ci is preferred to Cj," where P specifies a condition that must be satisfied by the strategic attributes associated with two candidate alternatives, and Ci and Cj are general selection criteria such as "Select alternative with the minimum (or maximum) value of a specific attribute ATTR." In the first part of the paper, a deep analysis of various trade-off problems and various interdependencies among choices is presented. The second part proposes a general paradigm for strategic reasoning under trade-offs. The paradigm is evaluated using a diagnostic application in the medical domain. Finally, the proposal is discussed in the context of related work.

#index 443176
#* Query Optimization in Multidatabase Systems Considering Schema Conflicts
#@ Chiang Lee;Chia-Jung Chen
#t 1997
#c 7
#% 6797
#% 83933
#% 85086
#% 85089
#% 102748
#% 111912
#% 111913
#% 126332
#% 126335
#% 140389
#% 169052
#% 191164
#% 201926
#% 286417
#% 415954
#% 427195
#% 435111
#% 442654
#% 442692
#% 442700
#% 443002
#% 443006
#% 462001
#% 462352
#% 462510
#% 463257
#% 463445
#% 463736
#% 464013
#% 464035
#% 480788
#% 511018
#! In a multidatabase system, the participating databases are autonomous. The schemas of these databases may be different in various ways, while the same information is represented. A global query issued against the global database needs to be translated to a proper form before it can be executed in a local database. Since data requested by a query (or a part of a query) is sometimes available in multiple sites, the site (database) that processes the query with the least cost is the desired query processing site. In this paper, we study the effect of differences in schemas on the cost of query processing in a multidatabase environment. We first classify schema conflicts to different types. For each type of conflict, we show how much more or less complex a translated query can become in comparison with the originally user-issued global query. Based on this observation, we propose an analytical method that considers the conflicts between local databases and finds the database(s) that renders the least execution cost in processing a global query. This research introduces a new level of query optimization (termed the schema-level optimization) in multidatabase environments. Our results provide a new dimension of enhancement for the capability of query optimizer in multidatabase systems.

#index 443177
#* Schema Evolution of an Object-Oriented Real-Time Database System for Manufacturing Automation
#@ Lei Zhou;Elke A. Rundensteiner;Kang G. Shin
#t 1997
#c 7
#% 27238
#% 32903
#% 32970
#% 37969
#% 37972
#% 62012
#% 72378
#% 91211
#% 103298
#% 110148
#% 111345
#% 111361
#% 124813
#% 126700
#% 146201
#% 158051
#% 167278
#% 170601
#% 173745
#% 362753
#% 404772
#% 437028
#% 437044
#% 442704
#% 442967
#% 442969
#% 443145
#% 463896
#% 463907
#% 511025
#! The database schemata often experience considerable changes during the development and initial use phases of database systems for advanced applications such as manufacturing automation and computer-aided design. An automated schema-evolution system can significantly reduce the amount of effort and potential errors related to schema changes. Although schema evolution for nonreal-time databases was the subject of previous research, its impact on real-time database systems remains unexplored. These advanced applications typically utilize object-oriented data models to handle complex data types. However, there exists no agreed-upon real-time object-oriented data model that can be used as a foundation to define a schema-evolution framework. Therefore, we first design a conceptual real-time object-oriented data model, called Real-time Object Model with Performance Polymorphism (ROMPP). It captures the key characteristics of real-time applications驴namely, timing constraints and performance polymorphism驴by utilizing specialization-dimension and letter-class hierarchy constructs, respectively. We then re-evaluate previous (nonreal-time) schema-evolution support in the context of real-time databases. This results in modifications to the semantics of schema changes and to the needs of schema-change resolution rules and schema invariants. Furthermore, we expand the schema-change framework with new constructs驴including new schema-change operators, new resolution rules, and new invariants驴necessary for handling the real-time characteristics of ROMPP. We adopt and extend an axiomatic model to express the semantics of ROMPP schema changes. Using manufacturing-control applications, we demonstrate the applicability of ROMPP and the potential benefits of the proposed schema-evolution system.

#index 443178
#* Content-Based Indexing of Multimedia Databases
#@ Jian-Kang Wu
#t 1997
#c 7
#% 43774
#% 61477
#% 83962
#% 151718
#% 166863
#% 183355
#% 211529
#% 363680
#% 434658
#% 445709
#% 452795
#! Content-based retrieval of multimedia database calls for content-based indexing techniques. Different from conventional databases, where data items are represented by a set of attributes of elementary data types, multimedia objects in multimedia databases are represented by a collection of features; similarity of object contents depends on context and frame of reference; and features of objects are characterized by multimodal feature measures. These lead to great challenges for content-based indexing. On the other hand, there are special requirements on content-based indexing: To support visual browsing, similarity retrieval, and fuzzy retrieval, nodes of the index should represent certain meaningful categories. That is to say that certain semantics must be added when performing indexing. ContIndex, the context-based indexing technique presented in this paper, is proposed to meet these challenges and special requirements. The indexing tree is formally defined by adapting a classification-tree concept. Horizontal links among nodes in the same level enhance the flexibility of the index. A special neural-network model, called Learning based on Experiences and Perspectives (LEP), has been developed to create node categories by fusing multimodal feature measures. It brings into the index the capability of self-organizing nodes with respect to certain context and frames of reference. An icon image is generated for each intermediate node to facilitate visual browsing. Algorithms have been developed to support multimedia object archival and retrieval using ContIndex. ContIndex has been successfully applied to two applications: A facial image retrieval system, CAFIIR, and a trademark archival and registration system, STAR.

#index 443179
#* A Method of Learning Implication Networks from Empirical Data: Algorithm and Monte-Carlo Simulation-Based Validation
#@ Jiming Liu;Michel C. Desmarais
#t 1997
#c 7
#% 20853
#% 44876
#% 109042
#% 128623
#% 129987
#% 130114
#% 152129
#% 154453
#% 458279
#% 527664
#% 696946
#% 835738
#! This paper describes an algorithmic means for inducing implication networks from empirical data samples. The induced network enables efficient inferences about the values of network nodes if certain observations are made. This implication induction method is approximate in nature as probablistic network requirements are relaxed in the construction of dependence relationships based on statistical testing. In order to examine the effectiveness and validity of the induction method, several Monte-Carlo simulations were conducted, where theoretical Bayesian networks were used to generate empirical data samples驴some of which were used to induce implication relations, whereas others were used to verify the results of evidential reasoning with the induced networks. The values in the implication networks were predicted by applying a modified version of the Dempster-Shafer belief updating scheme. The results of predictions were, furthermore, compared to the ones generated by Pearl's stochastic simulation method [21], a probabilistic reasoning method that operates directly on the theoretical Bayesian networks. The comparisons consistently show that the results of predictions based on the induced networks would be comparable to those generated by Pearl's method, when reasoning in a variety of uncertain knowledge domains驴those that were simulated using the presumed theoretical probabilistic networks of different topologies. Moreover, our validation experiments also reveal that the comparable performance of the implication-network-based-reasoning method can be achieved with much less computational cost than Pearl's stochastic simulation method; specifically, in all our experiments, the ratio between the actual CPU time required by our method and that by Pearl's is approximately 1:100.

#index 443180
#* Efficient Scheduling of Page Access in Index-Based Join Processing
#@ Chee Yong Chan;Beng Chin Ooi
#t 1997
#c 7
#% 1388
#% 18614
#% 50725
#% 51375
#% 86954
#% 114577
#% 136740
#% 442665
#% 442675
#! This paper examines the issue of scheduling page accesses in join processing, and proposes new heuristics for the following scheduling problems: 1) an optimal page access sequence for a join such that there are no page reaccesses using the minimum number of buffer pages, and 2) an optimal page access sequence for a join such that the number of page reaccesses for a given number of buffer pages is minimum. Our experimental performance results show that the new heuristics perform better than existing heuristics for the first problem and also perform better for the second problem, provided that the number of available buffer pages is not much less than the optimal buffer size.

#index 443181
#* Designing Access Methods for Bitemporal Databases
#@ Anil Kumar;Vassilis J. Tsotras;Christos Faloutsos
#t 1998
#c 7
#% 672
#% 16028
#% 56081
#% 58371
#% 68091
#% 70370
#% 77945
#% 83105
#% 86950
#% 88697
#% 102759
#% 102809
#% 137893
#% 157868
#% 163440
#% 163442
#% 182672
#% 210182
#% 243780
#% 246003
#% 287070
#% 427199
#% 435156
#% 442781
#% 442967
#% 442972
#% 443130
#% 462480
#% 480093
#% 527802
#% 571296
#% 656697
#! By supporting the valid and transaction time dimensions, bitemporal databases represent reality more accurately than conventional databases. In this paper, we examine the issues involved in designing efficient access methods for bitemporal databases, and propose the partial-persistence and the double-tree methodologies. The partial-persistence methodology reduces bitemporal queries to partial persistence problems for which an efficient access method is then designed. The double-tree methodology "sees" each bitemporal data object as consisting of two intervals (a valid-time and a transaction-time interval) and divides objects into two categories according to whether the right endpoint of the transaction time interval is already known. A common characteristic of both methodologies is that they take into account the properties of each time dimension. Their performance is compared with a straightforward approach that "sees" the intervals associated with a bitemporal object as composing one rectangle, which is stored in a single multidimensional access method. Given that some limited additional space is available, our experimental results show that the partial-persistence methodology provides the best overall performance, especially for transaction timeslice queries. For those applications that require ready, off-the-shelf, access methods, the double-tree methodology is a good alternative.

#index 443182
#* A Multiagent Update Process in a Database with Temporal Data Dependencies and Schema Versioning
#@ Avigdor Gal;Opher Etzion
#t 1998
#c 7
#% 32914
#% 58345
#% 59350
#% 62022
#% 115661
#% 116045
#% 122918
#% 125967
#% 140609
#% 163047
#% 163440
#% 163442
#% 166940
#% 172956
#% 177755
#% 268797
#% 286176
#% 286250
#% 286856
#% 287003
#% 437276
#% 442887
#% 442962
#% 442967
#% 442970
#% 442985
#% 442988
#% 442990
#% 443029
#% 445669
#% 480609
#% 480626
#% 482069
#! Temporal data dependencies are high-level linguistic constructs that define relationships among values of data-elements in temporal databases. These constructs enable the support of schema versioning as well as the definition of consistency requirements for a single time-point and among values in different time-points. In this paper, we present a multiagent update process in a database with temporal data dependencies and schema versioning. The update process supports the evolution of dependencies over time and the use of temporal operators within temporal data dependencies. The temporal dependency language is presented, along with the temporal dependency graph驴which serves as the executable data structure. A thorough discussion of the feasibility, performance, and consistency of the presented model is provided.

#index 443183
#* Topological Invariants for Lines
#@ Eliseo Clementini;Paolino Di Felice
#t 1998
#c 7
#% 24519
#% 28099
#% 64786
#% 108509
#% 184260
#% 195459
#% 254710
#% 362928
#% 408638
#% 435137
#% 442847
#% 526859
#% 526863
#% 526995
#% 526998
#% 527008
#% 527013
#% 546590
#% 549078
#% 600046
#! A set of topological invariants for relations between lines embedded in the 2-dimensional Euclidean space is given. The set of invariants is proven to be necessary and sufficient to characterize topological equivalence classes of binary relations between simple lines. The topology of arbitrarily complex geometric scenes is described with a variation of the same set of invariants. Polynomial time algorithms are given to assess topological equivalence of two scenes. The relevance of identifying such a set of invariants and efficient algorithms is due to application areas of spatial database systems, where a model for describing topological relations between planar features is sought.

#index 443184
#* OSAM*.KBMS/P: A Parallel, Active, Object-Oriented Knowledge Base Server
#@ Stanley Y. W. Su;Ramamohanrao Jawadi;Prashant Cherukuri;Qiang Li;Richard Nartey
#t 1998
#c 7
#% 18606
#% 37972
#% 45257
#% 58361
#% 62021
#% 70370
#% 77980
#% 77999
#% 83933
#% 86942
#% 115661
#% 116049
#% 122908
#% 122909
#% 122911
#% 122914
#% 140056
#% 154342
#% 157532
#% 158047
#% 320902
#% 340633
#% 403195
#% 442733
#% 442963
#% 464043
#% 479584
#% 480078
#% 480600
#% 480620
#% 480621
#% 480765
#% 480779
#% 480800
#% 480938
#% 480942
#% 481261
#% 481448
#% 702152
#! An active object-oriented knowledge base server can provide many desirable features for supporting a wide spectrum of advanced and complex database applications. Knowledge rules, which are used to define a variety of database tasks to be performed automatically on the occurrence of some events, often need much more sophisticated rule-specification and control mechanisms than the traditional priority-based mechanism to specify the control structural relationships and parallel execution properties among rules. The underlying object-oriented (OO) knowledge representation model must provide a means to model the structural relationships among data entities and the control structures among rules in a uniform fashion. The transaction execution model must provide a means to incorporate the execution of structured rules in a transaction framework. Also, a parallel implementation of an active knowledge base server is essential to achieve the needed efficiency in processing nested transactions and rules. In this paper, we present the architecture, implementation, and performance of a parallel active OO knowledge base server, which has the following features. First, the server is developed based on an extended OO knowledge representation model that models rules as objects and their control structural relationships as association types. This is analogous to the modeling of entities as objects and their structural relationships as association types. Thus, entities and rules, and their structures, can be uniformly modeled. Second, the server uses a graph-based transaction model that can naturally incorporate the control semantics of structured rules and guarantee the serializable execution of rules as subtransactions. Thus, the rule-execution model is uniformly integrated with that of transactions. Third, it uses an asynchronous parallel execution model to process the graph-based transactions and structured rules. This server, named OSAM*.KBMS/P, has been implemented on a shared-nothing multiprocessor system (nCUBE2) to verify and evaluate the proposed knowledge-representation model, graph-based transaction model, and asynchronous parallel-execution model.

#index 443185
#* Arbitration (or How to Merge Knowledge Bases)
#@ Paolo Liberatore;Marco Schaerf
#t 1998
#c 7
#% 2655
#% 8417
#% 109945
#% 111378
#% 131559
#% 137868
#% 158909
#% 416007
#! Knowledge-based systems must be able to "intelligently" manage a large amount of information coming from different sources and at different moments in time. Intelligent systems must be able to cope with a changing world by adopting a "principled" strategy. Many formalisms have been put forward in the artificial intelligence (AI) and database (DB) literature to address this problem. Among them, belief revision is one of the most successful frameworks to deal with dynamically changing worlds. Formal properties of belief revision have been investigated by Alchourron, Gärdenfors, and Makinson, who put forward a set of postulates stating the properties that a belief revision operator should satisfy. Among these properties, a basic assumption of revision is that the new piece of information is totally reliable and, therefore, must be in the revised knowledge base. Different principles must be applied when there are two different sources of information and each one has a different view of the situation驴the two views contradicting each other. If we do not have any reason to consider any of the sources completely unreliable, the best we can do is to "merge" the two views in a new and consistent one, trying to preserve as much information as possible. We call this merging process arbitration. In this paper, we investigate the properties that any arbitration operator should satisfy. In the style of Alchourron, Gärdenfors, and Makinson we propose a set of postulates, analyze their properties, and propose actual operators for arbitration.

#index 443186
#* A Logic-Based Transformation System
#@ Jeffrey J. P. Tsai;Bing Li;Thomas Weigert
#t 1998
#c 7
#% 23447
#% 42481
#% 48289
#% 59973
#% 69292
#% 70367
#% 130789
#% 132224
#% 395111
#! In spite of advances in various transformation systems [5], [8], the transformation of a nonmonotonic-logic-based requirements specification into a procedural (imperative) language program has not been investigated. This paper presents a logic-based transformation system that can transform a nonmonotonic-logic-based specification, the Frame-and-Rule Oriented Requirement Specification Language (FRORL) [12], into procedural language programs. We discuss how to handle nonmonotonic inheritance in FRORL and then establish a matrix-based data flow and dependency analysis mechanism to find all the possible data transformation paths in a logic-based specification. Using a newly developed algorithm, we can adjust the execution sequence of a logic-based specification so that the functions included in the logic-based specification can be represented by a sequential procedural language program.

#index 443187
#* Coherence Approach to Logic Program Revision
#@ Li-Yan Yuan;Jia-Huai You
#t 1998
#c 7
#% 77167
#% 95248
#% 130379
#% 136798
#% 146270
#% 147604
#% 154317
#% 173127
#% 217017
#% 268779
#% 501032
#% 1907803
#! In this paper, we present a new approach to the problem of revising extended programs; we base this approach on the coherence theory initially advocated by Gardenfors for belief revision. Our approach resolves contradiction by removing only conflicting information, not the believed source of it, and therefore, keeps information loss minimal. Furthermore, since there is no need to search for problematic assumptions, as is done in the traditional assumption-removal approach, our approach provides a skeptical revision semantics that is tractable. We define the skeptical and credulous coherence semantics and show that both semantics can be characterized in terms of the fixpoint semantics of a revised program using a simple program-revision technique. These semantics provide a suitable framework for knowledge and belief revision in the context of logic programs. Semantical properties and advantages of the proposed revision semantics are also analyzed.

#index 443188
#* Reorganizing Knowledge to Improve Performance
#@ A. Lopez-Suarez;M. Kamel
#t 1998
#c 7
#% 68243
#% 163721
#% 166347
#% 173565
#% 444762
#% 449530
#! This paper presents a method to reorganize rules in knowledge bases with the objective of improving their performance. Knowledge reorganization is achieved through the combination of rule compression and abstraction techniques. The effectiveness of this methodology is evaluated in terms of pattern matching activity and execution times using knowledge bases from several application areas.

#index 443189
#* Advanced Transaction Processing in Multilevel Secure File Stores
#@ Elisa Bertino;Sushil Jajodia;Luigi Mancini;Indrajit Ray
#t 1998
#c 7
#% 9241
#% 319768
#% 320469
#% 322637
#% 374401
#% 403195
#% 444381
#% 462628
#% 488006
#% 664471
#% 664531
#! The concurrency control requirements for transaction processing in a multilevel secure file system are different from those in conventional transaction processing systems. In particular, there is the need to coordinate transactions at different security levels avoiding both potential timing covert channels and the starvation of transactions at higher security levels. Suppose a transaction at a lower security level attempts to write a data item that is being read by a transaction at a higher security level. On the one hand, a timing covert channel arises if the transaction at the lower security level is either delayed or aborted by the scheduler. On the other hand, the transaction at the high security level may be subjected to an indefinite delay if it is forced to abort repeatedly. This paper extends the classical two-phase locking mechanism to multilevel secure file systems. The scheme presented here prevents potential timing covert channels and avoids the abort of higher level transactions nonetheless guaranteeing serializability. The programmer is provided with a powerful set of linguistic constructs that supports exception handling, partial rollback, and forward recovery. The proper use of these constructs can prevent the indefinite delay in completion of a higher level transaction, and allows the programmer to trade off starvation with transaction isolation.

#index 443190
#* The Design and Implementation of Seeded Trees: An Efficient Method for Spatial Joins
#@ Ming-Ling Lo;Chinya V. Ravishankar
#t 1998
#c 7
#% 18614
#% 25924
#% 32913
#% 58369
#% 68091
#% 86950
#% 86952
#% 152937
#% 172909
#% 285932
#% 427199
#% 462957
#% 463116
#% 463577
#% 463595
#% 480093
#% 526847
#% 527012
#! Existing methods for spatial joins require pre-existing spatial indices or other precomputation, but such approaches are inefficient and limited in generality. Operand data sets of spatial joins may not all have precomputed indices, particularly when they are dynamically generated by other selection or join operations. Also, existing spatial indices are mostly designed for spatial selections, and are not always efficient for joins. This paper explores the design and implementation of seeded trees [1], which are effective for spatial joins and efficient to construct at join time. Seeded trees are R-tree-like structures, but divided into seed levels and grown levels. This structure facilitates using information regarding the join to accelerate the join process, and allows efficient buffer management. In addition to the basic structure and behavior of seeded trees, we present techniques for efficient seeded tree construction, a new buffer management strategy to lower I/O costs, and theoretical analysis for choosing algorithmic parameters. We also present methods for reducing space requirements and improving the stability of seeded tree performance with no additional I/O costs. Our performance studies show that the seeded tree method outperforms other tree-based methods by far both in terms of the number disk pages accessed and weighted I/O costs. Further, its performance gain is stable across different input data, and its incurred CPU penalties are also lower.

#index 443191
#* A Highly Effective Partition Selection Policy for Object Database Garbage Collection
#@ Jonathan E. Cook;Alexander L. Wolf;Benjamin G. Zorn
#t 1998
#c 7
#% 32912
#% 52029
#% 58343
#% 77677
#% 83126
#% 102745
#% 103608
#% 116203
#% 126812
#% 139032
#% 152904
#% 152930
#% 210201
#% 317993
#% 318024
#% 319021
#% 320108
#% 452764
#% 463741
#% 473366
#% 481756
#% 604239
#% 674286
#% 697617
#% 753876
#! We investigate methods to improve the performance of algorithms for automatic storage reclamation of object databases. These algorithms are based on a technique called partitioned garbage collection, in which a subset of the entire database is collected independently of the rest. We evaluate how different application, database system, and garbage collection implementation parameters affect the performance of garbage collection in object database systems. We focus specifically on investigating the policy that is used to select which partition in the database should be collected. Three of the policies that we investigate are based on the intuition that the values of overwritten pointers provide good hints about where to find garbage. A fourth policy investigated chooses the partition with the greatest presence in the I/O buffer. Using simulations based on a synthetic database, we show that one of our policies requires less I/O to collect more garbage than any existing implementable policy. Furthermore, that policy performs close to a locally optimal policy over a wide range of simulation parameters, including database size, collection rate, and database connectivity. We also show what impact these simulation parameters have on application performance and investigate the expected costs and benefits of garbage collection in such systems.

#index 443192
#* Distributed Optimistic Concurrency Control Methods for High-Performance Transaction Processing
#@ Alexander Thomasian
#t 1998
#c 7
#% 1326
#% 2544
#% 3951
#% 9241
#% 27057
#% 28618
#% 35080
#% 39634
#% 40180
#% 54513
#% 101820
#% 114710
#% 116055
#% 135877
#% 152596
#% 286836
#% 287470
#% 290467
#% 360720
#% 373822
#% 403195
#% 427217
#% 442696
#% 444242
#% 444340
#% 445733
#% 462621
#% 462779
#% 462788
#% 462961
#% 463092
#% 480256
#% 597589
#% 605712
#! There is an ever-increasing demand for more complex transactions and higher throughputs in transaction processing systems leading to higher degrees of transaction concurrency and, hence, higher data contention. The conventional two-phase locking (2PL) Concurrency Control (CC) method may, therefore, restrict system throughput to levels inconsistent with the available processing capacity. This is especially a concern in shared-nothing or data-partitioned systems due to the extra latencies for internode communication and a reliable commit protocol. The optimistic CC (OCC) is a possible solution, but currently proposed methods have the disadvantage of repeated transaction restarts. We present a distributed OCC method followed by locking, such that locking is an integral part of distributed validation and two-phase commit. This method ensures at most one re-execution, if the validation for the optimistic phase fails. Deadlocks, which are possible with 2PL, are prevented by preclaiming locks for the second execution phase. This is done in the same order at all nodes. We outline implementation details and compare the performance of the new OCC method with distributed 2PL through a detailed simulation that incorporates queueing effects at the devices of the computer systems, buffer management, concurrency control, and commit processing. It is shown that for higher data contention levels, the hybrid OCC method allows a much higher maximum transaction throughput than distributed 2PL in systems with high processing capacities. In addition to the comparison of CC methods, the simulation study is used to study the effect of varying the number of computer systems with a fixed total processing capacity and the effect of locality of access in each case. We also describe several interesting variants of the proposed OCC method, including methods for handling access variance, i.e., when rerunning a transaction results in accesses to a different set of objects.

#index 443193
#* Efficient Attribute-Oriented Generalization for Knowledge Discovery from Large Databases
#@ Colin L. Carter;Howard J. Hamilton
#t 1998
#c 7
#% 106656
#% 125635
#% 156980
#% 169704
#% 201894
#% 452747
#% 452821
#% 463903
#% 481290
#% 618426
#% 1290164
#! We present GDBR (Generalize DataBase Relation) and FIGR (Fast, Incremental Generalization and Regeneralization), two enhancements of Attribute-Oriented Generalization, a well-known knowledge discovery from databases technique. GDBR and FIGR are both O(n) and, as such, are optimal. GDBR is an on-line algorithm and requires only a small, constant amount of space. FIGR also requires a constant amount of space that is generally reasonable although, under certain circumstances, may grow large. FIGR is incremental, allowing changes to the database to be reflected in the generalization results without rereading input data. FIGR also allows fast regeneralization to both higher and lower levels of generality without rereading input. We compare GDBR and FIGR to two previous algorithms, LCHR and AOI, which are O(n log n) and O(np), respectively, where n is the number of input tuples and p the number of tuples in the generalized relation. Both require O(n) space that, for large input, causes memory problems. We implemented all four algorithms and ran empirical tests, and we found that GDBR and FIGR are faster. In addition, their runtimes increase only linearly as input size increases, while the runtimes of LCHR and AOI increase greatly when input size exceeds memory limitations.

#index 443194
#* Efficient Data Mining for Path Traversal Patterns
#@ Ming-Syan Chen;Jong Soo Park;Philip S. Yu
#t 1998
#c 7
#% 152934
#% 172892
#% 173771
#% 186340
#% 257945
#% 394246
#% 443164
#% 449588
#% 460862
#% 463257
#% 463903
#% 480940
#% 480964
#% 481281
#% 481290
#% 481588
#! In this paper, we explore a new data mining capability that involves mining path traversal patterns in a distributed information-providing environment where documents or objects are linked together to facilitate interactive access. Our solution procedure consists of two steps. First, we derive an algorithm to convert the original sequence of log data into a set of maximal forward references. By doing so, we can filter out the effect of some backward references, which are mainly made for ease of traveling and concentrate on mining meaningful user access sequences. Second, we derive algorithms to determine the frequent traversal patterns驴i.e., large reference sequences驴from the maximal forward references obtained. Two algorithms are devised for determining large reference sequences; one is based on some hashing and pruning techniques, and the other is further improved with the option of determining large reference sequences in batch so as to reduce the number of database scans required. Performance of these two methods is comparatively analyzed. It is shown that the option of selective scan is very advantageous and can lead to prominent performance improvement. Sensitivity analysis on various parameters is conducted.

#index 443195
#* Discovering Frequent Event Patterns with Multiple Granularities in Time Sequences
#@ Claudio Bettini;X. Sean Wang;Sushil Jajodia;Jia-Ling Lin
#t 1998
#c 7
#% 1267
#% 107137
#% 162493
#% 163434
#% 172892
#% 213963
#% 225003
#% 408638
#% 417598
#% 452821
#% 463738
#% 463903
#% 542940
#! An important usage of time sequences is to discover temporal patterns. The discovery process usually starts with a user-specified skeleton, called an event structure, which consists of a number of variables representing events and temporal constraints among these variables; the goal of the discovery is to find temporal patterns, i.e., instantiations of the variables in the structure that appear frequently in the time sequence. This paper introduces event structures that have temporal constraints with multiple granularities, defines the pattern-discovery problem with these structures, and studies effective algorithms to solve it. The basic components of the algorithms include timed automata with granularities (TAGs) and a number of heuristics. The TAGs are for testing whether a specific temporal pattern, called a candidate complex event type, appears frequently in a time sequence. Since there are often a huge number of candidate event types for a usual event structure, heuristics are presented aiming at reducing the number of candidate event types and reducing the time spent by the TAGs testing whether a candidate type does appear frequently in the sequence. These heuristics exploit the information provided by explicit and implicit temporal constraints with granularity in the given event structure. The paper also gives the results of an experiment to show the effectiveness of the heuristics on a real data set.

#index 443196
#* Optimization of Rule-Based Systems Using State Space Graphs
#@ Blaz Zupan;Albert Mo Kim Cheng
#t 1998
#c 7
#% 94458
#% 116045
#% 144497
#% 442816
#% 442885
#% 442929
#% 445781
#% 543155
#% 589690
#! Embedded rule-based expert systems must satisfy stringent timing constraints when applied to real-time environments. The paper describes a novel approach to reduce the response time of rule-based expert systems. Our optimization method is based on a construction of the reduced cycle-free finite state space graph. In contrast with traditional state space graph derivation, our optimization algorithm starts from the final states (fixed points) and gradually expands the state space graph until all of the states with a reachable fixed point are found. The new and optimized system is then synthesized from the constructed state space graph. We present several algorithms implementing the optimization method. They vary in complexity as well as in the usage of concurrency and state-equivalency驴both targeted toward minimizing the size of the optimized state space graph. Though depending on the algorithm used, optimized rule-based systems: 1) in general have better response time in that they require fewer rule firings to reach the fixed point; 2) are stable, i.e., have no cycles that would result in the instability of execution; and 3) have no redundant rules. We also address the issue of deterministic execution and propose optimization algorithms that generate the rule-bases with single corresponding fixed points for every initial state. The synthesis method also determines the tight response time bound of the new system and can identify unstable states in the original rule-base. No information other than the rule-based real-time decision program itself is given to the optimization method. The optimized system is guaranteed to compute correct results independent of the scheduling strategy and execution environment.

#index 443197
#* ADOME: An Advanced Object Modeling Environment
#@ Qing Li;Frederick H. Lochovsky
#t 1998
#c 7
#% 5972
#% 12149
#% 23931
#% 36683
#% 37972
#% 52373
#% 55252
#% 57478
#% 57951
#% 62021
#% 71550
#% 80438
#% 80439
#% 83526
#% 102788
#% 109160
#% 111345
#% 111361
#% 116049
#% 116203
#% 120696
#% 140389
#% 152921
#% 168957
#% 209648
#% 408062
#% 435153
#% 442725
#% 480599
#% 480600
#% 480621
#% 480779
#% 480790
#% 481095
#% 481448
#% 487900
#! ADOME, which stands for ADvanced Object Modeling Environment, is an approach to integrating data and knowledge management based on object-oriented technology. Next generation information systems will require more flexible data modeling capabilities than those provided by current object-oriented DBMSs. In particular, integration of data and knowledge management capabilities will become increasingly important. In this context, ADOME provides versatile role facilities that serve as "dynamic binders" between data objects and production rules, thereby facilitating flexible data and knowledge management integration. A prototype that implements this mechanism and the associated operators has been constructed on top of a commercial object-oriented DBMS and a rule-base system.

#index 443198
#* Temporal Semantic Assumptions and Their Use in Databases
#@ Claudio Bettini;X. Sean Wang;Sushil Jajodia
#t 1998
#c 7
#% 18615
#% 21136
#% 23013
#% 32915
#% 36683
#% 99137
#% 137916
#% 152936
#% 163442
#% 168262
#% 169831
#% 178813
#% 201923
#% 225003
#% 268796
#% 286257
#% 287221
#% 361445
#% 417598
#% 452818
#% 461861
#% 463287
#% 467630
#% 527797
#% 533357
#! Data explicitly stored in a temporal database are often associated with certain semantic assumptions. Each assumption can be viewed as a way of deriving implicit information from explicitly stored data. Rather than leaving the task of deriving (possibly infinite) implicit data to application programs, as is the case currently, it is desirable that this be handled by the database management system. To achieve this, this paper formalizes and studies two types of semantic assumptions: point-based and interval-based. The point-based assumptions include those assumptions that use interpolation methods over values at different time instants, while the interval-based assumptions include those that involve the conversion of values across different time granularities. The paper presents techniques on: 1) how assumptions on specific sets of attributes can be automatically derived from the specification of interpolation and conversion functions, and 2) given the representation of assumptions, how a user query can be converted into a system query such that the answer of this system query over the explicit data is the same as that of the user query over the explicit and the implicit data. To precisely illustrate concepts and algorithms, the paper uses a logic-based abstract query language. The paper also shows how the same concepts can be applied to concrete temporal query languages.

#index 443199
#* A Decision Model for Choosing the Optimal Level of Storage in Temporal Databases
#@ Debabrata Dey;Terence M. Barron;Aditya N. Saharia
#t 1998
#c 7
#% 3236
#% 18615
#% 32915
#% 43028
#% 163438
#% 163440
#% 200993
#% 286256
#% 287007
#% 287221
#% 406493
#% 442967
#% 463861
#% 571252
#! A database allows its users to reduce uncertainty about the world. However, not all properties of all objects can always be stored in a database. As a result, the user may have to use probabilistic inference rules to estimate the data required for his decisions. A decision based on such estimated data may not be perfect. We call the costs associated with such suboptimal decisions the cost of incomplete information. This cost can be reduced by expanding the database to contain more information; such expansion will increase the data-related costs because of more data collection, manipulation, storage, and retrieval. A database designer must then consider the trade-off between the cost of incomplete information and the data-related costs, and choose a design that minimizes the overall cost to the organization. In temporal databases, the sheer volume of the data involved makes such a trade-off at design time all the more important. In this paper, we develop probabilistic inference rules that allow us to infer missing values in spatial, as well as temporal, dimension. We then use the framework for developing guidelines for designing and reorganizing temporal databases, which explicitly includes a trade-off between the incomplete information and the data-related costs.

#index 443200
#* Scalability Analysis of Declustering Methods for Multidimensional Range Queries
#@ Bongki Moon;Joel H. Saltz
#t 1998
#c 7
#% 13032
#% 30073
#% 43171
#% 43179
#% 43201
#% 64431
#% 67168
#% 83235
#% 86951
#% 108505
#% 112180
#% 115661
#% 116042
#% 135559
#% 159080
#% 172881
#% 188719
#% 202647
#% 219677
#% 285932
#% 286962
#% 339622
#% 415957
#% 427199
#% 435201
#% 442469
#% 442698
#% 442700
#% 463721
#% 466952
#% 469603
#% 479936
#% 480794
#% 481109
#% 499803
#% 565461
#! Efficient storage and retrieval of multiattribute data sets has become one of the essential requirements for many data-intensive applications. The Cartesian product file has been known as an effective multiattribute file structure for partial-match and best-match queries. Several heuristic methods have been developed to decluster Cartesian product files across multiple disks to obtain high performance for disk accesses. Although the scalability of the declustering methods becomes increasingly important for systems equipped with a large number of disks, no analytic studies have been done so far. In this paper, we derive formulas describing the scalability of two popular declustering methods驴Disk Modulo and Fieldwise Xor驴for range queries, which are the most common type of queries. These formulas disclose the limited scalability of the declustering methods, and this is corroborated by extensive simulation experiments. From the practical point of view, the formulas given in this paper provide a simple measure that can be used to predict the response time of a given range query and to guide the selection of a declustering method under various conditions.

#index 443201
#* Differential Relational Calculus for Integrity Maintenance
#@ Levent V. Orman
#t 1998
#c 7
#% 271
#% 45257
#% 51400
#% 53392
#% 67457
#% 102314
#% 164364
#% 222616
#% 318050
#% 318271
#% 384112
#% 427223
#% 442727
#% 442767
#% 481128
#! A differential calculus for first-order logic is developed to enforce database integrity. Formal differentiation of first-order sentences is useful in maintaining database integrity, since once a database constraint is expressed as a first-order sentence, its derivative with respect to a transaction provides the necessary and sufficient condition for maintaining integrity. The derivative is often much simpler to test than the original constraint since it maintains integrity differentially by assuming integrity before the transaction, and testing only for new violations. The formal differentiation requires no resolution search, but only substitution. It is more efficient than resolution-based approaches; and it provides a considerably more general solution than previous substitution-based methods since it is valid for all first-order sentences and with all transactions involving arbitrary collections of atomic changes to the database. It also produces a large number of sufficient conditions that are often less strict than those of the previous approaches; and it can be extended to accommodate many dynamic constraints.

#index 443202
#* Principal Interconnections in Higher Order Hebbian-Type Associative Memories
#@ Jung-Hua Wang
#t 1998
#c 7
#% 24214
#% 92148
#% 1859987
#% 1862543
#! The existence of principal interconnections useful in solving the proliferation problem in higher order Hebbian-type associative memories is introduced. Among all legal interconnections, we prove there exists a subset Tpr that carries more information than the others. Regardless of the network order p, the elements in Tpr are shown to be those interconnections T that fall within the range of$$\sqrt {m_s} \le \left| T \right| \le 2 \sqrt {m_s},$$where ms equals the number of stored codewords. Memories that use only Tpr can maintain original generalization performance, using less than 50 percent of the total number of interconnections.

#index 443203
#* A Qualitative Discriminant Approach for Generating Quantitative Belief Functions
#@ Noel (Kweku-Muata) Bryson;Ayodele Mobolurin
#t 1998
#c 7
#% 167202
#% 442845
#! We present an approach that will be useful in knowledge acquisition from experts on the degree of belief in, or the probability of, the truthfulness of various propositions. Its advantages include exploring the given problem situation using linguistic quantifiers; avoiding the premature use of numeric measures; and identifying input data that is inconsistent with the theory of belief functions.

#index 443204
#* Compile-Time and Runtime Analysis of Active Behaviors
#@ Elena Baralis;Stefano Ceri;Stefano Paraboschi
#t 1998
#c 7
#% 1797
#% 83315
#% 86939
#% 86944
#% 116044
#% 153004
#% 167258
#% 168765
#% 170898
#% 172954
#% 179619
#% 182421
#% 205241
#% 210182
#% 213978
#% 223887
#% 403195
#% 459253
#% 459266
#% 464698
#% 480620
#% 481106
#% 481128
#% 481456
#% 501935
#% 501936
#% 501948
#% 535190
#! Active rules may interact in complex and sometimes unpredictable ways, thus possibly yielding infinite rule executions by triggering each other indefinitely. This paper presents analysis techniques focused on detecting termination of rule execution. We describe an approach which combines static analysis of a rule set at compile-time and detection of endless loops during rule processing at runtime. The compile-time analysis technique is based on the distinction between mutual triggering and mutual activation of rules. This distinction motivates the introduction of two graphs defining rule interaction, called Triggering and Activation Graphs, respectively. This analysis technique allows us to identify reactive behaviors which are guaranteed to terminate and reactive behaviors which may lead to infinite rule processing. When termination cannot be guaranteed at compile-time, it is crucial to detect infinite rule executions at runtime. We propose a technique for identifying loops which is based on recognizing that a given situation has already occurred in the past and, therefore, will occur an infinite number of times in the future. This technique is potentially very expensive, therefore, we explain how it can be implemented in practice with limited computational effort. A particular use of this technique allows us to develop cycle monitors, which check that critical rule sequences, detected at compile time, do not repeat forever. We bridge compile-time analysis to runtime monitoring by showing techniques, based on the result of rule analysis, for the identification of rule sets that can be independently monitored and for the optimal selection of cycle monitors.

#index 443205
#* Path Dictionary: A New Access Method for Query Processing in Object-Oriented Databases
#@ Wang-Chien Lee;Dik Lun Lee
#t 1998
#c 7
#% 18614
#% 57955
#% 58372
#% 77649
#% 77656
#% 77996
#% 83148
#% 86954
#% 152938
#% 172333
#% 286189
#% 442665
#% 462474
#% 462798
#% 463603
#% 463740
#% 511155
#% 534716
#! We present a new access method, called the path dictionary index (PDI) method, for supporting nested queries on object-oriented databases. PDI supports object traversal and associative search, respectively, with a path dictionary and a set of attribute indexes built on top of the path dictionary. We discuss issues on indexing and query processing in object-oriented databases; describe the operations of the new mechanism; develop cost models for its storage overhead and query and update costs; and compare the new mechanism to the path index method. The result shows that the path dictionary index method is significantly better than the path index method over a wide range of parameters in terms of retrieval and update costs and that the storage overhead grows slowly with the number of indexed attributes.

#index 443206
#* Collaborative Multimedia Systems: Synthesis of Media Objects
#@ K. Selçuk Candan;P. Venkat Rangan;V. s. Subrahmanian
#t 1998
#c 7
#% 70370
#% 151321
#% 173691
#% 173693
#% 173695
#% 194028
#% 194049
#% 194063
#% 217208
#% 219919
#% 452790
#% 464216
#% 661698
#! When a group {I1, ..., In} of individuals wishes to collaboratively construct a complex multimedia document, the first requirement is that they be able to manipulate media-objects created by one another. For instance, if individual Ij wishes to access some media objects present at participant Ik's site, he must be able to: 1) retrieve this object from across the network, 2) ensure that the object is in a form that is compatible with the viewing/editing resources he has available at his node, and 3) ensure that the object has the desired quality (such as image size and resolution). Furthermore, he must be able to achieve these goals at the lowest possible cost. In this paper, we develop a theory of media objects, and present optimal algorithms for collaborative object sharing/synthesis of the sort envisaged above. We then extend the algorithms to incorporate quality constraints (such as image size) as well as distribution across multiple nodes. The theoretical model is validated by an experimental implementation that supports the theoretical results.

#index 443207
#* Temporal Association Algebra: A Mathematical Foundation for Processing Object-Oriented Temporal Databases
#@ Stanley Y. W. Su;Soon J. Hyun;Hsin-Hsing M. Chen
#t 1998
#c 7
#% 18615
#% 18772
#% 32915
#% 43028
#% 53706
#% 77999
#% 83526
#% 111284
#% 154330
#% 154342
#% 163440
#% 168773
#% 189321
#% 213223
#% 236667
#% 286256
#% 286257
#% 286831
#% 319244
#% 340307
#% 442887
#% 442960
#% 452815
#% 461863
#% 462787
#% 463254
#% 480609
#% 480962
#% 487752
#% 568176
#! This paper describes an object-oriented temporal association algebra (called TA-algebra) which is intended to serve as a formal foundation for supporting a pattern-based query specification and processing paradigm. Different from the traditional table-and-attribute-based paradigm, the pattern-based paradigm views the intension of an object-oriented temporal database as a network of object classes interconnected by different association types and its extension as a network of associated temporal object instances. Consistent with this view, queries can be specified in terms of patterns of temporal object associations or nonassociations (i.e., linear, tree and network structures of object classes/objects with logical AND and OR branches). TA-algebra provides a set of algebraic operators for processing these patterns and allows the direct and/or indirect associations and/or nonassociations among temporal object instances to be more explicitly represented and maintained during processing than the traditional tabular representation of temporary or final query results. TA-algebra operators are based on time-interval and valid-time semantics and they preserve the closure property. The algebra is capable of operating on heterogeneous as well as homogeneous patterns of object associations. Both homogeneous and heterogeneous patterns are decomposed into a set of primitive temporal pattern instances for uniform treatment. This paper formally defines the TA-algebra operators and their mathematical properties. The applications of these operators in query decomposition and processing are illustrated by examples.

#index 443208
#* Hierarchical Encoded Path Views for Path Query Processing: An Optimal Model and Its Performance Evaluation
#@ Ning Jing;Yun-Wu Huang;Elke A. Rundensteiner
#t 1998
#c 7
#% 241
#% 70370
#% 77979
#% 83158
#% 83159
#% 99461
#% 106116
#% 139176
#% 152958
#% 214719
#% 214769
#% 232454
#% 340642
#% 462057
#% 463409
#% 463413
#% 463421
#% 463583
#% 464223
#% 480284
#% 481094
#% 565445
#% 566112
#! Efficient path computation is essential for applications such as intelligent transportation systems (ITS) and network routing. In ITS navigation systems, many path requests can be submitted over the same, typically huge, transportation network within a small time window. While path precomputation (path view) would provide an efficient path query response, it raises three problems which must be addressed: 1) precomputed paths exceed the current computer main memory capacity for large networks; 2) disk-based solutions are too inefficient to meet the stringent requirements of these target applications; and 3) path views become too costly to update for large graphs (resulting in out-of-date query results). We propose a hierarchical encoded path view (HEPV) model that addresses all three problems. By hierarchically encoding partial paths, HEPV reduces the view encoding time, updating time and storage requirements beyond previously known path precomputation techniques, while significantly minimizing path retrieval time. We prove that paths retrieved over HEPV are optimal. We present complete solutions for all phases of the HEPV approach, including graph partitioning, hierarchy generation, path view encoding and updating, and path retrieval. In this paper, we also present an in-depth experimental evaluation of HEPV based on both synthetic and real GIS networks. Our results confirm that HEPV offers advantages over alternative path finding approaches in terms of performance and space efficiency.

#index 443209
#* Techniques for Update Handling in the Enhanced Client-Server DBMS
#@ Alex Delis;Nick Roussopoulos
#t 1998
#c 7
#% 121
#% 1757
#% 2853
#% 3771
#% 3948
#% 6798
#% 68148
#% 70068
#% 77005
#% 83126
#% 83127
#% 89775
#% 98469
#% 100060
#% 102802
#% 102803
#% 111351
#% 111361
#% 114988
#% 115674
#% 116061
#% 117751
#% 125595
#% 159275
#% 172879
#% 184804
#% 209996
#% 403195
#% 437229
#% 442704
#% 445733
#% 462351
#% 462620
#% 462621
#% 464219
#% 479920
#% 480095
#% 480252
#% 480930
#% 481108
#% 636467
#% 660067
#! The Client-Server computing paradigm has significantly influenced the way modern Database Management Systems are designed and built. In such systems, clients maintain data pages in their main-memory caches, originating from the server's database. The Enhanced Client-Server architecture takes advantage of all the available client resources, including their long-term memory. Clients can cache server data into their own disk units if these data are part of their operational spaces. However, when updates occur at the server, a number of clients may need to not only be notified about these changes, but also obtain portions of the updates as well. In this paper, we examine the problem of managing server imposed updates that affect data cached on client disk managers. We propose a number of server update propagation techniques in the context of the Enhanced Client-Server DBMS architecture, and examine the performance of these strategies through detailed simulation experiments. In addition, we study how the various settings of the network affect the performance of these policies.

#index 443210
#* Performance Analysis of Three Text-Join Algorithms
#@ Weiyi Meng;Clement Yu;Wei Wang;Naphtali Rishe
#t 1998
#c 7
#% 85086
#% 85089
#% 118762
#% 159018
#% 169052
#% 172922
#% 286916
#% 321931
#% 406493
#% 442710
#% 442943
#% 463909
#% 480788
#! When a multidatabase system contains textual database systems (i.e., information retrieval systems), queries against the global schema of the multidatabase system may contain a new type of joins驴joins between attributes of textual type. Three algorithms for processing such a type of joins are presented and their I/O costs are analyzed in this paper. Since such a type of joins often involves document collections of very large size, it is very important to find efficient algorithms to process them. The three algorithms differ on whether the documents themselves or the inverted files on the documents are used to process the join. Our analysis and the simulation results indicate that the relative performance of these algorithms depends on the input document collections, system characteristics, and the input query. For each algorithm, the type of input document collections with which the algorithm is likely to perform well is identified. An integrated algorithm that automatically selects the best algorithm to use is also proposed.

#index 443211
#* Common Subexpression Processing in Multiple-Query Processing
#@ Fa-Chung Fred Chen;Margaret H. Dunham
#t 1998
#c 7
#% 36117
#% 287667
#% 411750
#% 462025
#% 462950
#% 463759
#% 479772
#% 480268
#! The efficiency of common subexpression identification is critical to the performance of multiple-query processing. In this paper, we develop a multigraph for representing and facilitating the processing of multiple queries. In addition to the traditional multiple-query processing approaches in exploiting common subexpressions for identical and subsumption cases, the proposed multigraph processing also covers the overlap case. A performance study shows the viability of this technique when compared to an earlier multigraph approach.

#index 443212
#* Dependability and Performance Measures for the Database Practitioner
#@ Toby J. Teorey;Wee Teck Ng
#t 1998
#c 7
#% 121
#% 9240
#% 26737
#% 34310
#% 43026
#% 83933
#% 159224
#% 291242
#% 383830
#% 403195
#! We estimate the availability, reliability, and mean transaction time (response time) for repairable database configurations, centralized or distributed, in which each service component is continuously available for repair. Reliability, the probability that the entire transaction can execute properly without failure, is computed as a function of mean time to failure (MTTF) and mean time to repair (MTTR). Mean transaction time in the system is a function of the mean service delay time for the transaction over all components, plus restart delays due to component failures, plus queuing delays for contention. These estimates are potentially applicable to more generalized distributed systems.

#index 443213
#* TR$\Re$-String: A Geometry-Based Representation for Efficient and Effective Retrieval of Images by Spatial Similarity
#@ Venkat N. Gudivada
#t 1998
#c 7
#% 23998
#% 68781
#% 78243
#% 78251
#% 100198
#% 115462
#% 124680
#% 176344
#% 181409
#% 236048
#% 407995
#% 437405
#% 442807
#% 481283
#% 526688
#% 661683
#! A spatial similarity algorithm assesses the degree to which the spatial relationships among the domain objects in a database image conform to those specified in the query image. In this paper, we propose a geometry-based structure for representing the spatial relationships in the images and an associated spatial similarity algorithm. The proposed algorithm recognizes both translation, scale, and rotation variants of an image, and variants of the image generated by an arbitrary composition of translation, scale, and rotation transformations. The algorithm has 驴(n log n) time complexity in terms of the number of objects common to the database and query images. The retrieval effectiveness of the proposed algorithm is evaluated using the TESSA image collection.

#index 443214
#* Bottom-Up Construction of Ontologies
#@ Paul E. van der Vet;Nicolaas J. I. Mars
#t 1998
#c 7
#% 59912
#% 86507
#% 117568
#% 131394
#% 136361
#% 156337
#% 170710
#% 181343
#% 1275336
#! We present a particular way of building ontologies that proceeds in a bottom-up fashion. Concepts are defined in a way that mirrors the way their instances are composed out of smaller objects. The smaller objects themselves may also be modeled as being composed. Bottom-up ontologies are flexible through the use of implicit and, hence, parsimonious part-whole and subconcept-superconcept relations. The bottom-up method complements current practice, where, as a rule, ontologies are built top-down. The design method is illustrated by an example involving ontologies of pure substances at several levels of detail. It is not claimed that bottom-up construction is a generally valid recipe; indeed, such recipes are deemed uninformative or impossible. Rather, the approach is intended to enrich the ontology developer's toolkit.

#index 443215
#* The Knowledge Acquisition and Representation Language, KARL
#@ Dieter Fensel;Jürgen Angele;Rudi Studer
#t 1998
#c 7
#% 16268
#% 33376
#% 36342
#% 36683
#% 53388
#% 53395
#% 53706
#% 53995
#% 64421
#% 64443
#% 74070
#% 82786
#% 84990
#% 101953
#% 115390
#% 116192
#% 116203
#% 130009
#% 131239
#% 134108
#% 150888
#% 156339
#% 164934
#% 164947
#% 164952
#% 165365
#% 166559
#% 189739
#% 197428
#% 231930
#% 361458
#% 362942
#% 364852
#% 365414
#% 370526
#% 404067
#% 459115
#% 460128
#% 460288
#% 471117
#% 538269
#% 539065
#! The Knowledge Acquisition and Representation Language (KARL) combines a description of a knowledge-based system at the conceptual level (a so-called model of expertise) with a description at a formal and executable level. Thus, KARL allows the precise and unique specification of the functionality of a knowledge-based system independent of any implementation details. A KARL model of expertise contains the description of domain knowledge, inference knowledge, and procedural control knowledge. For capturing these different types of knowledge, KARL provides corresponding modeling primitives based on Frame-Logic and Dynamic Logic. A declarative semantics for a complete KARL model of expertise is given by a combination of these two types of logic. In addition, an operational definition of this semantics, which relies on a fixpoint approach, is given. This operational semantics defines the basis for the implementation of the KARL interpreter, which includes appropriate algorithms for efficiently executing KARL specifications. This enables the evaluation of KARL specifications by means of testing.

#index 443216
#* New Approach to Requirements Trade-Off Analysis for Complex Systems
#@ Jonathan Lee;Jong-Yih Kuo
#t 1998
#c 7
#% 4863
#% 28066
#% 50729
#% 65831
#% 65840
#% 68047
#% 74868
#% 88437
#% 91003
#% 139864
#% 168026
#% 169469
#% 174309
#% 212710
#% 444861
#% 446066
#% 666681
#! In this paper, we propose a faceted requirement classification scheme for analyzing heterogeneous requirements. The representation of vague requirements is based on Zadeh's canonical form in test-score semantics and an extension of the notion of soft conditions. The trade-off among vague requirements is analyzed by identifying the relationship between requirements, which could be either conflicting, irrelevant, cooperative, counterbalance, or independent. Parameterized aggregation operators, fuzzy and/or, are selected to combine individual requirements. An extended hierarchical aggregation structure is proposed to establish a four-level requirements hierarchy to facilitate requirements and criticalities aggregation through the fuzzy and/or. A compromise overall requirement can be obtained through the aggregation of individual requirements based on the requirements hierarchy. The proposed approach provides a framework for formally analyzing and modeling conflicts between requirements, and for users to better understand relationships among their requirements.

#index 443217
#* A Framework for Learning in Search-Based Systems
#@ Sudeshna Sarkar;P. p. Chakrabarti;Sujoy Ghose
#t 1998
#c 7
#% 241
#% 11619
#% 25470
#% 42002
#% 61220
#% 124689
#% 124691
#% 130198
#% 195237
#% 442986
#% 449561
#% 1784272
#! In this paper, we provide an overall framework for learning in search-based systems that are used to find optimum solutions to problems. This framework assumes that prior knowledge is available in the form of one or more heuristic functions (or features) of the problem domain. An appropriate clustering strategy is used to partition the state space into a number of classes based on the available features. The number of classes formed will depend on the resource constraints of the system. In the training phase, example problems are run using a standard admissible search algorithm. In this phase, heuristic information corresponding to each class is learned. This new information can be used in the problem-solving phase by appropriate search algorithms so that subsequent problem instances can be solved more efficiently. In this framework, we also show that heuristic information of forms other than the conventional single-valued underestimate value can be used, since we maintain the heuristic of each class explicitly. We show some novel search algorithms that can work with some such forms. Experimental results have been provided for some domains.

#index 443218
#* Consistency Checking in Complex Object Database Schemata with Integrity Constraints
#@ Domenico Beneventano;Sonia Bergamaschi;Stefano Lodi;Claudio Sartori
#t 1998
#c 7
#% 413
#% 16765
#% 27043
#% 47712
#% 58347
#% 58356
#% 64441
#% 99441
#% 101435
#% 114579
#% 116091
#% 116299
#% 117899
#% 125595
#% 164387
#% 189739
#% 319822
#% 364433
#% 380546
#% 395684
#% 442879
#% 452778
#% 458576
#% 462337
#% 462510
#% 464543
#% 560057
#! Integrity constraints are rules that should guarantee the integrity of a database. Provided an adequate mechanism to express them is available, the following question arises: Is there any way to populate a database which satisfies the constraints supplied by a database designer? That is, does the database schema, including constraints, admit at least a nonempty model? This work answers the above question in a complex object database environment, providing a theoretical framework, including the following ingredients: 1) two alternative formalisms, able to express a relevant set of state integrity constraints with a declarative style; 2) two specialized reasoners, based on the tableaux calculus, able to check the consistency of complex objects database schemata expressed with the two formalisms. The proposed formalisms share a common kernel, which supports complex objects and object identifiers, and which allow the expression of acyclic descriptions of: classes, nested relations and views, built up by means of the recursive use of record, quantified set, and object type constructors and by the intersection, union, and complement operators. Furthermore, the kernel formalism allows the declarative formulation of typing constraints and integrity rules. In order to improve the expressiveness and maintain the decidability of the reasoning activities, we extend the kernel formalism into two alternative directions. The first formalism, ${\cal OLCP,}$ introduces the capability of expressing path relations. Because cyclic schemas are extremely useful, we introduce a second formalism, ${\cal OLCD,}$ with the capability of expressing cyclic descriptions but disallowing the expression of path relations. In fact, we show that the reasoning activity in ${\cal OLCDP}$ (i.e., ${\cal OLCP}$ with cycles) is undecidable.

#index 443219
#* Efficient Differential Timeslice Computation
#@ Kristian Torp;Leo Mark;Christian S. Jensen
#t 1998
#c 7
#% 13015
#% 13016
#% 40633
#% 59350
#% 83330
#% 86953
#% 88056
#% 98469
#% 108499
#% 111284
#% 140389
#% 163440
#% 286256
#% 442781
#% 442806
#% 442967
#% 452782
#% 480096
#% 546596
#% 674378
#! Transaction-time databases support access to not only the current database state, but also previous database states. Supporting access to previous database states requires large quantities of data and necessitates efficient temporal query processing techniques. In previous work, we have presented a log-based storage structure and algorithms for the differential computation of previous database states. Timeslices驴i.e., previous database states驴are computed by traversing a log of database changes, using previously computed and cached timeslices as outsets. When computing a new timeslice, the cache will contain two candidate outsets: an earlier outset and a later outset. The new timeslice can be computed by either incrementally updating the earlier outset or decrementally "downdating" the later outset using the log. The cost of this computation is determined by the size of the log between the outset and the new timeslice. This paper proposes an efficient algorithm that identifies the cheaper outset for the differential computation. The basic idea is to compute the sizes of the two pieces of the log by maintaining and using a tree structure on the timestamps of the database changes in the log. The lack of a homogeneous node structure, a controllable and high fill-factor for nodes, and of appropriate node allocation in existing tree structures (e.g., B+-trees, Monotonic B+-trees, and Append-only trees) render existing tree structures unsuited for our use. Consequently, a specialized tree structure, the Pointer-less Insertion tree, is developed to support the algorithm. As a proof of concept, we have implemented a main memory version of the algorithm and its tree structure.

#index 443220
#* Temporal Synchronization Models for Multimedia Data
#@ Elisa Bertino;Elena Ferrari
#t 1998
#c 7
#% 2178
#% 6806
#% 13040
#% 15421
#% 36166
#% 58373
#% 70370
#% 72133
#% 82720
#% 98072
#% 104740
#% 111584
#% 111874
#% 135556
#% 151382
#% 154311
#% 172885
#% 210388
#% 219919
#% 317871
#% 319244
#% 434736
#% 445701
#% 452790
#% 452796
#% 463567
#% 463902
#% 478962
#% 480799
#% 503562
#% 533357
#% 541256
#% 557756
#% 568074
#% 1848271
#! Multimedia information systems are considerably more complex than traditional ones in that they deal with very heterogeneous data such as text, video, and audio驴characterized by different characteristics and requirements. One of the central characteristics of multimedia data is that of being heavily time-dependent in that they are usually related by temporal relationships that must be maintained during playout. In this paper, we discuss problems related to modeling temporal synchronization specifications for multimedia data. We investigate the characteristics that a model must possess to properly express the timing relationships among multimedia data, and we provide a classification for the various models proposed in the literature. For each devised category, several examples are presented, whereas the most representative models of each category are illustrated in detail. Then, the presented models are compared with respect to the devised requirements, and future research issues are discussed.

#index 443221
#* Declustering and Load-Balancing Methods for Parallelizing Geographic Information Systems
#@ Shashi Shekhar;Sivakumar Ravada;Vipin Kumar;Douglas Chubb;Greg Turner
#t 1998
#c 7
#% 2244
#% 7509
#% 63666
#% 86951
#% 112180
#% 115672
#% 116064
#% 120638
#% 140385
#% 145416
#% 286962
#% 319247
#% 408396
#% 427199
#% 436033
#% 437369
#% 463598
#% 479936
#% 481277
#% 527023
#% 565461
#! Declustering and load-balancing are important issues in designing a high-performance geographic information system (HPGIS), which is a central component of many interactive applications(such as real-time terrain visualization. The current literature provides efficient methods for declustering spatial point-data. However, there has been little work toward developing efficient declustering methods for collections of extended objects, like chains of line-segments and polygons. In this paper, we focus on the data-partitioning approach to parallelizing GIS operations. We provide a framework for declustering collections of extended spatial objects by identifying the following key issues: 1) the work-load metric, 2) the spatial-extent of the work-load, 3) the distribution of the work-load over the spatial-extent, and 4) the declustering method. We identify and experimentally evaluate alternatives for each of these issues. In addition, we also provide a framework for dynamically balancing the load between different processors. We experimentally evaluate the proposed declustering and load-balancing methods on a distributed memory MIMD machine (Cray T3D). Experimental results show that the spatial-extent and the work-load metric are important issues in developing a declustering method. Experiments also show that the replication of data is usually needed to facilitate dynamic load-balancing, since the cost of local processing is often less than the cost of data transfer for extended spatial objects. In addition, we also show that the effectiveness of dynamic load-balancing techniques can be improved by using declustering methods to determine the subsets of spatial objects to be transferred during runtime.

#index 443222
#* Navigational Accesses in a Temporal Object Model
#@ Elisa Bertino;Elena Ferrari;Giovanna Guerrini
#t 1998
#c 7
#% 3523
#% 168773
#% 234903
#% 286831
#% 361445
#% 395735
#% 442808
#% 458542
#% 618566
#! A considerable research effort has been devoted in past years to query languages for temporal data in the context of both the relational and the object-oriented model. Object-oriented databases provide a navigational approach for data access based on object references. In this paper, we investigate the navigational approach to querying object-oriented databases. We formally define the notion of temporal path expression, and we address on a formal basis issues related to the correctness of such expressions. In particular, we focus on static analysis and give a set of conditions ensuring that an expression always results in a correct access at runtime.

#index 443223
#* Knowledge Representation Using Fuzzy Petri Nets-Revisited
#@ T. v. Manoj;John Leena;Rajan B. Soney
#t 1998
#c 7
#% 442719
#! In the paper entitled "Knowledge Representation Using Fuzzy Petri Nets" [1], Chen, Ke, and Chang proposed an algorithm which determines whether there exists an antecedent-consequence relationship from a fuzzy proposition ds to proposition dj and if the degree of truth of proposition ds is given, then the degree of truth of proposition dj can be evaluated. The fuzzy reasoning algorithm proposed in [1] was found not to be working with all types of data. Here we propose 1) a modified form of the algorithm, and 2) concept of hierarchical Fuzzy Petri Nets for data abstraction.

#index 443224
#* Generating Broadcast Programs that Support Range Queries
#@ Kian-Lee Tan;Jeffrey Xu Yu
#t 1998
#c 7
#% 201897
#% 631861
#! To disseminate information via broadcasting, a data server must construct a broadcast "program" that meets the needs of the client population. Existing works on generating broadcast programs have shown the effectiveness of nonuniform broadcast programs in reducing the average access times of objects for nonuniform access patterns. However, these broadcast programs perform poorly for range queries. This correspondence presents a new algorithm to generate broadcast programs that facilitate range queries without sacrificing much on the performance of single object retrievals.

#index 443225
#* Addendum to "Current Approaches to Handling Imperfect Information in Data and Knowledge Bases"
#@ Simon Parsons
#t 1998
#c 7
#% 443038

#index 443226
#* Addendum to "On Satisfiability, Equivalence, and Implication Problems Involving Conjunctive Queries in Database Systems"
#@ Sha Guo;Wei Sun;Mark A. Weiss
#t 1998
#c 7
#% 443061

#index 443227
#* The Distributed Constraint Satisfaction Problem: Formalization and Algorithms
#@ Makoto Yokoo;Edmund H. Durfee;Toru Ishida;Kazuhiro Kuwabara
#t 1998
#c 7
#% 82819
#% 126390
#% 160245
#% 193442
#% 287470
#% 534174
#! In this paper, we develop a formalism called a distributed constraint satisfaction problem (distributed CSP) and algorithms for solving distributed CSPs. A distributed CSP is a constraint satisfaction problem in which variables and constraints are distributed among multiple agents. Various application problems in Distributed Artificial Intelligence can be formalized as distributed CSPs. We present our newly developed technique called asynchronous backtracking that allows agents to act asynchronously and concurrently without any global control, while guaranteeing the completeness of the algorithm. Furthermore, we describe how the asynchronous backtracking algorithm can be modified into a more efficient algorithm called an asynchronous weak-commitment search, which can revise a bad decision without exhaustive search by changing the priority order of agents dynamically. The experimental results on various example problems show that the asynchronous weak-commitment search algorithm is, by far more, efficient than the asynchronous backtracking algorithm and can solve fairly large-scale problems.

#index 443228
#* An Extended Algebra for Constraint Databases
#@ Alberto Belussi;Elisa Bertino;Barbara Catania
#t 1998
#c 7
#% 59305
#% 68194
#% 78365
#% 89407
#% 101956
#% 159506
#% 164406
#% 190332
#% 191590
#% 191592
#% 201872
#% 210349
#% 210388
#% 213956
#% 268787
#% 277323
#% 384978
#% 427199
#% 435148
#% 463571
#% 464552
#% 464697
#% 464702
#% 476994
#% 480093
#% 481113
#% 526841
#% 527014
#% 527164
#% 542443
#% 542444
#% 562132
#! Constraint relational databases use constraints to both model and query data. A constraint relation contains a finite set of generalized tuples. Each generalized tuple is represented by a conjunction of constraints on a given logical theory and, depending on the logical theory and the specific conjunction of constraints, it may possibly represent an infinite set of relational tuples. For their characteristics, constraint databases are well suited to model multidimensional and structured data, like spatial and temporal data. The definition of an algebra for constraint relational databases is important in order to make constraint databases a practical technology. In this paper, we extend the previously defined constraint algebra (called generalized relational algebra). First, we show that the relational model is not the only possible semantic reference model for constraint relational databases and we show how constraint relations can be interpreted under the nested relational model. Then, we introduce two distinct classes of constraint algebras, one based on the relational algebra, and one based on the nested relational algebra, and we present an algebra of the latter type. The algebra is proved equivalent to the generalized relational algebra when input relations are modified by introducing generalized tuple identifiers. However, from a user point of view, it is more suitable. Thus, the difference existing between such algebras is similar to the difference existing between the relational algebra and the nested relational algebra, dealing with only one level of nesting. We also show how external functions can be added to the proposed algebra.

#index 443229
#* Goal-Directed Reasoning with ACE-SSM
#@ Michel Benaroch
#t 1998
#c 7
#% 55936
#% 75896
#% 110011
#% 115331
#% 116332
#% 134111
#% 150838
#% 150854
#% 156337
#% 191678
#% 211585
#% 408024
#% 444990
#% 444996
#% 459138
#! The goal of knowledge-based systems (KBSs) is not only to produce a solution to a problem that these systems face but also to construct驴implicitly or explicitly驴a situation-specific model (SSM) that explicates the rationale behind that solution. This paper focuses on how KBSs can benefit from the availability of explicit goal knowledge that reflects the underlying structure (ontology) of SSMs constructed for an application task. It first shows how goal knowledge can be captured. Then, it explains how ACE-SSM驴an architecture for constructing explicit SSMs驴uses this knowledge to direct the construction of explicit SSMs. Finally, it discusses benefits that KBSs can derive from the availability of explicit SSMs and their underlying goal knowledge. Some of these benefits pertain to ways to simplify the construction and maintenance of KBSs through reuse, while others relate to ways to endow KBSs with more robust problem-solving and explanation capabilities. These benefits are illustrated using concrete examples.

#index 443230
#* Efficient Recursive Aggregation and Negation in Deductive Databases
#@ David B. Kemp;Kotagiri Ramamohanarao
#t 1998
#c 7
#% 5965
#% 11797
#% 23902
#% 29253
#% 33376
#% 36683
#% 53388
#% 73005
#% 83144
#% 100601
#% 103705
#% 123056
#% 123068
#% 172951
#% 196695
#% 205234
#% 268765
#% 277343
#% 368248
#% 435130
#% 435132
#% 459254
#% 480602
#! We present an efficient evaluation technique for modularly stratified deductive database programs for which the local strata level mappings are known at compile time. We present an important subclass of these programs (called EMS-programs) in which one can easily express problems, such as shortest distance, company ownership, bill of materials, and preferential vote counting. Programs written in this style have an easy-to-understand semantics and can be efficiently computed. Another important virtue of these programs is that their modular-stratification properties are independent of the extensional database.

#index 443231
#* A Unified Data Model for Representing Multimedia, Timeline, and Simulation Data
#@ John David N. Dionisio;Alfonso F. Cárdenas
#t 1998
#c 7
#% 57478
#% 68281
#% 91028
#% 117900
#% 125958
#% 135556
#% 152010
#% 152011
#% 154336
#% 157706
#% 175279
#% 184249
#% 395217
#% 395778
#% 442808
#% 442902
#% 442974
#% 452796
#% 452797
#% 480625
#% 480799
#% 481102
#% 703549
#! This paper describes a unified data model that represents multimedia, timeline, and simulation data utilizing a single set of related data modeling constructs. A uniform model for multimedia types structures image, sound, video, and long text data in a consistent way, giving multimedia schemas and queries a degree of data independence even for these complex data types. Information that possesses an intrinsic temporal element can all be represented using a construct called a stream. Streams can be aggregated into parallel multistreams, thus providing a structure for viewing multiple sets of time-based information. The unified stream construct permits real-time measurements, numerical simulation data, and visualizations of that data to be aggregated and manipulated using the same set of operators. Prototypes based on the model have been implemented for two medical application domains: thoracic oncology and thermal ablation therapy of brain tumors. Sample schemas, queries, and screenshots from these domains are provided. Finally, a set of examples is included for an accompanying visual query language discussed in detail in another document.

#index 443232
#* Incremental Maintenance of Materialized Object-Oriented Views in MultiView: Strategies and Performance Evaluation
#@ Harumi A. Kuno;Elke A. Rundensteiner
#t 1998
#c 7
#% 13016
#% 32914
#% 59350
#% 65540
#% 71558
#% 102780
#% 110152
#% 152928
#% 152994
#% 153301
#% 164364
#% 172327
#% 187751
#% 196474
#% 198467
#% 200966
#% 201928
#% 201929
#% 221756
#% 252372
#% 346843
#% 443145
#% 458608
#% 462199
#% 462624
#% 463887
#% 464234
#% 464542
#% 480623
#% 480958
#% 534878
#% 563577
#% 571216
#% 614575
#% 614594
#% 680607
#% 703002
#! View materialization is a promising technique for achieving the data sharing and virtual restructuring capabilities needed by advanced applications such as data warehousing and workflow management systems. Much existing work addresses the problem of how to maintain the consistency of materialized relational views under update operations. However, little progress has been made thus far regarding the topic of view materialization in object-oriented databases (OODBs). In this paper, we demonstrate that there are several significant differences between the relational and object-oriented paradigms that can be exploited when addressing the object-oriented view materialization problem. First, we propose techniques that prune update propagation by exploiting knowledge of the subsumption relationships between classes to identify branches of classes to which we do not need to propagate updates and by using derivation ordering to eliminate self-canceling propagation. Second, we use encapsulated interfaces, combined with the fact that any unique database property is inherited from a single location, to provide a "registration service" by which virtual classes can register their interest in specific properties and be notified upon modification of those properties. Third, we introduce the notion of hierarchical registrations that further optimizes update propagation by organizing the registration structures according to the class generalization hierarchy, thereby pruning the set of classes that are notified of updates. We have successfully implemented all proposed techniques in the MultiView system on top of the GemStone OODBMS. To the best of our knowledge, MultiView is the first OODB view system to provide updatable materialized virtual classes and virtual schemata. In this paper, we also present a cost model for our update algorithms, and we report results from the experimental studies we have run on the MultiView system, measuring the impact of various optimization strategies incorporated into our materialization update algorithms.

#index 443233
#* Algebraic Identities and Query Optimization in a Parametric Model for Relational Temporal Databases
#@ Shashi K. Gadia;Sunil S. Nair
#t 1998
#c 7
#% 11804
#% 18615
#% 18772
#% 43028
#% 43191
#% 49596
#% 64150
#% 68201
#% 83105
#% 93076
#% 111284
#% 135384
#% 201927
#% 291858
#% 321468
#% 368248
#% 408602
#% 427199
#% 452751
#% 452782
#% 462480
#% 464032
#% 466811
#% 480934
#% 564241
#! This paper presents algebraic identities and algebraic query optimization for a parametric model for temporal databases. The parametric model has several features not present in the classical model. In this model, a key is explicitly designated with a relation, and an operator is available to change the key. The algebra for the parametric model is three-sorted; it includes 1) relational expressions that evaluate to relations, 2) domain expressions that evaluate to time domains, and 3) Boolean expressions that evaluate to TRUE or FALSE. The identities in the parametric model are classified as weak identities and strong identities. Weak identities in this model are largely counterparts of the identities in classical relational databases. Rather than establishing weak identities from scratch, a meta inference mechanism, introduced in the paper, allows weak identities to be induced from their respective classical counterpart. On the other hand, the strong identities will be established from scratch. An algorithm is presented for algebraic optimization to transform a query to an equivalent query that will execute more efficiently.

#index 443234
#* Adaptive Prefetching and Storage Reorganization In A Log-Structured Storage System
#@ Chye Lin Chee;Hongjun Lu;Hong Tang;C. v. Ramamoorthy
#t 1998
#c 7
#% 376
#% 1748
#% 43172
#% 69239
#% 86944
#% 125937
#% 131555
#% 152904
#% 152905
#% 157234
#% 159080
#% 159171
#% 160331
#% 210173
#% 232521
#% 287377
#% 319549
#% 442703
#% 442704
#% 442705
#% 452752
#% 463264
#% 463722
#% 702700
#! We present a storage management system that has the ability to adapt to the data access characteristics of the application that uses it based on collection and analysis of runtime statistics. This feature is especially useful in the storage management layer of database systems, where applications exhibit relatively predictable access patterns. Adaptive reorganization is performed by the storage management system in a manner that optimizes the access patterns of the system for which it is used. We enhance the log-structured storage system that naturally caters for write optimization, with the addition of a statistics collection mechanism to determine data access patterns of applications. The storage system can serve as a testbed for a variety of statistics analysis and clustering mechanisms. Higher level application-specific data clustering mechanisms can be used to override the storage system's low-level clustering mechanisms. In addition, the analysis techniques and reorganization scheme can be used in other storage systems. Performance results from our prototype show potential response time speedups of up to 83 percent over the basic log-structured file system in the best case, using a combination of storage reorganization and prefetching.

#index 443235
#* Scaling Access to Heterogeneous Data Sources with DISCO
#@ Anthony Tomasic;Louiqa Raschid;Patrick Valduriez
#t 1998
#c 7
#% 22948
#% 102748
#% 111912
#% 111913
#% 116303
#% 136740
#% 142228
#% 158200
#% 168676
#% 188853
#% 194984
#% 198465
#% 201976
#% 210176
#% 210178
#% 227994
#% 233702
#% 235914
#% 252374
#% 435149
#% 452838
#% 463909
#% 479449
#% 479452
#% 480612
#% 480788
#% 481923
#% 481935
#% 481939
#% 481944
#% 482116
#% 565454
#% 565460
#% 565470
#% 614590
#% 631866
#% 631868
#! Accessing many data sources aggravates problems for users of heterogeneous distributed databases. Database administrators must deal with fragile mediators, that is, mediators with schemas and views that must be significantly changed to incorporate a new data source. When implementing translators of queries from mediators to data sources, database implementors must deal with data sources that do not support all the functionality required by mediators. Application programmers must deal with graceless failures for unavailable data sources. Queries simply return failure and no further information when data sources are unavailable for query processing. The Distributed Information Search COmponent (DISCO) addresses these problems. Data modeling techniques manage the connections to data sources, and sources can be added transparently to the users and applications. The interface between mediators and data sources flexibly handles different query languages and different data source functionality. Query rewriting and optimization techniques rewrite queries so they are efficiently evaluated by sources. Query processing and evaluation semantics are developed to process queries over unavailable data sources. In this article, we describe 1) the distributed mediator architecture of DISCO; 2) the data model and its modeling of data source connections; 3) the interface to underlying data sources and the query rewriting process; and 4) query processing semantics. We describe several advantages of our system.

#index 443236
#* Database Migration: A New Architecture for Transaction Processing in Broadband Networks
#@ Takahiro Hara;Kaname Harumoto;Masahiko Tsukamoto;Shojiro Nishio
#t 1998
#c 7
#% 1486
#% 32884
#% 83334
#% 152946
#% 152947
#% 172916
#% 245113
#% 296935
#% 427195
#% 463728
#% 464832
#! Due to recent developments in network technologies, broader channel bandwidth is becoming prevalent in worldwide networks. As one of the new technologies making good use of such broadband channels, dynamic relocation of databases through networks, which we call database migration, will soon be used in practice as a powerful and basic database operation. We propose two transaction processing methods to take advantage of database migration in broadband networks. These methods choose the most efficient transaction processing method between the conventional method, based on the two-phase commit protocol, and our method, using database migration. We also propose a concurrency control mechanism and a recovery mechanism for our proposed methods. Simulation results are presented comparing the performance of our proposed methods and the conventional transaction processing method based on the two-phase commit protocol. The results demonstrate that the effective use of database migration produces better performance than the conventional method.

#index 443237
#* Database Reorganization in Parallel Disk Arrays with I/O Service Stealing
#@ Peter Zabback;Ibrahim Onyuksel;Peter Scheuermann;Gerhard Weikum
#t 1998
#c 7
#% 102807
#% 287656
#% 322531
#% 442860
#% 460873
#% 480939
#! We present a model for data reorganization in parallel disk systems that is geared toward load balancing in an environment with periodic access patterns. Data reorganization is performed by disk cooling, i.e., migrating files or extents from the hottest disks to the coldest ones. We develop an approximate queueing model for determining the effective arrival rates of cooling requests and discuss its use in assessing the costs versus benefits of cooling actions.

#index 443238
#* Effects of Update Techniques on Main Memory Database System Performance
#@ Le Gruenwald;YuWei Chen;Jing Huang
#t 1998
#c 7
#% 667
#% 9241
#% 12638
#% 102806
#% 442837
#! Update technique is an important issue related to database recovery. In a main memory database environment, transaction execution can be processed without any I/O, and all I/O operations involved are for recovery purposes. The efficiency of update techniques therefore has an important impact on the performance of main memory database systems. In this paper, we compared the techniques of immediate and deferred update based on a database machine, MARS. The simulation results showed that immediate update outperforms deferred update unless system failure is a frequent occurrence.

#index 443239
#* Introducing New Advisory Board and Editorial Board Members
#@ Farokh B. Bastani
#t 1998
#c 7

#index 443240
#* Data and Knowledge Management in Multimedia Systems
#@ P. Bruce Berra;Arif Ghafoor
#t 1998
#c 7
#! First Page of the Article

#index 443241
#* Knowledge-Based Image Retrieval with Spatial and Temporal Constructs
#@ Wesley W. Chu;Chih-Cheng Hsu;Alfonso F. Cárdenas;Ricky K. Taira
#t 1998
#c 7
#% 59542
#% 90846
#% 101955
#% 154991
#% 184249
#% 245519
#% 319244
#% 435139
#% 442800
#% 442902
#% 442974
#% 443053
#% 443231
#% 445703
#% 445709
#% 452790
#% 452795
#% 452797
#% 452804
#% 463902
#% 464041
#% 480786
#! A knowledge-based approach to retrieve medical images by feature and content with spatial and temporal constructs is developed. Selected objects of interest in a medical image (e.g., x-ray, MR image) are segmented, and contours are generated from these objects. Features (e.g., shape, size, texture) and content (e.g., spatial relationships among objects) are extracted and stored in a feature and content database. Knowledge about image features can be expressed as a hierarchical structure called a Type Abstraction Hierarchy (TAH). The high-level nodes in the TAH represent more general concepts than low-level nodes. Thus, traversing along TAH nodes allows approximate matching by feature and content if an exact match is not available. TAHs can be generated automatically by clustering algorithms based on feature values in the databases and hence are scalable to large collections of image features. Further, since TAHs are generated based on user classes and applications, they are context- and user-sensitive. A knowledge-based semantic image model is proposed that consists of four layers (raw data layer, feature and content layer, schema layer, and knowledge layer) to represent the various aspects of an image objects' characteristics. The model provides a mechanism for accessing and processing spatial, evolutionary, and temporal queries. A knowledge-based spatial temporal query language (KSTL) has developed that extends ODMG's OQL and supports approximate matching of feature and content, conceptual terms, and temporal logic predicates. Further, a visual query language has been developed that accepts point-click-and-drag visual iconic input on the screen that is then translated into KSTL. User models are introduced to provide default parameter values for specifying query conditions. We have implemented a Knowledge-Based Medical Database System (KMeD) at UCLA, and it is currently under evaluation by the medical staff. The results from this research should be applicable to other multimedia information systems as well.

#index 443242
#* Fast and Effective Retrieval of Medical Tumor Shapes
#@ Philip (Flip) Korn;Nicholas Sidiropoulos;Christos Faloutsos;Eliot Siegel;Zenon Protopapas
#t 1998
#c 7
#% 13041
#% 36287
#% 58638
#% 64431
#% 86950
#% 86951
#% 88056
#% 102772
#% 103936
#% 131055
#% 169940
#% 172949
#% 196977
#% 201876
#% 227857
#% 227937
#% 245787
#% 248797
#% 319508
#% 321455
#% 359751
#% 377548
#% 411694
#% 427199
#% 437405
#% 443698
#% 452795
#% 460862
#% 463414
#% 479462
#% 480093
#% 481460
#% 481947
#% 482109
#% 534183
#% 565447
#% 571100
#! We investigate the problem of retrieving similar shapes from a large database; in particular, we focus on medical tumor shapes ("Find tumors that are similar to a given pattern."). We use a natural similarity function for shape-matching, based on concepts from mathematical morphology, and we show how it can be lower-bounded by a set of shape features for safely pruning candidates, thus giving fast and correct output. These features can be organized in a spatial access method, leading to fast indexing for range queries and nearest-neighbor queries. In addition to the lower-bounding, our second contribution is the design of a fast algorithm for nearest-neighbor search, achieving significant speedup while provably guaranteeing correctness. Our experiments demonstrate that roughly 90 percent of the candidates can be pruned using these techniques, resulting in up to 27 times better performance compared to sequential scan.

#index 443243
#* Supporting Ranked Boolean Similarity Queries in MARS
#@ Michael Ortega;Yong Rui;Kaushik Chakrabarti;Kriengkrai Porkaew;Sharad Mehrotra;Thomas S. Huang
#t 1998
#c 7
#% 73520
#% 86929
#% 86950
#% 120270
#% 136740
#% 150417
#% 172949
#% 210172
#% 213981
#% 228351
#% 238757
#% 319273
#% 375017
#% 406493
#% 411694
#% 427199
#% 435141
#% 437405
#% 443729
#% 464195
#% 464726
#% 479462
#% 480093
#% 481279
#% 481770
#% 589735
#% 626324
#% 968853
#! To address the emerging needs of applications that require access to and retrieval of multimedia objects, we are developing the Multimedia Analysis and Retrieval System (MARS) [29]. In this paper, we concentrate on the retrieval subsystem of MARS and its support for content-based queries over image databases. Content-based retrieval techniques have been extensively studied for textual documents in the area of automatic information retrieval [40], [4]. This paper describes how these techniques can be adapted for ranked retrieval over image databases. Specifically, we discuss the ranking and retrieval algorithms developed in MARS based on the Boolean retrieval model and describe the results of our experiments that demonstrate the effectiveness of the developed model for image retrieval.

#index 443244
#* Data Resource Selection in Distributed Visual Information Systems
#@ Wendy Chang;Gholamhosein Sheikholeslami;Jia Wang;Aidong Zhang
#t 1998
#c 7
#% 176501
#% 194246
#% 194276
#% 210173
#% 219847
#% 239682
#% 241129
#% 374261
#% 422874
#% 434858
#% 437405
#% 452795
#% 479451
#% 481281
#% 481748
#% 632314
#% 632363
#! With the advances in multimedia databases and the popularization of the Internet, it is now possible to access large image and video repositories distributed throughout the world. One of the challenging problems in such an access is how the information in the respective databases can be summarized to enable an intelligent selection of relevant database sites based on visual queries. This paper presents an approach to solve this problem based on image content-based indexing of a metadatabase at a query distribution server. The metadatabase records a summary of the visual content of the images in each database through image templates and statistical features characterizing the similarity distributions of the images. The selection of the databases is done by searching the metadatabase using a ranking algorithm that uses query similarity to a template and the features of the databases associated with the template. Two selection approaches, termed mean-based and histogram-based approaches, are presented. The database selection mechanisms have been implemented in a metaserver, and extensive experiments have been performed to demonstrate the effectiveness of the database selection approaches.

#index 443245
#* WVTDB-A Semantic Content-Based Video Database System on the World Wide Web
#@ Haitao Jiang;Ahmed K. Elmagarmid
#t 1998
#c 7
#% 91028
#% 149252
#% 151355
#% 198952
#% 199855
#% 238413
#% 250250
#% 319244
#% 382491
#% 422870
#% 422871
#% 435932
#% 437405
#% 452796
#% 463902
#% 481272
#% 520122
#% 632282
#% 632356
#% 1180066
#! This paper describes the design and implementation of a web-based video database system (WVTDB) that demonstrates our research on video data modeling, semantic content-based video query, and video database system architecture. The video data model of WVTDB is based on multilevel video data abstractions and annotation layering, thus allowing dynamic and incremental video annotation and indexing, multiuser view sharing, and video data reuse. Users can query, retrieve, and browse video data based on their semantic content descriptions and temporal constraints on the video segments. WVTDB employs a modular system architecture that supports distributed video query processing and subquery caching. Several techniques, such as video wrappers and lazy delivery, are also proposed specifically to address the network bandwidth limitations for this kind of web-based system. We also address adaptivity, data access control, and user profile issues.

#index 443246
#* Automatic Composition Techniques for Video Production
#@ Gulrukh Ahanger;Thomas D. C. Little
#t 1998
#c 7
#% 194221
#% 268761
#% 406493
#% 422872
#% 422883
#% 422897
#% 434690
#% 452790
#% 464069
#% 603554
#% 632283
#! Video production involves the process of capturing, editing, and composing video segments for delivery to a consumer. A composition must yield a coherent presentation of an event or narrative. This process can be automated if appropriate domain-specific metadata are associated with video segments and composition techniques are established. Automation leads to the support of dynamic composition and customization for applications such as news on demand. In this paper, we present techniques to achieve dynamic, real-time, and cohesive video composition and customization. We also identify metrics for evaluating our techniques with respect to existing manually produced video-based news. The results of such an evaluation show that the quality of automatic composition is comparable to驴and in some cases, better than驴broadcast news video composition. The results also validate the assertions on which the automatic composition techniques are based.

#index 443247
#* Video Content Management in Consumer Devices
#@ Nevenka Dimitrova;Thomas McGee;Herman Elenbaas;Jacquelyn Martino
#t 1998
#c 7
#% 127587
#% 149252
#% 173678
#% 185265
#% 194185
#% 206633
#% 220109
#% 228351
#% 238917
#% 239697
#% 240178
#% 246235
#% 434807
#% 661684
#! Methods for video content analysis are necessary for the growing amount of video information delivered to consumers today. In this paper, we present a system for video content analysis called Vitamin, which provides management of a home video library. The system presents the user with a Visual Table of Contents that provides an overview of the video content and direct access to particular points in the stored video. In this process, we apply a computationally inexpensive, simple, yet powerful mechanism for cut detection and keyframe filtering. Our initial implementation and results show that this system can perform video content extraction in real time on a low-end platform that matches a Visual Table of Contents extracted by an expert.

#index 443248
#* A Multistep Approach for Shape Similarity Search in Image Databases
#@ Mihael Ankerst;Hans-Peter Kriegel;Thomas Seidl
#t 1998
#c 7
#% 102772
#% 117665
#% 158905
#% 169940
#% 172949
#% 201876
#% 201893
#% 227856
#% 227937
#% 237187
#% 248797
#% 421052
#% 435141
#% 443889
#% 464195
#% 479462
#% 479655
#% 481947
#% 481956
#% 482109
#% 527026
#% 527158
#! Shape similarity search is a crucial task in image databases, particularly in the presence of errors induced by segmentation or scanning images. The resulting slight displacements or rotations have not been considered so far in the literature. We present a new similarity model that flexibly addresses this problem. By specifying neighborhood influence weights, the user may adapt the similarity distance functions to her or his requirements or preferences. Technically, the new similarity model is based on quadratic forms for which we present a multistep query processing architecture particularly for high dimensions as they occur in image databases. Our algorithm to reduce the dimensionality of quadratic form-based similarity queries results in a lower-bounding distance function that is proven to provide an optimal filter selectivity. Experiments on our test database of 10,000 images demonstrate the applicability and the performance of our approach even in high dimensions such as 1,024.

#index 443249
#* Reducing the Storage Requirements of a Perfect Hash Function
#@ Paolino Di Felice;Ugo Madama
#t 1998
#c 7
#% 71567
#% 129023
#% 442859
#! The amount of memory required by perfect hash functions at retrieval time is one of the primary issues to be taken into account when looking for such functions. This correspondence gives empirical evidence about the effectiveness of a strategy suitable to significantly reduce the memory requirements of the order preserving minimal perfect hash function proposed by Czech et al. [3].

#index 443250
#* Objective vs. Subjective Measures of Error-Proneness for Rule-Based Programs
#@ Trevor T. Moores
#t 1998
#c 7
#% 192499
#% 204980
#! This paper relates four objective measures of program structure and three subjective ratings of program complexity to the number of postrelease documented errors contained within 80 commercially developed Prolog programs. All seven measures show a significant correlation with the number of errors. A factor analysis showed that the objective and subjective measures were indeed different measures, although a hierarchical analysis of oblique factors showed a strong common root. Finally, the Mann-Whitney U test was used to determine whether the measures could differentiate between those programs with errors and those with no documented errors. The results suggest that "complexity" can be grounded in terms of the difficulty to debug or test a program, while measures of "structure" require a detailed count of the number of predicates used within the program.

#index 443251
#* 1998 Index, IEEE Transactions on Knowledge and Data Engineering Vol. 10
#@  IEEE Transactions on Knowledge and Data Engineering Staff
#t 1998
#c 7

#index 443252
#* Editor-in-Chief Prefaces Special-Edition Tribute
#@ Farokh B. Bastani
#t 1999
#c 7

#index 443253
#* Professor Ramamoorthy: A Personal Introduction
#@ Raymond T. Yeh
#t 1999
#c 7

#index 443254
#* Concurrency Control in Database Systems
#@ Bharat Bhargava
#t 1999
#c 7
#% 555
#% 3645
#% 13426
#% 25378
#% 28030
#% 35072
#% 59359
#% 172880
#% 229851
#% 286836
#% 287303
#% 287352
#% 289207
#% 289224
#% 317987
#% 320187
#% 320902
#% 374001
#% 403195
#% 435118
#% 442687
#% 445667
#% 531907
#% 660942
#% 688641
#! Ideas that are used in the design, development, and performance of concurrency control mechanisms have been summarized. The locking, time-stamp, optimistic-based mechanisms are included. The ideas of validation in optimistic approach are presented in some detail. The degree of concurrency and classes of serializability for various algorithms have been presented. Questions that relate arrival rate of transactions with degree of concurrency and performance have been briefly presented. Finally, several useful ideas for increasing concurrency have been summarized. They include flexible transactions, adaptability, prewrites, multidimensional timestamps, and relaxation of two-phase locking.

#index 443255
#* The Indispensability of Dispensable Indexes
#@ Elisa Bertino;Beng Chin Ooi
#t 1999
#c 7
#% 10392
#% 18614
#% 57955
#% 58371
#% 64431
#% 86950
#% 86952
#% 102809
#% 116056
#% 163440
#% 201880
#% 211566
#% 285932
#% 287070
#% 382477
#% 427199
#% 435135
#% 442665
#% 443181
#% 461925
#% 462503
#% 462957
#% 480093
#% 481417
#% 481956
#% 526866
#! The design of new indexes has been driven by many factors, such as data types, operations, and application environment. The increasing demand for database systems to support new applications such as online analytical processing (OLAP), spatial databases, and temporal databases has continued to fuel the development of new indexes. In this paper, we summarize the major considerations in developing new indexes, paying particular attention to progress made in the design of indexes for spatial, temporal databases, and object-oriented databases (OODB). Our discussion focuses on the general concepts or features of these indexes, thus presenting the building blocks for meeting the challenges of designing new indexes for novel applications to be encountered in the future.

#index 443256
#* State of the Art in Parallel Search Techniques for Discrete Optimization Problems
#@ Ananth Grama;Vipin Kumar
#t 1999
#c 7
#% 241
#% 3533
#% 19644
#% 46623
#% 46624
#% 52784
#% 69005
#% 87338
#% 91752
#% 98216
#% 100557
#% 140385
#% 156706
#% 165298
#% 167926
#% 173327
#% 202611
#% 212097
#% 226108
#% 235670
#% 260845
#% 318025
#% 318985
#% 437790
#% 444270
#% 444442
#% 444479
#% 471791
#% 500048
#% 696720
#! Discrete optimization problems arise in a variety of domains, such as VLSI design, transportation, scheduling and management, and design optimization. Very often, these problems are solved using state space search techniques. Due to the high computational requirements and inherent parallel nature of search techniques, there has been a great deal of interest in the development of parallel search methods since the dawn of parallel computing. Significant advances have been made in the use of powerful heuristics and parallel processing to solve large-scale discrete optimization problems. Problem instances that were considered computationally intractable only a few years ago are routinely solved currently on server-class symmetric multi-processors and small workstation clusters. Parallel game-playing programs are challenging the best human minds at games like chess. In this paper, we describe the state of the art in parallel algorithms used for solving discrete optimization problems. We address heuristic and nonheuristic techniques for searching graphs as well as trees, and speedup anomalies in parallel search that are caused by the inherent speculative nature of search techniques.

#index 443257
#* Temporal Data Management
#@ Christian S. Jensen;Richard Thomas Snodgrass
#t 1999
#c 7
#% 77323
#% 123589
#% 139661
#% 198066
#% 209730
#% 225003
#% 279170
#% 287070
#% 361445
#% 366617
#% 442967
#% 443289
#! A wide range of database applications manage time-varying information. Existing database technology currently provides little support for managing such data. The research area of temporal databases has made important contributions in characterizing the semantics of such information and in providing expressive and efficient means to model, store, and query temporal data. This paper introduces the reader to temporal data management, surveys state-of-the-art solutions to challenging aspects of temporal data management, and points to research directions.

#index 443258
#* Spatial Databases-Accomplishments and Research Needs
#@ Shashi Shekhar;Sanjay Chawla;Siva Ravada;Andrew Fetterer;Xuan Liu;Chang-tien Lu
#t 1999
#c 7
#% 68091
#% 111368
#% 152902
#% 152937
#% 152940
#% 210187
#% 211931
#% 223645
#% 235402
#% 235884
#% 260056
#% 380546
#% 382974
#% 427199
#% 435137
#% 435138
#% 437611
#% 442847
#% 443105
#% 443183
#% 443208
#% 464831
#% 481428
#% 481620
#% 481759
#% 526999
#% 527029
#% 565461
#% 758494
#! Spatial databases, addressing the growing data management and analysis needs of spatial applications such as Geographic Information Systems, have been an active area of research for more than two decades. This research has produced a taxonomy of models for space, spatial data types and operators, spatial query languages and processing strategies, as well as spatial indexes and clustering techniques. However, more research is needed to improve support for network and field data, as well as query processing (e.g., cost models, bulk load). Another important need is to apply spatial data management accomplishments to newer applications, such as data warehouses and multimedia information systems. The objective of this paper is to identify recent accomplishments and associated research needs of the near term.

#index 443259
#* Techniques and Systems for Image and Video Retrieval
#@ Y. Alp Aslandogan;Clement T. Yu
#t 1999
#c 7
#% 23998
#% 68091
#% 86376
#% 91028
#% 144031
#% 152958
#% 156430
#% 169940
#% 183432
#% 192684
#% 194203
#% 201880
#% 207030
#% 219036
#% 219840
#% 219845
#% 219847
#% 220106
#% 220107
#% 227976
#% 232707
#% 241292
#% 245788
#% 247926
#% 248796
#% 405325
#% 427199
#% 434690
#% 437407
#% 437408
#% 437509
#% 443053
#% 452791
#% 461912
#% 464046
#% 527880
#% 676177
#% 968502
#! Storage and retrieval of multimedia has become a requirement for many contemporary information systems. These systems need to provide browsing, querying, navigation, and, sometimes, composition capabilities involving various forms of media. In this survey, we review techniques and systems for image and video retrieval. We first look at visual features for image retrieval such as color, texture, shape, and spatial relationships. The indexing techniques are discussed for these features. Nonvisual features include captions, annotations, relational attributes, and structural descriptions. Temporal aspects of video retrieval and video segmentation are discussed next. We review several systems for image and video retrieval including research, commercial, and World Wide Web-based systems. We conclude with an overview of current challenges and future trends for image and video retrieval.

#index 443260
#* Semantic Modeling and Knowledge Representation in Multimedia Databases
#@ Wasfi Al-Khatib;Y. Francis Day;Arif Ghafoor;P. Bruce Berra
#t 1999
#c 7
#% 23998
#% 69329
#% 73774
#% 156430
#% 166097
#% 192795
#% 197902
#% 202318
#% 207030
#% 212685
#% 319244
#% 422874
#% 422877
#% 434681
#% 434690
#% 437405
#% 437406
#% 437407
#% 437408
#% 442974
#% 443053
#% 443241
#% 443889
#% 452790
#% 452796
#% 457641
#% 527880
#% 589921
#% 592180
#% 661684
#! In this paper, we present the current state of the art in semantic data modeling of multimedia data. Semantic conceptualization can be performed at several levels of information granularity, leading to multilevel indexing and searching mechanisms. Various models at different levels of granularity are compared. At the finest level of granularity, multimedia data can be indexed based on image contents, such as identification of objects and faces. At a coarser level of granularity, indexing of multimedia data can be focused on events and episodes, which are higher level abstractions. In light of the above, we also examine modeling and indexing techniques of multimedia documents.

#index 443261
#* A Survey on Content-Based Retrieval for Multimedia Databases
#@ Atsuo Yoshitaka;Tadao Ichikawa
#t 1999
#c 7
#% 23998
#% 54463
#% 65945
#% 86521
#% 109214
#% 113930
#% 113938
#% 149277
#% 151355
#% 194192
#% 194203
#% 213959
#% 239661
#% 239697
#% 319244
#% 434681
#% 434753
#% 437405
#% 442780
#% 442827
#% 442974
#% 443053
#% 452790
#% 452795
#% 452796
#% 480625
#% 481283
#% 481600
#% 527876
#% 527886
#% 619832
#% 619906
#% 619935
#% 632265
#% 636391
#% 661684
#% 1180064
#% 1180120
#% 1180123
#% 1180138
#! Conventional database systems are designed for managing textual and numerical data, and retrieving such data is often based on simple comparisons of text/numerical values. However, this simple method of retrieval is no longer adequate for the multimedia data, since the digitized representation of images, video, or data itself does not convey the reality of these media items. In addition, composite data consisting of heterogeneous types of data also associates with the semantic content acquired by a user's recognition. Therefore, content-based retrieval for multimedia data is realized taking such intrinsic features of multimedia data into account. Implementation of the content-based retrieval facility is not based on a single fundamental, but is closely related to an underlying data model, a priori knowledge of the area of interest, and the scheme for representing queries. This paper surveys recent studies on content-based retrieval for multimedia databases from the point of view of three fundamental issues. Throughout the discussion, we assume databases that manage only nontextual/numerical data, such as image or video, are also in the category of multimedia databases.

#index 443262
#* Caching on the World Wide Web
#@ Charu Aggarwal;Joel L. Wolf;Philip S. Yu
#t 1999
#c 7
#% 43171
#% 122671
#% 149242
#% 152943
#% 176498
#% 176500
#% 194197
#% 197517
#% 209651
#% 209891
#% 223400
#% 255027
#% 255035
#% 255038
#% 395431
#% 481450
#% 642534
#! With the recent explosion in usage of the World Wide Web, the problem of caching Web objects has gained considerable importance. Caching on the Web differs from traditional caching in several ways. The nonhomogeneity of the object sizes is probably the most important such difference. In this paper, we give an overview of caching policies designed specifically for Web objects and provide a new algorithm of our own. This new algorithm can be regarded as a generalization of the standard LRU algorithm. We examine the performance of this and other Web caching algorithms via event- and trace-driven simulation.

#index 443263
#* Mobile Computing and Databases-A Survey
#@ Daniel Barbará
#t 1999
#c 7
#% 471
#% 555
#% 2027
#% 4618
#% 7580
#% 32884
#% 54037
#% 66172
#% 77005
#% 114573
#% 116082
#% 124011
#% 124743
#% 151529
#% 169835
#% 172874
#% 172876
#% 201537
#% 201897
#% 210179
#% 227885
#% 320813
#% 435134
#% 435158
#% 443127
#% 449588
#% 458535
#% 461887
#% 461902
#% 464065
#% 464214
#% 480935
#% 480965
#% 558014
#% 564239
#% 602675
#% 609905
#% 614999
#% 617260
#% 635795
#% 635819
#% 673016
#% 978507
#! The emergence of powerful portable computers, along with advances in wireless communication technologies, has made mobile computing a reality. Among the applications that are finding their way to the market of mobile computing驴those that involve data management驴hold a prominent position. In the past few years, there has been a tremendous surge of research in the area of data management in mobile computing. This research has produced interesting results in areas such as data dissemination over limited bandwidth channels, location-dependent querying of data, and advanced interfaces for mobile computers. This paper is an effort to survey these techniques and to classify this research in a few broad areas.

#index 443264
#* Warehouse Creation-A Potential Roadblock to Data Warehousing
#@ Jaideep Srivastava;Ping-Yao Chen
#t 1999
#c 7
#% 7531
#% 22948
#% 85086
#% 111913
#% 111920
#% 126307
#% 126332
#% 140389
#% 164153
#% 184209
#% 188077
#% 199537
#% 201889
#% 210182
#% 366617
#% 420053
#% 442692
#% 443069
#% 462352
#% 462644
#% 463445
#% 481604
#% 482067
#! Data warehousing is gaining in popularity as organizations realize the benefits of being able to perform sophisticated analyses of their data. Recent years have seen the introduction of a number of data-warehousing engines, from both established database vendors as well as new players. The engines themselves are relatively easy to use and come with a good set of end-user tools. However, there is one key stumbling block to the rapid development of data warehouses, namely that of warehouse population. Specifically, problems arise in populating a warehouse with existing data since it has various types of heterogeneity. Given the lack of good tools, this task has generally been performed by various system integrators, e.g., software consulting organizations which have developed in-house tools and processes for the task. The general conclusion is that the task has proven to be labor-intensive, error-prone, and generally frustrating, leading a number of warehousing projects to be abandoned mid-way through development. However, the picture is not as grim as it appears. The problems that are being encountered in warehouse creation are very similar to those encountered in data integration, and they have been studied for about two decades. However, not all problems relevant to warehouse creation have been solved, and a number of research issues remain. The principal goal of this paper is to identify the common issues in data integration and data-warehouse creation. We hope this will lead: 1) developers of warehouse creation tools to examine and, where appropriate, incorporate the techniques developed for data integration, and 2) researchers in both the data integration and the data warehousing communities to address the open research issues in this important area.

#index 443265
#* Agents in a Nutshell-A Very Brief Introduction
#@ Caroline C. Hayes
#t 1999
#c 7
#% 5201
#% 18600
#% 43144
#% 152778
#% 159110
#% 159112
#% 159113
#% 174161
#% 212659
#% 241037
#% 241284
#% 373996
#% 445079
#% 634291
#% 1275351
#% 1290176
#% 1776338
#! Agent-based approaches are becoming increasingly important because of their generality, flexibility, modularity, and ability to take advantage of distributed resources. Agents are used in information retrieval, entertainment, coordinating multiple robots, and modeling economic systems. Agents can recommend music, tell stories, and interact with people. They are useful for reducing humans' work and information load in tasks such as medical monitoring and battlefield reasoning. Agents are already changing the way in which we gather information, manage investments, and conduct business. This article provides an introduction to agent issues, outlines motivations for using agent-based paradigms, and describes some of their current uses.

#index 443266
#* Cooperative Multiagent Systems: A Personal View of the State of the Art
#@ Victor R. Lesser
#t 1999
#c 7
#% 34899
#% 68239
#% 68349
#% 82812
#% 97617
#% 97618
#% 158039
#% 160218
#% 162305
#% 168033
#% 174569
#% 189698
#% 215475
#% 215483
#% 215532
#% 233135
#% 241037
#% 241195
#% 366647
#% 431493
#% 443113
#% 602827
#% 636349
#% 1478733
#! Scientific research and practice in multiagent systems focuses on constructing computational frameworks, principles, and models for how both small and large societies of intelligent, semiautonomous agents can interact effectively to achieve their goals. This article provides a personal view of the key application areas for cooperative multiagent systems, the major intellectual problems in building such systems, the underlying principles governing their design, and the major directions and challenges for future developments in this field.

#index 443267
#* Nonmonotonic Logic Programming
#@ V. s. Subrahmanian
#t 1999
#c 7
#% 1146
#% 33376
#% 41760
#% 53385
#% 53388
#% 60500
#% 64407
#% 66101
#% 77167
#% 103705
#% 112797
#% 113812
#% 114723
#% 121630
#% 123108
#% 132224
#% 137789
#% 137875
#% 139922
#% 147485
#% 147504
#% 147511
#% 164405
#% 176464
#% 181220
#% 189738
#% 189980
#% 190636
#% 200279
#% 205234
#% 206990
#% 217662
#% 227956
#% 268779
#% 381636
#% 383293
#% 408478
#% 442955
#% 443072
#% 443075
#% 499497
#% 499499
#% 499500
#% 499506
#% 499509
#% 499512
#% 499514
#% 499638
#% 499651
#% 499653
#% 499656
#% 501037
#% 556918
#! This paper provides a survey of the state of the art in nonmonotonic logic programming. In particular, we survey advances in the declarative semantics of logic programs, in query processing procedures for nonmonotonic logic programs, and in recent extensions of the nonmonotonic logic programming paradigm.

#index 443268
#* Fuzzy Logic-A Modern Perspective
#@ John Yen
#t 1999
#c 7
#% 46013
#% 91879
#% 115251
#% 157721
#% 182919
#% 183595
#% 207540
#% 217576
#% 262261
#% 379747
#% 444964
#% 496430
#% 1780566
#% 1780826
#% 1787856
#% 1787977
#% 1787992
#! Traditionally, fuzzy logic (FL) has been viewed in the artificial intelligence (AI) community as an approach for managing uncertainty. In the 1990s, however, fuzzy logic has emerged as a paradigm for approximating a functional mapping. This complementary modern view about the technology offers new insights about the foundation of fuzzy logic, as well as new challenges regarding the identification of fuzzy models. In this paper, we will first review some of the major milestones in the history of developing fuzzy logic technology. After a short summary of major concepts in fuzzy logic, we discuss a modern view about the foundation of two types of fuzzy rules. Finally, we review some of the research in addressing various challenges regarding automated identification of fuzzy rule-based models.

#index 443269
#* Rule-Induction and Case-Based Reasoning: Hybrid Architectures Appear Advantageous
#@ Nick Cercone;Aijun An;Christine Chan
#t 1999
#c 7
#% 99396
#% 115092
#% 120634
#% 136350
#% 176887
#% 184490
#% 198076
#% 533968
#% 706993
#% 1290056
#! Researchers have embraced a variety of machine learning (ML) techniques in their efforts to improve the quality of learning programs. The recent evolution of hybrid architectures for machine learning systems has resulted in several approaches that combine rule-induction methods with case-based reasoning techniques to engender performance improvements over more-traditional one-representation architectures. We briefly survey several major rule-induction and case-based reasoning ML systems. We then examine some interesting hybrid combinations of these systems, and explain their strengths and weaknesses as learning systems. We present a balanced approach to constructing a hybrid architecture, along with arguments in favor of this balance and mechanisms for achieving a proper balance. Finally, we present some initial empirical results from testing our ideas and draw some conclusions based on those results.

#index 443270
#* Generalization and Generalizability Measures
#@ Benjamin W. Wah
#t 1999
#c 7
#% 697
#% 8933
#% 65444
#% 76929
#% 78886
#% 81860
#% 92135
#% 101907
#% 114994
#% 442828
#% 442986
#% 449576
#% 449587
#% 451031
#% 465704
#% 690846
#% 1860065
#! In this paper, we define the generalization problem, summarize various approaches in generalization, identify the credit assignment problem, and present the problem and some solutions in measuring generalizability. We discuss anomalies in the ordering of hypotheses in a subdomain when performance is normalized and averaged, and show conditions under which anomalies can be eliminated. To generalize performance across subdomains, we present a measure called probability of win that measures the probability whether one hypothesis is better than another. Finally, we discuss some limitations in using probabilities of win and illustrate their application in finding new parameter values for TimberWolf, a package for VLSI cell placement and routing.

#index 443271
#* Knowledge-Based Software Architectures: Acquisition, Specification, and Verification
#@ Jeffrey J. P. Tsai;Alan Liu;Eric Juan;Avinash Sahay
#t 1999
#c 7
#% 727
#% 1791
#% 3873
#% 19625
#% 35775
#% 71315
#% 84047
#% 88434
#% 90639
#% 99204
#% 101958
#% 112334
#% 114571
#% 115727
#% 115728
#% 115729
#% 115730
#% 115731
#% 126289
#% 132087
#% 132224
#% 139864
#% 158184
#% 160339
#% 166220
#% 176253
#% 181407
#% 184451
#% 185248
#% 208204
#% 208410
#% 211148
#% 212998
#% 213736
#% 215280
#% 215282
#% 218686
#% 221112
#% 231921
#% 264465
#% 374130
#% 395111
#% 440561
#% 442809
#% 442825
#% 442925
#% 443186
#% 444851
#% 444852
#% 444910
#% 445823
#% 445881
#% 460994
#% 541433
#% 541576
#% 542974
#% 542987
#% 543360
#% 543784
#% 557777
#% 568448
#% 594974
#% 641624
#! The concept of knowledge-based software architecture has recently emerged as a new way to improve our ability to effectively construct and maintain complex large-scale software systems. Under this new paradigm, software engineers are able to do evolutionary design of complex systems through architecture specification, design rationale capture, architecture validation and verification, and architecture transformation. This paper surveys some of the important techniques that have been developed to support these activities. In particular, we are interested in knowledge/requirement acquisition and analysis. We survey some tools that use the knowledge-based approach to solve these problems. We also discuss various software architecture styles, architecture description languages (ADLs), and features of ADLs that help build better software systems. We then compare various ADLs based on these features. The efficient methods that were developed for verification, validation, and high assurance of architectures are also discussed. Based on our survey results, we give a basis for comparing the various knowledge-based systems and list these comparisons in the form of a table.

#index 443272
#* Verification and Validation of Knowledge-Based Systems
#@ Wei-Tek Tsai;Rama Vishnuvajjala;Du Zhang
#t 1999
#c 7
#% 17405
#% 18637
#% 20561
#% 27363
#% 54049
#% 60463
#% 116001
#% 116005
#% 116007
#% 116010
#% 116012
#% 116014
#% 142756
#% 157715
#% 162638
#% 169607
#% 367346
#% 368206
#% 442922
#% 515664
#% 666681
#! Knowledge-based systems (KBS) are being used in many applications areas where their failures can be costly because of the losses in services, property, or even life. To ensure their reliability and dependability, it is therefore important that these systems are verified and validated before they are deployed. This paper provides perspectives on issues and problems that impact the verification and validation (V&V) of KBS. Some of the reasons V&V of KBS is difficult are presented. The paper also provides an overview of different techniques and tools that have been developed for performing V&V activities. Finally, some of the research issues that are relevant for future work in this field are discussed.

#index 443273
#* Data Management Issues and Trade-Offs in CSCW Systems
#@ Atul Prakash;Hyong Sop Shim;Jang Ho Lee
#t 1999
#c 7
#% 18566
#% 31790
#% 43653
#% 80448
#% 87513
#% 87524
#% 87525
#% 124010
#% 128261
#% 128268
#% 173870
#% 173879
#% 173985
#% 194608
#% 204007
#% 204453
#% 216224
#% 216232
#% 216276
#% 216278
#% 216344
#% 216365
#% 216372
#% 577384
#% 617362
#% 635814
#% 956019
#% 956047
#! Substantial interest has developed in recent years in building computer systems that support cooperative work among groups without the need for physical proximity. This paper examines some of the difficult data management issues in designing systems for computer-supported cooperative work (CSCW). Specifically, we consider an example CSCW system to support large-scale team science over the Internet, Collaboratory Builder's Environment; we discuss the issues of managing shared data in such systems, reducing information overload, and providing group awareness and access control. We discuss several promising approaches to these issues. We point out where a significant gap remains in addressing the requirements of such systems and where designers have to make design trade-offs that can be difficult to evaluate. Finally, we discuss several open issues for future work.

#index 443274
#* Information Survivability for Evolvable and Adaptable Real-Time Command and Control Systems
#@ Bhavani M. Thuraisingham;John A. Maurer
#t 1999
#c 7
#% 172878
#% 561901
#% 622861
#% 622925
#! MITRE's Evolvable Real-Time C3 (Command, Control, and Communications) project has developed an approach that would enable current real-time systems to evolve into the systems of the future. This paper first summarizes the design and implementation of an infrastructure for an evolvable real-time C3 system. Then, a detailed discussion of the infrastructure requirements for a survivable real-time C3 system is presented. Finally security issues for survivability, as well as open implementation of the infrastructure, are described. In particular, adaptable middleware for survivable systems is discussed.

#index 443275
#* Dynamic Configuration Management in Reliable Distributed Real-Time Information Systems
#@ K. H. (Kane) Kim;Chittur Subbaraman
#t 1999
#c 7
#% 69618
#% 126825
#% 177456
#% 187392
#% 187409
#% 437028
#% 437761
#% 439994
#% 594975
#% 615184
#% 622847
#% 622876
#% 622904
#% 656851
#! Large-scale information systems emerging in challenging application fields must meet the high standards of reliability, maintainability, and service interruption bound requirements. Their operations are entirely, or partially, of the distributed real-time data object manipulation type. A new architecture for such systems is presented in this paper. The original aspects of the architecture are mainly in two parts: 1) the time-triggered message-triggered object (TMO) structuring of the middleware and the application software of distributed real-time information systems; and 2) the dynamic configuration management subsystem (DCMS), based on the supervisor-based network surveillance (SNS) scheme. The positive impacts of this TMO structuring on maintainability and service interruption bounds are first discussed, with distributed replicated information service systems and other systems as examples. Then, the main discussion dwells on the DCMS architecture驴in particular, formal presentation of its key component: the SNS scheme. As a component of DCMS, the network surveillance (NS) subsystem enables fast learning by each interested fault-free node in the system of the faults or repair completion events occurring in other parts of the system. Currently, concrete real-time NS schemes effective in distributed systems based on point-to-point network architectures are scarce. The SNS scheme presented in this paper is a semicentralized real-time NS scheme effective in a variety of point-to-point networks. This scheme is highly scalable. An efficient implementation model for the SNS scheme is presented that can be easily adapted to various commercial operating system kernels. This paper also presents a formal analysis of the SNS scheme, on the basis of the implementation model, to obtain its strongly competitive tight bounds on the fault detection latency. Finally, some DCMS implementation issues are discussed that remain to be addressed in future research.

#index 443276
#* Software Metrics Knowledge and Databases for Project Management
#@ Raymond A. Paul;Tosiyasu L. Kunii;Yoshihisa Shinagawa;Muhammad F. Khan
#t 1999
#c 7
#% 58636
#% 367870
#% 437209
#% 437211
#% 437212
#% 440675
#% 440855
#% 440984
#% 440996
#! Construction and maintenance of large, high-quality software projects is a complex, error-prone, and difficult process. Tools employing software database metrics can play an important role in efficient execution and management of such large projects. In this paper, we present a generic framework to address this problem. This framework incorporates database and knowledge-base tools, a formal set of software test and evaluation metrics, and a suite of advanced analytic techniques for extracting information and knowledge from available data. The proposed combination of critical metrics and analytic tools can enable highly efficient and cost-effective management of large and complex software projects. The framework has potential for greatly reducing venture risks and enhancing the production quality in the domain of large software project management.

#index 443277
#* Dynamic Programming in Datalog with Aggregates
#@ Sergio Greco
#t 1999
#c 7
#% 33376
#% 35562
#% 36683
#% 53388
#% 55408
#% 83144
#% 99455
#% 100601
#% 101646
#% 103704
#% 103705
#% 123056
#% 123068
#% 123070
#% 135641
#% 169235
#% 169238
#% 190332
#% 194125
#% 231883
#% 268771
#% 268779
#% 268781
#% 384978
#% 408638
#% 461220
#% 464693
#% 480602
#% 480952
#% 598376
#% 644560
#! Dynamic programming is a general technique for solving optimization problems. It is based on the division of problems into simpler subproblems that can be computed separately. In this paper, we show that Datalog with aggregates and other nonmonotonic constructs can express classical dynamic programming optimization problems in a natural fashion, and then we discuss the important classes of queries and applications that benefit from these techniques.

#index 443278
#* Techniques for Increasing the Stream Capacity of A High-Performance Multimedia Server
#@ Divyesh Jadav;Alok N. Choudhary;P. Bruce Berra
#t 1999
#c 7
#% 376
#% 43172
#% 91028
#% 124017
#% 149274
#% 159084
#% 172881
#% 173593
#% 173595
#% 173689
#% 201931
#% 201932
#% 201933
#% 204397
#% 434677
#% 437056
#% 452791
#% 452798
#% 460869
#% 519989
#% 661707
#% 661711
#! High-performance servers and high-speed networks will form the backbone of the infrastructure required for distributed multimedia information systems. A server for an interactive distributed multimedia system may require thousands of gigabytes of storage space and high I/O bandwidth. In order to maximize system utilization and, thus, minimize cost, it is essential that the load be balanced among each of the server's components viz., the disks, the interconnection network, and the scheduler. Many algorithms for maximizing retrieval capacity from the storage system have been proposed in the literature. This paper presents techniques for improving server capacity by assigning media requests to the nodes of a server so as to balance the load on the interconnection network and the scheduling nodes. Five policies for request assignment驴round-robin (RR), minimum link allocation (MLA), minimum contention allocation (MCA), weighted minimum link allocation (WMLA), and weighted minimum contention allocation (WMCA)驴are developed. The performance of these policies on a server model developed earlier is presented. We also consider the issue of file replication, and develop two schemes for storing the replicas, the Parent Group Based Round-Robin Placement (PGBRRP) scheme, and the Group Wide Round-Robin Placement (GWRRP) scheme. The performance of the request assignment policies in the presence of file replication is presented.

#index 443279
#* Resource Scheduling In A High-Performance Multimedia Server
#@ HweeHwa Pang;Bobby Jose;M. s. Krishnan
#t 1999
#c 7
#% 25017
#% 77990
#% 91028
#% 115006
#% 141183
#% 149623
#% 151337
#% 151340
#% 159079
#% 159084
#% 186132
#% 201693
#% 201931
#% 201933
#% 204397
#% 204401
#% 204542
#% 205774
#% 235280
#% 235282
#% 235286
#% 288821
#% 452798
#% 462779
#% 463916
#% 481282
#% 481766
#% 519989
#% 520280
#% 566127
#% 591388
#% 655000
#% 661706
#% 704664
#% 979351
#! Supporting continuous media (CM) data驴such as video and audio驴imposes stringent demands on the retrieval performance of a multimedia server. In this paper, we propose and evaluate a set of data-placement and retrieval algorithms to exploit the full capacity of the disks in a multimedia server. The data-placement algorithm declusters every object over all of the disks in the server驴using a time-based declustering unit驴with the aim of balancing the disk load. As for runtime retrieval, the quintessence of the algorithm is to give each disk advance notification of the blocks that have to be fetched in the impending time periods, so that the disk can optimize its service schedule accordingly. Moreover, in processing a block request for a replicated object, the server will dynamically channel the retrieval operation to the most lightly loaded disk that holds a copy of the required block. We have implemented a multimedia server based on these algorithms. Performance tests reveal that the server achieves very high disk efficiency. Specifically, each disk is able to support up to 25 MPEG-1 streams. Moreover, experiments suggest that the aggregate retrieval capacity of the server scales almost linearly with the number of disks.

#index 443280
#* Join Index Hierarchy: An Indexing Structure for Efficient Navigation in Object-Oriented Databases
#@ Jiawei Han;Zhaohui (Alex) Xie;Yongjian Fu
#t 1999
#c 7
#% 18614
#% 52400
#% 57955
#% 58372
#% 83148
#% 86954
#% 90641
#% 102761
#% 116056
#% 116090
#% 132271
#% 136740
#% 152942
#% 201891
#% 217610
#% 286189
#% 287489
#% 320113
#% 442665
#% 442941
#% 461925
#% 462078
#% 462171
#% 462798
#% 462957
#% 463116
#% 463603
#% 463720
#% 480780
#% 481435
#% 571255
#% 614594
#! A novel indexing structure驴join index hierarchy驴is proposed to handle the "gotos on disk" problem in object-oriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

#index 443281
#* A Hybrid Estimator for Selectivity Estimation
#@ Yibei Ling;Wei Sun;Naphtali D. Rishe;Xianjing Xiang
#t 1999
#c 7
#% 25208
#% 41096
#% 54023
#% 54047
#% 77940
#% 82346
#% 102316
#% 102786
#% 112270
#% 116084
#% 117076
#% 126699
#% 152585
#% 152915
#% 152917
#% 172902
#% 172924
#% 201921
#% 201936
#% 268747
#% 286389
#% 442099
#% 464044
#% 464056
#% 464062
#% 480283
#% 481749
#! Traditional sampling-based estimators infer the actual selectivity of a query based purely on runtime information gathering, excluding the previously collected information, which underutilizes the information available. Table-based and parametric estimators extrapolate the actual selectivity of a query based only on the previously collected information, ignoring on-line information, which results in inaccurate estimation in a frequently updated environment. We propose a novel hybrid estimator that utilizes and optimally combines the on-line and previously collected information. Theoretical analysis demonstrates that the on-line and previously collected information is complementary and that the comprehensive utilization of the on-line and previously collected information is of value for further performance improvement. Our theoretical results are validated by a comprehensive experimental study using a practical database, in the presence of insert, delete, and update operations. The hybrid approach is very promising in the sense that it provides the adaptive mechanism that allows the optimal combination of information obtained from different sources in order to achieve a higher estimation accuracy and reliability.

#index 443282
#* Proof of the Correctness of EMYCIN Sequential Propagation Under Conditional Independence Assumptions
#@ Xudong Luo;Chengqi Zhang
#t 1999
#c 7
#% 24013
#% 44876
#% 63206
#% 68244
#% 128618
#% 472584
#% 689017
#! In this paper, we prove that under the assumption of conditional independence, the EMYCIN formula for sequential propagation can be derived strictly from the definition of the certainty factor according to probability theory. We already have known that Adams [1] and Schocken [15] have proved that the EMYCIN formula for parallel propagation is partially consistent with probability theory. Our result supplements their contribution and, together with theirs, explains why the EMYCIN model has been working reasonably well.

#index 443283
#* 1998 TKDE Reviewers List
#@  IEEE Transactions on Knowledge and Data Engineering Staff
#t 1999
#c 7
#! First Page of the Article

#index 443284
#* Querying Multimedia Presentations Based on Content
#@ Taekyong Lee;Lei Sheng;Tolga Bozkaya;Nevzat Hurkan Balkir;Z. Meral Özsoyoglu;Gultekin Özsoyoglu
#t 1999
#c 7
#% 32904
#% 53998
#% 64441
#% 66208
#% 67459
#% 82351
#% 101955
#% 106243
#% 116048
#% 125601
#% 154334
#% 192709
#% 209061
#% 210388
#% 214693
#% 216621
#% 245787
#% 268797
#% 268799
#% 287497
#% 290413
#% 402992
#% 422966
#% 442887
#% 461912
#% 463902
#% 464073
#% 464827
#% 480946
#% 481434
#% 705638
#% 706391
#% 1848263
#% 1848267
#! In this paper, we consider the problem of querying multimedia presentations based on content information. We believe that presentations should become an integral part of multimedia database systems and users should be able to store, query, and, possibly, manipulate multimedia presentations using a single database management system software. Multimedia presentations are modeled as presentation graphs, which are directed acyclic graphs that visually specify multimedia presentations. Each node of a presentation graph represents a media stream. Edges depict sequential or concurrent playout of streams during the presentation. Information captured in each individual stream and the presentation order of streams constitute the content information of the presentation. Querying multimedia presentation graphs based on content is important for the retrieval of information from a database. We present a graph data model for the specification of multimedia presentations and discuss query languages as effective tools to query and manipulate multimedia presentation graphs with respect to content information. To query the information flow throughout a multimedia presentation, as well as in each individual multimedia stream, we use revised versions of temporal operators Next, Connected, and Until, together with path formulas. These constructs allow us to specify and query paths along a presentation graph. We present an icon-based, graphical query language, GVISUAL, that provides iconic representations for these constructs and a user-friendly graphical interface for query specification. We also present an OQL-like language, GOQL (Graph OQL), with similar constructs, that allows textual and more traditional specifications of graph queries. Finally, we introduce GCalculus (Graph Calculus), a calculus-based language that establishes the formal grounds for the use of temporal operators in path formulas and for querying presentation graphs with respect to content information. We also discuss GCalculus/S (GCalculus with sets) which avoids highly complex query expressions by eliminating universal path quantifier, the negation operator, and the universal quantifier. GCalculus/S represents the formal basis for GVISUAL, i.e., GVISUAL uses the constructs of GCalculus/S directly.

#index 443285
#* Response Time Driven Multimedia Data Objects Allocation for Browsing Documents in Distributed Environments
#@ Siu-Kai So;Ishfaq Ahmad;Kamalakar Karlapalem
#t 1999
#c 7
#% 44934
#% 56450
#% 73775
#% 111874
#% 175720
#% 211480
#% 225006
#% 245787
#% 410980
#% 434706
#% 436952
#% 464216
#% 481766
#% 511160
#% 662251
#% 663299
#% 1180154
#% 1848381
#! Distributed information processing, in many world wide web applications, requires access, transfer, and synchronization of large multimedia data objects (MDOs) (such as, audio, video, and images) across the communication network. Moreover, the end users have started expecting very fast response times and high quality of service. Since the transfer of large MDOs across the communication network contributes to the response time observed by the end users, the problem of allocating these MDOs so as to minimize the response time becomes very challenging. This problem becomes more complex in the context of hypermedia documents (web pages), wherein the MDOs need to be synchronized during presentation to the end users. Note that the basic problem of data allocation in distributed database environments is NP-complete. Therefore, there is a need to pursue and evaluate solutions based on heuristics which generate near-optimal MDO allocation. In this paper, we address this problem by: 1) conceptualizing this problem by using a navigational model to represent hypermedia documents and their access behavior from end users, and by capturing the synchronization requirements on MDOs, 2) formulating the problem by developing a base case cost model for response time, and generalizing it to incorporate user interaction and buffer memory constraints, 3) designing two algorithms to find near-optimal solutions for allocating MDOs of the hypermedia documents while adhering to the synchronization requirements, and 4) evaluating the trade-off between the time complexity to get the solution and the quality of solution by comparing the solutions generated by the algorithms with the optimal solutions generated through an exhaustive search.

#index 443286
#* Database Design Principles for Placement of Delay-Sensitive Data on Disks
#@ Stavros Christodoulakis;Fenia A. Zioga
#t 1999
#c 7
#% 114572
#% 151340
#% 172881
#% 186132
#% 195080
#% 195111
#% 204397
#% 204542
#% 211475
#% 248018
#% 249264
#% 291640
#% 442375
#% 464031
#% 479479
#% 507887
#% 513572
#% 571066
#% 681746
#! We investigate design principles for placing striped delay-sensitive data on a number of disks in a distributed environment. The cost formulas of our performance model allow us to calculate the maximum number of users that can be supported by n disks, as well as to study the impact of other performance-tuning options. We show that, for fixed probabilities of accessing the delay-sensitive objects, partitioning the set of disks is always better than striping in all of the disks. Then, given a number n of disks and r distinct delay-sensitive objects with probabilities of access p1, p2, ..., pr that must be striped across r different disk partitions (i.e., nonoverlapping subsets of the n disks), we use the theory of Schur functions in order to find what is the optimal number of disks that must be allocated to each partition. For objects with different consumption rates, we provide an analytic solution to the problem of disk partitioning. We analyze the problem of grouping the more- and less-popular delay-sensitive objects together in partitions驴when the partitions are less than the objects驴so that the number of supported users is maximized. Finally, we analyze the trade-off of striping on all the disks versus partitioning the set of the disks when the access probabilities of the delay-sensitive objects change with time.

#index 443287
#* Design and Evaluation of a Generic Software Architecture for On-Demand Video Servers
#@ Jonathan Chien-Liang Liu;David H. C. Du;Simon S. Y. Shim;Jenwei Hsieh;MengJou Lin
#t 1999
#c 7
#% 57049
#% 83981
#% 89775
#% 114572
#% 124017
#% 151340
#% 159080
#% 159084
#% 184803
#% 186137
#% 195080
#% 195083
#% 195111
#% 201931
#% 201932
#% 205772
#% 208743
#% 208749
#% 212723
#% 219914
#% 219915
#% 219916
#% 235282
#% 237736
#% 240795
#% 452791
#% 452798
#% 632299
#% 632333
#% 702942
#% 1501990
#% 1797485
#% 1848380
#% 1852658
#! High-bandwidth and real-time constraints for supporting concurrent video accesses make generic software architecture design for high-performance on-demand video servers challenging. This challenging task can be even more complicated when we consider that a generic software architecture should be applied to different hardware platforms. In this paper, we introduce the design, implementation, and evaluation of a generic software architecture for on-demand video servers. We describe different key components on controlling the storage and network devices within the server. The interactive collaborations between these software components are also illustrated. The experimental results indicate a very promising direction on exploring the right combinations of these software components. The server is, thus, able to increase the number of concurrent video accesses with the same hardware configuration. For instance, with the right combinations, the system achieved about 80 percent of the storage system bandwidth of four disks, about 70 percent of the storage system bandwidth of six disks, and generally reached the maximal achieved SCSI bandwidth when eight disks are used over two SCSI buses (i.e., four disks on each SCSI bus). Our research and experimental results are based on video servers currently under construction across a variety of hardware platforms, including SMP, DMP, and clusters of PC or workstations. The most-advanced prototype server is based on an SGI shared-memory multiprocessor with a mass storage system consisting of RAID-3 disk arrays. With all the enabling/management schemes, we were able to further investigate interesting research issues by considering the user's access profiles for taking advantage of popular video titles. The results were significant, with a range of 60 percent improvement given a 512 Kbyte block size. In addition to the experimental results, theoretical performance models were also developed that closely match to our collected experimental results.

#index 443288
#* Symbolic Interpretation of Artificial Neural Networks
#@ Ismail A. Taha;Joydeep Ghosh
#t 1999
#c 7
#% 17185
#% 112232
#% 136350
#% 160857
#% 175368
#% 184090
#% 204015
#% 204434
#% 245797
#% 259101
#% 376683
#% 407292
#% 618434
#% 693091
#% 706759
#% 1275289
#% 1860390
#! Hybrid Intelligent Systems that combine knowledge-based and artificial neural network systems typically have four phases involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training, and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule-extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.

#index 443289
#* Temporal Entity-Relationship Models-A Survey
#@ Heidi Gregersen;Christian S. Jensen
#t 1999
#c 7
#% 18615
#% 43191
#% 68197
#% 99137
#% 104917
#% 104922
#% 106916
#% 111284
#% 139661
#% 140389
#% 163440
#% 168773
#% 171771
#% 209730
#% 260116
#% 287631
#% 361445
#% 384872
#% 462787
#% 463572
#% 482069
#% 533477
#% 534361
#% 534523
#% 546596
#! The Entity-Relationship (ER) model, using varying notations and with some semantic variations, is enjoying a remarkable, and increasing, popularity in both the research community驴the computer science curriculum驴and in industry. In step with the increasing diffusion of relational platforms, ER modeling is growing in popularity. It has been widely recognized that temporal aspects of database schemas are prevalent and difficult to model using the ER model. As a result, how to enable the ER model to properly capture time-varying information has, for a decade and a half, been an active area in the database-research community. This has led to the proposal of close to a dozen temporally enhanced ER models. This paper surveys all temporally enhanced ER models known to the authors. It is the first paper to provide a comprehensive overview of temporal ER modeling and it, thus, meets a need for consolidating and providing easy access to the research in temporal ER modeling. In the presentation of each model, the paper examines how the time-varying information is captured in the model and presents the new concepts and modeling constructs of the model. A total of 19 different design properties for temporally enhanced ER models are defined, and each model is characterized according the these properties.

#index 443290
#* Potential Cases, Methodologies, and Strategies of Synthesis of Solutions in Distributed Expert Systems
#@ Minjie Zhang;Chengqi Zhang
#t 1999
#c 7
#% 52318
#% 130867
#% 444672
#% 541510
#% 689017
#! In this paper, firstly, potential synthesis cases in distributed expert systems (DESs) and types of DESs are identified. Based on these results, necessary conditions of synthesis strategies in different synthesis cases are recognized. Secondly, two methodologies for designing synthesis strategies in distributed expert systems are investigated. They are analysis methods and inductive methods. Thirdly, two methodologies are discussed based on the points of performance, complexity, and requirements.

#index 443291
#* Guest Editor's Introduction to the Special Issue on Web Technologies
#@ Michael R. Lyu
#t 1999
#c 7

#index 443292
#* MAgNET: Mobile Agents for Networked Electronic Trading
#@ Prithviraj Dasgupta;Nitya Narasimhan;Louise E. Moser;P. m. Melliar-Smith
#t 1999
#c 7
#% 202011
#% 240955
#% 252811
#% 268223
#% 271114
#% 385733
#% 452512
#% 452516
#% 452548
#% 482846
#% 540279
#% 596234
#% 743843
#% 912260
#! Electronic commerce technology offers the opportunity to integrate and optimize the global production and distribution supply chain. The computers of the various corporations, located throughout the world, will communicate with each other to determine the availability of components, to place and confirm orders, and to negotiate delivery timescales. In this paper, we describe MAgNET, a system for networked electronic trading that is based on the Java mobile agent technology, called aglets. Aglets are dispatched by the buyer to the various suppliers, where they negotiate orders and deliveries, returning to the buyer with their best deals for approval. MAgNET handles the deep supply chain, where a supplier may need to contact further suppliers of subcomponents in order to respond to an enquiry. Experimental results demonstrate the feasibility of using the Java aglet technology for electronic commerce.

#index 443293
#* An Approach to Mobile Software Robots for the WWW
#@ Kazuhiko Kato;Yuuichi Someya;Katsuya Matsubara;Kunihiko Toumura;Hirotake Abe
#t 1999
#c 7
#% 36102
#% 132271
#% 166234
#% 179936
#% 196459
#% 224356
#% 410754
#% 437562
#% 443056
#% 444217
#% 482846
#% 483014
#% 488072
#% 555105
#! This paper describes a framework for developing mobile software robots by using the Planet mobile object system, which is characterized by language-neutral layered architecture, the native code execution of mobile objects, and asynchronous object passing. We propose an approach to implementing mobile Web search robots that takes full advantage of these characteristics, and we base our discussion of its effectiveness on experiments conducted in the Internet environment. The results show that the Planet approach to mobile Web search robots significantly reduces the amount of data transferred via the Internet and that it enables the robots to work more efficiently than the robots in the conventional stationary scheme whenever nontrivial amounts of HTML files are processed.

#index 443294
#* Proxy Cache Algorithms: Design, Implementation, and Performance
#@ Junho Shim;Peter Scheuermann;Radek Vingralek
#t 1999
#c 7
#% 152943
#% 176500
#% 209653
#% 209698
#% 255027
#% 255035
#% 268090
#% 275454
#% 404765
#% 408396
#% 464228
#% 566126
#% 635804
#% 642534
#% 978363
#% 978372
#% 978378
#% 979356
#% 979357
#! Caching at proxy servers is one of the ways to reduce the response time perceived by World Wide Web users. Cache replacement algorithms play a central rolfe in the response time reduction by selecting a subset of documents for caching, so that a given performance metric is maximized. At the same time, the cache must take extra steps to guarantee some form of consistency of the cached documents. Cache consistency algorithms enforce appropriate guarantees about the staleness of the cached documents. We describe a unified cache maintenance algorithm, LNC-R-W3-U, which integrates both cache replacement and consistency algorithms. The LNC-R-W3-U algorithm evicts documents from the cache based on the delay to fetch each document into the cache. Consequently, the documents that took a long time to fetch are preferentially kept in the cache. The LNC-R-W3-U algorithm also considers in the eviction consideration the validation rate of each document, as provided by the cache consistency component ofLNC-R-W3-U. Consequently, documents that are infrequently updated and thus seldom require validations are preferentially retained in the cache. We describe the implementation of LNC-R-W3-U and its integration with the Apache 1.2.6 code base. Finally, we present a trace-driven experimental study of LNC-R-W3-U performance and its comparison with other previously published algorithms for cache maintenance.

#index 443295
#* Volume Leases for Consistency in Large-Scale Systems
#@ Jian Yin;Lorenzo Alvisi;Michael Dahlin;Calvin Lin
#t 1999
#c 7
#% 29590
#% 36103
#% 65484
#% 65498
#% 131554
#% 202145
#% 202146
#% 208562
#% 213467
#% 463481
#% 464228
#% 531907
#% 635804
#% 635857
#% 642534
#% 979027
#% 979356
#! This article introduces volume leases as a mechanism for providing server-driven cache consistency for large-scale, geographically distributed networks. Volume leases retain the good performance, fault tolerance, and server scalability of the semantically weaker client-driven protocols that are now used on the web. Volume leases are a variation of object leases, which were originally designed for distributed file systems. However, whereas traditional object leases amortize overheads over long lease periods, volume leases exploit spatial locality to amortize overheads across multiple objects in a volume. This approach allows systems to maintain good write performance even in the presence of failures. Using trace-driven simulation, we compare three volume lease algorithms against four existing cache consistency algorithms and show that our new algorithms provide strong consistency while maintaining scalability and fault-tolerance. For a trace-based workload of web accesses, we find that volumes can reduce message traffic at servers by 40 percent compared to a standard lease algorithm, and that volumes can considerably reduce the peak load at servers when popular objects are modified.

#index 443296
#* WebCompanion: A Friendly Client-Side Web Prefetching Agent
#@ Reinhard P. Klemm
#t 1999
#c 7
#% 160390
#% 209895
#% 211739
#% 255035
#% 630264
#% 978379
#% 978380
#! Accessing remote sites of the World Wide Web is often a frustrating experience for users because of long Web page retrieval times even over relatively fast Internet connections. Users are more likely to embrace the further expansion of the role of the Web into a major infrastructure for electronic commerce and for information, application, and multimedia delivery if Web accesses can be accelerated. One technique that attempts this is prefetching. We built a client-side Java-implemented prefetching agent, WebCompanion, which employs a novel adaptive, fast, and selective online prefetching strategy based on estimated round-trip times for Web resources. This strategy efficiently hides the access latencies for slow resources while at the same time limiting the network and server overhead and local resource consumption to moderate levels. Our extensive experiments show an average access speed-up of greater than 50 percent and an average network byte overhead of less than 150 percent using WebCompanion over a fast Internet connection. We measured a slight acceleration in accessing the Web through WebCompanion even in a pessimistic scenario where the user never requests a prefetched document.

#index 443297
#* Enabling Concept-Based Relevance Feedback for Information Retrieval on the WWW
#@ Chia-Hui Chang;Ching-Chi Hsu
#t 1999
#c 7
#% 46809
#% 115478
#% 118726
#% 118771
#% 169729
#% 218978
#% 218992
#% 232717
#% 255167
#% 676183
#! The World Wide Web is a world of great richness, but finding information on the Web is also a great challenge. Keyword-based querying has been an immediate and efficient way to specify and retrieve related information that the user inquires. However, conventional document ranking based on an automatic assessment ofdocument relevance to the query may not be the best approach when little information is given, as in most cases. In order to clarify the ambiguity of the short queries given by users, we propose the idea of concept-based relevance feedback for Web information retrieval. The idea is to have users give two to three times more feedback in the same amount of time that would be required to give feedback for conventional feedback mechanisms. Under this design principle, we apply clustering techniques to the initial search results to provide concept-based browsing. We show the performances of various feedback interface designs and compare their pros and cons. We shall measure precision and relative recall to show how clustering improves performance over conventional similarity ranking and, most importantly, we shall show how the assistance of concept-based presentation reduces browsing labor.

#index 443298
#* Continual Queries for Internet Scale Event-Driven Information Delivery
#@ Ling Liu;Calton Pu;Wei Tang
#t 1999
#c 7
#% 13015
#% 13016
#% 32914
#% 58361
#% 66654
#% 77005
#% 116082
#% 152928
#% 199556
#% 201928
#% 202156
#% 210212
#% 231387
#% 237262
#% 241207
#% 248853
#% 300957
#% 440708
#% 442706
#% 445891
#% 462212
#% 462645
#% 464043
#% 479904
#% 480268
#% 480768
#% 481448
#% 631866
#% 635797
#% 978507
#! In this paper we introduce the concept of continual queries, describe the design of a distributed event-driven continual query system驴OpenCQ, and outline the initial implementation of OpenCQ on top of the distributed interoperable information mediation system DIOM [22], [20]. Continual queries are standing queries that monitor update of interest and return results whenever the update reaches specified thresholds. In OpenCQ, users may specify to the system the information they would like to monitor (such as the events or the update thresholds they are interested in). Whenever the information of interest becomes available, the system immediately delivers it to the relevant users; otherwise, the system continually monitors the arrival of the desired information and pushes it to the relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data management systems such as DBMSs and Web search engines, OpenCQ exhibits two important features: 1. it provides push-enabled, event-driven, content-sensitive information delivery capabilities, and 2. it combines pull and push services in a unified framework. By event-driven we mean that the update events of interest to be monitored are specified by users or applications. By content-sensitive, we mean the evaluation of the trigger condition happens only when a potentially interesting change occurs. And, by push-enabled, we mean the active delivery of query results or triggering of actions without user intervention.

#index 443299
#* Managing Complex Documents Over the WWW: A Case Study for XML
#@ Paolo Ciancarini;Fabio Vitali;Cecilia Mascolo
#t 1999
#c 7
#% 212
#% 115390
#% 175334
#% 212759
#% 227825
#% 238089
#% 238453
#% 246333
#% 255130
#% 268094
#% 376157
#% 452528
#% 505947
#% 523154
#% 530829
#% 530853
#% 530987
#% 538272
#! The use of the World Wide Web as a communication medium for knowledge engineers and software designers is limited by the lack of tools for writing, sharing, and verifying documents written with design notations. For instance, the Z language has a rich set of mathematical characters, and requires graphic-rich boxes and schemas for structuring a specification document. It is difficult to integrate Z specifications and text on WWW pages written with HTML, and traditional tools are not suited for the task. On the other hand, a newly proposed standard for markup languages, namely XML, allows one to define any set of markup elements; hence, it is suitable for describing any kind of notation. Unfortunately, the proposed standard for rendering XML documents, namely XSL, provides for text-only (although sophisticated) rendering of XML documents, and thus it cannot be used for more complex notations. We present a Java-based tool for applying any notation to elements of XML documents. These XML documents can thus be shown on current-generation WWW browsers with Java capabilities. A complete package for displayingZ specifications has been implemented and integrated with standard text parts. Being a complete rendering engine, text parts andZ specifications can be freely intermixed, and all the standard features of XML (including HTML links and form elements) are available outside and inside Z specifications. Furthermore, the extensibility of our engine allows any additional notations to be supported and integrated with the ones we describe here.

#index 443300
#* JPernLite: Extensible Transaction Services for the WWW
#@ Jingshuang Yang;Gail E. Kaiser
#t 1999
#c 7
#% 9241
#% 32897
#% 86938
#% 103739
#% 126965
#% 151662
#% 172878
#% 173762
#% 203051
#% 209675
#% 211528
#% 215223
#% 221392
#% 230397
#% 240732
#% 240747
#% 261273
#% 284880
#% 380316
#% 424262
#% 462201
#% 480259
#! Concurrency control is one of the key problems in design and implementation of collaborative systems such as hypertext/hypermedia systems, CAD/CAM systems, and software development environments. Most existing systems store data in specialized databases with built-in concurrency control policies, usually implemented via locking. It is desirable to construct such collaborative systems on top of the World Wide Web, but most Web servers do not support even conventional transactions, let alone distributed (multi-Website) transactions or flexible concurrency control mechanisms oriented toward teamwork驴such as event notification, shared locks, and fine granularity locks. We present a transaction server that operates independently of Web servers or the collaborative systems, to fill the concurrency control gap. By default, the transaction server enforces the conventional atomic transaction model, where sets of operations are performed in an all-or-nothing fashion and isolated from concurrent users. The server can be tailored dynamically to apply more sophisticated concurrency control policies appropriate for collaboration. The transaction server also supports applications employing information resources other than Web servers, such as legacy databases, CORBA objects, and other hypermedia systems. Our implementation permits a wide range of system architecture styles.

#index 443301
#* An Alternative Paradigm for Scalable On-Demand Applications: Evaluating and Deploying the Interactive Multimedia Jukebox
#@ Kevin C. Almeroth;Mostafa H. Ammar
#t 1999
#c 7
#% 69296
#% 171433
#% 173593
#% 245971
#% 255118
#% 255120
#% 434677
#% 468110
#% 610577
#% 612098
#% 1502085
#% 1830069
#% 1848364
#! Straightforward, one-way delivery of audio/video through television sets has existed for many decades. In the 1980s, new services like pay-per-view and video-on-demand were touted as the 驴killer applications驴 for interactive TV. However, the hype quickly died away, leaving only hard technical problems and costly systems. As an alternative, we propose a new jukebox paradigm offering flexibility in how programs are requested and scheduled for playout. The jukebox scheduling paradigm offers flexibility ranging from complete viewer control (true video-on-demand), to complete service provider control (traditional broadcast TV). In this paper, we first describe our proposed jukebox paradigm and relate it to other on-demand paradigms. We also describe several critical research issues, including the one-to-many delivery of content, program scheduling policies, server location, and the provision of advanced services like VCR-style interactivity and advanced reservations. In addition, we present our implementation of a jukebox-based service called the Interactive Multimedia Jukebox(IMJ). The IMJ provides scheduling via the World Wide Web (WWW) and content delivery via the Multicast Backbone (MBone). For the IMJ, we present usage statistics collected during the past couple of years. Furthermore, using this data and a simulation environment, we show that jukebox systems have the potential to scale to very large numbers of viewers.

#index 443302
#* Workflow and End-User Quality of Service Issues in Web-Based Education
#@ Mladen A. Vouk;Donald L. Bitzer;Richard L. Klevans
#t 1999
#c 7
#% 194234
#% 215609
#% 239178
#% 440950
#% 444987
#% 452466
#% 639724
#% 707776
#% 966169
#% 1801057
#! The option of obtaining education over networks is quickly becoming a reality for all those who have access to the Internet and the World Wide Web (WWW). However, at present, network-based education (NBE) over the WWW and the Internet in general faces a number of pitfalls. The problems range from inadequate end-user quality of service (QoS), to inadequate materials, to shortcomings in learning paradigms, and to missing or inappropriate student assessment and feedback mechanisms. In this paper, we discuss some major issues that, although mostly solved for NBE, still face Web-based education (WBE). These include the required workflow-oriented technological and quality of service support. In discussing the issues, we use examples from a wide-area NBE/WBE system called NovaNET and a WBE system called Web Lecture System (WLS). We recommend that WBE system developers construct operational user (workflow) profiles before building their content and interfaces. Our experience is that, especially for synchronous WBE systems, user-level round-trip keystroke delays should not exceed about 250 ms and the overall availability of the system (including network-related service failures) should be at least 0.95. We also suggest that a successful WBE system will have a sound auto-adaptive knowledge assessment component, a 驴virtual驴 laboratory capability, and a set of strong collaborative functions. Until WBE systems match teacher/instructor and student workflows both technologically (as an appliance) and pedagogically, WBE, except in its most primitive form, will not be an effective educational tool.

#index 443303
#* Spatial Join Processing Using Corner Transformation
#@ Ju-Won Song;Kyu-Young Whang;Young-Koo Lee;Min-Jae Lee;Sang-Wook Kim
#t 1999
#c 7
#% 13041
#% 68091
#% 77928
#% 86950
#% 152937
#% 172908
#% 172909
#% 210186
#% 210187
#% 435124
#% 435137
#% 462041
#% 463595
#% 479453
#% 480587
#% 510675
#% 526864
#% 566113
#! Spatial join finds pairs of spatial objects having a specific spatial relationship in spatial database systems. Since spatial join is a fairly expensive operation, we need an efficient algorithm taking advantage of the characteristics of available spatial access methods. In this paper, we propose a spatial join algorithm using corner transformation and show its excellence through experiments. To the extent of authors' knowledge, the spatial join processing using corner transformation is new. In corner transformation, two regions in one file joined with two adjacent regions in the other file share a large common area. The proposed algorithm utilizes this property in order to reduce the number of disk accesses for spatial join. Experimental results show that the performance of the algorithm is generally better than that of the R*-tree based algorithm proposed by Brinkhoff et al. This is a strong indication that corner transformation is a promising category of spatial access methods and that spatial operations can be performed better in the transform space than in the original space. This reverses the common belief that transformation will adversely effect the clustering. We also briefly mention that the join algorithm based on corner transformation has a nice property of being amenable to parallel processing. We believe that our result will provide a new insight towards transformation-based processing of spatial operations.

#index 443304
#* Data Categorization Using Decision Trellises
#@ Paolo Frasconi;Marco Gori;Giovanni Soda
#t 1999
#c 7
#% 20724
#% 44876
#% 68244
#% 105771
#% 130878
#% 136350
#% 143182
#% 156186
#% 169358
#% 170665
#% 197387
#% 232106
#% 232117
#% 361100
#% 368248
#% 443025
#% 449559
#% 503560
#% 651280
#% 669185
#% 1476310
#% 1650767
#% 1860077
#! We introduce a probabilistic graphical model for supervised learning on databases with categorical attributes. The proposed belief network contains hidden variables that play a role similar to nodes in decision trees and each of their states either corresponds to a class label or to a single attribute test. As a major difference with respect to decision trees, the selection of the attribute to be tested is probabilistic. Thus, the model can be used to assess the probability that a tuple belongs to some class, given the predictive attributes. Unfolding the network along the hidden states dimension yields a trellis structure having a signal flow similar to second order connectionist networks. The network encodes context specific probabilistic independencies to reduce parametric complexity. We present a custom tailored inference algorithm and derive a learning procedure based on the expectation-maximization algorithm. We propose decision trellises as an alternative to decision trees in the context of tuple categorization in databases, which is an important step for building data mining systems. Preliminary experiments on standard machine learning databases are reported, comparing the classification accuracy of decision trellises and decision trees induced by C4.5. In particular, we show that the proposed model can offer significant advantages for sparse databases in which many predictive attributes are missing.

#index 443305
#* Building Hypertext Links By Computing Semantic Similarity
#@ Stephen J. Green
#t 1999
#c 7
#% 151709
#% 169729
#% 230521
#% 230523
#% 230535
#% 240738
#% 394709
#% 706971
#% 740329
#! Most current automatic hypertext generation systems rely on term repetition to calculate the relatedness of two documents. There are well-recognized problems with such approaches, most notably, a vulnerability to the effects of synonymy (many words for the same concept) and polysemy (many concepts for the same word). We propose a novel method for automatic hypertext generation that is based on a technique called lexical chaining, a method for discovering sequences of related words in a text. This method uses a more general notion of document relatedness, and attempts to take into account the effects of synonymy and polysemy. We also present the results of an empirical study designed to test this method in the context of a question answering task from a database of newspaper articles.

#index 443306
#* The Ant System Applied to the Quadratic Assignment Problem
#@ Vittorio Maniezzo;Alberto Colorni
#t 1999
#c 7
#% 289037
#% 1780532
#! In recent years, there has been growing interest in algorithms inspired by the observation of natural phenomena to define computational procedures that can solve complex problems. In this article, we describe a distributed heuristic algorithm that was inspired by the observation of the behavior of ant colonies, and we propose its use for the Quadratic Assignment Problem. The results obtained in solving several classical instances of the problem are compared with those obtained from other evolutionary heuristics to evaluate the quality of the proposed system.

#index 443307
#* Server Capacity Planning for Web Traffic Workload
#@ Krishna Kant;Youjip Won
#t 1999
#c 7
#% 160390
#% 188026
#% 196042
#% 236763
#% 252458
#% 440145
#% 545474
#% 642532
#% 978363
#% 978374
#! The goal of this paper is to provide a methodology for determining bandwidth requirements for various hardware components of a World Wide Web server. The paper assumes a traditional symmetric multiprocessor (SMP) architecture for theweb-server, although the same analysis applies to an SMP node in a cluster. The paper derives formulae for bandwidth demandsfor memory, processor data bus, network adapters, disk adapters, I/O-memory paths, and I/O buses. Since the web workload characteristics vary widely, three sample workloads are considered for illustrative purposes: 1) standard SPECweb96,2) a SPECweb96-like workload that assumes dynamic data and retransmissions, and 3) WebProxy, which models a web proxyserver that does not do much caching and, thus, has rather severe requirements. The results point to a few general conclusions regarding Web workloads. In particular, reduction in memory/data bus bandwidth by using the virtual interface architecture (VIA)is very desirable, and the connectivity needs may go well beyond the capabilities of traditional systems based on the traditionalPCI-bus. Web workloads also demand a significantly higher memory bandwidth than data bus bandwidth and this disparity isexpected to increase with the use of VIA. Also, the current efforts to offload TCP/IP processing may require a larger headroomin I/O subsystem bandwidth than in the processor-memory subsystem.

#index 443308
#* A New Architecture for Integration of CORBA and OODB
#@ Ruey-Kai Sheu;Kai-Chih Liang;Shyan-Ming Yuan;Win-tsung Lo
#t 1999
#c 7
#% 116203
#% 227650
#% 235914
#% 240469
#% 482387
#% 1829998
#! Object-oriented database system (OODB) supports an object-oriented data model with the functionality of persistency and transaction semantics. In order to facilitate the use of OODB, the Object Database Management Group (ODMG) defined a standard for object database management system. On the other hand, the Object Management Group (OMG) defined the Common Object Request Broker Architecture (CORBA), which is an emerging standard of distributed object technology providing the interconnection network between distributed objects. For the sake of matching these two object models, taking the advantages of merging both of them, and building a more sophisticated infrastructure, the integration of CORBA and OODB is currently an urgent and important issue in distributed object systems. Instead of using Object Database Adapter (ODA) suggested by the ODMG, in this paper, we provide a novel way of reusing the Object Transaction Service (OTS) and wrapping techniques to introduce OODB into CORBA automatically. Through our design, CORBA clients or OODB object implementers do not need to learn any knowledge of each other. In addition, error recovery is also provided to guarantee the consistency of object states. The whole task for integrating CORBA and OODB is done transparently by our proposed preprocessor, which plays an important role in solving problems encountered by ORB and OODB vendors easily.

#index 443309
#* Part-Whole Relationship Categories and Their Application in Object-Oriented Analysis
#@ Renate Motschnig-Pitrik;Jens Kaasbøll
#t 1999
#c 7
#% 4797
#% 30569
#% 38696
#% 54046
#% 58373
#% 75905
#% 83273
#% 94770
#% 107082
#% 115009
#% 116185
#% 140389
#% 157392
#% 159224
#% 168251
#% 169311
#% 183516
#% 196354
#% 197998
#% 204267
#% 206295
#% 221337
#% 221341
#% 221344
#% 287631
#% 435121
#% 442856
#% 479758
#% 534369
#% 534874
#% 564481
#% 566425
#! Part decomposition and, conversely, the construction of composite objects out of individual parts have long been recognized as ubiquitous and essential mechanisms involving abstraction. This applies, in particular, in areas such as CAD, manufacturing, software development, and computer graphics. Although the part-of relationship is distinguished in object-oriented modeling techniques, it ranks far behind the concept of generalization/specialization and a rigorous definition of its semantics is still missing. In this paper, we first show in which ways a shift in emphasis on the part-of relationship leads to analysis and design models that are easier to understand and to maintain. We then investigate the properties of part-of relationships in order to define their semantics. This is achieved by means of a categorization of part-of relationships and by associating semantic constraints with individual categories. We further suggest a precise and, compared with existing techniques, less redundant specification of constraints accompanying part-of categories based on the degree of exclusiveness and dependence of parts on composite objects. Although the approach appears generally applicable, the object-oriented Unified Modeling Language (UML) is used to present our findings. Several examples demonstrate the applicability of the categories introduced.

#index 443310
#* Mining Multiple-Level Association Rules in Large Databases
#@ Jiawei Han;Yongjian Fu
#t 1999
#c 7
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 223781
#% 227919
#% 340291
#% 443082
#% 443091
#% 452747
#% 463883
#% 463903
#% 464204
#% 481290
#% 481754
#% 481758
#% 481954
#% 527021
#! A top-down progressive deepening method is developed for efficient mining of multiple-level association rules from large transaction databases based on the Apriori principle. A group of variant algorithms is proposed based on the ways of sharing intermediate results, with the relative performance tested and analyzed. The enforcement of different interestingness measurements to find more interesting rules, and the relaxation of rule conditions for finding 驴level-crossing驴 association rules, are also investigated in the paper. Our study shows that efficient algorithms can be developed from large databases for the discovery of interesting and strong multiple-level association rules.

#index 443311
#* Induction By Attribute Elimination
#@ Xindong Wu;David Urpani
#t 1999
#c 7
#% 21533
#% 73374
#% 93697
#% 99396
#% 136350
#% 362396
#% 449588
#! In most data-mining applications where induction is used as the primary tool for knowledge extraction from real-world databases, it is difficult to precisely identify a complete set of relevant attributes. This paper introduces a new rule induction algorithm called Rule Induction Two In One (RITIO), which eliminates attributes in the order of decreasing irrelevancy. Like ID3-like decision tree construction algorithms, RITIO makes use of the entropy measure as a means of constraining the hypothesis search space; but, unlike ID3-like algorithms, the hypotheses language is the rule structure and RITIO generates rules without constructing decision trees. The final concept description produced by RITIO is shown to be largely based on only the most relevant attributes. Experimental results confirm that, even on noisy, industrial databases, RITIO achieves high levels of predictive accuracy.

#index 443312
#* Splitting and Merging Version Spaces to Learn Disjunctive Concepts
#@ Tzung-Pei Hong;Shian-Shyong Tseng
#t 1999
#c 7
#% 58324
#% 442912
#% 443124
#% 688946
#! We have modified the original version space strategy in order to learn disjunctive concepts incrementally and without saving past training instances. The algorithm time complexity is also analyzed, and its correctness is proven.

#index 443313
#* Finding Interesting Patterns Using User Expectations
#@ Bing Liu;Wynne Hsu;Lai-Fun Mun;Hing-Yan Lee
#t 1999
#c 7
#% 17532
#% 25443
#% 90725
#% 100324
#% 136350
#% 172386
#% 174161
#% 179008
#% 182919
#% 212710
#% 232106
#% 384956
#% 394745
#% 443092
#% 452747
#% 452819
#% 452821
#% 452822
#% 1499588
#! One of the major problems in the field of knowledge discovery (or data mining) is the interestingness problem. Past research and applications have found that, in practice, it is all too easy to discover a huge number of patterns in a database. Most of these patterns are actually useless or uninteresting to the user. But due to the huge number of patterns, it is difficult for the user to comprehend them and to identify those interesting to him/her. To prevent the user from being overwhelmed by the large number of patterns, techniques are needed to rank them according to their interestingness. In this paper, we propose such a technique, called the user-expectation method. In this technique, the user is first asked to provide his/her expected patterns according to his/her past knowledge or intuitive feelings. Given these expectations, the system uses a fuzzy matching technique to match the discovered patterns against the user's expectations, and then rank the discovered patterns according to the matching results. A variety of rankings can be performed for different purposes, such as to confirm the user's knowledge and to identify unexpected patterns, which are by definition interesting. The proposed technique is general and interactive.

#index 443314
#* Pattern Discovery by Residual Analysis and Recursive Partitioning
#@ Tom Chau;Andrew K. C. Wong
#t 1999
#c 7
#% 10181
#% 33917
#% 73474
#% 92142
#% 107633
#% 143058
#% 234978
#% 361100
#% 380342
#% 443172
#% 690539
#% 1780538
#! In this paper, a novel method of pattern discovery is proposed. It is based on the theoretical formulation of a contingency table of events. Using residual analysis and recursive partitioning, statistically significant events are identified in a data set. These events constitute the important information contained in the data set and are easily interpretable as simple rules, contour plots, or parallel axes plots. In addition, an informative probabilistic description of the data is automatically furnished by the discovery process. Following a theoretical formulation, experiments with real and simulated data will demonstrate the ability to discover subtle patterns amid noise, the invariance to changes of scale, cluster detection, and discovery of multidimensional patterns. It is shown that the pattern discovery method offers the advantages of easy interpretation, rapid training, and tolerance to noncentralized noise.

#index 443315
#* The Impact of Data Quality Information on Decision Making: An Exploratory Analysis
#@ InduShobha N. Chengalur-Smith;Donald P. Ballou;Harold L. Pazer
#t 1999
#c 7
#% 61070
#% 83308
#% 234805
#% 382342
#% 442973
#% 912094
#! This paper describes an experiment that explores the consequences of providing information regarding the quality of data used in decision making. The subjects in the study were given three types of information about the data's quality: none, two-point ordinal, and interval scale. This information was made available to the subjects, along with the actual data. Two decision strategies were explored: conjunctive and weighted linear additive. Two decision environments were used: a simple environment and a relatively complex environment. Various combinations of these factors were employed to explore several issues. These include complacency, consensus, and consistency. The paper provides preliminary insights into which type of data-quality information is most effective and the circumstances in which data-quality information is most effective. Such knowledge would be of value to those responsible for designing databases that support decision-makers. Overall, we find that in a situation where subjects are confronted with clearly differentiated alternatives, the inclusion of data-quality information impacted the selection of the preferred alternative while maintaining group consensus.

#index 443316
#* Automatic Text Categorization and Its Application to Text Retrieval
#@ Wai Lam;Miguel Ruiz;Padmini Srinivasan
#t 1999
#c 7
#% 67565
#% 118736
#% 165110
#% 165111
#% 169718
#% 169777
#% 184486
#% 206402
#% 217251
#% 219051
#% 219052
#% 219053
#% 237052
#% 262050
#% 817843
#% 840583
#! We develop an automatic text categorization approach and investigate its application to text retrieval. The categorization approach is derived from a combination of a learning paradigm known as instance-based learning and an advanced document retrieval technique known as retrieval feedback. We demonstrate the effectiveness of our categorization approach using two real-world document collections from the MEDLINE database. Next, we investigate the application of automatic categorization to text retrieval. Our experiments clearly indicate that automatic categorization improves the retrieval performance compared with no categorization. We also demonstrate that the retrieval performance using automatic categorization achieves the same retrieval quality as the performance using manual categorization. Furthermore, detailed analysis of the retrieval performance on each individual test query is provided.

#index 443317
#* A One-Phase Algorithm to Detect Distributed Deadlocks in Replicated Databases
#@ Ajay D. Kshemkalyani;Mukesh Singhal
#t 1999
#c 7
#% 9241
#% 39634
#% 65353
#% 65723
#% 112295
#% 153303
#% 165296
#% 170560
#% 176487
#% 197921
#% 317806
#% 318399
#% 320204
#% 442867
#% 445802
#% 462329
#% 602675
#% 617245
#% 662053
#! Replicated databases that use quorum-consensus algorithms to perform majority voting are prone to deadlocks. Due to the P-out-of-Q nature of quorum requests, deadlocks that arise are generalized deadlocks and are hard to detect. We present an efficient distributed algorithm to detect generalized deadlocks in replicated databases. The algorithm performs reduction of a distributed wait-for-graph (WFG) to determine the existence of a deadlock. If sufficient information to decide the reducibility of a node is not available at that node, the algorithm attempts reduction later in a lazy manner. We prove the correctness of the algorithm. The algorithm has a message complexity of $2e$ messages and a worst-case time complexity of $2d+2$ hops, where $e$ is the number of edges and $d$ is the diameter of the WFG. The algorithm is shown to perform significantly better in both time and message complexity than the best known existing algorithms. We conjecture that this is an optimal algorithm, in time and message complexity, to detect generalized deadlocks if no transaction has complete knowledge of the topology of the WFG or the system and the deadlock detection is to be carried out in a distributed manner.

#index 443318
#* Data Consistency in Intermittently Connected Distributed Systems
#@ Evaggelia Pitoura;Bharat Bhargava
#t 1999
#c 7
#% 3083
#% 3645
#% 9240
#% 9241
#% 77005
#% 102804
#% 123096
#% 131554
#% 137753
#% 172382
#% 172875
#% 175253
#% 176491
#% 202144
#% 202146
#% 210179
#% 224889
#% 240016
#% 286967
#% 340608
#% 340660
#% 384050
#% 437181
#% 443004
#% 500721
#% 558014
#% 562178
#% 609905
#% 617260
#% 647073
#% 660942
#! Mobile computing introduces a new form of distributed computation in which communication is most often intermittent, low-bandwidth, or expensive, thus providing only weak connectivity. In this paper, we present a replication scheme tailored for such environments. Bounded inconsistency is defined by allowing controlled deviation among copies located at weakly connected sites. A dual database interface is proposed that in addition to read and write operations with the usual semantics supports weak read and write operations. In contrast to the usual read and write operations that read consistent values and perform permanent updates, weak operations access only local and potentially inconsistent copies and perform updates that are only conditionally committed. Exploiting weak operations supports disconnected operation since mobile clients can employ them to continue to operate even while disconnected. The extended database interface coupled with bounded inconsistency offers a flexible mechanism for adapting replica consistency to the networking conditions by appropriately balancing the use of weak and normal operations. Adjusting the degree of divergence among copies provides additional support for adaptivity. We present transaction-oriented correctness criteria for the proposed schemes, introduce corresponding serializability-based methods, and outline protocols for their implementation. Then, some practical examples of their applicability are provided. The performance of the scheme is evaluated for a range of networking conditions and varying percentages of weak transactions by using an analytical model developed for this purpose.

#index 443319
#* Multilevel Filtering for High-Dimensional Image Data: Why and How
#@ Raymond T. Ng;Dominic Tam
#t 1999
#c 7
#% 50685
#% 65393
#% 68091
#% 83962
#% 86950
#% 120270
#% 137887
#% 146206
#% 153260
#% 169940
#% 213975
#% 219847
#% 237204
#% 285932
#% 320113
#% 321250
#% 427199
#% 435141
#% 452795
#% 458521
#% 464195
#% 479649
#% 480093
#% 480950
#% 481956
#! It has been shown that filtering is a promising way to support efficient content-based retrievals from image data. However, all existing studies on filtering restrict their attention to two levels. In this paper, we consider filtering structures that have at least three levels. In the first half of the paper, by analyzing the CPU and I/O costs of various structures, we provide analytic evidence on why three-level structures can often outperform corresponding two-level ones. We provide further experimental results showing that the three-level structures are typically the best, and can beat the two-level ones by a wide margin. In the second half of the paper, we study how to find the (near-) optimal three-level structure for a given dataset. We develop an optimizer that can handle this task effectively and efficiently. Experimental results indicate that in tens of seconds of CPU time, the optimizer can find a filtering structure whose total runtime per query exceeds that of the real optimal structure by only 2-3 percent .

#index 443320
#* Indexing Valid Time Databases via B+-Trees
#@ Mario A. Nascimento;Margaret H. Dunham
#t 1999
#c 7
#% 13041
#% 16028
#% 68091
#% 86950
#% 102810
#% 108499
#% 116064
#% 135557
#% 140389
#% 149632
#% 163440
#% 172370
#% 182672
#% 182700
#% 182902
#% 213001
#% 225004
#% 241271
#% 287070
#% 361445
#% 427199
#% 442967
#% 452782
#% 462956
#% 463729
#% 463749
#% 480093
#% 481928
#% 527802
#% 562166
#! We present an approach, named MAP21, which uses standard $\rm B^+$-trees to provide efficient indexing of valid time ranges. The MAP21 approach is based on mapping one dimensional ranges to one dimensional points where the lexicographical order among the ranges is preserved. The proposed approach may employ more than one tree, each indexing a disjoint subset of the indexed data. When compared to the Time Index and the $\rm R^*$-tree we show that MAP21's performance is comparable to or better than those, depending on the type of query. In terms of storage, MAP21's structure was less than 10 percent larger than the $\rm R^*$-tree's and much smaller than the Time Index's. The main contribution of this paper though, is to show that standard $\rm B^+$-trees, available in virtually any DBMS, can be used to provide an efficient temporal index.

#index 443321
#* Qualitative and Quantitative Temporal Constraints and Relational Databases: Theory, Architecture, and Applications
#@ Vittorio Brusoni;Luca Console;Paolo Terenziani;Barbara Pernici
#t 1999
#c 7
#% 663
#% 2298
#% 17485
#% 18615
#% 21136
#% 21144
#% 75812
#% 82720
#% 100613
#% 107137
#% 111284
#% 135384
#% 137042
#% 166497
#% 176883
#% 184796
#% 190332
#% 228800
#% 319244
#% 361445
#% 405391
#% 442970
#% 443038
#% 445155
#% 463572
#% 467630
#% 480934
#% 497488
#% 527793
#% 527800
#% 546596
#% 558691
#! Many different applications in different areas need to deal with both: 1) databases, in order to take into account large amounts of structured data, and 2) quantitative and qualitative temporal constraints about such data. In this paper, we propose an approach that extends: 1) temporal databases, and 2) artificial intelligence temporal reasoning techniques and integrate them in order to face such a need. Regarding temporal reasoning, we consider some results that we proved recently about efficient query answering in the Simple Temporal Problem framework and we extend them in order to deal with partitioned sets of constraints and to support relational database operations. Regarding databases, we extend the relational model in order to consider also qualitative and quantitative temporal constraints both in the data (data expressiveness) and in the queries (query expressiveness). We then propose a modular architecture integrating a relational database with a temporal reasoner. We also consider classes of applications that fit into our approach and consider patient management in a hospital as an example.

#index 443322
#* Linear Spiral Hashing for Expansible Files
#@ Ye-In Chang;Chien-I. Lee;Wann-Bay ChangLiaw
#t 1999
#c 7
#% 512
#% 538
#% 1387
#% 8826
#% 38226
#% 201617
#% 286252
#% 286310
#% 286834
#% 287020
#% 287088
#% 287317
#% 295947
#% 319055
#% 442791
#% 462038
#% 462495
#% 479592
#% 479761
#% 482041
#% 681091
#! The goal of dynamic hashing is to design a function and a file structure that allow the address space allocated to the file to be increased and reduced without reorganizing the whole file. In this paper, we propose a new scheme for dynamic hashing in which the growth of a file occurs at a rate of $\frac{n+k}{n}$ per full expansion, where $n$ is the number of pages of the file and $k$ is a given integer constant which is smaller than $n$, as compared to a rate of two in linear hashing. Like linear hashing, the proposed scheme (called linear spiral hashing) requires no index; however, the proposed scheme may or may not add one more physical page, instead of always adding one more page in linear hashing, when a split occurs. Therefore, linear spiral hashing can maintain a more stable performance through the file expansions and have much better storage utilization than linear hashing. From our performance analysis, linear spiral hashing can achieve nearly 97 percent storage utilization as compared to 78 percent storage utilization by using linear hashing, which is also verified by a simulation study.

#index 443323
#* Knowledge Discovery by Inductive Neural Networks
#@ LiMin Fu
#t 1999
#c 7
#% 697
#% 66937
#% 68820
#% 92148
#% 136350
#% 175368
#% 376683
#% 1860296
#! A new neural network model for inducing symbolic knowledge from empirical data is presented. This model capitalizes on the fact that the certainty-factor-based activation function can improve the network generalization performance from a limited amount of training data. The formal properties of the procedure for extracting symbolic knowledge from such a trained neural network are investigated. In the domain of molecular genetics, a case study demonstrated that the described learning system effectively discovered the prior domain knowledge with some degree of refinement. Also, in cross-validation experiments, the system outperformed C4.5, a commonly used rule learning system.

#index 443324
#* 1999 Index IEEE Transactions on Knowledge and Data Engineering, Vol. 11
#@  IEEE Transactions on Knowledge and Data Engineering Staff
#t 1999
#c 7
#! First Page of the Article

#index 443325
#* Generalized Analytic Rule Extraction for Feedforward Neural Networks
#@ Amit Gupta;Sang Park;Siuwa M. Lam
#t 1999
#c 7
#% 36276
#% 57456
#% 136350
#% 155313
#% 160857
#% 163573
#% 443090
#% 675612
#% 1275289
#% 1780538
#! This paper suggests the 驴Input-Network-Training-Output-Extraction-Knowledge驴 framework to classify existing rule extraction algorithms for feedforward neural networks. Based on the suggested framework, we identify the major practices of existing algorithms as relying on the technique of generate and test, which leads to exponential complexity, relying on specialized network structure and training algorithms, which leads to limited applications and reliance on the interpretation of hidden nodes, which leads to proliferation of classification rules and their incomprehensibility. In order to generalize the applicability of rule extraction, we propose the rule extraction algorithm GeneraLized Analytic Rule Extraction (GLARE), and demonstrate its efficacy by comparing it with neural networks per se and the popular rule extraction program for decision trees, C4.5.

#index 443326
#* High Dimensional Similarity Joins: Algorithms and Performance Evaluation
#@ Nick Koudas;Kenneth C. Sevcik
#t 2000
#c 7
#% 672
#% 2115
#% 41684
#% 58369
#% 68091
#% 86950
#% 152937
#% 172949
#% 210186
#% 210187
#% 227932
#% 281750
#% 319601
#% 411694
#% 427199
#% 435141
#% 460862
#% 462070
#% 480093
#% 481609
#% 481956
#% 526847
#! Current data repositories include a variety of data types, including audio, images, and time series. State-of-the-art techniques for indexing such data and doing query processing rely on a transformation of data elements into points in a multidimensional feature space. Indexing and query processing then take place in the feature space. In this paper, we study algorithms for finding relationships among points in multidimensional feature spaces, specifically algorithms for multidimensional joins. Like joins of conventional relations, correlations between multidimensional feature spaces can offer valuable information about the data sets involved. We present several algorithmic paradigms for solving the multidimensional join problem and we discuss their features and limitations. We propose a generalization of the Size Separation Spatial Join algorithm, named Multidimensional Spatial Join (MSJ), to solve the multidimensional join problem. We evaluate MSJ along with several other specific algorithms, comparing their performance for various dimensionalities on both real and synthetic multidimensional data sets. Our experimental results indicate that MSJ, which is based on space filling curves, consistently yields good performance across a wide range of dimensionalities.

#index 443327
#* Efficient Cost Models for Spatial Queries Using R-Trees
#@ Yannis Theodoridis;Emmanuel Stefanakis;Timos Sellis
#t 2000
#c 7
#% 327
#% 1331
#% 2115
#% 32898
#% 32913
#% 58369
#% 68091
#% 77928
#% 86950
#% 137887
#% 152937
#% 153260
#% 164360
#% 172908
#% 191595
#% 213974
#% 213975
#% 227868
#% 252304
#% 252608
#% 257892
#% 271917
#% 273685
#% 273886
#% 286237
#% 317933
#% 319244
#% 359751
#% 380546
#% 427199
#% 435137
#% 462218
#% 464831
#% 480093
#% 482092
#% 482095
#% 503853
#! Selection and join queries are fundamental operations in Data Base Management Systems (DBMS). Support for nontraditional data, including spatial objects, in an efficient manner is of ongoing interest in database research. Toward this goal, access methods and cost models for spatial queries are necessary tools for spatial query processing and optimization. In this paper, we present analytical models that estimate the cost (in terms of node and disk accesses) of selection and join queries using R-tree-based structures. The proposed formulae need no knowledge of the underlying R-tree structure(s) and are applicable to uniform-like and nonuniform data distributions. In addition, experimental results are presented which show the accuracy of the analytical estimations when compared to actual runs on both synthetic and real data sets.

#index 443328
#* The Effect of Buffering on the Performance of R-Trees
#@ Scott T. Leutenegger;Mario A. López
#t 2000
#c 7
#% 86950
#% 137887
#% 153260
#% 181978
#% 213975
#% 286237
#% 286835
#% 427199
#% 462059
#% 462218
#% 463430
#% 479645
#% 480093
#% 481455
#! Past R-tree studies have focused on the number of nodes visited as a metric of query performance. Since database systems usually include a buffering mechanism, we propose that the number of disk accesses is a more realistic measure of performance. We develop a buffer model to analyze the number of disk accesses required for spatial queries using R-trees. The model can be used to evaluate the quality of R-tree update operations, such as various node splitting and tree restructuring policies, as measured by query performance on the resulting tree. We use our model to study the performance of three well-known R-tree loading algorithms. We show that ignoring buffer behavior and using number of nodes accessed as a performance metric can lead to incorrect conclusions, not only quantitatively, but also qualitatively. In addition, we consider the problem of how many levels of the R-tree should be pinned in the buffer.

#index 443329
#* Indexing the Solution Space: A New Technique for Nearest Neighbor Search in High-Dimensional Space
#@ Stefan Berchtold;Daniel A. Keim;Hans-Peter Kriegel;Thomas Seidl
#t 2000
#c 7
#% 1693
#% 2115
#% 24108
#% 86792
#% 86950
#% 102772
#% 121989
#% 169940
#% 201876
#% 217292
#% 221319
#% 227856
#% 227939
#% 237187
#% 248796
#% 248797
#% 411694
#% 435141
#% 458741
#% 463414
#% 463425
#% 464195
#% 464859
#% 481956
#% 502775
#% 526849
#% 526865
#% 527026
#! Similarity search in multimedia databases requires an efficient support of nearest-neighbor search on a large set of high-dimensional points as a basic operation for query processing. As recent theoretical results show, state of the art approaches to nearest-neighbor search are not efficient in higher dimensions. In our new approach, we therefore precompute the result of any nearest-neighbor search which corresponds to a computation of the Voronoi cell of each data point. In a second step, we store conservative approximations of the Voronoi cells in an index structure efficient for high-dimensional data spaces. As a result, nearest neighbor search corresponds to a simple point query on the index structure. Although our technique is based on a precomputation of the solution space, it is dynamic, i.e., it supports insertions of new data points. An extensive experimental evaluation of our technique demonstrates the high efficiency for uniformly distributed as well as real data. We obtained a significant reduction of the search time compared to nearest neighbor search in other index structures such as the X-tree.

#index 443330
#* Sync Classes: A Framework for Optimal Scheduling of Requests in Multimedia Storage Servers
#@ Leana Golubchik;V. s. Subrahmanian;Sherry Marcus;Joachim Biskup
#t 2000
#c 7
#% 107702
#% 151340
#% 172881
#% 173593
#% 173689
#% 195083
#% 195335
#% 201678
#% 208749
#% 209901
#% 247118
#% 452791
#% 481438
#% 511160
#% 591370
#% 677163
#! There have been many proposals on how media-on-demand servers can effectively allow clients to share resources. In this paper, given a set of clients, we show how these clients may be partitioned into 驴sync-classes驴驴sets of clients who can be serviced through allocation of a single set of resources. As a set of clients may be partitioned into sync-classes in many different ways, we show that a very large class of cost functions may be used to determine which partition to choose. We provide algorithms to compute such optimal splits. Our framework is very generic in the following ways: 1) The system may plug-in any cost function whatsoever, as long as it satisfies four common-sense axioms that evaluate costs, and 2) the system may evaluate the future anticipated requests of a user using any user model (e.g., a Markovian model) that has a specified I/O interface. Thus, a wide variety of predictive methods (of what the user will do) and a wide variety of costing methods may be used within our framework.

#index 443331
#* Dynamically Negotiated Resource Management for Data Intensive Application Suites
#@ Gary J. Nutt;Scott Brandt;Adam J. Griff;Sam Siewert;Marty Humphrey;Toby Berk
#t 2000
#c 7
#% 107700
#% 122875
#% 202138
#% 202153
#% 239984
#% 239996
#% 240013
#% 520441
#% 615570
#% 979127
#% 1829998
#! In contemporary computers and networks of computers, various application domains are making increasing demands on the system to move data from one place to another, particularly under some form of soft real-time constraint. A brute force technique for implementing applications in this type of domain demands excessive system resources, even though the actual requirements by different parts of the application vary according to the way it is being used at the moment. A more sophisticated approach is to provide applications with the ability to dynamically adjust resource requirements according to their precise needs, as well as the availability of system resources. This paper describes a set of principles for designing systems to provide support for soft real-time applications using dynamic negotiation. Next, the execution level abstraction is introduced as a specific mechanism for implementing the principles. The utility of the principles and the execution level abstraction is then shown in the design of three resource managers that facilitate dynamic application adaptation: Gryphon, EPA/RT-PCIP, and the DQM architectures.

#index 443332
#* MPGS: An Interactive Tool for the Specification and Generation of Multimedia Presentations
#@ Elisa Bertino;Elena Ferrari;Marco Stolf
#t 2000
#c 7
#% 70370
#% 98072
#% 104740
#% 111874
#% 154311
#% 219846
#% 219919
#% 319244
#% 434736
#% 443220
#% 445701
#% 452790
#% 452796
#% 463902
#% 464069
#% 1848271
#! Multimedia presentations are composed of objects belonging to different data types such as video, audio, text, and image. An important aspect is that, quite often, the user defining a presentation needs to express sophisticated temporal and spatial constraints among the objects composing the presentation. In this paper, we present a system (called MPGS驴Multimedia Presentation Generator System) which supports the specification of constraints among multimedia objects and the generation of multimedia presentations according to the specified constraints. The constraint model provided by MPGS is very flexible and powerful in terms of the kinds of object constraints it can represent. A large number of innovative features are supported including: asynchronous and simultaneous spatial constraints; components of interest and priority levels; motion functions. Obviously, the flexibility provided to the users requires the development of nontrivial techniques to check constraint consistency and to generate a presentation satisfying the specified constraints. In this paper, we illustrate the solutions we have devised in the framework of MPGS.

#index 443333
#* Natural Language Grammatical Inference with Recurrent Neural Networks
#@ Steve Lawrence;C. Lee Giles;Sandiway Fong
#t 2000
#c 7
#% 57104
#% 90394
#% 91872
#% 92195
#% 92636
#% 96671
#% 106561
#% 111449
#% 111451
#% 132588
#% 132674
#% 132675
#% 169320
#% 170665
#% 186991
#% 204015
#% 212288
#% 273657
#% 376589
#% 390964
#% 408458
#% 442952
#% 443022
#% 476887
#% 496878
#% 748810
#% 1051414
#% 1780637
#% 1860109
#! This paper examines the inductive inference of a complex grammar with neural networks驴specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-through-time training algorithm, significant learning was possible. It was found that certain architectures are better able to learn an appropriate grammar. The operation of the networks and their training is analyzed. Finally, the extraction of rules in the form of deterministic finite state automata is investigated.

#index 443334
#* Correction to 'Server Capacity Planning for Web Traffic Workload'
#@ Krishna Kant;Youjip Won
#t 2000
#c 7
#! First Page of the Article

#index 443335
#* An Architecture for Survivable Coordination in Large Distributed Systems
#@ Dahlia Malkhi;Michael K. Reiter
#t 2000
#c 7
#% 31789
#% 65721
#% 73103
#% 86476
#% 87350
#% 124016
#% 131567
#% 172267
#% 176254
#% 181249
#% 193149
#% 210194
#% 233524
#% 233532
#% 250120
#% 267204
#% 287303
#% 315625
#% 318270
#% 319994
#% 320187
#% 484086
#% 513683
#% 588258
#% 596060
#% 602675
#% 616876
#% 617353
#% 617375
#% 617431
#% 787256
#! Coordination among processes in a distributed system can be rendered very complex in a large-scale system where messages may be delayed or lost and when processes may participate only transiently or behave arbitrarily, e.g., after suffering a security breach. In this paper, we propose a scalable architecture to support coordination in such extreme conditions. Our architecture consists of a collection of persistent data servers that implement simple shared data abstractions for clients, without trusting the clients or even the servers themselves. We show that, by interacting with these untrusted servers, clients can solve distributed consensus, a powerful and fundamental coordination primitive. Our architecture is very practical and we describe the implementation of its main components in a system called Fleet.

#index 443336
#* The Cost of Recovery in Message Logging Protocols
#@ Sriram Rao;Lorenzo Alvisi;Harrick M. Vin
#t 2000
#c 7
#% 1823
#% 86476
#% 131565
#% 162115
#% 202981
#% 223286
#% 320187
#% 380564
#% 445921
#% 594332
#% 602709
#% 617371
#% 631916
#% 697556
#! Past research in message logging has focused on studying the relative overhead imposed by pessimistic, optimistic, and causal protocols during failure-free executions. In this paper, we give the first experimental evaluation of the performance of these protocols during recovery. Our results suggest that applications face a complex trade-off when choosing a message logging protocol for fault tolerance. On the one hand, optimistic protocols can provide fast failure-free execution and good performance during recovery, but are complex to implement and can create orphan processes. On the other hand, orphan-free protocols either risk being slow during recovery, e.g., sender-based pessimistic and causal protocols, or incur a substantial overhead during failure-free execution, e.g., receiver-based pessimistic protocols. To address this trade-off, we propose hybrid logging protocols, a new class of orphan-free protocols. We show that hybrid protocols perform within two percent of causal logging during failure-free execution and within two percent of receiver-based logging during recovery.

#index 443337
#* Design and Analysis of an Integrated Checkpointing and Recovery Scheme for Distributed Applications
#@ Bina Ramamurthy;Shambhu Upadhyaya;Bharat Bhargava
#t 2000
#c 7
#% 3527
#% 41837
#% 131565
#% 207597
#% 224698
#% 285813
#% 320187
#% 434221
#% 442239
#% 442279
#% 445921
#% 594229
#% 636226
#! An integrated checkpointing and recovery scheme which exploits the low latency and high coverage characteristics of a concurrent error detection scheme is presented. Message dependency which is the main source of multistep rollback in distributed systems is minimized by using a new message validation technique derived from the notion of concurrent error detection. The concept of a new global state matrix is introduced to track error checking and message dependency in a distributed system and assist in the recovery. The analytical model, algorithms, and data structures to support an easy implementation of the new scheme are presented. The completeness and correctness of the algorithms are proved. A number of scenarios and illustrations that give the details of the analytical model are presented. The benefits of the integrated checkpointing scheme are quantified by means of simulation using an object-oriented test framework.

#index 443338
#* Hierarchical Error Detection in a Software Implemented Fault Tolerance (SIFT) Environment
#@ Saurabh Bagchi;Balaji Srinivasan;Keith Whisnant;Zbigniew Kalbarczyk;Ravishankar K. Iyer
#t 2000
#c 7
#% 146399
#% 204909
#% 204910
#% 204912
#% 224698
#% 276987
#% 318270
#% 394812
#% 437687
#% 439863
#% 442020
#% 617350
#% 617354
#% 656822
#! This paper proposes a hierarchical error detection framework for a Software Implemented Fault Tolerance (SIFT) layer of a distributed system. A four-level error detection hierarchy is proposed in the context of Chameleon, a software environment for providing adaptive fault-tolerance in an environment of commercial off-the-shelf (COTS) system components and software. The design and implementation of a software-based distributed signature monitoring scheme, which is central to the proposed four-level hierarchy, is described. Both intralevel and interlevel optimizations that minimize the overhead of detection and are capable of adapting to runtime requirements are proposed. The paper presents results from a prototype implementation of two levels of the error detection hierarchy and results of a detailed simulation of the overall environment. The results indicate a substantial increase in availability due to the detection framework and help in understanding the trade-offs between overhead and coverage for different combinations of techniques.

#index 443339
#* An Efficient Evaluation of a Fuzzy Equi-Join Using Fuzzy Equality Indicators
#@ Weining Zhang;Ke Wang
#t 2000
#c 7
#% 2783
#% 48721
#% 72031
#% 90725
#% 166819
#% 383684
#% 462792
#% 463888
#% 464038
#% 480774
#! We propose a new measure of fuzzy equality comparison based on the similarity of possibility distributions. we define a type of fuzzy equi驴join based on the new fuzzy equality comparison, and allow threshold values to be associated with predicates of the join condition. A sort驴merge join algorithm based on a partial order of intervals is used to evaluate the fuzzy equi-join. In order for the evaluation to be efficient, we identify various mappings, called fuzzy equality (FE) indicators, that will determine appropriate intervals for fuzzy data with different characteristics. Experiment results from our preliminary simulation of the algorithm show a significant improvement of efficiency when FE indicators are used with the sort驴merge join algorithm.

#index 443340
#* An Implementation of Logical Analysis of Data
#@ Endre Boros;Peter L. Hammer;Toshihide Ibaraki;Alexander Kogan;Eddy Mayoraz;Ilya Muchnik
#t 2000
#c 7
#% 69612
#% 92542
#% 117656
#% 156186
#% 167633
#% 257874
#% 269449
#% 480746
#! The paper describes a new, logic-based methodology for analyzing observations. The key features of the Logical Analysis of Data (LAD) are the discovery of minimal sets of features necessary for explaining all observations and the detection of hidden patterns in the data capable of distinguishing observations describing 驴positive驴 outcome events from 驴negative驴 outcome events. Combinations of such patterns are used for developing general classification procedures. An implementation of this methodology is described in the paper along with the results of numerical experiments demonstrating the classification performance of LAD in comparison with the reported results of other procedures. In the final section, we describe three pilot studies on applications of LAD to oil exploration, psychometric testing, and the analysis of developments in the Chinese transitional economy. These pilot studies demonstrate not only the classification power of LAD, but also its flexibility and capability to provide solutions to various case-dependent problems.

#index 443341
#* Optimization and Evaluation of Disjunctive Queries
#@ Jens Claussen;Alfons Kemper;Guido Moerkotte;Klaus Peithner;Michael Steinbrunn
#t 2000
#c 7
#% 32889
#% 32890
#% 43162
#% 58359
#% 58377
#% 90641
#% 103369
#% 114709
#% 116090
#% 136740
#% 152940
#% 152942
#% 172931
#% 210206
#% 235914
#% 286916
#% 287034
#% 318049
#% 320217
#% 395669
#% 411554
#% 480443
#% 481121
#% 481293
#% 481621
#% 481915
#% 564428
#% 565457
#! It is striking that the optimization of disjunctive queries驴i.e., those which contain at least one or-connective in the query predicate驴has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the true-stream with tuples satisfying the selection (join) predicate and the false-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to 驴bypass驴 costly predicates whenever the 驴fate驴 of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building block approach. We show that our evaluation technique allows to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: One is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by 驴moving驴 all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNF- or DNF-based query predicate.

#index 443342
#* Consistent Schema Version Removal: An Optimization Technique for Object-Oriented Views
#@ Viviane M. Crestana-Jensen;Amy J. Lee;Elke A. Rundensteiner
#t 2000
#c 7
#% 32903
#% 102780
#% 152904
#% 169812
#% 172327
#% 199805
#% 221756
#% 238157
#% 443145
#% 458608
#% 462053
#% 462199
#% 464234
#% 480958
#% 487590
#% 534878
#% 563577
#% 581613
#% 614595
#! Powerful solutions enabling interoperability must allow applications to evolve and requirements of shared databases to change, while minimizing such changes on other integrated applications. Several approaches, such as the transparent schema evolution system (TSE) by Ra et al., schema versions by Lautemann, and integrated views by Bertino, have been proposed to make interoperability possible by using object-oriented techniques. These approaches may generate a large number of schema versions over time resulting in an excessive build-up of classes and underlying object instances, not all being necessarily still in use. This results in degradation of system performance due to the view maintenance and the storage overhead costs. In this paper, we address the problem of removing obsolete view schemas. We characterize four potential problems of schema consistency that could be caused by removal of a single derived class. We demonstrate that schema version removal is sensitive to the order in which individual classes are processed, and present a formal dependency model that captures all dependencies between classes as logic clauses and manipulates them to make decisions on class deletions and nondeletions while guaranteeing the consistency of the schema. We have also developed and proven consistent a dependency graph (DG) representation of the formal model. Lastly, we present a cost model for evaluating alternative removal patterns on DG to assure selection of the optimal solution. The proposed techniques have been implemented in our Schema View Removal (SVR) tool. Lastly, we report experimental findings for applying our techniques for consistent schema version removal on the MultiView/TSE system.

#index 443343
#* Justification for Inclusion Dependency Normal Form
#@ Mark Levene;Millist W. Vincent
#t 2000
#c 7
#% 16
#% 4279
#% 36683
#% 59779
#% 69283
#% 103700
#% 125557
#% 136347
#% 286860
#% 287295
#% 287333
#% 287754
#% 289424
#% 411570
#% 415979
#% 463270
#% 463853
#% 480250
#% 527109
#% 836134
#! Functional dependencies (FDs) and inclusion dependencies (INDs) are the most fundamental integrity constraints that arise in practice in relational databases. In this paper, we address the issue of normalization in the presence of FDs and INDs and, in particular, the semantic justification for Inclusion Dependency Normal Form (IDNF), a normal form which combines Boyce-Codd normal form with the restriction on the INDs that they be noncircular and key-based. We motivate and formalize three goals of database design in the presence of FDs and INDs: noninteraction between FDs and INDs, elimination of redundancy and update anomalies, and preservation of entity integrity. We show that, as for FDs, in the presence of INDs being free of redundancy is equivalent to being free of update anomalies. Then, for each of these properties, we derive equivalent syntactic conditions on the database design. Individually, each of these syntactic conditions is weaker than IDNF and the restriction that an FD not be embedded in the righthand side of an IND is common to three of the conditions. However, we also show that, for these three goals of database design to be satisfied simultaneously, IDNF is both a necessary and sufficient condition.

#index 443344
#* Efficient Subgraph Isomorphism Detection: A Decomposition Approach
#@ Bruno T. Messmer;Horst Bunke
#t 2000
#c 7
#% 57793
#% 65345
#% 82762
#% 93225
#% 100193
#% 116623
#% 124679
#% 131680
#% 136318
#% 183568
#% 288990
#% 408396
#% 443891
#% 461330
#% 465863
#! Graphs are a powerful and universal data structure useful in various subfields of science and engineering. In this paper, we propose a new algorithm for subgraph isomorphism detection from a set of a priori known model graphs to an input graph that is given online. The new approach is based on a compact representation of the model graphs that is computed offline. Subgraphs that appear multiple times within the same or within different model graphs are represented only once, thus reducing the computational effort to detect them in an input graph. In the extreme case where all model graphs are highly similar, the run-time of the new algorithm becomes independent of the number of model graphs. Both a theoretical complexity analysis and practical experiments characterizing the performance of the new approach will be given.

#index 443345
#* Optimal Data Placement on Disks: A Comprehensive Solution for Different Technologies
#@ Peter Triantafillou;Stavros Christodoulakis;Costas Georgiadis
#t 2000
#c 7
#% 18669
#% 43195
#% 57049
#% 159079
#% 187204
#% 237195
#% 374002
#! The problem of optimally placing data on disks (ODP) to maximize disk-access performance has long been recognized as important. Solutions to this problem have been reported for some widely available disk technologies, such as magnetic CAV and optical CLV disks. However, important new technologies such as multizoned magnetic disks, have been recently introduced. For such technologies no formal solution to the ODP problem has been reported. In this paper, we first identify the fundamental characteristics of disk-device technologies which influence the solution to the ODP problem. We develop a comprehensive solution to the problem that covers all currently available disk technologies. We show how our comprehensive solution can be reduced to the solutions for existing disk technologies, contributing thus a solution to the ODP problem for multizoned disks. Our analytical solution has been validated through simulations and through its reduction to the known solutions for particular disks. Finally, we study how the solution for multizoned disks is affected by the disk and data characteristics.

#index 443346
#* A Generalized Definition of Rough Approximations Based on Similarity
#@ Roman Slowinski;Daniel Vanderpooten
#t 2000
#c 7
#% 38847
#% 198097
#% 365894
#% 366687
#! This paper proposes new definitions of lower and upper approximations, which are basic concepts of the rough set theory. These definitions follow naturally from the concept of ambiguity introduced in this paper. The new definitions are compared to the classical definitions and are shown to be more general, in the sense that they are the only ones which can be used for any type of indiscernibility or similarity relation.

#index 443347
#* The PSTR/SNS Scheme for Real-Time Fault Tolerance via Active Object Replication and Network Surveillance
#@ K. H. (Kane) Kim;Chittur Subbaraman
#t 2000
#c 7
#% 124146
#% 187392
#% 187409
#% 218684
#% 437028
#% 437761
#% 443275
#% 615184
#% 622847
#% 622865
#% 622876
#! The time-triggered message-triggered object (TMO) scheme was formulated a few years ago as a major extension of the conventional object structuring schemes with the idealistic goal of facilitating general-form design and timeliness-guaranteed design of complex real-time application systems. Recently, as a new scheme for realizing TMO-structured distributed and parallel computer systems capable of both hardware and software fault tolerance, we have formulated and demonstrated the primary-shadow TMO replication (PSTR) scheme. An important new extension of the PSTR scheme discussed in this paper is an integration of the PSTR scheme and a network surveillance (NS) scheme. This extension results in a significant improvement in the fault coverage and recovery time bound achieved. The NS scheme adopted is a recently developed scheme effective in a wide range of point-to-point networks and it is called the supervisor-based NS (SNS) scheme. The integration of the PSTR scheme and the SNS scheme is called the PSTR/SNS scheme. The recovery time bound of the PSTR/SNS scheme is analyzed on the basis of an implementation model that can be easily adapted to various commercial operating system kernels.

#index 443348
#* Scalable Parallel Data Mining for Association Rules
#@ Eui-Hong (Sam) Han;George Karypis;Vipin Kumar
#t 2000
#c 7
#% 25998
#% 140385
#% 152934
#% 199538
#% 201894
#% 227922
#% 340290
#% 420067
#% 443085
#% 443091
#% 463883
#% 481100
#% 481290
#% 481754
#% 481758
#% 678196
#! In this paper, we propose two new parallel formulations of the Apriori algorithm that is used for computing association rules. These new formulations, IDD and HD, address the shortcomings of two previously proposed parallel formulations CD and DD. Unlike the CD algorithm, the IDD algorithm partitions the candidate set intelligently among processors to efficiently parallelize the step of building the hash tree. The IDD algorithm also eliminates the redundant work inherent in DD, and requires substantially smaller communication overhead than DD. But IDD suffers from the added cost due to communication of transactions among processors. HD is a hybrid algorithm that combines the advantages of CD and DD. Experimental results on a 128-processor Cray T3E show that HD scales just as well as the CD algorithm with respect to the number of transactions, and scales as well as IDD with respect to increasing candidate set size.

#index 443349
#* Discovering Structural Association of Semistructured Data
#@ Ke Wang;Huiqing Liu
#t 2000
#c 7
#% 152934
#% 210214
#% 262071
#% 340295
#% 462062
#% 463919
#% 464720
#% 481290
#% 481602
#! Many semistructured objects are similarly, though not identically, structured. We study the problem of discovering 驴typical驴 substructures of a collection of semistructured objects. The discovered structures can serve the following purposes: 1) the 驴table-of-contents驴 for gaining general information of a source, 2) a road map for browsing and querying information sources, 3) a basis for clustering documents, 4) partial schemas for providing standard database access methods, and 5) user/customer's interests and browsing patterns. The discovery task is impacted by structural features of semistructured data in a nontrivial way and traditional data mining frameworks are inapplicable. We define this discovery problem and propose a solution.

#index 443350
#* Scalable Algorithms for Association Mining
#@ Mohammed J. Zaki
#t 2000
#c 7
#% 124750
#% 152934
#% 171557
#% 201075
#% 201894
#% 227917
#% 227922
#% 232136
#% 237200
#% 248791
#% 248813
#% 288779
#% 340289
#% 340291
#% 408396
#% 420067
#% 443091
#% 459020
#% 462219
#% 463883
#% 464714
#% 481290
#% 481754
#% 481779
#% 614619
#! Association rule discovery has emerged as an important problem in knowledge discovery and data mining. The association mining task consists of identifying the frequent itemsets and then, forming conditional implication rules among them. In this paper, we present efficient algorithms for the discovery of frequent itemsets which forms the compute intensive phase of the task. The algorithms utilize the structural properties of frequent itemsets to facilitate fast discovery. The items are organized into a subset lattice search space, which is decomposed into small independent chunks or sublattices, which can be solved in memory. Efficient lattice traversal techniques are presented which quickly identify all the long frequent itemsets and their subsets if required. We also present the effect of using different database layout schemes combined with the proposed decomposition and traversal techniques. We experimentally compare the new algorithms against the previous approaches, obtaining improvements of more than an order of magnitude for our test databases.

#index 443351
#* Response Time Analysis of OPS5 Production Systems
#@ Albert Mo Kim Cheng;Jeng-Rung Chen
#t 2000
#c 7
#% 25469
#% 37905
#% 74396
#% 94458
#% 95979
#% 116045
#% 179005
#% 442733
#% 445781
#% 445870
#! This paper focuses on the problem of determining a priori the maximal response time of rule-based programs. The response time analysis problem is an important problem, especially for real-time systems. We study this problem in the context of OPS5 production systems. Two aspects of the response time of a program are investigated, the maximal number of rule firings and the maximal number of basic comparisons made by the Rete network during the execution of the program. The response time analysis problem is in general undecidable. However, a program terminates in a finite time if the rule triggering pattern of this program satisfies certain conditions. In this paper, we present four such termination conditions for OPS5 production systems. An algorithm for computing an upper bound on the number of rule firings is then given. To have a better idea of the time required during execution, we present an algorithm that computes the maximal time required during the match phase in terms of the number of comparisons made by the Rete network. This measurement is sufficient since the match phase consumes about 90 percent of the execution time.

#index 443352
#* Load Sharing in Distributed Multimedia-on-Demand Systems
#@ Y. c. Tay;HweeHwa Pang
#t 2000
#c 7
#% 1252
#% 58312
#% 60811
#% 66336
#% 68531
#% 84047
#% 89837
#% 114554
#% 137443
#% 143893
#% 151575
#% 156993
#% 190324
#% 195336
#% 201693
#% 204537
#% 204538
#% 243547
#% 319255
#% 319580
#% 320187
#% 374261
#% 437045
#% 442400
#% 443279
#% 445670
#% 520280
#% 602776
#% 603387
#% 631846
#% 631878
#% 660965
#% 835744
#! Service providers have begun to offer multimedia-on-demand services to residential estates by installing isolated, small-scale multimedia servers at individual estates. Such an arrangement allows the service providers to operate without relying on a high-speed, large-capacity metropolitan area network, which is still not available in many countries. Unfortunately, installing isolated servers could incur very high server costs, as each server requires spare bandwidth to cope with fluctuations in user demand. In this paper, we explore the feasibility of linking up several small multimedia servers to a (limited-capacity) network, and allowing servers with idle retrieval bandwidth to help out servers that are temporarily overloaded; the goal is to minimize the waiting time for service to begin. We identify four characteristics of load sharing in a distributed multimedia system that differentiate it from load balancing in a conventional distributed system. We then introduce a GWQ load sharing algorithm that fits and exploits these characteristics; it puts all servers' pending requests in a global queue, from which a server with idle capacity obtains additional jobs. The performance of the algorithm is captured by an analytical model, which we validate through simulations. Both the analytical and simulation models show that the algorithm vastly reduces wait times at the servers. The analytical model also provides guidelines for capacity planning. Finally, we propose an enhanced GWQ+L algorithm that allows a server to reclaim active local requests that are being serviced remotely. Simulation experiments indicate that the scheduling decisions of GWQ+L are optimal, in the sense that it enables the distributed servers to approximate the performance of a large centralized server.

#index 443353
#* Real-Time Index Concurrency Control
#@ Jayant R. Haritsa;S. Seshadri
#t 2000
#c 7
#% 748
#% 6716
#% 36118
#% 37968
#% 101820
#% 102808
#% 116085
#% 116087
#% 124815
#% 172910
#% 268786
#% 286929
#% 288821
#% 291189
#% 317933
#% 339349
#% 339357
#% 427199
#% 435110
#% 480266
#% 480589
#% 480965
#% 481601
#% 531907
#% 561427
#! Real-time database systems are expected to rely heavily on indexes to speed up data access and, thereby, help more transactions meet their deadlines. Accordingly, high-performance index concurrency control (ICC) protocols are required to prevent contention for the index from becoming a bottleneck. In this paper, we develop real-time variants of a representative set of classical B-tree ICC protocols and, using a detailed simulation model, compare their performance for real-time transactions with firm deadlines. We also present and evaluate a new real-time ICC protocol called GUARD-link that augments the classical B-link protocol with a feedback-based admission control mechanism. Both point and range queries, as well as the undos of the index actions of aborted transactions are included in the scope of our study. The performance metrics used in evaluating the ICC protocols are the percentage of transactions that miss their deadlines and the fairness with respect to transaction type and size. Our experimental results show that the performance characteristics of the real-time version of an ICC protocol could be significantly different from the performance of the same protocol in a conventional (nonreal-time) database system. In particular, B-link protocols, which are reputed to provide the best overall performance in conventional database systems, perform poorly under heavy real-time loads. The new GUARD-link protocol, however, although based on the B-link approach, delivers the best performance (with respect to all performance metrics) for a variety of real-time transaction workloads, by virtue of its admission control mechanism. In fact, GUARD-link provides close to ideal fairness in most environments. These and other results presented here represent the first work in the area of real-time index concurrency control.

#index 443354
#* A Hybrid Representation of Vague Collections for Distributed Object Management Systems
#@ Oliver Haase;Andreas Henrich
#t 2000
#c 7
#% 663
#% 32879
#% 64412
#% 68140
#% 84656
#% 108510
#% 112498
#% 120862
#% 161102
#% 199860
#% 287007
#% 287313
#% 287333
#% 339614
#% 395668
#% 411572
#% 415959
#% 481602
#% 526851
#% 562183
#% 616343
#! An important characteristic of distributed object management systems is that due to network or machine failure, the environment may become partitioned into subenvironments that cannot communicate with each other. In some application scenarios, it is important that the subenvironments remain operable even in this case. In particular, queries should be processed in an appropriate way. To this end, the final and all intermediate results of a query in a distributed object management system must be regarded as potentially vague. In this paper, we propose a hybrid representation for vague sets and vague multisets designed for this application context. The representation consists of an enumerating part, which contains the elements we could access during query processing, and a descriptive part, which describes the relevant elements we could not access. We introduce Propagation Rules which can be used to minimize the vagueness of a query result represented in this hybrid way. The main advantage of our approach is that the descriptive part of the representation can be used to improve the enumerating part during query processing.

#index 443355
#* Dynamic Taxonomies: A Model for Large Information Bases
#@ Giovanni M. Sacco
#t 2000
#c 7
#% 58894
#% 68000
#% 90641
#% 118771
#% 144023
#% 161722
#% 214711
#% 375017
#% 604316
#! A new taxonomic model for structuring and accessing large heterogeneous information bases is presented. The model is designed to simplify both classification and access by computer-illiterate people. It defines simple and intuitive operations to access large information bases at the conceptual level and at different levels of abstraction, in a totally assisted way, through a simple, yet effective visual interface. The model can also be used to summarize result sets computed by other query methods, such as information retrieval, shape retrieval, etc., and to provide user maps for complex hypermedia networks. The experience gained by applying this model to commercial applications is reported.

#index 443356
#* Building Probabilistic Networks: 'Where Do the Numbers Come From?' Guest Editors' Introduction
#@ Marek J. Druzdzel;Linda C. van der Gaag
#t 2000
#c 7
#% 44876
#% 59918
#% 129987
#% 197387
#% 217078
#% 237038
#% 277396
#% 388024
#% 443025
#% 527860
#% 528027
#% 1650338
#% 1650615
#% 1650644
#% 1650789
#% 1784146
#% 1784188
#! First Page of the Article

#index 443357
#* Network Engineering for Agile Belief Network Models
#@ Kathryn Blackmond Laskey;Suzanne M. Mahoney
#t 2000
#c 7
#% 158
#% 39317
#% 44876
#% 95671
#% 101218
#% 101231
#% 101237
#% 107414
#% 115083
#% 116366
#% 128428
#% 147677
#% 380725
#% 527832
#% 527848
#% 1650310
#% 1650607
#% 1650680
#% 1650731
#% 1650734
#% 1650789
#! The construction of a large, complex belief network model, like any major system development effort, requires a structured process to manage system design and development. This paper describes a belief network engineering process based on the spiral system lifecycle model. The problem of specifying numerical probability distributions for random variables in a belief network is best treated not in isolation, but within the broader context of the system development effort as a whole. Because structural assumptions determine which numerical probabilities or parameter values need to be specified, there is an interaction between specification of structure and parameters. Evaluation of successive prototypes serves to refine system requirements, ensure that modeling and elicitation effort are focused productively, and prioritize directions of enhancement and improvement for future prototypes. Explicit representation of semantic information associated with probability assessments facilitates tracing of the rationale for modeling decisions, as well as supporting maintenance and enhancement of the knowledge base.

#index 443358
#* Dealing with the Expert Inconsistency in Probability Elicitation
#@ Stefano Monti;Giuseppe Carenini
#t 2000
#c 7
#% 129987
#% 197387
#% 217078
#% 277396
#% 527665
#% 559045
#! In this paper, we present and discuss our experience in the task of probability elicitation from experts for the purpose of belief network construction. In our study, we applied four techniques. Three of these techniques are available from the literature, whereas the fourth one is a technique that we developed by adapting a method for the assessment of preferences to the task of probability elicitation. The new technique is based on the Analytic Hierarchy Process (AHP) proposed by Saaty [12], [13], and it allows for the quantitative assessment of the expert inconsistency. The method is, in our opinion, very promising and lends itself to be applied more extensively to the task of probability elicitation.

#index 443359
#* Constructing Bayesian Networks for Medical Diagnosis from Incomplete and Partially Correct Statistics
#@ Daniel Nikovski
#t 2000
#c 7
#% 44876
#% 89748
#% 383276
#% 527688
#! The paper discusses several knowledge engineering techniques for the construction of Bayesian networks for medical diagnostics when the available numerical probabilistic information is incomplete or partially correct. This situation occurs often when epidemiological studies publish only indirect statistics and when significant unmodeled conditional dependence exists in the problem domain. While nothing can replace precise and complete probabilistic information, still a useful diagnostic system can be built with imperfect data by introducing domain-dependent constraints. We propose a solution to the problem of determining the combined influences of several diseases on a single test result from specificity and sensitivity data for individual diseases. We also demonstrate two techniques for dealing with unmodeled conditional dependencies in a diagnostic network. These techniques are discussed in the context of an effort to design a portable device for cardiac diagnosis and monitoring from multimodal signals.

#index 443360
#* A Causal Probabilistic Network for Optimal Treatment of Bacterial Infections
#@ Leonard Leibovici;Michal Fishman;Henrik C. Schønheyder;Christian Riekehr;Brian Kristensen;Ilana Shraga;Steen Andreassen
#t 2000
#c 7
#% 420055
#! The fatality rate associated with severe bacterial infections is about 30 percent and appropriate antibiotic treatment reduces it by half. Unfortunately, about a third of antibiotic treatments prescribed by physicians are inappropriate. We have built a causal probabilistic network (CPN) for treatment of severe bacterial infections. The net is based on modules, each module representing a site of infection. The general configuration of a module is as follows: Major distribution factors define groups of patients, each of them with a definite prevalence of infection caused by a given pathogen. Minor distribution factors multiply the likelihood of one pathogen, without changing much of the prevalence of infection. Infection caused by a pathogen causes local and generalized signs and symptoms. Antibiotic treatment is appropriate if it matches the susceptibility of the pathogens in vitro and appropriate treatment is associated with a gain in life expectancy. This is balanced against the cost of the drug, side effects, and ecological damage, to reach the most cost effective treatment. The net was constructed in such a way that the data for the conditional probability tables will be available, even if it meant sometimes giving up on fine modeling details. For data, we used large databases collected by us in the last 10 years and data from the literature. The CPN was a convenient way to combine data from databases collected at different locations and times with published information. Although the net is based on detailed and large databases, its calibration to new sites requires data that is available in most modern hospitals.

#index 443361
#* Logic-Based Query Optimization for Object Databases
#@ John Grant;Jarek Gryz;Jack Minker;Louiqa Raschid
#t 2000
#c 7
#% 53390
#% 69272
#% 72036
#% 86954
#% 111368
#% 125614
#% 162221
#% 169844
#% 172874
#% 188853
#% 198465
#% 198473
#% 199580
#% 201898
#% 248788
#% 442677
#% 459242
#% 459251
#% 462033
#% 462066
#% 462171
#% 463100
#% 463266
#% 464053
#% 464056
#% 464203
#% 481101
#% 481916
#% 564419
#% 571216
#! We present a technique for transferring query optimization techniques, developed for relational databases, into object databases. We demonstrate this technique for ODMG database schemas defined in ODL and object queries expressed in OQL. The object schema is represented using a logical representation (Datalog). Semantic knowledge about the object data model, e.g., class hierarchy information, relationship between objects, etc., as well as semantic knowledge about a particular schema and application domain are expressed as integrity constraints. An OQL object query is represented as a logic query and query optimization is performed in the Datalog representation. We obtain equivalent (optimized) logic queries and, subsequently, obtain equivalent (optimized) OQL queries for each equivalent logic query. In this paper, we present one optimization technique for semantic query optimization (SQO) based on the residue technique of [6], [7], [8]. We show that our technique generalizes previous research on SQO for object databases. We handle a large class of OQL queries, including queries with constructors and methods. We demonstrate how SQO can be used to eliminate queries which contain contradictions and simplify queries, e.g., by eliminating joins, or by reducing the access scope for evaluating a query to some specific subclass(es). We also demonstrate how the definition of a method, or integrity constraints describing the method, can be used in optimizing a query with a method.

#index 443362
#* View Operations on Objects with Roles for a Statically Typed Database Language
#@ Antonio Albano;Giuseppe Antognoni;Giorgio Ghelli
#t 2000
#c 7
#% 1746
#% 65540
#% 83526
#% 83537
#% 83541
#% 89644
#% 96336
#% 99437
#% 102780
#% 102788
#% 164390
#% 168712
#% 180650
#% 189746
#% 206909
#% 207804
#% 238157
#% 381758
#% 435152
#% 458608
#% 480958
#% 481095
#% 487926
#% 562122
#% 562146
#% 571085
#! To deal with the evolution of data and applications and with the existence of multiple views for the same data, the object data model needs to be extended with two different sets of operations: object extension operations, to allow an object to dynamically change its type, and object viewing operations, to allow an object to be seen as if it had a different structure. Object extension and object viewing operations are related in that they are both identity-preserving operations, but different in that object extension may modify the behavior of the original object while object viewing creates a new view for the original object without modifying its behavior. In this paper, a set of object viewing operations is defined in the context of a statically and strongly typed database programming language which supports objects with roles, and the relationships with object extension and role mechanisms are discussed. We then show how the object viewing operations can be used to give the semantics of a higher level mechanism to define views for object databases. Examples of the use of these operations are given with reference to the prototype implementation of the language Galileo 97.

#index 443363
#* Efficiently Supporting Temporal Granularities
#@ Curtis E. Dyreson;William S. Evans;Hong Lin;Richard Thomas Snodgrass
#t 2000
#c 7
#% 99137
#% 123589
#% 125967
#% 153645
#% 160425
#% 162238
#% 186970
#% 189808
#% 199809
#% 207552
#% 238946
#% 240149
#% 259487
#% 286239
#% 287688
#% 319244
#% 361445
#% 442919
#% 443161
#% 463738
#! Granularity is an integral feature of temporal data. For instance, a person's age is commonly given to the granularity of years and the time of their next airline flight to the granularity of minutes. A granularity creates a discrete image, in terms of granules, of a (possibly continuous) time-line. We present a formal model for granularity in temporal operations that is integrated with temporal indeterminacy, or 驴don't know when驴 information. We also minimally extend the syntax and semantics of SQL-92 to support mixed granularities. This support rests on two operations, scale and cast, that move times between granularities, e.g., from days to months. We demonstrate that our solution is practical by showing how granularities can be specified in a modular fashion, and by outlining a time- and space-efficient implementation. The implementation uses several optimization strategies to mitigate the expense of accommodating multiple granularities.

#index 443364
#* Trigger Inheritance and Overriding in an Active Object Database System
#@ Elisa Bertino;Giovanna Guerrini;Isabella Merlo
#t 2000
#c 7
#% 58361
#% 73005
#% 77680
#% 86335
#% 86944
#% 123085
#% 124757
#% 125951
#% 152921
#% 169799
#% 197480
#% 211931
#% 226837
#% 249687
#% 265119
#% 384978
#% 394417
#% 459001
#% 464221
#% 480600
#% 480621
#% 480765
#% 480779
#% 481457
#% 487576
#% 487926
#% 501940
#% 517883
#% 531284
#! An active database is a database in which some operations are automatically executed when specified events happen and particular conditions are met. Several systems supporting active rules in an object-oriented data model have been proposed. However, several issues related to the integration of triggers with object-oriented modeling concepts have not been satisfactorily addressed. In this paper, we discuss issues related to trigger inheritance and refinement in the context of the Chimera active object-oriented data model. In particular, we introduce a semantics for an active object language that takes into account trigger inheritance and supports trigger overriding. Moreover, we state conditions on trigger overriding ensuring that trigger semantics is preserved in subclasses.

#index 443365
#* An Adaptive Access Method for Broadcast Data under an Error-Prone Mobile Environment
#@ Shou-Chih Lo;Arbee L. P. Chen
#t 2000
#c 7
#% 140613
#% 143040
#% 152954
#% 169835
#% 172382
#% 172876
#% 175253
#% 201897
#% 227885
#% 235940
#% 247246
#% 248838
#% 248893
#% 437181
#% 443127
#% 464065
#% 481777
#% 631861
#% 635819
#! A ubiquitous information environment can be achieved by the mobile computing technologies. In this environment, users carrying their portable computers can retrieve local or remote information anywhere and at anytime. Data broadcast, with its advantages, has become a powerful means to disseminate data in wireless communications. Indexing methods for the broadcast data have been proposed to speedup access time and reduce power consumption. However, the influence of access failures has not been discussed. For the error-prone mobile environment, the occurrence of access failures is often due to disconnections, handoffs, and communication noises. In this paper, based on the distributed indexing scheme, we propose an adaptive access method which tolerates the access failures. The basic idea is to use index replication to recover from the access failures. One mechanism, named search range, is provided to dynamically record the range where the desired data item may exist. According to the search range, an unfinished search can be efficiently resumed by finding an available index replicate. A performance analysis is given to show the benefits of the method. Also, the concept of version bits is applied to deal with the updates of the broadcast data.

#index 443366
#* MDARTS: A Multiprocessor Database Architecture for Hard Real-Time Systems
#@ Victor B. Lortz;Kang G. Shin;Jinho Kim
#t 2000
#c 7
#% 27238
#% 37968
#% 37969
#% 37973
#% 37974
#% 54583
#% 55683
#% 55718
#% 62012
#% 62015
#% 67497
#% 74408
#% 77990
#% 103369
#% 124815
#% 124816
#% 129851
#% 140389
#% 288821
#% 339362
#% 339366
#% 367727
#% 437028
#% 442832
#% 442835
#% 442836
#% 444441
#% 462482
#% 480766
#% 481454
#% 622828
#% 622861
#! Complex real-time systems need databases to support concurrent data access and provide well-defined interfaces between software modules. However, conventional database systems and prior real-time database systems do not provide the performance or predictability needed by high-speed, hard real-time applications. To address this need, we have designed, implemented, and evaluated an object-oriented database system called MDARTS (Multiprocessor Database Architecture for Real-Time Systems). MDARTS avoids the client-server overhead of most prior real-time database systems and object-oriented, real-time systems by moving transaction execution into application tasks. By eliminating these sources of overhead and focusing on basic data management services for control systems (data sharing, serializable transactions, and multiprocessor support), our MDARTS prototype provides hard real-time transaction times approximately three orders of magnitude faster than prior real-time database systems. MDARTS ensures bounded locking delay by disabling preemption when a transaction is waiting for a lock and, hence, allows for the estimation of worst-case transaction execution times. Another contribution of MDARTS is that it supports explicit declarations of real-time requirements and semantic constraints within application code. The MDARTS library examines these declarations at application initialization time and attempts to construct objects that are compatible with the requirements. Besides local shared-memory transactions with hard real-time response time guarantees, MDARTS also supports remote transactions that use remote procedure calls for data access with less stringent timing constraints. Our MDARTS prototype is implemented in C++ and it runs on VME-based multiprocessors and Sun workstations.

#index 443367
#* Performance Modeling of Distributed and Replicated Databases
#@ Matthias Nicola;Matthias Jarke
#t 2000
#c 7
#% 2157
#% 2260
#% 2544
#% 3083
#% 4185
#% 7576
#% 15523
#% 27057
#% 42052
#% 45258
#% 53110
#% 68208
#% 70077
#% 77005
#% 87365
#% 115661
#% 118041
#% 126292
#% 132361
#% 135558
#% 137753
#% 169925
#% 173986
#% 183291
#% 194938
#% 194955
#% 204474
#% 207725
#% 210179
#% 216033
#% 217560
#% 219482
#% 224894
#% 225006
#% 225017
#% 229843
#% 247071
#% 248825
#% 248893
#% 276124
#% 286967
#% 360720
#% 365700
#% 380821
#% 395431
#% 403195
#% 442693
#% 442696
#% 442713
#% 442839
#% 443032
#% 443076
#% 444242
#% 462641
#% 462933
#% 480085
#% 480256
#% 481584
#% 488146
#% 511166
#% 511171
#% 554304
#% 554635
#% 562180
#% 610967
#% 635834
#% 646494
#% 648449
#% 648455
#% 672534
#% 680584
#% 688641
#% 786939
#% 835744
#! This paper surveys performance models for distributed and replicated database systems. Over the last 20 years, a variety of such performance models have been developed and they differ in 1) which aspects of a real system are or are not captured in the model (e.g., replication, communication, nonuniform data access, etc.) and 2) how these aspects are modeled. We classify the different alternatives and modeling assumptions and discuss their interdependencies and expressiveness for the representation of distributed databases. This leads to a set of building blocks for analytical performance models. To illustrate the work that is surveyed, we select a combination of these proven modeling concepts and give an example of how to compose a balanced analytical model of a replicated database. We use this example to show how to derive meaningful performance values and to discuss the applicability and expressiveness of performance models for distributed and replicated databases. Finally, we compare the analytical results to measurements in a distributed database system.

#index 443368
#* Guest Editors' Introduction: Special Section on the 15th International Conference on Data Engineering
#@ Masaru Kitsuregawa;Michael P. Papazoglou;Calton Pu
#t 2000
#c 7
#! First Page of the Article

#index 443369
#* Querying Time Series Data Based on Similarity
#@ Davood Rafiei;Alberto O. Mendelzon
#t 2000
#c 7
#% 67552
#% 86950
#% 88056
#% 172949
#% 172950
#% 191581
#% 201876
#% 227857
#% 248790
#% 248797
#% 273704
#% 285932
#% 427199
#% 460862
#% 461885
#% 462231
#% 464196
#% 480952
#% 481609
#% 481611
#% 481920
#% 534183
#% 616530
#% 710020
#! We study similarity queries for time series data where similarity is defined, in a fairly general way, in terms of a distance function and a set of affine transformations on the Fourier series representation of a sequence. We identify a safe set of transformations supporting a wide variety of comparisons and show that this set is rich enough to formulate operations such as moving average and time scaling. We also show that queries expressed using safe transformations can efficiently be computed without prior knowledge of the transformations. We present a query processing algorithm that uses the underlying multidimensional index built over the data set to efficiently answer similarity queries. Our experiments show that the performance of this algorithm is competitive to that of processing ordinary (exact match) queries using the index, and much faster than sequential scanning. We propose a generalization of this algorithm for simultaneously handling multiple transformations at a time, and give experimental results on the performance of the generalized algorithm.

#index 443370
#* Query Rewriting for SWIFT (First) Answers
#@ Kian-Lee Tan;Cheng Hian Goh;Beng Chin Ooi
#t 2000
#c 7
#% 127860
#% 172811
#% 214602
#% 221376
#% 227883
#% 227894
#% 287005
#% 340635
#% 386623
#% 479623
#% 479816
#% 479976
#% 632001
#! Traditionally, the answer to a database query is construed as the set of all tuples that meet the criteria stated. Strict adherence to this notion in query evaluation is, however, increasingly unsatisfactory because decision makers are more prone to adopting an exploratory strategy for information search which we call 驴getting some answers quickly, and perhaps more later.驴 From a decision-maker's perspective, such a strategy is optimal for coping with information overload and makes economic sense (when used in conjunction with a micropayment mechanism). These new requirements present new opportunities for database query optimization. In this paper, we propose a progressive query processing strategy that exploits this behavior to conserve system resources and to minimize query response time and user waiting time. This is accomplished by the heuristic decomposition of user queries into subqueries that can be evaluated on demand. To illustrate the practicality of the proposed methods, we describe the architecture of a prototype system that provides a nonintrusive implementation of our approach. Finally, we present experimental results obtained from an empirical study conducted using an Oracle Server that demonstrate the benefits of the progressive query processing strategy.

#index 443371
#* An Approach to Active Spatial Data Mining Based on Statistical Information
#@ Wei Wang;Jiong Yang;Richard Muntz
#t 2000
#c 7
#% 210173
#% 227996
#% 234756
#% 248802
#% 248865
#% 443083
#% 479655
#% 479658
#% 481281
#% 489241
#% 489249
#% 527035
#% 527160
#% 566128
#! Spatial data mining presents new challenges due to the large size of spatial data, the complexity of spatial data types, and the special nature of spatial access methods.Most research in this area has focused on efficient query processing of static data. This paper introduces an active spatial data mining approach that extends the current spatial data mining algorithms to efficiently support user-defined triggers on dynamically evolving spatial data. To exploit the locality of the effect of an update and the nature of spatial data, we employ a hierarchical structure with associated statistical information at the various levels of the hierarchy and decompose the user-defined trigger into a set of subtriggers associated with cells in the hierarchy. Updates are suspended in the hierarchy until their cumulative effect might cause the trigger to fire. It is shown that this approach achieves three orders of magnitude improvement over the naive approach that reevaluate the condition over the database for each update, while both approaches produce the same result without any delay. Moreover, this scheme can support incremental query processing as well.

#index 443372
#* A Database Approach for Modeling and Querying Video Data
#@ Mohand-Saïd Hacid;Cyril Decleir;Jacques Kouloumdjian
#t 2000
#c 7
#% 57043
#% 64443
#% 69529
#% 91028
#% 113953
#% 115395
#% 146273
#% 151355
#% 159241
#% 172885
#% 189739
#% 190332
#% 190648
#% 191577
#% 191590
#% 198465
#% 201872
#% 204648
#% 210214
#% 210388
#% 211477
#% 213959
#% 216621
#% 239576
#% 239709
#% 248010
#% 248809
#% 319244
#% 382491
#% 422871
#% 443245
#% 445702
#% 445792
#% 452796
#% 458521
#% 461923
#% 463567
#% 464702
#% 464720
#% 477211
#% 481272
#% 520122
#% 534155
#% 552988
#% 627210
#% 631990
#% 632357
#% 637602
#% 683308
#! Indexing video data is essential for providing content-based access. In this paper, we consider how database technology can offer an integrated framework for modeling and querying video data. As many concerns in video (e.g., modeling and querying) are also found in databases, databases provide an interesting angle to attack many of the problems. From a video applications perspective, database systems provide a nice basis for future video systems. More generally, database research will provide solutions to many video issues, even if these are partial or fragmented. From a database perspective, video applications provide beautiful challenges. Next generation database systems will need to provide support for multimedia data (e.g., image, video, audio). These data types require new techniques for their management (i.e., storing, modeling, querying, etc.). Hence, new solutions are significant. This paper develops a data model and a rule-based query language for video content-based indexing and retrieval. The data model is designed around the object and constraint paradigms. A video sequence is split into a set of fragments. Each fragment can be analyzed to extract the information (symbolic descriptions) of interest that can be put into a database. This database can then be searched to find information of interest. Two types of information are considered: 1) the entities (objects) of interest in the domain of a video sequence, and 2) video frames which contain these entities. To represent this information, our data model allows facts as well as objects and constraints. The model consists of two layers: 1) Feature & Content Layer (or Audiovisual Layer), intended to contain video visual features such as colors, contours, etc., 2) Semantic Layer, which provides the (conceptual) content dimension of videos. We present a declarative, rule-based, constraint query language that can be used to infer relationships about information represented in the model. Queries can refer to the form dimension (i.e., information of the Feature & Content Layer), to the content dimension (i.e., information of the Semantic Layer), or to both. A program of the language is a rule-based system formalizing our knowledge of a video target application and it can also be considered as a (deductive) video database on its own right. The language has both a clear declarative and operational semantics.

#index 443373
#* Analysis of Range Queries and Self-Spatial Join Queries on Real Region Datasets Stored Using an R-Tree
#@ Guido Proietti;Christos Faloutsos
#t 2000
#c 7
#% 32913
#% 86950
#% 137887
#% 153260
#% 172949
#% 201876
#% 213975
#% 285924
#% 405926
#% 427199
#% 443453
#% 480093
#% 481620
#% 632104
#! In this paper, we study the node distribution of an R-tree storing region data, like, for instance, islands, lakes, or human-inhabited areas. We will show that real region datasets are packed in an R-tree into minimum bounding rectangles (MBRs) whose area distribution follows the same power law, named REGAL (REGion Area Law), as that for the regions themselves. Moreover, these MBRs are packed in their turn into MBRs following the same law, and so on iteratively, up to the root of the R-tree. Based on this observation, we are able to accurately estimate the search effort for range queries, using a small number of easy-to-retrieve parameters. Furthermore, since our analysis exploits, through a realistic mathematical model, the proximity relations existing among the regions in the dataset, we show how to use our model to predict the selectivity of a self-spatial join query posed on the dataset. Experiments on a variety of real datasets (islands, lakes, human-inhabited areas) show that our estimations are accurate, enjoying a geometric average relative error ranging from 22 percent to 32 percent for the search effort of a range query, and from 14 percent to 34 percent for the selectivity of a self-spatial join query. This is significantly better than using a naive model based on uniformity assumption, which gives rise to a geometric average relative error up to 270 percent and up to 85 percent for the two problems, respectively.

#index 443374
#* Analysis and Comparison of Declustering Schemes for Interactive Navigation Queries
#@ Chung-Min Chen;Rakesh K. Sinha
#t 2000
#c 7
#% 43179
#% 153400
#% 153633
#% 215403
#% 261738
#% 279383
#% 286962
#% 289297
#% 300171
#% 339622
#% 371227
#% 443200
#% 461922
#% 462233
#% 469603
#% 479936
#% 481109
#% 565461
#% 609704
#% 631955
#% 632069
#% 637794
#! Declustering schemes enable parallel retrievals of raster-spatial data by allocating data among multiple disks. Previous work in the literature has focused on static range queries. In this paper, we focus on interactive navigation queries, which exhibit a new class of data access patterns. We analyze and compare the performance of previously known declustering schemes from the perspective of navigation queries. These schemes include a class of latin-square-based schemes, Disk Modulo, Fieldwise Exclusive-OR, Hilbert Curve Allocation Method, and a random assignment scheme. We show that, unlike the case of range queries, disk modulo outperforms other schemes and gives nearly optimal performance for navigation queries. In addition, we propose a new scheme under the constraint of bounded window size驴a common constraint in practice due to resource limitation such as monitor resolution or memory size. Through extensive analysis, we establish guidelines on how these schemes can be tuned to provide guaranteed performance under the constraint of bounded window size.

#index 443375
#* Supporting Dynamic Interactions among Web-Based Information Sources
#@ Athman Bouguettaya;Boualem Benatallah;Lily Hendra;Mourad Ouzzani;James Beard
#t 2000
#c 7
#% 2035
#% 111912
#% 111913
#% 116303
#% 215227
#% 224771
#% 227886
#% 227981
#% 227994
#% 229827
#% 230432
#% 230433
#% 244105
#% 248819
#% 261741
#% 339754
#% 340295
#% 385766
#% 395735
#% 452498
#% 452530
#% 452531
#% 458743
#% 481602
#% 481923
#% 487369
#% 614598
#% 631959
#% 705958
#! The ubiquity of the World Wide Web offers an ideal opportunity for the deployment of highly distributed applications. Now that connectivity is no longer an issue, the attention has turned to providing a middleware infrastructure that will sustain data sharing among Web-accessible databases. We present a dynamic architecture and system for describing, locating, and accessing data from Web-accessible databases. We propose the use of flexible organizational constructs service links and coalitions to facilitate data organization, discovery, and sharing among Internet-accessible databases. A language is also proposed to support the definition and manipulation of these constructs. The implementation combines Java, CORBA, database API (JDBC), agent, and database technologies to support a scalable and portable architecture interconnecting large networks of heterogeneous and autonomous databases. We report on an experiment to provide uniform access to a Web of healthcare-related databases.

#index 443376
#* Mobile Agents for World Wide Web Distributed Database Access
#@ Stavros Papastavrou;George Samaras;Evaggelia Pitoura
#t 2000
#c 7
#% 161722
#% 209675
#% 224766
#% 245999
#% 247649
#% 278273
#% 384050
#% 385733
#% 511915
#% 540279
#% 555111
#% 562491
#% 585326
#% 589301
#! The popularity of the Web as a universal access mechanism for network information has created the need for developing web-based DBMS client/server applications. However, the current commercial applet-based approaches for accessing database systems offer limited flexibility, scalability, and robustness. In this paper, we propose a new framework for Web-based distributed access to database systems based on Java-based mobile agents. The framework supports lightweight, portable, and autonomous clients as well as operation on slow or expensive networks. The implementation of the framework using the aglet workbench shows that its performance is comparable to, and in some case outperforms, the current approach. In fact, in wireless and dial-up environments and for average size transactions, a client/agent/server adaptation of the framework provides a performance improvement of approximately a factor of ten. For the fixed network, the gains are about 40 percentand 30 percent, respectively. We expect our framework to perform even better when deployed using different implementation platforms as indicated by our preliminary results from an implementation based on Voyager.

#index 443377
#* E-DEVICE: An Extensible Active Knowledge Base System with Multiple Rule Type Support
#@ Nick Bassiliades;Ioannis Vlahavas;Ahmed K. Elmagarmid
#t 2000
#c 7
#% 36683
#% 82465
#% 86942
#% 115563
#% 120696
#% 152928
#% 154986
#% 164379
#% 170898
#% 185434
#% 190533
#% 194983
#% 213979
#% 227869
#% 252426
#% 452763
#% 464215
#% 480779
#% 480942
#% 562139
#% 565260
#% 571097
#! This paper describes E-DEVICE, an extensible active knowledge base system (KBS) that supports the processing of event-driven, production, and deductive rules into the same active OODB system. E-DEVICE provides the infrastructure for the smooth integration of various declarative rule types, such as production and deductive rules, into an active OODB system that supports low-level event-driven rules only by: 1) mapping each declarative rule into one event-driven rule, offering centralized rule selection control for correct run-time behavior and conflict resolution, and 2) using complex events to map the conditions of declarative rules and monitor the database to incrementally match those conditions. E-DEVICE provides the infrastructure for easily extending the system by adding: 1) new rule types as subtypes of existing ones, and 2) transparent optimizations to the rule matching network. The resulting system is a flexible, yet efficient, KBS that gives the user the ability to express knowledge in a variety of high-level forms for advanced problem solving in data intensive applications.

#index 443378
#* Enhancing Disjunctive Datalog by Constraints
#@ Francesco Buccafurri;Nicola Leone;Pasquale Rullo
#t 2000
#c 7
#% 877
#% 8417
#% 30098
#% 36683
#% 42986
#% 53388
#% 53392
#% 70168
#% 77147
#% 78634
#% 95339
#% 95643
#% 103704
#% 103705
#% 114723
#% 118359
#% 164405
#% 171061
#% 173127
#% 179921
#% 181023
#% 181220
#% 207074
#% 231786
#% 231883
#% 235018
#% 243831
#% 244091
#% 244372
#% 268765
#% 268771
#% 315533
#% 408396
#% 416007
#% 464531
#% 556918
#% 562049
#% 563606
#% 598376
#% 600179
#! This paper presents an extension of Disjunctive Datalog (${\rm{DATALOG}}^{\vee,}{^\neg}$) by integrity constraints. These are of two types: strong, that is, classical integrity constraints and weak, that is, constraints that are satisfied if possible. While strong constraints must be satisfied, weak constraints express desiderata, that is, they may be violated驴actually, their semantics tends to minimize the number of violated instances of weak constraints. Weak constraints may be ordered according to their importance to express different priority levels. As a result, the proposed language (call it, ${\rm{DATALOG}}^{\vee,}{^{\neg,}}{^c}$) is well-suited to represent common sense reasoning and knowledge-based problems arising in different areas of computer science such as planning, graph theory optimizations, and abductive reasoning. The formal definition of the language is first given. The declarative semantics of ${\rm{DATALOG}}^{\vee,}{^{\neg,}}{^c}$ is defined in a general way that allows us to put constraints on top of any existing (model-theoretic) semantics for ${\rm{DATALOG}}^{\vee,}{^\neg}$ programs. Knowledge representation issues are then addressed and the complexity of reasoning on ${\rm{DATALOG}}^{\vee,}{^{\neg,}}{^c}$ programs is carefully determined. An in-depth discussion on complexity and expressiveness of ${\rm{DATALOG}}^{\vee,}{^{\neg,}}{^c}$ is finally reported. The discussion contrasts ${\rm{DATALOG}}^{\vee,}{^{\neg,}}{^c}$ to ${\rm{DATALOG}}^{\vee,}{^\neg}$ and highlights the significant increase in knowledge modeling ability carried out by constraints.

#index 443379
#* Correction to 'Continual Queries for Internet Scale Event-Driven Information Delivery'
#@ Ling Liu;Calton Pu;Wei Tang
#t 2000
#c 7
#% 241207
#% 443298
#! First Page of the Article

#index 443380
#* Integrating Security and Real-Time Requirements Using Covert Channel Capacity
#@ Sang H. Son;Ravi Mukkamala;Rasikan David
#t 2000
#c 7
#% 9241
#% 117903
#% 124818
#% 194950
#% 225089
#% 227954
#% 238734
#% 255493
#% 274299
#% 322637
#% 462628
#% 566387
#% 583740
#% 664471
#% 664498
#% 664554
#% 840577
#! Database systems for real-time applications must satisfy timing constraints associated with transactions in addition to maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that, due to the conflicting goals of each requirement, trade-offs need to be made between security and timeliness. We first define mutual information, a measure of the degree to which security is being satisfied by a system. A secure two-phase locking protocol is then described and a scheme is proposed to allow partial violations of security for improved timeliness. Analytical expressions for the mutual information of the resultant covert channel are derived and a feedback control scheme is proposed that does not allow the mutual information to exceed a specified upper bound. Results showing the efficacy of the scheme obtained through simulation experiments are also discussed.

#index 443381
#* ASEP: A Secure and Flexible Commit Protocol for MLS Distributed Database Systems
#@ Indrajit Ray;Luigi V. Mancini;Sushil Jajodia;Elisa Bertino
#t 2000
#c 7
#% 1486
#% 4619
#% 9241
#% 113745
#% 151157
#% 159735
#% 488150
#% 488181
#! The classical Early Prepare commit protocol (EP), used in many commercial systems, is not suitable for use in multilevel secure distributed databases systems that employ a locking protocol for concurrency control. This is because EP requires that read locks are not released by a participant during its window of uncertainty; however, it is not possible for a locking protocol to provide this guarantee in a multilevel secure system (since the read lock of a higher-level transaction on a lower-level data object must be released whenever a lower-level transaction wants to write the same data). The only available work in the literature, namely the Secure Early Prepare protocol (SEP), overcomes this difficulty by aborting those distributed transactions that release their low-level read locks prematurely. We see this approach as being too restrictive. One of the major benefits of distributed processing is its robustness to failures, and SEP fails to take advantage of this. In this work, we propose the Advanced Secure Early Prepare commit protocol (ASEP) to solve the above problem together with a number of language primitives that can be used as system calls in distributed transactions. These primitives permit features like partial rollback and forward recovery to be incorporated within the transaction model, and allow a distributed transaction to proceed even when a participant has released its low-level read locks prematurely. This not only offers flexibility, but can also be used, if desired, by a sophisticated programmer to trade off consistency for atomicity of the distributed transaction.

#index 443382
#* Secure Databases: Constraints, Inference Channels, and Monitoring Disclosures
#@ Alexander Brodsky;Csilla Farkas;Sushil Jajodia
#t 2000
#c 7
#% 35004
#% 36683
#% 43189
#% 273692
#% 442709
#% 442782
#% 443010
#% 507406
#! This paper investigates the problem of inference channels that occur when database constraints are combined with nonsensitive data to obtain sensitive information. We present an integrated security mechanism, called the Disclosure Monitor, which guarantees data confidentiality by extending the standard mandatory access control mechanism with a Disclosure Inference Engine. The Disclosure Inference Engine generates all the information that can be disclosed to a user based on the user's past and present queries and the database and metadata constraints. The Disclosure Inference Engine operates in two modes: data-dependent mode, when disclosure is established based on the actual data items, and data-independent mode, when only queries are utilized to generate the disclosed information. The disclosure inference algorithms for both modes are characterized by the properties of soundness (i.e., everything that is generated by the algorithm is disclosed) and completeness (i.e., everything that can be disclosed is produced by the algorithm). The technical core of this paper concentrates on the development of sound and complete algorithms for both data-dependent and data-independent disclosures.

#index 443383
#* Exploiting Spatial Indexes for Semijoin-Based Join Processing in Distributed Spatial Databases
#@ Kian-Lee Tan;Beng Chin Ooi;David J. Abel
#t 2000
#c 7
#% 1758
#% 13041
#% 18614
#% 68091
#% 83933
#% 86950
#% 86952
#% 152937
#% 172908
#% 172909
#% 210186
#% 210187
#% 227932
#% 285932
#% 289282
#% 427199
#% 462218
#% 462957
#% 463116
#% 463595
#% 479453
#% 479797
#% 480093
#% 481920
#% 489213
#% 526847
#% 527025
#! In a distributed spatial database system, a user may issue a query that relates two spatial relations not stored at the same site. Because of the sheer volume and complexity of spatial data, spatial joins between two spatial relations at different sites are expensive in terms of computation and transmission cost. In this paper, we address the problems of processing spatial joins in a distributed environment. We propose a semijoin-like operator, called the spatial semijoin, to prune away objects that will not contribute to the join result. This operator also reduces both the transmission and local processing costs for a later join operation. However, the cost of the elimination process must be taken into account, and we consider approaches to minimize these overheads. We also studied and compared two families of distributed join algorithms that are based on the spatial semijoin operator. The first is based on multidimensional approximations obtained from an index such as the R-tree, and the second is based on single-dimensional approximations obtained from object mapping. We conducted experiments on real data sets and report the results in this paper.

#index 443384
#* 2000 Index IEEE Transaction on Knowledge and Data Engineering,Vol. 12
#@  IEEE Transactions on Knowledge and Data Engineering Staff
#t 2000
#c 7
#! First Page of the Article

#index 443385
#* Object-Based Selective Materialization for Efficient Implementation of Spatial Data Cubes
#@ Nebojsa Stefanovic;Jiawei Han;Krzysztof Koperski
#t 2000
#c 7
#% 201893
#% 207552
#% 210182
#% 223781
#% 227880
#% 227996
#% 339369
#% 420053
#% 435137
#% 442847
#% 443083
#% 463595
#% 463742
#% 479450
#% 481281
#% 481951
#% 527021
#% 527160
#% 527170
#! With a huge amount of data stored in spatial databases and the introduction of spatial components to many relational or object-relational databases, it is important to study the methods for spatial data warehousing and OLAP of spatial data. In this paper, we study methods for spatial OLAP, by integration of nonspatial OLAP methods with spatial database implementation techniques. A spatial data warehouse model, which consists of both spatial and nonspatial dimensions and measures, is proposed. Methods for computation of spatial data cubes and analytical processing on such spatial data cubes are studied, with several strategies proposed, including approximation and selective materialization of the spatial objects resulted from spatial OLAP operations. The focus of our study is on a method for spatial cube construction, called object-based selective materialization, which is different from cuboid-based selective materialization proposed in previous studies of nonspatial data cube construction. Rather than using a cuboid as an atomic structure during the selective materialization, we explore granularity on a much finer level, that of a single cell of a cuboid. Several algorithms are proposed for object-based selective materialization of spatial data cubes and the performance study has demonstrated the effectiveness of these techniques.

#index 443386
#* Semantic Query Optimization for Query Plans of Heterogeneous Multidatabase Systems
#@ Chun-Nan Hsu;Craig A. Knoblock
#t 2000
#c 7
#% 33376
#% 36683
#% 68199
#% 69272
#% 116303
#% 153040
#% 188076
#% 188853
#% 201976
#% 213437
#% 232151
#% 318049
#% 408396
#% 442677
#% 442678
#% 442851
#% 452824
#% 463735
#% 480280
#% 481293
#% 481923
#% 571217
#% 689733
#% 704693
#% 1290115
#% 1499470
#! New applications of information systems, such as electronic commerce and healthcare information systems, need to integrate a large number of heterogeneous databases over computer networks. Answering a query in these applications usually involves selecting relevant information sources and generating a query plan to combine the data automatically. As significant progress has been made in source selection and plan generation, the critical issue has been shifting to query optimization. This paper presents a semantic query optimization (SQO) approach to optimizing query plans of heterogeneous multidatabase systems. This approach provides global optimization for query plans as well as local optimization for subqueries that retrieve data from individual database sources. An important feature of our local optimization algorithm is that we prove necessary and sufficient conditions to eliminate an unnecessary join in a conjunctive query of arbitrary join topology. This feature allows our optimizer to utilize more expressive relational rules to provide a wider range of possible optimizations than previous work in SQO. The local optimization algorithm also features a new data structure called AND-OR implication graphs to facilitate the search for optimal queries. These features allow the global optimization to effectively use semantic knowledge to reduce data transmission cost. We have implemented this approach into the pesto query plan optimizer as a part of the sims information mediator. Experimental results demonstrate that pesto can provide significant savings in query execution cost over query plan execution without optimization.

#index 443387
#* Performance Analysis of Parallel Query Processing Algorithms for Object-Oriented Databases
#@ Stanley Y. W. Su;Sanjay Ranka;Xiang He
#t 2000
#c 7
#% 18614
#% 43161
#% 58352
#% 86948
#% 116090
#% 150309
#% 152904
#% 165947
#% 189321
#% 213223
#% 285936
#% 339790
#% 340633
#% 442700
#% 442963
#% 462625
#% 463108
#% 473205
#% 479920
#% 480595
#% 480596
#% 481942
#% 563920
#! In recent years, parallel processing and optimization algorithms for processing object-oriented databases have drawn a considerable amount of attention from the database research community. Two general types of algorithms have been introduced: hybrid-hash pointer-based algorithms and multiwavefront algorithms. In this work, we quantitatively analyze the two algorithms and develop analytical formulas to capture the main performance features of these two approaches. We study their performance in three application environments: One is characterized by large databases having many object classes, each of which contains a large number of instances; the second one is characterized by large databases having many object classes, each of which contains a relatively small number of instances; and the third one is by large databases having object classes of varying sizes. A horizontal data partitioning strategy, in which each object class is partitioned into horizontal segments stored across all processors, is used in the first environment. A class-per-node assignment strategy, in which instances of each object class are stored in a single processor, is used in the second environment. In the third environment, object classes are partitioned horizontally and assigned to a varying number of processors depending on their different sizes. Our analytical results show that the multiwavefront algorithm has three distinguishing features which contribute to its better performance: 1) two-phase processing strategy, 2) vertical partitioning of horizontal segments, and 3) dynamic determination of 驴collision point驴 in multiwavefront propagations which results in an optimized query execution plan. We show that if these features are adopted by a hybrid-hash, pointer-based algorithm, its performance will be comparable with that of the multiwavefront algorithm because the difference in CPU time between them is negligible. The assumed computing environment is a network of workstations having a share-nothing architecture. The schema and some queries selected from the OO7 benchmark are used in the performance analyses and comparisons. The queries are modified slightly in different data environments in order to reflect the features of diverse database applications.

#index 443388
#* Introducing the New Editor-in-Chief of the IEEE Transactions on Knowledge and Data Engineering
#@ Farokh B. Bastani
#t 2001
#c 7

#index 443389
#* Guest Editorial Introduction to the Special Section on the 16th International Conference on Data Engineering
#@ David B. Lomet;Gerhard Weikum
#t 2001
#c 7
#! First Page of the Article

#index 443390
#* Automating Statistics Management for Query Optimizers
#@ Surajit Chaudhuri;Vivek Narasayya
#t 2001
#c 7
#% 36119
#% 201921
#% 210190
#% 248820
#% 248821
#% 299989
#% 458523
#% 462079
#% 479656
#% 481749
#% 482092
#% 482100
#% 482123
#% 565429
#! Statistics play a key role in influencing the quality of plans chosen by a database query optimizer. In this paper, we identify the statistics that are essential for an optimizer. We introduce novel techniques that help significantly reduce the set of statistics that need to be created without sacrificing the quality of query plans generated. We discuss how these techniques can be leveraged to automate statistics management in databases. We have implemented and experimentally evaluated our approach on Microsoft SQL Server 7.0.

#index 443391
#* A Foundation for Conventional and Temporal Query Optimization Addressing Duplicates and Ordering
#@ Giedrius Slivinskas;Christian S. Jensen;Richard Thomas Snodgrass
#t 2001
#c 7
#% 18615
#% 77323
#% 83144
#% 86943
#% 111284
#% 137866
#% 164376
#% 178813
#% 198066
#% 207552
#% 225004
#% 234756
#% 271935
#% 287268
#% 289370
#% 301179
#% 319244
#% 335715
#% 361445
#% 375015
#% 387508
#% 416029
#% 442967
#% 443233
#% 443257
#% 462230
#% 480764
#% 481928
#% 565462
#% 641038
#! Most real-world databases contain substantial amounts of time-referenced, or temporal, data. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query representation, optimization, and processing mechanisms must be provided. This paper presents a foundation for query optimization that integrates conventional and temporal query optimization and is suitable for both conventional DBMS architectures and ones where the temporal support is obtained via a layer on top of a conventional DBMS. This foundation captures duplicates and ordering for all queries, as well as coalescing for temporal queries, thus generalizing all existing approaches known to the authors. It includes a temporally extended relational algebra to which SQL and temporal SQL queries may be mapped, six types of algebraic equivalences, concrete query transformation rules that obey different equivalences, a procedure for determining which types of transformation rules are applicable for optimizing a query, and a query plan enumeration algorithm. The presented approach partitions the work required by the database implementor to develop a provably correct query optimizer into four stages: The database implementor has to 1) specify operations formally, 2) design and prove correct appropriate transformation rules that satisfy any of the six equivalence types, 3) augment the mechanism that determines when the different types of rules are applicable to ensure that the enumeration algorithm applies the rules correctly, and 4) ensure that the mapping generates a correct initial query plan.

#index 443392
#* DEMON: Mining and Monitoring Evolving Data
#@ Venkatesh Ganti;Johannes Gehrke;Raghu Ramakrishnan
#t 2001
#c 7
#% 36672
#% 46809
#% 80995
#% 201075
#% 210173
#% 223781
#% 232136
#% 248790
#% 248792
#% 248813
#% 273693
#% 273900
#% 464204
#% 464706
#% 479627
#% 479658
#% 479799
#% 481290
#% 511333
#% 539791
#% 631986
#! Data mining algorithms have been the focus of much research recently. In practice, the input data to a data mining process resides in a large data warehouse whose data is kept up-to-date through periodic or occasional addition and deletion of blocks of data. Most data mining algorithms have either assumed that the input data is static, or have been designed for arbitrary insertions and deletions of data records. In this paper, we consider a dynamic environment that evolves through systematic addition or deletion of blocks of data. We introduce a new dimension, called the data span dimension, which allows user-defined selections of a temporal subset of the database. Taking this new degree of freedom into account, we describe efficient model maintenance algorithms for frequent itemsets and clusters. We then describe a generic algorithm that takes any traditional incremental model maintenance algorithm and transforms it into an algorithm that allows restrictions on the data span dimension. We also develop an algorithm for automatically discovering a specific class of interesting block selection sequences. In a detailed experimental study, we examine the validity and performance of our ideas on synthetic and real datasets.

#index 443393
#* Finding Interesting Associations without Support Pruning
#@ Edith Cohen;Mayur Datar;Shinji Fujiwara;Aristides Gionis;Piotr Indyk;Rajeev Motwani;Jeffrey D. Ullman;Cheng Yang
#t 2001
#c 7
#% 124010
#% 152934
#% 190611
#% 204673
#% 220706
#% 227883
#% 227917
#% 227919
#% 243166
#% 248790
#% 249321
#% 479484
#% 479973
#% 481290
#% 616528
#! Association-rule mining has heretofore relied on the condition of high support to do its work efficiently. In particular, the well-known a priori algorithm is only effective when the only rules of interest are relationships that occur very frequently. However, there are a number of applications, such as data mining, identification of similar web documents, clustering, and collaborative filtering, where the rules of interest have comparatively few instances in the data. In these cases, we must look for highly correlated items, or possibly even causal relationships between infrequent items. We develop a family of algorithms for solving this problem, employing a combination of random sampling and hashing techniques. We provide analysis of the algorithms developed and conduct experiments on real and synthetic data to obtain a comparative performance analysis.

#index 443394
#* Multiple Similarity Queries: A Basic DBMS Operation for Mining in Metric Databases
#@ Bernhard Braunmüller;Martin Ester;Hans-Peter Kriegel;Jörg Sander
#t 2001
#c 7
#% 68091
#% 86950
#% 132779
#% 169940
#% 227856
#% 232106
#% 237187
#% 248811
#% 252304
#% 300166
#% 376266
#% 427199
#% 443083
#% 443889
#% 479462
#% 479649
#% 481956
#% 482109
#% 488941
#% 527021
#% 527026
#% 632009
#% 632060
#! Metric databases are databases where a metric distance function is defined for pairs of database objects. In such databases, similarity queries in the form of range queries or k-nearest-neighbor queries are the most important query types. In traditional query processing, single queries are issued independently by different users. In many data mining applications, however, the database is typically explored by iteratively asking similarity queries for answers of previous similarity queries. In this paper, we introduce a generic scheme for such data mining algorithms and we investigate two orthogonal approaches, reducing I/O cost as well as CPU cost, to speed-up the processing of multiple similarity queries. The proposed techniques apply to any type of similarity query and to an implementation based on an index or using a sequential scan. Parallelization yields an additional impressive speed-up. An extensive performance evaluation confirms the efficiency of our approach.

#index 443395
#* Adlet: An Active Document Abstraction for Multimedia Information Fusion
#@ Shi-Kuo Chang;Taieb Znati
#t 2001
#c 7
#% 85086
#% 110011
#% 137610
#% 161754
#% 172394
#% 185254
#% 187985
#% 197531
#% 201992
#% 210214
#% 210684
#% 340295
#% 385733
#% 434752
#% 464720
#% 481602
#% 481923
#% 614598
#% 648866
#% 718207
#! A new approach is described for the fusion of multimedia information based on the concept of active documents advertising on the Internet, whereby the metadata of a document travels in the network to seek out documents of interest to the parent document and, at the same time, advertises its parent document to other interested documents. This abstraction of metadata is called an adlet, which is the core of our approach. Two important features make this approach applicable to multimedia information fusion, information retrieval, data mining, geographic information systems, and medical information systems: 1) any document, including a web page, database record, video file, audio file, image and even paper documents, can be enhanced by an adlet and become an active document, and 2) any node in a nonactive network can be enhanced by adlet-savvy software and the adlet-enhanced node can coexist with other nonenhanced nodes. An experimental prototype provides a testbed for feasibility studies in a hybrid active network environment.

#index 443396
#* On the 'Dimensionality Curse' and the 'Self-Similarity Blessing'
#@ Flip Korn;Bernd-Uwe Pagel;Christos Faloutsos
#t 2001
#c 7
#% 84654
#% 86950
#% 137887
#% 164360
#% 213974
#% 213975
#% 227856
#% 227939
#% 237187
#% 237204
#% 248015
#% 248017
#% 248796
#% 252304
#% 271202
#% 273714
#% 285924
#% 300160
#% 427199
#% 435141
#% 437509
#% 460862
#% 462059
#% 462239
#% 464859
#% 479649
#% 479973
#% 480093
#% 480132
#% 480133
#% 480304
#% 481455
#% 481599
#% 481620
#% 481947
#% 481956
#% 527026
#% 632035
#! Spatial queries in high-dimensional spaces have been studied extensively recently. Among them, nearest-neighbor queries are important in many settings, including spatial databases (Find the $k$ closest cities) and multimedia databases (Find the $k$ most similar images). Previous analyses have concluded that nearest-neighbor search is hopeless in high dimensions due to the notorious 驴curse of dimensionality.驴 Here, we show that this may be overpessimistic. We show that what determines the search performance (at least for R-tree-like structures) is the intrinsic dimensionality of the data set and not the dimensionality of the address space (referred to as the embedding dimensionality). The typical (and often implicit) assumption in many previous studies is that the data is uniformly distributed, with independence between attributes. However, real data sets overwhelmingly disobey these assumptions; rather, they typically are skewed and exhibit intrinsic (驴fractal驴) dimensionalities that are much lower than their embedding dimension, e.g., due to subtle dependencies between attributes. In this paper, we show how the Hausdorff and Correlation fractal dimensions of a data set can yield extremely accurate formulas that can predict the I/O performance to within one standard deviation on multiple real and synthetic data sets. The practical contributions of this work are our accurate formulas, which can be used for query optimization in spatial and multimedia databases. The major theoretical contribution is the 驴deflation驴 of the dimensionality curse: Our formulas and our experiments show that previous worst-case analyses of nearest-neighbor search in high dimensions are overpessimistic to the point of being unrealistic. The performance depends critically on the intrinsic (驴fractal驴) dimensionality as opposed to the embedding dimension that the uniformity and independence assumptions incorrectly imply.

#index 443397
#* Analysis of the Clustering Properties of the Hilbert Space-Filling Curve
#@ Bongki Moon;H. v. Jagadish;Christos Faloutsos;Joel H. Saltz
#t 2001
#c 7
#% 13032
#% 13041
#% 42091
#% 64431
#% 86951
#% 117775
#% 118213
#% 159079
#% 227706
#% 321305
#% 434154
#% 443128
#% 565447
#! Several schemes for the linear mapping of a multidimensional space have been proposed for various applications, such as access methods for spatio-temporal databases and image compression. In these applications, one of the most desired properties from such linear mappings is clustering, which means the locality between objects in the multidimensional space being preserved in the linear space. It is widely believed that the Hilbert space-filling curve achieves the best clustering [1], [14]. In this paper, we analyze the clustering property of the Hilbert space-filling curve by deriving closed-form formulas for the number of clusters in a given query region of an arbitrary shape (e.g., polygons and polyhedra). Both the asymptotic solution for the general case and the exact solution for a special case generalize previous work [14]. They agree with the empirical results that the number of clusters depends on the hypersurface area of the query region and not on its hypervolume. We also show that the Hilbert curve achieves better clustering than the z curve. From a practical point of view, the formulas given in this paper provide a simple measure that can be used to predict the required disk access behaviors and, hence, the total access time.

#index 443398
#* Guest Editors' Introduction: Special Section on Connectionist Models for Learning in Structured Domains
#@ Paolo Frasconi;Marco Gori;Alessandro Sperduti
#t 2001
#c 7
#% 50262
#% 90390
#% 90391
#% 90393
#% 481201
#% 496116
#% 1860191
#% 1860363
#% 1862573
#! First Page of the Article

#index 443399
#* Simple Strategies to Encode Tree Automata in Sigmoid Recursive Neural Networks
#@ Rafael C. Carrasco;Mikel L. Forcada
#t 2001
#c 7
#% 22959
#% 132674
#% 191861
#% 191863
#% 212288
#% 225179
#% 404772
#% 959439
#% 1051436
#% 1860191
#% 1860363
#% 1862613
#! Recently, a number of authors have explored the use of recursive neural nets (RNN) for the adaptive processing of trees or tree-like structures. One of the most important language-theoretical formalizations of the processing of tree-structured data is that of deterministic finite-state tree automata (DFSTA). DFSTA may easily be realized as RNN using discrete-state units, such as the threshold linear unit. A recent result by Síima (Neural Network World7 (1997), pp. 679驴686) shows that any threshold linear unit operating on binary inputs can be implemented in an analog unit using a continuous activation function and bounded real inputs. The constructive proof finds a scaling factor for the weights and reestimates the bias accordingly. In this paper, we explore the application of this result to simulate DFSTA in sigmoid RNN (that is, analog RNN using monotonically growing activation functions) and also present an alternative scheme for one-hot encoding of the input that yields smaller weight values and, therefore, works at a lower saturation level.

#index 443400
#* Integrating Linguistic Primitives in Learning Context-Dependent Representation
#@ Samuel W. K. Chan
#t 2001
#c 7
#% 50262
#% 78946
#% 90391
#% 90393
#% 90394
#% 92135
#% 92196
#% 111448
#% 111449
#% 136369
#% 197856
#% 230530
#% 279967
#% 363667
#% 375017
#% 440679
#% 442935
#% 742146
#% 747593
#% 756517
#% 1272305
#% 1290072
#% 1860361
#% 1860363
#% 1862573
#% 1862662
#! This paper presents an explicit connectionist-inspired, language learning model in which the process of settling on a particular interpretation for a sentence emerges from the interaction of a set of 驴soft驴 lexical, semantic, and syntactic primitives. We address how these distinct linguistic primitives can be encoded from different modular knowledge sources but strongly involved in an interactive processing in such a way as to make implicit linguistic information explicit. The learning of a quasi-logical form, called context-dependent representation, is inherently incremental and dynamical in such a way that every semantic interpretation will be related to what has already been presented in the context created by prior utterances. With the aid of the context-dependent representation, the capability of the language learning model in text understanding is strengthened. This approach also shows how the recursive and compositional role of a sentence as conveyed in the syntactic structure can be modeled in a neurobiologically motivated linguistics based on dynamical systems rather on combinatorial symbolic architecture. Experiments with more than 2,000 sentences in different languages illustrating the influences of the context-dependent representation on semantic interpretation, among other issues, are included.

#index 443401
#* Symbolic vs. Connectionist Learning: An Experimental Comparison in a Structured Domain
#@ Pasquale Foggia;Roberto Genna;Mario Vento
#t 2001
#c 7
#% 90388
#% 90390
#% 90391
#% 90393
#% 92776
#% 103913
#% 156421
#% 174161
#% 211677
#% 251403
#% 288990
#% 396021
#% 442952
#% 443768
#% 449508
#% 481374
#% 625932
#% 669522
#% 1784071
#% 1860191
#% 1860363
#% 1862573
#% 1862662
#! During the last two decades, the attempts of finding effective and efficient solutions to the problem of learning any kind of structured information have been splitting the scientific community. A 驴holy war驴 has been fought between the advocates of a symbolic approach to learning and the advocates of a connectionist approach. One of the most repeated claims of the symbolic party has been that symbolic methods are able to cope with structured information while connectionist ones are not. However, in the last few years, the possibility of employing connectionist methods for structured data has been widely investigated and several approaches have been proposed. Does this mean that the connectionist partisans are about to win the ultimate battle? Is connectionism the 驴One True Approach驴 to knowledge learning? The paper discusses this topic and gives an experimental answer to these questions. In details, first, a novel algorithm for learning structured descriptions, ascribable to the category of symbolic techniques, is proposed. It faces the problem directly in the space of graphs by defining the proper inference operators, as graph generalization and graph specialization, and obtains general and consistent prototypes with a low computational cost with respect to other symbolic learning systems. Successively, the proposed algorithm is compared with a recent connectionist method for learning structured data [17] with reference to a problem of handwritten character recognition from a standard database publicly available on the Web. Finally, after a discussion highlighting pros and cons of symbolic and connectionist approaches, some conclusions, quantitatively supported by the experimental data, are drawn. The orthogonality of the two approaches strongly suggests their combination in a multiclassifier system so as to retain the strengths of both of them, while overcoming their weaknesses. The results on the experimental case-study demonstrated that the adoption of a parallel combination scheme of the two algorithms could improve the recognition performance of about 10 percent. A truce or an alliance between the symbolic and the connectionist worlds?

#index 443402
#* Generalization Ability of Folding Networks
#@ Barbara Hammer
#t 2001
#c 7
#% 90391
#% 164838
#% 212288
#% 230144
#% 235262
#% 235379
#% 237433
#% 237658
#% 256125
#% 359194
#% 493582
#% 539452
#% 1808838
#% 1809314
#% 1809459
#% 1860363
#! The information theoretical learnability of folding networks, a very successful approach capable of dealing with tree structured inputs, is examined. We find bounds on the VC, pseudo-, and fat shattering dimension of folding networks with various activation functions. As a consequence, valid generalization of folding networks can be guaranteed. However, distribution independent bounds on the generalization error cannot exist in principle. We propose two approaches which take the specific distribution into account and allow us to derive explicit bounds on the deviation of the empirical error from the real error of a learning algorithm: The first approach requires the probability of large trees to be limited a priori and the second approach deals with situations where the maximum input height in a concrete learning example is restricted.

#index 443403
#* Hierarchical Growing Cell Structures: TreeGCS
#@ Victoria J. Hodge;Jim Austin
#t 2001
#c 7
#% 36672
#% 184200
#% 211526
#% 234978
#% 476737
#% 493560
#% 1860324
#! We propose a hierarchical clustering algorithm (TreeGCS) based upon the Growing Cell Structure (GCS) neural network of Fritzke. Our algorithm refines and builds upon the GCS base, overcoming an inconsistency in the original GCS algorithm, where the network topology is susceptible to the ordering of the input vectors. Our algorithm is unsupervised, flexible, and dynamic and we have imposed no additional parameters on the underlying GCS algorithm. Our ultimate aim is a hierarchical clustering neural network that is both consistent and stable and identifies the innate hierarchical structure present in vector-based data. We demonstrate improved stability of the GCS foundation and evaluate our algorithm against the hierarchy generated by an ascendant hierarchical clustering dendogram. Our approach emulates the hierarchical clustering of the dendogram. It demonstrates the importance of the parameter settings for GCS and how they affect the stability of the clustering.

#index 443404
#* Incremental Syntactic Parsing of Natural Language Corpora with Simple Synchrony Networks
#@ Peter C. R. Lane;James B. Henderson
#t 2001
#c 7
#% 92148
#% 111449
#% 199757
#% 237658
#% 304311
#% 363592
#% 374794
#% 422614
#% 443333
#% 496878
#% 708948
#% 741115
#% 747965
#% 1271869
#% 1860191
#% 1860363
#% 1862662
#! This article explores the use of Simple Synchrony Networks (SSNs) for learning to parse English sentences drawn from a corpus of naturally occurring text. Parsing natural language sentences requires taking a sequence of words and outputting a hierarchical structure representing how those words fit together to form constituents. Feed-forward and Simple Recurrent Networks have had great difficulty with this task, in part because the number of relationships required to specify a structure is too large for the number of unit outputs they have available. SSNs have the representational power to output the necessary $O(n^2)$ possible structural relationships because SSNs extend the $O(n)$ incremental outputs of Simple Recurrent Networks with the $O(n)$ entity outputs provided by Temporal Synchrony Variable Binding. This article presents an incremental representation of constituent structures which allows SSNs to make effective use of both these dimensions. Experiments on learning to parse naturally occurring text show that this output format supports both effective representation and effective generalization in SSNs. To emphasize the importance of this generalization ability, this article also proposes a short-term memory mechanism for retaining a bounded number of constituents during parsing. This mechanism improves the $O(n^2)$ speed of the basic SSN architecture to linear time, but experiments confirm that the generalization ability of SSN networks is maintained.

#index 443405
#* Learning Distributed Representations of Concepts Using Linear Relational Embedding
#@ Alberto Paccanaro;Geoffrey E. Hinton
#t 2001
#c 7
#% 90391
#% 92148
#% 132674
#% 138308
#% 272325
#! In this paper, we introduce Linear Relational Embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization.

#index 443406
#* Clustering and Classification in Structured Data Domains Using Fuzzy Lattice Neurocomputing (FLN)
#@ Vassilios Petridis;Vassilis G. Kaburlasos
#t 2001
#c 7
#% 23408
#% 90391
#% 92195
#% 111415
#% 136350
#% 181861
#% 212288
#% 222577
#% 256440
#% 262059
#% 268097
#% 328830
#% 443297
#% 443305
#% 445317
#% 445319
#% 708398
#% 1275293
#% 1499571
#% 1788038
#% 1860191
#% 1860363
#% 1860371
#% 1860547
#% 1862573
#! A connectionist scheme, namely, 驴-Fuzzy Lattice Neurocomputing scheme or 驴-FLN for short, which has been introduced in the literature lately for clustering in a lattice data domain, is employed in this work for computing clusters of directed graphs in a master-graph. New tools are presented and used here, including a convenient inclusion measure function for clustering graphs. A directed graph is treated by 驴-FLN as a single datum in the mathematical lattice of subgraphs stemming from a master-graph. A series of experiments is detailed where the master-graph emanates from a Thesaurus of spoken language synonyms. The words of the Thesaurus are fed to 驴-FLN in order to compute clusters of semantically related words, namely, hyperwords. The arithmetic parameters of 驴-FLN can be adjusted so as to calibrate the total number of hyperwords computed in a specific application. It is demonstrated how the employment of hyperwords implies a reduction, based on the a priori knowledge of semantics contained in the Thesaurus, in the number of features to be used for document classification. In a series of comparative experiments for document classification, it appears that the proposed method favorably improves classification accuracy in problems involving longer documents, whereas performance deteriorates in problems involving short documents.

#index 443407
#* Representation and Processing of Structures with Binary Sparse Distributed Codes
#@ Dmitri A. Rachkovskij
#t 2001
#c 7
#% 65345
#% 85153
#% 89955
#% 90388
#% 90390
#% 90391
#% 90393
#% 237775
#% 273309
#% 369953
#% 492949
#% 856742
#% 1862573
#! The schemes for compositional distributed representations include those allowing on-the-fly construction of fixed dimensionality codevectors to encode structures of various complexity. Similarity of such codevectors takes into account both structural and semantic similarity of represented structures. In this paper, we provide a comparative description of sparse binary distributed representation developed in the framework of the associative-projective neural network architecture and the more well-known holographic reduced representations of Plate and binary spatter codes of Kanerva. The key procedure in associative-projective neural networks is context-dependent thinning which binds codevectors and maintains their sparseness. The codevectors are stored in structured memory array which can be realized as distributed auto-associative memory. Examples of distributed representation of structured data are given. Fast estimation of the similarity of analogical episodes by the overlap of their codevectors is used in the modeling of analogical reasoning both for retrieval of analogs from memory and for analogical mapping.

#index 443408
#* Global Viewing of Heterogeneous Data Sources
#@ Silvana Castano;Valeria De Antonellis;Sabrina De Capitani di Vimercati
#t 2001
#c 7
#% 1371
#% 22948
#% 55294
#% 67565
#% 85086
#% 106916
#% 126330
#% 136347
#% 158907
#% 158908
#% 169050
#% 188853
#% 198058
#% 213225
#% 213442
#% 227992
#% 229827
#% 237184
#% 263981
#% 340295
#% 430422
#% 442861
#% 442917
#% 462048
#% 464050
#% 479449
#% 481280
#% 481598
#% 481760
#% 481923
#% 481935
#% 535356
#% 535676
#! The problem of defining global views of heterogeneous data sources to support querying and cooperation activities is becoming more and more important due to the availability of multiple data sources within complex organizations and in global information systems. Global views are defined to provide a unified representation of the information in the different sources by analyzing conceptual schemas associated with them and resolving possible semantic heterogeneity. In the paper, we propose an affinity-based unification method for global view construction. In the method: 1) The concept of affinity is introduced to assess the level of semantic relationship between elements in different schemas by taking into account semantic heterogeneity; 2) Schema elements are classified by affinity levels using clustering procedures so that their different representations can be analyzed for unification; 3) Global views are constructed starting from selected clusters by unifying representations of their elements. Experiences of applying the proposed unification method and the associated tool environment artemis on databases of the Italian Public Administration information systems are described.

#index 443409
#* Optimizing Large Join Queries Using A Graph-Based Approach
#@ Chiang Lee;Chi-Sheng Shih;Yaw-Huei Chen
#t 2001
#c 7
#% 32877
#% 43161
#% 58376
#% 83232
#% 86949
#% 101463
#% 102786
#% 155003
#% 178814
#% 339660
#% 339727
#% 340668
#% 369764
#% 394617
#% 411554
#% 427195
#% 443002
#% 443006
#% 480419
#% 480424
#% 480608
#% 480615
#% 480943
#% 481104
#% 481110
#% 564733
#! Although many query tree optimization strategies have been proposed in the literature, there still is a lack of a formal and complete representation of all possible permutations of query operations (i.e., execution plans) in a uniform manner. A graph-theoretic approach presented in this paper provides a sound mathematical basis for representing a query and searching for an execution plan. In this graph model, a node represents an operation and a directed edge between two nodes indicates the order of executing these two operations in an execution plan. Each node is associated with a weight and so is an edge. The weight is an expression containing optimization required parameters, such as relation size, tuple size, join selectivity factors. All possible execution plans are representable in this graph and each spanning tree of the graph becomes an execution plan. It is a general model which can be used in the optimizer of a DBMS for interal query representation. On the basis of this model, we devise an algorithm that finds a near optimal execution plan using only polynomial time. The algorithm is compared with a few other popular optimization methods. Experiments show that the proposed algorithm is superior to the others under most circumstances.

#index 443410
#* Hyperlog: A Graph-Based System for Database Browsing, Querying, and Update
#@ Alexandra Poulovassilis;Stefan G. Hild
#t 2001
#c 7
#% 23902
#% 34670
#% 36309
#% 56639
#% 58356
#% 68680
#% 82351
#% 84650
#% 88701
#% 96355
#% 109496
#% 154334
#% 157133
#% 164415
#% 182768
#% 268797
#% 277328
#% 277344
#% 286267
#% 287003
#% 442732
#% 480760
#! Hyperlog is a declarative, graph-based language that supports database querying and update. It visualizes schema information, data, and query output as sets of nested graphs, which can be stored, browsed, and queried in a uniform way. Thus, the user need only be familiar with a very small set of syntactic constructs. Hyperlog queries consist of a set of graphs that are matched against the database. Database updates are supported by means of programs consisting of a set of rules. This paper discusses the formulation, evaluation, expressiveness, and optimization of Hyperlog queries and programs. We also describe a prototype implementation of the language and we compare and contrast our approach with work in a number of related areas, including visual database languages, graph-based data models, database update languages, and production rule systems.

#index 443411
#* Correction to 'MPGS: An Interactive Tool for the Specification and Generation of Multimedia Presentations'
#@ Elisa Bertino;Elena Ferrari;Marco Stolf
#t 2001
#c 7
#! First Page of the Article

#index 443412
#* Guest Editors' Introduction: Special Section on Semantic Issues of Multimedia Systems
#@ Zahir Tari;Robert Meersman
#t 2001
#c 7
#! First Page of the Article

#index 443413
#* Emergent Semantics through Interaction in Image Databases
#@ Simone Santini;Amarnath Gupta;Ramesh Jain
#t 2001
#c 7
#% 4178
#% 196977
#% 211091
#% 212690
#% 238913
#% 248798
#% 284557
#% 535980
#% 636418
#% 706089
#% 1180246
#% 1759974
#! In this paper, we briefly discuss some aspects of image semantics and the role that it plays for the design of image databases. We argue that images don't have an intrinsic meaning, but that they are endowed with a meaning by placing them in the context of other images and by the user interaction. From this observation, we conclude that, in an image, database users should be allowed to manipulate not only the individual images, but also the relation between them. We present an interface model based on the manipulation of configurations of images.

#index 443414
#* Data Semantics for Improving Retrieval Performance of Digital News Video Systems
#@ Gulrukh Ahanger;Thomas D. C. Little
#t 2001
#c 7
#% 70370
#% 173676
#% 183353
#% 194021
#% 238915
#% 238916
#% 375388
#% 406493
#% 422871
#% 422897
#% 422899
#% 437407
#% 437509
#% 443246
#% 443889
#% 632283
#% 648114
#% 718385
#! We propose a novel four-step hybrid approach for retrieval and composition of video newscasts based on information contained in different metadata sets. In the first step, we use conventional retrieval techniques to isolate video segments from the data universe using segment metadata. In the second step, retrieved segments are clustered into potential news items using a dynamic technique sensitive to the information contained in the segments. In the third step, we apply a transitive search technique to increase the recall of the retrieval system. In the final step, we increase recall performance by identifying segments possessing creation-time relationships. A quantitative analysis of the performance of the process on a newscast composition shows an increase in recall by 59 percent over the conventional keyword-based search technique used in the first step.

#index 443415
#* ZYX-A Multimedia Document Model for Reuse and Adaptation of Multimedia Content
#@ Susanne Boll;Wolfgang Klas
#t 2001
#c 7
#% 111584
#% 201880
#% 286742
#% 422927
#% 422973
#% 434717
#% 434874
#% 452790
#% 481603
#% 627049
#% 662395
#% 1180260
#! Advanced multimedia applications require adequate support for the modeling of multimedia content by multimedia document models. More and more this support calls for not only the adequate modeling of the temporal and spatial course of a multimedia presentation and its interactions, but also for the partial reuse of multimedia documents and adaptation to a given user context. However, our thorough investigation of existing standards for multimedia document models such as HTML, MHEG, SMIL, and HyTime leads to us the conclusion that these standard models do not provide sufficient modeling support for reuse and adaptation. Therefore, we propose a new approach for the modeling of adaptable and reusable multimedia content, the ZYX model. The model offers primitives that provide驴beyond the more or less common primitives for temporal, spatial, and interaction modeling驴a variform support for reuse of structure and layout of document fragments and for the adaptation of the content and its presentation to the user context. We present the model in detail and illustrate the application and effectiveness of these concepts by samples taken from our Cardio-OP application in the domain of cardiac surgery. With the ZYX model, we developed a comprehensive means for advanced multimedia content creation: support for template-driven authoring of multimedia content and support for flexible, dynamic composition of multimedia documents customized to the user's local context and needs. The approach significantly impacts and supports the authoring process in terms of methodology and economic aspects.

#index 443416
#* Fuzzy Logic Techniques in Multimedia Database Querying: A Preliminary Investigation of the Potentials
#@ Didier Dubois;Henri Prade;Florence Sèdes
#t 2001
#c 7
#% 10172
#% 36244
#% 40313
#% 48720
#% 66654
#% 84549
#% 90358
#% 169940
#% 172927
#% 201934
#% 206249
#% 206890
#% 210212
#% 210214
#% 227859
#% 227998
#% 228824
#% 236409
#% 237191
#% 237870
#% 238917
#% 247487
#% 319244
#% 383684
#% 393800
#% 434753
#% 434808
#% 434809
#% 437405
#% 437407
#% 442886
#% 443261
#% 443268
#% 443284
#% 463919
#% 464720
#% 464726
#% 467287
#% 479783
#% 535986
#% 562348
#% 566111
#% 1788921
#! Fuzzy logic is known for providing a convenient tool for interfacing linguistic categories with numerical data and for expressing user's preference in a gradual and qualitative way. Fuzzy set methods have been already applied to the representation of flexible queries and to the modeling of uncertain pieces of information in databases systems, as well as in information retrieval. This methodology seems to be even more promising in multimedia databases which have a complex structure and from which documents have to be retrieved and selected not only from their contents, but also from 驴the idea驴 the user has of their appearance, through queries specified in terms of user's criteria. This paper provides a preliminary investigation of the potential applications of fuzzy logic in multimedia databases. The problem of comparing semistructured documents is first discussed. Querying issues are then more particularly emphasized. We distinguish two types of request, namely, those which can be handled within some extended version of an SQL-like language and those for which one has to elicit user's preference through examples.

#index 443417
#* Constructing the Dependency Structure of a Multiagent Probabilistic Network
#@ S. K.  Michael Wong;Cory J. Butz
#t 2001
#c 7
#% 44876
#% 67866
#% 246759
#% 247593
#% 286816
#% 287754
#% 287792
#% 289237
#% 289336
#% 289350
#% 289424
#% 346917
#% 380725
#% 384978
#% 400439
#% 403535
#% 408680
#% 527830
#% 636372
#% 637820
#% 836134
#% 1650341
#% 1650691
#% 1650726
#% 1650818
#! A probabilistic network consists of a dependency structure and corresponding probability tables. The dependency structure is a graphical representation of the conditional independencies that are known to hold in the problem domain. In this paper, we propose an automated process for constructing the combined dependency structure of a multiagent probabilistic network. Each domain expert supplies any known conditional independency information and not necessarily an explicit dependency structure. Our method determines a succinct representation of all the supplied independency information called a minimal cover. This process involves detecting all inconsistent information and removing all redundant information. A unique dependency structure of the multiagent probabilistic network can be constructed directly from this minimal cover. The main result of this paper is that the constructed dependency structure is a perfect-map of the minimal cover. That is, every probabilistic conditional independency logically implied by the minimal cover can be inferred from the dependency structure and every probabilistic conditional independency inferred from the dependency structure is logically implied by the minimal cover.

#index 443418
#* A New Uncertainty Measure for Belief Networks with Applications to Optimal Evidential Inferencing
#@ Jiming Liu;David A. Maluf;Michel C. Desmarais
#t 2001
#c 7
#% 44876
#% 69333
#% 95228
#% 103309
#% 109042
#% 174161
#% 182919
#% 443179
#% 644560
#! This paper is concerned with the problem of measuring the uncertainty in a broad class of belief networks, as encountered in evidential reasoning applications. In our discussion, we give an explicit account of the networks concerned, and coin them the Dempster-Shafer (D-S) belief networks. We examine the essence and the requirement of such an uncertainty measure based on well-defined discrete event dynamical systems concepts. Furthermore, we extend the notion of entropy for the D-S belief networks in order to obtain an improved optimal dynamical observer. The significance and generality of the proposed dynamical observer of measuring uncertainty for the D-S belief networks lie in that it can serve as a performance estimator as well as a feedback for improving both the efficiency and the quality of the D-S belief network-based evidential inferencing. We demonstrate, with Monte Carlo simulation, the implementation and the effectiveness of the proposed dynamical observer in solving the problem of evidential inferencing with optimal evidence node selection.

#index 443419
#* Trends in Databases: Reasoning and Mining
#@ Jef Wijsen
#t 2001
#c 7
#% 3422
#% 5191
#% 152934
#% 157158
#% 189636
#% 189872
#% 196419
#% 210160
#% 213977
#% 225003
#% 275936
#% 279170
#% 280449
#% 296539
#% 443058
#% 452747
#% 464076
#% 464204
#% 481588
#! We propose a temporal dependency, called trend dependency (TD), which captures a significant family of data evolution regularities. An example of such regularity is 驴Salaries of employees generally do not decrease.驴 TDs compare attributes over time using operators of $\{,\leq,\geq,\ne\}$. We define a satisfiability problem that is the dual of the logical implication problem for TDs and we investigate the computational complexity of both problems. As TDs allow expressing meaningful trends, 驴mining驴 them from existing databases is interesting. For the purpose of TD mining, TD satisfaction is characterized by support and confidence measures. We study the problem $\rm TDMINE$: given a temporal database, mine the TDs that conform to a given template and whose support and confidence exceed certain threshold values. The complexity of $\rm TDMINE$ is studied, as well as algorithms to solve the problem.

#index 443420
#* Global Scheduling for Flexible Transactions in Heterogeneous Distributed Database Systems
#@ Aidong Zhang;Marian Nodine;Bharat Bhargava
#t 2001
#c 7
#% 6797
#% 9241
#% 43175
#% 54581
#% 83125
#% 86940
#% 99823
#% 102753
#% 111378
#% 111916
#% 114583
#% 116076
#% 146202
#% 172880
#% 287220
#% 435104
#% 435118
#% 462019
#% 462778
#% 463091
#% 463101
#% 463124
#% 480456
#% 480771
#! A heterogeneous distributed database environment integrates a set of autonomous database systems to provide global database functions. A flexible transaction approach has been proposed for the heterogeneous distributed database environments. In such an environment, flexible transactions can increase the failure resilience of global transactions by allowing alternate (but in some sense equivalent) executions to be attempted when a local database system fails or some subtransactions of the global transaction abort. In this paper, we study the impact of compensation, retry, and switching to alternative executions on global concurrency control for the execution of flexible transactions. We propose a new concurrency control criterion for the execution of flexible and local transactions, termed F-serializability, in the error-prone heterogeneous distributed database environments. We then present a scheduling protocol that ensures F-serializability on global schedules. We also demonstrate that this scheduler avoids unnecessary aborts and compensation.

#index 443421
#* Automata for the Assessment of Knowledge
#@ Cornelia E. Dowling;Cord Hockemeyer
#t 2001
#c 7
#% 7672
#% 44534
#% 231460
#% 242886
#% 386517
#% 404772
#% 553097
#! The results of this paper can be applied to construct efficient algorithms for the adaptive assessment of the students' knowledge. The problem of knowledge assessment is a special case of the problem of assessing the state of a system. These results are obtained suggesting a new assessment algorithm whose formal equivalence to previously suggested assessment algorithms is proven. A simulation study illustrates the vast improvements of efficiency with the new algorithm.

#index 443422
#* A Survey of Languages for Specifying Dynamics: A Knowledge Engineering Perspective
#@ Pascal van Eck;Joeri Engelfriet;Dieter Fensel;Frank van Harmelen;Yde Venema;Mark Willems
#t 2001
#c 7
#% 22161
#% 37977
#% 55936
#% 67827
#% 82786
#% 101952
#% 101953
#% 115390
#% 121129
#% 134101
#% 134108
#% 146250
#% 150046
#% 164934
#% 169697
#% 184440
#% 188283
#% 204010
#% 205177
#% 205178
#% 210761
#% 244096
#% 253400
#% 258933
#% 264853
#% 264855
#% 277196
#% 332925
#% 361458
#% 363096
#% 365414
#% 369768
#% 374130
#% 406964
#% 426119
#% 443215
#% 444996
#% 459624
#% 472713
#% 480366
#% 486744
#% 490315
#% 499018
#% 520165
#% 544434
#% 544549
#! During the last years, a number of formal specification languages for knowledge-based systems has been developed. Characteristics for knowledge-based systems are a complex knowledge base and an inference engine which uses this knowledge to solve a given problem. Specification languages for knowledge-based systems have to cover both aspects. They have to provide the means to specify a complex and large amount of knowledge and they have to provide the means to specify the dynamic reasoning behavior of a knowledge-based system. This paper focuses on the second aspect. For this purpose, we survey existing approaches for specifying dynamic behavior in related areas of research. In fact, we have taken approaches for the specification of information systems (Language for Conceptual Modeling and Troll), approaches for the specification of database updates and logic programming (Transaction Logic and Dynamic Database Logic) and the generic specification framework of Abstract State Machines.

#index 443423
#* Debiasing Training Data for Inductive Expert System Construction
#@ Vijay S. Mookerjee
#t 2001
#c 7
#% 4098
#% 25646
#% 25655
#% 42994
#% 71616
#% 84238
#% 85573
#% 90661
#% 93751
#% 99394
#% 99423
#% 120634
#% 130034
#% 160852
#% 185135
#% 207195
#% 207252
#% 369236
#% 444865
#% 449588
#% 451057
#% 1272369
#% 1777047
#! We study the presence of economic bias in the training data used to develop inductive expert systems. Such bias arises when an expert considers economic factors in decision making. We find that the presence of economic bias is particularly harmful when there is an economic misalignment between the expert and the user of the induced expert system. Such misalignment is referred to as differential bias. The most significant contribution of this study is a training data debiasing procedure that uses a genetic algorithm to reconstruct training data that is relatively free of economic bias. We conduct a series of simulation experiments that show: 1) the economic performance of accuracy and value seeking algorithms is statistically the same when the training data has economic bias, 2) both accuracy and value seeking algorithms suffer in the presence of differential bias, 3) the proposed debiasing procedure significantly combats differential bias, and 4) the debiasing procedure is quite robust with respect to estimation errors in its input parameters.

#index 443424
#* Redundancy Detection in Semistructured Case Bases
#@ Kirsti Racine;Qiang Yang
#t 2001
#c 7
#% 115462
#% 153503
#% 159108
#% 168280
#% 258186
#% 359837
#% 406493
#% 444839
#% 1275276
#% 1499568
#! With the dramatic proliferation of case-based reasoning systems in commercial applications, many case bases are now becoming legacy systems. They represent a significant portion of an organization's assets, but they are large and difficult to maintain. One of the contributing factors is that these case bases are often large and yet unstructured or semistructured; they are represented in natural language text. Adding to the complexity is the fact that the case bases are often authored and updated by different people from a variety of knowledge sources, making it highly likely for a case base to contain redundant and inconsistent knowledge. In this paper, we present methods and a system for maintaining large and semistructured case bases. We focus on a difficult problem in case base maintenance: redundancy detection. This problem is particularly pervasive when one deals with a semistructured case base. We will discuss an information-retrieval-based algorithm and an implemented system for solving this problem. As the ability to contain the knowledge acquisition problem is of paramount importance, our method allows one to express relevant domain expertise for detecting redundancy naturally and effortlessly. Empirical evaluations of the system demonstrate the effectiveness of the methods in several large domains.

#index 443425
#* Query Languages for Sequence Databases: Termination and Complexity
#@ Giansalvatore Mecca;Anthony J. Bonner
#t 2001
#c 7
#% 13014
#% 33376
#% 53385
#% 100607
#% 123111
#% 164378
#% 164420
#% 275304
#% 464551
#! This paper develops a query language for sequence databases, such as genome databases and text databases. Unlike relational data, queries over sequential data can easily produce infinite answer sets since the universe of sequences is infinite, even for a finite alphabet. The challenge is to develop query languages that are both highly expressive and finite. This paper develops such a language as a subset of a logic for string databases called Sequence Datalog. The main idea is to use safe recursion to control and limit unsafe recursion. The main results are the definition of a finite form of recursion, called domain-bounded recursion, and a characterization of its complexity and expressive power. Although finite, the resulting class of programs is highly expressive since its data complexity is complete for the elementary functions.

#index 443426
#* Correction to 'Automating Statistics Management for Query Optimizers'
#@ Surajit Chaudhuri;Vivek Narasayya
#t 2001
#c 7
#! First Page of the Article

#index 443427
#* A New Approach to Online Generation of Association Rules
#@ Charu C. Aggarwal;Philip S. Yu
#t 2001
#c 7
#% 152934
#% 172386
#% 201894
#% 210160
#% 210182
#% 461909
#% 462238
#% 463883
#% 463903
#% 481281
#% 481290
#% 481588
#% 481604
#% 481754
#% 481758
#% 481779
#% 481780
#% 481948
#% 481951
#! We discuss the problem of online mining of association rules in a large database of sales transactions. The online mining is performed by preprocessing the data effectively in order to make it suitable for repeated online queries. We store the preprocessed data in such a way that online processing may be done by applying a graph theoretic search algorithm whose complexity is proportional to the size of the output. The result is an online algorithm which is independent of the size of the transactional data and the size of the preprocessed data. The algorithm is almost instantaneous in the size of the output. The algorithm also supports techniques for quickly discovering association rules from large itemsets. The algorithm is capable of finding rules with specific items in the antecedent or consequent. These association rules are presented in a compact form, eliminating redundancy. The use of nonredundant association rules helps significantly in the reduction of irrelevant noise in the data mining process.

#index 443428
#* Toward Multidatabase Mining: Identifying Relevant Databases
#@ Huan Liu;Hongjun Lu;Jun Yao
#t 2001
#c 7
#% 136350
#% 179008
#% 210160
#% 211059
#% 232106
#% 232141
#% 232147
#% 232170
#% 232173
#% 385563
#% 385564
#% 442814
#% 443148
#% 449566
#% 452747
#% 452821
#% 481588
#% 1476365
#% 1499588
#! Various tools and systems for knowledge discovery and data mining are developed and available for applications. However, when we are immersed in heaps of databases, an immediate question is where we should start mining. It is not true that the more databases, the better for data mining. It is only true when the databases involved are relevant to a task at hand. In this paper, breaking away from the conventional data mining assumption that many databases be joined into one, we argue that the first step for multidatabase mining is to identify databases that are most likely relevant to an application; without doing so, the mining process can be lengthy, aimless, and ineffective. A measure of relevance is thus proposed for mining tasks with an objective of finding patterns or regularities about certain attributes. An efficient algorithm for identifying relevant databases is described. Experiments are conducted to verify the measure's performance and to exemplify its application.

#index 443429
#* A Parametric Approach to Deductive Databases with Uncertainty
#@ Laks V. S. Lakshmanan;Nematollaah Shiri
#t 2001
#c 7
#% 7689
#% 33376
#% 47960
#% 53400
#% 66101
#% 67902
#% 101612
#% 103634
#% 124785
#% 144840
#% 181038
#% 190638
#% 213981
#% 251691
#% 289052
#% 368248
#% 462952
#% 470500
#% 516045
#% 599549
#! Numerous frameworks have been proposed in recent years for deductive databases with uncertainty. On the basis of how uncertainty is associated with the facts and rules in a program, we classify these frameworks into implication-based (IB) and annotation-based (AB) frameworks. In this paper, we take the IB approach and propose a generic framework, called the parametric framework, as a unifying umbrella for IB frameworks. We develop the declarative, fixpoint, and proof-theoretic semantics of programs in our framework and show their equivalence. Using the framework as a basis, we then study the query optimization problem of containment of conjunctive queries in this framework and establish necessary and sufficient conditions for containment for several classes of parametric conjunctive queries. Our results yield tools for use in the query optimization for large classes of query programs in IB deductive databases with uncertainty.

#index 443430
#* Locating Objects in Mobile Computing
#@ Evaggelia Pitoura;George Samaras
#t 2001
#c 7
#% 9241
#% 36102
#% 83933
#% 102950
#% 143040
#% 175253
#% 179135
#% 189579
#% 194478
#% 194480
#% 198081
#% 211503
#% 217030
#% 225006
#% 237618
#% 241956
#% 245015
#% 245080
#% 248878
#% 248887
#% 248888
#% 248889
#% 248902
#% 248906
#% 260010
#% 269282
#% 273706
#% 273707
#% 273897
#% 281495
#% 295512
#% 296159
#% 318016
#% 384050
#% 384121
#% 421073
#% 437181
#% 452477
#% 461923
#% 464847
#% 480965
#% 503869
#% 503882
#% 1797801
#% 1830004
#% 1830212
#% 1848613
#% 1852664
#! In current distributed systems, the notion of mobility is emerging in many forms and applications. Mobility arises naturally in wireless computing since the location of users changes as they move. Besides mobility in wireless computing, software mobile agents are another popular form of moving objects. Locating objects, i.e., identifying their current location, is central to mobile computing. In this paper, we present a comprehensive survey of the various approaches to the problem of storing, querying, and updating the location of objects in mobile computing. The fundamental techniques underlying the proposed approaches are identified, analyzed, and classified along various dimensions.

#index 443431
#* Segmented Information Dispersal (SID) Data Layouts for Digital Video Servers
#@ Ariel Cohen;Walter A. Burkhard
#t 2001
#c 7
#% 43172
#% 83129
#% 83130
#% 91028
#% 128184
#% 146597
#% 151340
#% 159079
#% 162976
#% 172881
#% 201680
#% 201932
#% 210171
#% 219914
#% 237615
#% 249264
#% 251484
#% 291640
#% 382099
#% 610235
#% 632238
#% 656849
#% 661666
#% 703137
#! We present a novel data organization for disk arrays驴Segmented Information Dispersal (SID). SID provides protection against disk failures while ensuring that the reconstruction of the missing data requires only relatively small contiguous accesses to the available disks. SID has a number of properties that make it an attractive solution for fault-tolerant video servers. Under fault-free conditions, SID performs as well as RAID 5 and organizations based on balanced incomplete block designs (BIBD). Under failure, SID performs much better than RAID 5 since it significantly reduces the size of the disk accesses performed by the reconstruction process. SID also performs much better than BIBD by ensuring the contiguity of the reconstruction accesses. Contiguity is a very significant factor for video retrieval workloads, as we demonstrate in this paper. We present SID data organizations with a concise representation which enables the reconstruction process to efficiently locate the needed video and check data.

#index 443432
#* A Spatio-Temporal Semantic Model for Multimedia Database Systems and Multimedia Information Systems
#@ Shu-Ching Chen;R. L. Kashyap
#t 2001
#c 7
#% 179717
#% 197902
#% 197904
#% 204538
#% 317871
#% 317933
#% 322955
#% 427199
#% 434717
#% 434745
#% 437405
#% 442974
#% 445709
#% 452790
#% 452796
#% 463902
#% 464216
#% 519971
#% 611999
#! As more information sources become available in multimedia systems, the development of abstract semantic models for video, audio, text, and image data becomes very important. An abstract semantic model has two requirements: It should be rich enough to provide a friendly interface of multimedia presentation synchronization schedules to the users and it should be a good programming data structure for implementation in order to control multimedia playback. An abstract semantic model based on an augmented transition network (ATN) is presented. The inputs for ATNs are modeled by multimedia input strings. Multimedia input strings provide an efficient means for iconic indexing of the temporal/spatial relations of media streams and semantic objects. An ATN and its subnetworks are used to represent the appearing sequence of media streams and semantic objects. The arc label is a substring of a multimedia input string. In this design, a presentation is driven by a multimedia input string. Each subnetwork has its own multimedia input string. Database queries relative to text, image, and video can be answered via substring matching at subnetworks. Multimedia browsing allows users the flexibility to select any part of the presentation they prefer to see. This means that the ATN and its subnetworks can be included in multimedia database systems which are controlled by a database management system (DBMS). User interactions and loops are also provided in an ATN. Therefore, ATNs provide three major capabilities: multimedia presentations, temporal/spatial multimedia database searching, and multimedia browsing.

#index 443433
#* Performance Analysis of Distributed Deadlock Detection Algorithms
#@ Soojung Lee;Junguk L. Kim
#t 2001
#c 7
#% 35077
#% 51374
#% 65353
#% 66344
#% 70077
#% 112295
#% 190156
#% 199858
#% 199864
#% 258799
#% 284997
#% 286979
#% 318399
#% 374261
#% 462657
#% 463269
#% 631893
#% 660977
#% 696367
#! This paper presents a probabilistic performance analysis of a deadlock detection algorithm in distributed systems. Although there has been extensive study on deadlock detection algorithms in distributed systems, little attention has been paid to the study of the performance of these algorithms. Most work on performance study has been achieved through simulation but not through an analytic model. Min [14], to the best of our knowledge, made the sole attempt to evaluate the performance of distributed deadlock detection algorithms analytically. Being different from Min's [14], our analytic approach takes the time-dependent behavior of each process into consideration rather than simply taking the mean-value estimation. Furthermore, the relation among the times when deadlocked processes become blocked is studied, which enhances the accuracy of the analysis. We measure performance metrics such as duration of deadlock, the number of algorithm invocations, and the mean waiting time of a blocked process. It is shown that the analytic estimates are nearly consistent with simulation results.

#index 443434
#* Criss-Cross Hash Joins: Design and Analysis
#@ Ram D. Gopal;R. Ramesh;Stanley Zionts
#t 2001
#c 7
#% 1834
#% 3771
#% 13033
#% 18614
#% 44054
#% 50725
#% 51376
#% 58376
#% 86948
#% 91618
#% 98469
#% 102315
#% 102765
#% 114577
#% 152585
#% 152913
#% 152915
#% 156405
#% 156406
#% 156710
#% 172907
#% 174090
#% 192653
#% 341231
#% 408466
#% 427195
#% 442976
#% 444223
#% 445721
#% 462159
#% 462325
#% 462941
#% 479753
#! Join processing in relational database systems continues to be a difficult and challenging problem. In this research, we propose a criss-cross hash join strategy that draws from both hashing and indexing techniques, inheriting the advantages of each. To facilitate the criss-cross hash join, a simple data structure, termed page map, is introduced. The page maps aid in reducing the hashing effort incurred in the current hash based join methods. Furthermore, the page maps implicitly capture and exploit the possible inherent order among tuples in the relations, however partial it may be, to achieve superior performance. As the proposed methodology relies on the hashing scheme, the page maps are simpler, more compact, and easier to maintain than the traditional data structures associated with index based join methods. We develop the ideas intuitively first, followed by a formal development of the concepts and the algorithms. A detailed probabilistic analysis of the algorithms is presented and their performance is assessed through extensive empirical investigations. The empirical analysis suggests significant performance improvements over the current state-of-the-art hybrid hash method, especially in the presence of possible inherent order.

#index 443435
#* Toward Web-Based Application Management Systems
#@ Avigdor Gal;John Mylopoulos
#t 2001
#c 7
#% 90639
#% 164416
#% 183913
#% 213673
#% 248819
#% 318392
#% 340295
#% 360354
#% 366803
#% 480259
#% 480771
#% 481125
#% 511896
#% 511908
#% 591548
#% 596208
#% 978381
#! As Web technology spreads, the number, variety, and sophistication of Web-based information services is literally exploding. While some effort has been put into managing a single, centrally controlled Web site, current Web technologies offer little help for managing Web-based applications in-the-large. This is partly due to the distributed, heterogeneous, and open nature of such applications. This paper proposes a generic framework for managing Web-based applications which addresses both semantic and managerial issues. Semantic issues are addressed through the inclusion of a domain model component in the framework which describes the kinds of information that are available. Management issues are treated through a framework which includes formally-defined notions for an information model, information base consistency, transactions, and concurrency control. Thus, the proposed management system provides a semantically robust environment for Web-based information services while allowing for Web source independence.

#index 443436
#* SEE: A Spatial Exploration Environment Based on a Direct-Manipulation Paradigm
#@ Sudhir R. Kaushik;Elke A. Rundensteiner
#t 2001
#c 7
#% 115181
#% 116335
#% 172811
#% 201880
#% 219846
#% 221380
#% 227971
#% 232482
#% 239657
#% 286639
#% 362928
#% 396711
#% 436116
#% 442847
#% 445703
#% 462237
#% 464073
#% 464224
#% 482087
#% 528033
#% 562667
#% 588672
#% 641123
#% 662407
#% 725497
#% 727085
#! The need forproviding effective tools for analyzing and querying spatial data is becoming increasingly important with the explosion of data in applications such as geographic information systems, image databases, CAD, and remote sensing. The SEE (Spatial Exploration Environment) is the first effort at applying direct-manipulation visual information seeking (VIS) techniques to spatial data analysis by visually querying as well as browsing spatial data and reviewing the visual results for trend analysis. The SEE system incorporates a visual query language (SVIQUEL) that allows users to specify the relative spatial position (both topology and direction) between objects using direct manipulation. The quantitative SVIQUEL sliders (S-sliders) are complemented by the qualitative Active-Picture-for-Querying (APIQ) interface that allows the user to specify qualitative relative position queries. APIQ provides qualitative visual representations of the quantitative query specified by the S-sliders. This increases the utility of the system for spatial browsing and spatial trend discovery with no particular query in mind. The SVIQUEL queries are processed using a k-Bucket index structure specifically tuned for incremental processing of the multidimensional range queries that represent the class of queries that can be expressed by SVIQUEL. We have also designed a tightly integrated map visualization that helps to preserve the spatial context and a bar visualization that provides a qualitative abstraction of aggregates and enables the user to visualize the results of the spatial query as well as the nonspatial attributes of the underlying spatial objects. The SEE system has been fully implemented as an applet using JDK1.1.4. Finally, we compare the spatial exploration environment (SEE) technology with alternative spatial query environments with respect to its querying power and the ease of querying.

#index 443437
#* SQL/SDA: A Query Language for Supporting Spatial Data Analysis and Its Web-Based Implementation
#@ Hui Lin;Bo Huang
#t 2001
#c 7
#% 36683
#% 78365
#% 90724
#% 341167
#% 359260
#% 366803
#% 383059
#% 435148
#% 442847
#% 445703
#% 458583
#% 526841
#% 526843
#% 526995
#% 527031
#% 597475
#! An important trend of current GIS development is to provide easy and effective access to spatial analysis functionalities for supporting decision making based on geo-referenced data. Within the framework of the ongoing SQL standards for spatial extensions, a spatial query language, called SQL/SDA, has been designed to meet such a requirement. Since the language needs to incorporate the important derivation functions (e.g., map-overlay and feature-fusion) as well as the spatial relationship and metric functions, the functionality of the FROM clause in SQL is developed in addition to the SELECT and WHERE clauses. By restructuring the FROM clause via a subquery, SQL/SDA is well-adapted to the general spatial analysis procedures using current GIS packages. Such an extended SQL, therefore, stretches the capabilities of the previous ones. The implementation of SQL/SDA on the Internet adopts a hybrid model, which takes advantage of the Web GIS design methods in both the client side and server side. The client side of SQL/SDA, programmed in the Java language, provides a query interface by introducing visual constructs such as icons, listboxes, and comboboxes to assist in the composition of queries, thereby enhancing the usability of the language. The server side of SQL/SDA, which is composed of a query processor and Spatial Database Engine (SDE), carries out query processing on spatial databases after receiving user requests. It was demonstrated that using the familiar SELECT-FROM-WHERE statement instead of a single ad hoc command or procedural commands like macro language in some GIS packages, SQL/SDA offers users an efficient option to perform complicated multistep spatial data analyses on the Internet.

#index 443438
#* DOLPHIN: Digital Online Library Providing Human-Like Interactive Navigation
#@ Yoshiaki Seki;Tetsuo Hidaka
#t 2001
#c 7
#% 185254
#% 245814
#% 437506
#% 438051
#% 1830225
#% 1830405
#! In a digital library environment, we propose a new support concept that is like a reference service in real libraries. Based on the concept, we have developed a prototype, called DOLPHIN, using a frame-based knowledge base to support reference services in a digital library. DOLPHIN can guide the user to appropriate service.

#index 443439
#* A Dynamic Manifestation Approach for Providing Universal Access to Digital Library Objects
#@ Nabil R. Adam;Vijayalakshmi Atluri;Igg Adiwijaya;Sujata Banerjee;Richard Holowczak
#t 2001
#c 7
#% 399
#% 36236
#% 156513
#% 218213
#% 222246
#% 245787
#% 249976
#% 422876
#% 531426
#% 535805
#! Digital libraries are concerned with the creation and management of information sources, the movement of information across global networks, and the effective use of this information by a wide range of users. A digital library is a vast collection of objects that are of multimedia nature, e.g., text, video, images, and audio. Users wishing to access the digital library objects may possess varying capabilities, preferences, domain expertise, and may use different information appliances. Facilitating access to complex multimedia digital library objects that suits the users' requirements is known as universal access. In this paper, we present an object manifestation approach in which digital library objects automatically manifest themselves to cater to the users' capabilities and characteristics. We provide a formal framework, based on Petri nets, to represent the various components of the digital library objects, their modality and fidelity, and the playback synchronization relationships among them. We develop methodologies for object delivery without any deadtime under network delays.

#index 443440
#* Introduction to the Special Section on the Fifth International Workshop on Multimedia Information Systems
#@ Leana Golubchik;Satish K. Tripathi;Vassilis J. Tsotras
#t 2001
#c 7
#! First Page of the Article

#index 443441
#* Hierarchical Case-Based Reasoning Integrating Case-Based and Decompositional Problem-Solving Techniques for Plant-Control Software Design
#@ Barry Smyth;Mark T. Keane;Pádraig Cunningham
#t 2001
#c 7
#% 37909
#% 68243
#% 84381
#% 84384
#% 85574
#% 130157
#% 131984
#% 140191
#% 140587
#% 172505
#% 175391
#% 257885
#% 362441
#% 440730
#% 440733
#% 490445
#% 494284
#% 1272167
#% 1275276
#% 1275277
#! Case-based reasoning (CBR) is an artificial intelligence technique that emphasises the role of past experience during future problem solving. New problems are solved by retrieving and adapting the solutions to similar problems, solutions that have been stored and indexed for future reuse as cases in a case-base. The power of CBR is severely curtailed if problem solving is limited to the retrieval and adaptation of a single case so most CBR systems dealing with complex problem solving tasks have to use multiple cases. This paper describes and evaluates the technique of hierarchical case-based reasoning, which allows complex problems to be solved by reusing multiple cases at various levels of abstraction. The technique is described in the context of Déjà Vu, a CBR system aimed at automating plant-control software design.

#index 443442
#* Scheduling Algorithms for the Broadcast Delivery of Digital Products
#@ Joel L. Wolf;Mark S. Squillante;John J. Turek;Philip S. Yu;Jay Sethuraman
#t 2001
#c 7
#% 136962
#% 173593
#% 217812
#% 235857
#% 241295
#% 256021
#% 259634
#% 263728
#% 269631
#% 281866
#% 282523
#% 282756
#% 290747
#% 300146
#% 304594
#% 325202
#% 365433
#% 373810
#% 374261
#% 408396
#% 424285
#% 665402
#% 673003
#% 1147758
#! We provide scheduling algorithms that attempt to maximize the profits of a broadcast-based electronic delivery service for digital products purchased, for example, at e-commerce sites on the World Wide Web. Examples of such products include multimedia objects such as CDs and DVDs. Other examples include software and, with increasing popularity, electronic books as well. We consider two separate alternatives, depending in part on the sophistication of the set-top box receiving the product at the customer end. The first, more restrictive option, assumes that the atomic unit of transmission of the product is the entire object, which must be transmitted in order from start to finish. We provide a solution based in part on a transportation problem formulation for this so-called noncyclic scheduling problem. The second alternative, which is less restrictive, assumes that the product may be transmitted cyclically in smaller segments, starting from an arbitrary point in the object. Three heuristics are provided for this difficult cyclic scheduling problem. Both scenarios assume that the broadcasts of the same digital product to multiple customers can be 驴batched.驴 We examine the effectiveness of these algorithms via simulation experiments under varying parametric assumptions. Each of the three cyclic scheduling algorithms perform better than the noncyclic algorithm. Moreover, one of the cyclic scheduling algorithms emerges as the clear winner.

#index 443443
#* Minimizing Bandwidth Requirements for On-Demand Data Delivery
#@ Derek Eager;Mary Vernon;John Zahorjan
#t 2001
#c 7
#% 173593
#% 201678
#% 209901
#% 212727
#% 236747
#% 247118
#% 261877
#% 286868
#% 286869
#% 303708
#% 321241
#% 336298
#% 627006
#% 632253
#% 635331
#% 1180311
#% 1819585
#! Two recent techniques for multicast or broadcast delivery of streaming media can provide immediate service to each client request, yet achieve considerable client stream sharing which leads to significant server and network bandwidth savings. This paper considers 1) how well these recently proposed techniques perform relative to each other and 2) whether there are new practical delivery techniques that can achieve better bandwidth savings than the previous techniques over a wide range of client request rates. The principal results are as follows: First, the recent partitioned dynamic skyscraper technique is adapted to provide immediate service to each client request more simply and directly than the original dynamic skyscraper method. Second, at moderate to high client request rates, the dynamic skyscraper method has required server bandwidth that is significantly lower than the recent optimized stream tapping/patching/controlled multicast technique. Third, the minimum required server bandwidth for any delivery technique that provides immediate real-time delivery to clients increases logarithmically (with constant factor equal to one) as a function of the client request arrival rate. Furthermore, it is (theoretically) possible to achieve very close to the minimum required server bandwidth if client receive bandwidth is equal to two times the data streaming rate and client storage capacity is sufficient for buffering data from shared streams. Finally, we propose a new practical delivery technique, called hierarchical multicast stream merging (HMSM), which has a required server bandwidth that is lower than the partitioned dynamic skyscraper and is reasonably close to the minimum achievable required server bandwidth over a wide range of client request rates.

#index 443444
#* Indexing Animated Objects Using Spatiotemporal Access Methods
#@ George Kollios;Vassilis J. Tsotras;Dimitrios Gunopulos;Alex Delis;Marios Hadjieleftheriou
#t 2001
#c 7
#% 8917
#% 58371
#% 68091
#% 70370
#% 86950
#% 86952
#% 102759
#% 169940
#% 182672
#% 201876
#% 213975
#% 216621
#% 219847
#% 237204
#% 239697
#% 245787
#% 247387
#% 257892
#% 260045
#% 261733
#% 261903
#% 273706
#% 287070
#% 296090
#% 299979
#% 300174
#% 421073
#% 427199
#% 443130
#% 443181
#% 443257
#% 461912
#% 462059
#% 480093
#% 480473
#% 481304
#% 481455
#% 503882
#% 527166
#% 560837
#% 571076
#% 571296
#% 627030
#% 1857495
#! We present a new approach for indexing animated objects and efficiently answering queries about their position in time and space. In particular, we consider an animated movie as a spatiotemporal evolution. A movie is viewed as an ordered sequence of frames, where each frame is a 2D space occupied by the objects that appear in that frame. The queries of interest are range queries of the form, 驴find the objects that appear in area $S$ between frames $f_i$ and $f_j$驴 as well as nearest neighbor queries such as, 驴find the $q$ nearest objects to a given position $A$ between frames $f_i$ and $f_j$.驴 The straightforward approach to index such objects considers the frame sequence as another dimension and uses a 3D access method (such as, an R-Tree or its variants). This, however, assigns long 驴lifetime驴 intervals to objects that appear through many consecutive frames. Long intervals are difficult to cluster efficiently in a 3D index. Instead, we propose to reduce the problem to a partial-persistence problem. Namely, we use a 2D access method that is made partially persistent. We show that this approach leads to faster query performance while still using storage proportional to the total number of changes in the frame evolution. What differentiates this problem from traditional temporal indexing approaches is that objects are allowed to move and/or change their extent continuously between frames. We present novel methods to approximate such object evolutions. We formulate an optimization problem for which we provide an optimal solution for the case where objects move linearly. Finally, we present an extensive experimental study of the proposed methods. While we concentrate on animated movies, our approach is general and can be applied to other spatiotemporal applications as well.

#index 443445
#* Constructing Communication Subgraphs and Deriving an Optimal Synchronization Interval for Distributed Virtual Environment Systems
#@ John C. S. Lui
#t 2001
#c 7
#% 149228
#% 280510
#% 408396
#% 588994
#% 672563
#% 1793577
#% 1793579
#! In this paper, we consider problems of constructing a communication subgraph and deriving an optimal synchronization interval for a distributed virtual environment system (DVE). In general, a DVE system is a distributed system which allows many clients who are located in different parts of the network to concurrently explore and interact with each other under a high resolution, 3D, graphical virtual environment. Each client in a DVE system is represented by an avatar in the virtual environment, and each avatar can move and interact freely in the virtual environment. There are many challenging issues in designing a cost-effective DVE system. In this work, we address two important design issues, namely: 1) how to construct a communication subgraph which can efficiently carry traffic generated by all clients in a DVE system, and 2) how to guarantee that each participating client has the same consistent view of the virtual world. In other words, if there is an action taken by an avatar or if there is any change in the state of an object in the virtual world, every participating client will be able to view the change. To provide this consistent view, a DVE system needs to perform synchronization actions periodically. We present several algorithms for constructing a communication subgraph. In the subgraph construction, we try to reduce the consumption of network bandwidth resources or reduce the maximum delay between any two clients in a DVE system. Based on a given communication subgraph, we then derive the optimal synchronization interval so as to guarantee the view consistency among all participating clients. The derivation of the optimal synchronization interval is based on the theory of Markov chains and the fundamental matrix.

#index 443446
#* Nondeterministic, Nonmonotonic Logic Databases
#@ Fosca Giannotti;Giuseppe Manco;Mirco Nanni;Dino Pedreschi
#t 2001
#c 7
#% 36683
#% 53385
#% 55408
#% 64408
#% 176471
#% 234756
#% 268779
#% 280441
#% 333325
#% 417540
#% 417542
#% 459254
#% 459259
#% 459276
#% 461208
#% 478261
#% 544738
#% 558720
#! We consider in this paper an extension of Datalog with mechanisms for temporal, nonmonotonic, and nondeterministic reasoning, which we refer to as Datalog++. We show, by means of examples, its flexibility in expressing queries concerning aggregates and data cube. Also, we show how iterated fixpoint and stable model semantics can be combined to the purpose of clarifying the semantics of Datalog++ programs and supporting their efficient execution. Finally, we provide a more concrete implementation strategy on which basis the design of optimization techniques tailored for Datalog++ is addressed.

#index 443447
#* A Query Model to Synthesize Answer Intervals from Indexed Video Units
#@ Sujeet Pradhan;Keishi Tajima;Katsumi Tanaka
#t 2001
#c 7
#% 84656
#% 101649
#% 151355
#% 192709
#% 194009
#% 200941
#% 201893
#% 287313
#% 319244
#% 382491
#% 422883
#% 434690
#% 437405
#% 437509
#% 443136
#% 452790
#% 452796
#% 462797
#% 482843
#% 520122
#% 520126
#% 527787
#% 528063
#% 662394
#% 1180138
#! While a query result in a traditional database is a subset of the database, in a video database, it is a set of subintervals extracted from the raw video sequence. It is very hard, if not impossible, to predetermine all the queries that will be issued in the future, and all the subintervals that will become necessary to answer them. As a result, conventional query frameworks are not applicable to video databases. In this paper, we propose a new video query model that computes query results by dynamically synthesizing needed subintervals from fragmentary indexed intervals in the database. We introduce new interval operations required for that computation. We also propose methods to compute relative relevance of synthesized intervals to a given query. A query result is a list of synthesized intervals sorted in the order of their degree of relevance.

#index 443448
#* A Graph-Based Approach for Discovering Various Types of Association Rules
#@ Show-Jane Yen;Arbee L. P. Chen
#t 2001
#c 7
#% 201894
#% 248791
#% 282005
#% 304180
#% 340289
#% 443193
#% 443310
#% 463883
#% 481290
#% 481758
#! Mining association rules is an important task for knowledge discovery. We can analyze past transaction data to discover customer behaviors such that the quality of business decision can be improved. Various types of association rules may exist in a large database of customer transactions. The strategy of mining association rules focuses on discovering large itemsets, which are groups of items which appear together in a sufficient number of transactions. In this paper, we propose a graph-based approach to generate various types of association rules from a large database of customer transactions. This approach scans the database once to construct an association graph and then traverses the graph to generate all large itemsets. Empirical evaluations show that our algorithms outperform other algorithms which need to make multiple passes over the database.

#index 443449
#* Learning Similarity Matching in Multimedia Content-Based Retrieval
#@ Joo-Hwee Lim;Jian Kang Wu;Sumeet Singh;Desai Narasimhalu
#t 2001
#c 7
#% 91872
#% 225874
#% 284557
#% 437405
#! Many multimedia content-based retrieval systems allow query formulation with user setting of relative importance of features (e.g., color, texture, shape, etc) to mimic the user's perception of similarity. However, the systems do not modify their similarity matching functions, which are defined during the system development. In this paper, we present a neural network-based learning algorithm for adapting similarity matching function toward the user's query preference based on his/her relevance feedback. The relevance feedback is given as ranking errors (misranks) between the retrieved and desired lists of multimedia objects. The algorithm is demonstrated for facial image retrieval using the NIST Mugshot Identification Database with encouraging results.

#index 443450
#* Scalable Color Image Indexing and Retrieval Using Vector Wavelets
#@ Elif Albuz;Erturk Kocalar;Ashfaq A. Khokhar
#t 2001
#c 7
#% 58636
#% 75571
#% 100198
#% 116390
#% 120270
#% 169940
#% 196977
#% 208708
#% 219847
#% 228351
#% 232472
#% 254078
#% 359751
#% 458521
#% 1758843
#! This paper presents a scalable content-based image indexing and retrieval system based on vector wavelet coefficients of color images. Highly decorrelated wavelet coefficient planes are used to acquire a search efficient feature space. The feature space is subsequently indexed using properties of all the images in the database. Therefore, the feature key of an image not only corresponds to the content of the image itself but also to how much the image is different from the other images being stored in the database. The search time linearly depends on the number of images similar to the query image and is independent of the database size. We show that, in a database of 5,000 images, query search takes less than 30 msec on a 266 MHz Pentium II processor, compared to several seconds of retrieval time in the earlier systems proposed in the literature.

#index 443451
#* Correction to 'Integrating Security and Real-Time Requirements Using Covert Channel Capacity'
#@ Sang H. Son;Ravi Mukkamala;Rasikan David
#t 2001
#c 7
#% 443380
#! First Page of the Article

#index 443452
#* Mining Associations with the Collective Strength Approach
#@ C. C. Aggarwal;P. S. Yu
#t 2001
#c 7
#% 152934
#% 172386
#% 210160
#% 227917
#% 227919
#% 232106
#% 248012
#% 412588
#% 443082
#% 443164
#% 461909
#% 479785
#% 481290
#% 481758
#! The large itemset model has been proposed in the literature for finding associations in a large database of sales transactions. A different method for evaluating and finding itemsets referred to as strongly collective itemsets is proposed. We propose a criterion stressing the importance of the actual correlation of the items with one another rather than their absolute level of presence. Previous techniques for finding correlated itemsets are not necessarily applicable to very large databases. We provide an algorithm which provides very good computational efficiency, while maintaining statistical robustness. The fact that this algorithm relies on relative measures rather than absolute measures such as support also implies that the method can be applied to find association rules in data sets in which items may appear in a sizeable percentage of the transactions (dense data sets), data sets in which the items have varying density, or even negative association rules.

#index 443453
#* Accurate Modeling of Region Data
#@ G. Proietti;C. Faloutsos
#t 2001
#c 7
#% 13041
#% 64431
#% 86950
#% 137887
#% 153260
#% 164360
#% 172949
#% 223645
#% 237187
#% 246003
#% 285924
#% 405926
#% 435137
#% 464859
#% 481428
#% 481620
#! Spatial data appear in numerous applications, such as GISmultimediaand even traditional databases. Most of the analysis on spatial data has focused on point data, typically using the uniformity assumption, or, more accurately, a fractal distribution. However, no results exist for nonpoint spatial data, like 2D regions (e.g., islands), 3D volumes (e.g., physical objects in the real world), etc. This is exactly the problem we solve in this paper. Based on experimental evidence that real areas and volumes follow a 驴power law,驴 that we named REGAL (REGion Area Law), we show 1) the theoretical implications of our model and its connection with the ubiquitous fractals and 2) the first of its practical uses, namely, the selectivity estimation for range queries. Experiments on a variety of real data sets (islands, lakes, and human-inhabited areas) show that our method is extremely accurate, enjoying a maximum relative error ranging from 1 to 5 percent, versus 30-70 percent of a naive model that uses the uniformity assumption.

#index 443454
#* Efficient Processing of Nested Fuzzy SQL Queries in a Fuzzy Database
#@ Q. Yang;W. Zhang;C. Liu;J. Wu;C. Yu;H. Nakajima;N. D. Rishe
#t 2001
#c 7
#% 663
#% 2783
#% 32878
#% 48721
#% 72031
#% 77969
#% 86928
#% 90358
#% 90725
#% 94459
#% 116456
#% 136740
#% 166819
#% 287005
#% 287333
#% 383684
#% 443339
#% 444199
#% 463751
#% 464046
#% 479614
#% 479756
#% 480091
#% 480774
#% 564426
#% 750877
#% 1788921
#! In a fuzzy relational database where a relation is a fuzzy set of tuples and ill-known data are represented by possibility distributions, nested fuzzy queries can be expressed in the Fuzzy SQL language. Although it provides a very convenient way for users to express complex queries, a nested fuzzy query may be very inefficient to process with the naive evaluation method based on its semantics. In conventional databases, nested queries are unnested to improve the efficiency of their evaluation. In this paper, we extend the unnesting techniques to process several types of nested fuzzy queries. An extended merge-join is used to evaluate the unnested fuzzy queries. As shown by both theoretical analysis and experimental results, the unnesting techniques with the extended merge-join significantly improve the performance of evaluating nested fuzzy queries.

#index 443455
#* Aggregation of Imprecise and Uncertain Information in Databases
#@ S. McClean;B. Scotney;M. Shapcott
#t 2001
#c 7
#% 119816
#% 142899
#% 152588
#% 158203
#% 172386
#% 212999
#% 232136
#% 279484
#% 442692
#% 442830
#% 443030
#% 443038
#% 445120
#! Information stored in a database is often subject to uncertainty and imprecision. Probability theory provides a well-known and well understood way of representing uncertainty and may thus be used to provide a mechanism for storing uncertain information in a database. We consider the problem of aggregation using an imprecise probability data model that allows us to represent imprecision by partial probabilities and uncertainty using probability distributions. Most work to date has concentrated on providing functionality for extending the relational algebra with a view to executing traditional queries on uncertain or imprecise data. However, for imprecise and uncertain data, we often require aggregation operators that provide information on patterns in the data. Thus, while traditional query processing is tuple-driven, processing of uncertain data is often attribute-driven where we use aggregation operators to discover attribute properties. The aggregation operator that we define uses the Kullback-Leibler information divergence between the aggregated probability distribution and the individual tuple values to provide a probability distribution for the domain values of an attribute or group of attributes. The provision of such aggregation operators is a central requirement in furnishing a database with the capability to perform the operations necessary for knowledge discovery in databases.

#index 443456
#* Structured Development of Problem Solving Methods
#@ D. Fensel;E. Motta
#t 2001
#c 7
#% 2768
#% 21138
#% 25470
#% 37909
#% 48702
#% 76459
#% 84384
#% 85019
#% 105499
#% 107135
#% 115331
#% 116303
#% 116704
#% 134111
#% 150866
#% 163262
#% 191678
#% 197428
#% 205176
#% 208204
#% 220416
#% 258933
#% 265192
#% 270806
#% 270810
#% 311172
#% 320603
#% 354383
#% 362942
#% 364852
#% 370526
#% 418892
#% 441013
#% 444996
#% 445169
#% 459117
#% 459231
#% 459235
#% 459375
#% 459617
#% 459623
#% 459643
#% 459645
#% 460128
#% 495625
#% 585668
#% 586798
#% 743482
#% 1385470
#! Problem solving methods (PSMs) describe the reasoning components of knowledge-based systems as patterns of behavior that can be reused across applications. While the availability of extensive problem solving method libraries and the emerging consensus on problem solving method specification languages indicate the maturity of the field, a number of important research issues are still open. In particular, very little progress has been achieved on foundational and methodological issues. Hence, despite the number of libraries which have been developed, it is still not clear what organization principles should be adopted to construct truly comprehensive libraries, covering large numbers of applications and encompassing both task-specific and task-independent problem solving methods. In this paper, we address these 驴fundamental驴 issues and present a comprehensive and detailed framework for characterizing problem solving methods and their development process. In particular, we suggest that PSM development consists of introducing assumptions and commitments along a three-dimensional space defined in terms of problem-solving strategy, task commitments, and domain (knowledge) assumptions. Individual moves through this space can be formally described by means of adapters. In the paper, we illustrate our approach and argue that our architecture provides answers to three fundamental problems related to research in problem solving methods: 1) what is the epistemological structure and what are the modeling primitives of PSMs? 2) how can we model the PSM development process? and 3) how can we develop and organize truly comprehensive and manageable libraries of problem solving methods?

#index 443457
#* Disk Scheduling in Video Editing Systems
#@ W. G. Aref;I. Kamel;S. Ghandeharizadeh
#t 2001
#c 7
#% 124017
#% 149274
#% 149276
#% 151340
#% 159084
#% 173593
#% 201931
#% 395431
#% 422887
#% 437335
#% 519989
#! Modern video servers support both video-on-demand and nonlinear editing applications. Video-on-demand servers enable the user to view video clips or movies from a video database, while nonlinear editing systems enable the user to manipulate the content of the video database. Applications such as video and news editing systems require that the underlying storage server be able to concurrently record live broadcast information, modify prerecorded data, and broadcast an authored presentation. A multimedia storage server that efficiently supports such a diverse group of activities constitutes the focus of this study. A novel real-time disk scheduling algorithm is presented that treats both read and write requests in a homogeneous manner in order to ensure that their deadlines are met. Due to real-time demands of movie viewing, read requests have to be fulfilled within certain deadlines; otherwise, they are considered lost. Since the data to be written into disk is stored in main memory buffers, write requests can be postponed until critical read requests are processed. However, write requests still have to be processed within reasonable delays and without the possibility of indefinite postponement. This is due to the physical constraint of the limited size of the main memory write buffers. The new algorithm schedules both read and write requests appropriately, to minimize the amount of disk reads that do not meet their presentation deadlines, and to avoid indefinite postponement and large buffer sizes in the case of disk writes. Simulation results demonstrate that the proposed algorithm offers low violations of read deadlines, reduces waiting time for lower priority disk requests, and improves the throughput of the storage server by enhancing the utilization of available disk bandwidth.

#index 443458
#* Virtual Images for Similarity Retrieval in Image Databases
#@ G. Petraglia;M. Sebillo;M. Tucci;G. Tortora
#t 2001
#c 7
#% 23998
#% 68781
#% 78243
#% 100198
#% 100673
#% 116335
#% 124680
#% 150376
#% 169940
#% 256643
#% 317950
#% 319244
#% 406493
#% 437405
#% 442807
#% 442826
#% 443053
#% 443054
#% 443133
#% 445709
#% 458521
#% 554673
#% 592180
#% 725465
#% 1346975
#% 1378171
#% 1378231
#! We introduce the virtual image, an iconic index suited for pictorial information access in a pictorial database, and a similarity retrieval approach based on virtual images to perform content-based retrieval. A virtual image represents the spatial information contained in a real image in explicit form by means of a set of spatial relations. This is useful to efficiently compute the similarity between a query and an image in the database. We also show that virtual images support real-world applications that require translation, reflection, and/or rotation invariance of image representation.

#index 443459
#* Nonmonotonic Reasoning as Prioritized Argumentation
#@ J. -H. You;X. Wang;L. -Y. Yuan
#t 2001
#c 7
#% 1146
#% 21139
#% 36236
#% 36534
#% 100152
#% 100155
#% 103705
#% 142468
#% 142492
#% 151859
#% 154317
#% 157418
#% 171069
#% 175371
#% 198464
#% 213978
#% 231742
#% 289952
#% 417668
#% 495651
#% 499491
#% 499501
#% 501038
#% 566055
#% 1272277
#% 1273699
#! This paper proposes a formalism for nonmonotonic reasoning based on prioritized argumentation. We argue that nonmonotonic reasoning in general can be viewed as selecting monotonic inferences by a simple notion of priority among inference rules. More importantly, these types of constrained inferences can be specified in a knowledge representation language where a theory consists of a collection of rules of first order formulas and a priority among these rules. We recast default reasoning as a form of prioritized argumentation and illustrate how the parameterized formulation of priority may be used to allow various extensions and modifications to default reasoning. We also show that it is possible, but more difficult, to express prioritized argumentation by default logic: Even some particular forms of prioritized argumentation cannot be represented modularly by defaults under the same language.

#index 443460
#* Rewriting Queries Using Views
#@ S. Flesca;S. Greco
#t 2001
#c 7
#% 23898
#% 33376
#% 36683
#% 53385
#% 64424
#% 77167
#% 77168
#% 109403
#% 164405
#% 198465
#% 213982
#% 237189
#% 237190
#% 248032
#% 248038
#% 384978
#% 416034
#% 464056
#% 464203
#% 464717
#% 464727
#% 707146
#! In this paper, we consider the problem of answering queries using materialized views in the presence of negative goals. The solution is carried out by 驴inverting驴 views and deriving both positive and negative knowledge. In order to derive negative knowledge, we invert conjunctive views with negation into a set of (extended) views which may also have, in addition to negation-as-failure, a different form of negation called classical (or strong) negation. We also consider the case of disjunctive views and present a technique which permits us to infer both positive and negative knowledge. Furthermore, we extend previous techniques for inferring knowledge from views based on relations with functional dependencies. Finally, we present a prototype of a system developed at the University of Calabria.

#index 443461
#* Transaction Repair for Integrity Enforcement
#@ L. V. Orman
#t 2001
#c 7
#% 3047
#% 51400
#% 59350
#% 67457
#% 83315
#% 86946
#% 100827
#% 102314
#% 140054
#% 167258
#% 182424
#% 234797
#% 268764
#% 442727
#% 442767
#% 442973
#% 443138
#% 443201
#% 463419
#% 463590
#% 481128
#% 490317
#! A transaction repair system detects erroneous transactions as they update the database, and for each erroneous transaction, it finds the necessary and sufficient changes to the transaction that would repair it. Detection and repair are the two essential components of semantic integrity maintenance since detection alone would leave the user with the difficult responsibility of manually correcting and reentering an erroneous transaction. Both detection and repair rely on incremental integrity maintenance techniques. The detection process takes advantage of the integrity of the database before the transaction, and detects only the new errors introduced by the transaction. The repair process takes advantage of the integrity before the transaction and integrity violation after the transaction but before the repair. Such a two-step incremental analysis produces the minimal repair necessary and sufficient to correct the transaction. All necessary and sufficient repairs are generated, for all first order constraints, and by using only substitution with no resolution search. Multiple constraints are repaired in parallel, with no sequencing of constraints, and no possibility of cycles. Extensions to recursive constraints and nondeterministic transactions are provided.

#index 443462
#* Time-Space Trade-Off Analysis of Morphic Trie Images
#@ R. W. P. Luk
#t 2001
#c 7
#% 24076
#% 105938
#% 224135
#% 252608
#% 295947
#% 319848
#% 320817
#% 321327
#% 326878
#% 407822
#% 443055
#! In this paper, we explore the use of tries to represent tries. A morphic trie image is a trie that represents a set of transformed keywords using an isomorphism h: \Sigma^* \rightarrow (\sigma^q)^*. Morphic trie images using tenary alphabets achieve near optimal performances but approximation errors degrade their performances. A condition which determines whether tenary or bit tries should be used is found. Even though bit tries have better storage reduction in some cases, tenary tries access faster than bit tries. We show that the morphic trie images use less space than minimal prefix tries. If morphic trie images were compressed to form minimal prefix tries, then the total storage reduction is the product of the two. Approximation errors have no effect on minimal prefix tries.

#index 443463
#* Protecting Respondents' Identities in Microdata Release
#@ P. Samarati
#t 2001
#c 7
#% 36683
#% 67453
#% 374401
#% 664495
#% 664552
#! Today's globally networked society places great demand on the dissemination and sharing of information. While in the past released information was mostly in tabular and statistical form, many situations call today for the release of specific data (microdata). In order to protect the anonymity of the entities (called respondents) to which information refers, data holders often remove or encrypt explicit identifiers such as names, addresses, and phone numbers. Deidentifying data, however, provides no guarantee of anonymity. Released information often contains other data, such as race, birth date, sex, and ZIP code, that can be linked to publicly available information to reidentify respondents and inferring information that was not intended for disclosure. In this paper we address the problem of releasing microdata while safeguarding the anonymity of the respondents to which the data refer. The approach is based on the definition of k-anonymity. A table provides k-anonymity if attempts to link explicitly identifying information to its content map the information to at least k entities. We illustrate how k-anonymity can be provided without compromising the integrity (or truthfulness) of the information released by using generalization and suppression techniques. We introduce the concept of minimal generalization that captures the property of the release process not to distort the data more than needed to achieve k-anonymity, and present an algorithm for the computation of such a generalization. We also discuss possible preference policies to choose among different minimal generalizations.

#index 443464
#* Spatio-Temporal Composition of Video Objects: Representation and Querying in Video Database Systems
#@ N. Pissinou;I. Radev;K. Makki;W. J. Campbell
#t 2001
#c 7
#% 201880
#% 319244
#% 380823
#% 442807
#% 442974
#% 443220
#% 445792
#% 452790
#% 463902
#% 614630
#% 661684
#% 663301
#! A key characteristic of video data is the associated spatial and temporal semantics. It is important that a video model models the characteristics of objects and their relationships in time and space. Allen's 13 temporal relationships are often used in formulating queries that contain the temporal relationships among video frames. For the spatial relationships, most of the approaches are based on projecting objects on a two or three-dimensional coordinate system. However, very few attempts have been made formally to represent the spatio-temporal relationships of objects contained in the video data and to formulate queries with spatio-temporal constraints. The purpose of our work is to design a model representation for the specification of the spatio-temporal relationships among objects in video sequences. The model describes the spatial relationships among objects for each frame in a given video scene and the temporal relationships (for this frame) of the temporal intervals measuring the duration of these spatial relationships. It also models the temporal composition of an object, which reflects the evolution of object's spatial relationships over the subsequent frames in the video scene and in the entire video sequence. Our model representation also provides an effective and expressive way for the complete and precise specification of distances among objects in digital video. This model is a basis for the annotation of raw video.

#index 443465
#* Naive Semantics to Support Automated Database Design
#@ V. C. Storey;R. C. Goldstein;H. Ullrich
#t 2002
#c 7
#% 22948
#% 38689
#% 67873
#% 110011
#% 117899
#% 156337
#% 172378
#% 186583
#% 198006
#% 198055
#% 201401
#% 250472
#% 271706
#% 370096
#% 443467
#% 452773
#% 497942
#% 535023
#% 535540
#% 535711
#! Research devoted to developing knowledge-based tools for database design has demonstrated that it is possible to encode a great deal of process knowledge about database design in a (knowledge-based) computer program. However, experience with these tools shows that the contribution of an expert human designer extends beyond his or her knowledge of database design techniques. This paper discusses the application of an approach, called Naive Semantics (NS), to simulate the contributions made to a design based on the designer's general knowledge. Naive semantics involves the use of an extensible store of generally understood knowledge about the world. This paper describes the types of information that could be included in a Naive Semantics knowledge base and how that knowledge might be applied to increase the effectiveness of automated database design systems. Results of various implementations of ontologies based on Naive Semantics are discussed.

#index 443466
#* Mining Optimized Association Rules with Categorical and Numeric Attributes
#@ R. Rastogi;K. Shim
#t 2002
#c 7
#% 152934
#% 201894
#% 210160
#% 210162
#% 213977
#% 232102
#% 461909
#% 481290
#% 481588
#% 481754
#% 481758
#! Mining association rules on large data sets has received considerable attention in recent years. Association rules are useful for determining correlations between attributes of a relation and have applications in marketing, financial, and retail sectors. Furthermore, optimized association rules are an effective way to focus on the most interesting characteristics involving certain attributes. Optimized association rules are permitted to contain uninstantiated attributes and the problem is to determine instantiations such that either the support or confidence of the rule is maximized. In this paper, we generalize the optimized association rules problem in three ways: 1) association rules are allowed to contain disjunctions over uninstantiated attributes, 2) association rules are permitted to contain an arbitrary number of uninstantiated attributes, and 3) uninstantiated attributes can be either categorical or numeric. Our generalized association rules enable us to extract more useful information about seasonal and local patterns involving multiple attributes. We present effective techniques for pruning the search space when computing optimized association rules for both categorical and numeric attributes. Finally, we report the results of our experiments that indicate that our pruning algorithms are efficient for a large number of uninstantiated attributes, disjunctions, and values in the domain of the attributes.

#index 443467
#* A Methodology for Learning Across Application Domains for Database Design Systems
#@ V. C. Storey;D. Dey
#t 2002
#c 7
#% 4797
#% 13048
#% 22948
#% 25639
#% 38689
#% 49594
#% 55296
#% 67245
#% 85086
#% 106916
#% 111920
#% 115412
#% 116575
#% 158907
#% 187741
#% 198006
#% 198055
#% 250472
#% 271706
#% 297184
#% 314685
#% 370096
#% 443465
#% 445677
#% 452773
#% 535540
#% 535547
#% 535711
#! Although database design tools have been developed that attempt to automate (or semiautomate) the design process, these tools do not have the capability to capture common sense knowledge about business applications and store it in a context-specific manner. As a result, they rely on the user to provide a great deal of 驴trivial驴 details and do not function as well as a human designer who usually has some general knowledge of how an application might work based on his or her common sense knowledge of the real world. Common sense knowledge could be used by a database design system to validate and improve the quality of an existing design or even generate new designs. This requires that context-specific information about different database design applications be stored and generalized into information about specific application domains (e.g., pharmacy, daycare, hospital, university, manufacturing). Such information should be stored at the appropriate level of generality in a hierarchically structured knowledge base so that it can be inherited by the subdomains below. For this to occur, two types of learning must take place. First, knowledge about a particular application domain that is acquired from specific applications within that domain are generalized into a domain node (e.g., entities, relationships, and attributes from various hospital applications are generalized to a hospital node). This is referred to as within domain learning. Second, the information common to two (or more) related application domain nodes is generalized to a higher-level node; for example, knowledge from the car rental and video rental domains may be generalized to a rental node. This is called across domain learning. This paper presents a methodology for learning across different application domains based on a distance measure. The parameters used in this methodology were refined by testing on a set of representative cases; empirical testing provided further validation.

#index 443468
#* Finding Localized Associations in Market Basket Data
#@ C. C. Aggarwal;C. Procopiuc;P. S. Yu
#t 2002
#c 7
#% 36672
#% 152934
#% 172386
#% 210173
#% 227919
#% 232106
#% 248012
#% 248790
#% 248792
#% 252400
#% 273889
#% 273890
#% 280419
#% 300131
#% 406493
#% 462243
#% 479658
#% 479659
#% 479785
#% 480307
#% 481281
#% 481290
#% 631985
#! In this paper, we discuss a technique for discovering localized associations in segments of the data using clustering. Often, the aggregate behavior of a data set may be very different from localized segments. In such cases, it is desirable to design algorithms which are effective in discovering localized associations because they expose a customer pattern which is more specific than the aggregate behavior. This information may be very useful for target marketing. We present empirical results which show that the method is indeed able to find a significantly larger number of associations than what can be discovered by analysis of the aggregate data.

#index 443469
#* Indexing and Retrieval for Genomic Databases
#@ H. E. Williams;Justin Zobel
#t 2002
#c 7
#% 55490
#% 189867
#% 213786
#% 394709
#% 406493
#% 459007
#% 471414
#% 481107
#! Genomic sequence databases are widely used by molecular biologists for homology searching. Amino acid and nucleotide databases are increasing in size exponentially, and mean sequence lengths are also increasing. In searching such databases, it is desirable to use heuristics to perform computationally intensive local alignments on selected sequences and to reduce the costs of the alignments that are attempted. We present an index-based approach for both selecting sequences that display broad similarity to a query and for fast local alignment. We show experimentally that the indexed approach results in significant savings in computationally intensive local alignments and that index-based searching is as accurate as existing exhaustive search schemes.

#index 443470
#* Efficient Global Optimization for Image Registration
#@ Y. Chen;R. R. Brooks;S. S. Iyengar;N. S. V. Rao;J. Barhen
#t 2002
#c 7
#% 131055
#% 141574
#% 168999
#% 181468
#% 204431
#% 211587
#% 248183
#% 703534
#! The image registration problem of finding a mapping that matches data from multiple cameras is computationally intensive. Current solutions to this problem tolerate Gaussian noise, but are unable to perform the underlying global optimization computation in real time. This paper expands these approaches to other noise models and proposes the Terminal Repeller Unconstrained Subenergy Tunneling (TRUST) method, originally introduced by Cetin et al. as an appropriate global optimization method for image registration. TRUST avoids local minima entrapment, without resorting to exhaustive search by using subenergy-tunneling and terminal repellers. The TRUST method applied to the registration problem shows good convergence results to the global minimum. Experimental results show TRUST to be more computationally efficient than either tabu search or genetic algorithms.

#index 443471
#* A Distributed Learning Algorithm for Bayesian Inference Networks
#@ W. Lam;Am. Segre
#t 2002
#c 7
#% 68244
#% 101207
#% 101213
#% 129987
#% 159420
#% 183494
#% 183496
#% 183497
#% 197387
#% 251147
#% 420650
#% 527830
#% 560601
#% 1650660
#% 1650706
#! We present a new distributed algorithm for computing the minimum description length (MDL) in learning Bayesian inference networks from data. Our learning algorithm exploits both properties of the MDL-based score metric and a distributed, asynchronous, adaptive search technique called nagging. Nagging is intrinsically fault-tolerant, has dynamic load balancing features, and scales well. We demonstrate the viability, effectiveness, and scalability of our approach empirically with several experiments using networked machines. More specifically, we show that our distributed algorithm can provide optimal solutions for larger problems as well as good solutions for Bayesian networks of up to 150 variables.

#index 443472
#* 3D-List: A Data Structure for Efficient Video Query Processing
#@ C. C. Liu;A. L. P. Chen
#t 2002
#c 7
#% 23998
#% 57908
#% 68781
#% 166097
#% 190650
#% 240174
#% 320454
#% 321327
#% 434668
#% 442949
#% 445709
#% 452770
#% 452796
#% 463567
#% 481272
#% 481279
#% 632266
#% 641571
#! In this paper, a video query model based on the content of video and iconic indexing is proposed. We extend the notion of two-dimensional strings to three-dimensional strings (3D-Strings) for representing the spatial and temporal relationships among the symbols in both a video and a video query. The problem of video query processing is then transformed into a problem of three-dimensional pattern matching. To efficiently match the 3D-Strings, a data structure, called 3D-List, and its related algorithms are proposed. In this approach, the symbols of a video in the video database are retrieved from the video index and organized as a 3D-List according to the 3D-String of the video query. The related algorithms are then applied on the 3D-List to determine whether this video is an answer to the video query. Based on this approach, we have started a project called Vega. In this project, we have implemented a user friendly interface for specifying video queries, a video index tool for constructing the video index, and a video query processor based on the notion of 3D-List. Some experiments are also performed to show the efficiency and effectiveness of the proposed algorithms.

#index 443473
#* Finite Satisfiability of Integrity Constraints in Object-Oriented Database Schemas
#@ A. Formica
#t 2002
#c 7
#% 23952
#% 26722
#% 36683
#% 40983
#% 47712
#% 70370
#% 80152
#% 82281
#% 84990
#% 99441
#% 116091
#% 116202
#% 164387
#% 172979
#% 235914
#% 252427
#% 263983
#% 287370
#% 292006
#% 384112
#% 408062
#% 442721
#% 443173
#% 443218
#% 452778
#% 462353
#% 479763
#% 480103
#% 491695
#% 535506
#% 546303
#% 560825
#% 560925
#% 603813
#! Checking satisfiability of database constraints is a fundamental problem in database design. In addition, database constraints have to be not only satisfiable but also finitely satisfiable. This problem is generally addressed by using theorem provers that, being developed for first order logic formulas, are based on semidecidable procedures. Furthermore, even in simple cases, theorem provers are quite inefficient in dealing with comparison operators such as, for instance, the equality. In this paper, a decidable, sound, and complete method for checking finite satisfiability of a specific class of integrity constraints for object-oriented databases, including the equality constraints, is presented. The method, that is based on a graph-theoretic approach, has exponential complexity in the worst case, rarely occurring in practice.

#index 443474
#* A Comprehensive Analytical Performance Model for Disk Devices under Random Workloads
#@ P. Triantafillou;S. Christodoulakis;C. A. Georgiadis
#t 2002
#c 7
#% 18669
#% 43195
#% 123891
#% 128705
#% 159079
#% 184516
#% 187204
#% 229745
#% 237195
#% 242287
#% 249264
#% 261891
#% 317962
#% 374002
#% 437335
#% 442793
#% 442881
#% 443345
#% 632238
#% 661705
#% 979675
#! Our goal with this paper is to contribute a common theoretical framework for studying the performance of disk-storage devices. Understanding the performance behavior of these devices will allow prediction of the I/O cost in modern applications. Current disk technologies differ in terms of the fundamental modeling characteristics, which include the magnetic/optical nature, angular and linear velocities, storage capacities, and transfer rates. Angular and linear velocities, storage capacities, and transfer rates are made constant or variable in different existing disk products. Related work in this area has studied Constant Angular Velocity (CAV) magnetic disks and Constant Linear Velocity (CLV) optical disks. In this work, we present a comprehensive analytical model, validated through simulations, for the random retrieval performance of disk devices which takes into account all the above-mentioned fundamental characteristics and includes, as special cases, all the known disk-storage devices. Such an analytical model can be used, for example, in the query optimizer of large traditional databases as well as in an admission controller of multimedia storage servers. Besides the known models for magnetic CAV and optical CLV disks, our unifying model is also reducible to a model for a more recent disk technology, called zoned disks, the retrieval performance of which has not been modeled in detail before. The model can also be used to study the performance retrieval of possible future technologies which combine a number of the above characteristics and in environments containing different types of disks (e.g., magnetic-disk-based secondary storage and optical-disk-based tertiary storage). Using our model, we contribute an analysis of the performance behavior of zoned disks and we compare it against that for the traditional CAV disks, as well as against that of some possible/future technologies. This allows us to gain insights into the fundamental performance trade-offs.

#index 443475
#* High-Dimensional Similarity Joins
#@ K. Shim;R. Srikant;R. Agrawal
#t 2002
#c 7
#% 68091
#% 86950
#% 88056
#% 102772
#% 111868
#% 172909
#% 201893
#% 210187
#% 285932
#% 321455
#% 411694
#% 427199
#% 435141
#% 460862
#% 480093
#% 481609
#% 481956
#! Many emerging data mining applications require a similarity join between points in a high-dimensional domain. We present a new algorithm that utilizes a new index structure, called the $\epsilon$ tree, for fast spatial similarity joins on high-dimensional points. This index structure reduces the number of neighboring leaf nodes that are considered for the join test, as well as the traversal cost of finding appropriate branches in the internal nodes. The storage cost for internal nodes is independent of the number of dimensions. Hence, the proposed index structure scales to high-dimensional data. We analyze the cost of the join for the $\epsilon$ tree and the R-tree family, and show that the $\epsilon$ tree will perform better for high-dimensional joins. Empirical evaluation, using synthetic and real-life data sets, shows that similarity join using the $\epsilon$ tree is twice to an order of magnitude faster than the $R^+$ tree, with the performance gap increasing with the number of dimensions. We also discuss how some of the ideas of the $\epsilon$ tree can be applied to the R-tree family. These biased R-trees perform better than the corresponding traditional R-trees for high-dimensional similarity joins, but do not match the performance of the $\epsilon$ tree.

#index 443476
#* Semiautomatic Acquisition of Semantic Structures for Understanding Domain-Specific Natural Language Queries
#@ H. H. Meng;K. C. Siu
#t 2002
#c 7
#% 99645
#% 103497
#% 103506
#% 131325
#% 442981
#% 443858
#% 624483
#% 703777
#% 740916
#% 747953
#% 748555
#% 818029
#% 968443
#% 969426
#! This paper describes a methodology for semiautomatic grammar induction from unannotated corpora of information-seeking queries in a restricted domain. The grammar contains both semantic and syntactic structures, which are conducive to (spoken) natural language understanding. Our work aims to ameliorate the reliance of grammar development on expert handcrafting or on the availability of annotated corpora. To strive for reasonable coverage on real data, as well as portability across domains and languages, we adopt a statistical approach. Agglomerative clustering using the symmetrized divergence criterion groups words 驴spatially.驴 These words have similar left and right contexts and tend to form semantic classes. Agglomerative clustering using mutual information groups words 驴temporally.驴 These words tend to co-occur sequentially to form phrases or multiword entities. Our approach is amenable to the optional injection of prior knowledge to catalyze grammar induction. The resultant grammar is interpretable by humans and is amenable to hand-editing for refinement. Hence, our approach is semiautomatic in nature. Experiments were conducted using the atis (Air Travel Information Service) corpus and the semiautomatically-induced grammar $G_{SA}$ is compared to an entirely handcrafted grammar $G_H$. $G_H$ took two months to develop and gave concept error rates of 7 percent and 11.3 percent, respectively, in language understanding of two test corpora. $G_{SA}$ took only three days to produce and gave concept errors of 14 percent and 12.2 percent on the corresponding test corpora. These results provide a desirable trade-off between language understanding performance and grammar development effort.

#index 443477
#* A Time-Bound Cryptographic Key Assignment Scheme for Access Control in a Hierarchy
#@ W. G. Tzeng
#t 2002
#c 7
#% 1958
#% 40355
#% 63685
#% 78832
#% 139659
#% 191721
#% 204922
#% 204923
#% 318404
#% 319994
#% 374401
#% 381870
#% 514186
#% 527436
#! The cryptographic key assignment problem is to assign cryptographic keys to a set of partially ordered classes so that the cryptographic key of a higher class can be used to derive the cryptographic key of a lower class. In this paper, we propose a time-bound cryptographic key assignment scheme in which the cryptographic keys of a class are different for each time period, that is, the cryptographic key of class $C_i$ at time $t$ is $K_{i,t}$. Key derivation is constrained not only by the class relation, but also the time period. In our scheme, each user holds some secret parameters whose number is independent of the number of the classes in the hierarchy and the total time periods. We present two novel applications of our scheme. One is to broadcast data to authorized users in a multilevel-security way and the other is to construct a flexible cryptographic key backup system.

#index 443478
#* Practical Data-Oriented Microaggregation for Statistical Disclosure Control
#@ J. Domingo-Ferrer;J. M. Mateo-Sanz
#t 2002
#c 7
#% 67453
#% 169002
#% 374401
#% 375388
#! Microaggregation is a statistical disclosure control technique for microdata disseminated in statistical databases. Raw microdata (i.e., individual records or data vectors) are grouped into small aggregates prior to publication. Each aggregate should contain at least $k$ data vectors to prevent disclosure of individual information, where $k$ is a constant value preset by the data protector. No exact polynomial algorithms are known to date to microaggregate optimally, i.e., with minimal variability loss. Methods in the literature rank data and partition them into groups of fixed-size; in the multivariate case, ranking is performed by projecting data vectors onto a single axis. In this paper, candidate optimal solutions to the multivariate and univariate microaggregation problems are characterized. In the univariate case, two heuristics based on hierarchical clustering and genetic algorithms are introduced which are data-oriented in that they try to preserve natural data aggregates. In the multivariate case, fixed-size and hierarchical clustering microaggregation algorithms are presented which do not require data to be projected onto a single dimension; such methods clearly reduce variability loss as compared to conventional multivariate microaggregation on projected data.

#index 443479
#* Finding and Labeling the Subject of a Captioned Depictive Natural Photograph
#@ N. C. Rowe
#t 2002
#c 7
#% 100207
#% 166097
#% 167201
#% 207803
#% 219847
#% 239579
#% 239594
#% 259465
#% 437408
#% 443701
#% 676177
#% 1271380
#% 1378228
#! We address the problem of finding the subject of a photographic image intended to illustrate some physical object or objects (驴depictive驴) and taken by usual optical means without magnification (驴natural驴). This could help in developing digital image libraries since important image properties like subject size and color of a photograph are not usually mentioned in accompanying captions and can help rank the photograph retrievals for a user. We explore an approach that identifies the 驴visual focus驴 of the image and the 驴depicted concepts驴 in a caption and connects them. The visual focus is determined by using eight domain-independent characteristics of regions in the segmented image, and the caption depiction is identified by a set a rules applied to the parsed and interpreted caption. The visual-focus determination also does combinatorial optimization on sets of regions to find the set that best satisfies focus criteria. Experiments on 100 randomly selected image-caption pairs show significant improvement in precision of retrieval over simpler methods, and, particularly, emphasizes the value of segmentation of the image.

#index 443480
#* Redefining Clustering for High-Dimensional Applications
#@ C. C. Aggarwal;P. S. Yu
#t 2002
#c 7
#% 36672
#% 201893
#% 210173
#% 232764
#% 248790
#% 248792
#% 248798
#% 249321
#% 252400
#% 260008
#% 273889
#% 273890
#% 273891
#% 280417
#% 280419
#% 300131
#% 387914
#% 420081
#% 462243
#% 479658
#% 479659
#% 479962
#% 479973
#% 480307
#% 481281
#% 497939
#% 501353
#% 551620
#% 631984
#% 632073
#% 937189
#! Clustering problems are well-known in the database literature for their use in numerous applications, such as customer segmentation, classification, and trend analysis. High-dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Recent research results indicate that, in high-dimensional data, even the concept of proximity or clustering may not be meaningful. We introduce a very general concept of projected clustering which is able to construct clusters in arbitrarily aligned subspaces of lower dimensionality. The subspaces are specific to the clusters themselves. This definition is substantially more general and realistic than the currently available techniques which limit the method to only projections from the original set of attributes. The generalized projected clustering technique may also be viewed as a way of trying to redefine clustering for high-dimensional applications by searching for hidden subspaces with clusters which are created by interattribute correlations. We provide a new concept of using extended cluster feature vectors in order to make the algorithm scalable for very large databases. The running time and space requirements of the algorithm are adjustable and are likely to trade-off with better accuracy.

#index 443481
#* A Database Perspective on Geospatial Data Modeling
#@ A. Voisard;B. David
#t 2002
#c 7
#% 2115
#% 3523
#% 3901
#% 13043
#% 24070
#% 27043
#% 77927
#% 77929
#% 81968
#% 90724
#% 113841
#% 125600
#% 125631
#% 152941
#% 223645
#% 236412
#% 287631
#% 415958
#% 416030
#% 435137
#% 435148
#% 435157
#% 458583
#% 464007
#% 480113
#% 481434
#% 526694
#% 526700
#% 526858
#% 526860
#% 526996
#% 527006
#% 527007
#% 527030
#% 549073
#% 603136
#! We study the representation and manipulation of geospatial information in a database management system (DBMS). The geospatial data model that we use as a basis hinges on a complex object model, whose set and tuple constructors make it efficient for defining not only collections of geographic objects but also relationships among these objects. In addition, it allows easy manipulation of nonbasic types, such as spatial data types. We investigate the mapping of our reference model onto major commercial DBMS models, namely, a relational model extended to abstract data types (ADT) and an object-oriented model. Our analysis shows the strengths and limits of the two model types for handling highly structured data with spatial components.

#index 443482
#* Fast Indexing and Visualization of Metric Data Sets using Slim-Trees
#@ C. Traina, Jr.;A. Traina;C. Faloutsos;B. Seeger
#t 2002
#c 7
#% 64430
#% 84654
#% 86950
#% 164360
#% 201893
#% 227937
#% 237187
#% 248017
#% 252304
#% 280478
#% 281750
#% 294634
#% 322309
#% 427199
#% 437509
#% 479462
#% 479645
#% 480093
#% 481279
#% 481460
#% 546130
#% 619471
#% 632043
#% 632062
#% 650248
#% 742021
#! Many recent database applications must deal with similarity queries. For such applications, it is important to measure the similarity between two objects using the distance between them. Focusing on this problem, this paper proposes the Slim-tree, a new dynamic tree for organizing metric data sets in pages of fixed size. The Slim-tree uses the triangle inequality to prune distance calculations needed to answer similarity queries over objects in metric spaces. The proposed insertion algorithm uses new policies to select the nodes where incoming objects are stored. When a node overflows, the Slim-tree uses a Minimal Spanning Tree to help with the split. The new insertion algorithm leads to a tree with high storage utilization and improved query performance. The Slim-tree is the first metric access method to tackle the problem of overlap between nodes in metric spaces and to propose a technique to minimize it. The proposed 驴fat-factor驴 is a way to quantify whether a given tree can be improved and also to compare two trees. We show how to use the fat-factor to achieve accurate estimates of the search performance and also how to improve the performance of a metric tree through the proposed 驴Slim-down驴 algorithm. This paper also presents a new tool in the arsenal of resources of Slim-tree aimed at visualizing it. Visualization is a powerful tool for interactive data mining and for the visual tracking of the behavior of a tree under updates. Finally, we present a formula to estimate the number of disk accesses in range queries. Results from experiments with real and synthetic data sets show that the new algorithms of the Slim-tree lead to performance improvements. These results show that the Slim-tree outperforms the M-tree up to 200 percent for range queries. For insertion and split, the Minimal-Spanning-Tree-based algorithm achieves up to 40 times faster insertions. We observed improvements up to 40 percent in range queries after applying the Slim-down algorithm.

#index 443483
#* Trigger Condition Testing and View Maintenance Using Optimized Discrimination Networks
#@ E. N. Hanson;S. Bodagala;U. Chadaga
#t 2002
#c 7
#% 121
#% 1797
#% 32877
#% 43161
#% 51676
#% 53981
#% 86945
#% 86949
#% 102765
#% 102784
#% 152910
#% 156715
#% 206915
#% 210208
#% 286916
#% 411554
#% 442885
#% 443019
#% 463117
#% 480430
#% 481132
#% 481952
#% 707920
#% 708030
#! This paper presents a structure that can be used both for trigger condition testing and view materialization, along with a study of techniques for optimizing the structure. The structure presented is known as a discrimination network. The type of discrimination network introduced and studied in this paper is a highly general type of discrimination network which we call the Gator network. The structure of several alternative Gator network optimizers is described, along with a discussion of optimizer performance, output quality, and accuracy. The optimizers can choose an efficient Gator network for testing the conditions of a set of triggers or optimizing maintenance of a set of views, given information about the structure of the triggers or views, database size, predicate selectivity, and update frequency distribution. The efficiency of optimized Gator networks relative to alternatives is analyzed. The results indicate that overall, Gator networks can be optimized effectively and can give excellent performance for trigger condition testing and materialization of views.

#index 443484
#* Object-Orientated Design of Digital Library Platforms for Multiagent Environments
#@ A. N. Zincir-Heywood;M. I. Heywood;C. R. Chatwin
#t 2002
#c 7
#% 2035
#% 156991
#% 158907
#% 311870
#% 437511
#! The application of an object orientated (OO) methodology to the design of a platform for heterogeneous Digital Libraries with multiagent technologies is demonstrated. Emphasis is placed on maximizing the autonomy of the query processing activity. The Fusion OO paradigm is specifically employed as the basis for the design process due to the significance attributed to the development of object interfaces under static and dynamic conditions. Finally, characteristics of the proposed Domain Index Server (DIS) platform are contrasted with those of an alternative platform (UMDL) by way of the respective Fusion descriptions. This identifies a different emphasis to the interface design between objects in the two platforms: The DIS system uses more dynamic links while the UMDL system focuses on permanent and constant links. Simulation of the two platforms provides performance data that demonstrates the higher capacity of the DIS scheme.

#index 443485
#* A Content-Based Authorization Model for Digital Libraries
#@ N. R. Adam;V. Atluri;E. Bertino;E. Ferrari
#t 2002
#c 7
#% 56346
#% 91075
#% 91634
#% 174162
#% 204453
#% 208794
#% 222246
#% 224126
#% 242735
#% 245787
#% 249182
#% 263981
#% 263982
#% 275836
#% 315632
#% 443012
#% 443057
#% 443103
#% 481432
#% 507403
#% 592727
#% 647598
#% 1001649
#! Digital Libraries (DLs) introduce several challenging requirements with respect to the formulation, specification, and enforcement of adequate data protection policies. Unlike conventional database environments, a DL environment typically is characterized by dynamic user population, often making accesses from remote locations, and by an extraordinarily large amount of multimedia information, stored in a variety of formats. Moreover, in a DL environment, access policies are often specified based on user qualifications and characteristics, rather than user identity (for example, a user can be given access to an R-rated video only if he/she is older than 18 years). Another crucial requirement is the support for content-dependent authorizations on digital library objects (for example, all documents containing discussions on how to operate guns must be made available only to users who are 18 or older). Since traditional authorization models do not adequately meet access control requirements typical to DLs, in this paper, we propose a content-based authorization model suitable for a DL environment. Specifically, the most innovative features of our authorization model are: 1) flexible specification of authorizations based on the qualifications and characteristics of users (including positive and negative), 2) both content-dependent and content-independent access control to digital library objects, and 3) varying granularity of authorization objects ranging from sets of library objects to specific portions of objects.

#index 443486
#* Weighted Fuzzy Reasoning Using Weighted Fuzzy Petri Nets
#@ S. M. Chen
#t 2002
#c 7
#% 1513
#% 4172
#% 34874
#% 40312
#% 162432
#% 212710
#% 374605
#% 405877
#% 442719
#% 443015
#% 1784237
#% 1787873
#! This paper presents a Weighted Fuzzy Petri Net model (WFPN) and proposes a weighted fuzzy reasoning algorithm for rule-based systems based on Weighted Fuzzy Petri Nets. The fuzzy production rules in the knowledge base of a rule-based system are modeled by Weighted Fuzzy Petri Nets, where the truth values of the propositions appearing in the fuzzy production rules and the certainty factors of the rules are represented by fuzzy numbers. Furthermore, the weights of the propositions appearing in the rules are also represented by fuzzy numbers. The proposed weighted fuzzy reasoning algorithm can allow the rule-based systems to perform fuzzy reasoning in a more flexible and more intelligent manner.

#index 443487
#* A High-Level Petri Nets-Based Approach to Verifying Task Structures
#@ J. Lee;L. F. Lai
#t 2002
#c 7
#% 3460
#% 20561
#% 32965
#% 68047
#% 76459
#% 92692
#% 99196
#% 99197
#% 116704
#% 125386
#% 143370
#% 239378
#% 257626
#% 307270
#% 324757
#% 441588
#% 442922
#% 443216
#% 444706
#% 444761
#% 444895
#% 445808
#% 445902
#% 446066
#% 449587
#% 452775
#% 1780697
#% 1780860
#! As knowledge-based system technology gains wider acceptance, there is an increasing need for verifying knowledge-based systems to improve the reliability and quality. Traditionally, attention has been given on verifying knowledge-based systems at the knowledge level, which only addresses structural errors such as redundancy, conflict, and circularity in rule bases. No semantic error such as inconsistency in the requirements specification level has been checked. In this paper, we propose the use of task structures for modeling user requirements and domain knowledge at the requirements specification level, and the use of high-level Petri nets for expressing and verifying the task structure-based specifications. Issues in mapping task structures into high-level Petri nets are identified, for example, the representation of task decomposition, constraints, and state model; the distinction between follow and immediately follow operators; and the composition operator in task structures. The verification of task structures using high-level Petri nets is performed on model specifications of a task through constraints satisfaction and relaxation techniques and on process specifications of the task based on the reachability property and the notion of specificity.

#index 443488
#* Production Systems with Negation as Failure
#@ P. M. Dung;P. Mancarella
#t 2002
#c 7
#% 1146
#% 2079
#% 53385
#% 116292
#% 116294
#% 131385
#% 168778
#% 182421
#% 198464
#% 231742
#% 275032
#% 277342
#% 394417
#% 459259
#% 1425498
#% 1476305
#! We study action rule-based systems with two forms of negation, namely classical negation and 驴negation as failure to find a course of actions.驴 We show by several examples that adding negation as failure to such systems increases their expressiveness in the sense that real life problems can be represented in a natural and simple way. Then, we address the problem of providing a formal declarative semantics to these extended systems by adopting an argumentation-based approach which has been shown to be a simple unifying framework for understanding the declarative semantics of various nonmonotonic formalisms. In this way, we naturally define the grounded (well-founded), stable, and preferred semantics for production systems with negation as failure. Next, we characterize the class of stratified production systems, which enjoy the properties that the above mentioned semantics coincide and that negation as failure to find a course of actions can be computed by a simple bottom-up operator. Stratified production systems can be implemented on top of conventional production systems in two ways. The first way corresponds to the understanding of stratification as a form of priority assignment between rules. We show that this implementation, though sound, is not complete in the general case. Hence, we propose a second implementation by means of an algorithm which transforms a finite stratified production system into a classical one. This is a sound and complete implementation, though computationally hard, as shown in the paper.

#index 443489
#* Solving Constraint Optimization Problems from CLP-Style Specifications Using Heuristic Search Techniques
#@ P. Dasgupta;P. P. Chakrabarti;A. Dey;S. Ghose;W. Bibel
#t 2002
#c 7
#% 241
#% 2194
#% 25470
#% 30096
#% 36698
#% 56471
#% 64788
#% 70370
#% 111939
#% 124754
#% 126389
#% 137995
#% 158865
#% 174161
#% 181054
#% 192282
#% 269391
#% 419894
#% 420636
#% 515906
#% 533487
#% 743459
#% 1271951
#% 1273725
#! This paper presents a framework for efficiently solving logic formulations of combinatorial optimization problems using heuristic search techniques. In order to integrate cost, lower bound and upper bound specifications with conventional logic programming languages, we augment a CLP language with embedded constructs for specifying the cost function and with a few higher order predicates for specifying the lower and upper bound functions. We illustrate how this simple extension vastly enhances the ease with which optimization problems involving combinations of Min and Max can be specified in the extended language CLP* and show that CSLDNF resolution schemes are not efficient for solving optimization problems specified in this language. Therefore, we describe how any problem specified using CLP* can be converted into an implicit AND/OR graph, and present an algorithm GenSolve which can branch and bound using upper and lower bound estimates, thus exploiting the full pruning power of heuristic search techniques. A technical analysis of GenSolve is provided. We also provide experimental results comparing various control strategies for solving CLP* programs.

#index 443490
#* A Comparative Study of Various Nested Normal Forms
#@ W. Y. Mok
#t 2002
#c 7
#% 3808
#% 10245
#% 32888
#% 42401
#% 105779
#% 135556
#% 154332
#% 193287
#% 193291
#% 205246
#% 227962
#% 238413
#% 250473
#% 262249
#% 262998
#% 278619
#% 286995
#% 287754
#% 289424
#% 289425
#% 384872
#% 385637
#% 385828
#% 386381
#% 411706
#% 415998
#% 535534
#% 556309
#% 836134
#! As object-relational databases (ORDBs) become popular in the industry, it is important for database designers to produce database schemes with good properties in these new kinds of databases. One distinguishing feature of an ORDB is that its tables may not be in first normal form. Hence, ORDBs may contain nested relations along with other collection types. To help the design process of an ORDB, several normal forms for nested relations have recently been defined, and some of them are called nested normal forms. In this paper, we investigate four nested normal forms, which are NNF 20, NNF 21, NNF 23, and NNF 25, with respect to generalizing 4NF and BCNF, reducing redundant data values, and design flexibility. Another major contribution of this paper is that we provide an improved algorithm that generates nested relation schemes in NNF 2 from an $ database scheme, which is the most general type of acyclic database schemes. After presenting the algorithm for NNF 20, the algorithms of all of the four nested normal forms and the nested database schemes that they generate are compared. We discovered that when the given set of MVDs is not conflict-free, NNF 20 is inferior to the other three nested normal forms in reducing redundant data values. However, in all of the other cases considered in this paper, NNF 20 is at least as good as all of the other three nested normal forms.

#index 443491
#* Analyzing Outliers Cautiously
#@ X. Liu;G. Cheng;J. X. Wu
#t 2002
#c 7
#% 60576
#% 136350
#% 160258
#% 188076
#% 310175
#% 1499584
#! Outliers are difficult to handle because some of them can be measurement errors, while others may represent phenomena of interest, something significant from the viewpoint of the application domain. Statistical and computational methods have been proposed to detect outliers, but further analysis of outliers requires much relevant domain knowledge. In our previous work, we suggested a knowledge-based method for distinguishing between the measurement errors and phenomena of interest by modeling real measurements 驴how measurements should be distributed in an application domain. In this paper, we make this distinction by modeling measurement errors instead. This is a cautious approach to outlier analysis, which has been successfully applied to a medical problem and may find interesting applications in other domains such as science, engineering, finance, and economics.

#index 443492
#* A Performance Study of Robust Load Sharing Strategies for Distributed Heterogeneous Web Server Systems
#@ M. Colajanni;P. S. Yu
#t 2002
#c 7
#% 3107
#% 66336
#% 190383
#% 236110
#% 241955
#% 243929
#% 254540
#% 255208
#% 268110
#% 351979
#% 424281
#% 424286
#% 433757
#% 433831
#% 434210
#% 437442
#% 445670
#% 445831
#% 452477
#% 452478
#% 591420
#! Replication of information across multiple servers is becoming a common approach to support popular Web sites. A distributed architecture with some mechanisms to assign client requests to Web servers is more scalable than any centralized or mirrored architecture. In this paper, we consider distributed systems in which the Authoritative Domain Name Server (ADNS) of the Web site takes the request dispatcher role by mapping the URL hostname into the IP address of a visible node, that is, a Web server or a Web cluster interface. This architecture can support local and geographical distribution of the Web servers. However, the ADNS controls only a very small fraction of the requests reaching the Web site because the address mapping is not requested for each client access. Indeed, to reduce Internet traffic, address resolution is cached at various name servers for a time-to-live (TTL) period. This opens an entirely new set of problems that traditional centralized schedulers of parallel/distributed systems do not have to face. The heterogeneity assumption on Web node capacity, which is much more likely in practice, increases the order of complexity of the request assignment problem and severely affects the applicability and performance of the existing load sharing algorithms. We propose new assignment strategies, namely adaptive TTL schemes, which tailor the TTL value for each address mapping instead of using a fixed value for all mapping requests. The adaptive TTL schemes are able to address both the nonuniformity of client requests and the heterogeneous capacity of Web server nodes. Extensive simulations show that the proposed algorithms are very effective in avoiding node overload, even for high levels of heterogeneity and limited ADNS control.

#index 443493
#* SEAM: A State-Entity-Activity-Model for a Well-Defined Workflow Development Methodology
#@ A. Bajaj;S. Ram
#t 2002
#c 7
#% 4797
#% 117751
#% 132479
#% 140387
#% 167271
#% 185412
#% 189376
#% 194815
#% 230366
#% 245752
#% 258245
#% 287631
#% 287730
#% 312789
#% 322880
#% 339369
#% 365766
#% 384014
#% 406226
#% 443232
#% 443289
#% 443302
#% 444987
#% 534536
#% 535196
#% 535366
#% 840573
#% 1784292
#! Current conceptual workflow models use either informally defined conceptual models or several formally defined conceptual models that capture different aspects of the workflow, e.g., the data, process, and organizational aspects of the workflow. To the best of our knowledge, there are no algorithms that can amalgamate these models to yield a single view of reality. A fragmented conceptual view is useful for systems analysis and documentation. However, it fails to realize the potential of conceptual models to provide a convenient interface to automate the design and management of workflows. First, as a step toward accomplishing this objective, we propose SEAM (State-Entity-Activity-Model), a conceptual workflow model defined in terms of set theory. Second, no attempt has been made, to the best of our knowledge, to incorporate time into a conceptual workflow model. SEAM incorporates the temporal aspect of workflows. Third, we apply SEAM to a real-life organizational unit's workflows. In this work, we show a subset of the workflows modeled for this organization using SEAM. We also demonstrate, via a prototype application, how the SEAM schema can be implemented on a relational database management system. We present the lessons we learned about the advantages obtained for the organization and, for developers who choose to use SEAM, we also present potential pitfalls in using the SEAM methodology to build workflow systems on relational platforms. The information contained in this work is sufficient enough to allow application developers to utilize SEAM as a methodology to analyze, design, and construct workflow applications on current relational database management systems. The definition of SEAM as a context-free grammar, definition of its semantics, and its mapping to relational platforms should be sufficient also, to allow the construction of an automated workflow design and construction tool with SEAM as the user interface.

#index 443494
#* Efficient C4.5
#@ S. Ruggieri
#t 2002
#c 7
#% 70370
#% 129980
#% 136350
#% 273900
#% 298270
#% 314784
#% 420096
#% 420109
#% 420115
#% 443158
#% 449588
#% 459008
#% 481945
#% 481949
#% 660537
#% 1272280
#! We present an analytic evaluation of the runtime behavior of the C4.5 algorithm which highlights some efficiency improvements. Based on the analytic evaluation, we have implemented a more efficient version of the algorithm, called EC4.5. It improves on C4.5 by adopting the best among three strategies for computing the information gain of continuous attributes. All the strategies adopt a binary search of the threshold in the whole training set starting from the local threshold computed at a node. The first strategy computes the local threshold using the algorithm of C4.5, which, in particular, sorts cases by means of the quicksort method. The second strategy also uses the algorithm of C4.5, but adopts a counting sort method. The third strategy calculates the local threshold using a main-memory version of the RainForest algorithm, which does not need sorting. Our implementation computes the same decision trees as C4.5 with a performance gain of up to five times.

#index 443495
#* An ElGamal-Like Cryptosystem for Enciphering Large Messages
#@ M. S. Hwang;C. C. Chang;K. F. Hwang
#t 2002
#c 7
#% 319994
#% 374401
#% 513401
#! In practice, we usually require two cryptosystems, an asymmetric one and a symmetric one, to encrypt a large confidential message. The asymmetric cryptosystem is used to deliver secret key $SK$, while the symmetric cryptosystem is used to encrypt a large confidential message with the secret key $SK$. In this article, we propose a simple cryptosystem which allows a large confidential message to be encrypted efficiently. Our scheme is based on the Diffie-Hellman distribution scheme, together with the ElGamal cryptosystem.

#index 443496
#* On Scheduling Atomic and Composite Continuous Media Objects
#@ C. Shahabi;S. Ghandeharizadeh;S. Chaudhuri
#t 2002
#c 7
#% 43172
#% 151340
#% 159079
#% 186144
#% 197901
#% 201932
#% 319244
#% 360808
#% 434717
#% 434807
#% 452790
#% 464069
#% 480605
#% 481766
#% 519989
#% 1797483
#! In multiuser multimedia information systems (e.g., movie-on-demand, digital-editing), scheduling the retrievals of continuous media objects becomes a challenging task. This is because of both intra and inter Iobject time dependencies. Intraobject time dependency refers to the real-time display requirement of a continuous media object. Interobject time dependency is the temporal relationships defined among multiple continuous media objects. In order to compose tailored multimedia presentations, a user might define complex time dependencies among multiple continuous media objects with various lengths and display bandwidths. Scheduling the retrieval tasks corresponding to the components of such a presentation in order to respect both inter and intra task time dependencies is the focus of this study. To tackle this task scheduling problem (CRS), we start with a simpler scheduling problem (ARS) where there is no inter task time dependency (e.g., movie-on-demand). Next, we investigate an augmented version of ARS (termed $ARS^+$) where requests reserve displays in advance (e.g., reservation-based movie-on-demand). Finally, we extend our techniques proposed for $ARS$ and $ARS^+$ to address the $CRS$ problem. We also provide formal definition of these scheduling problems and proof of their NP-hardness.

#index 443497
#* Load Balancing of Parallelized Information Filters
#@ N. C. Rowe;A. Zaky
#t 2002
#c 7
#% 5183
#% 26729
#% 67978
#% 75571
#% 124004
#% 158970
#% 204009
#% 318049
#% 404763
#! We investigate the data-parallel implementation of a set of information filters used to rule out uninteresting data from a database or data stream. We develop an analytic model for the costs and advantages of load rebalancing for the parallel filtering processes, as well as a quick heuristic for its desirability. Our model uses binomial models of the filter processes and fits key parameters to the results of extensive simulations. Experiments confirm our model. Rebalancing should pay off whenever processor communications costs are high. Further experiments showed it can also pay off even with low communications costs for 16-64 processes and 1-10 data items per processor; then, imbalances can increase processing time by up to 52 percent in representative cases, and rebalancing can increase it by 78 percent, so our quick predictive model can be valuable. Results also show that our proposed heuristic rebalancing criterion gives close to optimal balancing. We also extend our model to handle variations in filter processing time per data item.

#index 443498
#* A Study of Concurrency Control in Real-Time, Active Database Systems
#@ A. H. Datta;S. H. Son
#t 2002
#c 7
#% 748
#% 27057
#% 37972
#% 37973
#% 77988
#% 86939
#% 117903
#% 122914
#% 124815
#% 168750
#% 172877
#% 194950
#% 268791
#% 286836
#% 288821
#% 404922
#% 442766
#% 442969
#% 480621
#% 480766
#% 614970
#% 672556
#% 677701
#! Real-Time, Active Database Systems (RTADBSs) have attracted a considerable amount of research attention in the very recent past and a number of important applications have been identified for such systems, such as telecommunications network management, automated air traffic control, automated financial trading, process control and military command, and control systems. In spite of the recognized importance of this area, very little research has been devoted to exploring the dynamics of transaction processing in RTADBSs. Concurrency Control (CC) constitutes an integral part of any transaction processing strategy and, thus, deserves special attention. In this paper, we study CC strategies in RTADBSs and postulate a number of CC algorithms. These algorithms exploit the special needs and features of RTADBSs and are shown to deliver substantially superior performance to conventional real-time CC algorithms.

#index 443499
#* Generalized Normal Forms for Probabilistic Relational Data
#@ D. Dey;S. Sarkar
#t 2002
#c 7
#% 4279
#% 18615
#% 22948
#% 36244
#% 36683
#% 44876
#% 55294
#% 67866
#% 108510
#% 158200
#% 158203
#% 205015
#% 209725
#% 235023
#% 237522
#% 287333
#% 442692
#% 442830
#% 442863
#% 443030
#% 480102
#% 481280
#% 573458
#% 836134
#! In recent years, several approaches have been proposed for representing uncertain data in a database. These approaches have typically extended the relational model by incorporating probability measures to capture the uncertainty associated with data items. However, previous research has not directly addressed the issue of normalization for reducing data redundancy and data anomalies in probabilistic databases. In this paper, we examine this issue. To that end, we generalize the concept of functional dependency to stochastic dependency and use that to extend the scope of normal forms to probabilistic databases. Our approach is a consistent extension of the conventional normalization theory and reduces to the latter.

#index 443500
#* Effect of Data Skewness and Workload Balance in Parallel Data Mining
#@ D. W. Cheung;S. D. Lee;V. Xiao
#t 2002
#c 7
#% 115608
#% 152934
#% 199538
#% 201894
#% 210160
#% 227917
#% 227922
#% 340290
#% 340291
#% 443085
#% 459006
#% 463883
#% 464204
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 678171
#! To mine association rules efficiently, we have developed a new parallel mining algorithm FPM on a distributed share-nothing parallel system in which data are partitioned across the processors. FPM is an enhancement of the FDM algorithm, which we previously proposed for distributed mining of association rules. FPM requires fewer rounds of message exchanges than FDM and, hence, has a better response time in a parallel environment. The algorithm has been experimentally found to outperform CD, a representative parallel algorithm for the same goal. The efficiency of FPM is attributed to the incorporation of two powerful candidate sets pruning techniques: distributed and global prunings. The two techniques are sensitive to two data distribution characteristics, data skewness, and workload balance. Metrics based on entropy are proposed for these two characteristics. The prunings are very effective when both the skewness and balance are high. In order to increase the efficiency of FPM, we have developed methods to partition a database so that the resulting partitions have high balance and skewness. Experiments have shown empirically that our partitioning algorithms can achieve these aims very well, in particular, the results are consistently better than a random partitioning. Moreover, the partitioning algorithms incur little overhead. So, using our partitioning algorithms and FPM together, we can mine association rules from a database efficiently.

#index 443501
#* Efficient Aggregation Algorithms for Compressed Data Warehouses
#@ J. Li;J. Srivastava
#t 2002
#c 7
#% 2248
#% 136740
#% 146203
#% 210182
#% 211575
#% 227868
#% 227880
#% 238968
#% 248805
#% 420053
#% 442685
#% 442695
#% 462204
#% 464182
#% 479812
#% 480075
#% 481951
#% 482049
#% 482082
#! Aggregation and cube are important operations for online analytical processing (OLAP). Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing cube for multidimensional data warehouses that store data sets in multidimensional arrays rather than in tables. However, to our knowledge, there is nothing to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP. This paper presents a set of aggregation algorithms on compressed data warehouses for multidimensional OLAP. These algorithms operate directly on compressed data sets, which are compressed by the mapping-complete compression methods, without the need to first decompress them. The algorithms have different performance behaviors as a function of the data set parameters, sizes of outputs and main memory availability. The algorithms are described and the I/O and CPU cost functions are presented in this paper. A decision procedure to select the most efficient algorithm for a given aggregation request is also proposed. The analysis and experimental results show that the algorithms have better performance on sparse data than the previous aggregation algorithms.

#index 443502
#* Mining Sequential Patterns with Regular Expression Constraints
#@ M. Garofalakis;R. Rastogi;K. Shim
#t 2002
#c 7
#% 172892
#% 248784
#% 248785
#% 383546
#% 443194
#% 459006
#% 461903
#% 463903
#% 481290
#% 481611
#% 482113
#! Discovering sequential patterns is an important problem in data mining with a host of application domains including medicine, telecommunications, and the World Wide Web. Conventional sequential pattern mining systems provide users with only a very restricted mechanism (based on minimum support) for specifying patterns of interest. As a consequence, the pattern mining process is typically characterized by lack of focus and users often end up paying inordinate computational costs just to be inundated with an overwhelming number of useless results. In this paper, we propose the use of Regular Expressions (REs) as a flexible constraint specification tool that enables user-controlled focus to be incorporated into the pattern mining process. We develop a family of novel algorithms (termed SPIRIT驴Sequential Pattern mIning with Regular expressIon consTraints) for mining frequent sequential patterns that also satisfy user-specified RE constraints. The main distinguishing factor among the proposed schemes is the degree to which the RE constraints are enforced to prune the search space of patterns during computation. Our solutions provide valuable insights into the trade-offs that arise when constraints that do not subscribe to nice properties (like antimonotonicity) are integrated into the mining process. A quantitative exploration of these trade-offs is conducted through an extensive experimental study on synthetic and real-life data sets. The experimental results clearly validate the effectiveness of our approach, showing that speedups of more than an order of magnitude are possible when RE constraints are pushed deep inside the mining process. Our experimentation with real-life data also illustrates the versatility of REs as a user-level tool for focusing on interesting patterns.

#index 443503
#* Pincer-Search: An Efficient Algorithm for Discovering the Maximum Frequent Set
#@ D. -I. Lin;Z. M. Kedem
#t 2002
#c 7
#% 152934
#% 201075
#% 201894
#% 227917
#% 227922
#% 232102
#% 237200
#% 248791
#% 300120
#% 329598
#% 412588
#% 420062
#% 443091
#% 459006
#% 459020
#% 461903
#% 464714
#% 464839
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 678171
#! Discovering frequent itemsets is a key problem in important data mining applications, such as the discovery of association rules, strong rules, episodes, and minimal keys. Typical algorithms for solving this problem operate in a bottom-up, breadth-first search direction. The computation starts from frequent 1-itemsets (the minimum length frequent itemsets) and continues until all maximal (length) frequent itemsets are found. During the execution, every frequent itemset is explicitly considered. Such algorithms perform well when all maximal frequent itemsets are short. However, performance drastically deteriorates when some of the maximal frequent itemsets are long. We present a new algorithm which combines both the bottom-up and the top-down searches. The primary search direction is still bottom-up, but a restricted search is also conducted in the top-down direction. This search is used only for maintaining and updating a new data structure, the maximum frequent candidate set. It is used to prune early candidates that would be normally encountered in the bottom-up search. A very important characteristic of the algorithm is that it does not require explicit examination of every frequent itemset. Therefore, the algorithm performs well even when some maximal frequent itemsets are long. As its output, the algorithm produces the maximum frequent set, i.e., the set containing all maximal frequent itemsets, thus specifying immediately all frequent itemsets. We evaluate the performance of the algorithm using well-known synthetic benchmark databases, real-life census, and stock market databases. The improvement in performance can be up to several orders of magnitude, compared to the best previous algorithms.

#index 443504
#* A Distance-Based Approach to Entity Reconciliation in Heterogeneous Databases
#@ D. Dey;S. Sarkar;P. De
#t 2002
#c 7
#% 22948
#% 25998
#% 51647
#% 55294
#% 85086
#% 92535
#% 111912
#% 111920
#% 126308
#% 158200
#% 158907
#% 178819
#% 184692
#% 189984
#% 201889
#% 213225
#% 227020
#% 237522
#% 248801
#% 272057
#% 375388
#% 442787
#% 462352
#% 481280
#% 481911
#% 535810
#% 535824
#! In modern organizations, decision makers must often be able to quickly access information from diverse sources in order to make timely decisions. A critical problem facing many such organizations is the inability to easily reconcile the information contained in heterogeneous data sources. To overcome this limitation, an organization must resolve several types of heterogeneity problems that may exist across different sources. In this paper, we examine one such problem called the entity heterogeneity problem, which arises when the same real-world entity type is represented using different identifiers in different applications. A decision-theoretic model to resolve the problem is proposed. Our model uses a distance measure to express the similarity between two entity instances. We have implemented the model and tested it on real-world data. The results indicate that the model performs quite well in terms of its ability to predict whether two entity instances should be matched or not. The model is shown to be computationally efficient. It also scales well to large relations from the perspective of the accuracy of prediction. Overall, the test results imply that this is certainly a viable approach in practical situations.

#index 443505
#* Product Schema Integration for Electronic Commerce-A Synonym Comparison Approach
#@ G. Yan;W. K. Ng;E.-P. Lim
#t 2002
#c 7
#% 22948
#% 55294
#% 55490
#% 85086
#% 111922
#% 157753
#% 198055
#% 218149
#% 222246
#% 227981
#% 247491
#% 261741
#% 275367
#% 339369
#% 408396
#% 445255
#% 462017
#% 462644
#% 481280
#% 629235
#! In any electronic commerce system, the heterogeneity of product descriptions is a critical impediment to efficient business information exchange. In the ABECOS electronic commerce system, buyer agents, seller agents, and directory agents liaise with one another in e-commerce activities. Only when agents have a common ontology of product descriptions (also called product schemas) are they able to interact seamlessly in e-commerce activities. This gives rise to the Product Schema Integration problem (PSI); the problem of integrating heterogeneous schemas of a certain product into one globally compatible schema. In this paper, we adopt an integration approach based on product attribute synonyms. We give a formal definition of the problem and show that it is NP-complete. We contrast our approach of study to conventional schema integration in federated databases. We also propose a set of approximate algorithms for PSI and evaluate their performance.

#index 443506
#* ACIRD: Intelligent Internet Document Organization and Retrieval
#@ S. -H. Lin;M. C. Chen;J. -M. Ho;Y. -M. Huang
#t 2002
#c 7
#% 39436
#% 55416
#% 67565
#% 70370
#% 73046
#% 84654
#% 86532
#% 115462
#% 118731
#% 136350
#% 152934
#% 165110
#% 169718
#% 210160
#% 219051
#% 232650
#% 237052
#% 262090
#% 406493
#% 443056
#% 448776
#% 449566
#% 481290
#% 677431
#% 677703
#% 729437
#% 836019
#! This paper presents an intelligent Internet information system, Automatic Classifier for the Internet Resource Discovery (ACIRD), which uses machine learning techniques to organize and retrieve Internet documents. ACIRD consists of a knowledge acquisition process, document classifier, and two-phase search engine. The knowledge acquisition process of ACIRD automatically learns classification knowledge from classified Internet documents. The document classifier applies learned classification knowledge to classify newly collected Internet documents into one or more classes. Experimental results indicate that ACIRD performs as well or better than human experts in both knowledge acquisition and document classification. By using the learned classification knowledge and the given class lattice, the ACIRD two-phase search engine responds to user queries with hierarchically structured navigable results (instead of a conventional flat ranked document list), which greatly aids users in locating information from numerous, diversified Internet documents.

#index 443507
#* Use of Analytical Performance Models for System Sizing and Resource Allocation in Interactive Video-on-Demand Systems Employing Data Sharing Techniques
#@ M. Y. Y. Leung;J. C. S. Lui;L. Golubchik
#t 2002
#c 7
#% 173593
#% 173594
#% 195335
#% 204401
#% 208749
#% 209901
#% 236083
#% 237738
#% 249267
#% 249268
#% 422965
#% 434823
#% 461913
#% 463912
#% 481282
#% 571058
#% 591370
#% 632260
#% 677163
#% 681769
#% 1848364
#! In designing cost-effective video-on-demand (VOD) servers, efficient resource management and proper system sizing are of great importance. In addition to large storage and I/O bandwidth requirements, support of interactive VCR functionality imposes additional resource requirements on the VOD system in terms of storage space, as well as disk and network bandwidth. Previous works have used data sharing techniques (such as batching, buffering, and adaptive piggybacking) to reduce the I/O demand on the storage server. However, such data sharing techniques complicate the provision of VCR functions and diminish the amount of benefit that can be obtained from data sharing techniques. The main contribution of this paper is a simple, yet powerful, analytical modeling approach which allows for analysis, system sizing, resource allocation, and parameter setting for a fairly general class of data sharing techniques which are used in conjunction with the providing of VCR-type functionality. Using this mathematical model, we can determine the proper amount of resources to be allocated for normal playback as well as for service of VCR functionality requests while satisfying predefined system performance requirements. To illustrate the usefulness of our model, we focus on a specific data sharing scheme which combines the use of batching, buffering, and adaptive piggybacking, as well as allows for the use of VCR functions. We show how to utilize this mathematical model for system sizing and resource allocation purposes驴that is, how to distribute the available resources between the service of normal playback and VCR functionality requests under various workloads and resource price ratios, so as to obtain the lowest system cost.

#index 443508
#* R++: Adding Path-Based Rules to C++
#@ D. Litman;P. F. Patel-Schneider;A. Mishra;J. Crawford;D. Dvorak
#t 2002
#c 7
#% 25469
#% 37972
#% 53531
#% 58345
#% 103369
#% 114323
#% 116049
#% 152921
#% 172191
#% 199801
#% 214787
#% 237236
#% 266674
#% 442732
#% 442816
#% 463423
#% 464043
#% 480621
#% 480763
#% 480779
#% 481259
#% 1290131
#% 1499538
#! Object-oriented languages and rule-based languages offer two distinct and useful programming abstractions. However, previous attempts to integrate data-driven rules into object-oriented languages have typically achieved an uneasy union at best. R++ is a new, closer integration of the rule-based and object-oriented paradigms that extends C++ with a single programming construct, the path-based rule, as a new kind of class member. Path-based rules驴data-driven rules that are restricted to following pointers between objects驴are like automatic methods that are triggered by changes to the objects they monitor. Path-based rules provide a useful level of abstraction that encourages a more declarative style of programming and are valuable in object-oriented designs as a means of modeling dynamic collections of interdependent objects. Unlike more traditional pattern-matching rules, path-based rules are not at odds with the object-oriented paradigm and offer performance advantages for many natural applications.

#index 443509
#* An Instance-Weighting Method to Induce Cost-Sensitive Trees
#@ K. M. Ting
#t 2002
#c 7
#% 92554
#% 136350
#% 160852
#% 169684
#% 191910
#% 458361
#% 465746
#% 466561
#% 520431
#% 1272369
#% 1499573
#! We introduce an instance-weighting method to induce cost-sensitive trees. It is a generalization of the standard tree induction process where only the initial instance weights determine the type of tree to be induced驴minimum error trees or minimum high cost error trees. We demonstrate that it can be easily adapted to an existing tree learning algorithm. Previous research provides insufficient evidence to support the idea that the greedy divide-and-conquer algorithm can effectively induce a truly cost-sensitive tree directly from the training data. We provide this empirical evidence in this paper. The algorithm incorporating the instance-weighting method is found to be better than the original algorithm in terms of total misclassification costs, the number of high cost errors, and tree size in two-class data sets. The instance-weighting method is simpler and more effective in implementation than a previous method based on altered priors.

#index 443510
#* A Modified Chi2 Algorithm for Discretization
#@ F. E. H. Tay;L. Shen
#t 2002
#c 7
#% 90661
#% 136350
#% 179769
#% 443148
#% 1272280
#! Since the ChiMerge algorithm was first proposed by Kerber in 1992, it has become a widely used and discussed discretization method. The Chi2 algorithm is a modification to the ChiMerge method. It automates the discretization process by introducing an inconsistency rate as the stopping criterion and it automatically selects the significance value. In addition, it adds a finer phase aimed at feature selection to broaden the applications of the ChiMerge algorithm. However, the Chi2 algorithm does not consider the inaccuracy inherent in ChiMerge's merging criterion. The user-defined inconsistency rate also brings about inaccuracy to the discretization process. These two drawbacks are first discussed in this paper and modifications to overcome them are then proposed. By comparison, results with original Chi2 algorithm using C4.5, the modified Chi2 algorithm, performs better than the original Chi2 algorithm. It becomes a completely automatic discretization method.

#index 443511
#* Unsupervised Learning with Mixed Numeric and Nominal Data
#@ Cen Li;Gautam Biswas
#t 2002
#c 7
#% 2783
#% 17631
#% 21281
#% 36672
#% 65440
#% 65441
#% 107038
#% 110996
#% 161241
#% 232117
#% 442873
#% 449569
#% 451052
#% 466890
#% 729437
#% 1272283
#% 1776206
#! This paper presents a Similarity-Based Agglomerative Clustering (SBAC) algorithm that works well for data with mixed numeric and nominal features. A similarity measure, proposed by Goodall for biological taxonomy, that gives greater weight to uncommon feature value matches in similarity computations and makes no assumptions of the underlying distributions of the feature values, is adopted to define the similarity measure between pairs of objects. An agglomerative algorithm is employed to construct a dendrogram and a simple distinctness heuristic is used to extract a partition of the data. The performance of SBAC has been studied on real and artificially generated data sets. Results demonstrate the effectiveness of this algorithm in unsupervised discovery tasks. Comparisons with other clustering schemes illustrate the superior performance of this approach.

#index 443512
#* Efficient Query Processing in Integrated Multiple Object Databases with Maybe Result Certification
#@ Jia-Ling Koh;Arbee L. P. Chen
#t 2002
#c 7
#% 12515
#% 22948
#% 24408
#% 24415
#% 77658
#% 111913
#% 116203
#% 158200
#% 158203
#% 169921
#% 213225
#% 233702
#% 286417
#% 287333
#% 442692
#% 442787
#% 442852
#% 442861
#% 442917
#% 444198
#% 462001
#% 462017
#% 462644
#% 463740
#% 480612
#% 480958
#% 535178
#% 535820
#% 614568
#! Within integrated multiple object databases, missing data occurs due to the missing attribute conflict as well as the existence of null values. A set of algorithms is provided in this paper to process the predicates of global queries with missing data. For providing more informative answers to users, the maybe results due to missing data are presented in addition to the certain results. The local maybe results may become certain results via the concept of object isomerism. One algorithm is designed based on the centralized approach in which data are forwarded to the same site for integration and processing. Furthermore, for reducing response time, the localized approaches evaluate the predicates within distinct component databases in parallel. The object signature is also applied in the design to further reduce the data transfer. These algorithms are compared and discussed according to the simulation results of both the total execution and response times. Alternately, the global schema may contain multivalued attributes with values derived from attribute values in different component databases. Hence, the proposed approaches are also extended to process the global queries involving this kind of multivalued attribute

#index 443513
#* Optimizing Main-Memory Join on Modern Hardware
#@ Stefan Manegold;Peter Boncz;Martin Kersten
#t 2002
#c 7
#% 13033
#% 18614
#% 68142
#% 116066
#% 172911
#% 201344
#% 251473
#% 251474
#% 251499
#% 273945
#% 280521
#% 286258
#% 287349
#% 328215
#% 427195
#% 437941
#% 439991
#% 440065
#% 442832
#% 442835
#% 460831
#% 464816
#% 473075
#% 479769
#% 479821
#% 480119
#% 565258
#% 566122
#% 571056
#% 610161
#! In the past decade, the exponential growth in commodity CPU's speed has far outpaced advances in memory latency. A second trend is that CPU performance advances are not only brought by increased clock rate, but also by increasing parallelism inside the CPU. Current database systems have not yet adapted to these trends and show poor utilization of both CPU and memory resources on current hardware. In this paper, we show how these resources can be optimized for large joins and translate these insights into guidelines for future database architectures, encompassing data structures, algorithms, cost modeling, and implementation. In particular, we discuss how vertically fragmented data structures optimize cache performance on sequential data access. On the algorithmic side, we refine the partitioned hash-join with a new partitioning algorithm called radix-cluster, which is specifically designed to optimize memory access. The performance of this algorithm is quantified using a detailed analytical model that incorporates memory access costs in terms of a limited number of parameters, such as cache sizes and miss penalties. We also present a calibration tool that extracts such parameters automatically from any computer hardware. The accuracy of our models is proven by exhaustive experiments conducted with the Monet database system on three different hardware platforms. Finally, we investigate the effect of implementation techniques that optimize CPU resource usage. Our experiments show that large joins can be accelerated almost an order of magnitude on modern RISC hardware when both memory and CPU resources are optimized.

#index 443514
#* Finding Patterns in Three-Dimensional Graphs: Algorithms and Applications to Scientific Data Mining
#@ Xiong Wang;Jason T. L. Wang;Dennis Shasha;Bruce A. Shapiro;Isidore Rigoutsos;Kaizhong Zhang
#t 2002
#c 7
#% 58167
#% 172892
#% 196817
#% 310549
#% 388776
#% 420062
#% 443143
#% 452829
#% 638153
#% 669123
#% 670031
#% 670349
#% 670351
#% 702221
#! This paper presents a method for finding patterns in 3D graphs. Each node in a graph is an undecomposable or atomic unit and has a label. Edges are links between the atomic units. Patterns are rigid substructures that may occur in a graph after allowing for an arbitrary number of whole-structure rotations and translations as well as a small number (specified by the user) of edit operations in the patterns or in the graph. (When a pattern appears in a graph only after the graph has been modified, we call that appearance 驴approximate occurrence.驴) The edit operations include relabeling a node, deleting a node and inserting a node. The proposed method is based on the geometric hashing technique, which hashes node-triplets of the graphs into a 3D table and compresses the label-triplets in the table. To demonstrate the utility of our algorithms, we discuss two applications of them in scientific data mining. First, we apply the method to locating frequently occurring motifs in two families of proteins pertaining to RNA-directed DNA Polymerase and Thymidylate Synthase and use the motifs to classify the proteins. Then, we apply the method to clustering chemical compounds pertaining to aromatic, bicyclicalkanes, and photosynthesis. Experimental results indicate the good performance of our algorithms and high recall and precision rates for both classification and clustering.

#index 443515
#* A Survey of Temporal Knowledge Discovery Paradigms and Methods
#@ John F. Roddick;Myra Spiliopoulou
#t 2002
#c 7
#% 1267
#% 16028
#% 21136
#% 108499
#% 116335
#% 135384
#% 139661
#% 149632
#% 152934
#% 172386
#% 172949
#% 172988
#% 191581
#% 227857
#% 227919
#% 227924
#% 232106
#% 232122
#% 234756
#% 259993
#% 260014
#% 260041
#% 280413
#% 280436
#% 280467
#% 280482
#% 280488
#% 280846
#% 285711
#% 319244
#% 388776
#% 442814
#% 443027
#% 443092
#% 443195
#% 445037
#% 452747
#% 452819
#% 459006
#% 459021
#% 460862
#% 462070
#% 462231
#% 462638
#% 463903
#% 464204
#% 464712
#% 477819
#% 477952
#% 477968
#% 479475
#% 479785
#% 480940
#% 481609
#% 481611
#% 504568
#% 511333
#% 534183
#% 536029
#% 536183
#% 552184
#% 562199
#% 630974
#! With the increase in the size of data sets, data mining has recently become an important research topic and is receiving substantial interest from both academia and industry. At the same time, interest in temporal databases has been increasing and a growing number of both prototype and implemented systems are using an enhanced temporal understanding to explain aspects of behavior associated with the implicit time-varying nature of the universe. This paper investigates the confluence of these two areas, surveys the work to date, and explores the issues involved and the outstanding problems in temporal data mining.

#index 443516
#* Query Relaxation by Structure and Semantics for Retrieval of Logical Web Documents
#@ Wen-Syan Li;K. Selçuk Candan;Quoc Vu;Divyakant Agrawal
#t 2002
#c 7
#% 240750
#% 240986
#% 249110
#% 249111
#% 262061
#% 268073
#% 268079
#% 268087
#% 281187
#% 282611
#% 282905
#% 479659
#% 479782
#% 479803
#% 539108
#% 539121
#% 676640
#! Since the Web encourages hypertext and hypermedia document authoring (e.g., HTML or XML), Web authors tend to create documents that are composed of multiple pages connected with hyperlinks. A Web document may be authored in multiple ways, such as, 1) all information in one physical page, or 2) a main page and the related information in separate linked pages. Existing Web search engines, however, return only physical pages containing keywords. In this paper, we introduce the concept of information unit, which can be viewed as a logical Web document consisting of multiple physical pages as one atomic retrieval unit. We present an algorithm to efficiently retrieve information units. Our algorithm can perform progressive query processing. These functionalities are essential for information retrieval on the Web and large XML databases. We also present experimental results on synthetic graphs and real Web data.

#index 443517
#* Clustering for Approximate Similarity Search in High-Dimensional Spaces
#@ Chen Li;Edward Chang;Hector Garcia-Molina;Gio Wiederhold
#t 2002
#c 7
#% 86950
#% 114667
#% 159841
#% 201876
#% 201935
#% 210173
#% 211820
#% 219847
#% 227939
#% 228351
#% 232472
#% 232764
#% 248792
#% 248796
#% 249321
#% 249322
#% 280452
#% 282552
#% 334033
#% 341355
#% 376266
#% 377755
#% 387508
#% 411694
#% 427199
#% 435141
#% 437405
#% 462222
#% 464195
#% 479462
#% 479649
#% 479799
#% 479973
#% 481281
#% 481956
#% 527026
#% 527174
#% 530277
#% 539791
#% 571079
#% 632011
#! In this paper, we present a clustering and indexing paradigm (called Clindex) for high-dimensional search spaces. The scheme is designed for approximate similarity searches, where one would like to find many of the data points near a target point, but where one can tolerate missing a few near points. For such searches, our scheme can find near points with high recall in very few IOs and perform significantly better than other approaches. Our scheme is based on finding clusters and, then, building a simple but efficient index for them. We analyze the trade-offs involved in clustering and building such an index structure, and present extensive experimental results.

#index 443518
#* Optimizing Queries with Foreign Functions in a Distributed Environment
#@ Pauray S. M. Tsai;Arbee L. P. Chen
#t 2002
#c 7
#% 13043
#% 38691
#% 38696
#% 58377
#% 102782
#% 102784
#% 111913
#% 116043
#% 129051
#% 152940
#% 172930
#% 172931
#% 201936
#% 210206
#% 285924
#% 286916
#% 408396
#% 442701
#% 442703
#% 442706
#% 444198
#% 452781
#% 464007
#% 464035
#% 479932
#% 481101
#% 481293
#% 481915
#% 564428
#% 565457
#! Foreign functions have been considered in the advanced database systems to support complex applications. In this paper, we consider optimizing queries with foreign functions in a distributed environment. In traditional distributed query processing, selection operations are locally processed before joins as much as possible so that the size of relations being transmitted and joined can be reduced. However, if selection predicates involve foreign functions, the cost of evaluating selections cannot be ignored. As a result, the execution order of selections and joins becomes significant, and the trade-off for reducing the costs of data transmission, join processing, and selection predicate evaluation needs to be carefully considered in query optimization. In this paper, a response time model is developed for estimating the cost of distributed query processing involving foreign functions. We explore the property of the problem and find an optimal algorithm with polynomial complexity for a special case of it. However, finding the optimal execution plan for the general case is NP-hard. We propose an efficient heuristic algorithm for solving the problem and the simulation result shows its good quality. The research result can also be applied to the advanced database systems and the multidatabase systems where the conversion function defined for the need of schema integration can be considered a type of foreign functions.

#index 443519
#* Exploring into Programs for the Recovery of Data Dependencies Designed
#@ Hee Beng Kuan Tan;Tok Wang Ling;Cheng Hian Goh
#t 2002
#c 7
#% 59349
#% 59350
#% 87049
#% 132222
#% 157760
#% 379923
#% 408145
#% 416036
#% 443201
#% 452821
#% 463257
#% 535187
#% 535197
#% 535212
#% 622224
#% 836134
#! Data dependencies play an important role in the design of a database. Many legacy database applications have been developed on old generation database management systems and conventional file systems. As a result, most of the data dependencies in legacy databases are not enforced in the database management systems. As such, they are not explicitly defined in database schema and are enforced in the transactions, which update the databases. It is very difficult and time consuming to find out the designed data dependencies manually during the maintenance and reengineering of database applications. In software engineering, program analysis has long been developed and proven as a useful aid in many areas. With the use of program analysis, this paper proposes a novel approach for the recovery of common data dependencies, i.e., functional dependencies, key constraints, inclusion dependencies, referential constraints, and sum dependencies, designed in a database from the behavior of transactions, which update the database. The approach is based on detecting program path patterns for implementing most commonly used methods to enforce these data dependencies.

#index 443520
#* Declarative Programs with Implicit Implications
#@ Vilas Wuwongse;Ekawit Nantajeewarawat
#t 2002
#c 7
#% 2298
#% 7679
#% 33376
#% 174161
#% 183909
#% 189739
#% 219205
#% 285540
#% 405391
#% 442931
#% 442977
#% 443099
#% 465181
#% 465326
#% 465477
#% 465492
#% 564771
#% 741212
#% 742585
#! In the presence of taxonomic information, there often exists implicit implication among atoms in an interpretation domain. A general framework is proposed for the discussion of an appropriate semantics for declarative programs with respect to such implicit implication. It is first assumed that the implicit implication can be predetermined and represented by a preorder on the interpretation domain. Under the consequent constraint that every interpretation must conform to the implicit implication, an appropriate model-theoretic semantics as well as its corresponding fixpoint semantics for declarative programs is described. Based on Köstler et al.'s foundation of fixpoint with subsumption, it is shown that, if the implicit-implication relation is, in addition, assumed to be a partial order, then the meaning of a program can be determined more efficiently by application of an immediate-consequence operator which involves only reduced representations, basically consisting only of their maximal elements, of subsets of the interpretation domain.

#index 443521
#* Spatio-Temporal Predicates
#@ Martin Erwig;Markus Schneider
#t 2002
#c 7
#% 399
#% 135384
#% 172369
#% 260062
#% 260066
#% 270714
#% 296077
#% 315005
#% 319244
#% 421073
#% 425792
#% 435148
#% 442974
#% 458583
#% 464007
#% 489249
#% 503882
#% 526694
#% 526841
#% 526995
#% 526998
#% 528064
#% 536031
#% 546590
#% 588714
#% 619824
#% 620024
#! This paper investigates temporal changes of topological relationships and thereby integrates two important research areas: First, two-dimensional topological relationships that have been investigated quite intensively and, second, the change of spatial information over time. We investigate spatio-temporal predicates, which describe developments of well-known spatial topological relationships. A framework is developed in which spatio-temporal predicates can be obtained by temporal aggregation of elementary spatial predicates and sequential composition. We compare our framework with two other possible approaches: one is based on the observation that spatio-temporal objects correspond to three-dimensional spatial objects for which existing topological predicates can be exploited. The other approach is to consider possible transitions between spatial configurations. These considerations help to identify a canonical set of spatio-temporal predicates.

#index 443522
#* Using OODB Modeling to Partition a Vocabulary in Structurally and Semantically Uniform Concept Groups
#@ Li-min Liu;Michael Halper;James Geller;Yehoshua Perl
#t 2002
#c 7
#% 82275
#% 115563
#% 116598
#% 125600
#% 171457
#% 214706
#% 235914
#% 375029
#% 395684
#% 430421
#! Controlled Vocabularies (CVs) are networks of concepts that unify disparate terminologies and facilitate the process of information sharing within an application domain. We describe a general methodology for representing an existing CV as an object-oriented database (OODB), called an Object-Oriented Vocabulary Repository (OOVR). A formal description of the OOVR methodology, which is based on a structural abstraction technique, is given, along with an algorithmic description and a number of theorems pertaining to some of the methodology's formal characteristics. An OOVR offers a two-level view of a CV, with the schema-level view serving as an important abstraction that can aid in orientation to the CV's contents. While an OOVR can also assist in traversals of the CV, we have identified certain special CV configurations where such traversals can be problematic. To address this, we introduce驴based on the original methodology驴an enhanced OOVR methodology that utilizes both structural and semantic features to partition and model a CV's constituent concepts. With its basis in the notions of area and the recursively defined articulation concept, an enhanced OOVR representation provides users with an improved CV view comprising groups of concepts uniform both in their structure and semantics. An algorithmic description of the singly rooted OOVR methodology and theorems describing some of its formal properties are given. The results of applying it to a large existing CV are discussed.

#index 443523
#* A Stochastic Programming Approach for Range Query Retrieval Problems
#@ Xian Liu;Wilsun Xu
#t 2002
#c 7
#% 1921
#% 32898
#% 36683
#% 45766
#% 101961
#% 119621
#% 135559
#% 237204
#% 273714
#% 285932
#% 286962
#% 287238
#% 287302
#% 287658
#% 289177
#% 289322
#% 291854
#% 317950
#% 318051
#% 321250
#% 321305
#% 348010
#% 405487
#% 411694
#% 415983
#% 415984
#% 442959
#% 443107
#% 480274
#! One of the important issues in range query (RQ) retrieval problems is to determine the key's resolution for multiattribute records. Conventional models need to be improved because of potential degeneracy, less desired computability, and possible inconsistency with the partial match query (PMQ) models. This paper presents a new RQ model to overcome these drawbacks and introduces a new methodology, stochastic programming (SP), to conduct the optimization process. The model is established by using a monotone increasing function to characterize range sizes. Three SP approaches, wait-and-see(WS), here-and-now (HN), and scenario tracking (ST) methods are integrated into this RQ model. Analytical expressions of the optimal solution are derived. It seems that HN has advantage over WS because the latter usually involves complicated multiple summations or integrals. For the ST method, a nonlinear programming software package is designed. Results of numerical experiments are presented that optimized a 10-dimensional RQ model and tracked a middle size (100) and a large size (1,000) scenarios.

#index 443524
#* Hashing Methods for Temporal Data
#@ George Kollios;Vassilis J. Tsotras
#t 2002
#c 7
#% 13019
#% 40633
#% 56081
#% 58371
#% 70370
#% 86950
#% 131243
#% 140389
#% 163440
#% 182672
#% 210186
#% 213080
#% 238413
#% 246003
#% 287070
#% 287317
#% 295947
#% 317933
#% 354943
#% 427199
#% 435156
#% 442967
#% 442972
#% 443130
#% 443181
#% 443257
#% 463751
#% 480595
#% 480797
#% 481934
#% 571296
#% 617838
#! External dynamic hashing has been used in traditional database systems as a fast method for answering membership queries. Given a dynamic set S of objects, a membership query asks whether an object with identity k is in (the most current state of) S. This paper addresses the more general problem of Temporal Hashing. In this setting, changes to the dynamic set are timestamped and the membership query has a temporal predicate, as in: "Find whether object with identity k was in set S at time t. " We present an efficient solution for this problem that takes an ephemeral hashing scheme and makes it partially persistent. Our solution, also termed partially persistent hashing, uses linear space on the total number of changes in the evolution of set S and has a small (O(\log_B(n/B))) query overhead. An experimental comparison of partially persistent hashing with various straightforward approaches (like external linear hashing, the Multiversion B-Tree, and the R*-tree) shows that it provides the faster membership query response time. Partially persistent hashing should be seen as an extension of traditional external dynamic hashing in a temporal environment. It is independent of the ephemeral dynamic hashing scheme used; while the paper concentrates on linear hashing, the methodology applies to other dynamic hashing schemes as well.

#index 443525
#* Connectionist Password Quality Tester
#@ Nigel Duffy;Arun Jagota
#t 2002
#c 7
#% 91872
#% 165190
#! We present a simple connectionist algorithm for testing the quality of computer passwords. Password quality is evaluated by testing the string against a large dictionary of words stored in a network in distributed form. Numerical simulations demonstrate the effectiveness of this approach. Computer security has always been an issue, more so in recent years due to the global network access. In this correspondence, we present a simple connectionist algorithm for testing the quality of computer passwords. A popular method of evaluating password quality is to test it against a large dictionary of words and near-words. Our algorithm is an approximate realization of this method. The large dictionary of words is stored in the network in distributed form. All stored words are stable; however, spurious memories may develop. Although there is no easy way to determine exactly which nonword strings become spurious, nor even exactly how many spurious memories form, the numerical simulations below reveal that the network works well in distinguishing words and near-words from structure-less strings. Thus, to evaluate a password, one would present it to the network and, if the network labeled it a memory, the password would be considered bad, otherwise good.

#index 443526
#* LH*G: A High-Availability Scalable Distributed Data Structure By Record Grouping
#@ Witold Litwin;Tore Risch
#t 2002
#c 7
#% 43172
#% 152946
#% 166234
#% 172916
#% 172918
#% 189578
#% 203481
#% 213080
#% 237615
#% 252608
#% 300165
#% 340298
#% 458997
#% 460871
#% 462773
#% 462779
#% 481296
#% 591531
#% 614605
#! LH*g is a high-availability extension of the LH* Scalable Distributed Data Structure. An LH*g file scales up with constant key search and insert performance, while surviving any single-site unavailability (failure). We achieve high-availability through a new principle of record grouping. A group is a logical structure of up to k records, where k is a file parameter. Every group contains a parity record allowing for the reconstruction of an unavailable member. The basic scheme may be generalized to support the unavailability of any number of sites, at the expense of storage and messaging. Other known high-availability schemes are static, or require more storage, or provide worse search performance.

#index 443527
#* The EVE Approach: View Synchronization in Dynamic Distributed Environments
#@ Amy J. Lee;Anisoara Nica;Elke A. Rundensteiner
#t 2002
#c 7
#% 36683
#% 59350
#% 122398
#% 137871
#% 154336
#% 159113
#% 172933
#% 188853
#% 198465
#% 198466
#% 199537
#% 201928
#% 213437
#% 213443
#% 213982
#% 213983
#% 248859
#% 273696
#% 274145
#% 301084
#% 443145
#% 458556
#% 463896
#% 463919
#% 464222
#% 481944
#% 482081
#% 511749
#% 565261
#% 581577
#% 614579
#% 631941
#! The construction and maintenance of data warehouses (views) in large-scale environments composed of numerous distributed and evolving information sources (ISs) such as the WWW has received great attention recently. Such environments are plagued with changing information because ISs tend to continuously evolve by modifying not only their content but also their query capabilities and interface and by joining or leaving the environment at any time. We are the first to introduce and address the problem of schema changes of ISs, while previous work in this area, such as incremental view maintenance, has mainly dealt with data changes at ISs. In this paper, we outline our solution approach to this challenging new problem of how to adapt views in such evolving environments. We identify a new view adaptation problem for view evolution in the context of ISs schema changes, which we call View Synchronization. We also outline the Evolvable View Environment (EVE) approach that we propose as framework for solving the view synchronization problem, along with our decisions concerning the key design issues surrounding EVE. The main contributions of this paper are: 1) we provide an E-SQL view definition language with which the view definer can direct the view evolution process, 2) we introduce a model for information source description which allows a large class of ISs to participate in our system dynamically, 3) we formally define what constitutes a legal view rewriting, 4) we develop replacement strategies for affected view components which are designed to meet the preferences expressed by E-SQL, 5) we prove the correctness of the replacement strategies, and 6) we provide a set of view synchronization algorithms based on those strategies. A prototype of our EVE system has successfully been built using Java, JDBC, Oracle, and MS Access.

#index 443528
#* A Graphical Query Language: VISUAL and Its Query Processing
#@ Nevzat Hurkan Balkir;Gultekin Ozsoyoglu;Z. Meral Ozsoyoglu
#t 2002
#c 7
#% 2247
#% 27056
#% 36683
#% 58354
#% 58356
#% 66208
#% 67459
#% 83606
#% 102770
#% 109496
#% 116048
#% 116091
#% 123590
#% 135874
#% 152940
#% 152949
#% 154400
#% 172930
#% 189739
#% 210206
#% 210215
#% 214693
#% 227968
#% 227969
#% 274158
#% 315005
#% 395735
#% 422966
#% 437096
#% 442995
#% 443284
#% 462474
#% 464061
#% 479478
#% 480799
#% 481102
#% 620008
#% 620060
#! This paper describes VISUAL, a graphical icon-based query language with a user-friendly graphical user interface for scientific databases and its query processing techniques. VISUAL is suitable for domains where visualization of the relationships is important for the domain scientist to express queries. In VISUAL, graphical objects are not tied to the underlying formalism; instead, they represent the relationships of the application domain. VISUAL supports relational, nested, and object-oriented models naturally and has formal basis. For ease of understanding and for efficiency reasons, two VISUAL semantics are introduced, namely, the interpretation and execution semantics. Translations from VISUAL to the Object Query Language (for portability considerations) and to an object algebra (for query processing purposes) are presented. Concepts of external and internal queries are developed as modularization tools.

#index 443529
#* ImageMap: An Image Indexing Method Based on Spatial Similarity
#@ Euripides G. M. Petrakis;Christos Faloutsos;King-Ip (David) Lin
#t 2002
#c 7
#% 23998
#% 181409
#% 183568
#% 201893
#% 213673
#% 219847
#% 227939
#% 228351
#% 275838
#% 286582
#% 311805
#% 318785
#% 437405
#% 443133
#% 443258
#% 443663
#% 443891
#% 479620
#% 481290
#% 481956
#! We introduce ImageMap, as a method for indexing and similarity searching in Image DataBases (IDBs). ImageMap answers 驴queries by example,驴 involving any number of objects or regions and taking into account their interrelationships. We adopt the most general image content representation, that is, Attributed Relational Graphs (ARGs), in conjunction with the well-accepted ARG editing distance on ARGs. We tested ImageMap on real and realistic medical images. Our method not only provides for visualization of the data set, clustering and data mining, but it also achieves up to 1,000-fold speed-up in search over sequential scanning, with zero or very few false dismissals.

#index 443530
#* SemQuery: Semantic Clustering and Querying on Heterogeneous Features for Visual Data
#@ Gholamhosein Sheikholeslami;Wendy Chang;Aidong Zhang
#t 2002
#c 7
#% 61477
#% 86950
#% 117076
#% 120270
#% 169940
#% 207504
#% 210173
#% 212690
#% 219845
#% 219847
#% 241129
#% 248010
#% 261855
#% 278831
#% 296284
#% 372042
#% 377548
#% 421069
#% 427199
#% 437405
#% 437409
#% 452795
#% 479799
#% 481281
#% 481947
#% 566128
#% 570889
#% 589929
#% 626558
#% 632363
#% 1180261
#! The effectiveness of the content-based image retrieval can be enhanced using heterogeneous features embedded in the images. However, since the features in texture, color, and shape are generated using different computation methods and thus may require different similarity measurements, the integration of the retrievals on heterogeneous features is a nontrivial task. In this paper, we present a semantics-based clustering and indexing approach, termed SemQuery, to support visual queries on heterogeneous features of images. Using this approach, the database images are classified based on their heterogeneous features. Each semantic image cluster contains a set of subclusters that are represented by the heterogeneous features that the images contain. An image is included in a semantic cluster if it falls within the scope of all the heterogeneous clusters of the semantic cluster. We also design a neural network model to merge the results of basic queries on individual features. A query processing strategy is then presented to support visual queries on heterogeneous features. An experimental analysis is conducted and presented to demonstrate the effectiveness and efficiency of the proposed approach.

#index 443531
#* CLARANS: A Method for Clustering Objects for Spatial Data Mining
#@ Raymond T. Ng;Jiawei Han
#t 2002
#c 7
#% 2115
#% 32877
#% 68091
#% 86949
#% 144300
#% 152934
#% 152935
#% 152937
#% 210173
#% 248790
#% 248792
#% 273890
#% 372188
#% 412588
#% 438137
#% 463595
#% 463742
#% 479799
#% 480610
#% 480940
#% 480964
#% 481281
#% 527022
#% 566128
#! Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. To this end, this paper has three main contributions. First, we propose a new clustering method called CLARANS, whose aim is to identify spatial structures that may be present in the data. Experimental results indicate that, when compared with existing clustering methods, CLARANS is very efficient and effective. Second, we investigate how CLARANS can handle not only points objects, but also polygon objects efficiently. One of the methods considered, called the IR-approximation, is very efficient in clustering convex and nonconvex polygon objects. Third, building on top of CLARANS, we develop two spatial data mining algorithms that aim to discover relationships between spatial and nonspatial attributes. Both algorithms can discover knowledge that is difficult to find with existing spatial data mining algorithms.

#index 443532
#* Fast Algorithms for Online Generation of Profile Association Rules
#@ Charu C. Aggarwal;Zheng Sun;Philip S. Yu
#t 2002
#c 7
#% 152934
#% 172386
#% 201894
#% 210160
#% 259994
#% 427199
#% 461909
#% 462238
#% 463883
#% 463903
#% 481281
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 527028
#! The results discussed in this paper are relevant to a large database consisting of consumer profile information together with behavioral (transaction) patterns. We introduce the concept of profile association rules, which discusses the problem of relating consumer buying behavior to profile information. The problem of online mining of profile association rules in this large database is discussed. We show how to use multidimensional indexing structures in order to actually perform the mining. The use of multidimensional indexing structures to perform profile mining provides considerable advantages in terms of the ability to perform very generic range-based online queries.

#index 443533
#* An Efficient Path Computation Model for Hierarchically Structured Topographical Road Maps
#@ Sungwon Jung;Sakti Pramanik
#t 2002
#c 7
#% 241
#% 16790
#% 25471
#% 47621
#% 51278
#% 58364
#% 77979
#% 88051
#% 91007
#% 109658
#% 139176
#% 172952
#% 214719
#% 214769
#% 271250
#% 275470
#% 287397
#% 442858
#% 443105
#% 443208
#% 462646
#% 463409
#% 463413
#% 463421
#% 463440
#% 463583
#% 479803
#% 480284
#% 527029
#% 549072
#% 565445
#! In this paper, we have developed a \big. HiTi\bigr. (Hierarchical MulTi) graph model for structuring large topographical road maps to speed up the minimum cost route computation. The \big. HiTi\bigr. graph model provides a novel approach to abstracting and structuring a topographical road map in a hierarchical fashion. We propose a new shortest path algorithm named \big. SPAH\bigr., which utilizes \big. HiTi\bigr. graph model of a topographical road map for its computation. We give the proof for the optimality of \big. SPAH\bigr.. Our performance analysis of \big. SPAH\bigr. on grid graphs showed that it significantly reduces the search space over existing methods. We also present an in-depth experimental analysis of HiTi graph method by comparing it with other similar works on grid graphs. Within the \big. HiTi\bigr. graph framework, we also propose a parallel shortest path algorithm named \big. ISPAH\bigr.. Experimental results show that inter query shortest path problem provides more opportunity for scalable parallelism than the intra query shortest path problem.

#index 443534
#* HAL: A Faster Match Algorithm
#@ Pou-Yung Lee;Albert Mo Kim Cheng
#t 2002
#c 7
#% 91220
#% 163721
#% 377175
#% 385833
#% 405349
#% 442929
#% 733622
#! Existing match algorithms treat the matching process like the querying process of relational databases. Owing to the combinatorial nature of the matching process, the match time greatly varies in different recognize-act cycles. Current match algorithms utilize local matching support networks with redundant working memory elements shared among rules involving the same classes. Since the match time is a dominant factor in the total execution time of a production system, such large match time makes production systems with existing match algorithms unsuitable for many applications. To reduce match time, we introduce the Heuristically-Annotated-Linkage (HAL) match algorithm. HAL differs from traditional match algorithms in that HAL employs a fixed-traversal-distance pseudobipartite network approach of treating rules and classes as objects, or nodes, in only one global pseudobipartite-graph-like connection and communication scheme. In addition, HAL is more efficient than other existing match algorithms because it is capable of immediate characterization of any new datum upon arrival. This paper reviews existing match algorithms, presents HAL, and analyzes the performance of HAL in comparison with existing algorithms.

#index 443535
#* Presentation Planning for Distributed VoD Systems
#@ Eenjun Hwang;B. Prabhakaran;V. S. Subrahmanian
#t 2002
#c 7
#% 25470
#% 151337
#% 151355
#% 164770
#% 172885
#% 176533
#% 204397
#% 205774
#% 210388
#% 216621
#% 219919
#% 245787
#% 257890
#% 288821
#% 340866
#% 437336
#% 452796
#% 463912
#% 481272
#% 520306
#% 520308
#% 661711
#% 1848269
#% 1852670
#! A distributed video-on-demand (VoD) system is one where a collection of video data is located at dispersed sites across a computer network. In a single site environment, a local video server retrieves video data from its local storage device. However, in distributed VoD systems, when a customer requests a movie from the local server, the server may need to interact with other servers located across the network. In this paper, we present different types of presentation plans that a local server can construct in order to satisfy a customer request. Informally speaking, a presentation plan is a temporally synchronized sequence of steps that the local server must perform in order to present the requested movie to the customer. This involves obtaining commitments from other video servers, obtaining commitments from the network service provider, as well as making commitments of local resources, while keeping within the limitations of available bandwidth, available buffer, and customer data consumption rates. Furthermore, in order to evaluate the quality of a presentation plan, we introduce two measures of optimality for presentation plans: minimizing wait time for a customer and minimizing access bandwidth which, informally speaking, specifies how much network/disk bandwidth is used. We develop algorithms to compute three different optimal presentation plans that work at a block level, or at a segment level, or with a hybrid mix of the two, and compare their performance through simulation experiments. We have also mathematically proven effects of increased buffer or bandwidth and data replications for presentation plans which had previously been verified experimentally in the literature.

#index 443536
#* Materialization and Its Metaclass Implementation
#@ Mohamed Dahchour;Alain Pirotte;Esteban Zimányi
#t 2002
#c 7
#% 103
#% 2078
#% 90639
#% 111361
#% 116185
#% 141551
#% 172325
#% 183913
#% 187741
#% 189746
#% 196354
#% 238531
#% 243204
#% 246170
#% 264773
#% 268154
#% 287370
#% 353982
#% 380128
#% 404085
#% 442909
#% 458608
#% 463265
#% 481451
#% 487810
#% 487951
#% 488121
#% 488763
#% 511164
#% 518170
#% 535520
#! Materialization is a powerful and ubiquitous abstraction pattern for conceptual modeling that relates a class of categories (e.g., models of cars) and a class of more concrete objects (e.g., individual cars). This paper presents materialization as a generic relationship between two classes of objects and describes an abstract implementation of it. The presentation is abstract in that it is not targeted at a specific object system. The target system is supposed to provide: 1) basic object-modeling facilities, supplemented with an explicit metaclass concept and 2) operations for dynamic schema evolution like creation or deletion of a subclass of a given class and modification of the type of an attribute of a class. The presentation is generic in that the semantics of materialization is implemented in a metaclass, which is a template to be instantiated in applications. Application classes are created as instances of the metaclass and they are thereby endowed with structure and behavior consistent with the generic semantics of materialization.

#index 443537
#* An Evaluation of Vertical Class Partitioning for Query Processing in Object-Oriented Databases
#@ Chi-wai Fung;Kamalakar Karlapalem;Qing Li
#t 2002
#c 7
#% 872
#% 86954
#% 273030
#% 317933
#% 320113
#% 442665
#% 462007
#% 481623
#% 511480
#% 614566
#% 631913
#% 726625
#! Vertical partitioning is a design technique for reducing the number of disk accesses to execute a given set of queries by minimizing the number of irrelevant instance variables accessed. This is accomplished by grouping the frequently accessed instance variables as vertical class fragments. The complexity of object-oriented database models due to subclass hierarchy and class composition hierarchy complicates the definition and representation of vertical partitioning of the classes, which makes the problem of vertical partitioning in OODBs very challenging. In this paper, we develop a comprehensive analytical cost model for processing of queries on vertically partitioned OODB classes. A set of analytical evaluation results is presented to show the effect of vertical partitioning, and to study the trade-off between the projection ratio versus selectivity factor vis-a-vis sequential versus index access. Furthermore, an empirical experimental prototype supporting vertical class partitioning has been implemented on a commercial OODB tool kit to validate our analytical cost model.

#index 443538
#* The Event Matching Language for Querying Temporal Data
#@ Tsz S. Cheng;Shashi K. Gadia
#t 2002
#c 7
#% 43028
#% 43191
#% 172369
#% 227951
#% 319244
#% 503878
#! Querying object evolution in temporal databases is interesting, but neither SQL-like algebraic languages nor general purpose languages take evolution into account. This paper presents a language, called the Event Matching Language, that is specifically designed for querying object evolution. The language is a pattern matching language based on the concept of cursor borrowed from SNOBOL4.

#index 443539
#* Flexible Robust Programming in Distributed Object Systems
#@ Mustaque Ahamad;Muthusamy Chelliah
#t 2002
#c 7
#% 1486
#% 22953
#% 31790
#% 32897
#% 43206
#% 45906
#% 59355
#% 65491
#% 86938
#% 87525
#% 116603
#% 122904
#% 132210
#% 151536
#% 172878
#% 287220
#% 290473
#% 318302
#% 403195
#% 437019
#% 462786
#% 480259
#% 480425
#% 480455
#% 481752
#% 517003
#! Distributed applications that access persistent objects must maintain object state consistency even when failures are encountered during the manipulation of such objects. The basic transaction model, which has been implemented by several systems to ensure consistent executions of distributed applications, is not flexible enough to meet the requirements of many complex distributed applications. This has also been recognized for advanced database applications and, as a result, extended transaction models have been developed. We argue that distributed applications that manipulate long-lived data can benefit from such transaction models. We take an approach which views the various transaction models as policies for building robust applications. Thus, we advocate that the system implement several transaction models. A robust application can be programmed in such a system using a combination of several transaction models to meet its consistency requirements. We use applications from the domain of computer-supported cooperative work to motivate such an approach. We also develop a set of system-level mechanisms which can be used to implement multiple transaction models in a uniform manner. These mechanisms are used to implement nested, split, and cooperating transaction models. A prototype system that has been implemented is described to demonstrate the feasibility of this approach.

#index 443540
#* Multiversion Locking Protocol with Freezing for Secure Real-Time Database Systems
#@ Chanjung Park;Seog Park;Sang H. Son
#t 2002
#c 7
#% 3652
#% 9241
#% 117903
#% 151157
#% 194928
#% 225089
#% 227954
#% 238729
#% 238731
#% 238734
#% 238737
#% 241781
#% 241801
#% 262259
#% 287230
#% 308084
#% 308087
#% 438098
#% 443380
#% 452837
#% 463279
#% 488318
#% 566387
#% 615445
#% 637782
#% 664471
#! Database systems for real-time applications must satisfy timing constraints associated with transactions. Typically, a timing constraint is expressed in the form of a deadline and is represented as a priority to be used by schedulers. Recently, security has become another important issue in many real-time applications. In many systems, sensitive information is shared by multiple users with different levels of security clearance. As more advanced database systems are being used in applications that need to support timeliness while managing sensitive information, there is an urgent need to develop protocols for concurrency control in transaction management that satisfy both timing and security requirements. In this paper, we propose a new multiversion concurrency control protocol that ensures that both security and real-time requirements are met. The proposed protocol is primarily based on locking. However, in order to satisfy timing constraints and security requirements, a new method, called the freezing method, is proposed. In order to show that our protocol provides a higher degree of concurrency than existing multiversion protocols, we define a new serializability for multiversion concurrency control, called FR-serializability, which is more general than traditional serializability. We present several examples to illustrate the behavior of our protocol, along with performance comparisons with other protocols. The simulation results show significant performance improvement of the new protocol.

#index 443541
#* Scheduling Transactions with Temporal Constraints: Exploiting Data Semantics
#@ Ming Xiong;Krithi Ramamritham;John A. Stankovic;Don Towsley;Rajendran Sivasankaran
#t 2002
#c 7
#% 37972
#% 111284
#% 117903
#% 124815
#% 124816
#% 158051
#% 201922
#% 268791
#% 330195
#% 403195
#% 438098
#% 442766
#% 442968
#% 442985
#% 480766
#% 571215
#% 615538
#% 615561
#% 615677
#% 635887
#! In this paper, issues involved in the design of a real-time database which maintains data temporal consistency are discussed. The concept of data-deadline is introduced and time cognizant transaction scheduling policies are proposed. Informally, data-deadline is a deadline assigned to a transaction due to the temporal constraints of the data accessed by the transaction. Further, two time cognizant forced wait policies which improve performance significantly by forcing a transaction to delay further execution until a new version of sensor data becomes available are proposed. A way to exploit temporal data similarity to improve performance is also proposed. Finally, these policies are evaluated through detailed simulation experiments. The simulation results show that taking advantage of temporal data semantics in transaction scheduling can significantly improve the performance of user transactions in real-time database systems. In particular, it is demonstrated that under the forced wait policy, the performance can be improved significantly. Further improvements result by exploiting data similarity.

#index 443542
#* Recovery from Malicious Transactions
#@ Paul Ammann;Sushil Jajodia;Peng Liu
#t 2002
#c 7
#% 1486
#% 9241
#% 18528
#% 32897
#% 67453
#% 91075
#% 116055
#% 116063
#% 122911
#% 122917
#% 158832
#% 176496
#% 183319
#% 227956
#% 238755
#% 261360
#% 268755
#% 287220
#% 287670
#% 403195
#% 442991
#% 443151
#% 480259
#% 480425
#% 481624
#% 592667
#% 664674
#! Preventive measures sometimes fail to deflect malicious attacks. In this paper, we adopt an information warfare perspective, which assumes success by the attacker in achieving partial, but not complete, damage. In particular, we work in the database context and consider recovery from malicious but committed transactions. Traditional recovery mechanisms do not address this problem, except for complete rollbacks, which undo the work of benign transactions as well as malicious ones, and compensating transactions, whose utility depends on application semantics. Recovery is complicated by the presence of benign transactions that depend, directly or indirectly, on the malicious transactions. We present algorithms to restore only the damaged part of the database. We identify the information that needs to be maintained for such algorithms. The initial algorithms repair damage to quiescent databases; subsequent algorithms increase availability by allowing new transactions to execute concurrently with the repair process. Also, via a study of benchmarks, we show practical examples of how offline analysis can efficiently provide the necessary data to repair the damage of malicious transactions.

#index 443543
#* An Object-Oriented Fuzzy Data Model for Similarity Detection in Image Databases
#@ Arun K. Majumdar;Indrajit Bhattacharya;Amit K. Saha
#t 2002
#c 7
#% 120270
#% 178072
#% 192684
#% 213673
#% 437405
#% 437407
#% 443213
#% 529966
#% 592180
#! In this paper, we introduce a fuzzy set theoretic approach for dealing with uncertainty in images in the context of spatial and topological relations existing among the objects in the image. We propose an object-oriented graph theoretic model for representing an image and this model allows us to assess the similarity between images using the concept of (fuzzy) graph matching. Sufficient flexibility has been provided in the similarity algorithm so that different features of an image may be independently focused upon.

#index 443544
#* The Presumed-Either Two-Phase Commit Protocol
#@ Gopi K. Attaluri;Kenneth Salem
#t 2002
#c 7
#% 4619
#% 197715
#% 318418
#% 403195
#% 462073
#% 481129
#! This paper describes the presumed-either two-phase commit protocol. Presumed-either exploits log piggybacking to reduce the cost of committing transactions. If timely piggybacking occurs, presumed-either combines the performance advantages of presumed-abort and presumed-commit. Otherwise, presumed-either behaves much like the widely-used presumed-abort protocol.

#index 443545
#* Probability Bounds for Goal Directed Queries in Bayesian Networks
#@ Michael V. Mannino;Vijay S. Mookerjee
#t 2002
#c 7
#% 1158
#% 44876
#% 67866
#% 119308
#% 183492
#% 380725
#% 443154
#% 446081
#% 449531
#% 573222
#% 1650712
#! We derive bounds on the probability of a goal node given a set of acquired input nodes. The bounds apply to decomposable networks; a class of Bayesian Networks encompassing causal trees and causal polytrees. The difficulty of computing the bounds depends on the characteristics of the decomposable network. For directly connected networks with binary goal nodes, tight bounds can be computed in polynomial time. For other kinds of decomposable networks, the derivation of the bounds requires solving an integer program with a nonlinear objective function, a computationally intractable problem in the worst case. We provide a relaxation technique that computes looser bounds in polynomial time for more complex decomposable networks. We briefly describe an application of the probability bounds to a record linkage problem.

#index 443546
#* Causal Maps: Theory, Implementation, and Practical Applications in Multiagent Environments
#@ Brahim Chaib-draa
#t 2002
#c 7
#% 89748
#% 115182
#% 119415
#% 135395
#% 162305
#% 166955
#% 167959
#% 188086
#% 188918
#% 215465
#% 215489
#% 216975
#% 266809
#% 274915
#% 431527
#% 442749
#% 486257
#% 1650690
#% 1780583
#! Analytical techniques are generally inadequate for dealing with causal interrelationships among a set of individual and social concepts. Usually, causal maps are used to cope with this type of interrelationships. However, the classical view of causal maps is based on an intuitive view with ad hoc rules and no precise semantics of the primitive concepts, nor a sound formal treatment of relations between concepts. In this paper, we solve this problem by proposing a formal model for causal maps with a precise semantics based on relation algebra and the software tool, CM-RELVIEW, in which it has been implemented. Then, we investigate the issue of using this tool in multiagent environments by explaining through different examples how and why this tool is useful for the following aspects: 1) the reasoning on agents' subjective views, 2) the qualitative distributed decision making, and 3) the organization of agents considered as a holistic approach. For each of these aspects, we focus on the computational mechanisms developed within CM-RELVIEW to support it.

#index 443547
#* Differential A*
#@ Karen I. Trovato;Leo Dorst
#t 2002
#c 7
#% 241
#% 25470
#% 319838
#% 1290111
#! A* graph search effectively computes the optimal solution path from start nodes to goal nodes in a graph, using a heuristic function. In some applications, the graph may change slightly in the course of its use and the solution path then needs to be updated. Very often, the new solution will differ only slightly from the old. Rather than perform the full A* on the new graph, we compute the necessary OPEN nodes from which the revised solution can be obtained by A*. In this "Differential A*" algorithm, the graph topology, transition costs, or start/goals may change simultaneously. We develop the algorithm and discuss when it gives an improvement over simply reapplying A*. We briefly discuss an application to robot path planning in configuration space, where such graph changes naturally arise.

#index 443548
#* Specifying and Enforcing Association Semantics via ORN in the Presence of Association Cycles
#@ Bryon K. Ehlmann;Gregory A. Riccardi;Naphtali D. Rishe;Jinyu Shi
#t 2002
#c 7
#% 14720
#% 54087
#% 68198
#% 235914
#% 287631
#% 442721
#% 462056
#% 463267
#% 480453
#% 480627
#% 909159
#! Object Relationship Notation (ORN) is a declarative scheme that allows a variety of common relationship types to be conveniently specified to a Database Management System (DBMS), thereby allowing their semantics to be automatically enforced by the DBMS. ORN can be integrated into any data model that represents binary associations or DBMS that implements them. In this paper, we give a brief description of ORN syntax and semantics and provide algorithms that can be used to implement ORN. These algorithms must deal with the presence of association cycles in the database. We explore in detail the problems caused by such cycles and how ORN and its implementation deal with them, and we show that ORN semantics are noncircular and unambiguous.

#index 443549
#* Local Reasoning and Knowledge Compilation for Efficient Temporal Abduction
#@ Luca Console;Paolo Terenziani;Daniele Theseider Dupré
#t 2002
#c 7
#% 21138
#% 36779
#% 59340
#% 78634
#% 82720
#% 85253
#% 107135
#% 107137
#% 110365
#% 116296
#% 181220
#% 184796
#% 257696
#% 319244
#% 443070
#% 445155
#% 497488
#% 748860
#% 936786
#! Generating abductive explanations is the basis of several problem solving activities such as diagnosis, planning, and interpretation. Temporal abduction means generating explanations that do not only account for the presence of observations, but also for temporal information on them, based on temporal knowledge in the domain theory. We focus on the case where such a theory contains temporal constraints that are required to be consistent with temporal information on observations. The aim of this paper is to propose efficient algorithms for computing temporal abductive explanations. Temporal constraints in the theory and in the observations can be used actively by an abductive reasoner in order to prune inconsistent candidate explanations at an early stage during their generation. However, checking temporal constraint satisfaction frequently generates some overhead. In the paper, we analyze two incremental ways of making this process efficient. First we show how, using a specific class of temporal constraints (which is expressive enough for many applications), such an overhead can be reduced significantly, yet preserving a full pruning power. In general, the approach does not affect the asymptotic complexity of the problem, but it provides significant advantages in practical cases. We also show that, for some special classes of theories, the asymptotic complexity is also reduced. We then show how, compiled knowledge based on temporal information, can be used to further improve the computation, thus, extending to the temporal framework previous results in the case of atemporal abduction. The paper provides both analytic and experimental evaluations of the computational advantages provided by our approaches.

#index 443550
#* Binary Rule Generation via Hamming Clustering
#@ Marco Muselli;Diego Liberati
#t 2002
#c 7
#% 53031
#% 53032
#% 56644
#% 60576
#% 92148
#% 92483
#% 111415
#% 112232
#% 126949
#% 136350
#% 143179
#% 160857
#% 204434
#% 229811
#% 296738
#% 328832
#% 376683
#% 407292
#% 440054
#% 443157
#% 443158
#% 443288
#% 1787943
#% 1788107
#% 1860644
#! The generation of a set of rules underlying a classification problem is performed by applying a new algorithm called Hamming Clustering (HC). It reconstructs the and-or expression associated with any Boolean function from a training set of samples. The basic kernel of the method is the generation of clusters of input patterns that belong to the same class and are close to each other according to the Hamming distance. Inputs which do not influence the final output are identified, thus automatically reducing the complexity of the final set of rules. The performance of HC has been evaluated through a variety of artificial and real-world benchmarks. In particular, its application in the diagnosis of breast cancer has led to the derivation of a reduced set of rules solving the associated classification problem.

#index 443551
#* Algorithms for Finding Attribute Value Group for Binary Segmentation of Categorical Databases
#@ Yasuhiko Morimoto;Takeshi Fukuda;Takeshi Tokuyama
#t 2002
#c 7
#% 2115
#% 8949
#% 66937
#% 136350
#% 182570
#% 198097
#% 209021
#% 210162
#% 213977
#% 235377
#% 278826
#% 419919
#% 459008
#% 479640
#% 479787
#% 481945
#% 481949
#% 482105
#% 494746
#% 1499573
#! We consider the problem of finding a set of attribute values that give a high quality binary segmentation of a database. The quality of a segmentation is defined by an objective function suitable for the user's objective, such as "mean squared error," "mutual information," or "\chi^2," each of which is defined in terms of the distribution of a given target attribute. Our goal is to find value groups on a given conditional domain that split databases into two segments, optimizing the value of an objective function. Though the problem is intractable for general objective functions, there are feasible algorithms for finding high quality binary segmentations when the objective function is convex, and we prove that the typical criteria mentioned above are all convex. We propose two practical algorithms, based on computational geometry techniques, which find a much better value group than conventional heuristics.

#index 443552
#* Efficient Queries over Web Views
#@ Giansalvatore Mecca;Alberto O. Mendelzon;Paolo Merialdo
#t 2002
#c 7
#% 18614
#% 42401
#% 77648
#% 77656
#% 83537
#% 99436
#% 116090
#% 139837
#% 140389
#% 168721
#% 237192
#% 248024
#% 248852
#% 261741
#% 274160
#% 308463
#% 384978
#% 411574
#% 411663
#% 411751
#% 462213
#% 464825
#% 479471
#! Large Web sites are becoming repositories of structured information that can benefit from being viewed and queried as relational databases. However, querying these views efficiently requires new techniques. Data usually resides at a remote site and is organized as a set of related HTML documents, with network access being a primary cost factor in query evaluation. This cost can be reduced by exploiting the redundancy often found in site design. We use a simple data model, a subset of the Araneus data model, to describe the structure of a Web site. We augment the model with link and inclusion constraints that capture the redundancies in the site. We map relational views of a site to a navigational algebra and show how to use the constraints to rewrite algebraic expressions, reducing the number of network accesses. We show that similar techniques can be used to maintain materialized views over sets of HTML pages.

#index 443553
#* Coordinated Placement and Replacement for Large-Scale Distributed Caches
#@ Madhukar R. Korupolu;Michael Dahlin
#t 2002
#c 7
#% 202148
#% 223400
#% 225006
#% 232771
#% 232777
#% 256883
#% 268093
#% 282558
#% 282760
#% 286466
#% 444336
#% 609890
#% 609912
#% 635787
#% 656698
#% 661477
#% 978378
#% 979357
#! In a large-scale information system such as a digital library or the web, a set of distributed caches can improve their effectiveness by coordinating their data placement decisions. Using simulation, we examine three practical cooperative placement algorithms, including one that is provably close to optimal, and we compare these algorithms to the optimal placement algorithm and several cooperative and noncooperative replacement algorithms. We draw five conclusions from these experiments: 1) Cooperative placement can significantly improve performance compared to local replacement algorithms, particularly when the size of individual caches is limited compared to the universe of objects; 2) although the amortizing placement algorithm is only guaranteed to be within 14 times the optimal, in practice it seems to provide an excellent approximation of the optimal; 3) in a cooperative caching scenario, the recent greedy-dual local replacement algorithm performs much better than the other local replacement algorithms; 4) our hierarchical-greedy-dual replacement algorithm yields further improvements over the greedy-dual algorithm especially when there are idle caches in the system; and 5) a key challenge to coordinated placement algorithms is generating good predictions of access patterns based on past accesses.

#index 443554
#* Parallel Star Join + DataIndexes: Efficient Query Processing in Data Warehouses and OLAP
#@ Anindya Datta;Debra VanderMeer;Krithi Ramamritham
#t 2002
#c 7
#% 58352
#% 115661
#% 152915
#% 152924
#% 172968
#% 191154
#% 207552
#% 210182
#% 223781
#% 227861
#% 340636
#% 385620
#% 394617
#% 442729
#% 442976
#% 443042
#% 462044
#% 462046
#% 463415
#% 480966
#! On-Line Analytical Processing (OLAP) refers to the technologies that allow users to efficiently retrieve data from the data warehouse for decision-support purposes. Data warehouses tend to be extremely large驴it is quite possible for a data warehouse to be hundreds of gigabytes to terabytes in size. Queries tend to be complex and ad hoc, often requiring computationally expensive operations such as joins and aggregation. Given this, we are interested in developing strategies for improving query processing in data warehouses by exploring the applicability of parallel processing techniques. In particular, we exploit the natural partitionability of a star schema and render it even more efficient by applying DataIndexes驴a storage structure that serves both as an index as well as data and lends itself naturally to vertical partitioning of the data. Dataindexes are derived from the various special purpose access mechanisms currently supported in commercial OLAP products. Specifically, we propose a declustering strategy which incorporates both task and data partitioning and present the Parallel Star Join (PSJ) Algorithm, which provides a means to perform a star join in parallel using efficient operations involving only rowsets and projection columns. We compare the performance of the PSJ Algorithm with two parallel query processing strategies. The first is a parallel join strategy utilizing the Bitmap Join Index (BJI), arguably the state-of-the-art OLAP join structure in use today. For the second strategy we choose a well-known parallel join algorithm, namely the pipelined hash algorithm. To assist in the performance comparison, we first develop a cost model of the disk access and transmission costs for all three approaches. Performance comparisons show that the DataIndex-based approach leads to dramatically lower disk access costs than the BJI, as well as the hybrid hash approaches, in both speedup and scaleup experiments, while the hash-based approach outperforms the BJI in disk access costs. With regard to transmission overhead, our performance results show that PSJ and BJI outperform the hash-based approach. Overall, our parallel star join algorithm and dataindexes form a winning combination.

#index 443555
#* Transaction Processing in Mobile, Heterogeneous Database Systems
#@ James B. Lim;A. R. Hurson
#t 2002
#c 7
#% 9241
#% 67458
#% 70370
#% 77982
#% 92669
#% 116304
#% 158907
#% 172880
#% 191168
#% 198045
#% 202140
#% 223278
#% 233499
#% 235017
#% 235911
#% 245017
#% 249968
#% 286244
#% 343871
#% 403195
#% 435104
#% 442853
#% 463269
#% 480259
#% 562178
#% 571089
#% 660942
#% 979356
#% 979694
#% 1797800
#! As technological advances are made in software and hardware, the feasibility of accessing information "any time, anywhere" is becoming a reality. Furthermore, the diversity and amount of information available to a given user is increasing at a rapid rate. In a mobile computing environment, a potentially large number of users may simultaneously access the global data; therefore, there is a need to provide a means to allow concurrent management of transactions. Current multidatabase concurrency control schemes do not address the limited bandwidth and frequent disconnection associated with wireless networks. This paper proposes a new hierarchical concurrency control algorithm. The proposed concurrency control algorithm驴v-lock驴uses global locking tables created with semantic information contained within the hierarchy. The locking tables are used to serialize global transactions, detect and remove global deadlocks. Additionally, data replication, at the mobile unit, is used to limit the effects of the restrictions imposed by a mobile environment. The replicated data provides additional availability in case of a weak connection or disconnection. Current research has concentrated on page and file-based caching or replication schemes to address the availability and consistency issues in a mobile environment. In a mobile, multidatabase environment, local autonomy restrictions prevent the use of a page or file-based data replication scheme. This paper proposes a new data replication scheme to address the limited bandwidth and local autonomy restrictions. Queries and the associated data are cached at the mobile unit as a complete object. Consistency is maintained by using a parity-based invalidation scheme. A simple prefetching scheme is used in conjunction with caching to further improve the effectiveness of the proposed scheme. Finally, a simulator was developed to evaluate the performance of the proposed algorithms. The simulation results are presented and discussed.

#index 443556
#* A Methodology to Retrieve Text Documents from Multiple Databases
#@ Clement Yu;King-Lup Liu;Weiyi Meng;Zonghuan Wu;Naphtali Rishe
#t 2002
#c 7
#% 54453
#% 67565
#% 184496
#% 194246
#% 194275
#% 218982
#% 230432
#% 232701
#% 247425
#% 253188
#% 262063
#% 262065
#% 262092
#% 268079
#% 274483
#% 280853
#% 280854
#% 282422
#% 282424
#% 282905
#% 287237
#% 298221
#% 359132
#% 406493
#% 443561
#% 479451
#% 479642
#% 481748
#% 567255
#% 584914
#% 631997
#% 671674
#! This paper presents a methodology for finding the n most similar documents across multiple text databases for any given query and for any positive integer n. This methodology consists of two steps. First, the contents of databases are indicated approximately by database representatives. Databases are ranked using their representatives with respect to the given query. We provide a necessary and sufficient condition to rank the databases optimally. In order to satisfy this condition, we provide three estimation methods. One estimation method is intended for short queries; the other two are for all queries. Second, we provide an algorithm, OptDocRetrv, to retrieve documents from the databases according to their rank and in a particular way. We show that if the databases containing the n most similar documents for a given query are ranked ahead of other databases, our methodology will guarantee the retrieval of the n most similar documents for the query. When the number of databases is large, we propose to organize database representatives into a hierarchy and employ a best-search algorithm to search the hierarchy. It is shown that the effectiveness of the best-search algorithm is the same as that of evaluating the user query against all database representatives.

#index 443557
#* Applications of Abduction: Testing Very Long Qualitative Simulations
#@ Tim Menzies;Robert F. Cohen;Sam Waugh;Simon Goss
#t 2002
#c 7
#% 3460
#% 6200
#% 67569
#% 107135
#% 109852
#% 115630
#% 132135
#% 134101
#% 151269
#% 161105
#% 163342
#% 197032
#% 204979
#% 215014
#% 217917
#% 275418
#% 320219
#% 339050
#% 486322
#% 1290269
#% 1478748
#! We can test a theory of "X" by checking if that theory can reproduce known behavior of "X." In the general case, this check for time-based simulations is only practical for short simulation runs. We show that, given certain reasonable language restrictions, the complexity of this check reduces to the granularity of the measurements. That is, provided a very long simulation run is only measured infrequently, this check is feasible.

#index 443558
#* An Approach of Implementing General Learning Companions for Problem Solving
#@ Chih-Yueh Chou;Tak-Wai Chan;Chi-Jen Lin
#t 2002
#c 7
#% 172901
#% 195253
#% 203584
#% 552466
#% 552612
#! Adding a learning companion, a computer simulated social agent, to a computer based learning system can enhance its educational value by enriching the way in which the computer and the user interact. This paper presents a novel simulation approach, named General Companion Modeling (referred to hereinafter as GCM), to implement learning companions in a general problem-solving domain. DwestAgent, a learning companion system, is also implemented using the GCM approach to demonstrate the feasibility of the proposed approach. In addition, the approach can help developers of learning companions clarify implementation issues and requirements involved in simulating 1) domain competencies, 2) learning competencies, 3) behaviors as a peer tutor, and 4)behaviors as a peer tutee of a learning companion. Using GCM, one can simulate learning companions with various characteristics by adjusting parameters within the proposed simulation framework.

#index 443559
#* Exploiting Data Mining Techniques for Broadcasting Data in Mobile Computing Environments
#@ Yücel Saygin;Özgür Ulusoy
#t 2002
#c 7
#% 114664
#% 152934
#% 175253
#% 201897
#% 227885
#% 249933
#% 280059
#% 287048
#% 287068
#% 443034
#% 443085
#% 443127
#% 443195
#% 463903
#% 464065
#% 464204
#% 480965
#% 481290
#% 482107
#% 584891
#! Mobile computers can be equipped with wireless communication devices that enable users to access data services from any location. In wireless communication, the server-to-client (downlink) communication bandwidth is much higher than the client-to-server (uplink) communication bandwidth. This asymmetry makes the dissemination of data to client machines a desirable approach. However, dissemination of data by broadcasting may induce high access latency in case the number of broadcast data items is large. In this paper, we propose two methods aiming to reduce client access latency of broadcast data. Our methods are based on analyzing the broadcast history (i.e., the chronological sequence of items that have been requested by clients) using data mining techniques. With the first method, the data items in the broadcast disk are organized in such a way that the items requested subsequently are placed close to each other. The second method focuses on improving the cache hit ratio to be able to decrease the access latency. It enables clients to prefetch the data from the broadcast disk based on the rules extracted from previous data request patterns. The proposed methods are implemented on a Web log to estimate their effectiveness. It is shown through performance experiments that the proposed rule-based methods are effective in improving the system performance in terms of the average latency as well as the cache hit ratio of mobile clients.

#index 443560
#* Efficient Join-Index-Based Spatial-Join Processing: A Clustering Approach
#@ Shashi Shekhar;Chang-Tien Lu;Sanjay Chawla;Sivakumar Ravada
#t 2002
#c 7
#% 1388
#% 18614
#% 51375
#% 70370
#% 114577
#% 136740
#% 152937
#% 172908
#% 172909
#% 236806
#% 239588
#% 258598
#% 268018
#% 319473
#% 323285
#% 366617
#% 404719
#% 442675
#% 443105
#% 443180
#% 443258
#% 462159
#% 462957
#% 463436
#% 463595
#% 469887
#% 482046
#% 526997
#% 565461
#% 836006
#! A join-index is a data structure used for processing join queries in databases. Join-indices use precomputation techniques to speed up online query processing and are useful for data sets which are updated infrequently. The I/O cost of join computation using a join-index with limited buffer space depends primarily on the page-access sequence used to fetch the pages of the base relations. Given a join-index, we introduce a suite of methods based on clustering to compute the joins. We derive upper bounds on the length of the page-access sequences. Experimental results with Sequoia 2000 data sets show that the clustering method outperforms existing methods based on sorting and online-clustering heuristics.

#index 443561
#* A Statistical Method for Estimating the Usefulness of Text Databases
#@ King-Lup Liu;Clement Yu;Weiyi Meng;Wensheng Wu;Naphtali Rishe
#t 2002
#c 7
#% 176501
#% 184486
#% 194246
#% 218982
#% 227891
#% 232701
#% 245788
#% 253188
#% 273926
#% 287015
#% 287459
#% 359132
#% 406493
#% 479642
#% 481748
#% 567255
#% 584914
#% 631997
#% 671674
#% 683257
#% 978381
#% 978507
#! Searching desired data on the Internet is one of the most common ways the Internet is used. No single search engine is capable of searching all data on the Internet. The approach that provides an interface for invoking multiple search engines for each user query has the potential to satisfy more users. When the number of search engines under the interface is large, invoking all search engines for each query is often not cost effective because it creates unnecessary network traffic by sending the query to a large number of useless search engines and searching these useless search engines wastes local resources. The problem can be overcome if the usefulness of every search engine with respect to each query can be predicted. In this paper, we present a statistical method to estimate the usefulness of a search engine for any given query. For a given query, the usefulness of a search engine in this paper is defined to be a combination of the number of documents in the search engine that are sufficiently similar to the query and the average similarity of these documents. Experimental results indicate that our estimation method is much more accurate than existing methods.

#index 443562
#* 2002 Index IEEE Transactions on Knowledge and Data Engineering, Vol. 14
#@  IEEE Transactions on Knowledge and Data Engineering Staff
#t 2002
#c 7
#! First Page of the Article

#index 452839
#* Formalizing an Engineering Approach to Cooperating Knowledge-Based Systems
#@ S. M. Deen;C. A. Johnson
#t 2003
#c 7
#% 136356
#% 189698
#% 212659
#% 256556
#% 256562
#% 282431
#% 359238
#% 362927
#% 437280
#% 443063
#% 498278
#% 543069
#% 589219
#! A theoretical grounding is provided for a Cooperating Knowledge-Based Systems (CKBS) model which is based upon agents, cooperation blocks, and cooperation block hierarchies. Our model describes the requirements for task decomposition, negotiation, cooperation and coordination, fault tolerance, and recoverability, these requirements in turn defining a holonic system. The behavioral properties of our model are described using state transition diagrams and properties of correctness and termination are proven.

#index 452840
#* Association and Content-Based Retrieval
#@ Chabane Djeraba
#t 2003
#c 7
#% 57485
#% 176501
#% 228351
#% 238916
#% 240195
#% 261880
#% 375017
#% 443244
#% 462216
#% 584886
#% 836019
#! In spite of important efforts in content-based indexing and retrieval during these last years, seeking relevant and accurate images remains a very difficult query. In the state-of-the-art approaches, the retrieval task may be efficient for some queries in which the semantic content of the query can be easily translated into visual features. For example, finding images of fires is simple because fires are characterized by specific colors (yellow and red). However, it is not efficient in other application fields in which the semantic content of the query is not easily translated into visual features. For example, finding images of birds during migrations is not easy because the system has to understand the query semantic. In the query, the basic visual features may be useful (a bird is characterized by a texture and a color), but they are not sufficient. What is missing is the generalization capability. Birds during migrations belong to the same repository of birds, so they share common associations among basic features (e.g., textures and colors) that the user cannot specify explicitly. In this paper, we present an approach that discovers hidden associations among features during image indexing. These associations discriminate image repositories. The best associations are selected on the basis of measures of confidence. To reduce the combinatory explosion of associations, because images of the database contain very large numbers of colors and textures, we consider a visual dictionary that group together similar colors and textures. Thus, the visual dictionary summarizes the image features. An algorithm based on a clustering strategy creates the visual dictionary. The associations discovered permit the automatic classification of images during their insertion into image repositories and return accurate and relevant results. More generally, we show that content and knowledge-based indexing and retrieval is more efficient than retrieval approaches based on content exclusively and inaugurate a new generation of approaches in which knowledge contributes to finding images in large image repositories.

#index 452841
#* Query Merging: Improving Query Subscription Processing in a Multicast Environment
#@ Arturo Crespo;Orkut Buyukkokten;Hector Garcia-Molina
#t 2003
#c 7
#% 54047
#% 66172
#% 124011
#% 175253
#% 201897
#% 237238
#% 248839
#% 248840
#% 255209
#% 464228
#% 571216
#% 595184
#% 610617
#% 635895
#! This paper introduces techniques for reducing data dissemination costs of query subscriptions in a multicast environment. The reduction is achieved by merging queries with overlapping, but not necessarily equal, answers. The paper formalizes the query-merging problem and introduces a general framework and cost model for evaluating merging. We prove that the problem is NP-hard and propose exhaustive algorithms and three heuristic algorithms: The Pair Merging Algorithm, the Directed Search Algorithm, and the Clustering Algorithm. We develop a simulator, which uses geographical queries as a representative example for evaluating the different heuristics and show that the performance of our heuristics is close to optimal.

#index 452842
#* Effective Scheduling of Detached Rules in Active Databases
#@ Stefant Ceri;Claudio Gennaro;Stefano Paraboschi;Giuseppe Serazzi
#t 2003
#c 7
#% 1252
#% 156993
#% 227879
#% 228001
#% 261737
#% 294671
#% 385995
#% 387063
#% 403195
#% 442766
#% 458544
#% 462080
#% 562058
#% 571215
#% 681387
#% 681396
#! While triggers have become a classical ingredient of relational database systems, research in active databases is aiming at extending the functionality and expressive power of active rules beyond the scope of relational triggers. One of the most important current trend concerns the support of detached active rules, i.e., of rules which are executed as separate transactions, running outside of the scope of the transaction which generates the triggering event. Detached rules have important applications in workflow management and global integrity maintenance across transactions. One of the main issues in designing the rule engine for detached rules is determining their optimal scheduling. In this paper, we study the performance of a detached rule scheduler whose objective is to minimize the interference of detached rule execution with regard to the normal transactional load. This objective is achieved by executing detached rules at given periods of time and by assigning them a fixed amount of dedicated threads; we study the performance of the scheduler relative to the two most critical design parameters, the frequency of execution of the scheduler, and the number of dedicated execution threads.

#index 452843
#* Rough-Fuzzy MLP: Modular Evolution, Rule Generation, and Evaluation
#@ Sankar K. Pal;Sushmita Mitra;Pabitra Mitra
#t 2003
#c 7
#% 4142
#% 136350
#% 157721
#% 160857
#% 166652
#% 169085
#% 366687
#% 369236
#% 443288
#% 505692
#% 565961
#% 1860390
#% 1860401
#% 1860667
#% 1860903
#! A methodology is described for evolving a Rough-fuzzy multi layer perceptron with modular concept using a genetic algorithm to obtain a structured network suitable for both classification and rule extraction. The modular concept, based on "divide and conquer" strategy, provides accelerated training and a compact network suitable for generating a minimum number of rules with high certainty values. The concept of variable mutation operator is introduced for preserving the localized structure of the constituting knowledge-based subnetworks, while they are integrated and evolved. Rough set dependency rules are generated directly from the real valued attribute table containing fuzzy membership values. Two new indices viz., "certainty" and "confusion" in a decision are defined for evaluating quantitatively the quality of rules. The effectiveness of the model and the rule extraction algorithm is extensively demonstrated through experiments alongwith comparisons.

#index 452844
#* Supporting Movement Pattern Queries in User-Specified Scales
#@ Yunyao Qu;Changzhou Wang;Like Gao;X. Sean Wang
#t 2003
#c 7
#% 172949
#% 227857
#% 260014
#% 382522
#% 390187
#% 460862
#% 461885
#% 481609
#% 481611
#% 534183
#! An important investigation of moving objects involves searching for objects with specific movement patterns, such as "going up," "going towards southwest," or a combination of these. Movement patterns can be in various scales, and larger-scale patterns usually span over longer time periods with greater disturbances ignored. Movement pattern queries ask for moving objects which show a given movement pattern in a specific scale. This paper studies database techniques to support fast evaluation of movement pattern queries in user-specified scales. The database is assumed to contain position information of moving objects sampled at a certain time interval. A movement pattern is defined as a regular expression of movement letters where each letter describes a set of movement directions. For each series of positions, movement directions of all scales are precomputed and results are mapped into points on a plane. Points on this plane usually cluster well and can be readily bounded by trapezoids. These bounding trapezoids are then stored in a relational database and the query language SQL can be used to help evaluate movement pattern queries. This paper also reports some experiments conducted on a real data set as well as a synthesized data set. Results show that both the precomputation algorithm and the bounding strategy are efficient and scalable.

#index 452845
#* Efficient Mining of Intertransaction Association Rules
#@ Anthony K. H. Tung;Hongjun Lu;Jiawei Han;Ling Feng
#t 2003
#c 7
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 213963
#% 227919
#% 227953
#% 248785
#% 273899
#% 420063
#% 461909
#% 463903
#% 479484
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 481954
#! Most of the previous studies on mining association rules are on mining intratransaction associations, i.e., the associations among items within the same transaction where the notion of the transaction could be the items bought by the same customer, the events happened on the same day, etc. In this study, we break the barrier of transactions and extend the scope of mining association rules from traditional single-dimensional, intratransaction associations to multidimensional, intertransaction associations. An intertransaction association describes the association relationships among different transactions. In a database of stock price information, an example of such an association is "if (company) A's stock goes up on day one, B's stock will go down on day two but go up on day four." In this case, no matter whether we treat company or day as the unit of transaction, the associated items belong to different transactions. Moreover, such an intertransaction association can be extended to associate multiple properties in the same rule, so that multidimensional intertransaction associations can also be defined and discovered. Mining intertransaction associations pose more challenges on efficient processing than mining intratransaction associations because the number of potential association rules becomes extremely large after the boundary of transactions is broken. In this study, we introduce the notion of intertransaction association rule, define its measurements: support and confidence, and develop an efficient algorithm, FITI (an acronym for "First Intra Then Inter"), for mining intertransaction associations, which adopts two major ideas: 1) an intertransaction frequent itemset contains only the frequent itemsets of its corresponding intratransaction counterpart; and 2) a special data structure is built among intratransaction frequent itemsets for efficient mining of intertransaction frequent itemsets. We compare FITI with EH-Apriori, the best algorithm in our previous proposal, and demonstrate a substantial performance gain of FITI over EH-Apriori. Further extensions of the method and its implications are also discussed in the paper.

#index 452846
#* Alternative Interest Measures for Mining Associations in Databases
#@ Edward R. Omiecinski
#t 2003
#c 7
#% 152934
#% 201894
#% 227917
#% 227919
#% 227922
#% 248785
#% 273898
#% 273899
#% 280436
#% 280487
#% 300120
#% 310494
#% 310507
#% 310558
#% 406493
#% 443091
#% 462238
#% 463883
#% 464822
#% 479627
#% 479643
#% 481290
#% 481588
#% 481754
#% 481758
#! Data mining is defined as the process of discovering significant and potentially useful patterns in large volumes of data. Discovering associations between items in a large database is one such data mining activity. In finding associations, support is used as an indicator as to whether an association is interesting. In this paper, we discuss three alternative interest measures for associations: any-confidence, all-confidence, and bond. We prove that the important downward closure property applies to both all-confidence and bond. We show that downward closure does not hold for any-confidence. We also prove that, if associations have a minimum all-confidence or minimum bond, then those associations will have a given lower bound on their minimum support and the rules produced from those associations will have a given lower bound on their minimum confidence as well. However, associations that have that minimum support (and likewise their rules that have minimum confidence) may not satisfy the minimum all-confidence or minimum bond constraint. We describe the algorithms that efficiently findall associations with a minimum all-confidence or minimum bond and present some experimental results.

#index 452847
#* Developing Data Allocation Schemes by Incremental Mining of User Moving Patterns in a Mobile Computing System
#@ Wen-Chih Peng;Ming-Syan Chen
#t 2003
#c 7
#% 70370
#% 152934
#% 175253
#% 198045
#% 220797
#% 225006
#% 237620
#% 245015
#% 245017
#% 245080
#% 413162
#% 443082
#% 443085
#% 443091
#% 443164
#% 443194
#% 462219
#% 463903
#% 480965
#% 481281
#% 481945
#% 511758
#% 589300
#% 631926
#% 1830002
#% 1830005
#% 1830352
#! In this paper, we present a new data mining algorithm which involves incremental mining for user moving patterns in a mobile computing environment and exploit the mining results to develop data allocation schemes so as to improve the overall performance of a mobile system. First, we propose an algorithm to capture the frequent user moving patterns from a set of log data in a mobile environment. The algorithm proposed is enhanced with the incremental mining capability and is able to discover new moving patterns efficiently without compromising the quality of results obtained. Then, in light of mining results of user moving patterns and the properties of data objects, we develop data allocation schemes that can utilize the knowledge of user moving patterns for proper allocation of both personal and shared data. By employing the data allocation schemes, the occurrences of costly remote accesses can be minimized and the performance of a mobile computing system is thus improved. For personal data allocation, two data allocation schemes, which explore different levels of mining results, are devised: one utilizes the set level of moving patterns and the other utilizes the path level of moving patterns. As can be seen later, the former is useful for the allocation of read-intensive data objects, whereas the latter is good for the allocation of update-intensive data objects. The data allocation schemes for shared data, which are able to achieve local optimization and global optimization, are also developed. Performance of these data allocation schemes is comparatively analyzed. It is shown by our simulation results that the knowledge obtained from the user moving patterns is very important in devising effective data allocation schemes which can lead to significant performance improvement in a mobile computing system.

#index 452848
#* Using Fuzzy Linguistic Representations to Provide Explanatory Semantics for Data Warehouses
#@ Ling Feng;Tharam S. Dillon
#t 2003
#c 7
#% 56
#% 25443
#% 95224
#% 114479
#% 176060
#% 199537
#% 201898
#% 201929
#% 210182
#% 210184
#% 212710
#% 213062
#% 223781
#% 227861
#% 227869
#% 227944
#% 248806
#% 248807
#% 256702
#% 266232
#% 277105
#% 383684
#% 384209
#% 394141
#% 459024
#% 459299
#% 461921
#% 462074
#% 462079
#% 462208
#% 462213
#% 462217
#% 464215
#% 464706
#% 479470
#% 479618
#% 479621
#% 479795
#% 481948
#% 481951
#% 482081
#% 482098
#% 542582
#% 635798
#% 1788921
#! A data warehouse integrates large amounts of extracted and summarized data from multiple sources for direct querying and analysis. While it provides decision makers with easy access to such historical and aggregate data, the real meaning of the data has been ignored. For example, "whether a total sales amount 1,000 items indicates a good or bad sales performance" is still unclear. From the decision makers' point of view, the semantics rather than raw numbers which convey the meaning of the data is very important. In this paper, we explore the use of fuzzy technology to provide this semantics for the summarizations and aggregates developed in data warehousing systems. A three layered data warehouse semantic model, consisting of quantitative (numerical) summarization, qualitative (categorical) summarization, and quantifier summarization, is proposed for capturing and explicating the semantics of warehoused data. Based on the model, several algebraic operators are defined. We also extend the SQL language to allow for flexible queries against such enhanced data warehouses.

#index 452849
#* Semantic Abstractions in the Multimedia Domain
#@ Elina Megalou;Thanasis Hadzilacos
#t 2003
#c 7
#% 73774
#% 102989
#% 115118
#% 116185
#% 116703
#% 131562
#% 171734
#% 171743
#% 172505
#% 179765
#% 191581
#% 197908
#% 197911
#% 201850
#% 210388
#% 217077
#% 248818
#% 287195
#% 319244
#% 443231
#% 443241
#% 443260
#% 452790
#% 535990
#% 663301
#% 1272167
#% 1275252
#! Information searching by exactly matching content is traditionally a strong point of machine searching; this is not, however, how human memory works and is rarely satisfactory for advanced retrieval tasks in any domain驴multimedia in particular, where the presentational aspects can be equally important to the semantic information content. A combined abstraction of the conceptual and presentational characteristics of multimedia applications, leading on the one hand to their conceptual structure (with classic semantics of the real-world modeled by entities, relationships, and attributes) and on the other to the presentational structure (including media type, logical structure, temporal synchronization, spatial (on the screen) "synchronization" and interactive behavior) is developed in this paper. Multimedia applications are construed as consisting of "Presentational Units:" elementary (a media object with play duration and screen position), and composite (recursive structures of PUs in the temporal, spatial, and logical dimensions). The fundamental concept introduced is that of Semantic Multimedia Abstractions (SMA): qualitative abstract descriptions of multimedia applications in terms of their conceptual and presentational properties at an adjustable level of abstraction. SMAs, which could be viewed as metadata, form an abstract space to be queried. A detailed study of possible abstractions (from multimedia applications to SMAs and SMA-to-SMA), a definition and query language for Semantic Multimedia Abstractions (SMA-L) and the corresponding SMA model (equivalent to extended OMT), as well as an implementation of a system capable of wrapping the presentational structure of XML-based documents complete this work, whose contribution lays in the classically fruitful boundary between AI, software engineering, and database research.

#index 452850
#* Optimizing Index Allocation for Sequential Data Broadcasting in Wireless Mobile Computing
#@ Ming-Syan Chen;Kun-Lung Wu;Philip S. Yu
#t 2003
#c 7
#% 91077
#% 102802
#% 172874
#% 172875
#% 172913
#% 175253
#% 198045
#% 201897
#% 225006
#% 234905
#% 287258
#% 316486
#% 316788
#% 342711
#% 384050
#% 408396
#% 443035
#% 443127
#% 452847
#% 461923
#% 462077
#% 464214
#% 480438
#% 480965
#% 632067
#% 635895
#! Energy saving is one of the most important issues in wireless mobile computing. Among others, one viable approach to achieving energy saving is to use an indexed data organization to broadcast data over wireless channels to mobile units. Using indexed broadcasting, mobile units can be guided to the data of interest efficiently and only need to be actively listening to the broadcasting channel when the relevant information is present. In this paper, we explore the issue of indexing data with skewed access for sequential broadcasting in wireless mobile computing. We first propose methods to build index trees based on access frequencies of data records. To minimize the average cost of index probes, we consider two cases: one for fixed index fanouts and the other for variant index fanouts, and devise algorithms to construct index trees for both cases. We show that the cost of index probes can be minimized not only by employing an imbalanced index tree that is designed in accordance with data access skew, but also by exploiting variant fanouts for index nodes. Note that, even for the same index tree, different broadcasting orders of data records will lead to different average data access times. To address this issue, we develop an algorithm to determine the optimal order for sequential data broadcasting to minimize the average data access time. Performance evaluation on the algorithms proposed is conducted. Examples and remarks are given to illustrate our results.

#index 452851
#* Semantic Caching and Query Processing
#@ Qun Ren;Margaret H. Dunham;Vijay Kumar
#t 2003
#c 7
#% 36683
#% 68086
#% 83126
#% 98469
#% 169844
#% 198038
#% 209634
#% 238413
#% 248806
#% 273707
#% 287200
#% 360716
#% 403195
#% 413181
#% 443061
#% 481916
#% 571216
#! Semantic caching is very attractive for use in distributed systems due to the reduced network traffic and the improved response time. It is particularly efficient for a mobile computing environment, where the bandwidth of wireless links is a major performance bottleneck. Previous work either does not provide a formal semantic caching model, or lacks efficient query processing strategies. This paper extends the existing research in three ways: formal definitions associated with semantic caching are presented, query processing strategies are investigated and, finally, the performance of the semantic cache model is examined through a detailed simulation study.

#index 452852
#* Slot Index Spatial Join
#@ Nikos Mamoulis;Dimitris Papadias
#t 2003
#c 7
#% 2115
#% 25924
#% 58369
#% 86950
#% 136740
#% 152937
#% 153260
#% 210186
#% 210187
#% 213975
#% 227932
#% 252304
#% 260036
#% 273886
#% 273887
#% 286237
#% 384872
#% 427199
#% 443190
#% 443258
#% 462059
#% 463595
#% 464831
#% 479453
#% 479473
#% 479797
#% 481428
#% 503853
#% 511636
#% 527012
#% 527180
#! Efficient processing of spatial joins is very important due to their high cost and frequent application in spatial databases and other areas involving multidimensional data. This paper proposes slot index spatial join (SISJ), an algorithm that joins a nonindexed data set with one indexed by an R-tree. We explore two optimization techniques that reduce the space requirements and the computational cost of SISJ and we compare it, analytically and experimentally, with other spatial join methods for two cases: 1) when the nonindexed input is read from disk and 2) when it is an intermediate result of a preceding database operator in a complex query plan. The importance of buffer splitting between consecutive join operators is also demonstrated through a two-join case study and a method that estimates the optimal splitting is proposed. Our evaluation shows that SISJ outperforms alternative methods in most cases and is suitable for limited memory conditions.

#index 452853
#* A Scalable Approach to Integrating Heterogeneous Aggregate Views of Distributed Databases
#@ Sally McClean;Bryan Scotney;Kieran Greer
#t 2003
#c 7
#% 152588
#% 191918
#% 279484
#% 443455
#! Aggregate views are commonly used for summarizing information held in very large databases such as those encountered in data warehousing, large scale transaction management, and statistical databases. Such applications often involve distributed databases that have developed independently and therefore may exhibit incompatibility, heterogeneity, and data inconsistency. We are here concerned with the integration of aggregates that have heterogeneous classification schemes where local ontologies, in the form of such classification schemes, may be mapped onto a common ontology. In previous work, we have developed a method for the integration of such aggregates; the method previously developed is efficient, but cannot handle innate data inconsistencies that are likely to arise when a large number of databases are being integrated. In this paper, we develop an approach that can handle data inconsistencies and is thus inherently much more scalable. In our new approach, we first construct a dynamic shared ontology by analyzing the correspondence graph that relates the heterogeneous classification schemes; the aggregates are then derived by minimization of the Kullback-Leibler information divergence using the EM (Expectation-Maximization) algorithm. Thus, we may assess whether global queries on such aggregates are answerable, partially answerable, or unanswerable in advance of computing the aggregates themselves.

#index 452854
#* An Information Retrieval Approach for Approximate Queries
#@ Pável Pereira Calado;Berthier Ribeiro-Neto
#t 2003
#c 7
#% 41230
#% 83336
#% 84656
#% 213443
#% 248801
#% 319273
#% 387427
#% 504581
#% 616150
#! With the growing availability of online information systems, a need for user interfaces that are flexible and easy to use has arisen. For such type of systems, an interface that allows the formulation of approximate queries can be of great utility since these allow the user to quickly explore the database contents even when he is unaware of the exact values of the database instances. Our work focuses on this problem, presenting a new model for ranking approximate answers and a new algorithm to compute the semantic similarity between attribute values, based on information retrieval techniques. To demonstrate the utility and usefulness of the approach, we perform a series of usability tests. The results suggest that our approach allows the retrieval of more relevant answers with less effort by the user.

#index 452855
#* Modeling Completeness versus Consistency Tradeoffs in Information Decision Contexts
#@ Donald P. Ballou;Harold L. Pazer
#t 2003
#c 7
#% 406493
#% 912094
#! Decision makers often confront the issue of whether to utilize information based on incomplete but consistent data or instead rely on complete but less consistent data. For a given decision context, this paper introduces a framework that permits the systematic exploration of this tradeoff between the completeness and consistency of data. The relative weight (importance) of completeness and consistency to the decision maker is an integral component of the approach. To examine the tradeoff, the paper explores in detail various facets of these data quality dimensions. These, in turn, lead to expressions for the measurement of completeness and consistency. The utility of various combinations of completeness and consistency for fixed and variable budgets provides guidance as to the appropriate tradeoff of these factors for specific decision contexts.

#index 452856
#* Conflict Resolution Using Logic Programming
#@ Jan Chomicki;Jorge Lobo;Shamim Naqvi
#t 2003
#c 7
#% 1797
#% 103705
#% 112318
#% 213978
#% 227951
#% 235018
#% 252258
#% 270718
#% 270722
#% 273687
#% 283126
#% 286536
#% 301567
#% 435147
#% 463590
#% 464915
#% 480620
#% 616819
#% 635840
#% 1393787
#! This paper addresses some issues involved in applying the event-condition-action (ECA) rule paradigm of active databases to policies驴collections of general principles specifying the desired behavior of a system. We use a declarative policy description language {\cal{PDL}}, in which policies are formulated as sets of ECA rules. The main contribution of the paper is a framework for detecting action conflicts and finding resolutions to these conflicts. Conflicts are captured as violations of action constraints. The semantics of rules and conflict detection and resolution are defined axiomatically using logic programs. Given a policy and a set of action constraints, the framework defines a range of monitors that filter the output of the policy to satisfy the constraints.

#index 452857
#* Call for Papers for a new journal IEEE Transactions on Mobile Computing
#@  IEEE Transactions on Knowledge and Data Engineering staff
#t 2003
#c 7

#index 452858
#* Termination and Confluence by Rule Prioritization
#@ Sara Comai;Letizia Tanca
#t 2003
#c 7
#% 33376
#% 58356
#% 73005
#% 77680
#% 83228
#% 86944
#% 116044
#% 116045
#% 125951
#% 153004
#% 197480
#% 205241
#% 277343
#% 442824
#% 459018
#% 459259
#% 461891
#% 481456
#% 501951
#% 501953
#% 501957
#% 553958
#% 565260
#! An active database system is a DBMS endowed with active rules, i.e., stored procedures activated by the system when specific events occur. The processing of active rules is characterized by two important properties: termination and confluence. We say that the processing of a set of active rules terminates if, given any initial active database state, the execution of the rules does not continue indefinitely; it is confluent if, for any initial database state, the final state is not influenced by the order of execution of the rules. Finding sufficient conditions for these properties to hold is a nontrivial problem, and the lack of a structured theory for the design of a system of active rules makes the analysis of the two properties more difficult. In this work, we translate a set of rules from any of the existing systems into an internal format; then, we translate the active rules into logical clauses, taking into account the system's execution semantics, and transfer to the active process known simple results about termination and determinism available in the literature for deductive rules.

#index 452859
#* Uniform Techniques for Deriving Similarities of Objects and Subschemes in Heterogeneous Databases
#@ Luigi Palopoli;Domenico Saccà;Giorgio Terracina;Domenico Ursino
#t 2003
#c 7
#% 3938
#% 22948
#% 55294
#% 126330
#% 127670
#% 158907
#% 158908
#% 163046
#% 227886
#% 260021
#% 287631
#% 292024
#% 300711
#% 309678
#% 326667
#% 331769
#% 442787
#% 442861
#% 462048
#% 495129
#% 571297
#% 747791
#! The availability of automatic tools for inferring semantics of database schemes is useful to solve several database design problems such as, that of obtaining Cooperative Information Systems or Data Warehouses from large sets of data sources. In this context, a main problem is to single out similarities or dissimilarities among scheme objects (interscheme properties). This paper presents graph-based techniques for a uniform derivation of interscheme properties including synonymies, homonymies, type conflicts, and subscheme similarities. These techniques are characterized by a common core: the computation of maximum weight matchings on some bipartite weighted graphs derived using a suitable metrics to measure semantic closeness of objects. The techniques have been implemented in a system prototype. Several experiments conducted with it, and (in part) accounted for in the paper, confirmed the effectiveness of our approach.

#index 452860
#* A New Temporal Pattern Identification Method for Characterization and Prediction of Complex Time Series Events
#@ Richard J. Povinelli;Xin Feng
#t 2003
#c 7
#% 136350
#% 172949
#% 207540
#% 232102
#% 232122
#% 243449
#% 252533
#% 283227
#% 359201
#% 369236
#% 460862
#% 462231
#% 616530
#% 710509
#! A new method for analyzing time series data is introduced in this paper. Inspired by data mining, the new method employs time-delayed embedding and identifies temporal patterns in the resulting phase spaces. An optimization method is applied to search the phase spaces for optimal heterogeneous temporal pattern clusters that reveal hidden temporal patterns, which are characteristic and predictive of time series events. The fundemantal concepts and framework of the method are explained in detail. The method is then applied to the characterization and prediction, with a high degree of accuracy, of the release of metal droplets from a welder. The results of the method are compared to those from a Time Delay Neural Network and the C4.5 decision tree algorithm.

#index 452861
#* Object-Based Directional Query Processing in Spatial Databases
#@ Xuan Liu;Shashi Shekhar;Sanjay Chawla
#t 2003
#c 7
#% 26168
#% 39656
#% 223645
#% 232477
#% 260056
#% 427199
#% 435140
#% 443258
#% 443327
#% 549078
#% 564083
#! Direction-based spatial relationships are critical in many domains, including geographic information systems (GIS) and image interpretation. They are also frequently used as selection conditions in spatial queries. In this paper, we explore the processing of object-based direction queries and propose a new open shape-based strategy (OSS). OSS models the direction region as an open shape and converts the processing of the direction predicates into the processing of topological operations between open shapes and closed geometry objects. The proposed strategy OSS makes it unnecessary to know the boundary of the embedding world and also eliminates the computation related to the world boundary. OSS reduces both I/O and CPU costs by greatly improving the filtering effectiveness. Our experimental evaluation shows that OSS consistently outperforms classical range query strategies (RQS) while the degree of performance improvement varies by several parameters. Experimental results also demonstrate that OSS is more scalable than RQS for large data sets.

#index 452862
#* Toward an Accurate Analysis of Range Queries on Spatial Data
#@ Ning An;Ji Jin;Anand Sivasubramaniam
#t 2003
#c 7
#% 3453
#% 32913
#% 43163
#% 77928
#% 86950
#% 86951
#% 137887
#% 153260
#% 164360
#% 168772
#% 181978
#% 213975
#% 242366
#% 273887
#% 286237
#% 411694
#% 419651
#% 427199
#% 427219
#% 443258
#% 462218
#% 480093
#% 480610
#% 481620
#% 631993
#% 632104
#! Analysis of range queries on spatial (multidimensional) data is both important and challenging. Most previous analysis attempts have made certain simplifying assumptions about the data sets and/or queries to keep the analysis tractable. As a result, they may not be universally applicable. This paper proposes a set of five analysis techniques to estimate the selectivity and number of index nodes accessed in serving a range query. The underlying philosophy behind these techniques is to maintain an auxiliary data structure, called a density file, whose creation is a one-time cost, which can be quickly consulted when the query is given. The schemes differ in what information is kept in the density file, how it is maintained, and how this information is looked up. It is shown that one of the proposed schemes, called Cumulative Density (CD), gives very accurate results (usually less than 5 percent error) using a diverse suite of point and rectangular data sets, that are uniform or skewed, and a wide range of query window parameters. The estimation takes a constant amount of time, which is typically lower than 1 percent of the time that it would take to execute the query, regardless of data set or query window parameters.

#index 452863
#* Mining Optimized Gain Rules for Numeric Attributes
#@ Sergey Brin;Rajeev Rastogi;Kyuseok Shim
#t 2003
#c 7
#% 136704
#% 152934
#% 201894
#% 210160
#% 210162
#% 213977
#% 280436
#% 282658
#% 461909
#% 462234
#% 479648
#% 481290
#% 481588
#% 481754
#% 481758
#% 631971
#! Association rules are useful for determining correlations between attributes of a relation and have applications in the marketing, financial, and retail sectors. Furthermore, optimized association rules are an effective way to focus on the most interesting characteristics involving certain attributes. Optimized association rules are permitted to contain uninstantiated attributes and the problem is to determine instantiations such that either the support, confidence, or gain of the rule is maximized. In this paper, we generalize the optimized gain association rule problem by permitting rules to contain disjunctions over uninstantiated numeric attributes. Our generalized association rules enable us to extract more useful information about seasonal and local patterns involving the uninstantiated attribute. For rules containing a single numeric attribute, we present an algorithm with linear complexity for computing optimized gain rules. Furthermore, we propose a bucketing technique that can result in a significant reduction in input size by coalescing contiguous values without sacrificing optimality. We also present an approximation algorithm based on dynamic programming for two numeric attributes. Using recent results on binary space partitioning trees, we show that the approximations are within a constant factor of the optimal optimized gain rules. Our experimental results with synthetic data sets for a single numeric attribute demonstrate that our algorithm scales up linearly with the attribute's domain size as well as the number of disjunctions. In addition, we show that applying our optimized rule framework to a population survey real-life data set enables us to discover interesting underlying correlations among the attributes.

#index 452864
#* Synthesizing High-Frequency Rules from Different Data Sources
#@ Xindong Wu;Shichao Zhang
#t 2003
#c 7
#% 90212
#% 132938
#% 152934
#% 169705
#% 199538
#% 201894
#% 210160
#% 227919
#% 243728
#% 248012
#% 248784
#% 248786
#% 252401
#% 273898
#% 300120
#% 310505
#% 312871
#% 379325
#% 379338
#% 379339
#% 379341
#% 379342
#% 418076
#% 420091
#% 443082
#% 443085
#% 443091
#% 464204
#% 481290
#% 481754
#% 511333
#% 520713
#% 703747
#% 1271840
#! Many large organizations have multiple data sources, such as different branches of an interstate company. While putting all data together from different sources might amass a huge database for centralized processing, mining association rules at different data sources and forwarding the rules (rather than the original raw data) to the centralized company headquarter provides a feasible way to deal with multiple data source problems. In the meanwhile, the association rules at each data source may be required for that data source in the first instance, so association analysis at each data source is also important and useful. However, the forwarded rules from different data sources may be too many for the centralized company headquarter to use. This paper presents a weighting model for synthesizing high-frequency association rules from different data sources. There are two reasons to focus on high-frequency rules. First, a centralized company headquarter is interested in high-frequency rules because they are supported by most of its branches for corporate profitability. Second, high-frequency rules have larger chances to become valid rules in the union of all data sources. In order to extract high-frequency rules efficiently, a procedure of rule selection is also constructed to enhance the weighting model by coping with low-frequency rules. Experimental results show that our proposed weighting model is efficient and effective.

#index 452865
#* Binding Propagation Techniques for the Optimization of Bound Disjunctive Queries
#@ Sergio Greco
#t 2003
#c 7
#% 11797
#% 36683
#% 53388
#% 58363
#% 64412
#% 68140
#% 73129
#% 101623
#% 105243
#% 114723
#% 129789
#% 154319
#% 176471
#% 181038
#% 235018
#% 244377
#% 305418
#% 384978
#% 443134
#% 464531
#% 473193
#% 479801
#% 493843
#% 497466
#% 499509
#% 499636
#% 501034
#% 501035
#% 556918
#% 560057
#% 561217
#% 587310
#% 1393787
#! This paper presents a technique for the optimization of bound queries on disjunctive deductive databases. The optimization is based on the rewriting of the source program into an equivalent program which can be evaluated more efficiently. The proposed optimization reduces the amount of data needed to answer the query and, consequently, 1) reduces the complexity of computing a single model and, more importantly, 2) greatly reduces the number of models to be considered. Although, in this paper, we consider the application of the magic-set method, other rewriting techniques defined for special classes of queries can also be applied. To show the relevance of our technique, we have implemented a prototype of an optimizer. Several experiments have confirmed the value of the technique.

#index 452866
#* Static Analysis of Logical Languages with Deferred Update Semantics
#@ Barbara Catania;Elisa Bertino
#t 2003
#c 7
#% 35562
#% 36683
#% 53393
#% 53400
#% 55408
#% 64414
#% 64415
#% 86944
#% 101646
#% 102547
#% 122398
#% 123589
#% 137871
#% 140410
#% 169697
#% 175108
#% 182421
#% 182424
#% 205153
#% 213979
#% 264860
#% 277325
#% 277345
#% 442702
#% 443059
#% 443162
#% 464713
#% 481615
#% 511652
#% 532630
#% 532639
#% 553834
#% 564961
#% 565260
#% 752892
#% 752894
#! Static analysis of declarative languages deals with the detection, at compile time, of program properties that can be used to better understand the program semantics and to improve the efficiency of program evaluation. In logical update languages, an interesting problem is the detection of conflicting updates, inserting and deleting the same fact, for transactions based on set-oriented updates and active rules. In this paper, we investigate this topic in the context of the U-Datalog language, a set-oriented update language for deductive databases [12], based on a deferred semantics. We first formally define relevant properties of U-Datalog programs, mainly related to update conflicts. Then, we prove that the defined properties are decidable and we propose an algorithm to detect such conditions. Finally, we show how the proposed techniques can be applied to other logical update languages. Our results are based on the concept of labeling and query-tree, first used in [30], [31], [32].

#index 452867
#* Model and Algorithm for Efficient Verification of High-Assurance Properties of Real-Time Systems
#@ Jeffrey J. P. Tsai;Eric Y. T. Juan;Avinash Sahay
#t 2003
#c 7
#% 1791
#% 35775
#% 37977
#% 54742
#% 101958
#% 138176
#% 157178
#% 160339
#% 162493
#% 182153
#% 183293
#% 184451
#% 204112
#% 212998
#% 215749
#% 215753
#% 235339
#% 264465
#% 319810
#% 323123
#% 334107
#% 393872
#% 441200
#% 442925
#% 444269
#% 445902
#% 511225
#% 512015
#% 541576
#% 542869
#% 542987
#% 543351
#% 543784
#% 543831
#% 543838
#% 543843
#% 727609
#! In this paper, we present a new compositional verification methodology for efficiently verifying high-assurance properties such as reachability and deadlock freedom of real-time systems. In this methodology, each component of real-time systems is initially specified as a timed automaton and it communicates with other components via synchronous and/or asynchronous communication channels. Then, each component is analyzed by a generation of its state-space graph which is formalized as a new state-space representation model called Multiset Labeled Transition Systems (MLTSs). Afterward, the state spaces of the components are hierarchically composed and simplified through a composition algorithm and a set of condensation rules, respectively, to get a condensed state space of the system. The simplified state spaces preserve equivalence with respect to deadlock and reachable states. Such equivalence is assured by our reduction theories called IOT-failure equivalence and IOT-state equivalence. To show the performance of our methodology, we developed a verification tool RT-IOTA and carried out experiments on some benchmarks such as CSMA/CD protocol, a rail-road crossing, an alternating bit-protocol, etc. Specifically, we look at the time taken to generate the state-space, the size of the state space, and the amount of reduction achieved by our condensation rules. The results demonstrate the strength of our new technique in dealing with the state-explosion problem.

#index 452868
#* Detecting and Representing Relevant Web Deltas in WHOWEDA
#@ Sourav S. Bhowmick;Sanjay Kumar Madria;Wee Keong Ng
#t 2003
#c 7
#% 84549
#% 186968
#% 209692
#% 210212
#% 227859
#% 256623
#% 289101
#% 316563
#% 340295
#% 378180
#% 394417
#% 424263
#% 443298
#% 462212
#% 535710
#% 536021
#% 545958
#% 635919
#% 637230
#% 659923
#! In this paper, we present a mechanism for detecting and representing changes, given the old and new versions of a set of interlinked Web documents, retrieved in response to a user's query. In particular, we show how to detect and represent Web deltas, i.e., changes in the Web documents that are relevant to a user's query in the context of our Web warehousing system called Whoweda (Warehouse of Web Data). In Whoweda, Web information is materialized views stored in Web tables in the form of Web tuples. These Web tuples, represented as directed graphs, can be manipulated using a set of Web algebraic operators. In this paper, we present a mechanism to detect relevant Web deltas using Web algebraic operators such as the Web join and the outer Web join. Web join is used to detect identical documents residing in two Web tables, whereas, outer Web join, a derivative of Web join, is used to identify dangling Web tuples. We show how to represent these changes using delta Web tables. We develop formal algorithms for the generation of delta Web tables identifying Web documents which have been added, deleted, or modified since the last query.

#index 452869
#* Determining Semantic Similarity among Entity Classes from Different Ontologies
#@ M. Andrea Rodríguez;Max J. Egenhofer
#t 2003
#c 7
#% 23952
#% 82275
#% 83273
#% 111912
#% 111922
#% 116202
#% 152968
#% 158907
#% 179876
#% 197986
#% 219036
#% 234793
#% 236256
#% 285540
#% 287730
#% 309678
#% 405391
#% 445309
#% 465914
#% 529313
#% 535820
#% 535824
#% 591543
#! Semantic similarity measures play an important role in information retrieval and information integration. Traditional approaches to modeling semantic similarity compute the semantic distance between definitions within a single ontology. This single ontology is either a domain-independent ontology or the result of the integration of existing ontologies. We present an approach to computing semantic similarity that relaxes the requirement of a single ontology and accounts for differences in the levels of explicitness and formalization of the different ontology specifications. A similarity function determines similar entity classes by using a matching process over synonym sets, semantic neighborhoods, and distinguishing features that are classified into parts, functions, and attributes. Experimental results with different ontologies indicate that the model gives good results when ontologies have complete and detailed representations of entity classes. While the combination of word matching and semantic neighborhood matching is adequate for detecting equivalent entity classes, feature matching allows us to discriminate among similar, but not necessarily equivalent entity classes.

#index 452870
#* Fuzzy Rule Base Systems Verification Using High-Level Petri Nets
#@ Stephen J. H. Yang;Jeffrey J. P. Tsai;Chyun-Chyi Chen
#t 2003
#c 7
#% 20561
#% 34874
#% 40177
#% 40312
#% 50730
#% 60463
#% 126610
#% 157720
#% 162639
#% 442719
#% 442764
#% 442922
#% 443015
#% 443028
#% 443115
#% 445172
#% 452775
#% 452776
#% 482271
#% 482704
#% 483021
#% 595498
#% 1780697
#% 1780984
#% 1787873
#! In this paper, we propose a Petri nets formalism for the verification of rule-based systems. Typical structural errors in a rule-based system are redundancy, inconsistency, incompleteness, and circularity. Since our verification is based on Petri nets and their incidence matrix, we need to transform rules into a Petri nets first, then derive an incidence matrix from the net. In order to let fuzzy rule-based systems detect above the structural errors, we are presenting a Petri-nets-based mechanism. This mechanism consists of three phases: rule normalization, rules transformation, and rule verification. Rules will be first normalized into Horn clauses, then transform the normalized rules into a high-level Petri net, and finally we verify these normalized rules. In addition, we are presenting our approach to simulate the truth conditions which still hold after a transition firing and negation in Petri nets for rule base modeling. In this paper, we refer to fuzzy rules as the rules with certainty factors, the degree of truth is computed in an algebraic form based on state equation which can be implemented in matrix computation in Petri nets. Therefore, the fuzzy reasoning problems can be transformed as the liner equation problems that can be solved in parallel. We have implemented a Petri nets tool to realize the mechanism presented fuzzy rules in this paper.

#index 452871
#* Performance Analysis of Location-Dependent Cache Invalidation Schemes for Mobile Environments
#@ Jianliang Xu;Xueyan Tang;Dik Lun Lee
#t 2003
#c 7
#% 70068
#% 77005
#% 172874
#% 221548
#% 229260
#% 243061
#% 245014
#% 259635
#% 281466
#% 281506
#% 281536
#% 300173
#% 300174
#% 303792
#% 309429
#% 309430
#% 309458
#% 309461
#% 344720
#% 419606
#% 461919
#% 461923
#% 462202
#% 464214
#% 464848
#% 479961
#% 481916
#% 503869
#% 589300
#% 1830002
#% 1849178
#! Mobile location-dependent information services are gaining increasing interest in both academic and industrial communities. In these services, data values depend on their locations. Caching frequently accessed data on mobile clients can help save wireless bandwidth and improve system performance. However, since client location changes constantly, location-dependent data may become obsolete not only due to updates performed on data items but also because of client movements across the network. To the best of the authors' knowledge, previous work on cache invalidation issues focused on data updates only. This paper considers data inconsistency caused by client movements and proposes three location-dependent cache invalidation schemes. The performance for the proposed schemes is investigated by both analytical study and simulation experiments in a scenario where temporal- and location-dependent updates coexist. Both analytical and experimental results show that, in most cases, the proposed methods substantially outperform the NSI scheme, which drops the entire cache contents when hand-off is performed.

#index 452872
#* Symbolic User-Defined Periodicity in Temporal Relational Databases
#@ Paolo Terenziani
#t 2003
#c 7
#% 16028
#% 43028
#% 49596
#% 64284
#% 66122
#% 71562
#% 100613
#% 111284
#% 114677
#% 135873
#% 181034
#% 184796
#% 190329
#% 200965
#% 225003
#% 263982
#% 268796
#% 277326
#% 319244
#% 361445
#% 443161
#% 443321
#% 443363
#% 445155
#% 463738
#% 464550
#% 527793
#% 618582
#% 618602
#% 618604
#% 744412
#! Calendars and periodicity play a fundamental role in many applications. Recently, some commercial databases started to support user-defined periodicity in the queries in order to provide 驴a human-friendly way of handling time驴 (see, e.g., TimeSeries in Oracle 8). On the other hand, only few relational data models support user-defined periodicity in the data, mostly using 驴mathematical驴 expressions to represent periodicity. In this paper, we propose a high-level 驴symbolic驴 language for representing user-defined periodicity which seems to us more human-oriented than mathematical ones, and we use the domain of Gadia's temporal elements in order to define its properties and its extensional semantics. We then propose a temporal relational model which supports user-defined 驴symbolic驴periodicity (e.g., to express 驴on the second Monday of each month驴) in the validity time of tuples and also copes with frame times (e.g., 驴from 1/1/98 to 28/2/98驴). We define the temporal counterpart of the standard operators of the relational algebra, and we introduce new temporal operators and functions. We also prove that our temporal algebra is a consistent extension of the classical (atemporal) one. Moreover, we define both a fully symbolic evaluation method for the operators on the periodicities in the validity times of tuples, which is correct but not complete, and semisymbolic one, which is correct and complete, and study their computational complexity.

#index 452873
#* Converting a Fuzzy Data Model to an Object-Oriented Design for Managing GIS Data Files
#@ Gregory Vert;Ashley Morris;Molly Stock
#t 2003
#c 7
#% 236412
#% 424933
#! A fuzzy entity relationship diagram (ERD) data model for managing information about sets of related data files was converted to an object design. To do this, we developed new methods incorporating object model flattening, entity payload data containerization, and a nonintegrated object model design to allow expression of fuzziness.

#index 578387
#* Guest Editor Introduction: Special Section on Online Analysis and Querying of Continuous Data Streams
#@ Rajeev Rastogi
#t 2003
#c 7
#! First Page of the Article

#index 578388
#* Clustering Data Streams: Theory and Practice
#@ Sudipto Guha;Adam Meyerson;Nina Mishra;Rajeev Motwani;Liadan O'Callaghan
#t 2003
#c 7
#% 2833
#% 36672
#% 54221
#% 115562
#% 136701
#% 210173
#% 214073
#% 232716
#% 232768
#% 248790
#% 248792
#% 248820
#% 249176
#% 266479
#% 271130
#% 271236
#% 271237
#% 273890
#% 273907
#% 282481
#% 282899
#% 282942
#% 299989
#% 303042
#% 316709
#% 320942
#% 325357
#% 333931
#% 338343
#% 338344
#% 338425
#% 338442
#% 341672
#% 347200
#% 347226
#% 347263
#% 379444
#% 379445
#% 397384
#% 397385
#% 420058
#% 443392
#% 479799
#% 479962
#% 481281
#% 481749
#% 536886
#% 566128
#% 568629
#% 593842
#% 593913
#% 593937
#% 593957
#% 593960
#% 594009
#% 594010
#% 594011
#% 594012
#% 594029
#% 656775
#% 659972
#% 660003
#% 993969
#% 1650386
#! The data stream model has recently attracted attention for its applicability to numerous types of data, including telephone records, Web documents, and clickstreams. For analysis of such data, the ability to process the data in a single pass, or a small number of passes, while using little memory, is crucial. We describe such a streaming algorithm that effectively clusters large data streams. We also provide empirical evidence of the algorithm's performance on synthetic and real data streams.

#index 578389
#* Comparing Data Streams Using Hamming Norms (How to Zero In)
#@ Graham Cormode;Mayur Datar;Piotr Indyk;S. Muthukrishnan
#t 2003
#c 7
#% 2833
#% 132779
#% 248821
#% 278835
#% 282942
#% 299989
#% 336610
#% 338425
#% 340581
#% 342600
#% 379443
#% 397369
#% 397385
#% 397414
#% 428155
#% 480156
#% 480628
#% 480805
#% 481749
#% 519953
#% 593957
#% 594012
#% 594029
#% 632029
#% 659979
#% 660004
#% 963734
#% 1863141
#! Massive data streams are now fundamental to many data processing applications. For example, Internet routers produce large scale diagnostic data streams. Such streams are rarely stored in traditional databases and instead must be processed 驴on the fly驴 as they are produced. Similarly, sensor networks produce multiple data streams of observations from their sensors. There is growing focus on manipulating data streams and, hence, there is a need to identify basic operations of interest in managing data streams, and to support them efficiently. We propose computation of the Hamming norm as a basic operation of interest. The Hamming norm formalizes ideas that are used throughout data processing. When applied to a single stream, the Hamming norm gives the number of distinct items that are present in that data stream, which is a statistic of great interest in databases. When applied to a pair of streams, the Hamming norm gives an important measure of (dis)similarity: the number of unequal item counts in the two streams. Hamming norms have many uses in comparing data streams. We present a novel approximation technique for estimating the Hamming norm for massive data streams; this relies on what we call the 驴l_0sketch驴 and we prove its accuracy. We test our approximation method on a large quantity of synthetic and real stream data, and show that the estimation is accurate to within a few percentage points.

#index 578390
#* One-Pass Wavelet Decompositions of Data Streams
#@ Anna C. Gilbert;Yannis Kotidis;S. Muthukrishnan;Martin J. Strauss
#t 2003
#c 7
#% 88344
#% 210190
#% 214073
#% 248812
#% 248822
#% 259995
#% 273682
#% 273903
#% 273907
#% 302724
#% 310500
#% 333872
#% 333926
#% 333983
#% 347200
#% 347226
#% 397354
#% 397385
#% 420123
#% 428155
#% 438133
#% 479795
#% 479816
#% 480306
#% 480465
#% 480628
#% 593957
#% 594012
#% 594029
#% 631923
#% 659979
#% 660004
#% 993969
#! We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general 驴sketch驴-based methods for capturing various linear projections and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data and provide accurate representation as our experiments with real data streams show.

#index 578391
#* Exploiting Punctuation Semantics in Continuous Data Streams
#@ Peter A. Tucker;David Maier;Tim Sheard;Leonidas Fegaras
#t 2003
#c 7
#% 91459
#% 136740
#% 172950
#% 227883
#% 296163
#% 300167
#% 310488
#% 333926
#% 340635
#% 378408
#% 397353
#% 413563
#% 428155
#% 462500
#% 480628
#% 480764
#% 500918
#% 650962
#% 660004
#% 979303
#% 993949
#! As most current query processing architectures are already pipelined, it seems logical to apply them to data streams. However, two classes of query operators are impractical for processing long or infinite data streams. Unbounded stateful operators maintain state with no upper bound in size and, so, run out of memory. Blocking operators read an entire input before emitting a single output and, so, might never produce a result. We believe that a priori knowledge of a data stream can permit the use of such operators in some cases. We discuss a kind of stream semantics called punctuated streams. Punctuations in a stream mark the end of substreams allowing us to view an infinite stream as a mixture of finite streams. We introduce three kinds of invariants to specify the proper behavior of operators in the presence of punctuation. Pass invariants define when results can be passed on. Keep invariants define what must be kept in local state to continue successful operation. Propagation invariants define when punctuation can be passed on. We report on our initial implementation and show a strategy for proving implementations of these invariants are faithful to their relational counterparts.

#index 578392
#* Efficient Approximation of Correlated Sums on Data Streams
#@ Rohit Ananthakrishna;Abhinandan Das;Johannes Gehrke;Flip Korn;S. Muthukrishnan;Divesh Srivastava
#t 2003
#c 7
#% 278835
#% 282942
#% 333926
#% 333931
#% 378388
#% 397354
#% 482082
#% 593957
#% 979303
#! In many applications such as IP network management, data arrives in streams and queries over those streams need to be processed online using limited storage. Correlated-sum (CS) aggregates are a natural class of queries formed by composing basic aggregates on (x,y) pairs and are of the form SUM{g(y) : x \leq f(AGG(x))}, where AGG(x) can be any basic aggregate and f(), g() are user-specified functions. CS-aggregates cannot be computed exactly in one pass through a data stream using limited storage; hence, we study the problem of computing approximate CS-aggregates. We guarantee a priori error bounds when AGG(x) can be computed in limited space (e.g., MIN, MAX, AVG), using two variants of Greenwald and Khanna's summary structure for the approximate computation of quantiles. Using real data sets, we experimentally demonstrate that an adaptation of the quantile summary structure uses much less space, and is significantly faster, than a more direct use of the quantile summary structure, for the same a posteriori error bounds. Finally, we prove that, when AGG(x) is a quantile (which cannot be computed over a data stream in limited space), the error of a CS-aggregate can be arbitrarily large.

#index 578393
#* On the Graph Traversal and Linear Binary-Chain Programs
#@ Yangjun Chen
#t 2003
#c 7
#% 1004
#% 3496
#% 5965
#% 11797
#% 13014
#% 23903
#% 23904
#% 33376
#% 36683
#% 66097
#% 69087
#% 101623
#% 101624
#% 101625
#% 141472
#% 442945
#% 462323
#% 463407
#% 463585
#% 480110
#% 569233
#! Grahne et al. have presented a graph algorithm for evaluating a subset of recursive queries. This method consists of two phases. In the first phase, the method transforms a linear binary-chain program into a set of equations over expressions containing predicate symbols. In the second phase, a graph is constructed from the equations and the answers are produced by traversing the relevant paths. Here, we describe a new algorithm which requires less time than Grahne's. The key idea of the improvement is to reduce the search space that will be traversed when a query is invoked. Furthermore, we speed up the evaluation of cyclic data by generating most answers directly in terms of the answers already found and the associated 驴path information驴 instead of traversing the corresponding paths as usual. In this way, our algorithm achieves a linear time complexity for both acyclic and cyclic data.

#index 578394
#* Pushing Support Constraints Into Association Rules Mining
#@ Ke Wang;Yu He;Jiawei Han
#t 2003
#c 7
#% 152934
#% 201894
#% 220706
#% 227917
#% 227919
#% 248012
#% 248785
#% 248791
#% 280409
#% 280439
#% 280487
#% 300120
#% 479484
#% 479817
#% 480154
#% 481290
#% 481588
#% 481754
#% 481758
#% 632029
#! Interesting patterns often occur at varied levels of support. The classic association mining based on a uniform minimum support, such as Apriori, either misses interesting patterns of low support or suffers from the bottleneck of itemset generation caused by a low minimum support. A better solution lies in exploiting support constraints, which specify what minimum support is required for what itemsets, so that only the necessary itemsets are generated. In this paper, we present a framework of frequent itemset mining in the presence of support constraints. Our approach is to 驴push驴 support constraints into the Apriori itemset generation so that the 驴best驴 minimum support is determined for each itemset at runtime to preserve the essence of Apriori. This strategy is called Adapative Apriori.Experiments show that Adapative Apriori is highly effective in dealing with the bottleneck of itemset generation.

#index 578395
#* Delta Abstractions: A Technique for Managing Database States in Runtime Debugging of Active Database Rules
#@ Susan D. Urban;Taoufik Ben Abdellatif;Suzanne W. Dietrich;Amy Sundermier
#t 2003
#c 7
#% 43211
#% 45257
#% 116045
#% 210192
#% 226771
#% 240190
#% 240245
#% 282004
#% 394417
#% 459266
#% 462212
#% 463568
#% 501935
#% 501936
#% 501948
#% 501949
#% 536144
#% 565274
#% 585951
#% 586007
#% 710200
#! Delta abstractions are introduced as a mechanism for managing database states during the execution of active database rules. Delta abstractions build upon the use of object deltas, capturing changes to individual objects through a system-supported, collapsible type structure. The object delta structure is implemented using object-oriented concepts such as encapsulation and inheritance so that all database objects inherit the ability to transparently create and manage delta values. Delta abstractions provide an additional layer to the database programmer for organizing object deltas according to different language components that induce database changes, such as methods and active rules. As with object deltas, delta abstractions are transparently created and maintained by the active database system. We define different types of delta abstractions as views of object deltas and illustrate how the services of delta abstractions can be used to inspect the state of active rule execution. An active rule analysis and debugging tool has been implemented to demonstrate the use of object deltas and delta abstractions for dynamic analysis of active rules at runtime.

#index 578396
#* Mining Asynchronous Periodic Patterns in Time Series Data
#@ Jiong Yang;Wei Wang;Philip S. Yu
#t 2003
#c 7
#% 227924
#% 232122
#% 248791
#% 260014
#% 266248
#% 280408
#% 280482
#% 280487
#% 338728
#% 342642
#% 420063
#% 443195
#% 459006
#% 463903
#% 464839
#% 479490
#% 479627
#% 481290
#% 481611
#% 566132
#% 631920
#% 631923
#% 631926
#! Periodicy detection in time series data is a challenging problem of great importance in many applications. Most previous work focused on mining synchronous periodic patterns and did not recognize the misaligned presence of a pattern due to the intervention of random noise. In this paper, we propose a more flexible model of asynchronous periodic pattern that may be present only within a subsequence and whose occurrences may be shifted due to disturbance. Two parameters min_rep and max_dis are employed to specify the minimum number of repetitions that is required within each segment of nondisrupted pattern occurrences and the maximum allowed disturbance between any two successive valid segments. Upon satisfying these two requirements, the longest valid subsequence of a pattern is returned. A two-phase algorithm is devised to first generate potential periods by distance-based pruning followed by an iterative procedure to derive and validate candidate patterns and locate the longest valid subsequence. We also show that this algorithm cannot only provide linear time complexity with respect to the length of the sequence but also achieve space efficiency.

#index 578397
#* P-AutoClass: Scalable Parallel Clustering for Mining Large Data Sets
#@ Clara Pizzuti;Domenico Talia
#t 2003
#c 7
#% 36672
#% 189880
#% 210173
#% 232106
#% 232117
#% 248790
#% 248792
#% 252303
#% 296738
#% 382327
#% 434086
#% 434167
#% 438137
#% 444833
#% 481281
#% 509224
#% 556420
#% 633175
#! Data clustering is an important task in the area of data mining. Clustering is the unsupervised classification of data items into homogeneous groups called clusters. Clustering methods partition a set of data items into clusters, such that items in the same cluster are more similar to each other than items in different clusters according to some defined criteria. Clustering algorithms are computationally intensive, particularly when they are used to analyze large amounts of data. A possible approach to reduce the processing time is based on the implementation of clustering algorithms on scalable parallel computers. This paper describes the design and implementation of P-AutoClass, a parallel version of the AutoClass system based upon the Bayesian model for determining optimal classes in large data sets. The P-AutoClass implementation divides the clustering task among the processors of a multicomputer so that each processor works on its own partition and exchanges intermediate results with the other processors. The system architecture, its implementation, and experimental performance results on different processor numbers and data sets are presented and compared with theoretical performance. In particular, experimental and predicted scalability and efficiency of P-AutoClass versus the sequential AutoClass system are evaluated and compared.

#index 578398
#* Multidimensional Declustering Schemes Using Golden Ratio and Kronecker Sequences
#@ Chung-Min Chen;Randeep Bhatia;Rakesh K. Sinha
#t 2003
#c 7
#% 43179
#% 153400
#% 215403
#% 227856
#% 252608
#% 282523
#% 286962
#% 299983
#% 319327
#% 335453
#% 339622
#% 378390
#% 443374
#% 461922
#% 462233
#% 463598
#% 464718
#% 469603
#% 479936
#% 480794
#% 481109
#% 609704
#% 631955
#% 632069
#% 637794
#! We propose a new declustering scheme for allocating uniform multidimensional data among parallel disks. The scheme, aimed at reducing disk access time for range queries, is based on Golden Ratio Sequences for two dimensions and Kronecker Sequences for higher dimensions. Using exhaustive simulation, we show that, in two dimensions, the worst-case (additive) deviation of the scheme from the optimal response time for any range query is one when the number of disks (M) is at most 22; its worst-case deviation is two when M \leq 94; and its worst-case deviation is four when M \leq 550. In two dimensions, we prove that whenever M is a Fibonacci number, the average performance of the scheme is within 14 percent of the (generally, unachievable) strictly optimal scheme and its worst-case response time is within a multiplicative factor three of the optimal response time for any query, and within a factor 1.5 of the optimal for large queries. We also present comprehensive simulation results, on two-dimensional as well as on higher-dimensional data, that compare and demonstrate the advantages of our scheme over some recently proposed schemes in the literature.

#index 578399
#* CSVD: Clustering and Singular Value Decomposition for Approximate Similarity Search in High-Dimensional Spaces
#@ Vittorio Castelli;Alexander Thomasian;Chung-Sheng Li
#t 2003
#c 7
#% 8161
#% 201876
#% 201893
#% 227856
#% 227924
#% 237187
#% 252304
#% 272708
#% 300131
#% 381086
#% 480307
#% 588605
#% 631963
#% 1809055
#! Nearest-neighbor search of high-dimensionality spaces is critical for many applications, such as content-based retrieval from multimedia databases, similarity search of patterns in data mining, and nearest-neighbor classification. Unfortunately, even with the aid of the commonly used indexing schemes, the performance of nearest-neighbor (NN) queries deteriorates rapidly with the number of dimensions. We propose a method, called Clustering with Singular Value Decomposition (CSVD), which supports efficient approximate processing of NN queries, while maintaining good precision-recall characteristics. CSVD groups homogeneous points into clusters and separately reduces the dimensionality of each cluster using SVD. Cluster selection for NN queries relies on a branch-and-bound algorithm and within-cluster searches can be performed with traditional or in-memory indexing methods. Experiments with texture vectors extracted from satellite images show that CSVD achieves significantly higher dimensionality reduction than plain SVD for the same Normalized Mean Squared Error (NMSE), which translates into a higher efficiency in processing approximate NN queries.

#index 578400
#* Haar Wavelets for Efficient Similarity Search of Time-Series: With and Without Time Warping
#@ Franky Kin-Pong Chan;Ada Wai-chee Fu;Clement Yu
#t 2003
#c 7
#% 95721
#% 137711
#% 172949
#% 201876
#% 214595
#% 227857
#% 227924
#% 248798
#% 257637
#% 273919
#% 310545
#% 403487
#% 403753
#% 460862
#% 462231
#% 463903
#% 464196
#% 480146
#% 631923
#! We address the handling of time series search based on two important distance definitions: Euclidean distance and time warping distance. Conventional method reduces the dimensionality by means of Discrete Fourier Transform. We apply the Haar Wavelet Transform technique and propose the use of a proper normalization so that the method can guarantee no false dismissal for Euclidean distance. We found that this method has competitive performance from our experiments. Euclidean distance measurement cannot handle the time shifts of patterns. It fails to match the same rise and fall patterns of sequences with different scales. A distance measure that handles this problem is the time warping distance. However, the complexity of computing the time warping distance function is high. Also, as time warping distance is not a metric, most indexing techniques would not guarantee any false dismissal. We propose efficient strategies to mitigate the problems of time warping. We suggest a Haar wavelet-based approximation function for time warping distance, called Low Resolution Time Warping, which results in less computation by trading off a small amount of accuracy. We apply our approximation function to similarity search in time series databases, and show by experiment that it is highly effective in suppressing the number of false alarms in similarity search.

#index 578401
#* Persistently Cached B-Trees
#@ Kazuhiko Kato
#t 2003
#c 7
#% 8826
#% 55451
#% 58372
#% 59351
#% 132271
#% 193133
#% 252608
#% 286835
#% 317933
#% 384872
#% 442669
#! This paper presents an approach to enhancing B-tree indexing performance by using a replication technique called persistent caching. A notable feature of the approach is its compatibility with ordinary B-Trees; it exploits only the otherwise unused area of each B-Tree page, and the basic behavior of B-Trees need not be changed. This paper evaluates the performance of persistently cached B-trees by showing the result of mathematical analysis and of experimental investigations.

#index 578402
#* Asynchronous Operations in Distributed Concurrency Control
#@ P. Krishna Reddy;Subhash Bhalla
#t 2003
#c 7
#% 1747
#% 9241
#% 65353
#% 68264
#% 70077
#% 76977
#% 146204
#% 165296
#% 210179
#% 237196
#% 237197
#% 273894
#% 273951
#% 315669
#% 348466
#% 442990
#% 462657
#% 571055
#% 609725
#! Distributed locking is commonly adopted for performing concurrency control in distributed systems. It incorporates additional steps for handling deadlocks. This activity is carried out by methods based on wait-for-graphs or probes. The present study examines detection of conflicts based on enhanced local processing for distributed concurrency control. In the proposed 驴edge detection驴 approach, a graph-based resolution of access conflicts has been adopted. The technique generates a uniform wait-for precedence order at distributed sites for transactions to execute. The earlier methods based on serialization graph testing are difficult to implement in a distributed environment. The edge detection approach is a fully distributed approach. It presents a unified technique for locking and deadlock detection exercises. The technique eliminates many deadlocks without incurring message overheads.

#index 578403
#* Knowledge-Based Search in Competitive Domains
#@ Steven Walczak
#t 2003
#c 7
#% 4275
#% 21222
#% 29126
#% 42002
#% 69530
#% 109940
#% 114422
#% 116725
#% 329392
#% 373776
#% 402790
#% 406511
#% 694413
#% 802251
#! Artificial Intelligence programs operating in competitive domains typically use brute-force search if the domain can be modeled using a search tree or alternately use nonsearch heuristics as in production rule-based expert systems. While brute-force techniques have recently proven to be a viable method for modeling domains with smaller search spaces, such as checkers and chess, the same techniques cannot succeed in more complex domains, such as shogi or go. This research uses a cognitive-based modeling strategy to develop a heuristic search technique based on cognitive thought processes with minimal domain specific knowledge. The cognitive-based search technique provides a significant reduction in search space complexity and, furthermore, enables the search paradigms to be extended to domains that are not typically thought of as search domains such as aerial combat or corporate takeovers.

#index 578404
#* Efficient Algorithms for Large-Scale Temporal Aggregation
#@ Bongki Moon;Ines Fernando Vega Lopez;Vijaykumar Immanuel
#t 2003
#c 7
#% 70370
#% 135384
#% 209730
#% 223781
#% 238413
#% 252608
#% 296635
#% 333874
#% 369764
#% 384872
#% 427195
#% 427219
#% 452818
#% 465010
#% 479910
#% 481288
#% 565462
#% 631922
#% 709080
#! The ability to model time-varying natures is essential to many database applications such as data warehousing and mining. However, the temporal aspects provide many unique characteristics and challenges for query processing and optimization. Among the challenges is computing temporal aggregates, which is complicated by having to compute temporal grouping. In this paper, we introduce a variety of temporal aggregation algorithms that overcome major drawbacks of previous work. First, for small-scale aggregations, both the worst-case and average-case processing time have been improved significantly. Second, for large-scale aggregations, the proposed algorithms can deal with a database that is substantially larger than the size of available memory. Third, the parallel algorithm designed on a shared-nothing architecture achieves scalable performance by delivering nearly linear scale-up and speed-up, even at the presence of data skew. The contributions made in this paper are particularly important because the rate of increase in database size and response time requirements has out-paced advancements in processor and mass storage technology.

#index 578405
#* A Hybrid Model for the Prediction of the Linguistic Origin of Surnames
#@ Patrizia Bonaventura;Marco Gori;Marco Maggini;Franco Scarselli;Jianqing Sheng
#t 2003
#c 7
#% 10024
#% 92148
#% 168315
#% 236096
#% 740346
#! The prediction of the linguistic origin of surnames is a basic functionality required in the design of high-quality multilanguage speech synthesizers. The assignment of a given string representing a surname to a specific language is typically based on a set of rules which can hardly be written in an explicit form. The approach we propose faces this problem combining a rule-based system with a module based on evidential reasoning and a module based on neural networks. The resulting hybrid system combines the different sources of information, merging both knowledge from experts on linguistics and knowledge automatically acquired using learning from examples. The system has been validated on a large database containing surnames belonging to four different languages, showing its effectiveness for real-world applications.

#index 578406
#* From Association to Classification: Inference Using Weight of Evidence
#@ Yang Wang;Andrew K. C. Wong
#t 2003
#c 7
#% 84511
#% 136350
#% 227919
#% 232136
#% 443172
#% 449588
#% 452821
#% 693091
#! Association and classification are two important tasks in data mining and knowledge discovery. Intensive studies have been carried out in both areas. But, how to apply discovered event associations to classification is still seldom found in current publications. Trying to bridge this gap, this paper extends our previous paper on significant event association discovery to classification. We propose to use weight of evidence to evaluate the evidence of a significant event association in support of, or against, a certain class membership. Traditional weight of evidence in information theory is extended here to measure the event associations of different orders with respect to a certain class. After the discovery of significant event associations inherent in a data set, it is easy and efficient to apply the weight of evidence measure for classifying an observation according to any attribute. With this approach, we achieve flexible prediction.

#index 641954
#* A Polynomial Algorithm for Optimal Univariate Microaggregation
#@ Stephen Lee Hansen;Sumitra Mukherjee
#t 2003
#c 7
#! Microaggregation is a technique used by statistical agencies to limit disclosure of sensitive microdata. Noting that no polynomial algorithms are known to microaggregate optimally, Domingo-Ferrer and Mateo-Sanz have presented heuristic microaggregation methods. This paper is the first to present an efficient polynomial algorithm for optimal univariate microaggregation. Optimal partitions are shown to correspond to shortest paths in a network.

#index 641955
#* Stability Analysis of Regional and National Voting Schemes by a Continuous Model
#@ Liang Chen;Naoyuki Tokuda
#t 2003
#c 7
#! The previous discrete-model-based stability analysis of regional and national voting has been extended to a continuous-model-based analysis in the simultaneous presence of white and concentrated components of noise, reconfirming the previous conclusion that regional voting with smaller sized regions always demonstrates an improved stability over those with larger sized regions, including the national voting in its limiting case in particular. The conclusion remains valid as long as the weak distribution assumption is valid.

#index 641956
#* Web-Log Mining for Predictive Web Caching
#@ Qiang Yang;Haining Henry Zhang
#t 2003
#c 7
#! Caching is a well-known strategy for improving the performance of Web-based systems. The heart of a caching system is its page replacement policy, which selects the pages to be replaced in a cache when a request arrives. In this paper, we present a Web-log mining method for caching Web objects and use this algorithm to enhance the performance of Web caching systems. In our approach, we develop an n-gram-based prediction algorithm that can predict future Web requests. The prediction model is then used to extend the well-known GDSF caching policy. We empirically show that the system performance is improved using the predictive-caching approach.

#index 641957
#* Security of Tzeng's Time-Bound Key Assignment Scheme for Access Control in a Hierarchy
#@ Xun Yi;Yiming Ye
#t 2003
#c 7
#! Tzeng proposes a time-bound cryptographic key assignment scheme for access control in a partial-order hierarchy. In this paper, we show that Tzeng's scheme is insecure against the collusion attack whereby three users conspire to access some secret class keys that they should not know according to Tzeng's scheme.

#index 641958
#* The Subgraph Bisimulation Problem
#@ Agostino Dovier;Carla Piazza
#t 2003
#c 7
#! We study the complexity of the Subgraph Bisimulation Problem, which relates to Graph Bisimulation as Subgraph Isomorphism relates to Graph Isomorphism, and we prove its NP-Completeness. Our analysis is motivated by its applications to semistructured databases.

#index 641959
#* On-Demand Forecasting of Stock Prices Using a Real-Time Predictor
#@ Yi-Fan Wang
#t 2003
#c 7
#! This paper presents a fuzzy stochastic prediction method for real-time predicting of stock prices. A complete contrast to the crisp stochastic method, it requires a fuzzy linguistic summary approach to computing parameters. This approach, which is found to be better than the gray prediction method, can eliminate outliers and limit the data to a normal condition for prediction, with a comparatively very small deviation of 4.5 percent.

#index 641960
#* Image Retrieval Based on Regions of Interest
#@ Khanh Vu;Kien A. Hua;Wallapak Tavanapong
#t 2003
#c 7
#! Query-by-example is the most popular query model in recent content-based image retrieval (CBIR) systems. A typical query image includes relevant objects (e.g., Eiffel Tower), but also irrelevant image areas (including background). The irrelevant areas limit the effectiveness of existing CBIR systems. To overcome this limitation, the system must be able to determine similarity based on relevant regions alone. We call this class of queries region-of-interest (ROI) queries and propose a technique for processing them in a sampling-based matching framework. A new similarity model is presented and an indexing technique for this new environment is proposed. Our experimental results confirm that traditional approaches, such as Local Color Histogram and Correlogram, suffer from the involvement of irrelevant regions. Our method can handle ROI queries and provide significantly better performance. We also assessed the performance of the proposed indexing technique. The results clearly show that our retrieval procedure is effective for large image data sets.

#index 641961
#* Peculiarity Oriented Multidatabase Mining
#@ Ning Zhong;Yiyu (Y. Y. ) Yao;Muneaki Ohshima
#t 2003
#c 7
#! Peculiarity rules are a new class of rules which can be discovered by searching relevance among a relatively small number of peculiar data. Peculiarity oriented mining in multiple data sources is different from, and complementary to, existing approaches for discovering new, surprising, and interesting patterns hidden in data. A theoretical framework for peculiarity oriented mining is presented. Within the proposed framework, we give a formal interpretation and comparison of three classes of rules, namely, association rules, exception rules, and peculiarity rules, as well as describe how to mine interesting peculiarity rules in multiple databases.

#index 641962
#* On Agent-Mediated Electronic Commerce
#@ Minghua He;Nicholas R. Jennings;Ho-Fung Leung
#t 2003
#c 7
#! This paper surveys and analyzes the state of the art of agent-mediated electronic commerce (e-commerce), concentrating particularly on the business-to-consumer (B2C) and business-to-business (B2B) aspects. From the consumer buying behavior perspective, agents are being used in the following activities: need identification, product brokering, buyer coalition formation, merchant brokering, and negotiation. The roles of agents in B2B e-commerce are discussed through the business-to-business transaction model that identifies agents as being employed in partnership formation, brokering, and negotiation. Having identified the roles for agents in B2C and B2B e-commerce, some of the key underpinning technologies of this vision are highlighted. Finally, we conclude by discussing the future directions and potential impediments to the wide-scale adoption of agent-mediated e-commerce.

#index 641963
#* An Approach for Measuring Semantic Similarity between Words Using Multiple Information Sources
#@ Yuhua Li;Zuhair A. Bandar;David McLean
#t 2003
#c 7
#! Semantic similarity between words is becoming a generic problem for many applications of computational linguistics and artificial intelligence. This paper explores the determination of semantic similarity by a number of information sources, which consist of structural semantic information from a lexical taxonomy and information content from a corpus. To investigate how information sources could be used effectively, a variety of strategies for using various possible information sources are implemented. A new measure is then proposed which combines information sources nonlinearly. Experimental evaluation against a benchmark set of human similarity ratings demonstrates that the proposed measure significantly outperforms traditional similarity measures.

#index 641964
#* Buffer Queries
#@ Edward P. F. Chan
#t 2003
#c 7
#! A class of commonly asked queries in a spatial database is known as buffer queries. An example of such a query is to “find house-power line pairs that are within 50 meters of each other.” A buffer query involves two spatial data sets and a distance d. The answer to this query are pairs of objects, one from each input set, that are within distance d of each other. Given nonpoint spatial objects, evaluation of buffer queries could be a costly operation, even when the numbers of objects in the input data sets are relatively small. This paper addresses the problem of how to evaluate this class of queries efficiently. A fundamental problem with buffer query evaluation is to find an efficient algorithm for solving the minimum distance (minDist) problem for lines and regions. An efficient minDist algorithm, which only requires a subsequence of segments from each object to be examined, is derived. Finding a fast minDist algorithm is the first step in evaluating a buffer query efficiently. It is observed that many, and sometimes even most, candidates can be proven in the answer without resorting to the relatively expensive minDist operation. A candidate is first evaluated with a least expensive technique—called 0-object filtering. If it fails, a more costly operation, called 1-object filtering, is applied. Finally, if both filterings fail, the most expensive minDist algorithm is invoked. To show the effectiveness of the these techniques, they are incorporated into the well-known tree join algorithm and tested with real-life as well as artificial data sets. Extensive experiments show that the proposed algorithm outperforms existing techniques by a wide margin in both execution time as well as IO accesses. More importantly, the performance gain improves drastically with the increase of distance values.

#index 641965
#* Image Representations and Feature Selection for Multimedia Database Search
#@ Theodoros Evgeniou;Massimiliano Pontil;Constantine Papageorgiou;Tomaso Poggio
#t 2003
#c 7
#! The success of a multimedia information system depends heavily on the way the data is represented. Although there are “natural” ways to represent numerical data, it is not clear what is a good way to represent multimedia data, such as images, video, or sound. In this paper, we investigate various image representations where the quality of the representation is judged based on how well a system for searching through an image database can perform—although the same techniques and representations can be used for other types of object detection tasks or multimedia data analysis problems. The system is based on a machine learning method used to develop object detection models from example images that can subsequently be used for examples to detect—search—images of a particular object in an image database. As a base classifier for the detection task, we use support vector machines (SVM), a kernel-based learning method. Within the framework of kernel classifiers, we investigate new image representations/kernels derived from probabilistic models of the class of images considered and present a new feature selection method which can be used to reduce the dimensionality of the image representation without significant losses in terms of the performance of the detection-search-system.

#index 641966
#* Temporal Probabilistic Object Bases
#@ Veronica Biazzo;Rosalba Giugno;Thomas Lukasiewicz;V. S. Subrahmanian
#t 2003
#c 7
#! There are numerous applications where we have to deal with temporal uncertainty associated with objects. The ability to automatically store and manipulate time, probabilities, and objects is important. We propose a data model and algebra for temporal probabilistic object bases (TPOBs), which allows us to specify the probability with which an event occurs at a given time point. In explicit TPOB-instances, the sets of time points along with their probability intervals are explicitly enumerated. In implicit TPOB-instances, sets of time points are expressed by constraints and their probability intervals by probability distribution functions. Thus, implicit object base instances are succinct representations of explicit ones; they allow for an efficient implementation of algebraic operations, while their explicit counterparts make defining algebraic operations easy. We extend the relational algebra to both explicit and implicit instances and prove that the operations on implicit instances correctly implement their counterpart on explicit instances.

#index 641967
#* Progressive Partition Miner: An Efficient Algorithm for Mining General Temporal Association Rules
#@ Chang-Hung Lee;Ming-Syan Chen;Cheng-Ru Lin
#t 2003
#c 7
#! In this paper, we explore a new problem of mining general temporal association rules in publication databases. In essence, a publication database is a set of transactions where each transaction T is a set of items of which each item contains an individual exhibition period. The current model of association rule mining is not able to handle the publication database due to the following fundamental problems, i.e., 1) lack of consideration of the exhibition period of each individual item and 2) lack of an equitable support counting basis for each item. To remedy this, we propose an innovative algorithm Progressive-Partition-Miner (abbreviated as PPM) to discover general temporal association rules in a publication database. The basic idea of PPM is to first partition the publication database in light of exhibition periods of items and then progressively accumulate the occurrence count of each candidate 2\hbox{-}{\rm{itemset}} based on the intrinsic partitioning characteristics. Algorithm PPM is also designed to employ a filtering threshold in each partition to early prune out those cumulatively infrequent 2\hbox{-}{\rm{itemsets}}. The feature that the number of candidate 2\hbox{-}{\rm{itemsets}} generated by PPM is very close to the number of frequent 2\hbox{-}{\rm{itemsets}} allows us to employ the scan reduction technique to effectively reduce the number of database scans. Explicitly, the execution time of PPM is, in orders of magnitude, smaller than those required by other competitive schemes that are directly extended from existing methods. The correctness of PPM is proven and some of its theoretical properties are derived. Sensitivity analysis of various parameters is conducted to provide many insights into Algorithm PPM.

#index 641968
#* Effectively Finding Relevant Web Pages from Linkage Information
#@ Jingyu Hou;Yanchun Zhang
#t 2003
#c 7
#! This paper presents two hyperlink analysis-based algorithms to find relevant pages for a given Web page (URL). The first algorithm comes from the extended cocitation analysis of the Web pages. It is intuitive and easy to implement. The second one takes advantage of linear algebra theories to reveal deeper relationships among the Web pages and to identify relevant pages more precisely and effectively. The experimental results show the feasibility and effectiveness of the algorithms. These algorithms could be used for various Web applications, such as enhancing Web search. The ideas and techniques in this work would be helpful to other Web-related researches.

#index 641969
#* In-Place Reconstruction of Version Differences
#@ Randal Burns;Larry Stockmeyer;Darrell D. E. Long
#t 2003
#c 7
#! In-place reconstruction of differenced data allows information on devices with limited storage capacity to be updated efficiently over low-bandwidth channels. Differencing encodes a version of data compactly as a set of changes from a previous version. Transmitting updates to data as a version difference saves both time and bandwidth. In-place reconstruction rebuilds the new version of the data in the storage or memory the current version occupies—no scratch space is needed for a second version. By combining these technologies, we support highly mobile applications on space-constrained hardware. We present an algorithm that modifies a differentially encoded version to be in-place reconstructible. The algorithm trades a small amount of compression to achieve this property. Our treatment includes experimental results that show our implementation to be efficient in space and time and verify that compression losses are small. Also, we give results on the computational complexity of performing this modification while minimizing lost compression.

#index 641970
#* External Sorting: Run Formation Revisited
#@ Per-Ake Larson
#t 2003
#c 7
#! External mergesort begins with a run formation phase creating the initial sorted runs. Run formation can be done by a load-sort-store algorithm or by replacement selection. A load-sort-store algorithm repeatedly fills available memory with input records, sorts them, and writes the result to a run file. Replacement selection produces longer runs than load-sort-store algorithms and completely overlaps sorting and I/O, but it has poor locality of reference resulting in frequent cache misses and the classical algorithm works only for fixed-length records. This paper introduces batched replacement selection: a cache-conscious version of replacement selection that works also for variable-length records. The new algorithm resembles AlphaSort in the sense that it creates small in-memory runs and merges them to form the output runs. Its performance is experimentally compared with three other run formation algorithms: classical replacement selection, Quicksort, and AlphaSort. The experiments show that batched replacement selection is considerably faster than classic replacement selection. For small records (average 100 bytes), CPU time was reduced by about 50 percent and elapsed time by 47-63 percent. It was also consistently faster than Quicksort, but it did not always outperform AlphaSort. Replacement selection produces fewer runs than Quicksort and AlphaSort. The experiments confirmed that this reduces the merge time whereas the effect on the overall sort time depends on the number of disks available.

#index 641971
#* Applying Automatically Derived Gene-Groups to Automatically Predict and Refine Metabolic Pathways
#@ Arvind K. Bansal;Christopher J. Woolverton
#t 2003
#c 7
#! This paper describes an automated technique to predict integrated pathways and refine existing metabolic pathways using the information of automatically derived, functionally similar gene-groups and orthologs (functionally equivalent genes) derived by the comparison of complete microbial genomes archived in GenBank. The described method integrates automatically derived orthologous and homologous gene-groups (http://www.mcs.kent.edu/~arvind/orthos.html) with the biochemical pathway template available at the KEGG database (http://www.genome.ad.jp), the enzyme information derived from the SwissProt enzyme database (http://expasys.hcuge.ch/), and the Ligand database (http://www.genome.ad.jp). The technique refines existing pathways (based upon the network of reactions of enzymes) by associating corresponding nonenzymatic and regulatory proteins to enzymes and operons and by identifying substituting homologs. The technique is suitable for building and refining integrated pathways using evolutionary diverse organisms. A methodology and the corresponding algorithm are presented. The technique is illustrated by comparing the genomes of E. coli and B. subtilis with M. tuberculosis. The findings about integrated pathways are briefly discussed.

#index 641972
#* Using Optimistic Atomic Broadcast in Transaction Processing Systems
#@ Bettina Kemme;Fernando Pedone;Gustavo Alonso;Andre Schiper;Matthias Wiesmann
#t 2003
#c 7
#! Atomic broadcast primitives are often proposed as a mechanism to allow fault-tolerant cooperation between sites in a distributed system. Unfortunately, the delay incurred before a message can be delivered makes it difficult to implement high performance, scalable applications on top of atomic broadcast primitives. Recently, a new approach has been proposed for atomic broadcast which, based on optimistic assumptions about the communication system, reduces the average delay for message delivery to the application. In this paper, we develop this idea further and show how applications can take even more advantage of the optimistic assumption by overlapping the coordination phase of the atomic broadcast algorithm with the processing of delivered messages. In particular, we present a replicated database architecture that employs the new atomic broadcast primitive in such a way that communication and transaction processing are fully overlapped, providing high performance without relaxing transaction correctness.

#index 641973
#* Specifying and Enforcing Application-Level Web Security Policies
#@ David Scott;Richard Sharp
#t 2003
#c 7
#! Application-level Web security refers to vulnerabilities inherent in the code of a Web-application itself (irrespective of the technologies in which it is implemented or the security of the Web-server/back-end database on which it is built). In the last few months, application-level vulnerabilities have been exploited with serious consequences: Hackers have tricked e-commerce sites into shipping goods for no charge, usernames and passwords have been harvested, and confidential information (such as addresses and credit-card numbers) has been leaked. In this paper, we investigate new tools and techniques which address the problem of application-level Web security. We 1) describe a scalable structuring mechanism facilitating the abstraction of security policies from large Web-applications developed in heterogeneous multiplatform environments; 2) present a set of tools which assist programmers in developing secure applications which are resilient to a wide range of common attacks; and 3) report results and experience arising from our implementation of these techniques.

#index 641974
#* Searching with Numbers
#@ Rakesh Agrawal;Ramakrishnan Srikant
#t 2003
#c 7
#! A large fraction of the useful web is comprised of specification documents that largely consist of \langleattribute name, numeric value\rangle pairs embedded in text. Examples include product information, classified advertisements, resumes, etc. The approach taken in the past to search these documents by first establishing correspondences between values and their names has achieved limited success because of the difficulty of extracting this information from free text. We propose a new approach that does not require this correspondence to be accurately established. Provided the data has “low reflectivity”, we can do effective search even if the values in the data have not been assigned attribute names and the user has omitted attribute names in the query. We give algorithms and indexing structures for implementing the search. We also show how hints (i.e., imprecise, partial correspondences) from automatic data extraction techniques can be incorporated into our approach for better accuracy on high reflectivity data sets. Finally, we validate our approach by showing that we get high precision in our answers on real data sets from a variety of domains.

#index 641975
#* Managing and Sharing Servents' Reputations in P2P Systems
#@ Ernesto Damiani;Sabrina De Capitani di Vimercati;Stefano Paraboschi;Pierangela Samarati
#t 2003
#c 7
#! Peer-to-peer information sharing environments are increasingly gaining acceptance on the Internet as they provide an infrastructure in which the desired information can be located and downloaded while preserving the anonymity of both requestors and providers. As recent experience with P2P environments such as Gnutella shows, anonymity opens the door to possible misuses and abuses by resource providers exploiting the network as a way to spread tampered-with resources, including malicious programs, such as Trojan Horses and viruses. In this paper, we propose an approach to P2P security where servents can keep track, and share with others, information about the reputation of their peers. Reputation sharing is based on a distributed polling algorithm by which resource requestors can assess the reliability of perspective providers before initiating the download. The approach complements existing P2P protocols and has a limited impact on current implementations. Furthermore, it keeps the current level of anonymity of requestors and providers, as well as that of the parties sharing their view on others' reputations.

#index 641976
#* Query Expansion by Mining User Logs
#@ Hang Cui;Ji-Rong Wen;Jian-Yun Nie;Wei-Ying Ma
#t 2003
#c 7
#! Queries to search engines on the Web are usually short. They do not provide sufficient information for an effective selection of relevant documents. Previous research has proposed the utilization of query expansion to deal with this problem. However, expansion terms are usually determined on term co-occurrences within documents. In this study, we propose a new method for query expansion based on user interactions recorded in user logs. The central idea is to extract correlations between query terms and document terms by analyzing user logs. These correlations are then used to select high-quality expansion terms for new queries. Compared to previous query expansion methods, ours takes advantage of the user judgments implied in user logs. The experimental results show that the log-based query expansion method can produce much better results than both the classical search method and the other query expansion methods.

#index 641977
#* Scalable Consistency Maintenance in Content Distribution Networks Using Cooperative Leases
#@ Anoop George Ninan;Purushottam Kulkarni;Prashant Shenoy;Krithi Ramamritham;Renu Tewari
#t 2003
#c 7
#! In this paper, we argue that cache consistency mechanisms designed for stand-alone proxies do not scale to the large number of proxies in a content distribution network and are not flexible enough to allow consistency guarantees to be tailored to object needs. To meet the twin challenges of scalability and flexibility, we introduce the notion of cooperative consistency along with a mechanism, called cooperative leases, to achieve it. By supporting \Delta{\hbox{-}}{\rm{consistency}} semantics and by using a single lease for multiple proxies, cooperative leases allow the notion of leases to be applied in a flexible, scalable manner to CDNs. Further, the approach employs application-level multicast to propagate server notifications to proxies in a scalable manner. We implement our approach in the Apache Web server and the Squid proxy cache and demonstrate its efficacy using a detailed experimental evaluation. Our results show a factor of 2.5 reduction in server message overhead and a 20 percent reduction in server state space overhead when compared to original leases albeit at an increased interproxy communication overhead.

#index 641978
#* The Yin/Yang Web: A Unified Model for XML Syntax and RDF Semantics
#@ Peter F. Patel-Schneider;Jerome Simeon
#t 2003
#c 7
#! XML is the W3C standard document format for writing and exchanging information on the Web. RDF is the W3C standard model for describing the semantics and reasoning about information on the Web. Unfortunately, RDF and XML—although very close to each other—are based on two different paradigms. We argue that, in order to lead the Semantic Web to its full potential, the syntax and the semantics of information need to work together. To this end, we develop a model theory for the XML XQuery 1.0 and XPath 2.0 Data Model, which provides a unified model for both XML and RDF. This unified model can serve as the basis for Web applications that deal with both data and semantics. We illustrate the use of this model on a concrete information integration scenario. Our approach enables each side of the fence to benefit from the other, notably, we show how the RDF world can take advantage of XML Schema description and XML query languages, and how the XML world can take advantage of the reasoning capabilities available for RDF. Our approach can also serve as a foundation for the next layer of the Semantic Web, the ontology layer, and we present a layering of an ontology language on top of our approach.

#index 641979
#* Topic-Sensitive PageRank: A Context-Sensitive Ranking Algorithm for Web Search
#@ Taher H. Haveliwala
#t 2003
#c 7
#! The original PageRank algorithm for improving the ranking of search-query results computes a single vector, using the link structure of the Web, to capture the relative “importance” of Web pages, independent of any particular search query. To yield more accurate search results, we propose computing a set of PageRank vectors, biased using a set of representative topics, to capture more accurately the notion of importance with respect to a particular topic. For ordinary keyword search queries, we compute the topic-sensitive PageRank scores for pages satisfying the query using the topic of the query keywords. For searches done in context (e.g., when the search query is performed by highlighting words in a Web page), we compute the topic-sensitive PageRank scores using the topic of the context in which the query appeared. By using linear combinations of these (precomputed) biased PageRank vectors to generate context-specific importance scores for pages at query time, we show that we can generate more accurate rankings than with a single, generic PageRank vector. We describe techniques for efficiently implementing a large-scale search system based on the topic-sensitive PageRank scheme.

#index 721132
#* Using Hybrid Knowledge Engineering and Image Processing in Color Virtual Restoration of Ancient Murals
#@ Baogang Wei;Yonghuai Liu;Yunhe Pan
#t 2003
#c 7
#% 434949
#% 1378228
#% 1854941
#! This paper proposes a novel scheme to virtually restore the colors of ancient murals. Our approach integrates artificial intelligence techniques with digital image processing methods. The knowledge related to the mural colors is first categorized into four types. A hybrid frame and rule-based approach is then developed to represent knowledge and to infer colors. An algorithm that takes into account color similarity and spatial proximity is developed to segment mural images. A novel color transformation method based on color histograms is finally proposed to restore the colors of murals. A number of experiments based on real images have demonstrated the validity of the proposed scheme for color restoration.

#index 721133
#* IFOOD: An Intelligent Fuzzy Object-Oriented Database Architecture
#@ Murat Koyuncu;Adnan Yazici
#t 2003
#c 7
#% 95221
#% 108510
#% 115008
#% 116546
#% 125595
#% 182670
#% 226771
#% 256785
#% 262261
#% 265768
#% 279819
#% 287313
#% 290483
#% 292029
#% 297820
#% 357240
#% 359475
#% 379326
#% 383684
#% 386381
#% 386518
#% 419750
#% 443038
#% 443197
#% 452754
#% 1787863
#% 1788062
#% 1788921
#! Next generation information system applications require powerful and intelligent information management that necessitates an efficient interaction between database and knowledge base technologies. It is also important for these applications to incorporate uncertainty in data objects, in integrity constraints, and/or in application. In this study, we propose an intelligent object-oriented database architecture, IFOOD, which permits the flexible modeling and querying of complex data and knowledge including uncertainty with powerful retrieval capability.

#index 721134
#* Image Content-Based Retrieval Using Chromaticity Moments
#@ George Paschos;Ivan Radev;Nagarajan Prabakar
#t 2003
#c 7
#% 120270
#% 192795
#% 213673
#% 219845
#% 256656
#% 277607
#% 437407
#% 443896
#% 592433
#% 626570
#% 649937
#% 1346975
#% 1819293
#% 1854560
#! A number of different approaches have been recently presented for image retrieval using color features. Most of these methods use the color histogram or some variation of it. If the extracted information is to be stored for each image, such methods may require a significant amount of space for storing the histogram, depending on a given image's size and content. In the method proposed in this paper, only a small number of features, called chromaticity moments, are required to capture the spectral content (chrominance) of an image. The proposed method is based on the concept of the chromaticity diagram and extracts a set of two-dimensional moments from it to characterize the shape and distribution of chromaticities of the given image. This representation is compact (only a few chromaticity moments per image are required) and constant (independent of image size and content), while its retrieval effectiveness is comparable to using the full chromaticity histogram.

#index 721135
#* InterBase-KB: Integrating a Knowledge Base System with a Multidatabase System for Data Warehousing
#@ Nick Bassiliades;Ioannis Vlahavas;Ahmed K. Elmagarmid;Elias N. Houstis
#t 2003
#c 7
#% 22948
#% 36683
#% 82465
#% 102748
#% 111913
#% 115563
#% 122913
#% 152928
#% 158200
#% 170898
#% 189769
#% 194983
#% 201928
#% 223781
#% 227947
#% 252426
#% 279100
#% 284470
#% 340301
#% 437126
#% 443232
#% 443377
#% 462208
#% 462619
#% 480623
#% 480779
#% 562139
#% 565454
#% 571097
#% 614579
#% 631968
#! This paper describes the integration of a multidatabase system and a knowledge-base system to support the data-integration component of a Data Warehouse. The multidatabase system integrates various component databases with a common query language; however, it does not provide capability for schema integration and other utilities necessary for Data Warehousing. In addition, the knowledge base system offers a declarative logic language with second-order syntax but first-order semantics for integrating the schemes of the data sources into the warehouse and for defining complex, recursively defined materialized views. Furthermore, deductive rules are also used for cleaning, checking the integrity and summarizing the data imported into the Data Warehouse. The Knowledge Base System features an efficient incremental view maintenance mechanism that is used for refreshing the Data Warehouse, without querying the data sources.

#index 721136
#* Efficient Causality-Tracking Timestamping
#@ Jean-Michel Hélary;Michel Raynal;Giovanna Melideo;Roberto Baldoni
#t 2003
#c 7
#% 1823
#% 7563
#% 25958
#% 99833
#% 104402
#% 114559
#% 121506
#% 192611
#% 202146
#% 243175
#% 320187
#% 360714
#% 787244
#% 807854
#% 1113174
#! Vector clocks are the appropriate mechanism used to track causality among the events produced by a distributed computation. Traditional implementations of vector clocks require application messages to piggyback a vector of nintegers (where n is the number of processes). This paper investigates the tracking of the causality relation on a subset of events (namely, the events that are defined as "relevant" from the application point of view) in a context where communication channels are not required to be FIFO, and where there is no a priori information on the connectivity of the communication graph or the communication pattern. More specifically, the paper proposes a suite of simple and efficient implementations of vector clocks that address the reduction of the size of message timestamps, i.e., they do their best to have message timestamps whose size is less than n. The relevance of such a suite of protocols is twofold. From a practical side, it constitutes the core of an adaptive timestamping software layer that can used by underlying applications. From a theoretical side, it provides a comprehensive view that helps better understand distributed causality-tracking mechanisms.

#index 721137
#* Efficient Biased Sampling for Approximate Clustering and Outlier Detection in Large Data Sets
#@ George Kollios;Dimitrios Gunopulos;Nick Koudas;Stefan Berchtold
#t 2003
#c 7
#% 1331
#% 34077
#% 82346
#% 116084
#% 210173
#% 248790
#% 248792
#% 248812
#% 248821
#% 248822
#% 259995
#% 271236
#% 271237
#% 273887
#% 273903
#% 273906
#% 273907
#% 273908
#% 273909
#% 300132
#% 300136
#% 300183
#% 300193
#% 479791
#% 479986
#% 481281
#% 481779
#% 482092
#% 503535
#% 566132
#% 593960
#% 617839
#! We investigate the use of biased sampling according to the density of the data set to speed up the operation of general data mining tasks, such as clustering and outlier detection in large multidimensional data sets. In density-biased sampling, the probability that a given point will be included in the sample depends on the local density of the data set. We propose a general technique for density-biased sampling that can factor in user requirements to sample for properties of interest and can be tuned for specific data mining tasks. This allows great flexibility and improved accuracy of the results over simple random sampling. We describe our approach in detail, we analytically evaluate it, and show how it can be optimized for approximate clustering and outlier detection. Finally, we present a thorough experimental evaluation of the proposed method, applying density-biased sampling on real and synthetic data sets, and employing clustering and outlier detection algorithms, thus highlighting the utility of our approach.

#index 721138
#* Atomic Broadcast in Asynchronous Crash-Recovery Distributed Systems and Its Use in Quorum-Based Replication
#@ Luís Rodrigues;Michel Raynal
#t 2003
#c 7
#% 9241
#% 65723
#% 124019
#% 158913
#% 193149
#% 204399
#% 204908
#% 233573
#% 251359
#% 272227
#% 272228
#% 320189
#% 394812
#% 403195
#% 437688
#% 444079
#% 508197
#% 531811
#% 546049
#% 602675
#% 617352
#% 631158
#% 635940
#% 787232
#! Atomic Broadcast is a fundamental problem of distributed systems: It states that messages must be delivered in the same order to their destination processes. This paper describes a solution to this problem in asynchronous distributed systems in which processes can crash and recover. A Consensus-based solution to Atomic Broadcast problem has been designed by Chandra and Toueg for asynchronous distributed systems where crashed processes do not recover. We extend this approach: It transforms any Consensus protocol suited to the crash-recovery model into an Atomic Broadcast protocol suited to the same model. We show that Atomic Broadcast can be implemented requiring few additional log operations in excess of those required by the Consensus. The paper also discusses how additional log operations can improve the protocol in terms of faster recovery and better throughput. To illustrate the use of the protocol, the paper also describes a solution to the replica management problem in asynchronous distributed systems in which processes can crash and recover. The proposed technique makes a bridge between established results on Weighted Voting and recent results on the Consensus problem.

#index 721139
#* A Data Mining Algorithm for Generalized Web Prefetching
#@ Alexandros Nanopoulos;Dimitrios Katsaros;Yannis Manolopoulos
#t 2003
#c 7
#% 152939
#% 202140
#% 211739
#% 249968
#% 250201
#% 256882
#% 271420
#% 309777
#% 316562
#% 337714
#% 340296
#% 420063
#% 443194
#% 443262
#% 443294
#% 443296
#% 463903
#% 464228
#% 479471
#% 481290
#% 571038
#% 571039
#% 611156
#% 963897
#% 963898
#% 978362
#% 978378
#% 1303525
#! Predictive Web prefetching refers to the mechanism of deducing the forthcoming page accesses of a client based on its past accesses. In this paper, we present a new context for the interpretation of Web prefetching algorithms as Markov predictors. We identify the factors that affect the performance of Web prefetching algorithms. We propose a new algorithm called WM o, which is based on data mining and is proven to be a generalization of existing ones. It was designed to address their specific limitations and its characteristics include all the above factors. It compares favorably with previously proposed algorithms. Further, the algorithm efficiently addresses the increased number of candidates. We present a detailed performance evaluation of WM o with synthetic and real data. The experimental results show that WM o can provide significant improvements over previously proposed Web prefetching algorithms.

#index 721140
#* Logical Structure Analysis and Generation for Structured Documents: A Syntactic Approach
#@ Kyong-Ho Lee;Yoon-Chul Choy;Sung-Bae Cho
#t 2003
#c 7
#% 3888
#% 54505
#% 66654
#% 114261
#% 126613
#% 131580
#% 131587
#% 152835
#% 181919
#% 232493
#% 266736
#% 296377
#% 318126
#% 443685
#% 493646
#% 534040
#% 584919
#% 625395
#% 629590
#% 706937
#% 1818528
#! This paper presents a syntactic method for sophisticated logical structure analysis that transforms document images with multiple pages and hierarchical structure into an electronic document based on SGML/XML. To produce a logical structure more accurately and quickly than previous works of which the basic units are text lines, the proposed parsing method takes text regions with hierarchical structure as input. Furthermore, we define a document model that is able to describe geometric characteristics and logical structure information of documents efficiently and present its automated creation method. Experimental results with 372 images scanned from the IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) show that the method has performed logical structure analysis successfully and generated a document model automatically. Particularly, the method generates SGML/XML documents as the result of structural analysis, so that it enhances the reusability of documents and independence of platform.

#index 721141
#* A Scalable Low-Latency Cache Invalidation Strategy for Mobile Environments
#@ Guohong Cao
#t 2003
#c 7
#% 86477
#% 100060
#% 172874
#% 172876
#% 201869
#% 211739
#% 245014
#% 259634
#% 274199
#% 279165
#% 324163
#% 328422
#% 360716
#% 399548
#% 419606
#% 437181
#% 443127
#% 464214
#% 481777
#% 482107
#% 566124
#% 632050
#% 635900
#! Caching frequently accessed data items on the client side is an effective technique for improving performance in a mobile environment. Classical cache invalidation strategies are not suitable for mobile environments due to frequent disconnections and mobility of the clients. One attractive cache invalidation technique is based on invalidation reports (IRs). However, the IR-based cache invalidation solution has two major drawbacks, which have not been addressed in previous research. First, there is a long query latency associated with this solution since a client cannot answer the query until the next IR interval. Second, when the server updates a hot data item, all clients have to query the server and get the data from the server separately, which wastes a large amount of bandwidth. In this paper, we propose an IR-based cache invalidation algorithm, which can significantly reduce the query latency and efficiently utilize the broadcast bandwidth. Detailed analytical analysis and simulation experiments are carried out to evaluate the proposed methodology. Compared to previous IR-based schemes, our scheme can significantly improve the throughput and reduce the query latency, the number of uplink request, and the broadcast bandwidth requirements.

#index 721142
#* Epidemic Algorithms for Replicated Databases
#@ JoAnne Holliday;Robert Steinke;Divyakant Agrawal;Amr El Abbadi
#t 2003
#c 7
#% 7563
#% 8194
#% 35764
#% 64147
#% 70068
#% 124019
#% 194935
#% 210179
#% 213978
#% 233499
#% 235084
#% 237196
#% 237197
#% 240016
#% 248825
#% 271217
#% 273894
#% 286244
#% 286836
#% 318258
#% 320187
#% 442722
#% 444142
#% 458546
#% 602675
#% 602842
#% 682245
#% 682291
#% 778539
#! We present a family of epidemic algorithms for maintaining replicated database systems. The algorithms are based on the causal delivery of log records where each record corresponds to one transaction instead of one operation. The first algorithm in this family is a pessimistic protocol that ensures serializability and guarantees strict executions. Since we expect the epidemic algorithms to be used in environments with low probability of conflicts among transactions, we develop a variant of the pessimistic algorithm which is optimistic in that transactions commit as soon as they terminate locally and inconsistencies are detected asynchronously as the effects of committed transactions propagate through the system. The last member of the family of epidemic algorithms is pessimistic and uses voting with quorums to resolve conflicts and improve transaction response time. A simulation study evaluates the performance of the protocols.

#index 721143
#* ClusterTree: Integration of Cluster Representation and Nearest-Neighbor Search for Large Data Sets with High Dimensions
#@ Dantong Yu;Aidong Zhang
#t 2003
#c 7
#% 70370
#% 86950
#% 153260
#% 201876
#% 212690
#% 227939
#% 248790
#% 248796
#% 248797
#% 252400
#% 261855
#% 273891
#% 280452
#% 318703
#% 411694
#% 464195
#% 479462
#% 479799
#% 481281
#% 481956
#% 570889
#% 631963
#% 712780
#! In this paper, we introduce the ClusterTree, a new indexing approach to representing clusters generated by any existing clustering approach. A cluster is decomposed into several subclusters and represented as the union of the subclusters. The subclusters can be further decomposed, which isolates the most related groups within the clusters. A ClusterTree is a hierarchy of clusters and subclusters which incorporates the cluster representation into the index structure to achieve effective and efficient retrieval. Our cluster representation is highly adaptive to any kind of cluster. It is well accepted that most existing indexing techniques degrade rapidly as the dimensions increase. The ClusterTree provides a practical solution to index clustered data sets and supports the retrieval of the nearest-neighbors effectively without having to linearly scan the high-dimensional data set. We also discuss an approach to dynamically reconstruct the ClusterTree when new data is added. We present the detailed analysis of this approach and justify it extensively with experiments.

#index 721144
#* Detection and Recovery Techniques for Database Corruption
#@ Philip Bohannon;Rajeev Rastogi;S. Seshadri;Avi Silberschatz;S. Sudarshan
#t 2003
#c 7
#% 68145
#% 114582
#% 116063
#% 151540
#% 202153
#% 268755
#% 340292
#% 376305
#% 403195
#% 422875
#% 427195
#% 480617
#% 480959
#% 481268
#% 481454
#% 482097
#% 674735
#! Increasingly, for extensibility and performance, special purpose application code is being integrated with database system code. Such application code has direct access to database system buffers, and as a result, the danger of data being corrupted due to inadvertent application writes is increased. Previously proposed hardware techniques to protect from corruption require system calls, and their performance depends on details of the hardware architecture. We investigate an alternative approach which uses codewords associated with regions of data to detect corruption and to prevent corrupted data from being used by subsequent transactions. We develop several such techniques which vary in the level of protection, space overhead, performance, and impact on concurrency. These techniques are implemented in the Dalí main-memory storage manager, and the performance impact of each on normal processing is evaluated. Novel techniques are developed to recover when a transaction has read corrupted data caused by a bad write and gone on to write other data in the database. These techniques use limited and relatively low-cost logging of transaction reads to trace the corruption and may also prove useful when resolving problems caused by incorrect data entry and other logical errors.

#index 721145
#* A Comparison of Standard Spell Checking Algorithms and a Novel Binary Neural Approach
#@ Victoria J. Hodge;Jim Austin
#t 2003
#c 7
#% 85590
#% 120649
#% 131061
#% 219033
#% 253521
#% 324015
#% 334549
#! In this paper, we propose a simple, flexible, and efficient hybrid spell checking methodology based upon phonetic matching, supervised learning, and associative matching in the AURA neural system. We integrate Hamming Distance and n-gram algorithms that have high recall for typing errors and a phonetic spell-checking algorithm in a single novel architecture. Our approach is suitable for any spell checking application though aimed toward isolated word error correction, particularly spell checking user queries in a search engine. We use a novel scoring scheme to integrate the retrieved words from each spelling approach and calculate an overall score for each matched word. From the overall scores, we can rank the possible matches. In this paper, we evaluate our approach against several benchmark spellchecking algorithms for recall accuracy. Our proposed hybrid methodology has the highest recall rate of the techniques evaluated. The method has a high recall rate and low-computational cost.

#index 721146
#* Cascade of Distributed and Cooperating Firewalls in a Secure Data Network
#@ Robert N. Smith;Yu Chen;Sourav Bhattacharya
#t 2003
#c 7
#% 97120
#% 172394
#% 180949
#% 228355
#% 357738
#% 380294
#% 381898
#% 442946
#% 443113
#% 594393
#! Security issues are critical in networked information systems, e.g., with financial information, corporate proprietary information, contractual and legal information, human resource data, medical records, etc. The theme of this paper is to address such diversity of security needs among the different information and resources connected over a secure data network. Installation of firewalls across the data network is a popular approach to providing a secure data network. However, single, individual firewalls may not provide adequate security protection to meet the user's needs. The cost of super firewalls, design flaws, as well as implementation inappropriateness with such firewalls may retain security loopholes. Toward this, the idea proposed in this paper is to introduce a cascade of (potentially simpler and less expensive) firewalls in the secure data network驴where, between the attacker node and the attacked node, multiple firewalls are expected to provide an added degree of protection. This approach, broadly following the theme of redundancy in Engineering Systems' Design, will increase the confidence and provide more completeness in the level of security protection by the firewalls. The cascade of (i.e., multiple) firewalls can be placed across the secure data network in many ways, not all of which are equally attractive from cost and end-to-end delay perspectives. Toward this, we present heuristics for placement of these firewalls across the different nodes and links of the network in a way that different users can have the level of security they individually need, without having to pay added hardware costs or excess network delay. Three metrics are proposed to evaluate these heuristics: cost, delay, and reduction of attacker's traffic. Performance of these heuristics is presented using simulation, along with some early analytical results. Our research also extends the firewall technology into the well-known advantages of distributed firewalls. Furthermore, the distributed firewalls can be designed to cooperate and stop an attacker's traffic closest to the attack point驴thereby reducing the amount of hacker's traffic into the network.

#index 721147
#* Scalable Semantic Brokering over Dynamic Heterogeneous Data Sources in InfoSleuth"
#@ Marian (Misty) Nodine;Anne Hee Hiong Ngu;Anthony Cassandra;William G. Bohrer
#t 2003
#c 7
#% 85086
#% 188853
#% 213437
#% 213445
#% 227886
#% 227992
#% 229827
#% 252315
#% 278443
#% 278604
#% 481923
#% 489367
#% 631868
#! InfoSleuth is an agent-based system for information discovery and retrieval in a dynamic, open environment. Brokering in InfoSleuth is a matchmaking process, recommending agents that provide services to agents requesting services. This paper discusses InfoSleuth's distributed multibroker design and implementation. InfoSleuth's brokering function combines reasoning over both the syntax and semantics of agents in the domain. This means the broker must reason over explicitly advertised information about agent capabilities to determine which agent can best provide the requested services. Robustness and scalability issues dictate that brokering must be distributable across collaborating agents. Our multibroker design is a peer-to-peer system that requires brokers to advertise to and receive advertisements from other brokers. Brokers collaborate during matchmaking to give a collective response to requests initiated by nonbroker agents. This results in a robust, scalable brokering system.

#index 721148
#* Data Management Challenges and Development for Military Information Systems
#@ Marion G. Ceruti
#t 2003
#c 7
#% 156337
#% 266237
#% 278426
#% 278604
#% 278610
#% 278619
#% 356270
#% 359833
#% 482555
#% 622939
#! This paper explores challenges facing information system professionals in the management of data and knowledge in the Department of Defense (DOD), particularly in the information systems utilized to support Command, Control, Communications, Computers, and Intelligence (C4I). These information systems include operational tactical systems, decision-support systems, modeling and simulation systems, and nontactical business systems, all of which affect the design, operation, interoperation, and application of C4I systems. Specific topics include issues in integration and interoperability, joint standards, data access, data aggregation, information system component reuse, and legacy systems. Broad technological trends, as well as the use of specific developing technologies are discussed in light of how they may enable the DOD to meet the present and future information-management challenges.

#index 721149
#* An Approach for Modeling and Analysis of Security System Architectures
#@ Yi Deng;Jiacun Wang;Jeffrey J. P. Tsai;Konstantin Beznosov
#t 2003
#c 7
#% 43262
#% 68144
#% 82391
#% 116185
#% 157392
#% 167468
#% 198189
#% 204453
#% 243345
#% 258396
#% 259304
#% 260112
#% 262278
#% 274303
#% 278976
#% 279321
#% 286490
#% 291104
#% 370029
#% 383495
#% 417921
#% 442946
#% 543827
#% 568448
#% 583772
#% 583776
#% 583782
#% 978370
#! Security system architecture governs the composition of components in security systems and interactions between them. It plays a central role in the design of software security systems that ensure secure access to distributed resources in networked environment. In particular, the composition of the systems must consistently assure security policies that it is supposed to enforce. However, there is currently no rigorous and systematic way to predict and assure such critical properties in security system design. In this paper, a systematic approach is introduced to address the problem. We present a methodology for modeling security system architecture and for verifying whether required security constraints are assured by the composition of the components. We introduce the concept of security constraint patterns, which formally specify the generic form of security policies that all implementations of the system architecture must enforce. The analysis of the architecture is driven by the propagation of the global security constraints onto the components in an incremental process. We show that our methodology is both flexible and scalable. It is argued that such a methodology not only ensures the integrity of critical early design decisions, but also provides a framework to guide correct implementations of the design. We demonstrate the methodology through a case study in which we model and analyze the architecture of the Resource Access Decision (RAD) Facility, an OMG standard for application-level authorization service.

#index 721150
#* Adaptive Leases: A Strong Consistency Mechanism for the World Wide Web
#@ Venkata Duvvuri;Prashant Shenoy;Renu Tewari
#t 2003
#c 7
#% 65498
#% 236763
#% 256882
#% 283823
#% 291640
#% 424281
#% 443295
#% 635804
#% 661477
#% 661507
#% 677513
#% 677545
#% 963887
#% 979356
#! In this paper, we argue that weak cache consistency mechanisms supported by existing Web proxy caches must be augmented by strong consistency mechanisms to support the growing diversity in application requirements. Existing strong consistency mechanisms are not appealing for Web environments due to their large state space or control message overhead. We focus on the lease approach that balances these trade-offs and present analytical models and policies for determining the optimal lease duration. We present extensions to the HTTP protocol to incorporate leases and, then, implement our techniques in the Squid proxy cache and the Apache Web server. Our experimental evaluation of the leases approach shows that 1) our techniques impose modest overheads even for long leases (a lease duration of 1 hour requires state to be maintained for 1,030 leases and imposes an per-object overhead of a control message every 33 minutes), 2) leases yields a 138-425 percent improvement over existing strong consistency mechanisms, and 3) the implementation overhead of leases is comparable to existing weak consistency mechanisms.

#index 721151
#* Reasoning about Uniqueness Constraints in Object Relational Databases
#@ Vitaliy L. Khizder;Grant E. Weddell
#t 2003
#c 7
#% 17870
#% 57521
#% 62318
#% 69283
#% 91073
#% 114579
#% 117899
#% 137867
#% 166208
#% 175746
#% 179616
#% 188853
#% 210169
#% 286860
#% 287295
#% 287336
#% 287677
#% 374001
#% 411568
#% 415979
#% 442879
#% 442977
#% 459291
#% 461605
#% 462510
#% 711915
#% 755572
#% 1289168
#! Uniqueness constraints such as keys and functional dependencies in the relational model are a core concept in information systems technology. In this paper, we consider uniqueness constraints suitable for object relational data models and identify a boundary between tractable and intractable varieties. The subclass that is tractable is still a strict generalization of both keys and relational functional dependencies. We present an efficient decision procedure for the logical implication problem of this subclass. The problem itself is formulated as an implication problem for a simple dialect of description logic (DL). DLs are a family of languages for knowledge representation that have many applications in information systems technology and for which model building procedures have been developed that can decide implication problems for dialects that are very expressive. Our own procedure complements this approach and can be integrated with these earlier procedures. Finally, to motivate our results, we review some applications of our procedure in query optimization.

#index 727656
#* Forecasting Association Rules Using Existing Data Sets
#@ Sam Y. Sung;Zhao Li;Chew L. Tan;Peter A. Ng
#t 2003
#c 7
#% 152934
#% 172386
#% 199538
#% 201894
#% 210160
#% 213977
#% 227917
#% 227919
#% 227953
#% 248785
#% 248786
#% 248813
#% 340290
#% 420059
#% 443091
#% 464839
#% 479484
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 481918
#% 481949
#% 661048
#! An important issue that needs to be addressed when using data mining tools is the validity of the rules outside of the data set from which they are generated. Rules are typically derived from the patterns in a particular data set. When a new situation occurs, the change in the set of rules obtained from the new data set could be significant. In this paper, we provide a novel model for understanding how the differences between two situations affect the changes of the rules, based on the concept of fine partitioned groups that we call caucuses. Using this model, we provide a simple technique called Combination Data Set, to get a good estimate of the set of rules for a new situation. Our approach works independently of the core mining process and it can be easily implemented with all variations of rule mining techniques. Through experiments with real-life and synthetic data sets, we show the effectiveness of our technique in finding the correct set of rules under different situations.

#index 727657
#* Variable Independence in Constraint Databases
#@ Jan Chomicki;Dina Goldin;Gabriel Kuper;David Toman
#t 2003
#c 7
#% 27056
#% 123068
#% 123070
#% 145194
#% 164406
#% 169238
#% 190332
#% 191586
#% 191592
#% 201872
#% 213955
#% 213956
#% 237186
#% 237904
#% 246560
#% 248802
#% 257462
#% 260062
#% 268787
#% 273691
#% 289370
#% 308490
#% 419916
#% 421071
#% 459296
#% 467630
#% 480289
#% 481113
#% 527014
#% 758494
#! In this paper, we study constraint databases with variable independence conditions (vics). Such databases occur naturally in the context of temporal and spatiotemporal database applications. Using computational geometry techniques, we show that variable independence is decidable for linear constraint databases. We also present a set of rules for inferring vics in relational algebra expressions. Using vics, we define a subset of relational algebra that is closed under restricted aggregation.

#index 727658
#* A Fuzzy-Logic Based Bidding Strategy for Autonomous Agents in Continuous Double Auctions
#@ Minghua He;Ho-fung Leung;Nicholas R. Jennings
#t 2003
#c 7
#% 159336
#% 185270
#% 212710
#% 214028
#% 252811
#% 257048
#% 260307
#% 271053
#% 271055
#% 326788
#% 333861
#% 433913
#% 443268
#% 588762
#% 636330
#% 659832
#% 723619
#% 1289308
#% 1788137
#% 1788144
#! Increasingly, many systems are being conceptualized, designed, and implemented as marketplaces in which autonomous software entities (agents) trade services. These services can be commodities in e-commerce applications or data and knowledge services in information economies. In many of these cases, there are both multiple agents that are looking to procure services and multiple agents that are looking to sell services at any one time. Such marketplaces are termed continuous double auctions (CDAs). Against this background, this paper develops new algorithms that buyer and seller agents can use to participate in CDAs. These algorithms employ heuristic fuzzy rules and fuzzy reasoning mechanisms in order to determine the best bid to make given the state of the marketplace. Moreover, we show how an agent can dynamically adjust its bidding behavior to respond effectively to changes in the supply and demand in the marketplace. We then show, by empirical evaluations, how our agents outperform four of the most prominent algorithms previously developed for CDAs (several of which have been shown to outperform human bidders in experimental studies).

#index 727659
#* An Efficient Technique for Nearest-Neighbor Query Processing on the SPY-TEC
#@ Dong-Ho Lee;Hyoung-Joo Kim
#t 2003
#c 7
#% 88056
#% 169940
#% 196977
#% 201876
#% 219847
#% 227856
#% 227939
#% 237187
#% 248796
#% 264161
#% 282552
#% 285932
#% 287466
#% 312403
#% 318703
#% 321455
#% 411694
#% 427199
#% 435141
#% 462239
#% 464195
#% 464841
#% 479649
#% 481956
#% 571070
#! The SPY-TEC (Spherical Pyramid-Technique) was proposed as a new indexing method for high-dimensional data spaces using a special partitioning strategy that divides a d-dimensional data space into 2d spherical pyramids. In the SPY-TEC, an efficient algorithm for processing hyperspherical range queries was introduced with a special partitioning strategy. However, the technique for processing k-nearest-neighbor queries, which are frequently used in similarity search, was not proposed. In this paper, we propose an efficient algorithm for processing nearest-neighbor queries on the SPY-TEC by extending the incremental nearest-neighbor algorithm. We also introduce a metric that can be used to guide an ordered best-first traversal when finding nearest neighbors on the SPY-TEC. Finally, we show that our technique significantly outperforms the related techniques in processing k-nearest-neighbor queries by comparing it to the R*-tree, the X-tree, and the sequential scan through extensive experiments.

#index 727660
#* Dynamic Buffer Allocation in Video-on-Demand Systems
#@ Sang-Ho Lee;Kyu-Young Whang;Yang-Sae Moon;Wook-Shin Han;Il-Yeol Song
#t 2003
#c 7
#% 159079
#% 172881
#% 173593
#% 173594
#% 193237
#% 208749
#% 239654
#% 241295
#% 333968
#% 398004
#% 479479
#% 593027
#% 614633
#! In video-on-demand (VOD) systems, as the size of the buffer allocated to user requests increases, initial latency and memory requirements increase. Hence, the buffer size must be minimized. The existing static buffer allocation scheme, however, determines the buffer size based on the assumption that the system is in the fully loaded state. Thus, when the system is in a partially loaded state, the scheme allocates a buffer larger than necessary to a user request. This paper proposes a dynamic buffer allocation scheme that allocates to user requests buffers of the minimum size in a partially loaded state, as well as in the fully loaded state. The inherent difficulty in determining the buffer size in the dynamic buffer allocation scheme is that the size of the buffer currently being allocated is dependent on the number of and the sizes of the buffers to be allocated in the next service period. We solve this problem by the predict-and-enforce strategy, where we predict the number and the sizes of future buffers based on inertia assumptions and enforce these assumptions at runtime. Any violation of these assumptions is resolved by deferring service to the violating new user request until the assumptions are satisfied. Since the size of the current buffer is dependent on the sizes of the future buffers, it is represented by a recurrence equation. We provide a solution to this equation, which can be computed at the system initialization time for runtime efficiency. We have performed extensive analysis and simulation. The results show that the dynamic buffer allocation scheme reduces initial latency (averaged over the number of user requests in service from one to the maximum capacity) to {\frac{1}{29.4}} \sim {\frac{1}{11.0}} of that for the static one and, by reducing the memory requirement, increases the number of concurrent user requests to 2.36 \sim 3.25 times that of the static one when averaged over the amount of system memory available. These results demonstrate that the dynamic buffer allocation scheme significantly improves the performance and capacity of VOD systems.

#index 727661
#* Feature Extraction Based on ICA for Binary Classification Problems
#@ Nojun Kwak;Chong-Ho Choi
#t 2003
#c 7
#% 19746
#% 80995
#% 115608
#% 136350
#% 176172
#% 190861
#% 237983
#% 240233
#% 273326
#% 278039
#% 424167
#% 443090
#% 452821
#% 466588
#% 763432
#% 1289252
#% 1780868
#% 1860052
#% 1860136
#% 1860796
#% 1860915
#! In manipulating data such as in supervised learning, we often extract new features from the original features for the purpose of reducing the dimensions of feature space and achieving better performance. In this paper, we show how standard algorithms for independent component analysis (ICA) can be appended with binary class labels to produce a number of features that do not carry information about the class labels驴these features will be discarded驴and a number of features that do. We also provide a local stability analysis of the proposed algorithm. The advantage is that general ICA algorithms become available to a task of feature extraction for classification problems by maximizing the joint mutual information between class labels and new features, although only for two-class problems. Using the new features, we can greatly reduce the dimension of feature space without degrading the performance of classifying systems.

#index 727662
#* Dimensionality Reduction in Automatic Knowledge Acquisition: A Simple Greedy Search Approach
#@ Samuel H. Huang
#t 2003
#c 7
#% 3596
#% 68777
#% 69430
#% 86926
#% 136350
#% 136731
#% 187739
#% 227486
#% 229972
#% 243727
#% 243728
#% 280086
#% 376266
#% 385564
#% 418076
#% 618434
#% 837722
#! Knowledge acquisition is the process of collecting domain knowledge, documenting the knowledge, and transforming it into a computerized representation. Due to the difficulties involved in eliciting knowledge from human experts, knowledge acquisition was identified as a bottleneck in the development of knowledge-based system. Over the past decades, a number of automatic knowledge acquisition techniques have been developed. However, the performance of these techniques suffers from the so called curse of dimensionality, i.e., difficulties arise when many irrelevant (or redundant) parameters exist. This paper presents a heuristic approach based on statistics and greedy search for dimensionality reduction to facilitate automatic knowledge acquisition. The approach deals with classification problems. Specifically, Chi-square statistics are used to rank the importance of individual parameters. Then, a backward search procedure is employed to eliminate parameters (less important parameters first) that do not contribute to class separability. The algorithm is very efficient and was found to be effective when applied to a variety of problems with different characteristics.

#index 727663
#* Benchmarking Attribute Selection Techniques for Discrete Class Data Mining
#@ Mark A. Hall;Geoffrey Holmes
#t 2003
#c 7
#% 136350
#% 169659
#% 243727
#% 243728
#% 260001
#% 465583
#% 465754
#% 465758
#% 466410
#! Data engineering is generally considered to be a central issue in the development of data mining applications. The success of many learning schemes, in their attempts to construct models of data, hinges on the reliable identification of a small set of highly predictive attributes. The inclusion of irrelevant, redundant, and noisy attributes in the model building process phase can result in poor predictive performance and increased computation. Attribute selection generally involves a combination of search and attribute utility estimation plus evaluation with respect to specific learning schemes. This leads to a large number of possible permutations and has led to a situation where very few benchmark studies have been conducted. This paper presents a benchmark comparison of several attribute selection methods for supervised classification. All the methods produce an attribute ranking, a useful devise for isolating the individual merit of an attribute. Attribute selection is achieved by cross-validating the attribute rankings with respect to a classification learner to find the best attributes. Results are reported for a selection of standard data sets and two diverse learning schemes C4.5 and naive Bayes.

#index 727664
#* A Practical (t, n) Threshold Proxy Signature Scheme Based on the RSA Cryptosystem
#@ Min-Shiang Hwang;Eric Jui-Lin Lu;Iuon-Chang Lin
#t 2003
#c 7
#% 98810
#% 104994
#% 214422
#% 319994
#% 374401
#% 443292
#% 443376
#% 443495
#% 486237
#% 486569
#% 495534
#% 513683
#% 514354
#% 568847
#% 1834656
#! In a (t,n) threshold proxy signature scheme, the original signer delegates the power of signing messages to a designated proxy group of n members. Any t or more proxy signers of the group can cooperatively issue a proxy signature on behalf of the original signer, but (t-1) or less proxy signers cannot. Previously, all of the proposed threshold proxy signature schemes have been based on the discrete logarithm problem and do not satisfy all proxy requirements. In this paper, we propose a practical, efficient, and secure (t,n) threshold proxy signature scheme based on the RSA cryptosystem. Our scheme satisfies all proxy requirements and uses only a simple Lagrange formula to share the proxy signature key. Furthermore, our scheme requires only 5 percent of the computational overhead and 8 percent of the communicational overhead required in Kim's scheme.

#index 727665
#* Visual Exploration of Large Relational Data Sets through 3D Projections and Footprint Splatting
#@ Li Yang
#t 2003
#c 7
#% 1211
#% 17144
#% 36672
#% 73520
#% 76703
#% 86286
#% 86300
#% 107621
#% 155968
#% 220639
#% 286639
#% 310526
#% 364386
#% 434477
#% 436509
#% 619521
#% 641108
#% 726016
#% 726032
#! This paper discusses 3D visualization and interactive exploration of large relational data sets through the integration of several well-chosen multidimensional data visualization techniques and for the purpose of visual data mining and exploratory data analysis. The basic idea is to combine the techniques of grand tour, direct volume rendering, and data aggregation in databases to deal with both the high dimensionality of data and a large number of relational records. Each technique has been enhanced or modified for this application. Specifically, positions of data clusters are used to decide the path of a grand tour. This cluster-guided tour makes intercluster-distance-preserving projections in which data clusters are displayed as separate as possible. A tetrahedral mapping method applied to cluster centroids helps in choosing interesting cluster-guided projections. Multidimensional footprint splatting is used to directly render large relational data sets. This approach abandons the rendering techniques that enhance 3D realism and focuses on how to efficiently produce real-time explanatory images that give comprehensive insights into global features such as data clusters and holes. Examples are given where the techniques are applied to large (more than a million records) relational data sets.

#index 727666
#* Network Caching Strategies for a Shared Data Distribution for a Predefined Service Demand Sequence
#@ Bharadwaj Veeravalli
#t 2003
#c 7
#% 70370
#% 208749
#% 249933
#% 318016
#% 340866
#% 381834
#% 408396
#% 422880
#% 422881
#% 422934
#% 422969
#% 434660
#% 434759
#% 443262
#% 463912
#% 1797480
#% 1852670
#! In this paper, we address the problem of minimizing the cost of transferring a document or a file requested by a set of users geographically separated on a network of nodes. We concentrate on theoretical aspects of data migration and caching on high-speed networks. Following the information caching paradigm introduced in the literature [CHECK END OF SENTENCE], [CHECK END OF SENTENCE], we present polynomial time optimal caching strategies that minimize the total monetary cost of all the service requests by the users on a high-speed network. We consider a scenario in which a large pool of customers from one or more remote sites on a network demand a document, situated at some site, for their use. We also assume that the users can request the document at different time instants. This process of distributing the requested document incurs communication costs due to the use of communication resources and caching costs of the document at some server sites before it is delivered to the users at their desired time instances. We configure the network as a fully connected topology in which the service providers manage and control the distribution of the requested document among the users. For a high-speed network, we show that a single copy of the requested document is sufficient to serve all the user requests in an optimal manner. We extend the study to a homogeneous case in which the communication costs are identical and caching costs at all the sites are identical. In this case, we demonstrate the adaptability of the algorithm in generating more than one copy when needed by the minimization process. Using these strategies, the network service providers can decide when, where, and for how long the requested documents must be cached at vantage sites to obtain an optimal solution. Illustrative examples are provided to ease the understanding.

#index 727667
#* Beyond Independence: Probabilistic Models for Query Approximation on Binary Transaction Data
#@ Dmitry Pavlov;Heikki Mannila;Padhraic Smyth
#t 2003
#c 7
#% 43163
#% 44876
#% 70370
#% 82346
#% 152934
#% 211044
#% 232136
#% 248822
#% 273902
#% 274152
#% 280448
#% 280494
#% 300120
#% 333946
#% 333986
#% 342607
#% 480306
#% 481290
#% 482092
#% 528023
#% 588067
#% 709446
#% 729437
#% 1272326
#! We investigate the problem of generating fast approximate answers to queries posed to large sparse binary data sets. We focus in particular on probabilistic model-based approaches to this problem and develop a number of techniques that are significantly more accurate than a baseline independence model. In particular, we introduce two techniques for building probabilistic models from frequent itemsets: the itemset maximum entropy model and the itemset inclusion-exclusion model. In the maximum entropy model, we treat itemsets as constraints on the distribution of the query variables and use the maximum entropy principle to build a joint probability model for the query attributes online. In the inclusion-exclusion model, itemsets and their frequencies are stored in a data structure, called an ADtree, that supports an efficient implementation of the inclusion-exclusion principle in order to answer the query. We empirically compare these two itemset-based models to direct querying of the original data, querying of samples of the original data, as well as other probabilistic models such as the independence model, the Chow-Liu tree model, and the Bernoulli mixture model. These models are able to handle high-dimensionality (hundreds or thousands of attributes), whereas most other work on this topic has focused on relatively low-dimensional OLAP problems. Experimental results on both simulated and real-world transaction data sets illustrate various fundamental trade offs between approximation error, model complexity, and the online time required to compute a query answer.

#index 727668
#* A Logical Framework for Querying and Repairing Inconsistent Databases
#@ Gianluigi Greco;Sergio Greco;Ester Zumpano
#t 2003
#c 7
#% 36683
#% 77165
#% 101956
#% 109403
#% 158909
#% 207782
#% 217503
#% 235018
#% 244354
#% 273687
#% 286503
#% 305347
#% 318601
#% 333910
#% 384978
#% 420698
#% 442755
#% 442939
#% 443134
#% 464050
#% 464915
#% 479763
#% 479801
#% 499509
#% 499712
#% 501038
#% 519568
#% 552833
#% 556918
#% 579722
#% 591534
#% 1393787
#! In this paper, we address the problem of managing inconsistent databases, i.e., databases violating integrity constraints. We propose a general logic framework for computing repairs and consistent answers over inconsistent databases. A repair for a possibly inconsistent database is a minimal set of insert and delete operations which makes the database consistent, whereas a consistent answer is a set of tuples derived from the database, satisfying all integrity constraints. In our framework, different types of rules defining general integrity constraints, repair constraints (i.e., rules defining conditions on the insertion or deletion of atoms), and prioritized constraints (i.e., rules defining priorities among updates and repairs) are considered. We propose a technique based on the rewriting of constraints into (prioritized) extended disjunctive rules with two different forms of negation (negation as failure and classical negation). The disjunctive program can be used for two different purposes: to compute "repairs" for the database and produce consistent answers, i.e., a maximal set of atoms which do not violate the constraints. We show that our technique is sound, complete (each preferred stable model defines a repair and each repair is derived from a preferred stable model), and more general than techniques previously proposed.

#index 727669
#* Itemset Trees for Targeted Association Querying
#@ Miroslav Kubat;Alaaeldin Hafez;Vijay V. Raghavan;Jayakrishna R. Lekkala;Wei Kian Chen
#t 2003
#c 7
#% 65440
#% 107026
#% 152934
#% 201894
#% 204531
#% 227917
#% 232136
#% 246016
#% 273898
#% 280433
#% 280436
#% 280454
#% 300120
#% 310494
#% 329598
#% 443085
#% 443091
#% 443310
#% 443427
#% 464204
#% 481290
#% 481758
#% 481779
#% 631914
#! Association mining techniques search for groups of frequently co-occurring items in a market-basket type of data and turn these groups into business-oriented rules. Previous research has focused predominantly on how to obtain exhaustive lists of such associations. However, users often prefer a quick response to targeted queries. For instance, they may want to learn about the buying habits of customers that frequently purchase cereals and fruits. To expedite the processing of such queries, we propose an approach that converts the market-basket database into an itemset tree. Experiments indicate that the targeted queries are answered in a time that is roughly linear in the number of market baskets, N. Also, the construction of the itemset tree has O(N) space and time requirements. Some useful theoretical properties are proven.

#index 727670
#* Scalable Cache Invalidation Algorithms for Mobile Data Access
#@ Ahmed Elmagarmid;Jin Jing;Abdelsalam (Sumi) Helal;Choonhwa Lee
#t 2003
#c 7
#% 147905
#% 172874
#% 172875
#% 172876
#% 175205
#% 175253
#% 201897
#% 245014
#% 341892
#% 464214
#! In this paper, we address the problem of cache invalidation in mobile and wireless client/server environments. We present cache invalidation techniques that can scale not only to a large number of mobile clients, but also to a large number of data items that can be cached in the mobile clients. We propose two scalable algorithms: the Multidimensional Bit-Sequence (MD-BS) algorithm and the Multilevel Bit-Sequence (ML-BS) algorithm. Both algorithms are based on our prior work on the Basic Bit-Sequences (BS) algorithm. Our study shows that the proposed algorithms are effective for a large number of cached data items with low update rates. The study also illustrates that the algorithms can be used with other complementary techniques to address the problem of cache invalidation for data items with varied update and access rates.

#index 727671
#* Adaptive and Incremental Processing for Distance Join Queries
#@ Hyoseop Shin;Bongki Moon;Sukho Lee
#t 2003
#c 7
#% 2115
#% 86950
#% 86952
#% 152937
#% 172908
#% 172909
#% 198573
#% 201876
#% 210186
#% 210187
#% 227894
#% 242366
#% 248797
#% 248804
#% 273685
#% 273902
#% 300160
#% 300162
#% 427199
#% 462239
#% 479623
#% 479797
#% 479967
#% 481620
#% 481947
#% 481956
#% 527026
#! A spatial distance join is a relatively new type of operation introduced for spatial and multimedia database applications. Additional requirements for ranking and stopping cardinality are often combined with the spatial distance join in online query processing or Internet search environments. These requirements pose new challenges as well as opportunities for more efficient processing of spatial distance join queries. In this paper, we first present an efficient k{\hbox{-}}\rm distance join algorithm that uses spatial indexes such as R-trees. Bidirectional node expansion and plane-sweeping techniques are used for fast pruning of distant pairs, and the plane-sweeping is further optimized by novel strategies for selecting a sweeping axis and direction. Furthermore, we propose adaptive multistage algorithms for k{\hbox{-}}{\rm{distance}} join and incremental distance join operations. Our performance study shows that the proposed adaptive multistage algorithms outperform previous work by up to an order of magnitude for both k{\hbox{-}}{\rm{distance}} join and incremental distance join queries, under various operational conditions.

#index 727672
#* On the Use of Conceptual Reconstruction for Mining Massively Incomplete Data Sets
#@ Srinivasan Parthasarathy;Charu C. Aggarwal
#t 2003
#c 7
#% 17144
#% 90157
#% 136350
#% 248027
#% 248798
#% 273699
#% 300184
#% 340309
#% 342614
#% 668895
#% 834990
#! Incomplete data sets have become almost ubiquitous in a wide variety of application domains. Common examples can be found in climate and image data sets, sensor data sets, and medical data sets. The incompleteness in these data sets may arise from a number of factors: In some cases, it may simply be a reflection of certain measurements not being available at the time, in others, the information may be lost due to partial system failure, or it may simply be a result of users being unwilling to specify attributes due to privacy concerns. When a significant fraction of the entries are missing in all of the attributes, it becomes very difficult to perform any kind of reasonable extrapolation on the original data. For such cases, we introduce the novel idea of conceptual reconstruction in which we create effective conceptual representations on which the data mining algorithms can be directly applied. The attraction behind the idea of conceptual reconstruction is to use the correlation structure of the data in order to express it in terms of concepts rather than the original dimensions. As a result, the reconstruction procedure estimates only those conceptual aspects of the data which can be mined from the incomplete data set, rather than force errors created by extrapolation. We demonstrate the effectiveness of the approach on a variety of real data sets.

#index 727673
#* Using Predeclaration for Efficient Read-Only Transaction Processing in Wireless Data Broadcast
#@ SangKeun Lee;Chong-Sun Hwang;Masaru Kitsuregawa
#t 2003
#c 7
#% 172874
#% 201897
#% 273893
#% 274199
#% 443127
#% 479961
#% 481777
#% 631861
#% 632025
#% 664067
#! Wireless data broadcast allows a large number of users to retrieve data simultaneously in mobile databases, resulting in an efficient way of using the scarce wireless bandwidth. However, the efficiency of data access methods is limited by an inherent property that data can only be accessed strictly sequentially by users. To properly cope with the inherent property, this paper presents three predeclaration-based transaction processing methods that yield a significant performance improvement in wireless data broadcast.

#index 727674
#* 2003 Index, IEEE Transactions on Knowledge and Data Engineering, Vol. 15
#@  IEEE Transactions on Knowledge and Data Engineering staff
#t 2003
#c 7
#! First Page of the Article

#index 729618
#* A Unified Probabilistic Framework for Web Page Scoring Systems
#@ Michelangelo Diligenti;Marco Gori;Marco Maggini
#t 2004
#c 7
#% 262061
#% 268073
#% 268079
#% 281251
#% 290830
#% 298221
#% 309151
#% 309779
#% 340147
#% 340919
#% 348172
#% 348173
#% 376266
#% 424321
#% 433902
#% 466574
#% 480309
#! Abstract--The definition of efficient page ranking algorithms is becoming an important issue in the design of the query interface of Web search engines. Information flooding is a common experience especially when broad topic queries are issued. Queries containing only one or two keywords usually match a huge number of documents, while users can only afford to visit the first positions of the returned list, which do not necessarily refer to the most appropriate answers. Some successful approaches to page ranking in a hyperlinked environment, like the Web, are based on link analysis. In this paper, we propose a general probabilistic framework for Web Page Scoring Systems (WPSS), which incorporates and extends many of the relevant models proposed in the literature. In particular, we introduce scoring systems for both generic (horizontal) and focused (vertical) search engines. Whereas horizontal scoring algorithms are only based on the topology of the Web graph, vertical ranking also takes the page contents into account and are the base for focused and user adapted search interfaces. Experimental results are reported to show the properties of some of the proposed scoring systems with special emphasis on vertical search.

#index 729619
#* An Enhanced Concurrency Control Scheme for Multidimensional Index Structures
#@ Seok Il Song;Young Ho Kim;Jae Soo Yoo
#t 2004
#c 7
#% 86950
#% 114582
#% 116087
#% 225835
#% 227864
#% 227939
#% 273888
#% 286929
#% 395431
#% 427199
#% 435141
#% 462240
#% 464195
#% 481759
#% 481956
#% 526870
#% 631963
#% 660534
#! Abstract--In this paper, we propose an enhanced concurrency control algorithm that maximizes the concurrency of multidimensional index structures. The factors that deteriorate the concurrency of index structures are node splits and minimum bounding region (MBR) updates in multidimensional index structures. The properties of our concurrency control algorithm are as follows: First, to increase the concurrency by avoiding lock coupling during MBR updates, we propose the PLC (Partial Lock Coupling) technique. Second, a new MBR update method is proposed. It allows searchers to access nodes where MBR updates are being performed. Finally, our algorithm holds exclusive latches not during whole split time but only during physical node split time that occupies the small part of a whole split process. For performance evaluation, we implement the proposed concurrency control algorithm and one of the existing link technique-based algorithms on MIDAS-III that is a storage system of a BADA-IV DBMS. We show through various experiments that our proposed algorithm outperforms the existing algorithm in terms of throughput and response time. Also, we propose a recovery protocol for our proposed concurrency control algorithm. The recovery protocol is designed to assure high concurrency and fast recovery.

#index 729620
#* Editorial: State of the Transactions
#@ Philip S. Yu
#t 2004
#c 7

#index 729621
#* PEBL: Web Page Classification without Negative Examples
#@ Hwanjo Yu;Jiawei Han;Kevin Chen-Chuan Chang
#t 2004
#c 7
#% 197394
#% 207610
#% 266215
#% 280817
#% 302406
#% 309141
#% 309142
#% 310556
#% 311027
#% 316533
#% 458379
#% 464641
#% 466263
#% 527494
#% 577235
#% 615723
#% 722756
#% 722811
#% 722812
#% 1860103
#! Abstract--Web page classification is one of the essential techniques for Web mining because classifying Web pages of an interesting class is often the first step of mining the Web. However, constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples. For instance, in order to construct a 驴homepage驴 classifier, one needs to collect a sample of homepages (positive examples) and a sample of nonhomepages (negative examples). In particular, collecting negative training examples requires arduous work and caution to avoid bias. This paper presents a framework, called Positive Example Based Learning (PEBL), for Web page classification which eliminates the need for manually collecting negative training examples in preprocessing. The PEBL framework applies an algorithm, called Mapping-Convergence (M-C), to achieve high classification accuracy (with positive and unlabeled data) as high as that of a traditional SVM (with positive and negative data). M-C runs in two stages: the mapping stage and convergence stage. In the mapping stage, the algorithm uses a weak classifier that draws an initial approximation of 驴strong驴 negative data. Based on the initial approximation, the convergence stage iteratively runs an internal classifier (e.g., SVM) which maximizes margins to progressively improve the approximation of negative data. Thus, the class boundary eventually converges to the true boundary of the positive class in the feature space. We present the M-C algorithm with supporting theoretical and experimental justifications. Our experiments show that, given the same set of positive examples, the M-C algorithm outperforms one-class SVMs, and it is almost as accurate as the traditional SVMs.

#index 729622
#* Domain-Specific Web Search with Keyword Spices
#@ Satoshi Oyama;Takashi Kokubo;Toru Ishida
#t 2004
#c 7
#% 136350
#% 226099
#% 252834
#% 255161
#% 266215
#% 311027
#% 375017
#% 376266
#% 387427
#% 449568
#% 449588
#% 458379
#% 464641
#% 465895
#% 495944
#% 615723
#% 660364
#% 1289346
#% 1476317
#! Abstract--Domain-specific Web search engines are effective tools for reducing the difficulty experienced when acquiring information from the Web. Existing methods for building domain-specific Web search engines require human expertise or specific facilities. However, we can build a domain-specific search engine simply by adding domain-specific keywords, called 驴keyword spices,驴 to the user's input query and forwarding it to a general-purpose Web search engine. Keyword spices can be effectively discovered from Web documents using machine learning technologies. This paper will describe domain-specific Web search engines that use keyword spices for locating recipes, restaurants, and used cars.

#index 729623
#* Performance Evaluation of an Optimal Cache Replacement Policy for Wireless Data Dissemination
#@ Jianliang Xu;Qinglong Hu;Wang-Chien Lee;Dik Lun Lee
#t 2004
#c 7
#% 172874
#% 175253
#% 201897
#% 209698
#% 245014
#% 249968
#% 259634
#% 290747
#% 303792
#% 314840
#% 316491
#% 316565
#% 328422
#% 341892
#% 344720
#% 345722
#% 404765
#% 419606
#% 443262
#% 443294
#% 452871
#% 462202
#% 464065
#% 464214
#% 464848
#% 480500
#% 566126
#% 659969
#% 706335
#% 721141
#% 1848595
#! Abstract--Data caching at mobile clients is an important technique for improving the performance of wireless data dissemination systems. However, variable data sizes, data updates, limited client resources, and frequent client disconnections make cache management a challenge. In this paper, we propose a gain-based cache replacement policy, Min-SAUD, for wireless data dissemination when cache consistency must be enforced before a cached item is used. Min-SAUD considers several factors that affect cache performance, namely, access probability, update frequency, data size, retrieval delay, and cache validation cost. This paper employs stretch as the major performance metric since it accounts for the data service time and, thus, is fair when items have different sizes. We prove that Min-SAUD achieves optimal stretch under some standard assumptions. Moreover, a series of simulation experiments have been conducted to thoroughly evaluate the performance of Min-SAUD under various system configurations. The simulation results show that, in most cases, the Min-SAUD replacement policy substantially outperforms two existing policies, namely, LRU and SAIU.

#index 729624
#* Demand-Driven Caching in Multiuser Environment
#@ Shen-Tat Goh;Beng Chin Ooi;Kian-Lee Tan
#t 2004
#c 7
#% 36117
#% 86949
#% 169844
#% 190153
#% 248806
#% 248811
#% 300166
#% 333848
#% 335726
#% 464056
#% 480268
#% 617869
#! Abstract--In this paper, we propose a novel demand-driven caching framework, called cache-on-demand (CoD). In CoD, intermediate/final answers of existing running queries are viewed as virtual caches that can be materialized if they are beneficial to incoming queries. Such an approach is essentially nonspeculative: the exact cost of investment and the return on investment are known, and the cache is certain to be reused! We address several issues for CoD to be realized. We also propose three optimizing strategies: Conform-CoD, Scramble-CoD, and Integrated-CoD. Conform-CoD and Scramble-CoD are based on a two-phase optimization framework, while Integrated-CoD operates in a single-phase framework. We conducted extensive performance study to evaluate the effectiveness of these algorithms. Our results show that all the CoD-based schemes can provide substantial performance improvement when compared with a predictive scheme and a no-caching scheme.

#index 729625
#* Personalized Web Search For Improving Retrieval Effectiveness
#@ Fang Liu;Clement Yu;Weiyi Meng
#t 2004
#c 7
#% 115462
#% 124009
#% 165111
#% 194285
#% 219049
#% 224113
#% 234992
#% 252753
#% 280817
#% 280856
#% 281366
#% 282422
#% 287214
#% 290149
#% 309133
#% 330769
#% 333932
#% 333945
#% 376266
#% 385946
#% 406493
#% 413613
#% 465747
#% 481748
#% 615723
#% 632050
#% 637576
#% 978507
#% 1275346
#! Abstract--Current Web search engines are built to serve all users, independent of the special needs of any individual user. Personalization of Web search is to carry out retrieval for each user incorporating his/her interests. We propose a novel technique to learn user profiles from users' search histories. The user profiles are then used to improve retrieval effectiveness in Web search. A user profile and a general profile are learned from the user's search history and a category hierarchy, respectively. These two profiles are combined to map a user query into a set of categories which represent the user's search intention and serve as a context to disambiguate the words in the user's query. Web search is conducted based on both the user query and the set of categories. Several profile learning and category mapping algorithms and a fusion algorithm are provided and evaluated. Experimental results indicate that our technique to personalize Web search is both effective and efficient.

#index 729626
#* Probabilistic Memory-Based Collaborative Filtering
#@ Kai Yu;Anton Schwaighofer;Volker Tresp;Xiaowei Xu;Hans-Peter Kriegel
#t 2004
#c 7
#% 115608
#% 173879
#% 202009
#% 202011
#% 220709
#% 266281
#% 280852
#% 283169
#% 301259
#% 314933
#% 319705
#% 330687
#% 342767
#% 351595
#% 420135
#% 420515
#% 465928
#% 466913
#% 495929
#% 528156
#% 714351
#% 722754
#% 722799
#% 1393622
#% 1499473
#% 1650569
#% 1673052
#! Abstract--Memory-based collaborative filtering (CF) has been studied extensively in the literature and has proven to be successful in various types of personalized recommender systems. In this paper, we develop a probabilistic framework for memory-based CF (PMCF). While this framework has clear links with classical memory-based CF, it allows us to find principled solutions to known problems of CF-based recommender systems. In particular, we show that a probabilistic active learning method can be used to actively query the user, thereby solving the 驴new user problem.驴 Furthermore, the probabilistic framework allows us to reduce the computational cost of memory-based CF by working on a carefully selected subset of user profiles, while retaining high accuracy. We report experimental results based on two real-world data sets, which demonstrate that our proposed PMCF framework allows an accurate and efficient prediction of user preferences.

#index 729627
#* An Efficient and Scalable Algorithm for Clustering XML Documents by Structure
#@ Wang Lian;David Wai-lok Cheung;Nikos Mamoulis;Siu-Ming Yiu
#t 2004
#c 7
#% 23614
#% 66654
#% 236416
#% 260974
#% 273922
#% 309525
#% 406493
#% 479956
#% 481125
#% 481281
#% 631985
#% 660000
#! Abstract--With the standardization of XML as an information exchange language over the net, a huge amount of information is formatted in XML documents. In order to analyze this information efficiently, decomposing the XML documents and storing them in relational tables is a popular practice. However, query processing becomes expensive since, in many cases, an excessive number of joins is required to recover information from the fragmented data. If a collection consists of documents with different structures (for example, they come from different DTDs), mining clusters in the documents could alleviate the fragmentation problem. We propose a hierarchical algorithm (S-GRACE) for clustering XML documents based on structural information in the data. The notion of structure graph (s-graph) is proposed, supporting a computationally efficient distance metric defined between documents and sets of documents. This simple metric yields our new clustering algorithm which is efficient and effective, compared to other approaches based on tree-edit distance. Experiments on real data show that our algorithm can discover clusters not easily identified by manual inspection.

#index 729628
#* Mining Web Informative Structures and Contents Based on Entropy Analysis
#@ Hung-Yu Kao;Shian-Hua Lin;Jan-Ming Ho;Ming-Syan Chen
#t 2004
#c 7
#% 67565
#% 214673
#% 232650
#% 255137
#% 262061
#% 268073
#% 268079
#% 271060
#% 271065
#% 275915
#% 281245
#% 282905
#% 309151
#% 309749
#% 309779
#% 319876
#% 330676
#% 330700
#% 330707
#% 330784
#% 340919
#% 341009
#% 387427
#% 413617
#% 438136
#% 443194
#% 443349
#% 458186
#% 480824
#% 577281
#! Abstract--In this paper, we study the problem of mining the informative structure of a news Web site that consists of thousands of hyperlinked documents. We define the informative structure of a news Web site as a set of index pages (or referred to as TOC, i.e., table of contents, pages) and a set of article pages linked by these TOC pages. Based on the Hyperlink Induced Topics Search (HITS) algorithm, we propose an entropy-based analysis (LAMIS) mechanism for analyzing the entropy of anchor texts and links to eliminate the redundancy of the hyperlinked structure so that the complex structure of a Web site can be distilled. However, to increase the value and the accessibility of pages, most of the content sites tend to publish their pages with intrasite redundant information, such as navigation panels, advertisements, copy announcements, etc. To further eliminate such redundancy, we propose another mechanism, called InfoDiscoverer, which applies the distilled structure to identify sets of article pages. InfoDiscoverer also employs the entropy information to analyze the information measures of article sets and to extract informative content blocks from these sets. Our result is useful for search engines, information agents, and crawlers to index, extract, and navigate significant information from a Web site. Experiments on several real news Web sites show that the precision and the recall of our approaches are much superior to those obtained by conventional methods in mining the informative structures of news Web sites. On the average, the augmented LAMIS leads to prominent performance improvement and increases the precision by a factor ranging from 122 to 257 percent when the desired recall falls between 0.5 and 1. In comparison with manual heuristics, the precision and the recall of InfoDiscoverer are greater than 0.956.

#index 729629
#* Guest Editors' Introduction: Special Section on Mining and Searching the Web
#@ Bing Liu;Soumen Chakrabarti
#t 2004
#c 7

#index 733620
#* A Compact and Accurate Model for Classification
#@ Mark Last;Oded Maimon
#t 2004
#c 7
#% 42994
#% 115608
#% 136350
#% 232106
#% 246831
#% 335012
#% 376266
#% 385564
#% 391425
#% 398045
#% 442814
#% 449588
#% 479640
#% 1214861
#% 1272280
#% 1781027
#! Abstract--We describe and evaluate an information-theoretic algorithm for data-driven induction of classification models based on a minimal subset of available features. The relationship between input (predictive) features and the target (classification) attribute is modeled by a tree-like structure termed an information network (IN). Unlike other decision-tree models, the information network uses the same input attribute across the nodes of a given layer (level). The input attributes are selected incrementally by the algorithm to maximize a global decrease in the conditional entropy of the target attribute. We are using the prepruning approach: When no attribute causes a statistically significant decrease in the entropy, the network construction is stopped. The algorithm is shown empirically to produce much more compact models than other methods of decision-tree learning while preserving nearly the same level of classification accuracy.

#index 733621
#* Extending the ODMG Object Model with Triggers
#@ Elisa Bertino;Giovanna Guerrini;Isabella Merlo
#t 2004
#c 7
#% 58361
#% 125951
#% 197480
#% 198482
#% 211931
#% 234797
#% 235914
#% 264996
#% 273936
#% 335500
#% 384978
#% 385995
#% 442665
#% 443364
#% 463580
#% 480600
#% 480621
#% 481457
#% 487576
#% 501940
#% 517883
#% 554750
#! Abstract--In this paper, we extend the standard for object-oriented databases, ODMG, with reactive features, by proposing a language for specifying triggers and defining its semantics. This extension has several implications, thus this work makes three different specific contributions. First, the definition of a declarative data manipulation language for ODMG, which is missing in the current version of the standard; such a definition requires revisiting data manipulation in ODMG and also addressing issues related to set-oriented versus instance-oriented computation. Then, the definition of a trigger language for ODMG, unifying also the SQL:1999 proposal and providing support for trigger inheritance and overriding. Finally, the development of a formal semantics for the proposed data manipulation and trigger languages.

#index 733622
#* A Graph-Based Approach for Timing Analysis and Refinement of OPS5 Knowledge-Based Systems
#@ Albert Mo Kim Cheng;Hsiu-yen Tsai
#t 2004
#c 7
#% 1797
#% 34000
#% 36191
#% 37905
#% 53531
#% 60814
#% 95979
#% 114703
#% 121536
#% 144497
#% 168444
#% 179005
#% 437280
#% 442733
#% 442816
#% 442929
#% 443196
#% 443351
#% 445781
#% 589690
#% 615099
#% 679377
#% 692741
#! Abstract--This paper examines the problem of predicting the timing behavior of knowledge-based systems for real-time applications. In particular, we describe a suite of tools which analyze OPS5 programs to understand their timing properties. First, a graphical representation of an OPS5 program is defined and evaluated. This graph represents the logical control flows of an OPS5 program. Most of our analysis is based on this data structure. Second, we describe a novel tool which verifies that an OPS5 program can terminate in finite time. If the termination of the OPS5 program is not expected, the 驴culprit驴 conditions are detected. These conditions are then used to correct the problem by adding extra rules to the original program. Third, another tool is introduced to aid timing analysis of OPS5 programs. This tool generates a set of test data which maximize the program execution time. Other functions are also provided to facilitate the timing analysis.

#index 733623
#* CAIM Discretization Algorithm
#@ Lukasz A. Kurgan;Krzysztof J. Cios
#t 2004
#c 7
#% 35065
#% 129980
#% 136350
#% 267537
#% 344807
#% 443148
#% 443880
#% 449566
#% 458174
#% 458178
#% 498294
#% 769688
#! Abstract--The task of extracting knowledge from databases is quite often performed by machine learning algorithms. The majority of these algorithms can be applied only to data described by discrete numerical or nominal attributes (features). In the case of continuous attributes, there is a need for a discretization algorithm that transforms continuous attributes into discrete ones. This paper describes such an algorithm, called CAIM (class-attribute interdependence maximization), which is designed to work with supervised data. The goal of the CAIM algorithm is to maximize the class-attribute interdependence and to generate a (possibly) minimal number of discrete intervals. The algorithm does not require the user to predefine the number of intervals, as opposed to some other discretization algorithms. The tests performed using CAIM and six other state-of-the-art discretization algorithms show that discrete attributes generated by the CAIM algorithm almost always have the lowest number of intervals and the highest class-attribute interdependency. Two machine learning algorithms, the CLIP4 rule algorithm and the decision tree algorithm, are used to generate classification rules from data discretized by CAIM. For both the CLIP4 and decision tree algorithms, the accuracy of the generated rules is higher and the number of the rules is lower for data discretized using the CAIM algorithm when compared to data discretized using six other discretization algorithms. The highest classification accuracy was achieved for data sets discretized with the CAIM algorithm, as compared with the other six algorithms.

#index 733624
#* A Fourier Spectrum-Based Approach to Represent Decision Trees for Mining Data Streams in Mobile Environments
#@ Hillol Kargupta;Byung-Hoon Park
#t 2004
#c 7
#% 91386
#% 136350
#% 156699
#% 273900
#% 280496
#% 280511
#% 310500
#% 342639
#% 345861
#% 360500
#% 390532
#% 394543
#% 424993
#% 424996
#% 449529
#% 449588
#% 565528
#% 715964
#% 1478814
#% 1499573
#! Abstract--This paper presents a novel Fourier analysis-based approach to combine, transmit, and visualize decision trees in a mobile environment. Fourier representation of a decision tree has several interesting properties that are particularly useful for mining data streams from small mobile computing devices connected through limited-bandwidth wireless networks. This paper presents algorithms to compute the Fourier spectrum of a decision tree and outlines a technique to construct a decision tree from its Fourier spectrum. It offers a framework to aggregate decision trees in their Fourier representations. It also describes the MobiMine, a mobile data stream mining system, that uses the developed techniques for mining stock-market data from handheld devices.

#index 733625
#* Addressing the Problems of Bayesian Network Classification of Video Using High-Dimensional Features
#@ Ankush Mittal;Loong-Fah Cheong
#t 2004
#c 7
#% 44323
#% 44876
#% 80995
#% 136350
#% 167633
#% 306308
#% 356892
#% 437405
#% 443259
#% 443260
#% 458261
#% 466084
#% 626896
#% 633046
#% 722757
#% 937303
#% 968502
#% 1378222
#% 1650666
#% 1650738
#! Abstract--Bayesian theory is of great interest in pattern classification. In this paper, we present an approach to aid in the effective application of Bayesian networks in tasks like video classification, where descriptors originate from varied sources and are large in number. In order to extend the application of conventional Bayesian theory to the case of continuous and nonparametric descriptor space, dimension partitioning into attributes by minimizing the discrete Bayes error is proposed. The partitioning output goes to the dimensionality reduction module. A new algorithm for dimensionality reduction for improving the classification accuracy is proposed based on the class pair discriminative capacity of the dimensions. It is also shown how attributes can be weighed automatically in a single-label assignment based on comparing the class pairs. A computationally efficient method to assign multiple labels on the samples is also presented. Comparison with standard classification tools on video data of more than 4,000 segments shows the potential of our approach in pattern classification.

#index 733626
#* Speculative Locking Protocols to Improve Performance for Distributed Database Systems
#@ P. Krishna Reddy;Masaru Kitsuregawa
#t 2004
#c 7
#% 4619
#% 9241
#% 12130
#% 27057
#% 37276
#% 39634
#% 58353
#% 91629
#% 114583
#% 114710
#% 116087
#% 157155
#% 190645
#% 194928
#% 227958
#% 286836
#% 287230
#% 320902
#% 403195
#% 411707
#% 442866
#% 442906
#% 462657
#% 463253
#% 480937
#% 481585
#% 489201
#% 495252
#% 531907
#% 571083
#! Abstract--In this paper, we have proposed speculative locking (SL) protocols to improve the performance of distributed database systems (DDBSs) by trading extra processing resources. In SL, a transaction releases the lock on the data object whenever it produces corresponding after-image during its execution. By accessing both before and after-images, the waiting transaction carries out speculative executions and retains one execution based on the termination (commit or abort) mode of the preceding transactions. By carrying out multiple executions for a transaction, SL increases parallelism without violating serializability criteria. Under the naive version of SL, the number of speculative executions of the transaction explodes with data contention. By exploiting the fact that a submitted transaction is more likely to commit than abort, we propose the SL variants that process transactions efficiently by significantly reducing the number of speculative executions. The simulation results indicate that even with manageable extra resources, these variants significantly improve the performance over two-phase locking in the DDBS environments where transactions spend longer time for processing and transaction-aborts occur frequently.

#index 733627
#* Evaluating Refined Queries in Top-k Retrieval Systems
#@ Kaushik Chakrabarti;Michael Ortega-Binderberger;Sharad Mehrotra;Kriengkrai Porkaew
#t 2004
#c 7
#% 41230
#% 201876
#% 213981
#% 227894
#% 239709
#% 248010
#% 248797
#% 286881
#% 403195
#% 443243
#% 479788
#% 479816
#% 480302
#% 481947
#% 482109
#% 527026
#% 626971
#% 631963
#% 632060
#% 1857498
#! Abstract--In many applications, users specify target values for certain attributes/features without requiring exact matches to these values in return. Instead, the result is typically a ranked list of 驴top k驴 objects that best match the specified feature values. User subjectivity is an important aspect of such queries, i.e., which objects are relevant to the user and which are not depends on the perception of the user. Due to the subjective nature of top-k queries, the answers returned by the system to an user query often do not satisfy the users need right away, either because the weights and the distance functions associated with the features do not accurately capture the users perception or because the specified target values do not fully capture her information need or both. In such cases, the user would like to refine the query and resubmit it in order to get back a better set of answers. While there has been a lot of research on query refinement models, there is no work that we are aware of on supporting refinement of top-k queries efficiently in a database system. Done naively, each 驴refined驴 query can be treated as a 驴starting驴 query and evaluated from scratch. This paper explores alternative approaches that significantly improve the cost of evaluating refined queries by exploiting the observation that the refined queries are not modified drastically from one iteration to another. Our experiments over a real-life multimedia data set show that the proposed techniques save more than 80 percent of the execution cost of refined queries over the naive approach and is more than an order of magnitude faster than a simple sequential scan.

#index 733628
#* Semantic Conflict Resolution Ontology (SCROL): An Ontology for Detecting and Resolving Data and Schema-Level Semantic Conflicts
#@ Sudha Ram;Jinsoo Park
#t 2004
#c 7
#% 58354
#% 69498
#% 85086
#% 111922
#% 156337
#% 163046
#% 172378
#% 188638
#% 250140
#% 250472
#% 269088
#% 282425
#% 307214
#% 331539
#% 332166
#% 421344
#% 443214
#% 480645
#% 511744
#% 571297
#% 591545
#% 709136
#! Abstract--Establishing semantic interoperability among heterogeneous information sources has been a critical issue in the database community for the past two decades. Despite the critical importance, current approaches to semantic interoperability of heterogeneous databases have not been sufficiently effective. We propose a common ontology called Semantic Conflict Resolution Ontology (SCROL) that addresses the inherent difficulties in the conventional approaches, i.e., federated schema and domain ontology approaches. SCROL provides a systematic method for automatically detecting and resolving various semantic conflicts in heterogeneous databases. SCROL provides a dynamic mechanism of comparing and manipulating contextual knowledge of each information source, which is useful in achieving semantic interoperability among heterogeneous databases. We show how SCROL is used for detecting and resolving semantic conflicts between semantically equivalent schema and data elements. In addition, we present evaluation results to show that SCROL can be successfully used to automate the process of identifying and resolving semantic conflicts.

#index 733629
#* On Using Partial Supervision for Text Categorization
#@ Charu C. Aggarwal;Stephen C. Gates;Philip S. Yu
#t 2004
#c 7
#% 36672
#% 118771
#% 144023
#% 165110
#% 218992
#% 232651
#% 232655
#% 232717
#% 248810
#% 262045
#% 262050
#% 262059
#% 266292
#% 273891
#% 280492
#% 406493
#% 458369
#% 465747
#% 571073
#! Abstract--In this paper, we discuss the merits of building text categorization systems by using supervised clustering techniques. Traditional approaches for document classification on a predefined set of classes are often unable to provide sufficient accuracy because of the difficulty of fitting a manually categorized collection of documents in a given classification model. This is especially the case for heterogeneous collections of Web documents which have varying styles, vocabulary, and authorship. Hence, this paper investigates the use of clustering in order to create the set of categories and its use for classification of documents. Completely unsupervised clustering has the disadvantage that it has difficulty in isolating sufficiently fine-grained classes of documents relating to a coherent subject matter. In this paper, we use the information from a preexisting taxonomy in order to supervise the creation of a set of related clusters, though with some freedom in defining and creating the classes. We show that the advantage of using partially supervised clustering is that it is possible to have some control over the range of subjects that one would like the categorization system to address, but with a precise mathematical definition of how each category is defined. An extremely effective way then to categorize documents is to use this a priori knowledge of the definition of each category. We also discuss a new technique to help the classifier distinguish better among closely related clusters.

#index 737969
#* Evaluating Keyword Selection Methods for WEBSOM Text Archives
#@ Arnulfo P. Azcarraga;Teddy N. Yap;Jonathan Tan;Tat Seng Chua
#t 2004
#c 7
#% 60576
#% 234978
#% 304421
#% 342661
#% 511811
#% 1860651
#! Abstract--The WEBSOM methodology, proven effective for building very large text archives, includes a method that extracts labels for each document cluster assigned to nodes in the map. However, the WEBSOM method needs to retrieve all the words of all the documents associated to each node. Since maps may have more than 100,000 nodes and since the archive may contain up to seven million documents, the WEBSOM methodology needs a faster alternative method for keyword selection. Presented here is such an alternative method that is able to quickly deduce meaningful labels per node in the map. It does this just by analyzing the relative weight distribution of the SOM weight vectors and by taking advantage of some characteristics of the random projection method used in dimensionality reduction. The effectiveness of this technique is demonstrated on news document collections.

#index 737970
#* Shape Understanding: Knowledge Generation and Learning
#@ Zbigniew Les;Magdalena Les
#t 2004
#c 7
#% 3903
#% 79441
#% 90846
#% 90961
#% 100209
#% 109388
#% 156425
#% 198079
#% 209622
#% 286672
#% 286673
#% 286674
#% 364567
#% 376266
#% 443832
#% 449588
#% 512239
#% 564711
#% 681717
#% 1297803
#! Abstract--In this paper, a method of knowledge generation as part of a shape-understanding method is presented. The proposed method of knowledge generation consists of: learning the description of new a posteriori classes, learning the concept of visual objects, and generation of the visual representation of "inner驴 objects. The visual concept, as part of the concept of the visual object, is expressed as a set of symbolic names that refers to possible classes of shape. The visual concept can be used to find the visual similarities between different visual objects, perform visual transformations as part of visual thinking capabilities of a system, and memorize a visual object as a symbolic representation. The knowledge obtained in the process of knowledge generation is integrated with an existing knowledge of a shape understanding system and used in the explanatory process. This system of shape understanding (SUS), that is, the implementation of the shape understanding method, is designed to imitate the visual thinking capabilities of the human visual system. The SUS consists of different types of experts that perform different processing and reasoning tasks and is designed to perform visual diagnosis in medical applications.

#index 737971
#* Self-Tuning of the Relationships among Rules' Components in Active Databases Systems
#@ David Botzer;Opher Etzion
#t 2004
#c 7
#% 5972
#% 45257
#% 52373
#% 58361
#% 62021
#% 122914
#% 168765
#% 197425
#% 288197
#% 385995
#% 394417
#% 442766
#% 443029
#% 443059
#% 462940
#% 480626
#% 481448
#% 501945
#% 591548
#! Abstract--Active databases systems are systems that detect events and trigger actions as a result of this detection. Active capabilities are provided by a set of rules, such that each rule consists of three components (event, condition, and action). A major performance issue in active databases is the issue of relationships among rule components. Current implementations of triggers do not allow flexibility in the selection of transaction policies (partition of rules to transactions); the intertransaction timing policies of rules' components, the intratransaction policies of commit and abort dependencies, and synchronization issues. While these decisions have a substantial impact on the application performance, they are not provided as design primitives; one of the reasons for that is that it is very difficult to manually tune these decisions. In some research prototypes of active databases, these relationships are encapsulated into a set of coupling modes. Each coupling mode represents a combination of decisions about the partition of rule components to transactions, the relative timing within a transaction, and the interrelationships among these transactions. This paper describes a self-tuning model that operates on a general active database. The optimization model strives to minimize a programmable goal function that reflects the system designer's preferences and the system behavior and the applications' semantics through constraint definitions. The tuning model strives to optimize the mutual relationships among the system rules' components.

#index 737972
#* WALRUS: A Similarity Retrieval Algorithm for Image Databases
#@ Apostol Natsev;Rajeev Rastogi;Kyuseok Shim
#t 2004
#c 7
#% 68091
#% 86950
#% 169940
#% 196977
#% 210173
#% 213673
#% 228351
#% 247889
#% 257637
#% 273919
#% 345848
#% 427199
#% 437405
#% 445326
#% 589740
#% 592154
#% 592420
#% 626558
#% 718437
#! Abstract--Approaches for content-based image querying typically extract a single signature from each image based on color, texture, or shape features. The images returned as the query result are then the ones whose signatures are closest to the signature of the query image. While efficient for simple images, such methods do not work well for complex scenes since they fail to retrieve images that match the query only partially, that is, only certain regions of the image match. This inefficiency leads to the discarding of images that may be semantically very similar to the query image since they may contain the same objects. The problem becomes even more apparent when we consider scaled or translated versions of the similar objects. In this paper, we propose WALRUS (WAveLet-based Retrieval of User-specified Scenes), a novel similarity retrieval algorithm that is robust to scaling and translation of objects within an image. WALRUS employs a novel similarity model in which each image is first decomposed into its regions and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying sizes and then clusters them based on the proximity of their signatures. An efficient dynamic programming algorithm is used to compute wavelet-based signatures for the sliding windows. Experimental results on real-life data sets corroborate the effectiveness of WALRUS's similarity model.

#index 737973
#* TEMPOS: A Platform for Developing Temporal Applications on Top of Object DBMS
#@ Marlon Dumas;Marie-Christine Fauvet;Pierre-Claude Scholl
#t 2004
#c 7
#% 28144
#% 35568
#% 101955
#% 125625
#% 168773
#% 186970
#% 214715
#% 225004
#% 260041
#% 286256
#% 287268
#% 294600
#% 315005
#% 335715
#% 361445
#% 423069
#% 443135
#% 459296
#% 482087
#% 488084
#% 535019
#% 568176
#% 588641
#% 618546
#! Abstract--This paper presents Tempos: a set of models and languages supporting the manipulation of temporal data on top of object DBMS. The proposed models exploit object-oriented technology to meet some important, yet traditionally neglected design criteria related to legacy code migration and representation independence. Two complementary ways for accessing temporal data are offered: a query language and a visual browser. The query language, namely TempOQL, is an extension of OQL supporting the manipulation of histories regardless of their representations, through fully composable functional operators. The visual browser offers operators that facilitate several time-related interactive navigation tasks, such as studying a snapshot of a collection of objects at a given instant, or detecting and examining changes within temporal attributes and relationships. Tempos models and languages have been formalized both at the syntactical and the semantical level and have been implemented on top of an object DBMS. The suitability of the proposals with regard to applications' requirements has been validated through concrete case studies.

#index 737974
#* Case Generation Using Rough Sets with Fuzzy Representation
#@ Sankar K. Pal;Pabitra Mitra
#t 2004
#c 7
#% 92533
#% 129212
#% 168280
#% 237867
#% 307100
#% 318436
#% 318716
#% 335962
#% 366687
#% 388227
#% 418030
#% 474155
#% 1784396
#% 1860401
#! Abstract--In this article, we propose a rough-fuzzy hybridization scheme for case generation. Fuzzy set theory is used for linguistic representation of patterns, thereby producing a fuzzy granulation of the feature space. Rough set theory is used to obtain dependency rules which model informative regions in the granulated feature space. The fuzzy membership functions corresponding to the informative regions are stored as cases along with the strength values. Case retrieval is made using a similarity measure based on these membership functions. Unlike the existing case selection methods, the cases here are cluster granules and not sample points. Also, each case involves a reduced number of relevant features. These makes the algorithm suitable for mining data sets, large both in dimension and size, due to its low-time requirement in case generation as well as retrieval. Superiority of the algorithm in terms of classification accuracy and case generation and retrieval times is demonstrated on some real-life data sets.

#index 737975
#* Incremental, Online, and Merge Mining of Partial Periodic Patterns in Time-Series Databases
#@ Walid G. Aref;Mohamed G. Elfeky;Ahmed K. Elmagarmid
#t 2004
#c 7
#% 172949
#% 273898
#% 273900
#% 443195
#% 449588
#% 459006
#% 460862
#% 463903
#% 464204
#% 479658
#% 479971
#% 481290
#% 481609
#% 481611
#% 631920
#% 631923
#% 631926
#% 632036
#! Abstract--Mining of periodic patterns in time-series databases is an interesting data mining problem. It can be envisioned as a tool for forecasting and prediction of the future behavior of time-series data. Incremental mining refers to the issue of maintaining the discovered patterns over time in the presence of more items being added into the database. Because of the mostly append only nature of updating time-series data, incremental mining would be very effective and efficient. Several algorithms for incremental mining of partial periodic patterns in time-series databases are proposed and are analyzed empirically. The new algorithms allow for online adaptation of the thresholds in order to produce interactive mining of partial periodic patterns. The storage overhead of the incremental online mining algorithms is analyzed. Results show that the storage overhead for storing the intermediate data structures pays off as the incremental online mining of partial periodic patterns proves to be significantly more efficient than the nonincremental nononline versions. Moreover, a new problem, termed merge mining, is introduced as a generalization of incremental mining. Merge mining can be defined as merging the discovered patterns of two or more databases that are mined independently of each other. An algorithm for merge mining of partial periodic patterns in time-series databases is proposed and analyzed.

#index 737976
#* On Automated Lesson Construction from Electronic Textbooks
#@ Gultekin Ozsoyoglu;Nevzat H. Balkir;Z. Meral Ozsoyoglu;Graham Cormode
#t 2004
#c 7
#% 36683
#% 55490
#% 138985
#% 200974
#% 248801
#% 290413
#% 316709
#% 334867
#% 356119
#% 422876
#% 422966
#% 443284
#% 479623
#% 479816
#% 709497
#% 993936
#! Abstract--An electronic book may be viewed as an application with a multimedia database. We define an electronic textbook as an electronic book that is used in conjunction with instructional resources such as lectures. In this paper, we propose an electronic textbook data model with topics, topic sources, metalinks (relationships among topics), and instructional modules which are multimedia presentations possibly capturing real-life lectures of instructors. Using the data model, the system provides users a topic-guided multimedia lesson construction. This paper concentrates, in detail, on the use of one metalink type in lesson construction, namely, prerequisite dependencies, and provides a sound and complete axiomatization of prerequisite dependencies. We present a simple automated way of constructing lessons for users where the user lists a set of topic names (s)he is interested in, and the system automatically constructs and delivers the "best驴 user-tailored lesson as a multimedia presentation, where "best驴 is characterized in terms of both topic closures with respect to prerequisite dependencies and what the user knows about topics. We model and present sample lesson construction requests for users, discuss their complexity, and give algorithms that evaluate such requests. For expensive lesson construction requests, we list heuristics and empirically evaluate their performance. We also discuss the worst-case performance guarantees of lesson request algorithms.

#index 740757
#* An Approach to Novelty Detection Applied to the Classification of Image Regions
#@ Sameer Singh;Markos Markou
#t 2004
#c 7
#% 94926
#% 302406
#% 310843
#% 328815
#% 400896
#% 414570
#% 418796
#% 424424
#% 496411
#% 592155
#% 729437
#% 985611
#% 1051463
#% 1275294
#% 1843653
#% 1860238
#% 1860652
#! Abstract--In this paper, we present a new framework for novelty detection. The framework evaluates neural networks as adaptive classifiers that are capable of novelty detection and retraining on the basis of newly discovered information. We apply our newly developed model to the application area of object recognition in video. This paper details the tools and methods needed for novelty detection such that data from unknown classes can be reliably rejected without any a priori knowledge of its characteristics. The rejected data is postprocessed to determine which samples can be manually labeled of a new type and used for retraining. In this paper, we compare the proposed framework with other novelty detection methods and discuss the results of adaptive retraining of neural network to recognize further unseen data containing the newly added objects.

#index 740758
#* Distributed Evaluation of Network Directory Queries
#@ Sihem Amer-Yahia;Divesh Srivastava;Dan Suciu
#t 2004
#c 7
#% 69791
#% 83933
#% 238087
#% 273897
#% 330305
#% 386455
#% 388237
#! Abstract--This paper describes novel efficient techniques for the distributed evaluation of hierarchical aggregate selection queries over LDAP directory data, distributed across multiple autonomous directory servers. Such queries are useful for emerging applications like the Directory Enabled Networks initiative. Our techniques follow the LDAP approach of distributed query evaluation by referrals, where each relevant server computes answers locally, and the LDAP client coordinates between directory servers. We make a conceptual separation between the identification of relevant servers and the distributed computation of answers. We focus on the challenging task of generating an efficient plan for evaluating hierarchical aggregate selection queries, which involves correlating directory entries across multiple servers. The key features of our plan are: 1) the network traffic consists of query answers, and auxiliary messages that depend only on the number of servers and the size of the query (not on the data size), 2) the coordination effort at the client is independent of the data size, and 3) potentially expensive server-to-server communication and coordination is avoided. We complement our analysis with experiments that show the robustness and scalability of our techniques for highly distributed directory query processing.

#index 740759
#* Optimal Models of Disjunctive Logic Programs: Semantics, Complexity, and Computation
#@ Nicola Leone;Francesco Scarcello;V. S. Subrahmanian
#t 2004
#c 7
#% 877
#% 7689
#% 35562
#% 68411
#% 69528
#% 75202
#% 101922
#% 124785
#% 144840
#% 181038
#% 231786
#% 235018
#% 268779
#% 345429
#% 400992
#% 443429
#% 443489
#% 495776
#% 501037
#% 514420
#% 556918
#% 561913
#% 598376
#! Abstract--Almost all semantics for logic programs with negation identify a set, SEM(P), of models of program P, as the intended semantics of P, and any model M in this class is considered a possible meaning of P with regard to the semantics the user has in mind. Thus, for example, in the case of stable models [CHECK END OF SENTENCE], choice models [CHECK END OF SENTENCE], answer sets [CHECK END OF SENTENCE], etc., different possible models correspond to different ways of "completing驴 the incomplete information in the logic program. However, different end-users may have different ideas on which of these different models in SEM(P) is a reasonable one from their point of view. For instance, given SEM(P), user U_1 may prefer model M_1\in SEM(P) to model M_2\in SEM(P) based on some evaluation criterion that she has. In this paper, we develop a logic program semantics based on Optimal Models. This semantics does not add yet another semantics to the logic programming arena--it takes as input an existing semantics SEM(P)and a user-specified objective function Obj, and yields a new semantics \underline{{\rm{Opt}}}(P)\subseteq SEM(P) that realizes the objective function within the framework of preferred models identified already by SEM(P). Thus, the user who may or may not know anything about logic programming has considerable flexibility in making the system reflect her own objectives by building "on top驴 of existing semantics known to the system. In addition to the declarative semantics, we provide a complete complexity analysis and algorithms to compute optimal models under varied conditions when SEM(P) is the stable model semantics, the minimal models semantics, and the all-models semantics.

#index 740760
#* The Vagabond Approach to Logging and Recovery in Transaction-Time Temporal Object Database Systems
#@ Kjetil Nrvåg
#t 2004
#c 7
#% 58371
#% 131555
#% 172948
#% 216907
#% 240007
#% 316561
#% 317988
#% 442705
#% 463592
#% 479932
#% 480096
#% 481126
#% 495258
#% 555030
#% 566134
#% 570884
#% 978515
#! Abstract--In most current database systems, data is updated in-place. In order to support recovery and increase performance, write-ahead logging is used. This logging defers the in-place updates. However, sooner or later, the updates have to be applied to the database. Even if this is done as a batch operation, it can result in many nonsequential writes. In order to avoid this, another approach is to eliminate the database completely and use a log-only approach. In this case, the log is written contiguously to the disk, in a no--overwrite way using large blocks. When using the log-only approach keeping previous versions comes almost for free, and this approach is therefore particularly interesting for transaction-time temporal object database systems. Although the log-only approach in its basic form is relatively straightforward, it is not trivial to support features such as steal/no-force buffer management, fuzzy checkpointing, and fast commit. In this paper, we describe, in detail, algorithms and strategies for object and log management that make support for these features possible.

#index 740761
#* Optimizing Similarity Search for Arbitrary Length Time Series Queries
#@ Tamer Kahveci;Ambuj K. Singh
#t 2004
#c 7
#% 86950
#% 172949
#% 181378
#% 212197
#% 227857
#% 248797
#% 248798
#% 273704
#% 316551
#% 333941
#% 427199
#% 460862
#% 464851
#% 480482
#% 481119
#% 481609
#% 481947
#% 504158
#% 617886
#% 617888
#% 631923
#% 632042
#% 632088
#% 632089
#% 659936
#% 659971
#! Abstract--We consider the problem of finding similar patterns in a time sequence. Typical applications of this problem involve large databases consisting of long time sequences of different lengths. Current time sequence search techniques work well for queries of a prespecified length, but not for arbitrary length queries. We propose a novel indexing technique that works well for arbitrary length queries. The proposed technique stores index structures at different resolutions for a given data set. We prove that this index structure is superior to existing index structures that use a single resolution. We propose a range query and nearest neighbor query technique on this index structure and prove the optimality of our index structure for these search techniques. The experimental results show that our method is 4 to 20 times faster than the current techniques, including Sequential Scan, for range queries and 3 times faster than Sequential Scan and other techniques for nearest neighbor queries. Because of the need to store information at multiple resolution levels, the storage requirement of our method could potentially be large. In the second part of the paper, we show how the index information can be compressed with minimal information loss. According to our experimental results, even after compressing the size of the index to one fifth, the total cost of our method is 3 to 15 times less than the current techniques.

#index 740762
#* A Support Vector Machine with a Hybrid Kernel and Minimal Vapnik-Chervonenkis Dimension
#@ Ying Tan;Jun Wang
#t 2004
#c 7
#% 116149
#% 190581
#% 197394
#% 251158
#% 266426
#% 269217
#% 269218
#% 294964
#% 309208
#% 592108
#% 632549
#% 1762960
#% 1809459
#% 1860523
#% 1860542
#% 1860545
#! Abstract--This paper presents a mechanism to train support vector machines (SVMs) with a hybrid kernel and minimal Vapnik-Chervonenkis (VC) dimension. After describing the VC dimension of sets of separating hyperplanes in a high-dimensional feature space produced by a mapping related to kernels from the input space, we proposed an optimization criterion to design SVMs by minimizing the upper bound of the VC dimension. This method realizes a structural risk minimization and utilizes a flexible kernel function such that a superior generalization over test data can be obtained. In order to obtain a flexible kernel function, we develop a hybrid kernel function and a sufficient condition to be an admissible Mercer kernel based on common Mercer kernels (polynomial, radial basis function, two-layer neural network, etc.). The nonnegative combination coefficients and parameters of the hybrid kernel are determined subject to the minimal upper bound of the VC dimension of the learning machine. The use of the hybrid kernel results in a better performance than those with a single common kernel. Experimental results are discussed to illustrate the proposed method and show that the SVM with the hybrid kernel outperforms that with a single common kernel in terms of generalization power.

#index 740763
#* Image Retrieval Using Multiple Evidence Ranking
#@ Tatiana Almeida Souza Coelho;Pável Pereira Calado;Lamarque Vieira Souza;Berthier Ribeiro-Neto;Richard Muntz
#t 2004
#c 7
#% 111303
#% 176530
#% 219047
#% 232710
#% 281174
#% 290703
#% 292684
#% 309104
#% 311805
#% 316148
#% 345767
#% 345771
#% 370075
#% 387427
#% 406493
#% 433677
#% 434970
#% 437405
#% 437407
#% 443178
#% 443259
#% 447947
#% 589742
#% 1854912
#! Abstract--The World Wide Web is the largest publicly available image repository and a natural source of attention. An immediate consequence is that searching for images on the Web has become a current and important task. To search for images of interest, the most direct approach is keyword-based searching. However, since images on the Web are poorly labeled, direct application of standard keyword-based image searching techniques frequently yields poor results. In this work, we propose a comprehensive solution to this problem. In our approach, multiple sources of evidence related to the images are considered. To allow combining these distinct sources of evidence, we introduce an image retrieval model based on Bayesian belief networks. To evaluate our approach, we perform experiments on a reference collection composed of 54,000 Web images. Our results indicate that retrieval using an image surrounding text passages is as effective as standard retrieval based on HTML tags. This is an interesting result because current image search engines in the Web usually do not take text passages into consideration. Most important, according to our results, the combination of information derived from text passages with information derived from HTML tags leads to improved retrieval, with relative gains in average precision figures of roughly 50 percent, when compared to the results obtained by the use of each source of evidence in isolation.

#index 740764
#* Association Rule Hiding
#@ Vassilios S. Verykios;Ahmed K. Elmagarmid;Elisa Bertino;Yucel Saygin;Elena Dasseni
#t 2004
#c 7
#% 67453
#% 232102
#% 300184
#% 333876
#% 442946
#% 443010
#% 488478
#% 488480
#% 586838
#! Abstract--Large repositories of data contain sensitive information that must be protected against unauthorized access. The protection of the confidentiality of this information has been a long-term goal for the database security research community and for the government statistical agencies. Recent advances in data mining and machine learning algorithms have increased the disclosure risks that one may encounter when releasing data to outside parties. A key problem, and still not sufficiently investigated, is the need to balance the confidentiality of the disclosed data with the legitimate needs of the data users. Every disclosure limitation method affects, in some way, and modifies true data values and relationships. In this paper, we investigate confidentiality issues of a broad category of rules, the association rules. In particular, we present three strategies and five algorithms for hiding a group of association rules, which is characterized as sensitive. One rule is characterized as sensitive if its disclosure risk is above a certain privacy threshold. Sometimes, sensitive rules should not be disclosed to the public since, among other things, they may be used for inferring sensitive data, or they may provide business competitors with an advantage. We also perform an evaluation study of the hiding algorithms in order to analyze their time complexity and the impact that they have in the original database.

#index 740765
#* iJADE Web-Miner: An Intelligent Agent Framework for Internet Shopping
#@ Raymond S. T. Lee;James N. K. Liu
#t 2004
#c 7
#% 202011
#% 232102
#% 255165
#% 255208
#% 268223
#% 286304
#% 355728
#% 383314
#% 443292
#% 462045
#% 536562
#% 566186
#% 586817
#% 589715
#% 617856
#% 661023
#% 1860660
#! Abstract--There is growing interest in using intelligent software agents for a variety of tasks, including navigating and retrieving information from the Internet and from databases, online shopping activities, user authentication, negotiation for resources, and decision making. This paper proposes an integrated framework for information retrieval and information filtering in the context of Internet shopping. The work focuses on applying agent technology, together with Web mining technology, to automate a series of product search and selection activities. It is based on a multiagent development platform, namely, iJADE (Intelligent Java Agent Development Environment), which supports various e-commerce applications. The framework comprises an automatic facial authentication utility and six other modules, namely, customer requirements definition, a requirement-fuzzification scheme, a fuzzy agents-negotiation scheme, a fuzzy product-selection scheme, a product-defuzzification scheme, and a product-evaluation scheme. A series of experiments were carried out and favorable results were produced in executing the framework. From an experimental point of view, we used a database of 1,020 facial images that were obtained under various conditions of facial expression, viewing perspective and size. An overall correct recognition rate of over 85 percent was attained. For the product selection test of our fuzzy shopper system, an average matching rate of more than 81 percent was achieved.

#index 740766
#* A Strip-Splitting-Based Optimal Algorithm for Decomposing a Query Window into Maximal Quadtree Blocks
#@ Yao-Hong Tsai;Kuo-Liang Chung;Wan-Yu Chen
#t 2004
#c 7
#% 42091
#% 68089
#% 193456
#% 232099
#% 443128
#% 445701
#! Abstract--Decomposing a query window into maximal quadtree blocks is a fundamental problem in quadtree-based spatial database. Recently, Proietti presented the first optimal algorithm for solving this problem. Given a query window of size n_1 \times n_2, Proietti's algorithm takes O(n_l) time, where n_l = max(n_1, n_2). Based on a strip-splitting approach, this paper presents a newoptimal algorithm for solving the same problem. Experimental results reveal that our proposed algorithm is quite competitive with Proietti's algorithm.

#index 740767
#* A Human-Computer Interactive Method for Projected Clustering
#@ Charu C. Aggarwal
#t 2004
#c 7
#% 36672
#% 190611
#% 210173
#% 248790
#% 248792
#% 273891
#% 280511
#% 300131
#% 310517
#% 310526
#% 314054
#% 342613
#% 387914
#% 407822
#% 436509
#% 438134
#% 462243
#% 479962
#% 480132
#% 480307
#% 481281
#% 481290
#% 594012
#% 631984
#% 661010
#! Abstract--Clustering is a central task in data mining applications such as customer segmentation. High-dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Therefore, techniques have recently been proposed to find clusters in hidden subspaces of the data. However, since the behavior of the data can vary considerably in different subspaces, it is often difficult to define the notion of a cluster with the use of simple mathematical formalizations. The widely used practice of treating clustering as the exact problem of optimizing an arbitrarily chosen objective function can often lead to misleading results. In fact, the proper clustering definition may vary not only with the application and data set but also with the perceptions of the end user. This makes it difficult to separate the definition of the clustering problem from the perception of an end-user. In this paper, we propose a system which performs high-dimensional clustering by cooperation between the human and the computer. The complex task of cluster creation is accomplished through a combination of human intuition and the computational support provided by the computer. The result is a system which leverages the best abilities of both the human and the computer for solving the clustering problem.

#index 740768
#* Discovery of Context-Specific Ranking Functions for Effective Information Retrieval Using Genetic Programming
#@ Weiguo Fan;Michael D. Gordon;Praveen Pathak
#t 2004
#c 7
#% 54435
#% 54970
#% 55490
#% 124073
#% 194283
#% 217268
#% 232653
#% 280041
#% 312689
#% 345139
#% 376266
#% 395687
#% 855000
#! Abstract--The Internet and corporate Intranets have brought a lot of information. People usually resort to search engines to find required information. However, these systems tend to use only one fixed ranking strategy regardless of the contexts. This poses serious performance problems when characteristics of different users, queries, and text collections are taken into account. In this paper, we argue that the ranking strategy should be context specific and we propose a new systematic method that can automatically generate ranking strategies for different contexts based on Genetic Programming (GP). The new method was tested on TREC data and the results are very promising.

#index 744786
#* Fourier Domain Scoring: A Novel Document Ranking Method
#@ Laurence A. F. Park;Kotagiri Ramamohanarao;Marimuthu Palaniswami
#t 2004
#c 7
#% 253191
#% 255165
#% 255170
#% 255177
#% 255179
#% 268079
#% 290703
#% 436330
#% 678757
#! Abstract--Current document retrieval methods use a vector space similarity measure to give scores of relevance to documents when related to a specific query. The central problem with these methods is that they neglect any spatial information within the documents in question. We present a new method, called Fourier Domain Scoring (FDS), which takes advantage of this spatial information, via the Fourier transform, to give a more accurate ordering of relevance to a document set. We show that FDS gives an improvement in precision over the vector space similarity measures for the common case of Web like queries, and it gives similar results to the vector space measures for longer queries.

#index 744787
#* A Keyword-Based Semantic Prefetching Approach in Internet News Services
#@ Cheng-Zhong Xu;Tamer I. Ibrahim
#t 2004
#c 7
#% 152939
#% 211739
#% 240011
#% 268184
#% 271420
#% 281251
#% 309777
#% 316139
#% 319666
#% 340296
#% 344447
#% 361854
#% 383790
#% 424281
#% 433879
#% 443296
#% 495944
#% 615723
#% 630264
#% 635923
#% 978362
#% 978380
#% 979694
#% 979706
#! Abstract--Prefetching is an important technique to reduce the average Web access latency. Existing prefetching methods are based mostly on URL graphs. They use the graphical nature of HTTP links to determine the possible paths through a hypertext system. Although the URL graph-based approaches are effective in prefetching of frequently accessed documents, few of them can prefetch those URLs that are rarely visited. This paper presents a keyword-based semantic prefetching approach to overcome the limitation. It predicts future requests based on semantic preferences of past retrieved Web documents. We apply this technique to Internet news services and implement a client-side personalized prefetching system: NewsAgent. The system exploits semantic preferences by analyzing keywords in URL anchor text of previously accessed documents in different news categories. It employs a neural network model over the keyword set to predict future requests. The system features a self-learning capability and good adaptability to the change of client surfing interest. NewsAgent does not exploit keyword synonymy for conservativeness in prefetching. However, it alleviates the impact of keyword polysemy by taking into account server-provided categorical information in decision-making and, hence, captures more semantic knowledge than term-document literal matching methods. Experimental results from daily browsing of ABC News, CNN, and MSNBC news sites for a period of three months show an achievement of up to 60 percent hit ratio due to prefetching.

#index 744788
#* An Expiration Age-Based Document Placement Scheme for Cooperative Web Caching
#@ Lakshmish Ramaswamy;Ling Liu
#t 2004
#c 7
#% 203479
#% 214953
#% 248768
#% 256883
#% 271616
#% 281152
#% 286466
#% 287199
#% 309830
#% 344060
#% 344720
#% 345610
#% 392115
#% 424282
#% 431032
#% 443262
#% 443553
#% 616309
#% 635852
#% 661467
#% 661477
#% 979357
#% 1862985
#! Abstract--The sharing of caches among proxies is an important technique to reduce Web traffic, alleviate network bottlenecks, and improve response time of document requests. Most existing work on cooperative caching has been focused on serving misses collaboratively. Very few have studied the effect of cooperation on document placement schemes and its potential enhancements on cache hit ratio and latency reduction. In this paper, we propose a new document placement scheme which takes into account the contentions at individual caches in order to limit the replication of documents within a cache group and increase document hit ratio. The main idea of this new scheme is to view the aggregate disk space of the cache group as a global resource of the group and uses the concept of cache expiration age to measure the contention of individual caches. The decision of whether to cache a document at a proxy is made collectively among the caches that already have a copy of this document. We refer to this new document placement scheme as the Expiration Age-based scheme (EA scheme for short). The EA scheme effectively reduces the replication of documents across the cache group, while ensuring that a copy of the document always resides in a cache where it is likely to stay for the longest time. We report our study on the potentials and limits of the EA scheme using both analytic modeling and trace-based simulation. The analytical model compares and contrasts the existing (ad hoc) placement scheme of cooperative proxy caches with our new EA scheme and indicates that the EA scheme improves the effectiveness of aggregate disk usage, thereby increasing the average time duration for which documents stay in the cache. The trace-based simulations show that the EA scheme yields higher hit rates and better response times compared to the existing document placement schemes used in most of the caching proxies.

#index 744789
#* On Data Management in Pervasive Computing Environments
#@ Filip Perich;Anupam Joshi;Timothy Finin;Yelena Yesha
#t 2004
#c 7
#% 194472
#% 198038
#% 201897
#% 250311
#% 264263
#% 296229
#% 309461
#% 324163
#% 339004
#% 443127
#% 462060
#% 480647
#% 481291
#% 562178
#% 566138
#% 575370
#% 617331
#% 622728
#! Abstract--This paper presents a framework to address new data management challenges introduced by data-intensive, pervasive computing environments. These challenges include a spatio-temporal variation of data and data source availability, lack of a global catalog and schema, and no guarantee of reconnection among peers due to the serendipitous nature of the environment. An important aspect of our solution is to treat devices as semiautonomous peers guided in their interactions by profiles and context. The profiles are grounded in a semantically rich language and represent information about users, devices, and data described in terms of "beliefs,驴 "desires,驴 and "intentions.驴 We present a prototype implementation of this framework over combined Bluetooth and Ad Hoc 802.11 networks and present experimental and simulation results that validate our approach and measure system performance.

#index 744790
#* Characterizing Web Usage Regularities with Information Foraging Agents
#@ Jiming Liu;Shiwu Zhang;Jie Yang
#t 2004
#c 7
#% 40796
#% 176500
#% 186340
#% 209662
#% 209891
#% 211739
#% 249110
#% 250201
#% 260312
#% 268197
#% 295141
#% 309749
#% 379330
#% 424280
#% 424281
#% 438136
#% 438553
#% 584891
#% 630264
#% 642534
#% 661023
#! Abstract--Researchers have recently discovered several interesting, self-organized regularities from the World Wide Web, ranging from the structure and growth of the Web to the access patterns in Web surfing. What remains to be a great challenge in Web log mining is how to explain user behavior underlying observed Web usage regularities. In this paper, we will address the issue of how to characterize the strong regularities in Web surfing in terms of user navigation strategies, and present an information foraging agent-based approach to describing user behavior. By experimenting with the agent-based decision models of Web surfing, we aim to explain how some Web design factors as well as user cognitive factors may affect the overall behavioral patterns in Web usage.

#index 744791
#* A Wavelet Framework for Adapting Data Cube Views for OLAP
#@ John R. Smith;Chung-Sheng Li;Anant Jhingran
#t 2004
#c 7
#% 116390
#% 181378
#% 210182
#% 227866
#% 227880
#% 248040
#% 259995
#% 287252
#% 461921
#% 462204
#% 464215
#% 479450
#% 481780
#% 481951
#% 626847
#% 1775062
#! Abstract--This article presents a method for adaptively representing multidimensional data cubes using wavelet view elements in order to more efficiently support data analysis and querying involving aggregations. The proposed method decomposes the data cubes into an indexed hierarchy of wavelet view elements. The view elements differ from traditional data cube cells in that they correspond to partial and residual aggregations of the data cube. The view elements provide highly granular building blocks for synthesizing the aggregated and range-aggregated views of the data cubes. We propose a strategy for selectively materializing alternative sets of view elements based on the patterns of access of views. We present a fast and optimal algorithm for selecting a nonexpansive set of wavelet view elements that minimizes the average processing cost for supporting a population of queries of data cube views. We also present a greedy algorithm for allowing the selective materialization of a redundant set of view element sets which, for measured increases in storage capacity, further reduces processing costs. Experiments and analytic results show that the wavelet view element framework performs better in terms of lower processing and storage cost than previous methods that materialize and store redundant views for online analytical processing (OLAP).

#index 744792
#* Reconciling Point-Based and Interval-Based Semantics in Temporal Relational Databases: A Treatment of the Telic/Atelic Distinction
#@ Paolo Terenziani;Richard Thomas Snodgrass
#t 2004
#c 7
#% 18615
#% 23013
#% 32915
#% 43028
#% 47521
#% 71472
#% 111284
#% 139661
#% 209730
#% 213959
#% 264851
#% 286257
#% 287221
#% 308448
#% 319244
#% 335715
#% 340825
#% 442967
#% 443198
#% 443257
#% 459296
#% 462230
#% 467630
#% 481928
#% 631921
#% 742629
#! Abstract--The analysis of the semantics of temporal data and queries plays a central role in the area of temporal databases. Although many different algebræ and models have been proposed, almost all of them are based on a point-based (snapshot) semantics for data. On the other hand, in the areas of linguistics, philosophy, and, recently, artificial intelligence, an oft-debated issue concerns the use of an interval-based versus a point-based semantics. In this paper, we first show some problems inherent in the adoption of a point-based semantics for data, then argue that these problems arise because there is no distinction drawn in the data between telic and atelic facts. We then introduce a three-sorted temporal model and algebra including coercion functions for transforming relations of one sort into relations of the other at query time which properly copes with these issues.

#index 744793
#* A Case Study of Applying Boosting Naive Bayes to Claim Fraud Diagnosis
#@ Stijn Viaene;Richard A. Derrig;Guido Dedene
#t 2004
#c 7
#% 44876
#% 136350
#% 180945
#% 209021
#% 246831
#% 246832
#% 312728
#% 331909
#% 340762
#% 342611
#% 361100
#% 424997
#% 466086
#% 466252
#% 520224
#% 729437
#! Abstract--In this paper, we apply the weight of evidence reformulation of AdaBoosted naive Bayes scoring due to Ridgeway et al. [38] to the problem of diagnosing insurance claim fraud. The method effectively combines the advantages of boosting and the explanatory power of the weight of evidence scoring framework. We present the results of an experimental evaluation with an emphasis on discriminatory power, ranking ability, and calibration of probability estimates. The data to which we apply the method consists of closed personal injury protection (PIP) automobile insurance claims from accidents that occurred in Massachusetts during 1993 and were previously investigated for suspicion of fraud by domain experts. The data mimic the most commonly occurring data configuration--that is, claim records consisting of information pertaining to several binary fraud indicators. The findings of the study reveal the method to be a valuable contribution to the design of intelligible, accountable, and efficient fraud detection support.

#index 744794
#* A Selectivity Model for Fragmented Relations: Applied in Information Retrieval
#@ Henk Ernst Blok;Sunil Choenni;Henk M. Blanken;Peter M. G. Apers
#t 2004
#c 7
#% 82346
#% 85086
#% 172902
#% 201921
#% 210188
#% 210190
#% 248821
#% 273908
#% 320113
#% 321250
#% 342682
#% 463873
#% 479452
#% 479623
#% 479795
#% 479816
#% 479967
#% 651606
#! Abstract--New application domains cause today's database sizes to grow rapidly, posing great demands on technology. Data fragmentation facilitates techniques (like distribution, parallelization, and main-memory computing) meeting these demands. Also, fragmentation might help to improve efficient processing of query types such as top {\rm{N}}. Database design and query optimization require a good notion of the costs resulting from a certain fragmentation. Our mathematically derived selectivity model facilitates this. Once its two parameters have been computed based on the fragmentation, after each (though usually infrequent) update, our model can forget the data distribution, resulting in fast and quite good selectivity estimation. We show experimental verification for Zipfian distributed IR databases.

#index 762650
#* Resilient and Coherence Preserving Dissemination of Dynamic Data Using Cooperating Peers
#@ S. Shah;K. Ramamritham;P. Shenoy
#t 2004
#c 7
#% 271622
#% 300179
#% 303712
#% 330581
#% 333969
#% 348124
#% 397355
#% 399551
#% 443298
#% 449097
#% 464228
#% 609890
#% 635804
#% 661477
#% 661478
#% 963655
#% 963886
#% 963887
#% 978365
#% 978376
#% 978378
#% 979357
#% 993975
#% 1015259
#! The focus of our work is to design and build a dynamic data distribution system that is coherence-preserving, i.e., the delivered data must preserve associated coherence requirements (the user-specified bound on tolerable imprecision) and resilient to failures. To this end, we consider a system in which a set of repositories cooperate with each other and the sources, forming a peer-to-peer network. In this system, necessary changes are pushed to the users so that they are automatically informed about changes of interest. We present techniques 1) to determine when to push an update from one repository to another for coherence maintenance, 2) to construct an efficient dissemination tree for propagating changes from sources to cooperating repositories, and 3) to make the system resilient to failures. An experimental evaluation using real world traces of dynamically changing data demonstrates that 1) careful dissemination of updates through a network of cooperating repositories can substantially lower the cost of coherence maintenance, 2) unless designed carefully, even push-based systems experience considerable loss in fidelity due to message delays and processing costs, 3) the computational and communication cost of achieving resiliency can be made to be low, and 4) surprisingly, adding resiliency can actually improve fidelity even in the absence of failures.

#index 762651
#* Efficient Semantic-Based Content Search in P2P Network
#@ H. T. Shen;Y. Shu;B. Yu
#t 2004
#c 7
#% 18616
#% 248027
#% 340175
#% 340176
#% 479649
#% 480647
#% 523066
#% 636008
#! Most existing Peer-to-Peer (P2P) systems support only title-based searches and are limited in functionality when compared to today's search engines. In this paper, we present the design of a distributed P2P information sharing system that supports semantic-based content searches of relevant documents. First, we propose a general and extensible framework for searching similar documents in P2P network. The framework is based on the novel concept of Hierarchial Summary Structure. Second, based on the framework, we develop our efficient document searching system by effectively summarizing and maintaining all documents within the network with different granularity. Finally, an experimental study is conducted on a real P2P prototype, and a large-scale network is further simulated. The results show the effectiveness, efficiency, and scalability of the proposed system.

#index 762652
#* The Piazza Peer Data Management System
#@ A. Y. Halevy;Z. G. Ives;Jayant Madhavan;P. Mork;D. Suciu;I. Tatarinov
#t 2004
#c 7
#% 85089
#% 102748
#% 111920
#% 123113
#% 191611
#% 210176
#% 229827
#% 237190
#% 248038
#% 248858
#% 263136
#% 283052
#% 309678
#% 340175
#% 340176
#% 342389
#% 348182
#% 397351
#% 479452
#% 480822
#% 481923
#% 496091
#% 519553
#% 572311
#% 577320
#% 577359
#% 723449
#% 1015302
#! Intuitively, data management and data integration tools should be well-suited for exchanging information in a semantically meaningful way. Unfortunately, they suffer from two significant problems: They typically require a comprehensive schema design before they can be used to store or share information and they are difficult to extend because schema evolution is heavyweight and may break backward compatibility. As a result, many small-scale data sharing tasks are more easily facilitated by non-database-oriented tools that have little support for semantics. The goal of the peer data management system (PDMS) is to address this need: We propose the use of a decentralized, easily extensible data management architecture in which any user can contribute new data, schema information, or even mappings between other peers' schemas. PDMSs represent a natural step beyond data integration systems, replacing their single logical schema with an interlinked collection of semantic mappings between peers' individual schemas. This paper describes several aspects of the Piazza PDMS, including the schema mediation formalism, query answering and optimization algorithms, and the relevance of PDMSs to the Semantic Web.

#index 762653
#* Guest Editors' Introduction: Special Section on Peer-to-Peer-Based Data Management
#@ Beng Chin Ooi;Kian-Lee Tan
#t 2004
#c 7

#index 762654
#* Trust-X: A Peer-to-Peer Framework for Trust Establishment
#@ E. Bertino;E. Ferrari;A. C. Squicciarini
#t 2004
#c 7
#% 252552
#% 314762
#% 429211
#% 453783
#% 572301
#% 613796
#% 663863
#% 664696
#! In this paper, we present {\rm{Trust}}{\hbox{-}}{\cal{X}}, a comprehensive XML-based [12] framework for trust negotiations, specifically conceived for a peer-to-peer environment. Trust negotiation is a promising approach for establishing trust in open systems like the Internet, where sensitive interactions may often occur between entities at first contact, with no prior knowledge of each other. The framework we propose takes into account all aspects related to negotiations, from the specification of the profiles and policies of the involved parties to the selection of the best strategy to succeed in the negotiation. {\rm{Trust}}{\hbox{-}}{\cal{X}} presents a number of innovative features, such as the support for protection of sensitive policies, the use of trust tickets to speed up the negotiation, and the support of different strategies to carry on a negotiation. In this paper, besides presenting the language to encode security information, we present the system architecture and algorithms according to which negotiations can take place.

#index 762655
#* PeerTrust: Supporting Reputation-Based Trust for Peer-to-Peer Electronic Communities
#@ Li Xiong;Ling Liu
#t 2004
#c 7
#% 314935
#% 316798
#% 340175
#% 340176
#% 341927
#% 341929
#% 342695
#% 348160
#% 378939
#% 378940
#% 378980
#% 378981
#% 420444
#% 433833
#% 433977
#% 505869
#% 543383
#% 577367
#% 580555
#% 607998
#% 631886
#% 739578
#% 1388581
#! Peer-to-peer (P2P) online communities are commonly perceived as an environment offering both opportunities and threats. One way to minimize threats in such communities is to use community-based reputations to help estimate the trustworthiness of peers. This paper presents PeerTrust驴a reputation-based trust supporting framework, which includes a coherent adaptive trust model for quantifying and comparing the trustworthiness of peers based on a transaction-based feedback system, and a decentralized implementation of such a model over a structured P2P network. PeerTrust model has two main features. First, we introduce three basic trust parameters and two adaptive factors in computing trustworthiness of peers, namely, feedback a peer receives from other peers, the total number of transactions a peer performs, the credibility of the feedback sources, transaction context factor, and the community context factor. Second, we define a general trust metric to combine these parameters. Other contributions of the paper include strategies used for implementing the trust model in a decentralized P2P environment, evaluation mechanisms to validate the effectiveness and cost of PeerTrust model, and a set of experiments that show the feasibility and benefit of our approach.

#index 762656
#* Efficient, Self-Contained Handling of Identity in Peer-to-Peer Systems
#@ K. Aberer;A. Datta;M. Hauswirth
#t 2004
#c 7
#% 119621
#% 232640
#% 268128
#% 284593
#% 340175
#% 340176
#% 342695
#% 343082
#% 381550
#% 384121
#% 397441
#% 401985
#% 433980
#% 433982
#% 496158
#% 496285
#% 505869
#% 590427
#% 636116
#% 646237
#% 646238
#% 674136
#! Identification is an essential building block for many services in distributed information systems. The quality and purpose of identification may differ, but the basic underlying problem is always to bind a set of attributes to an identifier in a unique and deterministic way. Name/directory services, such as DNS, X.500, or UDDI, are a well-established concept to address this problem in distributed information systems. However, none of these services addresses the specific requirements of peer-to-peer systems with respect to dynamism, decentralization, and maintenance. We propose the implementation of directories using a structured peer-to-peer overlay network and apply this approach to support self-contained maintenance of routing tables with dynamic IP addresses in structured P2P systems. Thus, we can keep routing tables intact without affecting the organization of the overlay networks, making it logically independent of the underlying network infrastructure. Even though the directory is self-referential, since it uses its own service to maintain itself, we show that it is robust due to a self-healing capability. For security, we apply a combination of PGP-like public key distribution and a quorum-based query scheme. We describe the algorithm as implemented in the P-Grid P2P lookup system (http://www.p-grid.org/) and give a detailed analysis and simulation results demonstrating the efficiency and robustness of our approach.

#index 762657
#* Main Memory Indexing: The Case for BD-Tree
#@ B. Cui;B. C. Ooi;J. Su;K. -L. Tan
#t 2004
#c 7
#% 252458
#% 300194
#% 321250
#% 333942
#% 333949
#% 397362
#% 464012
#% 479769
#% 480119
#! In this paper, we adapt and optimize the BD-tree for main memory data processing. We compare the memory-based BD-tree against the {\rm{B}}^+{\hbox{-}}{\rm{tree}} and {\rm{CSB}}^+{\hbox{-}}{\rm{tree}}. We present cost models for exact match query for these indexes, including L2 cache and translation lookahead buffer (TLB) miss model and execution time model. We also implemented these structures and conducted experimental study. Our analytical and experimental results show that a well-tuned BD-tree is superior in most cases.

#index 762658
#* A Support-Ordered Trie for Fast Frequent Itemset Discovery
#@ Y. -K. Woon;W. -K. Ng;E. -P. Lim
#t 2004
#c 7
#% 70370
#% 251693
#% 273898
#% 300120
#% 300124
#% 314933
#% 342716
#% 443164
#% 443350
#% 462238
#% 481290
#% 482521
#% 588067
#% 665648
#! The importance of data mining is apparent with the advent of powerful data collection and storage tools; raw data is so abundant that manual analysis is no longer possible. Unfortunately, data mining problems are difficult to solve and this prompted the introduction of several novel data structures to improve mining efficiency. Here, we will critically examine existing preprocessing data structures used in association rule mining for enhancing performance in an attempt to understand their strengths and weaknesses. Our analyses culminate in a practical structure called the SOTrieIT (Support-Ordered Trie Itemset) and two synergistic algorithms to accompany it for the fast discovery of frequent itemsets. Experiments involving a wide range of synthetic data sets reveal that its algorithms outperform FP-growth, a recent association rule mining algorithm with excellent performance, by up to two orders of magnitude and, thus, verifying its efficiency and viability.

#index 766663
#* Influential Rule Search Scheme (IRSS)-A New Fuzzy Pattern Classifier
#@ Amitava Chatterjee;Anjan Rakshit
#t 2004
#c 7
#% 112232
#% 160873
#% 207002
#% 221731
#% 310041
#% 357637
#% 443124
#% 1780563
#% 1780827
#% 1780863
#% 1780877
#% 1780908
#% 1781055
#% 1787877
#% 1787878
#% 1787911
#% 1788024
#% 1788035
#% 1788077
#% 1788191
#% 1788920
#% 1860292
#% 1860644
#% 1860707
#! Automatic generation of fuzzy rule base and membership functions from an input-output data set, for reliable construction of an adaptive fuzzy inference system, has become an important area of research interest. The present paper proposes a new robust, fast acting adaptive fuzzy pattern classification scheme, named influential rule search scheme (IRSS). In IRSS, rules which are most influential in contributing to the error produced by the adaptive fuzzy system are identified at the end of each epoch and subsequently modified for satisfactory performance. This fuzzy rule base adjustment scheme is accompanied by an output membership function adaptation scheme for fine tuning the fuzzy system architecture. This iterative method has shown a relatively high speed of convergence. Performance of the proposed IRSS is compared with other existing pattern classification schemes by implementing it for Fisher's iris data problem and Wisconsin breast cancer data problems.

#index 766664
#* Learning Functions Using Randomized Genetic Code-Like Transformations: Probabilistic Properties and Experimentations
#@ Hillol Kargupta;Rajeev Ayyagari;Samiran Ghosh
#t 2004
#c 7
#% 91386
#% 114994
#% 190581
#% 216060
#% 309208
#% 421184
#% 449588
#% 684841
#% 715964
#% 835998
#! Inductive learning of nonlinear functions plays an important role in constructing predictive models and classifiers from data. This paper explores a novel randomized approach to construct linear representations of nonlinear functions proposed elsewhere [CHECK END OF SENTENCE], [CHECK END OF SENTENCE]. This approach makes use of randomized codebooks, called the Genetic Code-Like Transformations (GCTs) for constructing an approximately linear representation of a nonlinear target function. This paper first derives some of the results presented elsewhere [CHECK END OF SENTENCE] in a more general context. Next, it investigates different probabilistic and limit properties of GCTs. It also presents several experimental results to demonstrate the potential of this approach.

#index 766665
#* Efficient Disk-Based K-Means Clustering for Relational Databases
#@ Carlos Ordonez;Edward Omiecinski
#t 2004
#c 7
#% 3888
#% 36683
#% 152934
#% 169358
#% 210173
#% 216500
#% 248790
#% 248792
#% 248813
#% 273891
#% 278011
#% 280419
#% 280463
#% 280521
#% 300120
#% 300131
#% 300213
#% 304945
#% 320942
#% 333929
#% 333933
#% 386381
#% 413619
#% 420081
#% 430881
#% 464998
#% 479962
#% 480153
#% 480812
#% 481281
#% 481290
#% 631985
#% 662751
#% 729437
#! K-means is one of the most popular clustering algorithms. This article introduces an efficient disk-based implementation of K-means. The proposed algorithm is designed to work inside a relational database management system. It can cluster large data sets having very high dimensionality. In general, it only requires three scans over the data set. It is optimized to perform heavy disk I/O and its memory requirements are low. Its parameters are easy to set. An extensive experimental section evaluates quality of results and performance. The proposed algorithm is compared against the Standard K-means algorithm as well as the Scalable K-means algorithm.

#index 766666
#* Mining Constrained Gradients in Large Databases
#@ Guozhu Dong;Jiawei Han;Joyce M. W. Lam;Jian Pei;Ke Wang;Wei Zou
#t 2004
#c 7
#% 152934
#% 210182
#% 223781
#% 227880
#% 248785
#% 273916
#% 280409
#% 280458
#% 300120
#% 333925
#% 420053
#% 420141
#% 459025
#% 464989
#% 479450
#% 479795
#% 480820
#% 481290
#% 481951
#% 632028
#! Many data analysis tasks can be viewed as search or mining in a multidimensional space (MDS). In such MDSs, dimensions capture potentially important factors for given applications, and cells represent combinations of values for the factors. To systematically analyze data in MDS, an interesting notion, called "cubegrade驴 was recently introduced by Imielinski et al. [CHECK END OF SENTENCE], which focuses on the notable changes in measures in MDS by comparing a cell (which we refer to as probe cell) with its gradient cells, namely, its ancestors, descendants, and siblings. We call such queries gradient analysis queries (GQs). Since an MDS can contain billions of cells, it is important to answer GQs efficiently. In this study, we focus on developing efficient methods for mining GQs constrained by certain (weakly) antimonotone constraints. Instead of conducting an independent gradient-cell search once per probe cell, which is inefficient due to much repeated work, we propose an efficient algorithm, LiveSet-Driven. This algorithm finds all good gradient-probe cell pairs in one search pass. It utilizes measure-value analysis and dimension-match analysis in a set-oriented manner, to achieve bidirectional pruning between the sets of hopeful probe cells and of hopeful gradient cells. Moreover, it adopts a hypertree structure and an H-cubing method to compress data and to maximize sharing of computation. Our performance study shows that this algorithm is efficient and scalable. In addition to data cubes, we extend our study to another important scenario: mining constrained gradients in transactional databases where each item is associated with some measures such as price. Such transactional databases can be viewed as sparse MDSs where items represent dimensions, although they have significantly different characteristics than data cubes. We outline efficient mining methods for this problem in this paper.

#index 766667
#* Privacy: A Machine Learning View
#@ Staal A. Vinterbo
#t 2004
#c 7
#% 107046
#% 234979
#% 248030
#% 287794
#% 300184
#% 329858
#% 333876
#% 388196
#% 408396
#% 442782
#% 443382
#% 449588
#% 478630
#% 577233
#% 577239
#% 577289
#% 586838
#% 635219
#! The problem of disseminating a data set for machine learning while controlling the disclosure of data source identity is described using a commuting diagram of functions. This formalization is used to present and analyze an optimization problem balancing privacy and data utility requirements. The analysis points to the application of a generalization mechanism for maintaining privacy in view of machine learning needs. We present new proofs of NP-hardness of the problem of minimizing information loss while satisfying a set of privacy requirements, both with and without the addition of a particular uniform coding requirement. As an initial analysis of the approximation properties of the problem, we show that the cell suppression problem with a constant number of attributes can be approximated within a constant. As a side effect, proofs of NP-hardness of the minimum k{\hbox{-}}{\rm{union}}, maximum k{\hbox{-}}{\rm{intersection}}, and parallel versions of these are presented. Bounded versions of these problems are also shown to be approximable within a constant.

#index 766668
#* TopCat: Data Mining for Topic Identification in a Text Corpus
#@ Chris Clifton;Robert Cooley;Jason Rennie
#t 2004
#c 7
#% 78171
#% 99690
#% 109222
#% 152934
#% 157710
#% 165115
#% 210986
#% 236701
#% 239588
#% 240197
#% 248784
#% 281186
#% 287218
#% 287285
#% 293974
#% 309131
#% 406493
#% 420073
#% 458379
#% 465754
#% 477627
#% 481290
#% 481758
#% 571073
#% 742446
#! TopCat (Topic Categories) is a technique for identifying topics that recur in articles in a text corpus. Natural language processing techniques are used to identify key entities in individual articles, allowing us to represent an article as a set of items. This allows us to view the problem in a database/data mining context: Identifying related groups of items. This paper presents a novel method for identifying related items based on traditional data mining techniques. Frequent itemsets are generated from the groups of items, followed by clusters formed with a hypergraph partitioning scheme. We present an evaluation against a manually categorized ground truth news corpus; it shows this technique is effective in identifying topics in collections of news articles.

#index 766669
#* An Efficient Algorithm to Compute Differences between Structured Documents
#@ Kyong-Ho Lee;Yoon-Chul Choy;Sung-Bae Cho
#t 2004
#c 7
#% 66654
#% 84549
#% 84755
#% 90847
#% 151389
#% 151393
#% 210212
#% 227859
#% 227998
#% 266736
#% 288885
#% 368174
#% 394417
#% 481130
#% 481931
#% 484421
#% 599921
#% 641022
#% 659923
#! SGML/XML are having a profound impact on data modeling and processing. This paper presents an efficient algorithm to compute differences between old and new versions of an SGML/XML document. The difference between the two versions can be considered to be an edit script that transforms one document tree into another. The proposed algorithm is based on a hybridization of bottom-up and top-down methods: The matching relationships between nodes in the two versions are produced in a bottom-up manner and then the top-down breadth-first search computes an edit script. Faster matching is achieved because the algorithm does not need to investigate the possible existence of matchings for all nodes. Furthermore, it can detect structurally meaningful changes such as the movement and copy of a subtree as well as simple changes to the node itself like insertion, deletion, and update.

#index 766670
#* Multistrategy Ensemble Learning: Reducing Error by Combining Ensemble Learning Techniques
#@ Geoffrey I. Webb;Zijian Zheng
#t 2004
#c 7
#% 136350
#% 209021
#% 235377
#% 312727
#% 312728
#% 424997
#% 486474
#% 565528
#% 702456
#% 837668
#% 1272280
#! Ensemble learning strategies, especially Boosting and Bagging decision trees, have demonstrated impressive capacities to improve the prediction accuracy of base learning algorithms. Further gains have been demonstrated by strategies that combine simple ensemble formation approaches. In this paper, we investigate the hypothesis that the improvement in accuracy of multistrategy approaches to ensemble learning is due to an increase in the diversity of ensemble members that are formed. In addition, guided by this hypothesis, we develop three new multistrategy ensemble learning techniques. Experimental results in a wide variety of natural domains suggest that these multistrategy ensemble learning techniques are, on average, more accurate than their component ensemble learning techniques.

#index 766671
#* Optimizing Top-k Selection Queries over Multimedia Repositories
#@ Surajit Chaudhuri;Luis Gravano;Amelie Marian
#t 2004
#c 7
#% 554
#% 67565
#% 71573
#% 77648
#% 77659
#% 108508
#% 132779
#% 164360
#% 172931
#% 210172
#% 213981
#% 227894
#% 238917
#% 248010
#% 249985
#% 278831
#% 287349
#% 287461
#% 287463
#% 318785
#% 333854
#% 387427
#% 397378
#% 411554
#% 411751
#% 443243
#% 464726
#% 479623
#% 479816
#% 479938
#% 479967
#% 480944
#% 591565
#% 614579
#% 659993
#! Repositories of multimedia objects having multiple types of attributes (e.g., image, text) are becoming increasingly common. A query on these attributes will typically request not just a set of objects, as in the traditional relational query model (filtering), but also a grade of match associated with each object, which indicates how well the object matches the selection condition (ranking). Furthermore, unlike in the relational model, users may just want the k top-ranked objects for their selection queries for a relatively small k. In addition to the differences in the query model, another peculiarity of multimedia repositories is that they may allow access to the attributes of each object only through indexes. In this paper, we investigate how to optimize the processing of top-k selection queries over multimedia repositories. The access characteristics of the repositories and the above query model lead to novel issues in query optimization. In particular, the choice of the indexes used to search the repository strongly influences the cost of processing the filtering condition. We define an execution space that is search-minimal, i.e., the set of indexes searched is minimal. Although the general problem of picking an optimal plan in the search-minimal execution space is NP-hard, we present an efficient algorithm that solves the problem optimally with respect to our cost model and execution space when the predicates in the query are independent. We also show that the problem of optimizing top-k selection queries can be viewed, in many cases, as that of evaluating more traditional selection conditions. Thus, both problems can be viewed together as an extended filtering problem to which techniques of query processing and optimization may be adapted.

#index 766672
#* Automatic Control of Workflow Processes Using ECA Rules
#@ Joonsoo Bae;Hyerim Bae;Suk-Ho Kang;Yeongho Kim
#t 2004
#c 7
#% 86939
#% 122914
#% 169054
#% 185412
#% 231760
#% 252361
#% 261273
#% 264781
#% 279152
#% 281974
#% 281980
#% 302433
#% 307270
#% 318491
#% 333653
#% 385995
#% 433850
#% 434336
#% 434393
#% 443064
#% 445945
#% 562058
#% 572302
#% 1841557
#! Changes in recent business environments have created the necessity for a more efficient and effective business process management. The workflow management system is software that assists in defining business processes as well as automatically controlling the execution of the processes. This paper proposes a new approach to the automatic execution of business processes using Event-Condition-Action (ECA) rules that can be automatically triggered by an active database. First of all, we propose the concept of blocks that can classify process flows into several patterns. A block is a minimal unit that can specify the behaviors represented in a process model. An algorithm is developed to detect blocks from a process definition network and transform it into a hierarchical tree model. The behaviors in each block type are modeled using ACTA formalism. This provides a theoretical basis from which ECA rules are identified. The proposed ECA rule-based approach shows that it is possible to execute the workflow using the active capability of database without users' intervention. The operation of the proposed methods is illustrated through an example process.

#index 772829
#* Privacy-Preserving Distributed Mining of Association Rules on Horizontally Partitioned Data
#@ Murat Kantarcioglu;Chris Clifton
#t 2004
#c 7
#% 31041
#% 142338
#% 169589
#% 261357
#% 300184
#% 301569
#% 319994
#% 333876
#% 340291
#% 443085
#% 481290
#% 577233
#% 577289
#% 993988
#! Data mining can extract important knowledge from large data collections驴but sometimes these collections are split among various parties. Privacy concerns may prevent the parties from directly sharing the data and some types of information about the data. This paper addresses secure mining of association rules over horizontally partitioned data. The methods incorporate cryptographic techniques to minimize the information shared, while adding little overhead to the mining task.

#index 772830
#* An Efficient Algorithm for Discovering Frequent Subgraphs
#@ Michihiro Kuramochi;George Karypis
#t 2004
#c 7
#% 184048
#% 207023
#% 286671
#% 300120
#% 300124
#% 320944
#% 329598
#% 342604
#% 408396
#% 420088
#% 443133
#% 443350
#% 445369
#% 449508
#% 459006
#% 464996
#% 466644
#% 481290
#% 481754
#% 543962
#% 550398
#% 550412
#% 577218
#% 577219
#% 625172
#% 629603
#% 629708
#% 631986
#% 1273674
#! Over the years, frequent itemset discovery algorithms have been used to find interesting patterns in various application areas. However, as data mining techniques are being increasingly applied to nontraditional domains, existing frequent pattern discovery approaches cannot be used. This is because the transaction framework that is assumed by these algorithms cannot be used to effectively model the data sets in these domains. An alternate way of modeling the objects in these data sets is to represent them using graphs. Within that model, one way of formulating the frequent pattern discovery problem is that of discovering subgraphs that occur frequently over the entire set of graphs. In this paper, we present a computationally efficient algorithm, called FSG, for finding all frequent subgraphs in large graph data sets. We experimentally evaluate the performance of FSG using a variety of real and synthetic data sets. Our results show that despite the underlying complexity associated with frequent subgraph discovery, FSG is effective in finding all frequently occurring subgraphs in data sets containing more than 200,000 graph transactions and scales linearly with respect to the size of the data set.

#index 772831
#* Mining Frequent Itemsets without Support Threshold: With and without Item Constraints
#@ Yin-Ling Cheung;Ada Wai-Chee Fu
#t 2004
#c 7
#% 172386
#% 201894
#% 227917
#% 248792
#% 300120
#% 300124
#% 342643
#% 399793
#% 399794
#% 461909
#% 464989
#% 479484
#% 480154
#% 481290
#% 481754
#% 632028
#! In classical association rules mining, a minimum support threshold is assumed to be available for mining frequent itemsets. However, setting such a threshold is typically hard. In this paper, we handle a more practical problem; roughly speaking, it is to mine N k-itemsets with the highest supports for k up to a certain k_{max} value. We call the results the N-most interesting itemsets. Generally, it is more straightforward for users to determine N and k_{max}. We propose two new algorithms, LOOPBACK and BOMO. Experiments show that our methods outperform the previously proposed Itemset-Loop algorithm, and the performance of BOMO can be an order of magnitude better than the original FP-tree algorithm, even with the assumption of an optimally chosen support threshold. We also propose the mining of "N-most interesting k-itemsets with item constraints.驴 This allows user to specify different degrees of interestingness for different itemsets. Experiments show that our proposed Double FP-trees algorithm, which is based on BOMO, is highly efficient in solving this problem.

#index 772832
#* Correct Execution of Transactions at Different Isolation Levels
#@ Shiyong Lu;Arthur Bernstein;Philip Lewis
#t 2004
#c 7
#% 117082
#% 201869
#% 213205
#% 295866
#% 302434
#% 318247
#% 320902
#% 464814
#% 632092
#% 645633
#! Many transaction processing applications execute at isolation levels lower than SERIALIZABLE in order to increase throughput and reduce response time. However, the resulting schedules might not be serializable and, hence, not necessarily correct. The semantics of a particular application determines whether that application will run correctly at a lower level and, in practice, it appears that many applications do. The decision to choose an isolation level at which to run an application and the analysis of the correctness of the resulting execution is usually done informally. In this paper, we develop a formal technique to analyze and reason about the correctness of the execution of an application at isolation levels other than SERIALIZABLE. We use a new notion of correctness, semantic correctness, a criterion weaker than serializability, to investigate correctness. In particular, for each isolation level, we prove a condition under which the execution of transactions at that level will be semantically correct. In addition to the ANSI/ISO isolation levels of READ UNCOMMITTED, READ COMMITTED, and REPEATABLE READ, we also prove a condition for correct execution at the READ-COMMITTED with first-committer-wins and at SNAPSHOT isolation. We assume that different transactions in the same application can be executing at different levels, but that each transaction is executing at least at READ UNCOMMITTED.

#index 772833
#* Distributed and Reactive Query Planning in R-MAGIC: An Agent-Based Multimedia Retrieval System
#@ Hiranmay Ghosh;Santanu Chaudhury
#t 2004
#c 7
#% 1017
#% 67866
#% 94245
#% 120104
#% 120270
#% 188519
#% 207803
#% 210388
#% 238916
#% 331769
#% 336069
#% 341300
#% 431517
#% 434908
#% 435065
#% 435067
#% 435068
#% 437405
#% 443241
#% 443246
#% 443266
#% 557050
#% 592155
#% 718454
#% 1830410
#! This paper presents the planning scheme for a cooperative agent-based multimedia retrieval architecture that integrates a heterogeneous set of repositories into a coherent information system. The agents in the system collaborate in context of a conceptual query to formulate unique retrieval strategies for the different collections. The retrieval plan makes need-based use of independent content analysis tools available on the network. The retrieval strategies for the repositories so formulated satisfy the specified constraints on quality of results and the response time requirements. The retrieval plan is reactively updated based on the retrieval performance at the individual repositories. We present some experimental results to show the effectiveness of the planning scheme for repositories with different characteristics and the scalability of the architecture. We present a prototype implementation of this architecture that integrates a set of dissimilar collections of multimedia data on Indian cultural heritage. A comparison of the retrieval results with some existing Internet search tools proves the effectiveness of the architecture.

#index 772834
#* Incremental Maintenance of Schema-Restructuring Views in SchemaSQL
#@ Andreas Koeller;Elke A. Rundensteiner
#t 2004
#c 7
#% 13016
#% 64148
#% 66204
#% 102748
#% 152928
#% 201928
#% 201929
#% 201976
#% 210210
#% 213969
#% 227947
#% 227989
#% 248800
#% 252374
#% 342957
#% 442767
#% 479968
#% 480969
#% 481944
#% 511897
#% 562199
#% 564202
#! The integration of data, especially from heterogeneous sources, is a hard and widely studied problem. One particularly challenging issue is the integration of sources that are semantically equivalent but schematically heterogeneous. While two such data sources may represent the same information, one may store the information inside tuples (data) while the other may store it in attribute or relation names (schema). The SchemaSQL query language is a recent solution to this problem powerful enough to restructure such sources into each other without the loss of information. In this paper, we propose the first incremental view maintenance strategy for such schema-restructuring views. Our strategy, based on an algebraic representation of the view query, correctly transforms a data update or a schema change to a source into sequences of schema and data updates to be applied to the view. We also introduce an optimization of incremental maintenance using batching. We present a proof of correctness of the propagation approach. We also describe the implementation of our SchemaSQL Query Processor and View Maintainer. Last, our experimental results demonstrate that, in many cases, incremental SchemaSQL view maintenance is significantly faster than complete view recomputation.

#index 772835
#* Querying Imprecise Data in Moving Object Environments
#@ Reynold Cheng;Dmitri V. Kalashnikov;Sunil Prabhakar
#t 2004
#c 7
#% 201876
#% 273706
#% 295512
#% 299979
#% 335047
#% 442615
#% 458849
#% 458853
#% 458857
#% 461923
#% 464859
#% 480473
#% 503882
#% 527176
#% 527187
#% 564133
#! In moving object environments, it is infeasible for the database tracking the movement of objects to store the exact locations of objects at all times. Typically, the location of an object is known with certainty only at the time of the update. The uncertainty in its location increases until the next update. In this environment, it is possible for queries to produce incorrect results based upon old data. However, if the degree of uncertainty is controlled, then the error of the answers to queries can be reduced. More generally, query answers can be augmented with probabilistic estimates of the validity of the answer. In this paper, we study the execution of probabilistic range and nearest-neighbor queries. The imprecision in answers to queries is an inherent property of these applications due to uncertainty in data, unlike the techniques for approximate nearest-neighbor processing that trade accuracy for performance. Algorithms for computing these queries are presented for a generic object movement model and detailed solutions are discussed for two common models of uncertainty in moving object databases. We study the performance of these queries through extensive simulations.

#index 772836
#* Workflow Mining: Discovering Process Models from Event Logs
#@ Wil van der Aalst;Ton Weijters;Laura Maruster
#t 2004
#c 7
#% 68187
#% 187253
#% 258498
#% 259602
#% 274098
#% 284600
#% 318041
#% 346653
#% 420063
#% 459021
#% 480644
#% 487125
#% 487251
#% 495976
#% 541753
#% 542161
#% 543891
#% 543959
#% 589166
#% 994005
#! Contemporary workflow management systems are driven by explicit process models, i.e., a completely specified workflow design is required in order to enact a given workflow process. Creating a workflow design is a complicated time-consuming process and, typically, there are discrepancies between the actual workflow processes and the processes as perceived by the management. Therefore, we have developed techniques for discovering workflow models. The starting point for such techniques is a so-called "workflow log驴 containing information about the workflow process as it is actually being executed. We present a new algorithm to extract a process model from such a log and represent it in terms of a Petri net. However, we will also demonstrate that it is not possible to discover arbitrary workflow processes. In this paper, we explore a class of workflow processes that can be discovered. We show that the \alpha{\hbox{-}}{\rm{algorithm}} can successfully mine any workflow represented by a so-called SWF-net.

#index 772837
#* Dependent Data Broadcasting for Unordered Queries in a Multiple Channel Mobile Environment
#@ Jiun-Long Huang;Ming-Syan Chen
#t 2004
#c 7
#% 247246
#% 259632
#% 259634
#% 274209
#% 287258
#% 305097
#% 309463
#% 316491
#% 342711
#% 369236
#% 422944
#% 430425
#% 443127
#% 452850
#% 536178
#% 575108
#% 575109
#% 632025
#% 632067
#% 635906
#% 635989
#! Data broadcast is a promising technique to improve the bandwidth utilization and conserve the power consumption in a mobile computing environment. In many applications, the data items broadcast are dependent upon one another. However, most prior studies on broadcasting dependent data are restricted to a single broadcast channel environment, and as a consequence, the results are of limited applicability to the upcoming mobile environments. In view of this, we relax this restriction and explore in this paper the problem of broadcasting dependent data in multiple broadcast channels. By analyzing the model of dependent data broadcasting, we derive several theoretical properties for the average access time in a multiple channel environment. In light of the theoretical results, we develop a genetic algorithm to generate broadcast programs. Our experimental results show that the theoretical results derived are able to guide the search of the genetic algorithm very effectively, thus leading to broadcast programs of very high quality.

#index 772838
#* Metadata for Anomaly-Based Security Protocol Attack Deduction
#@ Tysen Leckie;Alec Yasinsac
#t 2004
#c 7
#% 65961
#% 115763
#% 288885
#% 315636
#% 340031
#% 423636
#% 583731
#% 592668
#% 653946
#% 664506
#% 978252
#% 978636
#% 1001828
#! Anomaly-based Intrusion Detection Systems (IDS) have been widely recognized for their potential to prevent and reduce damage to information systems. In order to build their profiles and to generate their requisite behavior observations, these systems rely on access to payload data, either in the network or on the host system. With the growing reliance on encryption technology, less and less payload data is available for analysis. In order to accomplish intrusion detection in an encrypted environment, a new data representation must emerge. In this paper, we present a knowledge engineering approach to allow intrusion detection in an encrypted environment. Our approach relies on gathering and analyzing several forms of metadata relating to session activity of the principals involved and the protocols that they employ. We then apply statistical and pattern recognition methods to the metadata to distinguish between normal and abnormal activity and then to distinguish between legitimate and malicious behavior.

#index 772839
#* An Efficient Cost Model for Optimization of Nearest Neighbor Search in Low and Medium Dimensional Spaces
#@ Yufei Tao;Jun Zhang;Dimitris Papadias;Nikos Mamoulis
#t 2004
#c 7
#% 86950
#% 164360
#% 172949
#% 198573
#% 201876
#% 213975
#% 237187
#% 248017
#% 248822
#% 273887
#% 273903
#% 287466
#% 300162
#% 300193
#% 317313
#% 317380
#% 318703
#% 443396
#% 464859
#% 479649
#% 480133
#% 481956
#% 482109
#% 527328
#% 632043
#% 993966
#! Existing models for nearest neighbor search in multidimensional spaces are not appropriate for query optimization because they either lead to erroneous estimation or involve complex equations that are expensive to evaluate in real-time. This paper proposes an alternative method that captures the performance of nearest neighbor queries using approximation. For uniform data, our model involves closed formulae that are very efficient to compute and accurate for up to 10 dimensions. Further, the proposed equations can be applied on nonuniform data with the aid of histograms. We demonstrate the effectiveness of the model by using it to solve several optimization problems related to nearest neighbor search.

#index 772840
#* Content-Based Image Retrieval Based on a Fuzzy Approach
#@ Raghu Krishnapuram;Swarup Medasani;Sung-Hwan Jung;Young-Sik Choi;Rajesh Balasubramaniam
#t 2004
#c 7
#% 8164
#% 40313
#% 67565
#% 68091
#% 117077
#% 120270
#% 181409
#% 208708
#% 212690
#% 219847
#% 223567
#% 227481
#% 238912
#% 261856
#% 279094
#% 279098
#% 284557
#% 318785
#% 359751
#% 375388
#% 388500
#% 407995
#% 437404
#% 437409
#% 443133
#% 457299
#% 589735
#% 589738
#% 589910
#% 589924
#% 589926
#% 626558
#% 1180245
#% 1788159
#% 1788916
#! A typical content-based image retrieval (CBIR) system would need to handle the vagueness in the user queries as well as the inherent uncertainty in image representation, similarity measure, and relevance feedback. In this paper, we discuss how fuzzy set theory can be effectively used for this purpose and describe an image retrieval system called FIRST (Fuzzy Image Retrieval SysTem) which incorporates many of these ideas. FIRST can handle exemplar-based, graphical-sketch-based, as well as linguistic queries involving region labels, attributes, and spatial relations. FIRST uses Fuzzy Attributed Relational Graphs (FARGs) to represent images, where each node in the graph represents an image region and each edge represents a relation between two regions. The given query is converted to a FARG, and a low-complexity fuzzy graph matching algorithm is used to compare the query graph with the FARGs in the database. The use of an indexing scheme based on a leader clustering algorithm avoids an exhaustive search of the FARG database. We quantify the retrieval performance of the system in terms of several standard measures.

#index 772841
#* Managing Deadline Miss Ratio and Sensor Data Freshness in Real-Time Databases
#@ Kyoung-Don Kang;Sang H. Son;John A. Stankovic
#t 2004
#c 7
#% 117902
#% 117903
#% 149495
#% 158051
#% 170893
#% 184563
#% 194950
#% 201922
#% 268791
#% 273945
#% 430940
#% 442995
#% 443380
#% 480766
#% 586147
#% 615115
#% 636031
#% 716802
#% 993933
#% 1848989
#! The demand for real-time data services is increasing in many applications including e-commerce, agile manufacturing, and telecommunications network management. In these applications, it is desirable to execute transactions within their deadlines, i.e., before the real-world status changes, using fresh (temporally consistent) data. However, meeting these fundamental requirements is challenging due to dynamic workloads and data access patterns in these applications. Further, transaction timeliness and data freshness requirements may conflict. In this paper, we define average/transient deadline miss ratio and new data freshness metrics to let a database administrator specify the desired quality of real-time data services for a specific application. We also present a novel QoS management architecture for real-time databases to support the desired QoS even in the presence of unpredictable workloads and access patterns. To prevent overload and support the desired QoS, the presented architecture applies feedback control, admission control, and flexible freshness management schemes. A simulation study shows that our QoS-aware approach can achieve a near zero miss ratio and perfect freshness, meeting basic requirements for real-time transaction processing. In contrast, baseline approaches fail to support the desired miss ratio and/or freshness in the presence of unpredictable workloads and data access patterns.

#index 772842
#* Recovery of PTUIE Handling from Source Codes through Recognizing Its Probable Properties
#@ Hee Beng Kuan Tan;Ni Lar Thein
#t 2004
#c 7
#% 19622
#% 32897
#% 69389
#% 132222
#% 137551
#% 157760
#% 177452
#% 205417
#% 297770
#% 318178
#% 374001
#% 411111
#% 443519
#% 448360
#% 464338
#% 464508
#% 585800
#% 622349
#% 622385
#% 622412
#% 622419
#% 633344
#% 640584
#% 654997
#% 660722
#% 662119
#! Automated recovery of system features and their designs from program source codes is important in reverse engineering and system comprehension. It also helps in the testing of software. An error that is made by users in an input to an execution of a transaction and discovered only after the completion of the execution is called a posttransaction user-input error (PTUIE) of the transaction. For a transaction in any database application, usually, it is essential to provide transactions for correcting the effect that could result from any PTUIE of the transaction. We discover some probable properties that exist between the control flow graph of a transaction and the control flow graphs of transactions for correcting PTUIE of the former transaction. Through recognizing these properties, this paper presents a novel approach for the automated approximate recovery of provisions and designs for transactions to correct PTUIE of transactions in a database application. The approach recognizes these properties through analyzing the source codes of transactions in the database application statically.

#index 772843
#* Specifying Mining Algorithms with Iterative User-Defined Aggregates
#@ Fosca Giannotti;Giuseppe Manco;Franco Turini
#t 2004
#c 7
#% 36683
#% 77944
#% 201894
#% 216508
#% 234756
#% 244336
#% 248784
#% 248785
#% 248791
#% 249985
#% 287461
#% 300120
#% 333325
#% 384978
#% 420101
#% 420108
#% 443082
#% 443446
#% 465003
#% 477790
#% 477951
#% 479968
#% 480144
#% 480318
#% 481290
#% 481754
#% 481779
#% 481954
#% 484332
#% 704492
#! We present a way of exploiting domain knowledge in the design and implementation of data mining algorithms, with special attention to frequent patterns discovery, within a deductive framework. In our framework, domain knowledge is represented by way of deductive rules, and data mining algorithms are specified by means of iterative user-defined aggregates and implemented by means of user-defined predicates. This choice allows us to exploit the full expressive power of deductive rules without loosing in performance. Iterative user-defined aggregates have a fixed scheme, in which user-defined predicates are to be added. This feature allows the modularization of data mining algorithms, thus providing a way to integrate the proper domain knowledge exploitation in the right point. As a case study, the paper presents how user-defined aggregates can be exploited to specify and implement a version of the a priori algorithm. Some performance analyzes and comparisons are discussed in order to show the effectiveness of the approach.

#index 772844
#* An Efficient Subspace Sampling Framework for High-Dimensional Data Reduction, Selectivity Estimation, and Nearest-Neighbor Search
#@ Charu C. Aggarwal
#t 2004
#c 7
#% 86950
#% 201876
#% 201893
#% 209021
#% 210190
#% 248027
#% 248798
#% 249321
#% 300131
#% 300193
#% 316560
#% 333881
#% 333941
#% 333946
#% 333954
#% 342602
#% 342617
#% 397387
#% 427199
#% 435141
#% 480124
#% 480307
#% 481290
#% 481956
#% 482092
#% 631923
#! Data reduction can improve the storage, transfer time, and processing requirements of very large data sets. One of the challenges of designing effective data reduction techniques is to be able to preserve the ability to use the reduced format directly for a wide range of database and data mining applications. In this paper, we propose the novel idea of hierarchical subspace sampling in order to create a reduced representation of the data. The method is naturally able to estimate the local implicit dimensionalities of each point very effectively and, thereby, create a variable dimensionality reduced representation of the data. Such a technique is very adaptive about adjusting its representation depending upon the behavior of the immediate locality of a data point. An important property of the subspace sampling technique is that the overall efficiency of compression improves with increasing database size. Because of its sampling approach, the procedure is extremely fast and scales linearly both with data set size and dimensionality. We propose new and effective solutions to problems such as selectivity estimation and approximate nearest-neighbor search. These are achieved by utilizing the locality specific subspace characteristics of the data which are revealed by the subspace sampling technique.

#index 772845
#* Efficient Phrase-Based Document Indexing for Web Document Clustering
#@ Khaled M. Hammouda;Mohamed S. Kamel
#t 2004
#c 7
#% 36672
#% 67565
#% 115462
#% 143306
#% 232768
#% 260001
#% 262045
#% 267537
#% 278109
#% 281186
#% 296738
#% 300312
#% 304321
#% 304423
#% 320930
#% 321635
#% 397148
#% 406493
#% 420083
#% 445316
#% 465754
#% 465914
#% 466101
#% 495795
#% 529678
#% 577257
#% 629601
#% 649770
#% 650846
#! Document clustering techniques mostly rely on single term analysis of the document data set, such as the Vector Space Model. To achieve more accurate document clustering, more informative features including phrases and their weights are particularly important in such scenarios. Document clustering is particularly useful in many applications such as automatic categorization of documents, grouping search engine results, building a taxonomy of documents, and others. This paper presents two key parts of successful document clustering. The first part is a novel phrase-based document index model, the Document Index Graph, which allows for incremental construction of a phrase-based index of the document set with an emphasis on efficiency, rather than relying on single-term indexes only. It provides efficient phrase matching that is used to judge the similarity between documents. The model is flexible in that it could revert to a compact representation of the vector space model if we choose not to index phrases. The second part is an incremental document clustering algorithm based on maximizing the tightness of clusters by carefully watching the pair-wise document similarity distribution inside clusters. The combination of these two components creates an underlying model for robust and accurate document similarity calculation that leads to much improved results in Web document clustering over traditional methods.

#index 772846
#* Selective and Authentic Third-Party Distribution of XML Documents
#@ Elisa Bertino;Barbara Carminati;Elena Ferrari;Bhavani Thuraisingham;Amar Gupta
#t 2004
#c 7
#% 35004
#% 264261
#% 331766
#% 342345
#% 354534
#% 397367
#% 414366
#% 433922
#% 488015
#% 513367
#% 566391
#% 659992
#% 978647
#! Third-party architectures for data publishing over the Internet today are receiving growing attention, due to their scalability properties and to the ability of efficiently managing large number of subjects and great amount of data. In a third-party architecture, there is a distinction between the Owner and the Publisher of information. The Owner is the producer of information, whereas Publishers are responsible for managing (a portion of) the Owner information and for answering subject queries. A relevant issue in this architecture is how the Owner can ensure a secure and selective publishing of its data, even if the data are managed by a third-party, which can prune some of the nodes of the original document on the basis of subject queries and access control policies. An approach can be that of requiring the Publisher to be trusted with regard to the considered security properties. However, the serious drawback of this solution is that large Web-based systems cannot be easily verified to be secure and can be easily penetrated. For these reasons, in this paper, we propose an alternative approach, based on the use of digital signature techniques, which does not require the Publisher to be trusted. The security properties we consider are authenticity and completeness of a query response, where completeness is intended with regard to the access control policies stated by the information Owner. In particular, we show that, by embedding in the query response one digital signature generated by the Owner and some hash values, a subject is able to locally verify the authenticity of a query response. Moreover, we present an approach that, for a wide range of queries, allows a subject to verify the completeness of query results.

#index 772847
#* Making the Threshold Algorithm Access Cost Aware
#@ Christian A. Lang;Yuan-Chi Chang;John R. Smith
#t 2004
#c 7
#% 210172
#% 213981
#% 248014
#% 333854
#% 333975
#% 427199
#% 480330
#% 504162
#% 527026
#% 631988
#! Assume a database storing N objects with d numerical attributes or feature values. All objects in the database can be assigned an overall score that is derived from their single feature values (and the feature values of a user-defined query). The problem considered here is then to efficiently retrieve the k objects with minimum (or maximum) overall score. The well-known threshold algorithm (TA) was proposed as a solution to this problem. TA views the database as a set of d sorted lists storing the feature values. Even though TA is optimal with regard to the number of accesses, its overall access cost can be high since, in practice, some list accesses may be more expensive than others. We therefore propose to make TA access cost aware by choosing the next list to access such that the overall cost is minimized. Our experimental results show that this overall cost is close to the optimal cost and significantly lower than the cost of prior approaches.

#index 772848
#* Efficient Time-Bound Hierarchical Key Assignment Scheme
#@ Hung-Yu Chien
#t 2004
#c 7
#% 40355
#% 78832
#% 144890
#% 318404
#% 443477
#% 542597
#! The access privileges in distributed systems can be effectively organized as a hierarchical tree that consists of distinct classes. A hierarchical time-bound key assignment scheme is to assign distinct cryptographic keys to distinct classes according to their privileges so that users from a higher class can use their class key to derive the keys of lower classes, and the keys are different for each time period; therefore, key derivation is constrained by both the class relation and the time period. This paper shall propose, based on a tamper-resistant device, a new time-bound key assignment scheme that greatly improves the computational performance and reduces the implementation cost.

#index 772849
#* Blocking Reduction Strategies in Hierarchical Text Classification
#@ Aixin Sun;Ee-Peng Lim;Wee-Keong Ng;Jaideep Srivastava
#t 2004
#c 7
#% 169358
#% 219051
#% 248810
#% 260001
#% 280817
#% 309141
#% 344447
#% 420466
#% 458379
#% 465747
#% 466501
#! One common approach in hierarchical text classification involves associating classifiers with nodes in the category tree and classifying text documents in a top-down manner. Classification methods using this top-down approach can scale well and cope with changes to the category trees. However, all these methods suffer from blocking which refers to documents wrongly rejected by the classifiers at higher-levels and cannot be passed to the classifiers at lower-levels. In this paper, we propose a classifier-centric performance measure known as blocking factor to determine the extent of the blocking. Three methods are proposed to address the blocking problem, namely, Threshold Reduction, Restricted Voting, and Extended Multiplicative. Our experiments using Support Vector Machine (SVM) classifiers on the Reuters collection have shown that they all could reduce blocking and improve the classification accuracy. Our experiments have also shown that the Restricted Voting method delivered the best performance.

#index 772850
#* Comments on "A Practical (t,n) Threshold Proxy Signature Scheme Based on the RSA Cryptosystem'
#@ Guilin Wang;Feng Bao;Jianying Zhou;Robert H. Deng
#t 2004
#c 7
#% 25997
#% 214422
#% 319849
#% 319994
#% 378267
#% 486569
#% 495534
#% 513373
#% 568847
#% 653833
#% 724768
#% 727664
#% 1092839
#% 1386212
#% 1395152
#% 1834656
#! In a (t,n) threshold proxy signature scheme, the original signer can delegate his/her signing capability to n proxy signers such that any t or more proxy signers can sign messages on behalf of the former, but t-1 or less of them cannot do the same thing. Such schemes have been suggested for use in a number of applications, particularly, in distributed computing where delegation of rights is quite common. Based on the RSA cryptosystem, Hwang et al. [7] recently proposed an efficient (t,n) threshold proxy signature scheme. In this paper, we identify several security weaknesses in their scheme and show that their scheme is insecure.

#index 778724
#* Index Selection for Databases: A Hardness Study and a Principled Heuristic Solution
#@ Surajit Chaudhuri;Mayur Datar;Vivek Narasayya
#t 2004
#c 7
#% 36119
#% 176082
#% 210182
#% 217812
#% 232652
#% 248815
#% 273697
#% 287498
#% 346894
#% 462204
#% 480158
#% 482100
#% 503909
#% 565429
#% 566118
#% 576114
#% 632100
#% 662206
#! We study the index selection problem: Given a workload consisting of SQL statements on a database, and a user-specified storage constraint, recommend a set of indexes that have the maximum benefit for the given workload. We present a formal statement for this problem and show that it is computationally "hard驴 to solve or even approximate it. We develop a new algorithm for the problem which is based on treating the problem as a knapsack problem. The novelty of our approach lies in an LP (linear programming) based method that assigns benefits to individual indexes. For a slightly modified algorithm, that does more work, we prove that we can give instance specific guarantees about the quality of our solution. We conduct an extensive experimental evaluation of this new heuristic and compare it with previous solutions. Our results demonstrate that our solution is more scalable while achieving comparable quality.

#index 778725
#* Augmenting a Conceptual Model with Geospatiotemporal Annotations
#@ Vijay Khatri;Sudha Ram;Richard Thomas Snodgrass
#t 2004
#c 7
#% 727
#% 16028
#% 43028
#% 106916
#% 140389
#% 183523
#% 192329
#% 200992
#% 287068
#% 287631
#% 297188
#% 335715
#% 384014
#% 417826
#% 421072
#% 443363
#% 487534
#% 535969
#! While many real-world applications need to organize data based on space (e.g., geology, geomarketing, environmental modeling) and/or time (e.g., accounting, inventory management, personnel management), existing conventional conceptual models do not provide a straightforward mechanism to explicitly capture the associated spatial and temporal semantics. As a result, it is left to database designers to discover, design, and implement驴on an ad hoc basis驴the temporal and spatial concepts that they need. We propose an annotation-based approach that allows a database designer to focus first on nontemporal and nongeospatial aspects (i.e., "what驴) of the application and, subsequently, augment the conceptual schema with geospatiotemporal annotations (i.e., "when驴 and "where驴). Via annotations, we enable a supplementary level of abstraction that succinctly encapsulates the geospatiotemporal data semantics and naturally extends the semantics of a conventional conceptual model. An overarching assumption in conceptual modeling has always been that expressiveness and formality need to be balanced with simplicity. We posit that our formally defined annotation-based approach is not only expressive, but also straightforward to understand and implement.

#index 778726
#* A Modeling Technique for the Performance Analysis of Web Searching Applications
#@ Marco Scarpa;Antonio Puliafito;Massimo Villari;Angelo Zaia
#t 2004
#c 7
#% 207505
#% 251084
#% 268114
#% 268358
#% 287611
#% 306248
#% 434376
#% 443376
#% 541579
#% 649779
#! This paper proposes a methodological approach for the performance analysis of Web-based searching applications on the Internet. It specifically investigates the behavior of the Client/Server (C/S), Remote-Evaluation (REV) and Mobile-Agent (MA) communication paradigms and describes how Petri-net models can be developed to derive performance indices which can help the designer to improve the efficiency of his distributed applications. Our purpose is that of identifying a set of models that can help to understand the environmental situations in which such paradigms should be preferred or combined in order to optimize the performances of a distributed system. In particular, we propose a modeling technique applied to an Information Retrieval application on the World Wide Web. An analytical evaluation through the solution of non-Markovian Petri-net models is provided, which allows us to identify the main parameters, as well as the way they interact, to be taken into consideration when distributed applications are to be designed. An experimental environment is also studied in order to obtain real measurements used to validate the analytical models.

#index 778727
#* The Hierarchical Degree-of-Visibility Tree
#@ Lidan Shou;Zhiyong Huang;Kian-Lee Tan
#t 2004
#c 7
#% 109051
#% 131750
#% 270535
#% 427199
#% 480830
#% 590842
#! In this paper, we present a novel structure called the Hierarchical Degree-of-Visibility Tree (HDoV-tree) for visibility query processing in visualization systems. The HDoV-tree builds on and extends the R-tree such that 1) the search space is pruned based on the degree of visibility of objects and 2) internal nodes store level-of-details (LoDs) that represent a collection of objects in a coarser form. We propose two tree traversal algorithms that balance performance and visual fidelity, explore three storage structures for the HDoV-tree, and develop novel caching techniques for disk-based HDoV-tree. We implemented the HDoV-tree in a prototype walkthrough system called VISUAL. Our experimental study shows that VISUAL can lead to high frame rates without compromising visual fidelity.

#index 778728
#* Cluster Analysis for Gene Expression Data: A Survey
#@ Daxin Jiang;Chun Tang;Aidong Zhang
#t 2004
#c 7
#% 36672
#% 60576
#% 248792
#% 271870
#% 273890
#% 296738
#% 324431
#% 328317
#% 397382
#% 397641
#% 413549
#% 430746
#% 469422
#% 469425
#% 589373
#% 589434
#% 659967
#% 729972
#% 729987
#% 856734
#! DNA microarray technology has now made it possible to simultaneously monitor the expression levels of thousands of genes during important biological processes and across collections of related samples. Elucidating the patterns hidden in gene expression data offers a tremendous opportunity for an enhanced understanding of functional genomics. However, the large number of genes and the complexity of biological networks greatly increases the challenges of comprehending and interpreting the resulting mass of data, which often consists of millions of measurements. A first step toward addressing this challenge is the use of clustering techniques, which is essential in the data mining process to reveal natural structures and identify interesting patterns in the underlying data. Cluster analysis seeks to partition a given data set into groups based on specified features so that the data points within a group are more similar to each other than the points in different groups. A very rich literature on cluster analysis has developed over the past three decades. Many conventional clustering algorithms have been adapted or directly applied to gene expression data, and also new algorithms have recently been proposed specifically aiming at gene expression data. These clustering algorithms have been proven useful for identifying biologically relevant groups of genes and samples. In this paper, we first briefly introduce the concepts of microarray technology and discuss the basic elements of clustering on gene expression data. In particular, we divide cluster analysis for gene expression data into three categories. Then, we present specific challenges pertinent to each clustering category and introduce several representative approaches. We also discuss the problem of cluster validation in three aspects and review various methods to assess the quality and reliability of clustering results. Finally, we conclude this paper and suggest the promising trends in this field.

#index 778729
#* HARP: A Practical Projected Clustering Algorithm
#@ Kevin Y. Yip;David W. Cheung;Michael K. Ng
#t 2004
#c 7
#% 248790
#% 248792
#% 269534
#% 273891
#% 282894
#% 300131
#% 316709
#% 375388
#% 397382
#% 397384
#% 469422
#% 481281
#% 631985
#% 727908
#! In high-dimensional data, clusters can exist in subspaces that hide themselves from traditional clustering methods. A number of algorithms have been proposed to identify such projected clusters, but most of them rely on some user parameters to guide the clustering process. The clustering accuracy can be seriously degraded if incorrect values are used. Unfortunately, in real situations, it is rarely possible for users to supply the parameter values accurately, which causes practical difficulties in applying these algorithms to real data. In this paper, we analyze the major challenges of projected clustering and suggest why these algorithms need to depend heavily on user parameters. Based on the analysis, we propose a new algorithm that exploits the clustering status to adjust the internal thresholds dynamically without the assistance of user parameters. According to the results of extensive experiments on real and synthetic data, the new method has excellent accuracy and usability. It outperformed the other algorithms even when correct parameter values were artificially supplied to them. The encouraging results suggest that projected clustering can be a practical tool for various kinds of real applications.

#index 778730
#* Information Retrieval in Document Image Databases
#@ Yue Lu;Chew Lim Tan
#t 2004
#c 7
#% 132068
#% 162362
#% 235941
#% 263214
#% 263223
#% 280817
#% 288885
#% 399594
#% 420480
#% 420516
#% 442840
#% 443828
#% 493341
#% 493489
#% 504885
#% 588981
#% 592177
#% 625561
#% 625563
#% 627601
#% 658539
#% 658549
#% 658831
#% 679317
#! With the rising popularity and importance of document images as an information source, information retrieval in document image databases has become a growing and challenging problem. In this paper, we propose an approach with the capability of matching partial word images to address two issues in document image retrieval: word spotting and similarity measurement between documents. First, each word image is represented by a primitive string. Then, an inexact string matching technique is utilized to measure the similarity between the two primitive strings generated from two word images. Based on the similarity, we can estimate how a word image is relevant to the other and, thereby, decide whether one is a portion of the other. To deal with various character fonts, we use a primitive string which is tolerant to serif and font differences to represent a word image. Using this technique of inexact string matching, our method is able to successfully handle the problem of heavily touching characters. Experimental results on a variety of document image databases confirm the feasibility, validity, and efficiency of our proposed approach in document image retrieval.

#index 778731
#* Evaluation of Edge Caching/Offloading for Dynamic Content Delivery
#@ Chun Yuan;Yu Chen;Zheng Zhang
#t 2004
#c 7
#% 300177
#% 330581
#% 346714
#% 397357
#% 480474
#% 577348
#% 978365
#% 994021
#% 1303525
#! As dynamic content becomes increasingly dominant, it becomes an important research topic as how the edge resources such as client-side proxies, which are otherwise underutilized for such content, can be put into use. However, it is unclear what will be the best strategy, and the design/deployment trade offs lie therein. In this paper, using one representative e-commerce benchmark, we report our experience of an extensive investigation of different offloading and caching options. Our results point out that, while great benefits can be reached in general, advanced offloading strategies can be overly complex and even counterproductive. In contrast, simple augmentation at proxies to enable fragment caching and page composition achieves most of the benefit without compromising important considerations such as security. We also present Proxy+ architecture which supports such capabilities for existing Web applications with minimal reengineering effort.

#index 778732
#* Mining Sequential Patterns by Pattern-Growth: The PrefixSpan Approach
#@ Jian Pei;Jiawei Han;Behzad Mortazavi-Asl;Jianyong Wang;Helen Pinto;Qiming Chen;Umeshwar Dayal;Mei-Chun Hsu
#t 2004
#c 7
#% 172892
#% 248785
#% 248791
#% 259993
#% 300120
#% 310559
#% 338609
#% 342666
#% 397383
#% 413550
#% 420063
#% 425006
#% 459006
#% 463903
#% 464839
#% 464989
#% 464996
#% 466644
#% 477791
#% 479627
#% 481290
#% 577218
#% 629708
#% 631926
#% 631970
#% 631985
#% 729938
#! Sequential pattern mining is an important data mining problem with broad applications. However, it is also a difficult problem since the mining may have to generate or examine a combinatorially explosive number of intermediate subsequences. Most of the previously developed sequential pattern mining methods, such as GSP, explore a candidate generation-and-test approach [1] to reduce the number of candidates to be examined. However, this approach may not be efficient in mining large sequence databases having numerous patterns and/or long patterns. In this paper, we propose a projection-based, sequential pattern-growth approach for efficient mining of sequential patterns. In this approach, a sequence database is recursively projected into a set of smaller projected databases, and sequential patterns are grown in each projected database by exploring only locally frequent fragments. Based on an initial study of the pattern growth-based sequential pattern mining, FreeSpan [8], we propose a more efficient method, called PSP, which offers ordered growth and reduced projected databases. To further improve the performance, a pseudoprojection technique is developed in PrefixSpan. A comprehensive performance study shows that PrefixSpan, in most cases, outperforms the a priori-based algorithm GSP, FreeSpan, and SPADE [29] (a sequential pattern mining algorithm that adopts vertical data format), and PrefixSpan integrated with pseudoprojection is the fastest among all the tested algorithms. Furthermore, this mining methodology can be extended to mining sequential patterns with user-specified constraints. The high promise of the pattern-growth approach may lead to its further extension toward efficient mining of other kinds of frequent patterns, such as frequent substructures.

#index 778733
#* View Adaptation in the Fragment-Based Approach
#@ Zohra Bellahsene
#t 2004
#c 7
#% 201898
#% 210182
#% 273917
#% 300166
#% 340300
#% 463735
#% 464706
#% 480670
#% 481604
#% 482110
#% 482111
#% 565261
#% 572311
#% 1388097
#! View adaptation relies on adapting a set of materialized views in response to schema changes of source relations and/or after view redefinition. Recently, several view selection methods that are based on materializing fragments of the view rather than the whole view have been proposed. We call this approach the fragment-based approach. This paper presents a view adaptation method in the fragment-based approach, which is aimed at exploiting the opportunities to share not only materialized data, but also computation between the different views. In order to do this, the views are modeled using the so-called Multiview Materialization Graph, which represents the views as a bipartite directed acyclic graph whose nodes are operations and fragments of the views. Then, the adaptation is performed regarding all materialized views and not solely the old materialization of the view. However, the data independence is preserved for the views that are not affected by the change. On the contrary, in related work, the adaptation technique is based solely on the old materialization of the same view. We studied the impact of the fragmentation on the adaptation techniques and showed the advantages and drawbacks of this approach.

#index 784508
#* Semantics-Preserving Dimensionality Reduction: Rough and Fuzzy-Rough-Based Approaches
#@ Richard Jensen;Qiang Shen
#t 2004
#c 7
#% 38847
#% 56021
#% 136350
#% 154305
#% 168559
#% 168563
#% 217733
#% 268148
#% 347878
#% 366687
#% 375017
#% 376266
#% 385563
#% 400441
#% 425109
#% 450927
#% 450928
#% 458261
#% 497616
#% 498104
#% 505852
#% 579498
#% 1408574
#% 1860187
#% 1902503
#! Semantics-preserving dimensionality reduction refers to the problem of selecting those input features that are most predictive of a given outcome; a problem encountered in many areas such as machine learning, pattern recognition, and signal processing. This has found successful application in tasks that involve data sets containing huge numbers of features (in the order of tens of thousands), which would be impossible to process further. Recent examples include text processing and Web content classification. One of the many successful applications of rough set theory has been to this feature selection area. This paper reviews those techniques that preserve the underlying semantics of the data, using crisp and fuzzy rough set-based methodologies. Several approaches to feature selection based on rough set theory are experimentally compared. Additionally, a new area in feature selection, feature grouping, is highlighted and a rough set-based feature grouping technique is detailed.

#index 784509
#* Discovering Colocation Patterns from Spatial Data Sets: A General Approach
#@ Yan Huang;Shashi Shekhar;Hui Xiong
#t 2004
#c 7
#% 210187
#% 342635
#% 462218
#% 463759
#% 479797
#% 501066
#% 527021
#% 836006
#! Given a collection of Boolean spatial features, the colocation pattern discovery process finds the subsets of features frequently located together. For example, the analysis of an ecology data set may reveal symbiotic species. The spatial colocation rule problem is different from the association rule problem since there is no natural notion of transactions in spatial data sets which are embedded in continuous geographic space. In this paper, we provide a transaction-free approach to mine colocation patterns by using the concept of proximity neighborhood. A new interest measure, a participation index, is also proposed for spatial colocation patterns. The participation index is used as the measure of prevalence of a colocation for two reasons. First, this measure is closely related to the {\rm{cross}}{\hbox{-}}K function, which is often used as a statistical measure of interaction among pairs of spatial features. Second, it also possesses an antimonotone property which can be exploited for computational efficiency. Furthermore, we design an algorithm to discover colocation patterns. This algorithm includes a novel multiresolution pruning technique. Finally, experimental results are provided to show the strength of the algorithm and design decisions related to performance tuning.

#index 784510
#* Image Database Design Based on 9D-SPA Representation for Spatial Relations
#@ Po-Whei Huang;Chu-Hui Lee
#t 2004
#c 7
#% 23998
#% 54003
#% 78243
#% 124680
#% 213673
#% 219847
#% 245508
#% 395083
#% 437405
#% 443054
#% 443458
#% 443529
#% 443543
#% 444004
#% 527010
#% 554673
#% 1775151
#% 1854885
#! Spatial relationships between objects are important features for designing a content-based image retrieval system. In this paper, we propose a new scheme, called 9D-SPA representation, for encoding the spatial relations in an image. With this representation, important functions of intelligent image database systems such as visualization, browsing, spatial reasoning, iconic indexing, and similarity retrieval can be easily achieved. The capability of discriminating images based on 9D-SPA representation is much more powerful than any spatial representation method based on Minimum Bounding Rectangles or centroids of objects. The similarity measures using 9D-SPA representation provide a wide range of fuzzy matching capability in similarity retrieval to meet different user's requirements. Experimental results showed that our system is very effective in terms of recall and precision. In addition, the 9D-SPA representation can be incorporated into a two-level index structure to help reduce the search space of each query processing. The experimental results also demonstrated that, on average, only 0.1254 percent \sim 1.6829 percent of symbolic pictures (depending on various degrees of similarity) were accessed per query in an image database containing 50,000 symbolic pictures.

#index 784511
#* Effective Reformulations for Task Allocation in Distributed Systems with a Large Number of Communicating Tasks
#@ Syam Menon
#t 2004
#c 7
#% 44796
#% 131289
#% 236097
#% 269879
#% 306674
#% 308351
#% 444252
#% 444290
#% 577796
#% 595181
#% 723666
#% 728338
#! In any distributed processing environment, decisions need to be made concerning the assignment of computational task modules to various processors. Many versions of the task allocation problem have appeared in the literature. Intertask communication makes the assignment decision difficult; capacity limitations at the processors increase the difficulty. This problem is naturally formulated as a nonlinear integer program, but can be linearized to take advantage of commercial integer programming solvers. While traditional approaches to linearizing the problem perform well when only a few tasks communicate, they have considerable difficulty solving problems involving a large number of intercommunicating tasks. This paper introduces new mixed integer formulations for three variations of the task allocation problem. Results from extensive computational tests conducted over real and generated data indicate that the reformulations are particularly efficient when a large number of tasks communicate, solving reasonably large problems faster than other exact approaches available.

#index 784512
#* Rights Protection for Relational Data
#@ Radu Sion;Mikhail Atallah;Sunil Prabhakar
#t 2004
#c 7
#% 227956
#% 241787
#% 275836
#% 488310
#% 488637
#% 539761
#% 566390
#% 576109
#% 583804
#% 589250
#% 592726
#% 660348
#% 664665
#% 670633
#% 993944
#% 1395188
#! In this paper, we introduce a solution for relational database content rights protection through watermarking. Rights protection for relational data is of ever-increasing interest, especially considering areas where sensitive, valuable content is to be outsourced. A good example is a data mining application, where data is sold in pieces to parties specialized in mining it. Different avenues are available, each with its own advantages and drawbacks. Enforcement by legal means is usually ineffective in preventing theft of copyrighted works, unless augmented by a digital counterpart, for example, watermarking. While being able to handle higher level semantic constraints, such as classification preservation, our solution also addresses important attacks, such as subset selection and random and linear data changes. We introduce wmdb.*, a proof-of-concept implementation and its application to real-life data, namely, in watermarking the outsourced Wal-Mart sales data that we have available at our institute.

#index 784513
#* The D-Tree: An Index Structure for Planar Point Queries in Location-Based Wireless Services
#@ Jianliang Xu;Baihua Zheng;Wang-Chien Lee;Dik Lun Lee
#t 2004
#c 7
#% 86950
#% 90724
#% 100831
#% 169835
#% 227939
#% 235114
#% 252304
#% 259658
#% 309461
#% 318051
#% 335041
#% 339205
#% 342828
#% 382477
#% 427199
#% 433364
#% 435141
#% 438456
#% 442616
#% 443127
#% 452871
#% 464195
#% 479462
#% 480093
#% 480632
#% 481956
#% 567868
#% 632027
#% 654478
#% 755203
#! Location-based services (LBSs), considered as a killer application in the wireless data market, provide information based on locations specified in the queries. In this paper, we examine the indexing issue for querying location-dependent data in wireless LBSs; in particular, we focus on an important class of queries, planar point queries. To address the issues of responsiveness, energy consumption, and bandwidth contention in wireless communications, an index has to minimize the search time and maintain a small storage overhead. It is shown that the traditional point-location algorithms and spatial index structures fail to achieve either objective or both. This paper proposes a new index structure, called D-tree, which indexes spatial regions based on the divisions that form the boundaries of the regions. We describe how to construct a binary D-tree index, how to process queries based on the D-tree, and how to page the binary D-tree. Moreover, two parameterized methods for partitioning the original space, called fixed grid assignment (FGA) and adaptive grid assignment (AGA), are proposed to enhance the D-tree. The performance of the D-tree is evaluated using both synthetic and real data sets. Experimental results show that the proposed D-tree outperforms the well-known indexes such as the {\rm{R^*{\hbox{-}}tree}}, and that both the FGA and AGA approaches can achieve different performance trade-offs between the index search time and storage overhead by fine-tuning their algorithmic parameters.

#index 784514
#* Self-Stabilizing Real-Time OPS5 Production Systems
#@ Albert Mo Kim Cheng;Seiya Fujii
#t 2004
#c 7
#% 1797
#% 34000
#% 53531
#% 55309
#% 60035
#% 82196
#% 102184
#% 107899
#% 114312
#% 116045
#% 147129
#% 150208
#% 156380
#% 170083
#% 178728
#% 179005
#% 179619
#% 226606
#% 271252
#% 307476
#% 321618
#% 394745
#% 442344
#% 442733
#% 442929
#% 443098
#% 443196
#% 443204
#% 443351
#% 443534
#% 445781
#% 445836
#% 480435
#% 483527
#% 507878
#% 515895
#% 530072
#% 589690
#% 633971
#% 660210
#% 691136
#% 692741
#% 733622
#% 807849
#% 822907
#% 1478793
#! We examine the task of constructing bounded-time self-stabilizing rule-based systems that take their input from an external environment. Bounded response-time and self-stabilization are essential for rule-based programs that must be highly fault-tolerant and perform in a real-time environment. We present an approach for solving this problem using the OPS5 programming language as it is one of the most expressive and widely used rule-based programming languages. Bounded response-time of the program is ensured by constructing the state space graph so that the programmer can visualize the control flow of the program execution. Potential infinite firing sequences, if any, should be detected and the involved rules should be revised to ensure bounded termination. Both the input variables and internal variables are made fault-tolerant from corruption caused by transient faults via the introduction of new self-stabilizing rules in the program. Finally, the timing analysis of the self-stabilizing OPS5 program is shown in terms of the number of rule firings and the comparisons performed in the Rete network.

#index 784515
#* Range Aggregate Processing in Spatial Databases
#@ Yufei Tao;Dimitris Papadias
#t 2004
#c 7
#% 43163
#% 56081
#% 86950
#% 153260
#% 164360
#% 213975
#% 227866
#% 273887
#% 273902
#% 287070
#% 300193
#% 300849
#% 333874
#% 333977
#% 378398
#% 397350
#% 411356
#% 427199
#% 443130
#% 443181
#% 443444
#% 464215
#% 465010
#% 465060
#% 479473
#% 479822
#% 480093
#% 480801
#% 480817
#% 481304
#% 481620
#% 482092
#% 527189
#% 554911
#% 571296
#% 580214
#% 589264
#% 617845
#! A range aggregate query returns summarized information about the points falling in a hyper-rectangle (e.g., the total number of these points instead of their concrete ids). This paper studies spatial indexes that solve such queries efficiently and proposes the aggregate Point-tree (aP-tree), which achieves logarithmic cost to the data set cardinality (independently of the query size) for two-dimensional data. The aP-tree requires only small modifications to the popular multiversion structural framework and, thus, can be implemented and applied easily in practice. We also present models that accurately predict the space consumption and query cost of the aP-tree and are therefore suitable for query optimization. Extensive experiments confirm that the proposed methods are efficient and practical.

#index 784516
#* A Framework of Fuzzy Diagnosis
#@ Huaiqing Wang;Mingyi Zhang;Dongming Xu;Dan Zhang
#t 2004
#c 7
#% 877
#% 21137
#% 21138
#% 132173
#% 146831
#% 167637
#% 193498
#% 212220
#% 224478
#% 546675
#% 608590
#% 681594
#! Fault diagnosis has become an important component in intelligent systems, such as intelligent control systems and intelligent eLearning systems. Reiter's diagnosis theory, described by first-order sentences, has been attracting much attention in this field. However, descriptions and observations of most real-world situations are related to fuzziness because of the incompleteness and the uncertainty of knowledge, e.g., the fault diagnosis of student behaviors in the eLearning processes. In this paper, an extension of Reiter's consistency-based diagnosis methodology, Fuzzy Diagnosis, has been proposed, which is able to deal with incomplete or fuzzy knowledge. A number of important properties of the Fuzzy diagnoses schemes have also been established. The computing of fuzzy diagnoses is mapped to solving a system of inequalities. Some special cases, abstracted from real-world situations, have been discussed. In particular, the fuzzy diagnosis problem, in which fuzzy observations are represented by clause-style fuzzy theories, has been presented and its solving method has also been given. A student fault diagnostic problem abstracted from a simplified real-world eLearning case is described to demonstrate the application of our diagnostic framework.

#index 788998
#* A Generalized Temporal Role-Based Access Control Model
#@ James B. D. Joshi;Elisa Bertino;Usman Latif;Arif Ghafoor
#t 2005
#c 7
#% 204453
#% 258394
#% 263982
#% 270773
#% 270775
#% 270778
#% 316578
#% 319244
#% 340030
#% 342327
#% 342328
#% 344224
#% 411044
#% 592678
#% 761853
#! Role-based access control (RBAC) models have generated a great interest in the security community as a powerful and generalized approach to security management. In many practical scenarios, users may be restricted to assume roles only at predefined time periods. Furthermore, roles may only be invoked on prespecified intervals of time depending upon when certain actions are permitted. To capture such dynamic aspects of a role, a temporal RBAC (TRBAC) model has been recently proposed. However, the TRBAC model addresses the role enabling constraints only. In this paper, we propose a Generalized Temporal Role-Based Access Control (GTRBAC) model capable of expressing a wider range of temporal constraints. In particular, the model allows expressing periodic as well as duration constraints on roles, user-role assignments, and role-permission assignments. In an interval, activation of a role can further be restricted as a result of numerous activation constraints including cardinality constraints and maximum active duration constraints. The GTRBAC model extends the syntactic structure of the TRBAC model and its event and trigger expressions subsume those of TRBAC. Furthermore, GTRBAC allows expressing role hierarchies and separation of duty (SoD) constraints for specifying fine-grained temporal semantics.

#index 788999
#* Selection of Views to Materialize in a Data Warehouse
#@ Himanshu Gupta;Inderpal Singh Mumick
#t 2005
#c 7
#% 25470
#% 36117
#% 153041
#% 191154
#% 199537
#% 210182
#% 214219
#% 227869
#% 248855
#% 273697
#% 378402
#% 424925
#% 464706
#% 479476
#% 479646
#% 480158
#% 480670
#% 481288
#% 481608
#% 482100
#% 482110
#% 482111
#% 565469
#% 711230
#! A data warehouse stores materialized views of data from one or more sources, with the purpose of efficiently implementing decision-support or OLAP queries. One of the most important decisions in designing a data warehouse is the selection of materialized views to be maintained at the warehouse. The goal is to select an appropriate set of views that minimizes total query response time and the cost of maintaining the selected views, given a limited amount of resource, e.g., materialization time, storage space, etc. In this article, we have developed a theoretical framework for the general problem of selection of views in a data warehouse. We present polynomial-time heuristics for a selection of views to optimize total query response time under a disk-space constraint, for some important special cases of the general data warehouse scenario, viz.: 1) an AND view graph, where each query/view has a unique evaluation, e.g., when a multiple-query optimizer can be used to general a global evaluation plan for the queries, and 2) an OR view graph, in which any view can be computed from any one of its related views, e.g., data cubes. We present proofs showing that the algorithms are guaranteed to provide a solution that is fairly close to (within a constant factor ratio of) the optimal solution. We extend our heuristic to the general AND-OR view graphs. Finally, we address in detail the view-selection problem under the maintenance cost constraint and present provably competitive heuristics.

#index 789000
#* Semantic Approximation of Data Stream Joins
#@ Abhinandan Das;Johannes Gehrke;Mirek Riedewald
#t 2005
#c 7
#% 122671
#% 224218
#% 273908
#% 273911
#% 300179
#% 333931
#% 338425
#% 341700
#% 375017
#% 378388
#% 378408
#% 379445
#% 393844
#% 397353
#% 397354
#% 397385
#% 397414
#% 397426
#% 428155
#% 479984
#% 480628
#% 654444
#% 660004
#% 718437
#% 993948
#% 993949
#% 993999
#% 1015278
#% 1015279
#% 1015280
#! We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We make two main contributions. First, we define the problem space by discussing architectural models for data stream join processing and surveying suitable measures for the quality of an approximation of a set-valued query result. Second, we examine in detail a large part of this problem space. More precisely, we consider the number of generated result tuples as the quality measure and we propose optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data, we show the efficacy of our solutions.

#index 789001
#* Pruning and Visualizing Generalized Association Rules in Parallel Coordinates
#@ Li Yang
#t 2005
#c 7
#% 27058
#% 53948
#% 152934
#% 162710
#% 172386
#% 210162
#% 227919
#% 237200
#% 248791
#% 280433
#% 280436
#% 310496
#% 310515
#% 310525
#% 312048
#% 420073
#% 443427
#% 445403
#% 481290
#% 481758
#% 502132
#% 577216
#% 577252
#% 619521
#% 630982
#% 641130
#% 726032
#% 798816
#! One fundamental problem for visualizing frequent itemsets and association rules is how to present a long border of frequent itemsets in an itemset lattice. Another problem comes from the lack of an effective visual metaphor to represent many-to-many relationships. This paper proposes an approach for visualizing frequent itemsets and many-to-many association rules by a novel use of parallel coordinates. An association rule is visualized by connecting items in the rule, one item on each parallel coordinate, with continuous polynomial curves. In the presence of item taxonomy, each coordinate can be used to visualize an item taxonomy tree which can be expanded or shrunk by user interaction. This user interaction introduces a border, which separates displayable itemsets from nondisplayable ones, in the generalized itemset lattice. Only those itemsets that are both frequent and displayable are considered to be displayed. This approach of visualizing frequent itemsets and association rules has the following features: 1) It is capable of visualizing many-to-many rules and itemsets with many items. 2) It is capable of visualizing a large number of itemsets or rules by displaying only those ones whose items are selected by the user. 3) The closure properties of frequent itemsets and association rules are inherently supported such that the implied ones are not displayed. Usefulness of this approach is demonstrated through examples.

#index 789002
#* Shared Memory Parallelization of Data Mining Algorithms: Techniques, Programming Interface, and Performance
#@ Ruoming Jin;Ge Yang;Gagan Agrawal
#t 2005
#c 7
#% 36672
#% 92662
#% 136350
#% 189430
#% 201075
#% 201894
#% 227917
#% 227922
#% 232117
#% 237284
#% 237645
#% 252458
#% 273900
#% 299992
#% 299997
#% 299998
#% 300120
#% 300132
#% 313563
#% 316709
#% 328271
#% 329596
#% 338588
#% 348045
#% 379325
#% 420084
#% 420091
#% 434348
#% 434349
#% 437615
#% 437616
#% 443091
#% 443348
#% 452821
#% 459008
#% 465449
#% 471183
#% 479787
#% 481290
#% 481754
#% 481945
#% 509075
#% 631969
#% 660537
#% 678079
#% 708750
#% 1710873
#! With recent technological advances, shared memory parallel machines have become more scalable, and offer large main memories and high bus bandwidths. They are emerging as good platforms for data warehousing and data mining. In this paper, we focus on shared memory parallelization of data mining algorithms. We have developed a series of techniques for parallelization of data mining algorithms, including full replication, full locking, fixed locking, optimized full locking, and cache-sensitive locking. Unlike previous work on shared memory parallelization of specific data mining algorithms, all of our techniques apply to a large number of popular data mining algorithms. In addition, we propose a reduction-object-based interface for specifying a data mining algorithm. We show how our runtime system can apply any of the techniques we have developed starting from a common specification of the algorithm. We have carried out a detailed evaluation of the parallelization techniques and the programming interface. We have experimented with apriori and fp-tree-based association mining, k-means clustering, k-nearest neighbor classifier, and decision tree construction. The main results from our experiments are as follows: 1) Among full replication, optimized full locking, and cache-sensitive locking, there is no clear winner. Each of these three techniques can outperform others depending upon machine and dataset parameters. These three techniques perform significantly better than the other two techniques. 2) Good parallel efficiency is achieved for each of the four algorithms we experimented with, using our techniques and runtime system. 3) The overhead of the interface is within 10 percent in almost all cases. 4) In the case of decision tree construction, combining different techniques turned out to be crucial for achieving high performance.

#index 789003
#* Interpretable Hierarchical Clustering by Constructing an Unsupervised Decision Tree
#@ Jayanta Basak;Raghu Krishnapuram
#t 2005
#c 7
#% 129980
#% 136350
#% 182686
#% 227794
#% 262045
#% 269522
#% 272519
#% 290482
#% 316481
#% 328317
#% 465754
#% 479962
#% 729437
#% 817779
#% 1272280
#% 1388155
#! In this paper, we propose a method for hierarchical clustering based on the decision tree approach. As in the case of supervised decision tree, the unsupervised decision tree is interpretable in terms of rules, i.e., each leaf node represents a cluster, and the path from the root node to a leaf node represents a rule. The branching decision at each node of the tree is made based on the clustering tendency of the data available at the node. We present four different measures for selecting the most appropriate attribute to be used for splitting the data at every branching node (or decision node), and two different algorithms for splitting the data at each decision node. We provide a theoretical basis for the approach and demonstrate the capability of the unsupervised decision tree for segmenting various data sets. We also compare the performance of the unsupervised decision tree with that of the supervised one.

#index 789004
#* Constructing Suffix Tree for Gigabyte Sequences with Megabyte Memory
#@ Ching-Fung Cheung;Jeffrey Xu Yu;Hongjun Lu
#t 2005
#c 7
#% 186
#% 47710
#% 70370
#% 198770
#% 235941
#% 289010
#% 300312
#% 317163
#% 333679
#% 480484
#% 494014
#% 498538
#% 509775
#% 593764
#% 593861
#! Mammalian genomes are typically 3Gbps (gibabase pairs) in size. The largest public database NCBI (National Center for Biotechnology Information (http://www.ncbi.nlm.nih.gov)) of DNA contains more than 20 Gbps. Suffix trees are widely acknowledged as a data structure to support exact/approximate sequence matching queries as well as repetitive structure finding efficiently when they can reside in main memory. But, it has been shown as difficult to handle long DNA sequences using suffix trees due to the so-called memory bottleneck problems. The most space efficient main-memory suffix tree construction algorithm takes nine hours and 45 GB memory space to index the human genome [19]. In this paper, we show that suffix trees for long DNA sequences can be efficiently constructed on disk using small bounded main memory space and, therefore, all existing algorithms based on suffix trees can be used to handle long DNA sequences that cannot be held in main memory. We adopt a two-phase strategy to construct a suffix tree on disk: 1) to construct a diskbase suffix-tree without suffix links and 2) rebuild suffix links upon the suffix-tree being constructed on disk, if needed. We propose a new disk-based suffix tree construction algorithm, called DynaCluster, which shows O(n \log n) experimental behavior regarding CPU cost and linearity for I/O cost. DynaCluster needs 16MB main memory only to construct more than 200Mbps DNA sequences and significantly outperforms the existing disk-based suffix-tree construction algorithms using prepartitioning techniques in terms of both construction cost and query processing cost. We conducted extensive performance studies and report our findings in this paper.

#index 789005
#* Improving Availability and Performance with Application-Specific Data Replication
#@ Lei Gao;Mike Dahlin;Amol Nayate;Jiandan Zheng;Arun Iyengar
#t 2005
#c 7
#% 3765
#% 65498
#% 124019
#% 202144
#% 202146
#% 210179
#% 213080
#% 223378
#% 240016
#% 286465
#% 330581
#% 340142
#% 342363
#% 346714
#% 392578
#% 401965
#% 433941
#% 442680
#% 452379
#% 484384
#% 575096
#% 805470
#% 963656
#% 963899
#% 979681
#% 1303525
#! The emerging edge services architecture promises to improve the availability and performance of Web services by replicating servers at geographically distributed sites. A key challenge in such systems is data replication and consistency, so that edge server code can manipulate shared data without suffering the availability and performance penalties that would be incurred by accessing a traditional centralized database. This article explores using a distributed object architecture to build an edge service data replication system for an e-commerce application, the TPC-W benchmark, which simulates an online bookstore. We take advantage of application-specific semantics to design distributed objects that each manages a specific subset of shared information using simple and effective consistency models. Our experimental results show that by slightly relaxing consistency within individual distributed objects, our application realizes both high availability and excellent performance. For example, in one experiment, we find that our object-based edge server system provides five times better response time over a traditional centralized cluster architecture and a factor of nine improvement over an edge service system that distributes code but retains a centralized database.

#index 789006
#* Linear Temporal Sequences and Their Interpretation Using Midpoint Relationships
#@ John F. Roddick;Carl H. Mooney
#t 2005
#c 7
#% 399
#% 75818
#% 116335
#% 176883
#% 319244
#! The temporal interval relationships formalized by Allen, and later extended to accommodate semi-intervals by Freksa, have been widely utilized in both data modeling and artificial intelligence research to facilitate reasoning between the relative temporal ordering of events. In practice, however, some modifications to the relationships are necessary when linear temporal sequences are provided, when event times are aggregated, or when data is supplied to a granularity which is larger than required. This paper discusses these modifications and outlines a solution to this problem which accommodates any available knowledge of interval midpoints.

#index 789007
#* Mining Sequential Patterns from Multidimensional Sequence Data
#@ Chung-Ching Yu;Yen-Liang Chen
#t 2005
#c 7
#% 310559
#% 329537
#% 342666
#% 424908
#% 459006
#% 463903
#% 464996
#% 481290
#! The problem addressed in this paper is to discover the frequently occurred sequential patterns from databases. Although much work has been devoted to this subject, to the best of our knowledge, no previous research was able to find sequential patterns from d-dimensional sequence data, where d2. Without such a capability, many practical data would be impossible to mine. For example, an online stock-trading site may have a customer database, where each customer may visit a Web site in a series of days; each day takes a series of sessions and each session visits a series of Web pages. Then, the data for each customer forms a 3-dimensional list, where the first dimension is days, the second is sessions, and the third is visited pages. To mine sequential patterns from this kind of sequence data, two efficient algorithms have been developed in this paper.

#index 789008
#* Combining Partitional and Hierarchical Algorithms for Robust and Efficient Data Clustering with Cohesion Self-Merging
#@ Cheng-Ru Lin;Ming-Syan Chen
#t 2005
#c 7
#% 33306
#% 118771
#% 210173
#% 232102
#% 248790
#% 273891
#% 275360
#% 280407
#% 296738
#% 316709
#% 333933
#% 374537
#% 404505
#% 438137
#% 443082
#% 481281
#% 483675
#% 577229
#% 577280
#% 631985
#! Data clustering has attracted a lot of research attention in the field of computational statistics and data mining. In most related studies, the dissimilarity between two clusters is defined as the distance between their centroids or the distance between two closest (or farthest) data points. However, all of these measures are vulnerable to outliers and removing the outliers precisely is yet another difficult task. In view of this, we propose a new similarity measure, referred to as cohesion, to measure the intercluster distances. By using this new measure of cohesion, we have designed a two-phase clustering algorithm, called cohesion-based self-merging (abbreviated as CSM), which runs in time linear to the size of input data set. Combining the features of partitional and hierarchical clustering methods, algorithm CSM partitions the input data set into several small subclusters in the first phase and then continuously merges the subclusters based on cohesion in a hierarchical manner in the second phase. The time and the space complexities of algorithm CSM are analyzed. As shown by our performance studies, the cohesion-based clustering is very robust and possesses excellent tolerance to outliers in various workloads. More importantly, algorithm CSM is shown to be able to cluster the data sets of arbitrary shapes very efficiently and provide better clustering results than those by prior methods.

#index 789009
#* Fast Detection of XML Structural Similarity
#@ Sergio Flesca;Giuseppe Manco;Elio Masciari;Luigi Pontieri;Andrea Pugliese
#t 2005
#c 7
#% 66654
#% 210212
#% 265420
#% 291299
#% 359652
#% 431536
#% 460862
#% 462231
#% 464640
#% 479806
#% 480648
#% 480824
#% 495278
#% 534183
#% 629708
#% 659923
#% 729627
#% 734653
#! Because of the widespread diffusion of semistructured data in XML format, much research effort is currently devoted to support the storage and retrieval of large collections of such documents. XML documents can be compared as to their structural similarity, in order to group them into clusters so that different storage, retrieval, and processing techniques can be effectively exploited. In this scenario, an efficient and effective similarity function is the key of a successful data management process. We present an approach for detecting structural similarity between XML documents which significantly differs from standard methods based on graph-matching algorithms, and allows a significant reduction of the required computation costs. Our proposal roughly consists of linearizing the structure of each XML document, by representing it as a numerical sequence and, then, comparing such sequences through the analysis of their frequencies. First, some basic strategies for encoding a document are proposed, which can focus on diverse structural facets. Moreover, the theory of Discrete Fourier Transform is exploited to effectively and efficiently compare the encoded documents (i.e., signals) in the domain of frequencies. Experimental results reveal the effectiveness of the approach, also in comparison with standard methods.

#index 789010
#* Iterative Projected Clustering by Subspace Mining
#@ Man Lung Yiu;Nikos Mamoulis
#t 2005
#c 7
#% 248792
#% 273891
#% 300120
#% 300131
#% 397382
#% 397384
#% 438207
#% 480132
#% 481290
#% 481779
#% 577257
#% 614619
#% 659967
#% 727868
#% 727908
#% 729942
#! Irrelevant attributes add noise to high-dimensional clusters and render traditional clustering techniques inappropriate. Recently, several algorithms that discover projected clusters and their associated subspaces have been proposed. In this paper, we realize the analogy between mining frequent itemsets and discovering dense projected clusters around random points. Based on this, we propose a technique that improves the efficiency of a projected clustering algorithm (DOC). Our method is an optimized adaptation of the frequent pattern tree growth method used for mining frequent itemsets. We propose several techniques that employ the branch and bound paradigm to efficiently discover the projected clusters. An experimental study with synthetic and real data demonstrates that our technique significantly improves on the accuracy and speed of previous techniques.

#index 789011
#* Mining Closed and Maximal Frequent Subtrees from Databases of Labeled Rooted Trees
#@ Yun Chi;Yi Xia;Yirong Yang;Richard R. Muntz
#t 2005
#c 7
#% 222804
#% 262071
#% 300120
#% 408396
#% 466644
#% 522776
#% 577218
#% 587737
#% 629708
#% 727828
#% 727845
#% 727909
#% 729938
#% 729941
#% 737334
#% 745511
#% 765125
#% 814196
#! Tree structures are used extensively in domains such as computational biology, pattern recognition, XML databases, computer networks, and so on. One important problem in mining databases of trees is to find frequently occurring subtrees. Because of the combinatorial explosion, the number of frequent subtrees usually grows exponentially with the size of frequent subtrees and, therefore, mining all frequent subtrees becomes infeasible for large tree sizes. In this paper, we present CMTreeMiner, a computationally efficient algorithm that discovers only closed and maximal frequent subtrees in a database of labeled rooted trees, where the rooted trees can be either ordered or unordered. The algorithm mines both closed and maximal frequent subtrees by traversing an enumeration tree that systematically enumerates all frequent subtrees. Several techniques are proposed to prune the branches of the enumeration tree that do not correspond to closed or maximal frequent subtrees. Heuristic techniques are used to arrange the order of computation so that relatively expensive computation is avoided as much as possible. We study the performance of our algorithm through extensive experiments, using both synthetic data and data sets from real applications. The experimental results show that our algorithm is very efficient in reducing the search space and quickly discovers all closed and maximal frequent subtrees.

#index 789012
#* Outlier Mining in Large High-Dimensional Data Sets
#@ Fabrizio Angiulli;Clara Pizzuti
#t 2005
#c 7
#% 2115
#% 13032
#% 64431
#% 86951
#% 236617
#% 252608
#% 280515
#% 300136
#% 300183
#% 310552
#% 316709
#% 333929
#% 342625
#% 342641
#% 443397
#% 449400
#% 465015
#% 478624
#% 479791
#% 570886
#% 650942
#% 1499584
#! In this paper, a new definition of distance-based outlier and an algorithm, called HilOut, designed to efficiently detect the top n outliers of a large and high-dimensional data set are proposed. Given an integer k, the weight of a point is defined as the sum of the distances separating it from its k nearest-neighbors. Outlier are those points scoring the largest values of weight. The algorithm HilOut makes use of the notion of space-filling curve to linearize the data set, and it consists of two phases. The first phase provides an approximate solution, within a rough factor, after the execution of at most d + 1 sorts and scans of the data set, with temporal cost quadratic in d and linear in N and in k, where d is the number of dimensions of the data set and N is the number of points in the data set. During this phase, the algorithm isolates points candidate to be outliers and reduces this set at each iteration. If the size of this set becomes n, then the algorithm stops reporting the exact solution. The second phase calculates the exact solution with a final scan examining further the candidate outliers that remained after the first phase. Experimental results show that the algorithm always stops, reporting the exact solution, during the first phase after much less than d + 1 steps. We present both an in-memory and disk-based implementation of the HilOut algorithm and a thorough scaling analysis for real and synthetic data sets showing that the algorithm scales well in both cases.

#index 789013
#* Genetic Evolution Processing of Classification
#@ Siu-Yeung Cho;Zheru Chi
#t 2005
#c 7
#% 109728
#% 114994
#% 168999
#% 237658
#% 261209
#% 354202
#% 387300
#% 445326
#% 541079
#% 629994
#% 1777038
#% 1854380
#% 1854992
#% 1860077
#% 1860191
#% 1860363
#! This paper describes a method of structural pattern recognition based on a genetic evolution processing of data structures with neural networks representation. Conventionally, one of the most popular learning formulations of data structure processing is Backpropagation Through Structures (BPTS) [7]. The BPTS algorithm has been successfully applied to a number of learning tasks that involved structural patterns such as image, shape, and texture classifications. However, this BPTS typed algorithm suffers from the long-term dependency problem in learning very deep tree structures. In this paper, we propose the genetic evolution for this data structures processing. The idea of this algorithm is to tune the learning parameters by the genetic evolution with specified chromosome structures. Also, the fitness evaluation as well as the adaptive crossover and mutation for this structural genetic processing are investigated in this paper. An application to flowers image classification by a structural representation is provided for the validation of our method. The obtained results significantly support the capabilities of our proposed approach to classify and recognize flowers in terms of generalization and noise robustness.

#index 789014
#* Preserving Privacy by De-Identifying Face Images
#@ Elaine M. Newton;Latanya Sweeney;Bradley Malin
#t 2005
#c 7
#% 300184
#% 315986
#% 333876
#% 428404
#% 576761
#% 586838
#% 664070
#% 993988
#% 1862566
#! In the context of sharing video surveillance data, a significant threat to privacy is face recognition software, which can automatically identify known people, such as from a database of drivers' license photos, and thereby track people regardless of suspicion. This paper introduces an algorithm to protect the privacy of individuals in video surveillance data by de-identifying faces such that many facial characteristics remain but the face cannot be reliably recognized. A trivial solution to de-identifying faces involves blacking out each face. This thwarts any possible face recognition, but because all facial details are obscured, the result is of limited use. Many ad hoc attempts, such as covering eyes, fail to thwart face recognition because of the robustness of face recognition methods. This paper presents a new privacy-enabling algorithm, named k-Same, that guarantees face recognition software cannot reliably recognize de-identified faces, even though many facial details are preserved. The algorithm determines similarity between faces based on a distance metric and creates new faces by averaging image components, which may be the original image pixels (k--Same-Pixel) or eigenvectors (k-Same-Eigen). Results are presented on a standard collection of real face images with varying k.

#index 789015
#* Evaluation and Design of Online Cooperative Feedback Mechanisms for Reputation Management
#@ Ming Fan;Yong Tan;Andrew B. Whinston
#t 2005
#c 7
#% 314935
#% 326688
#% 591748
#% 1603129
#! This research evaluates and analyzes existing mechanisms of online reputation systems based on cooperative feedbacks of past transaction information. We find existing popular feedback systems do not provide sustained incentives for sellers to behave honestly over time. We propose a new design of reputation system based on exponential smoothing. This mechanism is shown to be more robust compared to the existing systems. We relax the assumption of a static product value and have implemented a two-level exponential smoothing policy to process and aggregate reputation information. The simulation results show that the policy can serve as a sustained incentive mechanism for the seller.

#index 789016
#* Rewriting Rules To Permeate Complex Similarity and Fuzzy Queries within a Relational Database System
#@ Wilma Penzo
#t 2005
#c 7
#% 41230
#% 136347
#% 182919
#% 212685
#% 213981
#% 215225
#% 227894
#% 238757
#% 246223
#% 292029
#% 326669
#% 333349
#% 333854
#% 333951
#% 359751
#% 384978
#% 397378
#% 411759
#% 437405
#% 443243
#% 443454
#% 458860
#% 464726
#% 479968
#% 481604
#% 493673
#% 571078
#% 586834
#% 1780690
#% 1788921
#! In recent years, the availability of complex data repositories (e.g., multimedia, genomic, semistructured databases) has paved the way to new potentials as to data querying. In this scenario, similarity and fuzzy techniques have proven to be successful principles for effective data retrieval. However, most proposals are domain specific and lack of a general and integrated approach to deal with generalized complex queries, i.e., queries where multiple conditions are expressed, possibly on complex as well as on traditional data. To overcome such limitations, much work has been devoted to the development of middleware systems to support query processing on multiple repositories. On a similar line, in this paper we present a formal framework to permeate complex similarity and fuzzy queries within a relational database system. As an example, we focus on multimedia data, which is represented in an integrated view with common database data. We have designed an application layer that relies on an algebraic query language, extended with MM-tailored operators, and that maps complex similarity and fuzzy queries to standard SQL statements that can be processed by a relational database system, exploiting standard facilities of modern extensible RDBMS. To show the applicability of our proposal, we implemented a prototype that provides the user with rich query capabilities, ranging from traditional database queries to complex queries gathering a mixture of Boolean, similarity, and fuzzy predicates on the data.

#index 789017
#* Spatiotemporal Aggregate Computation: A Survey
#@ Ines Fernando Vega Lopez;Richard T. Snodgrass;Bongki Moon
#t 2005
#c 7
#% 163440
#% 209730
#% 211085
#% 227866
#% 227883
#% 234756
#% 252304
#% 260065
#% 273910
#% 286256
#% 287268
#% 287362
#% 289370
#% 296159
#% 296635
#% 300173
#% 319601
#% 333872
#% 333874
#% 333955
#% 333977
#% 338425
#% 348603
#% 365700
#% 369764
#% 378388
#% 378398
#% 379445
#% 382477
#% 390187
#% 397354
#% 397386
#% 413604
#% 417820
#% 417826
#% 420053
#% 421063
#% 421072
#% 421073
#% 421120
#% 443363
#% 452818
#% 458843
#% 458857
#% 458858
#% 465010
#% 465162
#% 479910
#% 480628
#% 480801
#% 480817
#% 481608
#% 481951
#% 527166
#% 527177
#% 527189
#% 527190
#% 527194
#% 527195
#% 527326
#% 565462
#% 578404
#% 631922
#% 632071
#% 645385
#% 659917
#% 659961
#% 660007
#% 709080
#% 715707
#% 726621
#% 745442
#% 745458
#% 745485
#% 745486
#% 993998
#% 1393684
#! Spatiotemporal databases are becoming increasingly more common. Typically, applications modeling spatiotemporal objects need to process vast amounts of data. In such cases, generating aggregate information from the data set is more useful than individually analyzing every entry. In this paper, we study the most relevant techniques for the evaluation of aggregate queries on spatial, temporal, and spatiotemporal data. We also present a model that reduces the evaluation of aggregate queries to the problem of selecting qualifying tuples and the grouping of these tuples into collections on which an aggregate function is to be applied. This model give us a framework that allows us to analyze and compare the different existing techniques for the evaluation of aggregate queries. At the same time, it allows us to identify opportunities for research on types of aggregate queries that have not been studied.

#index 796197
#* Using AUC and Accuracy in Evaluating Learning Algorithms
#@ Jin Huang;Charles X. Ling
#t 2005
#c 7
#% 90156
#% 116149
#% 136350
#% 309208
#% 349550
#% 420077
#% 420146
#% 464606
#% 466086
#% 566871
#% 580510
#% 1272396
#% 1378224
#! The area under the ROC (Receiver Operating Characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. In this paper, we establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.

#index 796198
#* A Tree-Structured Index Allocation Method with Replication over Multiple Broadcast Channels in Wireless Environments
#@ Sungwon Jung;Byungkyu Lee;Sakti Pramanik
#t 2005
#c 7
#% 172913
#% 175253
#% 199555
#% 201897
#% 235940
#% 247246
#% 285813
#% 316486
#% 437181
#% 442624
#% 443127
#% 554886
#% 554894
#% 632025
#% 632067
#% 635901
#% 717717
#% 730027
#! Broadcast has often been used to disseminate frequently requested data efficiently to a large volume of mobile units over single or multiple channels. Since mobile units have limited battery power, the minimization of the access and tuning times for the broadcast data is an important problem. There have been many research efforts that focus on minimizing access and tuning times by providing indexes on the broadcast data. In this paper, we have studied an efficient index allocation method for broadcast data with skewed access frequencies over multiple physical channels which cannot be coalesced into a single high bandwidth channel. Previously proposed index allocation techniques have one of two problems. The first problem is that they require equal size for both index and data. The second problem is that their performance degrades when the number of given physical channels is not enough. These two problems will result in an increased average access time for the broadcast data. To cope with these problems, we propose a tree-structured index allocation method. Our method minimizes the average access time by broadcasting the hot data and their indices more frequently than the less hot data and their indexes over the dedicated index and data channels. We present an in-depth experimental and theoretical analysis of our method by comparing it with other similar techniques. Our performance analysis shows that it significantly decreases the average access and tuning times for the broadcast data over existing methods.

#index 796199
#* Adaptive Data Access in Broadcast-Based Wireless Environments
#@ Xu Yang;Athman Bouguettaya
#t 2005
#c 7
#% 169835
#% 172876
#% 245014
#% 247246
#% 287068
#% 287258
#% 330905
#% 443365
#% 458839
#% 631861
#% 632027
#% 635819
#% 640202
#! Power conservation and client waiting time reduction are two important aspects of data access efficiency in broadcast-based wireless communication systems. The intention of data access methods is to optimize client power consumption with the least possible overhead on client waiting time. In this paper, we propose an adaptive data access method which builds on the strengths of indexing and hashing techniques. We show that this method exhibits better average performance over the well-known index tree-based access methods. A new performance model is also proposed. This model uses more realistic assessment criteria, based on the combination of access and tuning times, for evaluating wireless access methods. This new model provides a dynamic framework to express the degree of importance of access and tuning times in an application. Under this new model, the adaptive method performance also outperforms the other access methods in the majority of cases.

#index 796200
#* Divide-and-Approximate: A Novel Constraint Push Strategy for Iceberg Cube Mining
#@ Ke Wang;Yuelong Jiang;Jeffrey Xu Yu;Guozhu Dong;Jiawei Han
#t 2005
#c 7
#% 152934
#% 210182
#% 227866
#% 227880
#% 248785
#% 248791
#% 273916
#% 280409
#% 333925
#% 342667
#% 464989
#% 465003
#% 479795
#% 480154
#% 481951
#% 631970
#! The iceberg cube mining computes all cells v, corresponding to GROUP BY partitions, that satisfy a given constraint on aggregated behaviors of the tuples in a GROUP BY partition. The number of cells often is so large that the result cannot be realistically searched without pushing the constraint into the search. Previous works have pushed antimonotone and monotone constraints. However, many useful constraints are neither antimonotone nor monotone. We consider a general class of aggregate constraints of the form f(v)\theta \sigma, where f is an arithmetic function of SQL-like aggregates and \theta is one of . We propose a novel pushing technique, called Divide-and-Approximate, to push such constraints. The idea is to recursively divide the search space and approximate the given constraint using antimonotone or monotone constraints in subspaces. This technique applies to a class called separable constraints, which properly contains all constraints built by an arithmetic function f of all SQL aggregates.

#index 796201
#* Indexing High-Dimensional Data for Efficient In-Memory Similarity Search
#@ Bin Cui;Beng Chin Ooi;Jianwen Su;Kian-Lee Tan
#t 2005
#c 7
#% 227937
#% 248796
#% 300194
#% 333940
#% 333942
#% 333949
#% 342828
#% 435141
#% 443482
#% 465160
#% 479462
#% 479649
#% 480307
#% 480632
#% 654481
#% 762657
#! In main memory systems, the L2 cache typically employs cache line sizes of 32-128 bytes. These values are relatively small compared to high-dimensional data, e.g., 32D. The consequence is that existing techniques (on low-dimensional data) that minimize cache misses are no longer effective. In this paper, we present a novel index structure, called \Delta{\hbox{-}}{\rm{tree}}, to speed up the high-dimensional query in main memory environment. The \Delta{\hbox{-}}{\rm{tree}} is a multilevel structure where each level represents the data space at different dimensionalities: the number of dimensions increases toward the leaf level. The remaining dimensions are obtained using Principal Component Analysis. Each level of the tree serves to prune the search space more efficiently as the lower dimensions can reduce the distance computation and better exploit the small cache line size. Additionally, the top-down clustering scheme can capture the feature of the data set and, hence, reduces the search space. We also propose an extension, called \Delta^+{\hbox{-}}{\rm{tree}}, that globally clusters the data space and then partitions clusters into small regions. The \Delta^+{\hbox{-}}{\rm{tree}} can further reduce the computational cost and cache misses. We conducted extensive experiments to evaluate the proposed structures against existing techniques on different kinds of data sets. Our results show that the \Delta^+{\hbox{-}}{\rm{tree}} is superior in most cases.

#index 796202
#* Projective Clustering by Histograms
#@ Eric Ka Ka Ng;Ada Wai-chee Fu;Raymond Chi-Wing Wong
#t 2005
#c 7
#% 152934
#% 201921
#% 210173
#% 248792
#% 273890
#% 273891
#% 280417
#% 296738
#% 300131
#% 342613
#% 397384
#% 443480
#% 459020
#% 479962
#% 481281
#% 481290
#% 566128
#% 632029
#% 659938
#! Recent research suggests that clustering for high-dimensional data should involve searching for "hidden驴 subspaces with lower dimensionalities, in which patterns can be observed when data objects are projected onto the subspaces. Discovering such interattribute correlations and location of the corresponding clusters is known as the projective clustering problem. In this paper, we propose an efficient projective clustering technique by histogram construction (EPCH). The histograms help to generate "signatures,驴 where a signature corresponds to some region in some subspace, and signatures with a large number of data objects are identified as the regions for subspace clusters. Hence, projected clusters and their corresponding subspaces can be uncovered. Compared to the best previous methods to our knowledge, this approach is more flexible in that less prior knowledge on the data set is required, and it is also much more efficient. Our experiments compare behaviors and performances of this approach and other projective clustering algorithms with different data characteristics. The results show that our technique is scalable to very large databases, and it is able to return accurate clustering results.

#index 796203
#* Semisupervised Learning of Hierarchical Latent Trait Models for Data Visualization
#@ Ian T. Nabney;Yi Sun;Peter Tino;Ata Kaban
#t 2005
#c 7
#% 103743
#% 164241
#% 234978
#% 251155
#% 257039
#% 341446
#% 341451
#% 345829
#% 349210
#% 424810
#% 471428
#% 494822
#% 619523
#% 726016
#% 726032
#% 729925
#% 729976
#! Recently, we have developed the hierarchical Generative Topographic Mapping (HGTM), an interactive method for visualization of large high-dimensional real-valued data sets. In this paper, we propose a more general visualization system by extending HGTM in three ways, which allows the user to visualize a wider range of data sets and better support the model development process. 1) We integrate HGTM with noise models from the exponential family of distributions. The basic building block is the Latent Trait Model (LTM). This enables us to visualize data of inherently discrete nature, e.g., collections of documents, in a hierarchical manner. 2) We give the user a choice of initializing the child plots of the current plot in either interactive, or automatic mode. In the interactive mode, the user selects "regions of interest,驴 whereas in the automatic mode, an unsupervised minimum message length (MML)-inspired construction of a mixture of LTMs is employed. The unsupervised construction is particularly useful when high-level plots are covered with dense clusters of highly overlapping data projections, making it difficult to use the interactive mode. Such a situation often arises when visualizing large data sets. 3) We derive general formulas for magnification factors in latent trait models. Magnification factors are a useful tool to improve our understanding of the visualization plots, since they can highlight the boundaries between data clusters. We illustrate our approach on a toy example and evaluate it on three more complex real data sets.

#index 796204
#* Relevant Data Expansion for Learning Concept Drift from Sparsely Labeled Data
#@ Dwi H. Widyantoro;John Yen
#t 2005
#c 7
#% 36672
#% 66937
#% 165110
#% 170405
#% 194985
#% 204531
#% 214321
#% 219049
#% 234990
#% 241033
#% 241036
#% 252011
#% 252753
#% 262084
#% 266787
#% 271082
#% 290149
#% 306468
#% 323130
#% 376266
#% 394709
#% 406493
#% 430759
#% 445316
#% 466408
#% 466580
#% 565545
#% 629674
#! Keeping track of changing interests is a natural phenomenon as well as an interesting tracking problem because interests can emerge and diminish at different time frames. Being able to do so with a few feedback examples poses an even more important and challenging problem because existing concept drift learning algorithms that handle the task typically suffer from it. This paper presents a new computational Framework for Extending Incomplete Labeled Data Stream (FEILDS), which extends the capability of existing algorithms for learning concept drift from a few labeled data. The system transforms the original input stream into a new stream that can be conveniently tracked by the existing learning algorithms. The experiment results reveal that FEILDS can significantly improve the performances of a Multiple Three-Descriptor Representation (MTDR) algorithm, Rocchio algorithm, and window-based concept drift learning algorithms when learning from a sparsely labeled data stream with respect to their performances without using FEILDS.

#index 796205
#* Analysis of Data Structures for Admission Control of Advance Reservation Requests
#@ Lars-Olof Burchard
#t 2005
#c 7
#% 269942
#% 422880
#% 511305
#% 520308
#% 521464
#% 553162
#% 848895
#% 1408955
#! Advance reservations are a useful method to allocate resources of various kinds in many different environments. Among others, a major advantage of this kind of reservation is the improved admission probability for requests that are made sufficiently early. In order to implement reliable admission control for these requests, it is important to store information in data structures about future allocations and to provide fast access to the available information. In this paper, two data structures are examined: a tree specially designed to support advance reservations is compared to arrays. Both structures are examined analytically and by measurements in a realistic network management system capable of advance reservations. It turns out that arrays are far better suited to support the required operations, in particular when large time intervals need to be scanned.

#index 796206
#* A Flexible Payment Scheme and Its Role-Based Access Control
#@ Hua Wang;Jinli Cao;Yanchun Zhang
#t 2005
#c 7
#% 249199
#% 258394
#% 264249
#% 270775
#% 286490
#% 308087
#% 319994
#% 342328
#% 379539
#% 493350
#% 513093
#% 514181
#% 536480
#% 554213
#% 567324
#% 664073
#% 978231
#% 978272
#% 1784693
#! This paper proposes a practical payment protocol with scalable anonymity for Internet purchases, and analyzes its role-based access control (RBAC). The protocol uses electronic cash for payment transactions. It is an offline payment scheme that can prevent a consumer from spending a coin more than once. Consumers can improve anonymity if they are worried about disclosure of their identities to banks. An agent provides high anonymity through the issue of a certification. The agent certifies reencrypted data after verifying the validity of the content from consumers, but with no private information of the consumers required. With this new method, each consumer can get the required anonymity level, depending on the available time, computation, and cost. We use RBAC to manage the new payment scheme and improve its integrity. With RBAC, each user may be assigned one or more roles, and each role can be assigned one or more privileges that are permitted to users in that role. To reduce conflicts of different roles and decrease complexities of administration, duty separation constraints, role hierarchies, and scenarios of end-users are analyzed.

#index 796207
#* An Extended Chi2 Algorithm for Discretization of Real Value Attributes
#@ Chao-Ton Su;Jyh-Hwa Hsu
#t 2005
#c 7
#% 154305
#% 156186
#% 443148
#% 443510
#% 501311
#% 501491
#% 998649
#! The Variable Precision Rough Sets (VPRS) model is a powerful tool for data mining, as it has been widely applied to acquire knowledge. Despite its diverse applications in many domains, the VPRS model unfortunately cannot be applied to real-world classification tasks involving continuous attributes. This requires a discretization method to preprocess the data. Discretization is an effective technique to deal with continuous attributes for data mining, especially for the classification problem. The modified Chi2 algorithm is one of the modifications to the Chi2 algorithm, replacing the inconsistency check in the Chi2 algorithm by using the quality of approximation, coined from the Rough Sets Theory (RST), in which it takes into account the effect of degrees of freedom. However, the classification with a controlled degree of uncertainty, or a misclassification error, is outside the realm of RST. This algorithm also ignores the effect of variance in the two merged intervals. In this study, we propose a new algorithm, named the extended Chi2 algorithm, to overcome these two drawbacks. By running the software of See5, our proposed algorithm possesses a better performance than the original and modified Chi2 algorithms.

#index 796208
#* EIC Editorial: TKDE Editorial Board Changes
#@ Xindong Wu
#t 2005
#c 7

#index 796209
#* Compression, Clustering, and Pattern Discovery in Very High-Dimensional Discrete-Attribute Data Sets
#@ Mehmet Koyuturk;Ananth Grama;Naren Ramakrishnan
#t 2005
#c 7
#% 200694
#% 274612
#% 280406
#% 314054
#% 319234
#% 322619
#% 415792
#% 420083
#% 420091
#% 478767
#% 481290
#% 481779
#% 570885
#% 614619
#% 729924
#% 731279
#% 754412
#! This paper presents an efficient framework for error-bounded compression of high-dimensional discrete-attribute data sets. Such data sets, which frequently arise in a wide variety of applications, pose some of the most significant challenges in data analysis. Subsampling and compression are two key technologies for analyzing these data sets. The proposed framework, PROXIMUS, provides a technique for reducing large data sets into a much smaller set of representative patterns, on which traditional (expensive) analysis algorithms can be applied with minimal loss of accuracy. We show desirable properties of PROXIMUS in terms of runtime, scalability to large data sets, and performance in terms of capability to represent data in a compact form and discovery and interpretation of interesting patterns. We also demonstrate sample applications of PROXIMUS in association rule mining and semantic classification of term-document matrices. Our experimental results on real data sets show that use of the compressed data for association rule mining provides excellent precision and recall values (above 90 percent) across a range of problem parameters while reducing the time required for analysis drastically. We also show excellent interpretability of the patterns discovered by PROXIMUS in the context of clustering and classification of terms and documents. In doing so, we establish PROXIMUS as a tool for both preprocessing data before applying computationally expensive algorithms and directly extracting correlated patterns.

#index 796210
#* Efficient Algorithms for Mining Closed Itemsets and Their Lattice Structure
#@ Mohammed J. Zaki;Ching-Jui Hsiao
#t 2005
#c 7
#% 201894
#% 227917
#% 232136
#% 248791
#% 296633
#% 300120
#% 300124
#% 310494
#% 310507
#% 316709
#% 338594
#% 384416
#% 443350
#% 459020
#% 465003
#% 466664
#% 481754
#% 631986
#% 729933
#% 729942
#! The set of frequent closed itemsets uniquely determines the exact frequency of all itemsets, yet it can be orders of magnitude smaller than the set of all frequent itemsets. In this paper, we present CHARM, an efficient algorithm for mining all frequent closed itemsets. It enumerates closed sets using a dual itemset-tidset search tree, using an efficient hybrid search that skips many levels. It also uses a technique called diffsets to reduce the memory footprint of intermediate computations. Finally, it uses a fast hash-based approach to remove any "nonclosed驴 sets found during computation. We also present CHARM-L, an algorithm that outputs the closed itemset lattice, which is very useful for rule generation and visualization. An extensive experimental evaluation on a number of real and synthetic databases shows that CHARM is a state-of-the-art algorithm that outperforms previous methods. Further, CHARM-L explicitly generates the frequent closed itemset lattice.

#index 796211
#* Simulated Annealing Using a Reversible Jump Markov Chain Monte Carlo Algorithm for Fuzzy Clustering
#@ Sanghamitra Bandyopadhyay
#t 2005
#c 7
#% 36672
#% 51676
#% 78792
#% 104472
#% 107459
#% 318800
#% 374537
#% 443948
#% 447451
#% 856734
#% 856769
#% 1787857
#% 1788006
#% 1788944
#% 1862532
#! In this paper, an approach for automatically clustering a data set into a number of fuzzy partitions with a simulated annealing using a Reversible Jump Markov Chain Monte Carlo algorithm is proposed. This is in contrast to the widely used fuzzy clustering scheme, the Fuzzy C-Means (FCM) algorithm, which requires the a priori knowledge of the number of clusters. The said approach performs the clustering by optimizing a cluster validity index, the Xie-Beni index. It makes use of the homogeneous Reversible Jump Markov Chain Monte Carlo (RJMCMC) kernel as the proposal so that the algorithm is able to jump between different dimensions, i.e., number of clusters, until the correct value is obtained. Different moves, like birth, death, split, merge, and update, are used for sampling a candidate state given the current state. The effectiveness of the proposed technique in optimizing the Xie-Beni index and thereby determining the appropriate clustering is demonstrated for both artificial and real-life data sets. In a part of the investigation, the utility of the fuzzy clustering scheme for classifying pixels in an IRS satellite image of Kolkata is studied. A technique for reducing the computation efforts in the case of satellite image data is incorporated.

#index 796212
#* Toward Integrating Feature Selection Algorithms for Classification and Clustering
#@ Huan Liu;Lei Yu
#t 2005
#c 7
#% 24175
#% 132927
#% 167633
#% 169659
#% 187739
#% 227486
#% 229963
#% 232102
#% 232147
#% 243727
#% 243728
#% 269634
#% 290482
#% 310561
#% 311027
#% 316709
#% 332082
#% 332083
#% 345824
#% 379236
#% 379237
#% 379238
#% 379239
#% 379240
#% 385563
#% 385564
#% 391410
#% 420138
#% 420139
#% 420140
#% 420146
#% 425047
#% 445219
#% 452821
#% 458261
#% 458371
#% 464444
#% 464614
#% 464630
#% 465741
#% 465754
#% 465905
#% 466269
#% 466410
#% 466414
#% 466912
#% 564259
#% 565527
#% 586825
#% 629628
#% 634536
#% 661048
#% 765518
#% 769966
#% 1390189
#! This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to demonstrate the use of feature selection in data mining. We conclude this work by identifying trends and challenges of feature selection research and development.

#index 796213
#* Toward Intelligent Assistance for a Data Mining Process: An Ontology-Based Approach for Cost-Sensitive Classification
#@ Abraham Bernstein;Foster Provost;Shawndra Hill
#t 2005
#c 7
#% 17630
#% 21298
#% 42994
#% 116704
#% 169653
#% 191910
#% 197056
#% 197059
#% 197060
#% 216500
#% 243728
#% 280406
#% 280499
#% 290482
#% 310495
#% 314784
#% 338609
#% 342611
#% 420091
#% 459716
#% 465753
#% 466722
#% 477491
#% 723244
#% 764473
#! A data mining (DM) process involves multiple stages. A simple, but typical, process might include preprocessing data, applying a data mining algorithm, and postprocessing the mining results. There are many possible choices for each stage, and only some combinations are valid. Because of the large space and nontrivial interactions, both novices and data mining specialists need assistance in composing and selecting DM processes. Extending notions developed for statistical expert systems we present a prototype Intelligent Discovery Assistant (IDA), which provides users with 1) systematic enumerations of valid DM processes, in order that important, potentially fruitful options are not overlooked, and 2) effective rankings of these valid processes by different criteria, to facilitate the choice of DM processes to execute. We use the prototype to show that an IDA can indeed provide useful enumerations and effective rankings in the context of simple classification processes. We discuss how an IDA could be an important tool for knowledge sharing among a team of data miners. Finally, we illustrate the claims with a demonstration of cost-sensitive classification using a more complicated process and data from the 1998 KDDCUP competition.

#index 796214
#* Mining and Reasoning on Workflows
#@ Gianluigi Greco;Antonella Guzzo;Giuseppe Manco;Domenico Sacca
#t 2005
#c 7
#% 185412
#% 203029
#% 246009
#% 248013
#% 273709
#% 300120
#% 310542
#% 345694
#% 346653
#% 408396
#% 420087
#% 459021
#% 461900
#% 463903
#% 464996
#% 466490
#% 466644
#% 481290
#% 488623
#% 509525
#% 562155
#% 565496
#% 577218
#% 629708
#% 729938
#% 733140
#% 993989
#! Today's workflow management systems represent a key technological infrastructure for advanced applications that is attracting a growing body of research, mainly focused in developing tools for workflow management, that allow users both to specify the "static驴 aspects, like preconditions, precedences among activities, and rules for exception handling, and to control its execution by scheduling the activities on the available resources. This paper deals with an aspect of workflows which has so far not received much attention even though it is crucial for the forthcoming scenarios of large scale applications on the Web: Providing facilities for the human system administrator for identifying the choices performed more frequently in the past that had lead to a desired final configuration. In this context, we formalize the problem of discovering the most frequent patterns of executions, i.e., the workflow substructures that have been scheduled more frequently by the system. We attacked the problem by developing two data mining algorithms on the basis of an intuitive and original graph formalization of a workflow schema and its occurrences. The model is used both to prove some intractability results that strongly motivate the use of data mining techniques and to derive interesting structural properties for reducing the search space for frequent patterns. Indeed, the experiments we have carried out show that our algorithms outperform standard data mining algorithms adapted to discover frequent patterns of workflow executions.

#index 796215
#* Antipole Tree Indexing to Support Range Search and K-Nearest Neighbor Search in Metric Spaces
#@ Domenico Cantone;Alfredo Ferro;Alfredo Pulvirenti;Diego Reforgiato Recupero;Dennis Shasha
#t 2005
#c 7
#% 54221
#% 84654
#% 114667
#% 144454
#% 175982
#% 193136
#% 210173
#% 232766
#% 248790
#% 248792
#% 249321
#% 271236
#% 281750
#% 282441
#% 287466
#% 294634
#% 320216
#% 322309
#% 336585
#% 340387
#% 342827
#% 376266
#% 443517
#% 443531
#% 479462
#% 479973
#% 481460
#% 481956
#% 503233
#% 544365
#% 546130
#% 570889
#% 571043
#% 572265
#% 617213
#% 631984
#% 632011
#% 715568
#% 841716
#! Range and k-nearest neighbor searching are core problems in pattern recognition. Given a database S of objects in a metric space M and a query object q in M, in a range searching problem the goal is to find the objects of S within some threshold distance to q, whereas in a k-nearest neighbor searching problem, the k elements of S closest to q must be produced. These problems can obviously be solved with a linear number of distance calculations, by comparing the query object against every object in the database. However, the goal is to solve such problems much faster. We combine and extend ideas from the M-Tree, the Multivantage Point structure, and the FQ-Tree to create a new structure in the "bisector tree驴 class, called the Antipole Tree. Bisection is based on the proximity to an "Antipole驴 pair of elements generated by a suitable linear randomized tournament. The final winners a,b of such a tournament are far enough apart to approximate the diameter of the splitting set. If {\rm{dist}}(a,b) is larger than the chosen cluster diameter threshold, then the cluster is split. The proposed data structure is an indexing scheme suitable for (exact and approximate) best match searching on generic metric spaces. The Antipole Tree outperforms by a factor of approximately two existing structures such as List of Clusters, M-Trees, and others and, in many cases, it achieves better clustering properties.

#index 796216
#* Comparison of Database Replication Techniques Based on Total Order Broadcast
#@ Matthias Wiesmann;Andre Schiper
#t 2005
#c 7
#% 9241
#% 27057
#% 51392
#% 86476
#% 204913
#% 210179
#% 275312
#% 323980
#% 427216
#% 451431
#% 480310
#% 484517
#% 507601
#% 508197
#% 566179
#% 594328
#% 617442
#% 617445
#% 629925
#% 635834
#% 635932
#% 636006
#% 648693
#% 675320
#% 682226
#% 721142
#% 1782847
#! In this paper, we present a performance comparison of database replication techniques based on total order broadcast. While the performance of total order broadcast-based replication techniques has been studied in previous papers, this paper presents many new contributions. First, it compares with each other techniques that were presented and evaluated separately, usually by comparing them to a classical replication scheme like distributed locking. Second, the evaluation is done using a finer network model than previous studies. Third, the paper compares techniques that offer the same consistency criterion (one-copy serializability) in the same environment using the same settings. The paper shows that, while networking performance has little influence in a LAN setting, the cost of synchronizing replicas is quite high. Because of this, total order broadcast-based techniques are very promising as they minimize synchronization between replicas.

#index 796217
#* Top-k Spatial Joins
#@ Manli Zhu;Dimitris Papadias;Jun Zhang;Dik Lun Lee
#t 2005
#c 7
#% 13041
#% 86950
#% 152937
#% 172909
#% 201876
#% 210186
#% 210187
#% 227932
#% 248804
#% 287466
#% 300162
#% 300174
#% 333973
#% 342956
#% 378398
#% 413806
#% 427199
#% 452852
#% 462957
#% 465060
#% 479453
#% 479797
#% 480093
#% 480819
#% 481956
#% 527180
#% 527189
#% 554911
#% 617845
#% 659919
#% 784515
#% 1015317
#! Given two spatial data sets A and B, a top-k spatial join retrieves the k objects from A or B that intersect the largest number of objects from the other data set. Depending on the application requirements, there exist several variations of the problem. For instance, B may be a point data set, and the goal may be to retrieve the regions of A that contain the maximum number of points. The processing of such queries with conventional spatial join algorithms is expensive. However, several improvements are possible based on the fact that we only require a small subset of the result (instead of all intersection/containments pairs). In this paper, we propose output-sensitive algorithms for top-k spatial joins that utilize a variety of optimizations for reducing the overhead.

#index 796218
#* Fast Recognition of Musical Genres Using RBF Networks
#@ Douglas Turnbull;Charles Elkan
#t 2005
#c 7
#% 190611
#% 361100
#% 413620
#% 729437
#% 733364
#% 741122
#% 741169
#% 1650609
#% 1860274
#! This paper explores the automatic classification of audio tracks into musical genres. Our goal is to achieve human-level accuracy with fast training and classification. This goal is achieved with radial basis function (RBF) networks by using a combination of unsupervised and supervised initialization methods. These initialization methods yield classifiers that are as accurate as RBF networks trained with gradient descent (which is hundreds of times slower). In addition, feature subset selection further reduces training and classification time while preserving classification accuracy. Combined, our methods succeed in creating an RBF network that matches the musical classification accuracy of humans. The general algorithmic contribution of this paper is to show experimentally that RBF networks initialized with a combination of methods can yield good classification performance without relying on gradient descent. The simplicity and computational efficiency of our initialization methods produce classifiers that are fast to train as well as fast to apply to novel data. We also present an improved method for initializing the k{\hbox{-}}{\rm means} clustering algorithm which is useful for both unsupervised and supervised initialization methods.

#index 800176
#* On Change Diagnosis in Evolving Data Streams
#@ Charu C. Aggarwal
#t 2005
#c 7
#% 227859
#% 273693
#% 297183
#% 308435
#% 310488
#% 310500
#% 342600
#% 345857
#% 464204
#% 481290
#% 527177
#% 577220
#% 578560
#% 632056
#% 632090
#% 654489
#% 659972
#% 729943
#% 769927
#% 1015261
#% 1016200
#! In recent years, the progress in hardware technology has made it possible for organizations to store and record large streams of transactional data. This results in databases which grow without limit at a rapid rate. This data can often show important changes in trends over time. In such cases, it is useful to understand, visualize, and diagnose the evolution of these trends. In this paper, we introduce the concept of velocity density estimation, a technique used to understand, visualize, and determine trends in the evolution of fast data streams. We show how to use velocity density estimation in order to create both temporal velocity profiles and spatial velocity profiles at periodic instants in time. These profiles are then used in order to predict three kinds of data evolution: dissolution, coagulation, and shift. Methods are proposed to visualize the changing data trends in a single online scan of the data stream and a computational requirement which is linear in the number of data points. The visualization techniques can also be used to provide online animations which show the changes in the data characteristics while they occur. In addition, batch processing techniques are proposed in order to quantify the level of change across different combinations of dimensions. This quantification is then used in order to determine dimensional combinations with significant evolution. The techniques discussed in this paper can be easily extended to spatiotemporal data, changes in data snapshots at fixed instances in time, or any other data which has a temporal component during its evolution.

#index 800177
#* A Fuzzy-Set-Based Reconstructed Phase Space Method for Idenitification of Temporal Patterns in Complex Time Series
#@ Xin Feng;Hai Huang
#t 2005
#c 7
#% 106716
#% 132590
#% 221502
#% 223517
#% 224182
#% 260645
#% 369236
#% 452860
#% 1860421
#! The new time series data mining framework proposed in this paper applies Reconstructured Phase Space (RPS) to identify temporal patterns that are characteristic and predictive of significant events in a complex time series. The new framework utilizes the fuzzy set and the Gaussian-shaped membership function to define temporal patterns in the time-delay embedding phase space. The resulting objective function represents not only the overall value of the event function, but also the weight of the vector in the temporal pattern cluster to which it contributes. Also, the new objective function is continuously differentiable so the gradient descent optimization such as quasi-Newton's method can be applied to search the optimal temporal patterns with much faster speed of convergence. The computational stability is significantly improved over the genetic algorithm originally used in our early framework. A new simple but effective two-step optimization strategy is proposed which further improves the search performance. Another significant contribution is the use of mutual information and false neighbors methods to estimate the time delay and the phase space dimension. We also implemented two experimental applications to demonstrate the effectiveness of the new framework with comparisons to the original framework and to the neural network prediction approach.

#index 800178
#* WISDOM: Web Intrapage Informative Structure Mining Based on Document Object Model
#@ Hung-Yu Kao;Jan-Ming Ho;Ming-Syan Chen
#t 2005
#c 7
#% 67565
#% 248808
#% 255137
#% 273925
#% 275915
#% 282905
#% 283050
#% 312861
#% 316530
#% 330676
#% 348180
#% 387427
#% 397605
#% 413616
#% 413617
#% 443349
#% 502137
#% 502140
#% 546376
#% 577281
#% 577323
#% 659925
#% 661023
#% 729628
#! To increase the commercial value and accessibility of pages, most content sites tend to publish their pages with intrasite redundant information, such as navigation panels, advertisements, and copyright announcements. Such redundant information increases the index size of general search engines and causes page topics to drift. In this paper, we study the problem of mining intrapage informative structure in news Web sites in order to find and eliminate redundant information. Note that intrapage informative structure is a subset of the original Web page and is composed of a set of fine-grained and informative blocks. The intrapage informative structures of pages in a news Web site contain only anchors linking to news pages or bodies of news articles. We propose an intrapage informative structure mining system called WISDOM (Web Intrapage Informative Structure Mining based on the Document Object Model) which applies Information Theory to DOM tree knowledge in order to build the structure. WISDOM splits a DOM tree into many small subtrees and applies a top-down informative block searching algorithm to select a set of candidate informative blocks. The structure is built by expanding the set using proposed merging methods. Experiments on several real news Web sites show high precision and recall rates which validates WISDOM's practical applicability.

#index 800179
#* Dual Clustering: Integrating Data Clustering over Optimization and Constraint Domains
#@ Cheng-Ru Lin;Ken-Hao Liu;Ming-Syan Chen
#t 2005
#c 7
#% 33306
#% 190581
#% 232102
#% 275360
#% 316709
#% 374537
#% 404505
#% 431295
#% 443082
#% 443689
#% 465004
#% 480632
#% 502122
#% 577280
#% 722810
#% 1393856
#% 1860692
#% 1860941
#! Spatial clustering has attracted a lot of research attention due to its various applications. In most conventional clustering problems, the similarity measurement mainly takes the geometric attributes into consideration. However, in many real applications, the nongeometric attributes are what users are concerned about. In the conventional spatial clustering, the input data set is partitioned into several compact regions and data points which are similar to one another in their nongeometric attributes may be scattered over different regions, thus making the corresponding objective difficult to achieve. To remedy this, we propose and explore in this paper a new clustering problem on two domains, called dual clustering, where one domain refers to the optimization domain and the other refers to the constraint domain. Attributes on the optimization domain are those involved in the optimization of the objective function, while those on the constraint domain specify the application dependent constraints. Our goal is to optimize the objective function in the optimization domain while satisfying the constraint specified in the constraint domain. We devise an efficient and effective algorithm, named Interlaced Clustering-Classification, abbreviated as ICC, to solve this problem. The proposed ICC algorithm combines the information in both domains and iteratively performs a clustering algorithm on the optimization domain and also a classification algorithm on the constraint domain to reach the target clustering effectively. The time and space complexities of the ICC algorithm are formally analyzed. Several experiments are conducted to provide the insights into the dual clustering problem and the proposed algorithm.

#index 800180
#* Effectively Mining and Using Coverage and Overlap Statistics for Data Integration
#@ Zaiqing Nie;Subbarao Kambhampati;Ullas Nambiar
#t 2005
#c 7
#% 210176
#% 262063
#% 316709
#% 340306
#% 342684
#% 342869
#% 413643
#% 479813
#% 480149
#% 481290
#% 481748
#% 481923
#% 482108
#% 496091
#% 571037
#% 631997
#% 659968
#% 665561
#% 745490
#% 1015359
#! Recent work in data integration has shown the importance of statistical information about the coverage and overlap of sources for efficient query processing. Despite this recognition, there are no effective approaches for learning the needed statistics. The key challenge in learning such statistics is keeping the number of needed statistics low enough to have the storage and learning costs manageable. In this paper, we present a set of connected techniques that estimate the coverage and overlap statistics,while keeping the needed statistics tightly under control. Our approach uses a hierarchical classification of the queries and threshold-based variants of familiar data mining techniques to dynamically decide the level of resolution at which to learn the statistics. We describe the details of our method,and present experimental results demonstrating the efficiency of the learning algorithms and the effectiveness of the learned statistics over both controlled data sources and in the context of BibFinder with autonomous online sources.

#index 800181
#* TFP: An Efficient Algorithm for Mining Top-K Frequent Closed Itemsets
#@ Jianyong Wang;Jiawei Han;Ying Lu;Petre Tzvetkov
#t 2005
#c 7
#% 227919
#% 248785
#% 248791
#% 273898
#% 280477
#% 299985
#% 300120
#% 338594
#% 342667
#% 431033
#% 463903
#% 465003
#% 481290
#% 481588
#% 632029
#% 662759
#% 727869
#% 729933
#% 729979
#% 729984
#! Frequent itemset mining has been studied extensively in literature. Most previous studies require the specification of a min_support threshold and aim at mining a complete set of frequent itemsets satisfying min_support. However, in practice, it is difficult for users to provide an appropriate min_support threshold. In addition, a complete set of frequent itemsets is much less compact than a set of frequent closed itemsets. In this paper, we propose an alternative mining task: mining top-k frequent closed itemsets of length no less than min_l, where k is the desired number of frequent closed itemsets to be mined, and min_l is the minimal length of each itemset. An efficient algorithm, called TFP, is developed for mining such itemsets without mins_support. Starting at min_support = 0 and by making use of the length constraint and the properties of top-k frequent closed itemsets, min_support can be raised effectively and FP-Tree can be pruned dynamically both during and after the construction of the tree using our two proposed methods: the closed node count and descendant_sum. Moreover, mining is further speeded up by employing a top-down and bottom-up combined FP-Tree traversing strategy, a set of search space pruning methods, a fast 2-level hash-indexed result tree, and a novel closed itemset verification scheme. Our extensive performance study shows that TFP has high performance and linear scalability in terms of the database size.

#index 800182
#* Video Data Mining: Semantic Indexing and Event Detection from the Association Perspective
#@ Xingquan Zhu;Xindong Wu;Ahmed K. Elmagarmid;Zhe Feng;Lide Wu
#t 2005
#c 7
#% 156430
#% 227939
#% 248865
#% 316709
#% 318851
#% 341285
#% 351784
#% 410276
#% 413596
#% 420063
#% 427199
#% 443245
#% 443261
#% 459006
#% 463903
#% 481290
#% 481758
#% 489580
#% 582043
#% 631926
#% 654481
#% 659926
#% 727902
#% 729970
#% 730122
#% 782526
#% 899439
#% 968505
#% 1279436
#% 1775378
#! Advances in the media and entertainment industries, including streaming audio and digital TV, present new challenges for managing and accessing large audio-visual collections. Current content management systems support retrieval using low-level features, such as motion, color, and texture. However, low-level features often have little meaning for naive users, who much prefer to identify content using high-level semantics or concepts. This creates a gap between systems and their users that must be bridged for these systems to be used effectively. To this end, in this paper, we first present a knowledge-based video indexing and content management framework for domain specific videos (using basketball video as an example). We will provide a solution to explore video knowledge by mining associations from video data. The explicit definitions and evaluation measures (e.g., temporal support and confidence) for video associations are proposed by integrating the distinct feature of video data. Our approach uses video processing techniques to find visual and audio cues (e.g., court field, camera motion activities, and applause), introduces multilevel sequential association mining to explore associations among the audio and visual cues, classifies the associations by assigning each of them with a class label, and uses their appearances in the video to construct video indices. Our experimental results demonstrate the performance of the proposed approach.

#index 800183
#* A Web Surfer Model Incorporating Topic Continuity
#@ Sankar K. Pal;B. L. Narayan;Soumitra Dutta
#t 2005
#c 7
#% 290830
#% 309868
#% 348148
#% 451536
#% 458369
#% 458379
#% 577328
#% 641979
#% 724581
#% 729618
#! This paper describes a surfer model which incorporates information about topic continuity derived from the surfer's history. Therefore, unlike earlier models, it captures the interrelationship between categorization (context) and ranking of Web documents simultaneously. The model is mathematically formulated. A scalable and convergent iterative procedure is provided for its implementation. Its different characteristic features, as obtained from the joint probability matrix, and their significance in Web intelligence are mentioned. Experiments performed on Web pages obtained from WebBase confirm the superiority of the model.

#index 800184
#* epsilon-SSVR: A Smooth Support Vector Machine for epsilon-Insensitive Regression
#@ Yuh-Jye Lee;Wen-Feng Hsieh;Chien-Ming Huang
#t 2005
#c 7
#% 190581
#% 201020
#% 206117
#% 251950
#% 277940
#% 290482
#% 309208
#% 315272
#% 342598
#% 416840
#% 420077
#% 425041
#% 836002
#% 1861262
#% 1861308
#! A new smoothing strategy for solving \epsilon{\hbox{-}}{\rm{support}} vector regression (\epsilon{\hbox{-}}{\rm{SVR}}), tolerating a small error in fitting a given data set linearly or nonlinearly, is proposed in this paper. Conventionally, \epsilon{\hbox{-}}{\rm{SVR}} is formulated as a constrained minimization problem, namely, a convex quadratic programming problem. We apply the smoothing techniques that have been used for solving the support vector machine for classification, to replace the \epsilon{\hbox{-}}{\rm{insensitive}} loss function by an accurate smooth approximation. This will allow us to solve \epsilon{\hbox{-}}{\rm{SVR}} as an unconstrained minimization problem directly. We term this reformulated problem as \epsilon{\hbox{-}}{\rm{smooth}} support vector regression (\epsilon{\hbox{-}}{\rm{SSVR}}). We also prescribe a Newton-Armijo algorithm that has been shown to be convergent globally and quadratically to solve our \epsilon{\hbox{-}}{\rm{SSVR}}. In order to handle the case of nonlinear regression with a massive data set, we also introduce the reduced kernel technique in this paper to avoid the computational difficulties in dealing with a huge and fully dense kernel matrix. Numerical results and comparisons are given to demonstrate the effectiveness and speed of the algorithm.

#index 800185
#* Toward an Agent-Based and Context-Oriented Approach for Web Services Composition
#@ Zakaria Maamar;Soraya Kouadri Mostefaoui;Hamdi Yahyaoui
#t 2005
#c 7
#% 15528
#% 204453
#% 211885
#% 431490
#% 438581
#% 452557
#% 452559
#% 577344
#% 578311
#% 622484
#% 636365
#% 643182
#% 659964
#% 664071
#% 722477
#% 722478
#% 737913
#% 763516
#% 797819
#% 1134895
#% 1180877
#! This paper presents an agent-based and context-oriented approach that supports the composition of Web services. A Web service is an accessible application that other applications and humans can discover and invoke to satisfy multiple needs. To reduce the complexity featuring the composition of Web services, two concepts are put forward, namely, software agent and context. A software agent is an autonomous entity that acts on behalf of users and the context is any relevant information that characterizes a situation. During the composition process, software agents engage in conversations with their peers to agree on the Web services that participate in this process. Conversations between agents take into account the execution context of the Web services. The security of the computing resources on which the Web services are executed constitutes another core component of the agent-based and context-oriented approach presented in this paper.

#index 800186
#* Techniques for Efficient Road-Network-Based Tracking of Moving Objects
#@ Alminas Civilis;Christian S. Jensen;Stardas Pakalnis
#t 2005
#c 7
#% 248906
#% 295512
#% 458849
#% 464847
#% 471194
#% 511668
#% 564114
#% 720033
#% 729866
#% 765162
#% 784284
#% 788231
#% 1113048
#! With the continued advances in wireless communications, geo-positioning, and consumer electronics, an infrastructure is emerging that enables location-based services that rely on the tracking of the continuously changing positions of entire populations of service users, termed moving objects. This scenario is characterized by large volumes of updates, for which reason location update technologies become important. A setting is assumed in which a central database stores a representation of each moving object's current position. This position is to be maintained so that it deviates from the user's real position by at most a given threshold. To do so, each moving object stores locally the central representation of its position. Then, an object updates the database whenever the deviation between its actual position (as obtained from a GPS device) and the database position exceeds the threshold. The main issue considered is how to represent the location of a moving object in a database so that tracking can be done with as few updates as possible. The paper proposes to use the road network within which the objects are assumed to move for predicting their future positions. The paper presents algorithms that modify an initial road-network representation, so that it works better as a basis for predicting an object's position; it proposes to use known movement patterns of the object, in the form of routes; and, it proposes to use acceleration profiles together with the routes. Using real GPS-data and a corresponding real road network, the paper offers empirical evaluations and comparisons that include three existing approaches and all the proposed approaches.

#index 800187
#* Ontological Evaluation of Enterprise Systems Interoperability Using ebXML
#@ Peter F. Green;Michael Rosemann;Marta Indulska
#t 2005
#c 7
#% 50500
#% 82397
#% 132577
#% 135028
#% 208275
#% 238921
#% 297188
#% 308139
#% 320504
#% 342980
#% 342981
#% 342982
#% 342984
#% 346054
#% 353637
#% 404240
#% 427869
#% 437054
#% 536357
#% 573898
#% 722487
#! Enterprise systems interoperability (ESI) is an important topic for business currently. This situation is evidenced, at least in part, by the number and extent of potential candidate protocols for such process interoperation, viz., ebXML, BPML, BPEL, and WSCI. Wide-ranging support for each of these candidate standards already exists. However, despite broad acceptance, a sound theoretical evaluation of these approaches has not yet been provided. We use the Bunge-Wand-Weber (BWW) models, in particular, the representation model, to provide the basis for such a theoretical evaluation. We, and other researchers, have shown the usefulness of the representation model for analyzing, evaluating, and engineering techniques in the areas of traditional and structured systems analysis, object-oriented modeling, and process modeling. In this work, we address the question, what are the potential semantic weaknesses of using ebXML alone for process interoperation between enterprise systems? We find that users will lack important implementation information because of representational deficiencies; due to ontological redundancy, the complexity of the specification is unnecessarily increased; and, users of the specification will have to bring in extra-model knowledge to understand constructs in the specification due to instances of ontological excess.

#index 813965
#* Editorial: TKDE Topic Area Revisions
#@ Xindong Wu;Christos Faloutsos
#t 2005
#c 7

#index 813966
#* Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions
#@ Gediminas Adomavicius;Alexander Tuzhilin
#t 2005
#c 7
#% 28673
#% 55490
#% 124004
#% 124010
#% 165663
#% 170649
#% 173879
#% 202009
#% 202011
#% 220707
#% 220709
#% 220711
#% 223781
#% 234992
#% 236729
#% 271082
#% 280447
#% 280852
#% 283169
#% 304425
#% 310488
#% 319705
#% 330687
#% 340941
#% 342767
#% 345044
#% 387427
#% 397133
#% 397155
#% 397941
#% 420118
#% 420121
#% 420134
#% 420515
#% 424021
#% 428272
#% 433966
#% 433968
#% 452563
#% 465906
#% 465928
#% 522882
#% 528182
#% 549443
#% 564279
#% 577245
#% 578684
#% 643007
#% 644498
#% 729437
#% 729626
#% 730049
#% 734590
#% 734591
#% 734592
#% 734593
#% 734594
#% 739574
#% 739578
#% 801785
#% 857276
#% 1272282
#% 1272396
#% 1650298
#% 1650399
#% 1650569
#% 1673017
#! This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multcriteria ratings, and a provision of more flexible and less intrusive types of recommendations.

#index 813967
#* Maximum Weighted Likelihood via Rival Penalized EM for Density Mixture Clustering with Automatic Model Selection
#@ Yiu-ming Cheung
#t 2005
#c 7
#% 72560
#% 77433
#% 179862
#% 232102
#% 256648
#% 430881
#% 723039
#! Expectation-Maximization (EM) algorithm [10] has been extensively used in density mixture clustering problems, but it is unable to perform model selection automatically. This paper, therefore, proposes to learn the model parameters via maximizing a weighted likelihood. Under a specific weight design, we give out a Rival Penalized Expectation-Maximization (RPEM) algorithm, which makes the components in a density mixture compete each other at each time step. Not only are the associated parameters of the winner updated to adapt to an input, but also all rivals' parameters are penalized with the strength proportional to the corresponding posterior density probabilities. Compared to the EM algorithm [10], the RPEM is able to fade out the redundant densities from a density mixture during the learning process. Hence, it can automatically select an appropriate number of densities in density mixture clustering. We experimentally demonstrate its outstanding performance on Gaussian mixtures and color image segmentation problem. Moreover, a simplified version of RPEM generalizes our recently proposed RPCCL algorithm [8] so that it is applicable to elliptical clusters as well with any input proportion. Compared to the existing heuristic RPCL [25] and its variants, this generalized RPCCL (G-RPCCL) circumvents the difficult preselection of the so-called delearning rate. Additionally, a special setting of the G-RPCCL not only degenerates to RPCL and its Type A variant, but also gives a guidance to choose an appropriate delearning rate for them. Subsequently, we propose a stochastic version of RPCL and its Type A variant, respectively, in which the difficult selection problem of delearning rate has been novelly circumvented. The experiments show the promising results of this stochastic implementation.

#index 813968
#* Using Evolutionary Algorithms for Defining the Sampling Policy of Complex N-Partite Networks
#@ Michel L. Goldstein;Gary G. Yen
#t 2005
#c 7
#% 240188
#% 330769
#% 366182
#% 369236
#% 378391
#% 443143
#% 446083
#% 462062
#% 481434
#% 481779
#% 579504
#! N-partite networks are natural representations of complex multi-entity databases. However, processing these networks can be a highly memory and computation-intensive task, especially when positive correlation exists between the degrees of vertices from different partitions. In order to improve the scalability of this process, this paper proposes two algorithms that make use of sampling for obtaining less expensive approximate results. The first algorithm is optimal for obtaining homogeneous discovery rates with a low memory requirement, but can be very slow in cases where the combined branching factor of these networks is too large. A second algorithm that incorporates concepts from evolutionary computation aims toward dealing with this slow convergence in the case when it is more interesting to increase approximation convergence speed of elements with high feature values. This algorithm makes use of the positive correlation between "local驴 branching factors and the feature values. Two applications examples are demonstrated in searching for most influential authors in collections of journal articles and in analyzing most active earthquake regions from a collection of earthquake events.

#index 813969
#* SMCA: A General Model for Mining Asynchronous Periodic Patterns in Temporal Databases
#@ Kuo-Yu Huang;Chia-Hui Chang
#t 2005
#c 7
#% 300120
#% 329537
#% 338586
#% 420063
#% 452845
#% 459006
#% 463903
#% 464839
#% 464986
#% 478626
#% 481290
#% 578396
#% 631926
#! Mining periodic patterns in time series databases is an important data mining problem with many applications. Previous studies have considered synchronous periodic patterns where misaligned occurrences are not allowed. However, asynchronous periodic pattern mining has received less attention and only been discussed for a sequence of symbols where each time point contains one event. In this paper, we propose a more general model of asynchronous periodic patterns from a sequence of symbol sets where a time slot can contain multiple events. Three parameters min\_rep, max\_dis, and global\_rep are employed to specify the minimum number of repetitions required for a valid segment of nondisrupted pattern occurrences, the maximum allowed disturbance between two successive valid segments, and the total repetitions required for a valid sequence. A 4-phase algorithm is devised to discover periodic patterns from a time series database presented in vertical format. The experiments demonstrate good performance and scalability with large frequent patterns.

#index 813970
#* KBA: Kernel Boundary Alignment Considering Imbalanced Data Distribution
#@ Gang Wu;Edward Y. Chang
#t 2005
#c 7
#% 80995
#% 190581
#% 209021
#% 269213
#% 294964
#% 304830
#% 341269
#% 393059
#% 420064
#% 425031
#% 458379
#% 465738
#% 466760
#% 730216
#% 765520
#% 769908
#% 1271973
#% 1272000
#% 1272365
#% 1378224
#! An imbalanced training data set can pose serious problems for many real-world data mining tasks that employ SVMs to conduct supervised learning. In this paper, we propose a kernel-boundary-alignment algorithm, which considers THE training data imbalance as prior information to augment SVMs to improve class-prediction accuracy. Using a simple example, we first show that SVMs can suffer from high incidences of false negatives when the training instances of the target class are heavily outnumbered by the training instances of a nontarget class. The remedy we propose is to adjust the class boundary by modifying the kernel matrix, according to the imbalanced data distribution. Through theoretical analysis backed by empirical study, we show that our kernel-boundary-alignment algorithm works effectively on several data sets.

#index 813971
#* Locking Protocols for Materialized Aggregate Join Views
#@ Gang Luo;Jeffrey F. Naughton;Curt J. Ellmann;Michael W. Watzke
#t 2005
#c 7
#% 4618
#% 9241
#% 114583
#% 172217
#% 279164
#% 289224
#% 289399
#% 328431
#% 333926
#% 403195
#% 416025
#% 464215
#% 464705
#% 480141
#% 1015304
#! The maintenance of materialized aggregate join views is a well-studied problem. However, to date the published literature has largely ignored the issue of concurrency control. Clearly, immediate materialized view maintenance with transactional consistency, if enforced by generic concurrency control mechanisms, can result in low levels of concurrency and high rates of deadlock. While this problem is superficially amenable to well-known techniques, such as fine-granularity locking and special lock modes for updates that are associative and commutative, we show that these previous high concurrency locking techniques do not fully solve the problem, but a combination of a "value-based驴 latch pool and these previous high concurrency locking techniques can solve the problem.

#index 813972
#* A Generic Scheme for Color Image Retrieval Based on the Multivariate Wald-Wolfowitz Test
#@ Christos Theoharatos;Nikolaos A. Laskaris;George Economou;Spiros Fotopoulos
#t 2005
#c 7
#% 120270
#% 190611
#% 192795
#% 213673
#% 219845
#% 219847
#% 318785
#% 342617
#% 391420
#% 393247
#% 397685
#% 428926
#% 437405
#% 443529
#% 533227
#% 589983
#% 592155
#% 626324
#% 626558
#% 626570
#% 641960
#% 645687
#% 721134
#% 737972
#% 1759699
#% 1857498
#! In this study, a conceptually simple, yet flexible and extendable strategy to contrast two different color images is introduced. The proposed approach is based on the multivariate Wald-Wolfowitz test, a nonparametric test that assesses the commonality between two different sets of multivariate observations. It provides an aggregate gauge of the match between color images, taking into consideration all the (selected) low-level characteristics, while alleviating correspondence issues. We show that a powerful measure of similarity between two color images can emerge from the statistical comparison of their representations in a properly formed feature space. For the sake of simplicity, the RGB-space is selected as the feature space, while we are experimenting with different ways to represent the images within this space. By altering the feature-extraction implementation, complementary ways to portray the image content appear. The reported results, from the application on a diverse collection of images, clearly demonstrate the effectiveness of our method, its superiority over previous methods, and suggest that even further improvements can be achieved along the same line of research. It is not only the unifying character that makes our strategy appealing, but also the fact that the retrieval performance does not increase continuously with the amount of details in the image representation. The latter sets an upper limit to the computational demands and reminds of performance plateaus reached by novel approaches in information retrieval.

#index 813973
#* Aggregate Nearest Neighbor Queries in Road Networks
#@ Man Lung Yiu;Nikos Mamoulis;Dimitris Papadias
#t 2005
#c 7
#% 248797
#% 287466
#% 316943
#% 333854
#% 413797
#% 421124
#% 443105
#% 443208
#% 443533
#% 462057
#% 463583
#% 480819
#% 729850
#% 729851
#% 745464
#% 765438
#% 1015317
#% 1015321
#! Aggregate nearest neighbor queries return the object that minimizes an aggregate distance function with respect to a set of query points. Consider, for example, several users at specific locations (query points) that want to find the restaurant (data point), which leads to the minimum sum of distances that they have to travel in order to meet. We study the processing of such queries for the case where the position and accessibility of spatial objects are constrained by spatial (e.g., road) networks. We consider alternative aggregate functions and techniques that utilize Euclidean distance bounds, spatial access methods, and/or network distance materialization structures. Our algorithms are experimentally evaluated with synthetic and real data. The results show that their relative performance depends on the problem characteristics.

#index 813974
#* A Statistical Model for User Preference
#@ Sung Young Jung;Jeong-Hee Hong;Taek-Soo Kim
#t 2005
#c 7
#% 115462
#% 151302
#% 152934
#% 284794
#% 420073
#% 420507
#% 438371
#% 458379
#% 465895
#% 465906
#% 465928
#% 481758
#% 757312
#% 1650569
#! Modeling user preference is one of the challenging issues in intelligent information systems. Extensive research has been performed to automatically analyze user preference and to utilize it. One problem still remains: The representation of preference, usually given by measure of vector similarity or probability, does not always correspond to common sense of preference. This problem gets worse in the case of negative preference. To overcome this problem, this paper presents a preference model using mutual information in a statistical framework. This paper also presents a method that combines information of joint features and alleviates problems arising from sparse data. Experimental results, compared with the previous recommendation models, show that the proposed model has the highest accuracy in recommendation tests.

#index 813975
#* Applying Semantic Knowledge to Real-Time Update of Access Control Policies
#@ Indrakshi Ray
#t 2005
#c 7
#% 9241
#% 32897
#% 42882
#% 43206
#% 67458
#% 77982
#% 99823
#% 101645
#% 114583
#% 116077
#% 116544
#% 122911
#% 139179
#% 167282
#% 204453
#% 225005
#% 243330
#% 247071
#% 252131
#% 263982
#% 287220
#% 287231
#% 307766
#% 312779
#% 323696
#% 345972
#% 480425
#% 488012
#% 491458
#% 507547
#% 635513
#% 660969
#% 664551
#% 664665
#% 767991
#% 1001827
#! Real-time update of access control policies, that is, updating policies while they are in effect and enforcing the changes immediately, is necessary for many security-critical applications. In this paper, we consider real-time update of access control policies in a database system. Updating policies while they are in effect can lead to potential security problems, such as, access to database objects by unauthorized users. In this paper, we propose several algorithms that not only prevent such security breaches but also ensure the correctness of execution. The algorithms differ from each other in the degree of concurrency provided and the semantic knowledge used. Of the algorithms presented, the most concurrency is achieved when transactions are decomposed into atomic steps. Once transactions are decomposed, the atomicity, consistency, and isolation properties no longer hold. Since the traditional transaction processing model can no longer be used to ensure the correctness of the execution, we use an alternate semantic-based transaction processing model. To ensure correct behavior, our model requires an application to satisfy a set of necessary properties, namely, semantic atomicity, consistent execution, sensitive transaction isolation, and policy-compliant. We show how one can verify an application statically to check for the existence of these properties.

#index 813976
#* Automatic Fragment Detection in Dynamic Web Pages and Its Impact on Caching
#@ Lakshmish Ramaswamy;Arun Iyengar;Ling Liu;Fred Douglis
#t 2005
#c 7
#% 255137
#% 348151
#% 348180
#% 397357
#% 482658
#% 536403
#% 577365
#% 616528
#% 659923
#% 660272
#% 960181
#% 963584
#% 993978
#% 1834931
#! Constructing Web pages from fragments has been shown to provide significant benefits for both content generation and caching. In order for a Web site to use fragment-based content generation, however, good methods are needed for fragmenting the Web pages. Manual fragmentation of Web pages is expensive, error prone, and unscalable. This paper proposes a novel scheme to automatically detect and flag fragments that are cost-effective cache units in Web sites serving dynamic content. Our approach analyzes Web pages with respect to their information sharing behavior, personalization characteristics, and change patterns. We identify fragments which are shared among multiple documents or have different lifetime or personalization characteristics. Our approach has three unique features. First, we propose a framework for fragment detection, which includes a hierarchical and fragment-aware model for dynamic Web pages and a compact and effective data structure for fragment detection. Second, we present an efficient algorithm to detect maximal fragments that are shared among multiple documents. Third, we develop a practical algorithm that effectively detects fragments based on their lifetime and personalization characteristics. This paper shows the results when the algorithms are applied to real Web sites. We evaluate the proposed scheme through a series of experiments, showing the benefits and costs of the algorithms. We also study the impact of using the fragments detected by our system on key parameters such as disk space utilization, network bandwidth consumption, and load on the origin servers.

#index 813977
#* Minimum Spanning Tree Partitioning Algorithm for Microaggregation
#@ Michael Laszlo;Sumitra Mukherjee
#t 2005
#c 7
#% 36360
#% 67453
#% 296738
#% 410276
#% 443478
#% 443984
#% 466458
#% 641954
#! This paper presents a clustering algorithm for partitioning a minimum spanning tree with a constraint on minimum group size. The problem is motivated by microaggregation, a disclosure limitation technique in which similar records are aggregated into groups containing a minimum of k records. Heuristic clustering methods are needed since the minimum information loss microaggregation problem is NP-hard. Our MST partitioning algorithm for microaggregation is sufficiently efficient to be practical for large data sets and yields results that are comparable to the best available heuristic methods for microaggregation. For data that contain pronounced clustering effects, our method results in significantly lower information loss. Our algorithm is general enough to accommodate different measures of information loss and can be used for other clustering applications that have a constraint on minimum group size.

#index 813978
#* Periodicity Detection in Time Series Databases
#@ Mohamed G. Elfeky;Walid G. Aref;Ahmed K. Elmagarmid
#t 2005
#c 7
#% 31489
#% 70370
#% 243299
#% 310542
#% 341100
#% 443195
#% 459006
#% 463903
#% 464839
#% 464986
#% 479971
#% 480156
#% 481290
#% 577256
#% 577275
#% 629677
#% 631926
#% 737975
#! Periodicity mining is used for predicting trends in time series data. Discovering the rate at which the time series is periodic has always been an obstacle for fully automated periodicity mining. Existing periodicity mining algorithms assume that the periodicity rate (or simply the period) is user-specified. This assumption is a considerable limitation, especially in time series data where the period is not known a priori. In this paper, we address the problem of detecting the periodicity rate of a time series database. Two types of periodicities are defined, and a scalable, computationally efficient algorithm is proposed for each type. The algorithms perform in O(n\log n) time for a time series of length n. Moreover, the proposed algorithms are extended in order to discover the periodic patterns of unknown periods at the same time without affecting the time complexity. Experimental results show that the proposed algorithms are highly accurate with respect to the discovered periodicity rates and periodic patterns. Real-data experiments demonstrate the practicality of the discovered periodic patterns.

#index 813979
#* ADMiRe: An Algebraic Data Mining Approach to System Performance Analysis
#@ Ning Jiang;Roy Villafane;Kien A. Hua;Abhijit Sawant;Kiran Prabhakara
#t 2005
#c 7
#% 149925
#% 152934
#% 209891
#% 210160
#% 238413
#% 250199
#% 311882
#% 319244
#% 334009
#% 342642
#% 397382
#% 443092
#% 443094
#% 460862
#% 463883
#% 479641
#% 480940
#% 481290
#% 536183
#% 631926
#% 661467
#% 728301
#! Performance analysis of computing systems is an increasingly difficult task due to growing system complexity. Traditional tools rely on ad hoc procedures. With these, determining which of the manifold system and workload parameters to examine is often a lengthy and highly speculative process. The analysis is often incomplete and, therefore, prone to revealing faulty conclusions and not uncovering useful tuning knowledge. We address this problem by introducing a data mining approach called ADMiRe (Analyzer for Data Mining Results). In this scheme, regression analysis is first applied to performance data to discover correlations between various system and workload parameters. The results of this analysis are summarized in sets of regression rules. The user can then formulate intuitive algebraic expressions to manipulate these sets of rules to capture critical information. To demonstrate this approach, we use ADMiRe to analyze an Oracle database system running the TPC-C (Transaction Processing Performance Council) benchmark. The results generated by ADMiRe were confirmed by Oracle experts. We also show that by applying ADMiRe to Microsoft Internet Information Server performance data, we can improve system performance by 20 percent.

#index 813980
#* Rights Protection for Categorical Data
#@ Radu Sion;Mikhail Atallah;Sunil Prabhakar
#t 2005
#c 7
#% 191721
#% 227956
#% 241787
#% 275836
#% 488310
#% 488637
#% 539734
#% 566385
#% 566390
#% 589250
#% 592726
#% 626669
#% 644401
#% 654449
#% 664665
#% 745531
#% 784512
#% 993944
#! A novel method of rights protection for categorical data through watermarking is introduced in this paper. New watermark embedding channels are discovered and associated novel watermark encoding algorithms are proposed. While preserving data quality requirements, the introduced solution is designed to survive important attacks, such as subset selection and random alterations. Mark detection is fully "blind驴 in that it doesn't require the original data, an important characteristic, especially in the case of massive data. Various improvements and alternative encoding methods are proposed and validation experiments on real-life data are performed. Important theoretical bounds including mark vulnerability are analyzed. The method is proved (experimentally and by analysis) to be extremely resilient to both alteration and data loss attacks, for example, tolerating up to 80 percent data loss with a watermark alteration of only 25 percent.

#index 813981
#* Domain-Driven Data Synopses for Dynamic Quantiles
#@ Anna C. Gilbert;Yannis Kotidis;S. Muthukrishnan;Martin J. Strauss
#t 2005
#c 7
#% 1331
#% 2152
#% 152934
#% 210160
#% 210190
#% 214073
#% 242366
#% 248820
#% 273907
#% 310585
#% 333931
#% 340670
#% 347226
#% 379445
#% 397354
#% 397369
#% 480156
#% 480465
#% 480628
#% 480805
#% 481775
#% 482104
#% 482123
#% 492912
#% 594029
#% 993959
#% 993969
#! In this paper, we present new algorithms for dynamically computing quantiles of a relation subject to insert as well as delete operations. At the core of our algorithms lies a small-space multiresolution representation of the underlying data distribution based on random subset sums or RSSs. These RSSs are updated with every insert and delete operation. When quantiles are demanded, we use these RSSs to estimate quickly, without having to access the data, all the quantiles, each guaranteed to be accurate to within user-specified precision. While quantiles have found many uses in databases, in this paper, our focus is primarily on network management applications that monitor the distribution of active sessions in the network. Our examples are drawn both from the telephony and the IP network, where the goal is to monitor the distribution of the length of active calls and IP flows, respectively, over time. For such applications, we propose a new type of histogram that uses RSSs for summarizing the dynamic parts of the distributions while other parts with small volume of sessions are approximated using simple counters.

#index 813982
#* Integration and Efficient Lookup of Compressed XML Accessibility Maps
#@ Mingfei Jiang;Ada Wai-Chee Fu
#t 2005
#c 7
#% 16230
#% 91075
#% 176496
#% 287026
#% 287493
#% 287670
#% 314755
#% 344639
#% 379248
#% 410276
#% 443103
#% 458582
#% 480489
#% 481124
#% 725290
#% 755175
#% 765450
#% 993971
#! XML is emerging as a useful platform-independent data representation language. As more and more XML data is shared across data sources, it becomes important to consider the issue of XML access control. One promising approach to store the accessibility information is based on the CAM (Compressed Accessibility Map). We make two advancements in this direction: 1) Previous work suggests that for each user group and each operation type, a different CAM is built. We observe that the performance and storage requirements can be further improved by combining multiple CAMs into an ICAM (Integrated CAM). We explore this possibility and propose an integration mechanism. 2) If the change in structure of the XML data is not frequent, we suggest an efficient lookup method, which can be applied to CAMs or ICAMs, with a much lower time complexity compared to the previous approach. We show by experiments the effectiveness of our approach.

#index 813983
#* A Multilevel Composability Model for Semantic Web Services
#@ Brahim Medjahed;Athman Bouguettaya
#t 2005
#c 7
#% 220416
#% 235458
#% 278443
#% 445446
#% 452901
#% 480807
#% 576091
#% 576214
#% 577524
#% 643964
#% 659964
#% 728755
#% 728757
#! We propose a composability model to ascertain that Web services can safely be combined, hence avoiding unexpected failures at runtime. Composability is checked through a set of rules organized into four levels: syntactic, static semantic, dynamic semantic, and qualitative levels. We introduce the concepts of composability degree and \tau{\hbox{-}}composability to cater for partial and total composability. We also propose a set of algorithms for checking composability. Finally, we conduct a performance study (analytical and experimental) of the proposed algorithms.

#index 813984
#* Experimentation with Local Consensus Ontologies with Implications for Automated Service Composition
#@ Andrew B. Williams;Anand Padmanabhan;M. Brian Blake
#t 2005
#c 7
#% 198058
#% 241284
#% 247505
#% 279514
#% 314946
#% 346499
#% 379086
#% 433960
#% 445446
#% 466158
#% 524076
#% 529190
#% 643160
#% 851853
#! Agent technologies represent a promising approach for the integration of interorganizational capabilities across distributed, networked environments. However, knowledge sharing interoperability problems can arise when agents incorporating differing ontologies try to synchronize their internal information. Moreover, in practice, agents may not have a common or global consensus ontology that will facilitate knowledge sharing and integration of functional capabilities. We propose a method to enable agents to develop a local consensus ontology during operation time as needed. By identifying similarities in the ontologies of their peer agents, a set of agents can discover new concepts/relations and integrate them into a local consensus ontology on demand. We evaluate this method, both syntactically and semantically, when forming local consensus ontologies with and without the use of a lexical database. We also report on the effects when several factors, such as the similarity measure, the relation search level depth, and the merge order, are varied. Finally, experimenting in the domain of agent-supported Web service composition, we demonstrate how our method allows us to successfully autonomously form service-oriented local consensus ontologies.

#index 813985
#* Query Processing in a Mobile Computing Environment: Exploiting the Features of Asymmetry
#@ Wen-Chih Peng;Ming-Syan Chen
#t 2005
#c 7
#% 121
#% 1758
#% 210177
#% 279165
#% 281420
#% 287068
#% 443035
#% 443042
#% 443065
#% 443263
#% 444223
#% 452786
#% 461923
#% 464847
#% 479786
#% 480965
#% 511674
#% 589300
#% 654482
#% 1830352
#! With the cutting edge technology advance in wireless and mobile computers, the query processing in a mobile environment involves join processing among different sites which include static servers and mobile computers. Because of the need for energy saving and also the presence of asymmetric features in a mobile computing environment, the conventional query processing for a distributed database cannot be directly applied to a mobile computing system. In this paper, we first explore three asymmetric features of a mobile environment. Then, in light of these features, we devise query processing methods for both join and query processing.Intuitively, employing semijoin operations in a mobile computing environment is able to further reduce both the amount of data transmission and energy consumption. A semijoin which is initiated by a mobile computer (respectively, the server) and is beneficial to reduce the cost of a join operation is termed a mobile-initiated or MI (respectively, server-initiated or SI) profitable semijoin. According to those asymmetric features of a mobile computing system, we examine three different join methods and devise some specific criteria to identify MI/SI profitable semijoins. For query processing, which refers to the processing of multijoin queries, we develop three query processing schemes. In particular, we formulate the query processing in a mobile computing system as a two-phase query processing procedure that can determine a join sequence and interleave that join sequence with SI profitable semijoins to reduce both the amount of data transmission and energy consumption. Performance of these join and query methods is comparatively analyzed and sensitivity analysis on several parameters is conducted. Furthermore, we develop a systematic procedure to derive the characteristic functions of MI and SI profitable semijoins. It is noted that, given some system parameters, those characteristic functions are very important in determining which join method is the most appropriate one to employ in that configuration. It is shown by our simulation results that, by exploiting the three asymmetric features, these characteristic functions are very powerful in reducing both the amounts of energy consumption and data transmission incurred and can lead to the design of an efficient and effective query processing procedure for a mobile computing environment.

#index 813986
#* Indexing Useful Structural Patterns for XML Query Processing
#@ Wang Lian;Nikos Mamoulis;David Wai-lok Cheung;S. M. Yiu
#t 2005
#c 7
#% 236416
#% 333981
#% 397359
#% 397360
#% 397375
#% 397379
#% 427199
#% 443349
#% 464204
#% 465018
#% 466644
#% 479956
#% 480488
#% 481290
#% 577218
#% 654452
#% 659999
#% 660000
#% 993968
#% 1015260
#% 1015277
#! Queries on semistructured data are hard to process due to the complex nature of the data and call for specialized techniques. Existing path-based indexes and query processing algorithms are not efficient for searching complex structures beyond simple paths, even when the queries are high-selective. We introduce the definition of minimal infrequent structures (MIS), which are structures that 1) exist in the data, 2) are not frequent with respect to a support threshold, and 3) all substructures of them are frequent. By indexing the occurrences of MIS, we can efficiently locate the high-selective substructures of a query, improving search performance significantly. An efficient data mining algorithm is proposed, which finds the minimal infrequent structures. Their occurrences in the XML data are then indexed by a lightweight data structure and used as a fast filter step in query evaluation. We validate the efficiency and applicability of our methods through experimentation on both synthetic and real data.

#index 813987
#* A Tree-Based Forward Digest Protocol to Verify Data Integrity in Distributed Media Streaming
#@ Ahsan Habib;Dongyan Xu;Mikhail Atallah;Bharat Bhargava;John Chuang
#t 2005
#c 7
#% 284284
#% 286984
#% 319994
#% 342334
#% 486712
#% 513367
#% 578421
#% 616941
#% 664704
#% 717101
#% 730123
#! We design a Tree-based Forward Digest Protocol (TFDP) to verify data integrity in distributed media streaming for content distribution. Several challenges arise, including the timing constraint of streaming sessions, the involvement of multiple senders, and the untrustworthiness of these senders. A comprehensive comparison is presented on the performance of existing protocols and TFDP, with respect to communication and computation overhead. Both simulation and Internet-based experimental results are presented to demonstrate the effectiveness of TFDP.

#index 813988
#* On Embedding Machine-Processable Semantics into Documents
#@ Krishnaprasad Thirunarayan
#t 2005
#c 7
#% 114858
#% 237328
#% 342443
#% 644854
#! Most Web and legacy paper-based documents are available in human comprehensible text form, not readily accessible to or understood by computer programs. Here, we investigate an approach to amalgamate XML technology with programming languages for representational purposes that can enhance traceability, thereby facilitating semiautomatic extraction and update. Specifically, we propose a modular technique to embed machine-processable semantics into a text document with tabular data via annotations, resulting sometimes in ill-formed XML fragments, and evaluate this technique vis a vis document querying, manipulation, and integration. The ultimate aim is to be able to author and extract human-readable and machine-comprehensible parts of a document hand in hand and keep them side by side.

#index 813989
#* Efficiently Mining Frequent Trees in a Forest: Algorithms and Applications
#@ Mohammed J. Zaki
#t 2005
#c 7
#% 184048
#% 187659
#% 232136
#% 237192
#% 262071
#% 282470
#% 300033
#% 325384
#% 333981
#% 342604
#% 431105
#% 462235
#% 463903
#% 465018
#% 480489
#% 577218
#% 629656
#% 629708
#% 727828
#% 727845
#% 727909
#% 729938
#% 729941
#% 737334
#% 745511
#% 765125
#% 769951
#% 772830
#% 832632
#! Mining frequent trees is very useful in domains like bioinformatics, Web mining, mining semistructured data, etc. We formulate the problem of mining (embedded) subtrees in a forest of rooted, labeled, and ordered trees. We present TreeMiner, a novel algorithm to discover all frequent subtrees in a forest, using a new data structure called scope-list. We contrast TreeMiner with a pattern matching tree mining algorithm (PatternMatcher), and we also compare it with TreeMinerD, which counts only distinct occurrences of a pattern. We conduct detailed experiments to test the performance and scalability of these methods. We also use tree mining to analyze RNA structure and phylogenetics data sets from bioinformatics domain.

#index 813990
#* Frequent Substructure-Based Approaches for Classifying Chemical Compounds
#@ Mukund Deshpande;Michihiro Kuramochi;Nikil Wale;George Karypis
#t 2005
#c 7
#% 28382
#% 39267
#% 136350
#% 137307
#% 300124
#% 331909
#% 342604
#% 376266
#% 391061
#% 402920
#% 413590
#% 420088
#% 425890
#% 443350
#% 445369
#% 466229
#% 466473
#% 466483
#% 466644
#% 629603
#% 629708
#% 631986
#% 727845
#% 727896
#% 769951
#% 772830
#% 1273674
#! Computational techniques that build models to correctly assign chemical compounds to various classes of interest have many applications in pharmaceutical research and are used extensively at various phases during the drug development process. These techniques are used to solve a number of classification problems such as predicting whether or not a chemical compound has the desired biological activity, is toxic or nontoxic, and filtering out drug-like compounds from large compound libraries. This paper presents a substructure-based classification algorithm that decouples the substructure discovery process from the classification model construction and uses frequent subgraph discovery algorithms to find all topological and geometric substructures present in the data set. The advantage of this approach is that during classification model construction, all relevant substructures are available allowing the classifier to intelligently select the most discriminating ones. The computational scalability is ensured by the use of highly efficient frequent subgraph discovery algorithms coupled with aggressive feature selection. Experimental evaluation on eight different classification problems shows that our approach is computationally scalable and, on average, outperforms existing schemes by 7 percent to 35 percent.

#index 813991
#* A Probabilistic Model for Mining Labeled Ordered Trees: Capturing Patterns in Carbohydrate Sugar Chains
#@ Nobuhisa Ueda;Kiyoko F. Aoki-Kinoshita;Atsuko Yamaguchi;Tatsuya Akutsu;Hiroshi Mamitsuka
#t 2005
#c 7
#% 44876
#% 137711
#% 246837
#% 291299
#% 292235
#% 304917
#% 349550
#% 464640
#% 484421
#% 546534
#% 571903
#% 629617
#% 629656
#% 643519
#% 729941
#% 769891
#% 770868
#% 778469
#% 833022
#% 1759322
#! Glycans, or carbohydrate sugar chains, which play a number of important roles in the development and functioning of multicellular organisms, can be regarded as labeled ordered trees. A recent increase in the documentation of glycan structures, especially in the form of database curation, has made mining glycans important for the understanding of living cells. We propose a probabilistic model for mining labeled ordered trees, and we further present an efficient learning algorithm for this model, based on an EM algorithm. The time and space complexities of this algorithm are rather favorable, falling within the practical limits set by a variety of existing probabilistic models, including stochastic context-free grammars. Experimental results have shown that, in a supervised problem setting, the proposed method outperformed five other competing methods by a statistically significant factor in all cases. We further applied the proposed method to aligning multiple glycan trees, and we detected biologically significant common subtrees in these alignments where the trees are automatically classified into subtypes already known in glycobiology. Extended abstracts of parts of the work presented in this paper have appeared in [35], [4], and [3].

#index 813992
#* Finding Patterns on Protein Surfaces: Algorithms and Applications to Protein Classification
#@ Xiong Wang
#t 2005
#c 7
#% 31702
#% 119522
#% 122307
#% 131745
#% 140400
#% 252304
#% 252910
#% 300194
#% 310556
#% 310564
#% 316709
#% 342405
#% 388776
#% 436113
#% 443349
#% 443514
#% 447440
#% 466505
#% 527019
#% 577218
#% 589429
#% 629603
#% 629630
#% 629637
#% 629694
#% 764392
#! A successful application of data mining to bioinformatics is protein classification. A number of techniques have been developed to classify proteins according to important features in their sequences, secondary structures, or three-dimensional structures. In this paper, we introduce a novel approach to protein classification based on significant patterns discovered on the surface of a protein. We define a notion called \alpha{\hbox{-}}{\rm{surface}}. We discuss the geometric properties of \alpha{\hbox{-}}{\rm{surface}} and present an algorithm that calculates the \alpha{\hbox{-}}{\rm{surface}} from a finite set of points in R^{3}. We apply the algorithm to extracting the \alpha{\hbox{-}}{\rm{surface}} of a protein and use a pattern discovery algorithm to discover frequently occurring patterns on the surfaces. The pattern discovery algorithm utilizes a new index structure called the \Delta{\rm{B}}^{+} tree. We use these patterns to classify the proteins. While most existing techniques focus on the binary classification problem, we apply our approach to classifying three families of proteins. Experimental results show the good performance of the proposed approach.

#index 813993
#* Using Fixed Point Theorems to Model the Binding in Protein-Protein Interactions
#@ Jinyan Li;Haiquan Li
#t 2005
#c 7
#% 465003
#% 481290
#% 833594
#! The binding in protein-protein interactions exhibits a kind of biochemical stability in cells. The mathematical notion of fixed points also describes stability. A point is a fixed point if it remains unchanged after a transformation by a function. Many points may not be a fixed point, but they may approach a stable status after multiple steps of transformation. In this paper, we define a point as a protein motif pair consisting of two traditional protein motifs. We propose a function and propose a method to discover stable motif pairs of this function from a large protein interaction sequence data set. There are many interesting properties for this function (for example, the convergence). Some of them are useful for gaining much efficiency in the discovery of those stable motif pairs; some are useful for explaining why our proposed fixed point theorems are a good way to model the binding of protein interactions. Our results are also compared to biological results to elaborate the effectiveness of our method.

#index 813994
#* Literature Extraction of Protein Functions Using Sentence Pattern Mining
#@ Jung-Hsien Chiang;Hsu-Chun Yu
#t 2005
#c 7
#% 360402
#% 575975
#% 744085
#% 830383
#% 830813
#% 830902
#! With the rapid growth of articles of genomics research, it has become a challenge for biomedical researchers to access this ever-increasing quantity of information to understand the newest discovery of functions of proteins they are studying. To facilitate functional annotation of proteins by utilizing the huge amounts of biomedical literature and transforming the knowledge into easily accessible database formats, the text mining technique thus becomes essential. In this paper, we propose the method of sentence pattern mining to extract protein functions from biomedical literature. To recognize variants of function terms correctly, we identify morphological, syntactic, and semantic variation forms. The proposed methods can be used to aid database curators in annotating protein functions and to assist biologists and medical researchers in searching protein functions from biomedical literature.

#index 813995
#* Information Retrieval and Knowledge Discovery Utilizing a BioMedical Patent Semantic Web
#@ Sougata Mukherjea;Bhuvan Bamba;Pankaj Kankar
#t 2005
#c 7
#% 240201
#% 248869
#% 268079
#% 281396
#% 282905
#% 289315
#% 309779
#% 332081
#% 348181
#% 413641
#% 577372
#% 730063
#! Before undertaking new biomedical research, identifying concepts that have already been patented is essential. A traditional keyword-based search on patent databases may not be sufficient to retrieve all the relevant information, especially for the biomedical domain. This paper presents BioPatentMiner, a system that facilitates information retrieval and knowledge discovery from biomedical patents. The system first identifies biological terms and relations from the patents and then integrates the information from the patents with knowledge from biomedical ontologies to create a Semantic Web. Besides keyword search and queries linking the properties specified by one or more RDF triples, the system can discover semantic associations between the Web resources. The system also determines the importance of the resources to rank the results of a search and prevent information overload while determining the semantic associations.

#index 813996
#* Knowledge Accumulation and Resolution of Data Inconsistencies during the Integration of Microbial Information Sources
#@ Peter Dawyndt;Marc Vancanneyt;Hans De Meyer;Jean Swings
#t 2005
#c 7
#% 22948
#% 42487
#% 55366
#% 70370
#% 131061
#% 158200
#% 169766
#% 201889
#% 207732
#% 221147
#% 235914
#% 245365
#% 255137
#% 287222
#% 310533
#% 311203
#% 317975
#% 319739
#% 320454
#% 375388
#% 420072
#% 442890
#% 466481
#% 479783
#% 480130
#% 481614
#% 546093
#% 668675
#! The Internet has emerged as an ever-increasing environment of multiple heterogeneous and autonomous data sources that contain relevant but overlapping information on microorganisms. Microbiologists might therefore seriously benefit from the design of intelligent software agents that assist in the navigation through this information-rich environment, together with the development of data mining tools that can aid in the discovery of new information. These applications heavily depend upon well-conditioned data samples that are correlated with multiple information sources, hence, accurate database merging operations are desirable. Information systems designed for joining the related knowledge provided by different microbial data sources are hampered by the labeling mechanism for referencing microbial strains and cultures that suffers from syntactical variation in the practical usage of the labels, whereas, additionally, synonymy and homonymy are also known to exist amongst the labels. This situation is even complicated by the observation that the label equivalence knowledge is itself fragmentarily recorded over several data sources which can be suspected of providing information that might be both incomplete and incorrect. This paper presents how extraction and integration of label equivalence information from several distributed data sources has led to the construction of a so-called integrated strain database, which helps to resolve most of the above problems. Given the fact that information retrieved from autonomous resources might be overlapping, incomplete, and incorrect, much energy was spent into the completion of missing information, the discovery of new associations between information objects, and the development and application of tools for error detection and correction. Through a thorough evaluation of the different levels of incompleteness and incorrectness encountered within the incorporated data sources, we have finally given proof of the added value of the integrated strain database as a necessary service provider for the seamless integration of microbial information sources.

#index 813997
#* Data Mining for Case-Based Reasoning in High-Dimensional Biological Domains
#@ Niloofar Arshadi;Igor Jurisica
#t 2005
#c 7
#% 90639
#% 96692
#% 182682
#% 234978
#% 243728
#% 316709
#% 359837
#% 376266
#% 387427
#% 418130
#% 425048
#% 448102
#% 464444
#% 761273
#% 772126
#% 830837
#! Case-based reasoning (CBR) is a suitable paradigm for class discovery in molecular biology, where the rules that define the domain knowledge are difficult to obtain and the number and the complexity of the rules affecting the problem are too large for formal knowledge representation. To extend the capabilities of CBR, we propose the mixture of experts for case-based reasoning (MOE4CBR), a method that combines an ensemble of CBR classifiers with spectral clustering and logistic regression. Our approach not only achieves higher prediction accuracy, but also leads to the selection of a subset of features that have meaningful relationships with their class labels. We evaluate MOE4CBR by applying the method to a CBR system called {TA3}驴a computational framework for CBR systems. For two ovarian mass spectrometry data sets, the prediction accuracy improves from 80 percent to 93 percent and from 90 percent to 98.4 percent, respectively. We also apply the method to leukemia and lung microarray data sets with prediction accuracy improving from 65 percent to 74 percent and from 60 percent to 70 percent, respectively. Finally, we compare our list of discovered biomarkers with the lists of selected biomarkers from other studies for the mass spectrometry data sets.

#index 813998
#* Hierarchical Decision Tree Induction in Distributed Genomic Databases
#@ Amir Bar-Or;Daniel Keren;Assaf Schuster;Ran Wolff
#t 2005
#c 7
#% 273900
#% 310500
#% 420096
#% 420115
#% 449588
#% 451128
#% 459008
#% 481945
#% 660537
#% 741335
#% 949205
#% 1499476
#% 1849768
#! Classification based on decision trees is one of the important problems in data mining and has applications in many fields. In recent years, database systems have become highly distributed, and distributed system paradigms, such as federated and peer-to-peer databases, are being adopted. In this paper, we consider the problem of inducing decision trees in a large distributed network of genomic databases. Our work is motivated by the existence of distributed databases in healthcare and in bioinformatics, and by the emergence of systems which automatically analyze these databases, and by the expectancy that these databases will soon contain large amounts of highly dimensional genomic data. Current decision tree algorithms require high communication bandwidth when executed on such data, which are large-scale distributed systems. We present an algorithm that sharply reduces the communication overhead by sending just a fraction of the statistical data. A fraction which is nevertheless sufficient to derive the exact same decision tree learned by a sequential learner on all the data in the network. Extensive experiments using standard synthetic SNP data show that the algorithm utilizes the high dependency among attributes, typical to genomic data, to reduce communication overhead by up to 99 percent. Scalability tests show that the algorithm scales well with both the size of the data set, the dimensionality of the data, and the size of the distributed system.

#index 813999
#* Translation Initiation Sites Prediction with Mixture Gaussian Models in Human cDNA Sequences
#@ Guoliang Li;Tze-Yun Leong;Louxin Zhang
#t 2005
#c 7
#% 136350
#% 251211
#% 290482
#% 361100
#% 420077
#% 481577
#% 565765
#% 742487
#! Translation initiation sites (TISs) are important signals in cDNA sequences. Many research efforts have tried to predict TISs in cDNA sequences. In this paper, we propose to use mixture Gaussian models for TIS prediction. Using both local features and some features generated from global measures, the proposed method predicts TISs with a sensitivity of 98 percent and a specificity of 93.6 percent. Our method outperforms many other existing methods in sensitivity while keeping specificity high. We attribute the improvement in sensitivity to the nature of the global features and the mixture Gaussian models.

#index 821865
#* Guest Editors' Introduction: Special Section on Intelligent Data Preparation
#@ Chengqi Zhang;Qiang Yang;Bing Liu
#t 2005
#c 7
#% 744739
#! First Page of the Article

#index 821866
#* A Discretization Algorithm Based on a Heterogeneity Criterion
#@ Xiaoyan Liu;Huaiqing Wang
#t 2005
#c 7
#% 35065
#% 99397
#% 136350
#% 179769
#% 420146
#% 443148
#% 443510
#% 443880
#% 458283
#% 733623
#% 741971
#% 744739
#% 840577
#! Discretization, as a preprocessing step for data mining, is a process of converting the continuous attributes of a data set into discrete ones so that they can be treated as the nominal features by machine learning algorithms. Those various discretization methods, that use entropy-based criteria, form a large class of algorithm. However, as a measure of class homogeneity, entropy cannot always accurately reflect the degree of class homogeneity of an interval. Therefore, in this paper, we propose a new measure of class heterogeneity of intervals from the viewpoint of class probability itself. Based on the definition of heterogeneity, we present a new criterion to evaluate a discretization scheme and analyze its property theoretically. Also, a heuristic method is proposed to find the approximate optimal discretization scheme. Finally, our method is compared, in terms of predictive error rate and tree size, with Ent-MDLC, a representative entropy-based discretization method well-known for its good performance. Our method is shown to produce better results than those of Ent-MDLC, although the improvement is not significant. It can be a good alternative to entropy-based discretization methods.

#index 821867
#* Toward Unsupervised Correlation Preserving Discretization
#@ Sameep Mehta;Srinivasan Parthasarathy;Hui Yang
#t 2005
#c 7
#% 99397
#% 136350
#% 210160
#% 213977
#% 248027
#% 379343
#% 443466
#% 469931
#% 481290
#% 565244
#% 629652
#% 727672
#% 785389
#! Discretization is a crucial preprocessing technique used for a variety of data warehousing and mining tasks. In this paper, we present a novel PCA-based unsupervised algorithm for the discretization of continuous attributes in multivariate data sets. The algorithm leverages the underlying correlation structure in the data set to obtain the discrete intervals and ensures that the inherent correlations are preserved. Previous efforts on this problem are largely supervised and consider only piecewise correlation among attributes. We consider the correlation among continuous attributes and, at the same time, also take into account the interactions between continuous and categorical attributes. Our approach also extends easily to data sets containing missing values. We demonstrate the efficacy of the approach on real data sets and as a preprocessing step for both classification and frequent itemset mining tasks. We show that the intervals are meaningful and can uncover hidden patterns in data. We also show that large compression factors can be obtained on the discretized data sets. The approach is task independent, i.e., the same discretized data set can be used for different data mining tasks. Thus, the data sets can be discretized, compressed, and stored once and can be used again and again.

#index 821868
#* Feature Subset Selection and Feature Ranking for Multivariate Time Series
#@ Hyunjin Yoon;Kiyoung Yang;Cyrus Shahabi
#t 2005
#c 7
#% 211820
#% 243728
#% 316709
#% 345824
#% 382522
#% 425048
#% 722929
#% 722943
#% 758532
#% 784569
#% 788670
#% 1290045
#% 1390189
#% 1391301
#% 1781033
#! Feature subset selection (FSS) is a known technique to preprocess the data before performing any data mining tasks, e.g., classification and clustering. FSS provides both cost-effective predictors and a better understanding of the underlying process that generated the data. We propose a family of novel unsupervised methods for feature subset selection from Multivariate Time Series (MTS) based on Common Principal Component Analysis, termed {\schmi CL}e{\schmi V}er. Traditional FSS techniques, such as Recursive Feature Elimination (RFE) and Fisher Criterion (FC), have been applied to MTS data sets, e.g., Brain Computer Interface (BCI) data sets. However, these techniques may lose the correlation information among features, while our proposed techniques utilize the properties of the principal component analysis to retain that information. In order to evaluate the effectiveness of our selected subset of features, we employ classification as the target data mining task. Our exhaustive experiments show that {\schmi CL}e{\schmi V}er outperforms RFE, FC, and random selection by up to a factor of two in terms of the classification accuracy, while taking up to 2 orders of magnitude less processing time than RFE and FC.

#index 821869
#* Introducing a Family of Linear Measures for Feature Selection in Text Categorization
#@ Elias F. Combarro;Elena Montanes;Irene Diaz;Jose Ranilla;Ricardo Mones
#t 2005
#c 7
#% 165110
#% 190581
#% 197394
#% 260001
#% 280817
#% 344447
#% 406493
#% 458379
#% 465754
#% 466266
#% 804908
#! Text Categorization, which consists of automatically assigning documents to a set of categories, usually involves the management of a huge number of features. Most of them are irrelevant and others introduce noise which could mislead the classifiers. Thus, feature reduction is often performed in order to increase the efficiency and effectiveness of the classification. In this paper, we propose to select relevant features by means of a family of linear filtering measures which are simpler than the usual measures applied for this purpose. We carry out experiments over two different corpora and find that the proposed measures perform better than the existing ones.

#index 821870
#* A New Dependency and Correlation Analysis for Features
#@ Guangzhi Qu;Salim Hariri;Mazin Yousif
#t 2005
#c 7
#% 136350
#% 169659
#% 243727
#% 243728
#% 465583
#% 466410
#% 466912
#% 633267
#% 686757
#% 727663
#% 793239
#! The quality of the data being analyzed is a critical factor that affects the accuracy of data mining algorithms. There are two important aspects of the data quality, one is relevance and the other is data redundancy. The inclusion of irrelevant and redundant features in the data mining model results in poor predictions and high computational overhead. This paper presents an efficient method concerning both the relevance of the features and the pairwise features correlation in order to improve the prediction and accuracy of our data mining algorithm. We introduce a new feature correlation metric Q_Y (X_i ,X_j ) and feature subset merit measure e(S) to quantify the relevance and the correlation among features with respect to a desired data mining task (e.g., detection of an abnormal behavior in a network service due to network attacks). Our approach takes into consideration not only the dependency among the features, but also their dependency with respect to a given data mining task. Our analysis shows that the correlation relationship among features depends on the decision task and, thus, they display different behaviors as we change the decision task. We applied our data mining approach to network security and validated it using the DARPA KDD99 benchmark data set. Our results show that, using the new decision dependent correlation metric, we can efficiently detect rare network attacks such as User to Root (U2R) and Remote to Local (R2L) attacks. The best reported detection rates for U2R and R2L on the KDD99 data sets were 13.2 percent and 8.4 percent with 0.5 percent false alarm, respectively. For U2R attacks, our approach can achieve a 92.5 percent detection rate with a false alarm of 0.7587 percent. For R2L attacks, our approach can achieve a 92.47 percent detection rate with a false alarm of 8.35 percent.

#index 821871
#* IDR/QR: An Incremental Dimension Reduction Algorithm via QR Decomposition
#@ Jieping Ye;Qi Li;Hui Xiong;Haesun Park;Ravi Janardan;Vipin Kumar
#t 2005
#c 7
#% 67565
#% 80995
#% 200694
#% 212689
#% 224113
#% 235342
#% 251654
#% 294837
#% 315284
#% 324288
#% 329562
#% 340309
#% 342828
#% 578399
#% 581716
#% 729437
#% 755463
#% 769964
#% 993986
#% 1776405
#% 1828409
#% 1860188
#% 1862539
#! Dimension reduction is a critical data preprocessing step for many database and data mining applications, such as efficient storage and retrieval of high-dimensional data. In the literature, a well-known dimension reduction algorithm is Linear Discriminant Analysis (LDA). The common aspect of previously proposed LDA-based algorithms is the use of Singular Value Decomposition (SVD). Due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in LDA, there has been little work on designing incremental LDA algorithms that can efficiently incorporate new data items as they become available. In this paper, we propose an LDA-based incremental dimension reduction algorithm, called IDR/QR, which applies QR Decomposition rather than SVD. Unlike other LDA-based algorithms, this algorithm does not require the whole data matrix in main memory. This is desirable for large data sets. More importantly, with the insertion of new data items, the IDR/QR algorithm can constrain the computational cost by applying efficient QR-updating techniques. Finally, we evaluate the effectiveness of the IDR/QR algorithm in terms of classification error rate on the reduced dimensional space. Our experiments on several real-world data sets reveal that the classification error rate achieved by the IDR/QR algorithm is very close to the best possible one achieved by other LDA-based algorithms. However, the IDR/QR algorithm has much less computational cost, especially when new data items are inserted dynamically.

#index 821872
#* Automatic Identification of Informative Sections of Web Pages
#@ Sandip Debnath;Prasenjit Mitra;Nirmal Pal;C. Lee Giles
#t 2005
#c 7
#! Web pages驴especially dynamically generated ones驴contain several items that cannot be classified as the "primary content,驴 e.g., navigation sidebars, advertisements, copyright notices, etc. Most clients and end-users search for the primary content, and largely do not seek the noninformative content. A tool that assists an end-user or application to search and process information from Web pages automatically, must separate the "primary content sections驴 from the other content sections. We call these sections as "Web page blocks驴 or just "blocks.驴 First, a tool must segment the Web pages into Web page blocks and, second, the tool must separate the primary content blocks from the noninformative content blocks. In this paper, we formally define Web page blocks and devise a new algorithm to partition an HTML page into constituent Web page blocks. We then propose four new algorithms, ContentExtractor, FeatureExtractor, K-FeatureExtractor, and L-Extractor. These algorithms identify primary content blocks by 1) looking for blocks that do not occur a large number of times across Web pages, by 2) looking for blocks with desired features, and by 3) using classifiers, trained with block-features, respectively. While operating on several thousand Web pages obtained from various Web sites, our algorithms outperform several existing algorithms with respect to runtime and/or accuracy. Furthermore, we show that a Web cache system that applies our algorithms to remove noninformative content blocks and to identify similar blocks across Web pages can achieve significant storage savings.

#index 821873
#* QA-Pagelet: Data Preparation Techniques for Large-Scale Data Analysis of the Deep Web
#@ James Caverlee;Ling Liu
#t 2005
#c 7
#% 248808
#% 255137
#% 262045
#% 262061
#% 281214
#% 283050
#% 290830
#% 310567
#% 329562
#% 348180
#% 387427
#% 427023
#% 447946
#% 480479
#% 632051
#% 654469
#% 745472
#% 745535
#% 765409
#% 765410
#% 1016163
#! This paper presents the QA-Pagelet as a fundamental data preparation technique for large-scale data analysis of the Deep Web. To support QA-Pagelet extraction, we present the Thor framework for sampling, locating, and partioning the QA-Pagelets from the Deep Web. Two unique features of the Thor framework are 1) the novel page clustering for grouping pages from a Deep Web source into distinct clusters of control-flow dependent pages and 2) the novel subtree filtering algorithm that exploits the structural and content similarity at subtree level to identify the QA-Pagelets within highly ranked page clusters. We evaluate the effectiveness of the Thor framework through experiments using both simulation and real data sets. We show that Thor performs well over millions of Deep Web pages and over a wide range of sources, including e-Commerce sites, general and specialized search engines, corporate Web sites, medical and legal resources, and several others. Our experiments also show that the proposed page clustering algorithm achieves low-entropy clusters, and the subtree filtering algorithm identifies QA-Pagelets with excellent precision and recall.

#index 821874
#* Hierarchical Taxonomy Preparation for Text Categorization Using Consistent Bipartite Spectral Graph Copartitioning
#@ Bin Gao;Tie-Yan Liu;Guang Feng;Tao Qin;Qian-Sheng Cheng;Wei-Ying Ma
#t 2005
#c 7
#% 99690
#% 190581
#% 224113
#% 309141
#% 313959
#% 342621
#% 342659
#% 344447
#% 387427
#% 466675
#% 577269
#% 642986
#% 643009
#% 729437
#% 729918
#% 763708
#% 770849
#% 807422
#! Multiclass classification has been investigated for many years in the literature. Recently, the scales of real-world multiclass classification applications have become larger and larger. For example, there are hundreds of thousands of categories employed in the Open Directory Project (ODP) and the Yahoo! directory. In such cases, the scalability of classification methods turns out to be a major concern. To tackle this problem, hierarchical classification is proposed and widely adopted to get better trade-off between effectiveness and efficiency. Unfortunately, many data sets are not explicitly organized in hierarchical forms and, therefore, hierarchical classification cannot be used directly. In this paper, we propose a novel algorithm to automatically mine a hierarchical structure from the flat taxonomy of a data corpus as a preparation for the adoption of hierarchical classification. In particular, we first compute matrices to represent the relations among categories, documents, and terms. And, then, we cocluster the three substances at different scales through consistent bipartite spectral graph copartitioning, which is formulated as a generalized singular value decomposition problem. At last, a hierarchical taxonomy is constructed from the category clusters. Our experiments showed that the proposed algorithm could discover very reasonable taxonomy hierarchy and help improve the classification accuracy.

#index 821875
#* Using Object Deputy Model to Prepare Data for Data Warehousing
#@ Zhiyong Peng;Qing Li;Ling Feng;Xuhui Li;Junqiang Liu
#t 2005
#c 7
#% 85089
#% 102780
#% 111913
#% 158200
#% 169921
#% 182899
#% 199537
#% 210211
#% 223781
#% 261691
#% 308509
#% 316550
#% 438507
#% 458608
#% 462619
#% 462624
#% 463900
#% 463919
#% 464232
#% 480958
#% 482111
#% 511906
#% 533355
#% 533903
#% 563742
#% 614579
#! Providing integrated access to multiple, distributed, heterogeneous databases and other information sources has become one of the leading issues in database research and the industry. One of the most effective approaches is to extract and integrate information of interest from each source in advance and store them in a centralized repository (known as a data warehouse). When a query is posed, it is evaluated directly at the warehouse without accessing the original information sources. One of the techniques that this approach uses to improve the efficiency of query processing is materialized view(s). Essentially, materialized views are used for data warehouses, and various methods for relational databases have been developed. In this paper, we will first discuss an object deputy approach to realize materialized object views for data warehouses which can also incorporate object-oriented databases. A framework has been developed using Smalltalk to prepare data for data warehousing, in which an object deputy model and database connecting tools have been implemented. The object deputy model can provide an easy-to-use way to resolve inconsistency and conflicts while preparing data for data warehousing, as evidenced by our empirical study.

#index 821876
#* Linear-Time Wrappers to Identify Atypical Points: Two Subset Generation Methods
#@ Saeed Hashemi
#t 2005
#c 7
#% 229972
#% 243727
#% 243728
#% 316709
#% 380342
#% 418152
#% 465746
#% 1250179
#% 1290045
#! The wrapper approach to identify atypical examples can be preferable to the filter approach (which may not be consistent with the classifier in use), but its running time is prohibitive. The fastest available wrappers are quadratic in the number of examples, which is far too expensive for atypical detection. The algorithm presented in this paper is a linear-time wrapper that is roughly 75 times faster than the quadratic wrappers on average over 7 classifiers and 20 data sets tested in this research. Also, two subset generation methods for the wrapper are introduced and compared. Atypical points are defined in this paper as the misclassified points that the proposed algorithm (Atypical Sequential Removing: ASR) finds not useful to the classification task. They may include outliers as well as overlapping samples. ASR can identify and rank atypical points in the whole data set without damaging the prediction accuracy. It is general enough that classifiers without reject option can use it. Experiments on benchmark data sets and different classifiers show promising results and confirm that this wrapper method has some advantages and can be used for atypical detection.

#index 821877
#* Security of Chien's Efficient Time-Bound Hierarchical Key Assignment Scheme
#@ Xun Yi
#t 2005
#c 7
#% 443477
#% 641957
#% 772848
#! Recently, Chien proposed a time-bound hierarchical key assignment scheme based on tamper-resistant devices. Without public key cryptography, Chien's scheme greatly reduces computation load and implementation cost. In this paper, we show that Chien's scheme is insecure against a collusion attack whereby three users conspire to access some secret class keys that they should not know according to Chien's scheme.

#index 821878
#* GHIC: A Hierarchical Pattern-Based Clustering Algorithm for Grouping Web Transactions
#@ Yinghui Yang;Balaji Padmanabhan
#t 2005
#c 7
#% 239588
#% 287285
#% 397382
#% 577296
#! Grouping customer transactions into segments may help understand customers better. The marketing literature has concentrated on identifying important segmentation variables (e.g., customer loyalty) and on using cluster analysis and mixture models for segmentation. The data mining literature has provided various clustering algorithms for segmentation without focusing specifically on clustering customer transactions. Building on the notion that observable customer transactions are generated by latent behavioral traits, in this paper, we investigate using a pattern-based clustering approach to grouping customer transactions. We define an objective function that we maximize in order to achieve a good clustering of customer transactions and present an algorithm, GHIC, that groups customer transactions such that itemsets generated from each cluster, while similar to each other, are different from ones generated from others. We present experimental results from user-centric Web usage data that demonstrates that GHIC generates a highly effective clustering of transactions.

#index 824928
#* On Combining Classifier Mass Functions for Text Categorization
#@ David A. Bell;J. W. Guan;Yaxin Bi
#t 2005
#c 7
#% 219051
#% 340904
#% 344447
#% 375017
#% 376266
#% 403696
#% 465895
#% 466572
#% 718846
#% 744739
#! Experience shows that different text classification methods can give different results. We look here at a way of combining the results of two or more different classification methods using an evidential approach. The specific methods we have been experimenting with in our group include the Support Vector Machine, kNN (nearest neighbors), kNN model-based approach (kNNM), and Rocchio methods, but the analysis and methods apply to any methods. We review these learning methods briefly, and then we describe our method for combining the classifiers. In a previous study, we suggested that the combination could be done using evidential operations [1] and that using only two focal points in the mass functions (see below) gives good results. However, there are conditions under which we should choose to use more focal points. We assess some aspects of this choice from an evidential reasoning perspective and suggest a refinement of the approach.

#index 824929
#* Continuous Similarity-Based Queries on Streaming Time Series
#@ Like Gao;Xiaoyang Sean Wang
#t 2005
#c 7
#% 116082
#% 172949
#% 227857
#% 248831
#% 285711
#% 300167
#% 300179
#% 397353
#% 404335
#% 428155
#% 443369
#% 460862
#% 461885
#% 464851
#% 471511
#% 481609
#% 632089
#% 659996
#% 660004
#% 1809954
#! In many applications, local or remote sensors send in streams of data, and the system needs to monitor the streams to discover relevant events/patterns and deliver instant reaction correspondingly. An important scenario is that the incoming stream is a continually appended time series, and the patterns are time series in a database. At each time when a new value arrives (called a time position), the system needs to find, from the database, the nearest or near neighbors of the incoming time series up to the time position. This paper attacks the problem by using Fast Fourier Transform (FFT) to efficiently find the cross correlations of time series, which yields, in a batch mode, the nearest and near neighbors of the incoming time series at many time positions. To take advantage of this batch processing in achieving fast response time, this paper uses prediction methods to predict future values. When the prediction length is long, FFT is used to compute the cross correlations of the predicted series (with the values that have already arrived) and the database patterns, and to obtain predicted distances between the incoming time series at many future time positions and the database patterns. If the prediction length is short, the direct computation method is used to obtain these predicted distances to avoid the overhead of using FFT. When the actual data value arrives, the prediction error together with the predicted distances is used to filter out patterns that are not possible to be the nearest or near neighbors, which provides fast responses. Experiments show that with reasonable prediction errors, the performance gain is significant. Especially, when the long term predictions are available, the proposed method can handle incoming data at a very fast streaming rate.

#index 824930
#* Using One-Class and Two-Class SVMs for Multiclass Image Annotation
#@ King-Shy Goh;Edward Y. Chang;Beitao Li
#t 2005
#c 7
#% 80995
#% 190581
#% 209021
#% 242781
#% 252009
#% 272518
#% 284563
#% 334313
#% 341269
#% 342706
#% 345848
#% 420077
#% 420485
#% 451622
#% 451642
#% 458352
#% 528198
#% 704646
#% 721163
#% 722756
#% 722757
#% 730148
#% 763386
#% 780804
#% 806875
#% 1272365
#% 1858012
#! We propose using one-class, two-class, and multiclass SVMs to annotate images for supporting keyword retrieval of images. Providing automatic annotation requires an accurate mapping of images' low-level perceptual features (e.g., color and texture) to some high-level semantic labels (e.g., landscape, architecture, and animals). Much work has been performed in this area; however, there is a lack of ability to assess the quality of annotation. In this paper, we propose a confidence-based dynamic ensemble (CDE), which employs a three-level classification scheme. At the base-level, CDE uses one-class Support Vector Machines (SVMs) to characterize a confidence factor for ascertaining the correctness of an annotation (or a class prediction) made by a binary SVM classifier. The confidence factor is then propagated to the multiclass classifiers at subsequent levels. CDE uses the confidence factor to make dynamic adjustments to its member classifiers so as to improve class-prediction accuracy, to accommodate new semantics, and to assist in the discovery of useful low-level features. Our empirical studies on a large real-world data set demonstrate CDE to be very effective.

#index 824931
#* Fast Algorithms for Frequent Itemset Mining Using FP-Trees
#@ Gosta Grahne;Jianfei Zhu
#t 2005
#c 7
#% 152934
#% 248791
#% 300120
#% 420062
#% 420063
#% 443164
#% 443350
#% 463903
#% 465003
#% 466664
#% 481290
#% 481754
#% 481779
#% 629644
#% 629671
#% 729418
#% 729933
#% 729942
#% 785343
#% 1015294
#! Efficient algorithms for mining frequent itemsets are crucial for mining association rules as well as for many other data mining tasks. Methods for mining frequent itemsets have been implemented using a prefix-tree structure, known as an FP-tree, for storing compressed information about frequent itemsets. Numerous experimental results have demonstrated that these algorithms perform extremely well. In this paper, we present a novel FP-array technique that greatly reduces the need to traverse FP-trees, thus obtaining significantly improved performance for FP-tree-based algorithms. Our technique works especially well for sparse data sets. Furthermore, we present new algorithms for mining all, maximal, and closed frequent itemsets. Our algorithms use the FP-tree data structure in combination with the FP-array technique efficiently and incorporate various optimization techniques. We also present experimental results comparing our methods with existing algorithms. The results show that our methods are the fastest for many cases. Even though the algorithms consume much memory when the data sets are sparse, they are still the fastest ones when the minimum support is low. Moreover, they are always among the fastest algorithms and consume less memory than other methods when the data sets are dense.

#index 824932
#* An Interactive Approach to Mining Gene Expression Data
#@ Daxin Jiang;Jian Pei;Aidong Zhang
#t 2005
#c 7
#% 60576
#% 273890
#% 324431
#% 397382
#% 438617
#% 469422
#% 469425
#% 589434
#% 659967
#% 727882
#% 727908
#% 729972
#% 769919
#! Effective identification of coexpressed genes and coherent patterns in gene expression data is an important task in bioinformatics research and biomedical applications. Several clustering methods have recently been proposed to identify coexpressed genes that share similar coherent patterns. However, there is no objective standard for groups of coexpressed genes. The interpretation of co-expression heavily depends on domain knowledge. Furthermore, groups of coexpressed genes in gene expression data are often highly connected through a large number of "intermediate驴 genes. There may be no clear boundaries to separate clusters. Clustering gene expression data also faces the challenges of satisfying biological domain requirements and addressing the high connectivity of the data sets. In this paper, we propose an interactive framework for exploring coherent patterns in gene expression data. A novel coherent pattern index is proposed to give users highly confident indications of the existence of coherent patterns. To derive a coherent pattern index and facilitate clustering, we devise an attraction tree structure that summarizes the coherence information among genes in the data set. We present efficient and scalable algorithms for constructing attraction trees and coherent pattern indices from gene expression data sets. Our experimental results show that our approach is effective in mining gene expression data and is scalable for mining large data sets.

#index 824933
#* A Data Envelopment Analysis-Based Approach for Data Preprocessing
#@ Parag C. Pendharkar
#t 2005
#c 7
#% 62579
#% 115241
#% 115266
#% 144521
#% 188419
#% 188972
#% 213287
#% 260516
#% 263960
#% 376247
#% 441730
#% 575617
#% 1860568
#% 1860623
#! In this paper, we show how the data envelopment analysis (DEA) model might be useful to screen training data so a subset of examples that satisfy monotonicity property can be identified. Using real-world health care and software engineering data, managerial monotonicity assumption, and artificial neural network (ANN) as a forecasting model, we illustrate that DEA-based data screening of training data improves forecasting accuracy of an ANN.

#index 824934
#* A Shrinking-Based Clustering Approach for Multidimensional Data
#@ Yong Shi;Yuqing Song;Aidong Zhang
#t 2005
#c 7
#% 70370
#% 80995
#% 210173
#% 232102
#% 248790
#% 248792
#% 248797
#% 273890
#% 273891
#% 296738
#% 316709
#% 349208
#% 466481
#% 479799
#% 481281
#% 566128
#% 631985
#% 664845
#% 1015291
#! Existing data analysis techniques have difficulty in handling multidimensional data. Multidimensional data has been a challenge for data analysis because of the inherent sparsity of the points. In this paper, we first present a novel data preprocessing technique called shrinking which optimizes the inherent characteristic of distribution of data. This data reorganization concept can be applied in many fields such as pattern recognition, data clustering, and signal processing. Then, as an important application of the data shrinking preprocessing, we propose a shrinking-based approach for multidimensional data analysis which consists of three steps: data shrinking, cluster detection, and cluster evaluation and selection. The process of data shrinking moves data points along the direction of the density gradient, thus generating condensed, widely-separated clusters. Following data shrinking, clusters are detected by finding the connected components of dense cells (and evaluated by their compactness). The data-shrinking and cluster-detection steps are conducted on a sequence of grids with different cell sizes. The clusters detected at these scales are compared by a cluster-wise evaluation measurement, and the best clusters are selected as the final result. The experimental results show that this approach can effectively and efficiently detect clusters in both low and high-dimensional spaces.

#index 824935
#* State-Space Optimization of ETL Workflows
#@ Alkis Simitsis;Panos Vassiliadis;Timos Sellis
#t 2005
#c 7
#% 83933
#% 136740
#% 166203
#% 300127
#% 301169
#% 318049
#% 386381
#% 428155
#% 480499
#% 482110
#% 533937
#% 562447
#% 577523
#% 726621
#% 800563
#% 1015303
#% 1388088
#! Extraction-Transformation-Loading (ETL) tools are pieces of software responsible for the extraction of data from several sources, their cleansing, customization, and insertion into a data warehouse. In this paper, we delve into the logical optimization of ETL processes, modeling it as a state-space search problem. We consider each ETL workflow as a state and fabricate the state space through a set of correct state transitions. Moreover, we provide an exhaustive and two heuristic algorithms toward the minimization of the execution cost of an ETL workflow. The heuristic algorithm with greedy characteristics significantly outperforms the other two algorithms for a large set of experimental cases.

#index 824936
#* Pattern Discovery on Australian Medical Claims Data-A Systematic Approach
#@ Ah Chung Tsoi;Shu Zhang;Markus Hagenbuchner
#t 2005
#c 7
#% 10237
#% 395959
#% 487992
#% 609199
#! The national health insurance system in Australia records details on medical services and claims provided to its population. An effective method to the discovery of temporal behavioral patterns in the data set is proposed in this paper. The method consists of a two-step approach which is applied recursively to the data set. First, a clustering algorithm is used to segment the data into classes. Then, hidden Markov models are employed to find the underlying temporal behavioral patterns. These steps are applied recursively to features extracted from the data set until convergence. The main objective is to minimize the misclassification of patient profiles into various classes. This results in a hierarchical tree model consisting of a number of classes; each class groups similar patient temporal behavioral patterns together. The capabilities of the proposed method are demonstrated through the application to a subset of the Australian national health insurance data set. It is shown that the proposed method not only clusters data into various categories of interest, but it also automatically marks the periods in which similar temporal behavioral patterns occurred.

#index 824937
#* A Formal Framework for Prefetching Based on the Type-Level Access Pattern in Object-Relational DBMSs
#@ Wook-Shin Han;Kyu-Young Whang;Yang-Sae Moon
#t 2005
#c 7
#% 13026
#% 58374
#% 68142
#% 111351
#% 116203
#% 152904
#% 152939
#% 210194
#% 235914
#% 248086
#% 299942
#% 404765
#% 442704
#% 442808
#% 464985
#% 479955
#% 480780
#% 555031
#% 581209
#! Prefetching is an effective method for minimizing the number of fetches between the client and the server in a database management system. In this paper, we formally define the notion of prefetching. We also formally propose new notions of the type-level access locality and type-level access pattern. The type-level access locality is a pheonomenon that repetitive patterns exist in the attributes referenced. The type-level access pattern is a pattern of attributes that are referenced in accessing the objects. We then develop an efficient capturing and prefetching policy based on this formal framework. Existing prefetching methods are based on object-level or page-level access patterns, which consist of object-ids or page-ids of the objects accessed. However, the drawback of these methods is that they work only when exactly the same objects or pages are accessed repeatedly. In contrast, even though the same objects are not accessed repeatedly, our technique effectively prefetches objects if the same attributes are referenced repeatedly, i.e., if there is type-level access locality. Many navigational applications in Object-Relational Database Management Systems (ORDBMSs) have type-level access locality. Therefore, our technique can be employed in ORDBMSs to effectively reduce the number of fetches, thereby significantly enhancing the performance. We also address issues in implementing the proposed algorithm. We have conducted extensive experiments in a prototype ORDBMS to show effectiveness of our algorithm. Experimental results using the OO7 benchmark, a real GIS application, and an XML application show that our technique reduces the number of fetches by orders of magnitude and improves the elapsed time by several factors over on-demand fetching and context-based prefetching, which is a state-of-the-art prefetching method. These results indicate that our approach provides a new paradigm in prefetching that improves performance of navigational applications significantly and is a practical method that can be implemented in commercial ORDBMSs.

#index 832568
#* A Threshold-Based Algorithm for Continuous Monitoring of k Nearest Neighbors
#@ Kyriakos Mouratidis;Dimitris Papadias;Spiridon Bakiras;Yufei Tao
#t 2005
#c 7
#% 201876
#% 287466
#% 318703
#% 341471
#% 378398
#% 397353
#% 421124
#% 442615
#% 495433
#% 527187
#% 527191
#% 579313
#% 654443
#% 654478
#% 736290
#% 765453
#% 800571
#% 800572
#% 810061
#% 1015321
#% 1016196
#! Assume a set of moving objects and a central server that monitors their positions over time, while processing continuous nearest neighbor queries from geographically distributed clients. In order to always report up-to-date results, the server could constantly obtain the most recent position of all objects. However, this naïve solution requires the transmission of a large number of rapid data streams corresponding to location updates. Intuitively, current information is necessary only for objects that may influence some query result (i.e., they may be included in the nearest neighbor set of some client). Motivated by this observation, we present a threshold-based algorithm for the continuous monitoring of nearest neighbors that minimizes the communication overhead between the server and the data objects. The proposed method can be used with multiple, static, or moving queries, for any distance definition, and does not require additional knowledge (e.g., velocity vectors) besides object locations.

#index 832569
#* Using Datacube Aggregates for Approximate Querying and Deviation Detection
#@ Themis Palpanas;Nick Koudas;Alberto Mendelzon
#t 2005
#c 7
#% 115608
#% 152588
#% 198467
#% 201921
#% 210190
#% 211044
#% 236410
#% 259995
#% 273889
#% 273907
#% 273909
#% 280448
#% 280494
#% 300136
#% 300138
#% 300183
#% 397371
#% 397390
#% 458537
#% 459025
#% 464215
#% 479648
#% 479791
#% 479986
#% 480158
#% 480803
#% 482095
#% 528023
#% 994004
#! Much research has been devoted to the efficient computation of relational aggregations and, specifically, the efficient execution of the datacube operation. In this paper, we consider the inverse problem, that of deriving (approximately) the original data from the aggregates. We motivate this problem in the context of two specific application areas, approximate query answering and data analysis. We propose a framework based on the notion of information entropy that enables us to estimate the original values in a data set, given only aggregated information about it. We then show how approximate queries on the data from which the aggregates were derived can be performed using our framework. We also describe an alternate use of the proposed framework that enables us to identify values that deviate from the underlying data distribution, suitable for data mining purposes. We present a detailed performance study of the algorithms using both real and synthetic data, highlighting the benefits of our approach as well as the efficiency of the proposed solutions. Finally, we evaluate our techniques with a case study on a real data set, which illustrates the applicability of our approach.

#index 832570
#* Comparing Relationships in Conceptual Modeling: Mapping to Semantic Classifications
#@ Veda C. Storey
#t 2005
#c 7
#% 2298
#% 4797
#% 92387
#% 136317
#% 153433
#% 156337
#% 196354
#% 198006
#% 225471
#% 231766
#% 234797
#% 245246
#% 248882
#% 250472
#% 264773
#% 270812
#% 271706
#% 279481
#% 297184
#% 297188
#% 306860
#% 318705
#% 342980
#% 370096
#% 397346
#% 435121
#% 437727
#% 443467
#% 445255
#% 445286
#% 445444
#% 450536
#% 474500
#% 533943
#% 534369
#% 534843
#% 535703
#% 572457
#% 573898
#% 722572
#% 794085
#% 995910
#! Much of the research that deals with understanding the real world and representing it in a conceptual model uses some form of the entity-relationship model as a means of representation. This research proposes an ontology for classifying relationship verb phrases based upon the domain and context of the application within which the relationship appears. The classification categories to which the verb phrases are mapped were developed based upon prior research in databases, ontologies, and linguistics. The usefulness of the ontology for comparing relationships when used in conjunction with an entity ontology is discussed. Together, these ontologies can be effective in comparing two conceptual database designs for integration and validation. Empirical testing of the ontology on a number of relationships from different application domains and contexts illustrates the usefulness of the research.

#index 832571
#* MAFIA: A Maximal Frequent Itemset Algorithm
#@ Doug Burdick;Manuel Calimlim;Jason Flannick;Johannes Gehrke;Tomi Yiu
#t 2005
#c 7
#% 152934
#% 201894
#% 227917
#% 232136
#% 248791
#% 252401
#% 280436
#% 300120
#% 300124
#% 310507
#% 329598
#% 338609
#% 443350
#% 443392
#% 459020
#% 462219
#% 462234
#% 462238
#% 464714
#% 465003
#% 466664
#% 480154
#% 481290
#% 481754
#% 481779
#% 631986
#% 978636
#% 1272179
#! We present a new algorithm for mining maximal frequent itemsets from a transactional database. The search strategy of the algorithm integrates a depth-first traversal of the itemset lattice with effective pruning mechanisms that significantly improve mining performance. Our implementation for support counting combines a vertical bitmap representation of the data with an efficient bitmap compression scheme. In a thorough experimental analysis, we isolate the effects of individual components of MAFIA including search space pruning techniques and adaptive compression. We also compare our performance with previous work by running tests on very different types of data sets. Our experiments show that MAFIA performs best when mining long itemsets and outperforms other algorithms on dense data by a factor of three to 30.

#index 832572
#* Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection
#@ Srivatsan Laxman;P. S. Sastry;K. P. Unnikrishnan
#t 2005
#c 7
#% 387941
#% 420063
#% 443195
#% 443502
#% 463903
#% 556804
#% 577226
#% 624552
#% 727902
#% 785333
#% 1562549
#! This paper establishes a formal connection between two common, but previously unconnected methods for analyzing data streams: discovering frequent episodes in a computer science framework and learning generative models in a statistics framework. We introduce a special class of discrete Hidden Markov Models (HMMs), called Episode Generating HMMs (EGHs), and associate each episode with a unique EGH. We prove that, given any two episodes, the EGH that is more likely to generate a given data sequence is the one associated with the more frequent episode. To be able to establish such a relationship, we define a new measure of frequency of an episode, based on what we call nonoverlapping occurrences of the episode in the data. An efficient algorithm is proposed for counting the frequencies for a set of episodes. Through extensive simulations, we show that our algorithm is both effective and more efficient than current methods for frequent episode discovery. We also show how the association between frequent episodes and EGHs can be exploited to assess the significance of frequent episodes discovered and illustrate empirically how this idea may be used to improve the efficiency of the frequent episode discovery.

#index 832573
#* Evolutionary Constructive Induction
#@ Mohammed Muharram;George D. Smith
#t 2005
#c 7
#% 124073
#% 136350
#% 290482
#% 466357
#% 637521
#% 1391645
#! Feature construction in classification is a preprocessing step in which one or more new attributes are constructed from the original attribute set, the object being to construct features that are more predictive than the original feature set. Genetic programming allows the construction of nonlinear combinations of the original features. We present a comprehensive analysis of genetic programming (GP) used for feature construction, in which four different fitness functions are used by the GP and four different classification techniques are subsequently used to build the classifier. Comparisons are made of the error rates and the size and complexity of the resulting trees. We also compare the overall performance of GP in feature construction with that of GP used directly to evolve a decision tree classifier, with the former proving to be a more effective use of the evolutionary paradigm.

#index 832574
#* Tri-Training: Exploiting Unlabeled Data Using Three Classifiers
#@ Zhi-Hua Zhou;Ming Li
#t 2005
#c 7
#% 116165
#% 170649
#% 209021
#% 252011
#% 283180
#% 290482
#% 311027
#% 316509
#% 451057
#% 466263
#% 466888
#% 565545
#% 577240
#% 722495
#% 748550
#% 811376
#% 816079
#! In many practical data mining applications, such as Web page classification, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. In this paper, a new co-training style semi-supervised learning algorithm, named tri-training, is proposed. This algorithm generates three classifiers from the original labeled example set. These classifiers are then refined using unlabeled examples in the tri-training process. In detail, in each round of tri-training, an unlabeled example is labeled for a classifier if the other two classifiers agree on the labeling, under certain conditions. Since tri-training neither requires the instance space to be described with sufficient and redundant views nor does it put any constraints on the supervised learning algorithm, its applicability is broader than that of previous co-training style algorithms. Experiments on UCI data sets and application to the Web page classification task indicate that tri-training can effectively exploit unlabeled data to enhance the learning performance.

#index 832575
#* Cost-Constrained Data Acquisition for Intelligent Data Preparation
#@ Xingquan Zhu;Xindong Wu
#t 2005
#c 7
#% 17144
#% 27068
#% 36672
#% 90157
#% 92533
#% 92554
#% 116165
#% 132697
#% 136350
#% 160852
#% 169717
#% 170649
#% 240849
#% 243731
#% 269634
#% 280406
#% 307100
#% 346340
#% 382342
#% 449566
#% 449588
#% 465762
#% 629616
#% 785570
#% 818916
#% 840577
#% 1250179
#% 1272369
#% 1673023
#! Real-world data is noisy and can often suffer from corruptions or incomplete values that may impact the models created from the data. To build accurate predictive models, data acquisition is usually adopted to prepare the data and complete missing values. However, due to the significant cost of doing so and the inherent correlations in the data set, acquiring correct information for all instances is prohibitive and unnecessary. An interesting and important problem that arises here is to select what kinds of instances to complete so the model built from the processed data can receive the "maximum驴 performance improvement. This problem is complicated by the reality that the costs associated with the attributes are different, and fixing the missing values of some attributes is inherently more expensive than others. Therefore, the problem becomes that given a fixed budget, what kinds of instances should be selected for preparation, so that the learner built from the processed data set can maximize its performance? In this paper, we propose a solution for this problem, and the essential idea is to combine attribute costs and the relevance of each attribute to the target concept, so that the data acquisition can pay more attention to those attributes that are cheap in price but informative for classification. To this end, we will first introduce a unique Economical Factor (EF) that seamlessly integrates the cost and the importance (in terms of classification) of each attribute. Then, we will propose a cost-constrained data acquisition model, where active learning, missing value prediction, and impact-sensitive instance ranking are combined for effective data acquisition. Experimental results and comparative studies from real-world data sets demonstrate the effectiveness of our method.

#index 832576
#* Secure Interoperation in a Multidomain Environment Employing RBAC Policies
#@ Basit Shafiq;James B. D. Joshi;Elisa Bertino;Arif Ghafoor
#t 2005
#c 7
#% 22948
#% 204453
#% 207074
#% 258394
#% 258405
#% 270778
#% 295792
#% 300466
#% 308081
#% 316578
#% 344222
#% 345974
#% 345976
#% 379249
#% 390457
#% 438370
#% 443007
#% 443214
#% 443505
#% 481280
#% 507395
#% 592727
#% 788998
#% 1015326
#! Multidomain application environments where distributed multiple organizations interoperate with each other are becoming a reality as witnessed by emerging Internet-based enterprise applications. Composition of a global coherent security policy that governs information and resource accesses in such environments is a challenging problem. In this paper, we propose a policy integration framework for merging heterogeneous Role-Based Access Control (RBAC) policies of multiple domains into a global access control policy. A key challenge in composition of this policy is the resolution of conflicts that may arise among the RBAC policies of individual domains. We propose an integer programming (IP)-based approach for optimal resolution of such conflicts. The optimality criterion is to maximize interdomain role accesses without exceeding the autonomy losses beyond the acceptable limit.

#index 832577
#* A Lightweight and Scalable e-Transaction Protocol for Three-Tier Systems with Centralized Back-End Database
#@ Paolo Romano;Francesco Quaglia;Bruno Ciciani
#t 2005
#c 7
#% 194332
#% 235084
#% 330245
#% 398111
#% 403195
#% 617284
#% 617443
#% 769240
#% 979106
#% 979167
#% 994023
#! The e-Transaction abstraction is a recent formalization of end-to-end reliability properties for three-tier systems. In this work, we present a protocol ensuring the e-Transaction guarantees in case the back-end tier consists of a centralized database. Our proposal addresses the case of stateless application servers, and is both simple and effective since 1) it does not employ any distributed commit protocol and 2) does not require coordination among the replicas of the application server.

#index 832578
#* On Rival Penalization Controlled Competitive Learning for Clustering with Automatic Cluster Number Selection
#@ Yiu-ming Cheung
#t 2005
#c 7
#% 72560
#% 176705
#% 196340
#% 256648
#% 717373
#% 1860971
#! The existing Rival Penalized Competitive Learning (RPCL) algorithm and its variants have provided an attractive way to perform data clustering without knowing the exact number of clusters. However, their performance is sensitive to the preselection of the rival delearning rate. In this paper, we further investigate the RPCL and present a mechanism to control the strength of rival penalization dynamically. Consequently, we propose the Rival Penalization Controlled Competitive Learning (RPCCL) algorithm and its stochastic version. In each of these algorithms, the selection of the delearning rate is circumvented using a novel technique. We compare the performance of RPCCL to RPCL in Gaussian mixture clustering and color image segmentation, respectively. The experiments have produced the promising results.

#index 832579
#* Accurate and Reliable Diagnosis and Classification Using Probabilistic Ensemble Simplified Fuzzy ARTMAP
#@ Chu Kiong Loo;M. V. C. Rao
#t 2005
#c 7
#% 23408
#% 261569
#% 593109
#% 1784199
#% 1860513
#% 1862532
#! In this paper, an accurate and effective probabilistic plurality voting method to combine outputs from multiple Simplified Fuzzy ARTMAP (SFAM) classifiers is presented. Five ELENA benchmark problems and five medical benchmark data sets have been used to evaluate the applicability and performance of the proposed Probabilistic Ensemble Simplified Fuzzy ARTMAP (PESFAM) network. Among the five benchmark problems in ELENA project, PESFAM outperforms the SFAM and Multi-layer Perceptron (MLP) classifier. In addition, the effectiveness of the proposed PESFAM is delineated in medical diagnosis applications. For the medical diagnosis and classification problems, PESFAM achieves 100 percent in accuracy, specificity, and sensitivity based on the 10-fold crossvalidation and these results are superior to those from other classification algorithms. In addition, the a posteri probability of the predicted class can be used to measure the prediction reliability of PESFAM. The experiments demonstrate the potential of the proposed multiple SFAM classifiers in offering an optimal solution to the data-ordering problem of SFAM implementation and also as an intelligent medical diagnosis tool.

#index 837602
#* Storing XML (with XSD) in SQL Databases: Interplay of Logical and Physical Designs
#@ Surajit Chaudhuri;Zhiyuan Chen;Kyuseok Shim;Yuqing Wu
#t 2005
#c 7
#% 172927
#% 273922
#% 333935
#% 397364
#% 397375
#% 465067
#% 479956
#% 480152
#% 480158
#% 482100
#% 504574
#% 567011
#% 570875
#% 632066
#% 632100
#% 654450
#% 659924
#% 765431
#% 1016141
#! Much of business XML data has accompanying XSD specifications. In many scenarios, "shredding驴 such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

#index 837603
#* Computing and Managing Cardinal Direction Relations
#@ Spiros Skiadopoulos;Christos Giannoukos;Nikos Sarkas;Panos Vassiliadis;Timos Sellis;Manolis Koubarakis
#t 2005
#c 7
#% 2115
#% 26168
#% 115672
#% 116607
#% 166244
#% 195459
#% 201880
#% 247008
#% 270714
#% 318377
#% 321281
#% 397398
#% 481283
#% 526851
#% 526998
#% 527320
#% 535142
#% 710974
#% 741459
#% 806735
#! Qualitative spatial reasoning forms an important part of the commonsense reasoning required for building intelligent Geographical Information Systems (GIS). Previous research has come up with models to capture cardinal direction relations for typical GIS data. In this paper, we target the problem of efficiently computing the cardinal direction relations between regions that are composed of sets of polygons and present two algorithms for this task. The first of the proposed algorithms is purely qualitative and computes, in linear time, the cardinal direction relations between the input regions. The second has a quantitative aspect and computes, also in linear time, the cardinal direction relations with percentages between the input regions. Our experimental evaluation indicates that the proposed algorithms outperform existing methodologies. The algorithms have been implemented and embedded in an actual system, CarDirect, that allows the user to 1) specify and annotate regions of interest in an image or a map, 2) compute cardinal direction relations between them, and 3) pose queries in order to retrieve combinations of interesting regions.

#index 837604
#* Document Clustering Using Locality Preserving Indexing
#@ Deng Cai;Xiaofei He;Jiawei Han
#t 2005
#c 7
#% 36672
#% 118749
#% 224113
#% 248027
#% 262059
#% 280822
#% 313959
#% 397147
#% 466675
#% 635713
#% 640459
#% 643008
#% 729437
#% 766418
#% 766432
#% 766434
#% 766435
#% 835741
#% 1011871
#! We propose a novel document clustering method which aims to cluster the documents into different semantic classes. The document space is generally of high dimensionality and clustering in such a high dimensional space is often infeasible due to the curse of dimensionality. By using Locality Preserving Indexing (LPI), the documents can be projected into a lower-dimensional semantic space in which the documents related to the same semantics are close to each other. Different from previous document clustering methods based on Latent Semantic Indexing (LSI) or Nonnegative Matrix Factorization (NMF), our method tries to discover both the geometric and discriminating structures of the document space. Theoretical analysis of our method shows that LPI is an unsupervised approximation of the supervised Linear Discriminant Analysis (LDA) method, which gives the intuitive motivation of our method. Extensive experimental evaluations are performed on the Reuters-21578 and TDT2 data sets.

#index 837605
#* STAVIES: A System for Information Extraction from Unknown Web Data Sources through Automatic Web Wrapper Generation Using Clustering Techniques
#@ Nikolaos K. Papadakis;Dimitrios Skoutas;Konstantinos Raftopoulos;Theodora A. Varvarigou
#t 2005
#c 7
#% 57485
#% 207376
#% 216509
#% 240955
#% 248808
#% 273925
#% 312860
#% 428148
#% 438676
#% 479807
#% 480648
#% 511733
#% 511897
#% 511902
#% 660272
#% 683722
#% 729978
#% 970828
#! A fully automated wrapper for information extraction from Web pages is presented. The motivation behind such systems lies in the emerging need for going beyond the concept of "human browsing.驴 The World Wide Web is today the main "all kind of information驴 repository and has been so far very successful in disseminating information to humans. By automating the process of information retrieval, further utilization by targeted applications is enabled. The key idea in our novel system is to exploit the format of the Web pages to discover the underlying structure in order to finally infer and extract pieces of information from the Web page. Our system first identifies the section of the Web page that contains the information to be extracted and then extracts it by using clustering techniques and other tools of statistical origin. STAVIES can operate without human intervention and does not require any training. The main innovation and contribution of the proposed system consists of introducing a signal-wise treatment of the tag structural hierarchy and using hierarchical clustering techniques to segment the Web pages. The importance of such a treatment is significant since it permits abstracting away from the raw tag-manipulating approach. Experimental results and comparisons with other state of the art systems are presented and discussed in the paper, indicating the high performance of the proposed algorithm.

#index 837606
#* Nested Monte Carlo EM Algorithm for Switching State-Space Models
#@ Cristina Adela Popescu;Yau Shu Wong
#t 2005
#c 7
#% 207501
#% 225837
#% 232122
#% 277483
#% 450529
#% 452860
#% 857094
#% 858737
#% 1042787
#! Switching state-space models have been widely used in many applications arising from science, engineering, economic, and medical research. In this paper, we present a Monte Carlo Expectation Maximization (MCEM) algorithm for learning the parameters and classifying the states of a state-space model with a Markov switching. A stochastic implementation based on the Gibbs sampler is introduced in the expectation step of the MCEM algorithm. We study the asymptotic properties of the proposed algorithm, and we also describe how a nesting approach and the Rao-Blackwellized forms can be employed to accelerate the rate of convergence of the MCEM algorithm. Finally, the performance and the effectiveness of the proposed method are demonstrated by applications to simulated and physiological experimental data.

#index 837607
#* Learning Concept Descriptions with Typed Evolutionary Programming
#@ Claire J. Thie;Christophe Giraud-Carrier
#t 2005
#c 7
#% 124073
#% 160830
#% 160831
#% 167411
#% 200223
#% 243357
#% 243370
#% 284608
#% 465859
#% 466540
#% 466551
#% 478114
#% 479296
#% 496247
#% 546646
#% 550555
#% 550567
#% 567398
#% 643852
#% 1022823
#% 1022829
#% 1248854
#! Examples and concepts in traditional concept learning tasks are represented with the attribute-value language. While enabling efficient implementations, we argue that such propositional representation is inadequate when data is rich in structure. This paper describes STEPS, a strongly-typed evolutionary programming system designed to induce concepts from structured data. STEPS' higher-order logic representation language enhances expressiveness, while the use of evolutionary computation dampens the effects of the corresponding explosion of the search space. Results on the PTE2 challenge, a major real-world knowledge discovery application from the molecular biology domain, demonstrate promise.

#index 837608
#* Learning Users' Interests by Quality Classification in Market-Based Recommender Systems
#@ Yan Zheng Wei;Luc Moreau;Nicholas R. Jennings
#t 2005
#c 7
#% 135414
#% 202009
#% 202011
#% 220706
#% 220711
#% 260778
#% 301259
#% 304425
#% 319705
#% 376266
#% 451407
#% 528182
#% 643154
#% 734590
#% 744739
#% 783440
#% 1272286
#% 1720586
#! Recommender systems are widely used to cope with the problem of information overload and, to date, many recommendation methods have been developed. However, no one technique is best for all users in all situations. To combat this, we have previously developed a market-based recommender system that allows multiple agents (each representing a different recommendation method or system) to compete with one another to present their best recommendations to the user. In our system, the marketplace encourages good recommendations by rewarding the corresponding agents who supplied them according to the users' ratings of their suggestions. Moreover, we have theoretically shown how our system incites the agents to bid in a manner that ensures only the best recommendations are presented. To do this effectively in practice, however, each agent needs to be able to classify its recommendations into different internal quality levels, learn the users' interests for these different levels, and then adapt its bidding behavior for the various levels accordingly. To this end, in this paper, we develop a reinforcement learning and Boltzmann exploration strategy that the recommending agents can exploit for these tasks. We then demonstrate that this strategy does indeed help the agents to effectively obtain information about the users' interests which, in turn, speeds up the market convergence and enables the system to rapidly highlight the best recommendations.

#index 837609
#* "Missing Is Useful': Missing Values in Cost-Sensitive Decision Trees
#@ Shichao Zhang;Zhenxing Qin;Charles X. Ling;Shengli Sheng
#t 2005
#c 7
#% 17144
#% 90157
#% 92554
#% 136350
#% 160852
#% 232117
#% 280437
#% 418112
#% 447606
#% 449566
#% 464639
#% 477640
#% 770791
#% 785338
#% 1272369
#% 1289281
#% 1499572
#! Many real-world data sets for machine learning and data mining contain missing values and much previous research regards it as a problem and attempts to impute missing values before training and testing. In this paper, we study this issue in cost-sensitive learning that considers both test costs and misclassification costs. If some attributes (tests) are too expensive in obtaining their values, it would be more cost-effective to miss out their values, similar to skipping expensive and risky tests (missing values) in patient diagnosis (classification). That is, "missing is useful驴 as missing values actually reduces the total cost of tests and misclassifications and, therefore, it is not meaningful to impute their values. We discuss and compare several strategies that utilize only known values and that "missing is useful驴 for cost reduction in cost-sensitive decision tree learning.

#index 837610
#* SynchRuler: A Rule-Based Flexible Synchronization Model with Model Checking
#@ Ramazan Savas Aygun;Aidong Zhang
#t 2005
#c 7
#% 58361
#% 84047
#% 166865
#% 172885
#% 175720
#% 219867
#% 234819
#% 247925
#% 257892
#% 261885
#% 261886
#% 286906
#% 319244
#% 341311
#% 434717
#% 435055
#% 452790
#% 480688
#% 519990
#% 571041
#% 632350
#% 659263
#% 1180136
#% 1848270
#! Flexible synchronization models cannot provide a proper way of managing user interactions that change the course of a presentation. In this paper, we present a flexible synchronization model, termed SynchRuler, which allows such user interactions including backward and skip. The synchronization rules, which are based on Event-Condition-Action (ECA) rules, are maintained to handle relationships among streams in SynchRuler. The synchronization rules are manipulated by the Receiver-Controller-Actor (RCA) scheme, where receivers, controllers, and actors are objects to receive events, to check conditions, and to execute actions, respectively. The verification of a multimedia presentation specification is performed with the synchronization model. The correctness of the model and the presentation is controlled with a technique called model checking. Model checker PROMELA/SPIN tool is used for automatic verification of the correctness of LTL (Linear Temporal Logic) formulas.

#index 837611
#* Localization Site Prediction for Membrane Proteins by Integrating Rule and SVM Classification
#@ Senqiang Zhou;Ke Wang
#t 2005
#c 7
#% 46803
#% 136350
#% 152934
#% 172892
#% 190581
#% 269217
#% 310539
#% 310559
#% 328945
#% 329537
#% 397383
#% 420077
#% 425048
#% 458379
#% 463903
#% 481290
#% 546791
#% 577256
#% 629610
#% 729953
#% 783497
#! We study the localization prediction of membrane proteins for two families of medically important disease-causing bacteria, called Gram-Negative and Gram-Positive bacteria. Each such bacterium has its cell surrounded by several layers of membranes. Identifying where proteins are located in a bacterial cell is of primary research interest for antibiotic and vaccine drug design. This problem has three requirements: First, with any subsequence of amino acid residues being potentially a dimension, it has an extremely high dimensionality, few being irrelevant. Second, the prediction of a target localization site must have a high precision in order to be useful to biologists, i.e., at least 90 percent or even 95 percent, while recall is as high as possible. Achieving such a precision is made harder by the fact that target sequences are often much fewer than background sequences. Third, the rationale of prediction should be understandable to biologists for taking actions. Meeting all these requirements presents a significant challenge in that a high dimensionality requires a complex model that is often hard to understand. The support vector machine (SVM) model has an outstanding performance in a high-dimensional space, therefore, it addresses the first two requirements. However, the SVM model involves many features in a single kernel function, therefore, it does not address the third requirement. We address all three requirements by integrating the SVM model with a rule-based model, where the understandable if-then rules capture "major structures驴 and the elaborated SVM model captures "subtle structures.驴 Importantly, the integrated model preserves the precision/recall performance of SVM and, at the same time, exposes major structures in a form understandable to the human user. We focus on searching for high quality rules and partitioning the prediction between rules and SVM so as to achieve these properties. We evaluate our method on several membrane localization problems. The purpose of this paper is not improving the precision/recall of SVM, but is manifesting the rationale of a SVM classifier through partitioning the classification between if-then rules and the SVM classifier and preserving the precision/recall of SVM.

#index 837612
#* A Scalable P2P Platform for the Knowledge Grid
#@ Hai Zhuge;Xiaoping Sun;Jie Liu;Erlin Yao;Xue Chen
#t 2005
#c 7
#% 65353
#% 172918
#% 213080
#% 337046
#% 340175
#% 342374
#% 342375
#% 349973
#% 429749
#% 481296
#% 496144
#% 496291
#% 636008
#% 636009
#% 636108
#% 654468
#% 722142
#% 722155
#% 745354
#% 762651
#% 763170
#% 772022
#% 803675
#% 928357
#% 963874
#% 1016166
#% 1112378
#% 1850764
#% 1850772
#! The Knowledge Grid needs to operate with a scalable platform to provide large-scale intelligent services. A key function of such a platform is to efficiently support various complex queries in a dynamic large-scale network environment. This paper proposes a platform to support index-based path queries by incorporating a semantic overlay with an underlying structured P2P network that provides object location and management services. Various distributed indexing structures can be dynamically formed by publishing semantic objects as indexing nodes. Queries are forwarded along the chains of semantic object pointers to search for objects. We investigate the deployment of a scalable distributed trie index for broadcast queries on key strings, propose a decentralized load balancing method for solving the problem of uneven load distribution incurred by heterogeneity of loads and node capacities and by the distributed trie index, and give an approach for improving the availability of the semantic overlay and its trie index. Experiments demonstrate the scalability of the proposed platform.

#index 837613
#* Correction to "Mining Closed and Maximal Frequent Subtrees from Databases of Labeled Rooted Trees"
#@ Yun Chi;Yi Xia;Yirong Yang;Richard R. Muntz
#t 2005
#c 7

#index 843872
#* Input Variable Selection: Mutual Information and Linear Mixing Measures
#@ Thomas Trappenberg;Jie Ouyang;Andrew Back
#t 2006
#c 7
#% 103051
#% 243728
#% 346340
#% 722929
#% 761601
#% 1809747
#% 1860802
#% 1861447
#! Determining the most appropriate inputs to a model has a significant impact on the performance of the model and associated algorithms for classification, prediction, and data analysis. Previously, we proposed an algorithm ICAIVS which utilizes independent component analysis (ICA) as a preprocessing stage to overcome issues of dependencies between inputs, before the data being passed through to an inout variable selection (IVS) stage. While we demonstrated previously with artificial data that ICA can prevent an overestimation of necessary input variables, we show here that mixing between input variables is common in real-world data sets so that ICA preprocessing is useful in practice. This experimental test is based on new measures introduced in this paper. Furthermore, we extend the implementation of our variable selection scheme to a statistical dependency test based on mutual information and test several algorithms on Gaussian and sub-Gaussian signals. Specifically, we propose a novel method of quantifying linear dependencies using ICA estimates of mixing matrices with a new Linear Mixing Measure (LMM).

#index 843873
#* Text Classification without Negative Examples Revisit
#@ Gabriel Pui Cheong Fung;Jeffrey X. Yu;Hongjun Lu;Philip S. Yu
#t 2006
#c 7
#% 46803
#% 118771
#% 194283
#% 280404
#% 280817
#% 304876
#% 311027
#% 340904
#% 344447
#% 458379
#% 464604
#% 464641
#% 464777
#% 466083
#% 577235
#% 629610
#% 727883
#% 729621
#% 800568
#% 840583
#% 1250186
#% 1279298
#! Traditionally, building a classifier requires two sets of examples: positive examples and negative examples. This paper studies the problem of building a text classifier using positive examples (P) and unlabeled examples (U). The unlabeled examples are mixed with both positive and negative examples. Since no negative example is given explicitly, the task of building a reliable text classifier becomes far more challenging. Simply treating all of the unlabeled examples as negative examples and building a classifier thereafter is undoubtedly a poor approach to tackling this problem. Generally speaking, most of the studies solved this problem by a two-step heuristic: First, extract negative examples (N) from U. Second, build a classifier based on P and N. Surprisingly, most studies did not try to extract positive examples from U. Intuitively, enlarging P by P' (positive examples extracted from U) and building a classifier thereafter should enhance the effectiveness of the classifier. Throughout our study, we find that extracting P' is very difficult. A document in U that possesses the features exhibited in P does not necessarily mean that it is a positive example, and vice versa. The very large size of and very high diversity in U also contribute to the difficulties of extracting P'. In this paper, we propose a labeling heuristic called PNLH to tackle this problem. PNLH aims at extracting high quality positive examples and negative examples from U and can be used on top of any existing classifiers. Extensive experiments based on several benchmarks are conducted. The results indicated that PNLH is highly feasible, especially in the situation where |P| is extremely small.

#index 843874
#* Fast and Memory Efficient Mining of Frequent Closed Itemsets
#@ Claudio Lucchese;Salvatore Orlando;Raffaele Perego
#t 2006
#c 7
#% 201894
#% 227917
#% 279120
#% 300120
#% 333877
#% 338594
#% 466490
#% 466491
#% 478770
#% 481290
#% 502141
#% 577234
#% 629611
#% 729933
#% 729942
#% 765529
#% 772329
#! This paper presents a new scalable algorithm for discovering closed frequent itemsets, a lossless and condensed representation of all the frequent itemsets that can be mined from a transactional database. Our algorithm exploits a divide-and-conquer approach and a bitwise vertical representation of the database and adopts a particular visit and partitioning strategy of the search space based on an original theoretical framework, which formalizes the problem of closed itemsets mining in detail. The algorithm adopts several optimizations aimed to save both space and time in computing itemset closures and their supports. In particular, since one of the main problems in this type of algorithms is the multiple generation of the same closed itemset, we propose a new effective and memory-efficient pruning technique, which, unlike other previous proposals, does not require the whole set of closed patterns mined so far to be kept in the main memory. This technique also permits each visited partition of the search space to be mined independently in any order and, thus, also in parallel. The tests conducted on many publicly available data sets show that our algorithm is scalable and outperforms other state-of-the-art algorithms like Closet+ and FP-Close, in some cases by more than one order of magnitude. More importantly, the performance improvements become more and more significant as the support threshold is decreased.

#index 843875
#* A New Complicated-Knowledge Representation Approach Based on Knowledge Meshes
#@ Hong-Sen Yan
#t 2006
#c 7
#% 275021
#% 442977
#% 442978
#% 443090
#% 443223
#% 443272
#% 443288
#% 472996
#% 618586
#% 733628
#% 1112387
#% 1860656
#! This paper presents a new complicated-knowledge representation method for the self-reconfiguration of complex systems such as complex software systems, complex manufacturing systems, and knowledgeable manufacturing systems. Herein, new concepts of a knowledge mesh (KM) and an agent mesh (AM) are proposed along with a new KM-based approach to complicated-knowledge representation. KM is the representation of such complicated macroknowledge as an advanced manufacturing mode, focusing on knowledge about the structure, functions, and information flows of an advanced manufacturing system. The multiple set, KM, and the mapping relationships between both, are then formally defined. The union, intersection, and minus operations on the multiple sets are proposed, and their properties proved. Then, the perfectness of a KM, the redundancy set between the two KMs, and the multiple redundancy set on the redundancy set are defined. Three examples are provided to illustrate the concepts of the KM, multiple set, multiple redundancy set, and logical operations. On the basis of the above, the KM-based inference engine is presented. In logical operations on KMs, each KM is taken as an operand. A new KM obtained by operations on KM multiple sets can be mapped into an AM for automatic reconfiguration of complex software systems. Finally, the combination of two real management modes is exemplified for the effective application of the new KM-based method to the self-reconfiguration of complex systems. It is worth mentioning that KM multiple sets can also be taken as a new formal representation of software systems if their corresponding AMs are the real software systems.

#index 843876
#* Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem
#@ Zhi-Hua Zhou;Xu-Ying Liu
#t 2006
#c 7
#% 5182
#% 92148
#% 169684
#% 280437
#% 310519
#% 342611
#% 424997
#% 443509
#% 451221
#% 458361
#% 466561
#% 466568
#% 476887
#% 520431
#% 577290
#% 765520
#% 769875
#% 998622
#% 1271973
#% 1272369
#% 1289281
#! This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both oversampling and undersampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and soft-ensemble, i.e., the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCI data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that cost-sensitive learning with multiclass tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multiclass tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training cost-sensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may, in fact, only be effective on learning with imbalanced two-class data sets.

#index 843877
#* Range Nearest-Neighbor Query
#@ Haibo Hu;Dik Lun Lee
#t 2006
#c 7
#% 86950
#% 201876
#% 227939
#% 248797
#% 287466
#% 342657
#% 427199
#% 443329
#% 443397
#% 462239
#% 464195
#% 479649
#% 480093
#% 480132
#% 480632
#% 481947
#% 527187
#% 654481
#% 878303
#% 993955
#! A range nearest-neighbor (RNN) query retrieves the nearest neighbor (NN) for every point in a range. It is a natural generalization of point and continuous nearest-neighbor queries and has many applications. In this paper, we consider the ranges as (hyper)rectangles and propose efficient in-memory processing and secondary memory pruning techniques for RNN queries in both 2D and high-dimensional spaces. These techniques are generalized for kRNN queries, which return the k nearest neighbors for every point in the range. In addition, we devise an auxiliary solution-based index EXO-tree to speed up any type of NN query. EXO-tree is orthogonal to any existing NN processing algorithm and, thus, can be transparently integrated. An extensive empirical study was conducted to evaluate the CPU and I/O performance of these techniques, and the study showed that they are efficient and robust under various data sets, query ranges, numbers of nearest neighbors, dimensions, and cache sizes.

#index 843878
#* Random Projection-Based Multiplicative Data Perturbation for Privacy Preserving Distributed Data Mining
#@ Kun Liu;Hillol Kargupta;Jessica Ryan
#t 2006
#c 7
#% 1868
#% 67453
#% 176172
#% 300184
#% 313975
#% 428404
#% 482071
#% 575967
#% 575969
#% 576111
#% 576761
#% 577233
#% 577289
#% 586838
#% 593928
#% 635215
#% 727904
#% 727929
#% 740764
#% 742048
#% 743280
#% 785340
#% 785414
#% 800513
#% 857387
#% 1068712
#% 1707132
#% 1757799
#! This paper explores the possibility of using multiplicative random projection matrices for privacy preserving distributed data mining. It specifically considers the problem of computing statistical aggregates like the inner product matrix, correlation coefficient matrix, and Euclidean distance matrix from distributed privacy sensitive data possibly owned by multiple parties. This class of problems is directly related to many other data-mining problems such as clustering, principal component analysis, and classification. This paper makes primary contributions on two different grounds. First, it explores Independent Component Analysis as a possible tool for breaching privacy in deterministic multiplicative perturbation-based models such as random orthogonal transformation and random rotation. Then, it proposes an approximate random projection-based technique to improve the level of privacy protection while still preserving certain statistical characteristics of the data. The paper presents extensive theoretical analysis and experimental results. Experiments demonstrate that the proposed technique is effective and can be successfully used for different types of privacy-preserving data mining applications.

#index 843879
#* Link Contexts in Classifier-Guided Topical Crawlers
#@ Gautam Pant;Padmini Srinivasan
#t 2006
#c 7
#% 176502
#% 190581
#% 232708
#% 268073
#% 268079
#% 268106
#% 269218
#% 281251
#% 290830
#% 309145
#% 311040
#% 330599
#% 340924
#% 340928
#% 348138
#% 348178
#% 406493
#% 420495
#% 431559
#% 449016
#% 480309
#% 577255
#% 662756
#% 717133
#% 729437
#% 760838
#% 760839
#% 803035
#% 835231
#% 840583
#! Context of a hyperlink or link context is defined as the terms that appear in the text around a hyperlink within a Web page. Link contexts have been applied to a variety of Web information retrieval and categorization tasks. Topical or focused Web crawlers have a special reliance on link contexts. These crawlers automatically navigate the hyperlinked structure of the Web while using link contexts to predict the benefit of following the corresponding hyperlinks with respect to some initiating topic or theme. Using topical crawlers that are guided by a Support Vector Machine, we investigate the effects of various definitions of link contexts on the crawling performance. We find that a crawler that exploits words both in the immediate vicinity of a hyperlink as well as the entire parent page performs significantly better than a crawler that depends on just one of those cues. Also, we find that a crawler that uses the tag tree hierarchy within Web pages provides effective coverage. We analyze our results along various dimensions such as link context quality, topic difficulty, length of crawl, training data, and topic domain. The study was done using multiple crawls over 100 topics covering millions of pages allowing us to derive statistically strong results.

#index 843880
#* MALLET-A Multi-Agent Logic Language for Encoding Teamwork
#@ Xiaocong Fan;John Yen;Michael Miller;Thomas R. Ioerger;Richard Volz
#t 2006
#c 7
#% 23011
#% 68239
#% 189698
#% 214197
#% 215532
#% 234819
#% 241019
#% 302099
#% 314845
#% 379079
#% 445079
#% 557382
#% 643130
#% 643133
#% 740267
#% 823960
#% 855466
#% 880238
#% 1272316
#% 1289304
#% 1704214
#! MALLET, a Multi-Agent Logic Language for Encoding Teamwork, is intended to enable expression of teamwork emulating human teamwork, allowing experimentation with different levels and forms of inferred team intelligence. A consequence of this goal is that the actual teamwork behavior is determined by the level of intelligence built into the underlying system as well as the semantics of the language. In this paper, we give the design objectives, the syntax, and an operational semantics for MALLET in terms of a transition system. We show how the semantics can be used to reason about the behaviors of team-based agents. The semantics can also be used to guide the implementation of various MALLET interpreters emulating different forms of team intelligence, as well as formally study the properties of team-based agents specified in MALLET. We have explored various forms of proactive information exchange behavior embodied in human teamwork using the CAST system, which implements a built-in MALLET interpreter.

#index 845217
#* Distance-Based Detection and Prediction of Outliers
#@ Fabrizio Angiulli;Stefano Basta;Clara Pizzuti
#t 2006
#c 7
#% 201259
#% 252304
#% 300136
#% 300183
#% 342625
#% 342641
#% 408396
#% 420064
#% 466086
#% 479791
#% 479986
#% 570886
#% 729912
#% 789012
#% 818916
#! A distance-based outlier detection method that finds the top outliers in an unlabeled data set and provides a subset of it, called outlier detection solving set, that can be used to predict the outlierness of new unseen objects, is proposed. The solving set includes a sufficient number of points that permits the detection of the top outliers by considering only a subset of all the pairwise distances from the data set. The properties of the solving set are investigated, and algorithms for computing it, with subquadratic time requirements, are proposed. Experiments on synthetic and real data sets to evaluate the effectiveness of the approach are presented. A scaling analysis of the solving set size is performed, and the false positive rate, that is, the fraction of new objects misclassified as outliers using the solving set instead of the overall data set, is shown to be negligible. Finally, to investigate the accuracy in separating outliers from inliers, ROC analysis of the method is accomplished. Results obtained show that using the solving set instead of the data set guarantees a comparable quality of the prediction, but at a lower computational cost.

#index 845218
#* An Integrated Framework for Visualized and Exploratory Pattern Discovery in Mixed Data
#@ Chung-Chian Hsu;Sheng-Hsuan Wang
#t 2006
#c 7
#% 23408
#% 36672
#% 136350
#% 163766
#% 200996
#% 216499
#% 234978
#% 238650
#% 243728
#% 296738
#% 347770
#% 376266
#% 385564
#% 413618
#% 420081
#% 452747
#% 618434
#% 818916
#% 1788040
#% 1860651
#% 1860652
#! Data mining uncovers hidden, previously unknown, and potentially useful information from large amounts of data. Compared to the traditional statistical and machine learning data analysis techniques, data mining emphasizes providing a convenient and complete environment for the data analysis. In this paper, we propose an integrated framework for visualized, exploratory data clustering, and pattern extraction from mixed data. We further discuss its implementation techniques: a generalized self-organizing map (GSOM) and an extended attribute-oriented induction (EAOI), which not only overcome the drawbacks of their original algorithms, but also provide additional analysis capabilities. Specifically, the GSOM facilitates the direct handling of mixed data, including categorical and numeric values. The EAOI enables exploration for major values hidden in the data and, in addition, offers an alternative for processing numeric attributes, instead of generalizing them. A prototype was developed for experiments with synthetic and real data sets, and comparison with those of the traditional approaches. The results confirmed the feasibility of the framework and the superiority of the extended techniques.

#index 845219
#* A Scalable Hybrid Approach for Extracting Head Components from Web Tables
#@ Sung-Won Jung;Hyuk-Chul Kwon
#t 2006
#c 7
#% 290482
#% 348147
#% 516423
#% 755816
#! We have established a preprocessing method for determining the meaningfulness of a table to allow for information extraction from tables on the Internet. A table offers a preeminent clue in text mining because it contains meaningful data displayed in rows and columns. However, tables are used on the Internet for both knowledge structuring and document design. Therefore, we were interested in determining whether or not a table has meaningfulness that is related to the structural information provided at the abstraction level of the table head. Accordingly, we: 1) investigated the types of tables present in HTML documents, 2) established the features that distinguished meaningful tables from others, 3) constructed a training data set using the established features after having filtered any obvious decorative tables, and 4) constructed a classification model using a decision tree. Based on these features, we set up heuristics for table head extraction from meaningful tables, and obtained an F-measure of 95.0 percent in distinguishing meaningful tables from decorative tables and an accuracy of 82.1 percent in extracting the table head from the meaningful tables.

#index 845220
#* Integrating K-Means Clustering with a Relational DBMS Using SQL
#@ Carlos Ordonez
#t 2006
#c 7
#% 210173
#% 216500
#% 248813
#% 252367
#% 278011
#% 280463
#% 280521
#% 300131
#% 300213
#% 320942
#% 342704
#% 413619
#% 413620
#% 420076
#% 420101
#% 430881
#% 466497
#% 654445
#% 662751
#% 662754
#% 766665
#% 771926
#% 1015363
#% 1051482
#! Integrating data mining algorithms with a relational DBMS is an important problem for database programmers. We introduce three SQL implementations of the popular K-means clustering algorithm to integrate it with a relational DBMS: 1) a straightforward translation of K-means computations into SQL, 2) an optimized version based on improved data organization, efficient indexing, sufficient statistics, and rewritten queries, and 3) an incremental version that uses the optimized version as a building block with fast convergence and automated reseeding. We experimentally show the proposed K-means implementations work correctly and can cluster large data sets. We identify which K-means computations are more critical for performance. The optimized and incremental K-means implementations exhibit linear scalability. We compare K-means implementations in SQL and C++ with respect to speed and scalability and we also study the time to export data sets outside of the DBMS. Experiments show that SQL overhead is significant for small data sets, but relatively low for large data sets, whereas export times become a bottleneck for C++.

#index 845221
#* On Characterization and Discovery of Minimal Unexpected Patterns in Rule Discovery
#@ Balaji Padmanabhan;Alexander Tuzhilin
#t 2006
#c 7
#% 152934
#% 210160
#% 227917
#% 248791
#% 280433
#% 280436
#% 304319
#% 310494
#% 310496
#% 443092
#% 443356
#% 443427
#% 464873
#% 466646
#% 478302
#% 478770
#% 479785
#% 501204
#% 502132
#% 536291
#% 631970
#% 729934
#% 769913
#% 1499588
#! A drawback of traditional data-mining methods is that they do not leverage prior knowledge of users. In prior work, we proposed a method that could discover unexpected patterns in data by using domain knowledge in a systematic manner. In this paper, we present new methods for discovering a minimal set of unexpected patterns by combining the two independent concepts of minimality and unexpectedness, both of which have been well-studied in the KDD literature. We demonstrate the strengths of this approach experimentally using a case study in a marketing domain.

#index 845222
#* An Integrated Data Preparation Scheme for Neural Network Data Analysis
#@ Lean Yu;Shouyang Wang;K. K. Lai
#t 2006
#c 7
#% 17144
#% 61477
#% 116374
#% 119374
#% 132583
#% 132775
#% 157722
#% 211794
#% 211942
#% 223781
#% 232102
#% 269634
#% 280854
#% 314709
#% 369236
#% 380352
#% 382342
#% 393316
#% 495275
#% 501187
#% 603818
#% 643637
#% 679304
#% 774664
#% 795636
#% 1478826
#! Data preparation is an important and critical step in neural network modeling for complex data analysis and it has a huge impact on the success of a wide variety of complex data analysis tasks, such as data mining and knowledge discovery. Although data preparation in neural network data analysis is important, some existing literature about the neural network data preparation are scattered, and there is no systematic study about data preparation for neural network data analysis. In this study, we first propose an integrated data preparation scheme as a systematic study for neural network data analysis. In the integrated scheme, a survey of data preparation, focusing on problems with the data and corresponding processing techniques, is then provided. Meantime, some intelligent data preparation solution to some important issues and dilemmas with the integrated scheme are discussed in detail. Subsequently, a cost-benefit analysis framework for this integrated scheme is presented to analyze the effect of data preparation on complex data analysis. Finally, a typical example of complex data analysis from the financial domain is provided in order to show the application of data preparation techniques and to demonstrate the impact of data preparation on complex data analysis.

#index 845223
#* A Basic Mathematical Framework for Conceptual Graphs
#@ Philip H. P. Nguyen;Dan Corbett
#t 2006
#c 7
#% 2298
#% 156337
#% 285540
#% 384416
#% 465341
#% 465655
#% 466147
#% 477661
#% 644903
#% 682443
#% 735938
#% 1271976
#% 1289175
#% 1289178
#! Based on the original idea of Sowa on conceptual graph and a recent formalism by Corbett on ontology, this paper presents a rigorous mathematization of basic concepts encountered in the Conceptual Structure Theory, including canon, ontology, conceptual graph, projection, and canonical formation operations, with the aim of deriving their mathematical properties and applying them to future research and development on knowledge representation. Our proposed formalism enhances the Conceptual Structure Theory and enables it to compare favorably with other alternative methods such as the Formal Concept Analysis theory.

#index 845224
#* Generalized Dimension-Reduction Framework for Recent-Biased Time Series Analysis
#@ Yanchang Zhao;Shichao Zhang
#t 2006
#c 7
#% 310580
#% 312054
#% 480132
#% 480146
#% 578400
#% 631923
#% 632089
#% 662757
#% 745513
#% 765136
#% 993958
#% 1015261
#! Recent-biased approximations have received increased attention recently as a mechanism for learning trend patterns from time series or data streams. They have shown promise for clustering time series and incrementally pattern maintaining. In this paper, we design a generalized dimension-reduction framework for recent-biased approximations, aiming at making traditional dimension-reduction techniques actionable in recent-biased time series analysis. The framework is designed in two ways: equi-segmented scheme and vari-segmented scheme. In both schemes, time series data are first partitioned into segments and a dimension-reduction technique is applied to each segment. Then, more coefficients are kept for more recent data while fewer kept for older data. Thus, more details are preserved for recent data and fewer coefficients are kept for the whole time series, which improves the efficiency greatly. We experimentally evaluate the proposed approach, and demonstrate that traditional dimension-reduction techniques, such as SVD, DFT, DWT, PIP, PAA, and landmarks, can be embedded into our framework for recent-biased approximations over streaming time series.

#index 845225
#* Transform-Space View: Performing Spatial Join in the Transform Space Using Original-Space Indexes
#@ Min-Jae Lee;Kyu-Young Whang;Wook-Shin Han;Il-Yeol Song
#t 2006
#c 7
#% 13032
#% 13041
#% 68091
#% 86950
#% 86951
#% 152937
#% 172909
#% 210186
#% 210187
#% 252304
#% 252608
#% 287466
#% 427199
#% 443303
#% 452852
#% 462617
#% 479453
#% 479472
#% 481956
#% 526864
#% 565447
#% 566113
#% 617872
#% 717163
#% 860913
#! Spatial joins find all pairs of objects that satisfy a given spatial relationship. In spatial joins using indexes, original-space indexes such as the R-tree are widely used. An original-space index is the one that indexes objects as represented in the original space. Since original-space indexes deal with extents of objects, it is relatively complex to optimize join algorithms using these indexes. On the other hand, transform-space indexes, which transform objects in the original space into points in the transform space and index them, deal only with points but no extents. Thus, optimization of join algorithms using these indexes can be relatively simple. However, the disadvantage of these join algorithms is that they cannot be applied to original-space indexes such as the R-tree. In this paper, we present a novel mechanism for achieving the best of these two types of algorithms. Specifically, we propose the new notion of the transform-space view and present the transform-space view join algorithm. The transform-space view is a virtual transform-space index based on an original-space index. It allows us to "interpret” or "view” an existing original-space index as a transform-space index with no space and negligible time overhead and without actually modifying the structure of the original-space index or changing object representation. The transform-space view join algorithm joins two original-space indexes in the transform space through the notion of the transform-space view. Through analysis and experiments, we verify the excellence of the transform-space view join algorithm. The transform-space view join algorithm always outperforms existing ones for all the data sets tested in terms of all three measures used: the one-pass buffer size (the minimum buffer size required for guaranteeing one disk access per page), the number of disk accesses for a given buffer size, and the wall clock time. Thus, it constitutes a lower-bound algorithm. We believe that the proposed transform-space view can be applied to developing various new spatial query processing algorithms in the transform space.

#index 845226
#* WebGuard: A Web Filtering Engine Combining Textual, Structural, and Visual Content-Based Analysis
#@ Mohamed Hammami;Youssef Chahir;Liming Chen
#t 2006
#c 7
#% 90661
#% 136350
#% 248810
#% 268079
#% 268087
#% 299537
#% 310514
#% 348178
#% 430761
#% 445567
#% 449588
#% 724616
#% 1857402
#! Along with the ever-growing Web comes the proliferation of objectionable content, such as sex, violence, racism, etc. We need efficient tools for classifying and filtering undesirable Web content. In this paper, we investigate this problem and describe WebGuard, an automatic machine learning-based pornographic Web site classification and filtering system. Unlike most commercial filtering products, which are mainly based on textual content-based analysis such as indicative keywords detection or manually collected black list checking, WebGuard relies on several major data mining techniques associated with textual, structural content-based analysis, and skin color related visual content-based analysis as well. Experiments conducted on a testbed of 400 Web sites including 200 adult sites and 200 nonpornographic ones showed WebGuard's filtering effectiveness, reaching a 97.4 percent classification accuracy rate when textual and structural content-based analysis was combined with visual content-based analysis. Further experiments on a black list of 12,311 adult Web sites manually collected and classified by the French Ministry of Education showed that WebGuard scored a 95.62 percent classification accuracy rate. The basic framework of WebGuard can apply to other categorization problems of Web sites which combine, as most of them do today, textual and visual content.

#index 849810
#* BORDER: Efficient Computation of Boundary Points
#@ Chenyi Xia;Wynne Hsu;Mong Li Lee;Beng Chin Ooi
#t 2006
#c 7
#% 86950
#% 135968
#% 232102
#% 296738
#% 300136
#% 300163
#% 316709
#% 333929
#% 342828
#% 427199
#% 438137
#% 465000
#% 465009
#% 480307
#% 480661
#% 730019
#% 783643
#% 993999
#% 994027
#% 1016191
#% 1016192
#! This work addresses the problem of finding boundary points in multidimensional data sets. Boundary points are data points that are located at the margin of densely distributed data such as a cluster. We describe a novel approach called BORDER (a BOundaRy points DEtectoR) to detect such points. BORDER employs the state-of-the-art database technique—the Gorder kNN join and makes use of the special property of the reverse k nearest neighbor (RkNN). Experimental studies on data sets with varying characteristics indicate that BORDER is able to detect the boundary points effectively and efficiently.

#index 849811
#* Enhancing Data Analysis with Noise Removal
#@ Hui Xiong;Gaurav Pandey;Michael Steinbach;Vipin Kumar
#t 2006
#c 7
#% 36672
#% 152934
#% 194285
#% 201889
#% 210173
#% 242235
#% 242237
#% 243728
#% 248790
#% 252304
#% 252836
#% 280404
#% 300136
#% 300183
#% 301169
#% 310546
#% 375017
#% 420072
#% 420078
#% 478624
#% 479799
#% 480496
#% 570886
#% 727897
#% 729912
#% 729939
#% 751575
#% 781774
#% 835018
#! Removing objects that are noise is an important goal of data cleaning as noise hinders most types of data analysis. Most existing data cleaning methods focus on removing noise that is the product of low-level data errors that result from an imperfect data collection process, but data objects that are irrelevant or only weakly relevant can also significantly hinder data analysis. Thus, if the goal is to enhance the data analysis as much as possible, these objects should also be considered as noise, at least with respect to the underlying analysis. Consequently, there is a need for data cleaning techniques that remove both types of noise. Because data sets can contain large amounts of noise, these techniques also need to be able to discard a potentially large fraction of the data. This paper explores four techniques intended for noise removal to enhance data analysis in the presence of high noise levels. Three of these methods are based on traditional outlier detection techniques: distance-based, clustering-based, and an approach based on the Local Outlier Factor (LOF) of an object. The other technique, which is a new method that we are proposing, is a hyperclique-based data cleaner (HCleaner). These techniques are evaluated in terms of their impact on the subsequent data analysis, specifically, clustering and association analysis. Our experimental results show that all of these methods can provide better clustering performance and higher quality association patterns as the amount of noise being removed increases, although HCleaner generally leads to better clustering performance and higher quality associations than the other three methods for binary data.

#index 849812
#* Effective and Efficient Dimensionality Reduction for Large-Scale and Streaming Data Preprocessing
#@ Jun Yan;Benyu Zhang;Ning Liu;Shuicheng Yan;Qiansheng Cheng;Weiguo Fan;Qiang Yang;Wensi Xi;Zheng Chen
#t 2006
#c 7
#% 46803
#% 169720
#% 187271
#% 243727
#% 243728
#% 269218
#% 324288
#% 367469
#% 387427
#% 465754
#% 577283
#% 627808
#% 719278
#% 729437
#% 763708
#% 769964
#% 817843
#% 818217
#% 873823
#% 1828410
#! Dimensionality reduction is an essential data preprocessing technique for large-scale and streaming data classification tasks. It can be used to improve both the efficiency and the effectiveness of classifiers. Traditional dimensionality reduction approaches fall into two categories: Feature Extraction and Feature Selection. Techniques in the feature extraction category are typically more effective than those in feature selection category. However, they may break down when processing large-scale data sets or data streams due to their high computational complexities. Similarly, the solutions provided by the feature selection approaches are mostly solved by greedy strategies and, hence, are not ensured to be optimal according to optimized criteria. In this paper, we give an overview of the popularly used feature extraction and selection algorithms under a unified framework. Moreover, we propose two novel dimensionality reduction algorithms based on the Orthogonal Centroid algorithm (OC). The first is an Incremental OC (IOC) algorithm for feature extraction. The second algorithm is an Orthogonal Centroid Feature Selection (OCFS) method which can provide optimal solutions according to the OC criterion. Both are designed under the same optimization criterion. Experiments on Reuters Corpus Volume-1 data set and some public large-scale text data sets indicate that the two algorithms are favorable in terms of their effectiveness and efficiency when compared with other state-of-the-art algorithms.

#index 849813
#* Learning Object Models from Semistructured Web Documents
#@ Shiren Ye;Tat-Seng Chua
#t 2006
#c 7
#% 170418
#% 259991
#% 261694
#% 266218
#% 280085
#% 312860
#% 334020
#% 348180
#% 350914
#% 445448
#% 577281
#% 577321
#% 654469
#% 660272
#% 729939
#% 729978
#% 731607
#% 743856
#% 754078
#% 779959
#% 853712
#% 854663
#% 1250181
#% 1860761
#! This paper presents an automated approach to learning object models by means of useful object data extracted from data-intensive semistructured web documents such as product descriptions. Modeling intensive data on the Web involves the following three phrases: First, we identify the object region covering the descriptions of object data when irrelevant contents from the web documents are excluded. Second, we partition the contents of different object data appearing in the object region and construct object data using hierarchical XML outputs. Third, we induce the abstract object model from the analogous object data. This model will match the corresponding object data from a Web site more precisely and comprehensively than the existing handcrafted ontologies. The main contribution of this study is in developing a fully automated approach to extract object data and object model from semistructured web documents using kernel-based matching and View Syntax interpretation. Our system, OnModer, can automatically construct object data and induce object models from complicated web documents, such as the technical descriptions of personal computers and digital cameras downloaded from manufacturers' and vendors' sites. A comparison with the available hand-crafted ontologies and tests on an open corpus demonstrate that our framework is effective in extracting meaningful and comprehensive models.

#index 849814
#* Toward Efficient Multifeature Query Processing
#@ H. V. Jagadish;Beng Chin Ooi;Heng Tao Shen;Kian-Lee Tan
#t 2006
#c 7
#% 248796
#% 252304
#% 299978
#% 333854
#% 342828
#% 397376
#% 479462
#% 479649
#% 480133
#% 480307
#% 480330
#% 480632
#% 571050
#% 632035
#% 745496
#! In many advanced applications, data are described by multiple high-dimensional features. Moreover, different queries may weight these features differently; some may not even specify all the features. In this paper, we propose our solution to support efficient query processing in these applications. We devise a novel representation that compactly captures f features into two components: The first component is a 2D vector that reflects a distance range (minimum and maximum values) of the f features with respect to a reference point (the center of the space) in a metric space and the second component is a bit signature, with two bits per dimension, obtained by analyzing each feature's descending energy histogram. This representation enables two levels of filtering: The first component prunes away points that do not share similar distance ranges, while the bit signature filters away points based on the dimensions of the relevant features. Moreover, the representation facilitates the use of a single index structure to further speed up processing. We employ the classical B^+{\hbox{-}}\rm tree for this purpose. We also propose a KNN search algorithm that exploits the access orders of critical dimensions of highly selective features and partial distances to prune the search space more effectively. Our extensive experiments on both real-life and synthetic data sets show that the proposed solution offers significant performance advantages over sequential scan and retrieval methods using single and multiple VA-files.

#index 849815
#* Optimizing Cyclic Join View Maintenance over Distributed Data Sources
#@ Bin Liu;Elke A. Rundensteiner
#t 2006
#c 7
#% 13016
#% 13018
#% 36117
#% 201928
#% 210210
#% 227947
#% 248795
#% 273911
#% 273918
#% 300141
#% 330305
#% 334043
#% 340300
#% 342700
#% 408396
#% 411554
#% 413556
#% 443065
#% 479452
#% 571217
#% 632085
#% 791180
#% 800501
#% 800565
#! Materialized views defined over distributed data sources are critical for many applications to ensure efficient access, reliable performance, and high availability. Materialized views need to be maintained upon source updates since stale view extents may not serve well or may even mislead user applications. Thus, view maintenance performance is one of the keys to the success of these applications. In this work, we investigate two maintenance strategies, extended batching and view graph transformation, for maintaining general join views where join conditions may exist between any pairs of data sources possibly with cycles. Many choices are available for maintaining cyclic join views. We thus propose a cost-driven view maintenance framework which generates optimized maintenance plans tuned to the environmental settings. The proposed framework has been implemented in the TxnWrap system. Experimental studies illustrate that our proposed optimization techniques significantly improve the view maintenance performance in a distributed environment.

#index 849816
#* Maintaining Sliding Window Skylines on Data Streams
#@ Yufei Tao;Dimitris Papadias
#t 2006
#c 7
#% 37861
#% 86950
#% 213975
#% 235114
#% 287466
#% 288976
#% 333854
#% 333926
#% 333940
#% 348603
#% 397352
#% 427199
#% 464987
#% 465167
#% 480671
#% 578392
#% 654444
#% 654463
#% 654480
#% 993954
#% 1015296
#% 1015324
#% 1016156
#! The skyline of a multidimensional data set contains the "best” tuples according to any preference function that is monotonic on each dimension. Although skyline computation has received considerable attention in conventional databases, the existing algorithms are inapplicable to stream applications because 1) they assume static data that are stored in the disk (rather than continuously arriving/expiring), 2) they focus on "one-time” execution that returns a single skyline (in contrast to constantly tracking skyline changes), and 3) they aim at reducing the I/O overhead (as opposed to minimizing the CPU-cost and main-memory consumption). This paper studies skyline computation in stream environments, where query processing takes into account only a "sliding window” covering the most recent tuples. We propose algorithms that continuously monitor the incoming data and maintain the skyline incrementally. Our techniques utilize several interesting properties of stream skylines to improve space/time efficiency by expunging data from the system as early as possible (i.e., before their expiration). Furthermore, we analyze the asymptotical performance of the proposed solutions, and evaluate their efficiency with extensive experiments.

#index 849817
#* An Error-Resilient and Tunable Distributed Indexing Scheme for Wireless Data Broadcast
#@ Jianliang Xu;Wang-Chien Lee;Xueyan Tang;Qing Gao;Shanping Li
#t 2006
#c 7
#% 169835
#% 201897
#% 247246
#% 264636
#% 341892
#% 430426
#% 442626
#% 443127
#% 452850
#% 452851
#% 479961
#% 631861
#% 632027
#% 635989
#% 721141
#% 729623
#% 730648
#% 755203
#% 1768468
#! Access efficiency and energy conservation are two critical performance concerns in a wireless data broadcast system. We propose in this paper a novel parameterized index called the exponential index that has a linear yet distributed structure for wireless data broadcast. Based on two tuning knobs, index base and chunk size, the exponential index can be tuned to optimize the access latency with the tuning time bounded by a given limit, and vice versa. The client access algorithm for the exponential index under unreliable broadcast is described. A performance analysis of the exponential index is provided. Extensive ns-2-based simulation experiments are conducted to evaluate the performance under various link error probabilities. Simulation results show that the exponential index substantially outperforms the state-of-the-art indexes. In particular, it is more resilient to link errors and achieves more performance advantages from index caching. The results also demonstrate its great flexibility in trading access latency with tuning time.

#index 849818
#* Multicampaign Assignment Problem
#@ Yong-Hyuk Kim;Byung-Ro Moon
#t 2006
#c 7
#% 36672
#% 124010
#% 173879
#% 202011
#% 220711
#% 280447
#% 280852
#% 301590
#% 314977
#% 388826
#% 393792
#% 408518
#% 443082
#% 566440
#% 630973
#% 644560
#% 734590
#% 734593
#% 767656
#! It is crucial to maximize targeting efficiency and customer satisfaction in personalized marketing. State-of-the-art techniques for targeting focus on the optimization of individual campaigns. Our motivation is the belief that the effectiveness of a campaign with respect to a customer is affected by how many precedent campaigns have been recently delivered to the customer. We raise the multiple recommendation problem, which occurs when performing several personalized campaigns simultaneously. We formulate the multicampaign assignment problem to solve this issue and propose algorithms for the problem. The algorithms include dynamic programming and efficient heuristic methods. We verify by experiments the effectiveness of the problem formulation and the proposed algorithms.

#index 849819
#* Combining Feature Reduction and Case Selection in Building CBR Classifiers
#@ Yan Li;Simon C. K. Shiu;Sankar K. Pal
#t 2006
#c 7
#% 92555
#% 168280
#% 190581
#% 263169
#% 329647
#% 345824
#% 366687
#% 399587
#% 420138
#% 494448
#% 737974
#% 742814
#% 780571
#% 1012881
#% 1290056
#% 1787935
#! CBR systems that are built for the classification problems are called CBR classifiers. This paper presents a novel and fast approach to building efficient and competent CBR classifiers that combines both feature reduction (FR) and case selection (CS). It has three central contributions: 1) it develops a fast rough-set method based on relative attribute dependency among features to compute the approximate reduct, 2) it constructs and compares different case selection methods based on the similarity measure and the concepts of case coverage and case reachability, and 3) CBR classifiers built using a combination of the FR and CS processes can reduce the training burden as well as the need to acquire domain knowledge. The overall experimental results demonstrating on four real-life data sets show that the combined FR and CS method can preserve, and may also improve, the solution accuracy while at the same time substantially reducing the storage space. The case retrieval time is also greatly reduced because the use of CBR classifier contains a smaller amount of cases with fewer features. The developed FR and CS combination method is also compared with the kernel PCA and SVMs techniques. Their storage requirement, classification accuracy, and classification speed are presented and discussed.

#index 863382
#* q-Gram Matching Using Tree Models
#@ Prahlad Fogla;Wenke Lee
#t 2006
#c 7
#% 5524
#% 8391
#% 69506
#% 120649
#% 121278
#% 216781
#% 269546
#% 289010
#% 320454
#% 321327
#% 333679
#% 414987
#% 480384
#% 546254
#% 547438
#% 664547
#% 768815
#% 1116726
#! q{\hbox{-}}\rm gram matching is used for approximate substring matching problems in a wide range of application areas, including intrusion detection. In this paper, we present a tree-based model to perform fast linear time q{\hbox{-}}{\rm gram} matching. All q{\hbox{-}}{\rm grams} present in the text are stored in a tree structure similar to Trie. We use a tree redundancy pruning algorithm to reduce the size of the tree without losing any information. We also use suffix links for fast q{\hbox{-}}{\rm gram} search during query matching. We compare our work with the Rabin-Karp-based hash-table technique, commonly used for multiple q{\hbox{-}}{\rm gram} search. We present results of experiments on system call sequence data used for intrusion detection.

#index 863383
#* Multitype Features Coselection for Web Document Clustering
#@ Shen Huang;Zheng Chen;Yong Yu;Wei-Ying Ma
#t 2006
#c 7
#% 67565
#% 115608
#% 211526
#% 243727
#% 252011
#% 280404
#% 296375
#% 316509
#% 344447
#% 465754
#% 643009
#% 715945
#% 1835183
#! Feature selection has been widely applied in text categorization and clustering. Compared to unsupervised selection, supervised feature selection is more successful in filtering out noise in most cases. However, due to a lack of label information, clustering can hardly exploit supervised selection. Some studies have proposed to solve this problem by "pseudoclass.” As empirical results show, this method is sensitive to selection criteria and data sets. In this paper, we propose a novel feature coselection for Web document clustering, which is called Multitype Features Coselection for Clustering (MFCC). MFCC uses intermediate clustering results in one type of feature space to help the selection in other types of feature spaces. Our experiments show that for most selection criteria, MFCC reduces effectively the noise introduced by "pseudoclass,” and further improves clustering performance.

#index 863384
#* On Optimal Rule Discovery
#@ Jiuyong Li
#t 2006
#c 7
#% 136350
#% 152934
#% 210162
#% 227917
#% 280436
#% 420112
#% 452822
#% 452846
#% 458178
#% 751575
#% 772329
#% 799029
#% 823641
#% 1272179
#! In machine learning and data mining, heuristic and association rules are two dominant schemes for rule discovery. Heuristic rule discovery usually produces a small set of accurate rules, but fails to find many globally optimal rules. Association rule discovery generates all rules satisfying some constraints, but yields too many rules and is infeasible when the minimum support is small. Here, we present a unified framework for the discovery of a family of optimal rule sets and characterize the relationships with other rule-discovery schemes such as nonredundant association rule discovery. We theoretically and empirically show that optimal rule discovery is significantly more efficient than association rule discovery independent of data structure and implementation. Optimal rule discovery is an efficient alternative to association rule discovery, especially when the minimum support is low.

#index 863385
#* A Transaction Mapping Algorithm for Frequent Itemsets Mining
#@ Mingjun Song;Sanguthevar Rajasekaran
#t 2006
#c 7
#% 152934
#% 248791
#% 300120
#% 300124
#% 329598
#% 465003
#% 466490
#% 481290
#% 729942
#! In this paper, we present a novel algorithm for mining complete frequent itemsets. This algorithm is referred to as the TM (Transaction Mapping) algorithm from hereon. In this algorithm, transaction ids of each itemset are mapped and compressed to continuous transaction intervals in a different space and the counting of itemsets is performed by intersecting these interval lists in a depth-first order along the lexicographic tree. When the compression coefficient becomes smaller than the average number of comparisons for intervals intersection at a certain level, the algorithm switches to transaction id intersection. We have evaluated the algorithm against two popular frequent itemset mining algorithms, FP-growth and dEclat, using a variety of data sets with short and long frequent patterns. Experimental data show that the TM algorithm outperforms these two algorithms.

#index 863386
#* A Unifying Framework for Detecting Outliers and Change Points from Time Series
#@ Jun-ichi Takeuchi;Kenji Yamanishi
#t 2006
#c 7
#% 115608
#% 280408
#% 280413
#% 342641
#% 477809
#% 479791
#% 577295
#% 632090
#% 749406
#% 1808676
#! We are concerned with the issue of detecting outliers and change points from time series. In the area of data mining, there have been increased interest in these issues since outlier detection is related to fraud detection, rare event discovery, etc., while change-point detection is related to event/trend change detection, activity monitoring, etc. Although, in most previous work, outlier detection and change point detection have not been related explicitly, this paper presents a unifying framework for dealing with both of them. In this framework, a probabilistic model of time series is incrementally learned using an online discounting learning algorithm, which can track a drifting data source adaptively by forgetting out-of-date statistics gradually. A score for any given data is calculated in terms of its deviation from the learned model, with a higher score indicating a high possibility of being an outlier. By taking an average of the scores over a window of a fixed length and sliding the window, we may obtain a new time series consisting of moving-averaged scores. Change point detection is then reduced to the issue of detecting outliers in that time series. We compare the performance of our framework with those of conventional methods to demonstrate its validity through simulation and experimental applications to incidents detection in network security.

#index 863387
#* TAPER: A Two-Step Approach for All-Strong-Pairs Correlation Query in Large Databases
#@ Hui Xiong;Shashi Shekhar;Pang-Ning Tan;Vipin Kumar
#t 2006
#c 7
#% 152934
#% 227919
#% 274146
#% 280487
#% 300120
#% 342597
#% 342667
#% 375017
#% 420112
#% 443466
#% 464603
#% 465003
#% 466669
#% 580588
#% 632028
#% 632029
#% 729971
#% 765455
#% 769909
#! Given a user-specified minimum correlation threshold \theta and a market-basket database with N items and T transactions, an all-strong-pairs correlation query finds all item pairs with correlations above the threshold \theta. However, when the number of items and transactions are large, the computation cost of this query can be very high. The goal of this paper is to provide computationally efficient algorithms to answer the all-strong-pairs correlation query. Indeed, we identify an upper bound of Pearson's correlation coefficient for binary variables. This upper bound is not only much cheaper to compute than Pearson's correlation coefficient, but also exhibits special monotone properties which allow pruning of many item pairs even without computing their upper bounds. A Two-step All-strong-Pairs corElation queRy (TAPER) algorithm is proposed to exploit these properties in a filter-and-refine manner. Furthermore, we provide an algebraic cost model which shows that the computation savings from pruning is independent of or improves when the number of items is increased in data sets with Zipf-like or linear rank-support distributions. Experimental results from synthetic and real-world data sets exhibit similar trends and show that the TAPER algorithm can be an order of magnitude faster than brute-force alternatives. Finally, we demonstrate that the algorithmic ideas developed in the TAPER algorithm can be extended to efficiently compute negative correlation and uncentered Pearson's correlation coefficient.

#index 863388
#* A Unified Log-Based Relevance Feedback Scheme for Image Retrieval
#@ Steven C. H. Hoi;Michael R. Lyu;Rong Jin
#t 2006
#c 7
#% 218978
#% 252011
#% 269218
#% 286881
#% 318785
#% 341269
#% 348155
#% 406493
#% 420077
#% 466263
#% 479788
#% 589960
#% 589961
#% 592428
#% 641976
#% 642985
#% 642989
#% 642990
#% 775257
#% 775472
#% 780689
#% 780690
#% 793245
#% 1180384
#% 1502482
#% 1775095
#% 1857498
#% 1858013
#! Relevance feedback has emerged as a powerful tool to boost the retrieval performance in content-based image retrieval (CBIR). In the past, most research efforts in this field have focused on designing effective algorithms for traditional relevance feedback. Given that a CBIR system can collect and store users' relevance feedback information in a history log, an image retrieval system should be able to take advantage of the log data of users' feedback to enhance its retrieval performance. In this paper, we propose a unified framework for log-based relevance feedback that integrates the log of feedback data into the traditional relevance feedback schemes to learn effectively the correlation between low-level image features and high-level concepts. Given the error-prone nature of log data, we present a novel learning technique, named Soft Label Support Vector Machine, to tackle the noisy data problem. Extensive experiments are designed and conducted to evaluate the proposed algorithms based on the COREL image data set. The promising experimental results validate the effectiveness of our log-based relevance feedback scheme empirically.

#index 863389
#* Keyword Proximity Search in XML Trees
#@ Vagelis Hristidis;Nick Koudas;Yannis Papakonstantinou;Divesh Srivastava
#t 2006
#c 7
#% 186
#% 268079
#% 291299
#% 333981
#% 397375
#% 465155
#% 479803
#% 498538
#% 654442
#% 659990
#% 659999
#% 660011
#% 810052
#% 993987
#% 1015258
#% 1016135
#! Recent works have shown the benefits of keyword proximity search in querying XML documents in addition to text documents. For example, given query keywords over Shakespeare's plays in XML, the user might be interested in knowing how the keywords cooccur. In this paper, we focus on XML trees and define XML keyword proximity queries to return the (possibly heterogeneous) set of minimum connecting trees (MCTs) of the matches to the individual keywords in the query. We consider efficiently executing keyword proximity queries on labeled trees (XML) in various settings: 1) when the XML database has been preprocessed and 2) when no indices are available on the XML database. We perform a detailed experimental evaluation to study the benefits of our approach and show that our algorithms considerably outperform prior algorithms and other applicable approaches.

#index 863390
#* Reverse Nearest Neighbors in Large Graphs
#@ Man Lung Yiu;Dimitris Papadias;Nikos Mamoulis;Yufei Tao
#t 2006
#c 7
#% 300163
#% 342827
#% 348615
#% 443208
#% 463583
#% 464223
#% 465009
#% 480661
#% 495433
#% 723439
#% 729850
#% 729851
#% 730019
#% 765438
#% 993999
#% 1015321
#% 1016191
#% 1016199
#! A reverse nearest neighbor (RNN) query returns the data objects that have a query point as their nearest neighbor (NN). Although such queries have been studied quite extensively in Euclidean spaces, there is no previous work in the context of large graphs. In this paper, we provide a fundamental lemma, which can be used to prune the search space while traversing the graph in search for RNN. Based on it, we develop two RNN methods; an eager algorithm that attempts to prune network nodes as soon as they are visited and a lazy technique that prunes the search space when a data point is discovered. We study retrieval of an arbitrary number k of reverse nearest neighbors, investigate the benefits of materialization, cover several query types, and deal with cases where the queries and the data objects reside on nodes or edges of the graph. The proposed techniques are evaluated in various practical scenarios involving spatial maps, computer networks, and the DBLP coauthorship graph.

#index 863391
#* On Consistent Reading of Entire Databases
#@ Kwok-Wa Lam;Victor C. S. Lee
#t 2006
#c 7
#% 9241
#% 54582
#% 286967
#% 320902
#% 442991
#! Many applications need to read an entire database in a consistent way. This global-reading of an entire database formulated as a global-read transaction (GRT) is not a trivial issue since it will cause a high degree of interference to other concurrent transactions. Conventional concurrency control protocols are obviously inadequate in handling the long-lived GRT. Previous studies [1], [3], [4] proposed additional tests, namely, the Color Test and the Shade Test, to handle conflicts between the GRT and update transactions. However, we discovered that both algorithms can bring about nonserializable schedules of transactions. We propose an enhanced algorithm directly built on the two algorithms to guarantee the serializability of transactions.

#index 863392
#* Mining Ontology for Automatically Acquiring Web User Information Needs
#@ Yuefeng Li;Ning Zhong
#t 2006
#c 7
#% 100324
#% 164547
#% 209662
#% 237052
#% 264706
#% 279755
#% 287037
#% 290728
#% 308770
#% 312861
#% 341712
#% 342655
#% 344447
#% 350914
#% 379333
#% 385946
#% 387427
#% 438669
#% 450043
#% 501320
#% 501321
#% 501323
#% 501476
#% 529190
#% 630984
#% 631914
#% 724557
#% 727825
#% 727874
#% 727913
#% 727915
#% 779877
#% 779879
#% 794508
#% 1279298
#% 1408629
#% 1861012
#! It is not easy to obtain the right information from the Web for a particular Web user or a group of users due to the obstacle of automatically acquiring Web user profiles. The current techniques do not provide satisfactory structures for mining Web user profiles. This paper presents a novel approach for this problem. The objective of the approach is to automatically discover ontologies from data sets in order to build complete concept models for Web user information needs. It also proposes a method for capturing evolving patterns to refine discovered ontologies. In addition, the process of assessing relevance in ontology is established. This paper provides both theoretical and experimental evaluations for the approach. The experimental results show that all objectives we expect for the approach are achievable.

#index 863393
#* A Framework for On-Demand Classification of Evolving Data Streams
#@ Charu C. Aggarwal;Jiawei Han;Jianyong Wang;Philip S. Yu
#t 2006
#c 7
#% 210173
#% 273900
#% 302724
#% 310500
#% 320942
#% 333926
#% 342600
#% 378388
#% 466908
#% 480628
#% 594012
#% 654489
#% 659972
#% 660003
#% 729932
#% 729965
#% 729973
#% 737351
#% 769927
#% 993958
#% 1011200
#% 1015261
#! Current models of the classification problem do not effectively handle bursts of particular classes coming in at different times. In fact, the current model of the classification problem simply concentrates on methods for one-pass classification modeling of very large data sets. Our model for data stream classification views the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and test streams are used for dynamic classification of data sets. This model reflects real-life situations effectively, since it is desirable to classify test streams in real time over an evolving training and test stream. The aim here is to create a classification system in which the training model can adapt quickly to the changes of the underlying data stream. In order to achieve this goal, we propose an on-demand classification process which can dynamically select the appropriate window of past training data to build the classifier. The empirical results indicate that the system maintains a high classification accuracy in an evolving data stream, while providing an efficient solution to the classification task.

#index 863394
#* Biclustering of Expression Data with Evolutionary Computation
#@ Federico Divina;Jesus S. Aguilar-Ruiz
#t 2006
#c 7
#% 320944
#% 347292
#% 397382
#% 397632
#% 465712
#% 466209
#% 469422
#% 490636
#% 557634
#% 589430
#% 659967
#% 727882
#% 727908
#% 729456
#% 1022783
#% 1715824
#% 1781221
#! Microarray techniques are leading to the development of sophisticated algorithms capable of extracting novel and useful knowledge from a biomedical point of view. In this work, we address the biclustering of gene expression data with evolutionary computation. Our approach is based on evolutionary algorithms, which have been proven to have excellent performance on complex problems, and searches for biclusters following a sequential covering strategy. The goal is to find biclusters of maximum size with mean squared residue lower than a given \delta. In addition, we pay special attention to the fact of looking for high-quality biclusters with large variation, i.e., with a relatively high row variance, and with a low level of overlapping among biclusters. The quality of biclusters found by our evolutionary approach is discussed and the results are compared to those reported by Cheng and Church, and Yang et al. In general, our approach, named SEBI, shows an excellent performance at finding patterns in gene expression data.

#index 863395
#* Adaptive Nonlinear Discriminant Analysis by Regularized Minimum Squared Errors
#@ Hyunsoo Kim;Barry L. Drake;Haesun Park
#t 2006
#c 7
#% 30385
#% 80995
#% 117560
#% 129292
#% 129320
#% 137131
#% 161990
#% 163186
#% 163496
#% 182235
#% 190581
#% 224113
#% 309208
#% 415834
#% 430739
#% 581716
#% 729437
#% 788663
#% 819917
#% 841687
#! Kernelized nonlinear extensions of Fisher's discriminant analysis, discriminant analysis based on generalized singular value decomposition (LDA/GSVD), and discriminant analysis based on the minimum squared error formulation (MSE) have recently been widely utilized for handling undersampled high-dimensional problems and nonlinearly separable data sets. As the data sets are modified from incorporating new data points and deleting obsolete data points, there is a need to develop efficient updating and downdating algorithms for these methods to avoid expensive recomputation of the solution from scratch. In this paper, an efficient algorithm for adaptive linear and nonlinear kernel discriminant analysis based on regularized MSE, called adaptive KDA/RMSE, is proposed. In adaptive KDA/RMSE, updating and downdating of the computationally expensive eigenvalue decomposition (EVD) or singular value decomposition (SVD) is approximated by updating and downdating of the QR decomposition achieving an order of magnitude speed up. This fast algorithm for adaptive kernelized discriminant analysis is designed by utilizing regularization techniques and the relationship between linear and nonlinear discriminant analysis and the MSE. In addition, an efficient algorithm to compute leave-one-out cross validation is also introduced by utilizing downdating of KDA/RMSE.

#index 863396
#* Pattern Discovery of Fuzzy Time Series for Financial Prediction
#@ Chiung-Hon Leon Lee;Alan Liu;Wen-Sung Chen
#t 2006
#c 7
#% 136580
#% 145308
#% 160978
#% 182919
#% 209745
#% 271661
#% 290482
#% 394745
#% 577221
#% 577275
#% 662750
#% 742814
#% 1780779
#% 1780950
#% 1860421
#% 1860828
#% 1860832
#% 1861267
#! A fuzzy time series data representation method based on the Japanese candlestick theory is proposed and used in assisting financial prediction. The Japanese candlestick theory is an empirical model of investment decision. The theory assumes that the candlestick patterns reflect the psychology of the market, and the investors can make their investment decision based on the identified candlestick patterns. We model the imprecise and vague candlestick patterns with fuzzy linguistic variables and transfer the financial time series data to fuzzy candlestick patterns for pattern recognition. A fuzzy candlestick pattern can bridge the gap between the investors and the system designer because it is visual, computable, and modifiable. The investors are not only able to understand the prediction process, but also to improve the efficiency of prediction results. The proposed approach is applied to financial time series forecasting problem for demonstration. By the prototype system which has been established, the investment expertise can be stored in the knowledge base, and the fuzzy candlestick pattern can also be identified automatically from a large amount of the financial trading data.

#index 863397
#* Test-Cost Sensitive Classification on Data with Missing Values
#@ Qiang Yang;Charles Ling;Xiaoyong Chai;Rong Pan
#t 2006
#c 7
#% 92554
#% 136350
#% 160852
#% 246831
#% 280437
#% 376266
#% 410276
#% 447606
#% 464639
#% 477640
#% 729437
#% 770791
#% 785338
#% 1272369
#% 1289281
#! In the area of cost-sensitive learning, inductive learning algorithms have been extended to handle different types of costs to better represent misclassification errors. Most of the previous works have only focused on how to deal with misclassification costs. In this paper, we address the equally important issue of how to handle the test costs associated with querying the missing values in a test case. When an attribute contains a missing value in a test case, it may or may not be worthwhile to take the extra effort in order to obtain a value for that attribute, or attributes, depending on how much benefit the new value will bring about in increasing the accuracy. In this paper, we consider how to integrate test-cost-sensitive learning with the handling of missing values in a unified framework that includes model building and a testing strategy. The testing strategies determine which attributes to perform the test on in order to minimize the sum of the classification costs and test costs. We show how to instantiate this framework in two popular machine learning algorithms: decision trees and naive Bayesian method. We empirically evaluate the test-cost-sensitive methods for handling missing values on several data sets.

#index 863398
#* Sample-Based Quality Estimation of Query Results in Relational Database Environments
#@ Donald P. Ballou;InduShobha N. Chengalur-Smith;Richard Y. Wang
#t 2006
#c 7
#% 3047
#% 54047
#% 227883
#% 234805
#% 242235
#% 273909
#% 273910
#% 289370
#% 300195
#% 315012
#% 402500
#% 426852
#% 565353
#% 778322
#% 798509
#% 912094
#! The quality of data in relational databases is often uncertain, and the relationship between the quality of the underlying base tables and the set of potential query results, a type of information product (IP), that could be produced from them has not been fully investigated. This paper provides a basis for the systematic analysis of the quality of such IPs. This research uses the relational algebra framework to develop estimates for the quality of query results based on the quality estimates of samples taken from the base tables. Our procedure requires an initial sample from the base tables; these samples are then used for all possible information IPs. Each specific query governs the quality assessment of the relevant samples. By using the same sample repeatedly, our approach is relatively cost effective. We introduce the Reference-Table Procedure, which can be used for quality estimation in general. In addition, for each of the basic algebraic operators, we discuss simpler procedures that may be applicable. Special attention is devoted to the Join operation. We examine various, relevant statistical issues, including how to deal with the impact on quality of missing rows in base tables. Finally, we address several implementation issues related to sampling.

#index 863399
#* Processing Moving Queries over Moving Objects Using Motion-Adaptive Indexes
#@ Bugra Gedik;Kun-Lung Wu;Philip S. Yu;Ling Liu
#t 2006
#c 7
#% 116082
#% 252304
#% 273706
#% 295512
#% 300174
#% 354745
#% 442615
#% 443298
#% 443327
#% 458853
#% 480473
#% 487371
#% 495433
#% 500896
#% 527187
#% 564133
#% 576115
#% 765453
#% 765454
#% 783533
#% 800571
#% 800572
#% 837273
#% 975260
#% 993955
#% 1015320
#% 1016193
#! This paper describes a motion-adaptive indexing scheme for efficient evaluation of moving continual queries (MCQs) over moving objects. It uses the concept of motion-sensitive bounding boxes (MSBs) to model moving objects and moving queries. These bounding boxes automatically adapt their sizes to the dynamic motion behaviors of individual objects. Instead of indexing frequently changing object positions, we index less frequently changing object and query MSBs, where updates to the bounding boxes are needed only when objects and queries move across the boundaries of their boxes. This helps decrease the number of updates to the indexes. More importantly, we use predictive query results to optimistically precalculate query results, decreasing the number of searches on the indexes. Motion-sensitive bounding boxes are used to incrementally update the predictive query results. Furthermore, we introduce the concepts of guaranteed safe radius and optimistic safe radius to extend our motion-adaptive indexing scheme to evaluating moving continual k\hbox{-}{\rm nearest\ neighbor\ }(k\rm NN) queries. Our experiments show that the proposed motion-adaptive indexing scheme is efficient for the evaluation of both moving continual range queries and moving continual k\rm NN queries.

#index 863400
#* Approximate Processing of Massive Continuous Quantile Queries over High-Speed Data Streams
#@ Xuemin Lin;Jian Xu;Qing Zhang;Hongjun Lu;Jeffrey Xu Yu;Xiaofang Zhou;Yidong Yuan
#t 2006
#c 7
#% 2115
#% 70370
#% 210190
#% 248820
#% 273907
#% 288976
#% 300179
#% 319179
#% 333931
#% 338425
#% 378388
#% 397354
#% 397369
#% 443298
#% 481775
#% 576105
#% 654476
#% 654488
#% 660004
#% 745533
#% 800494
#% 801696
#% 816392
#% 993958
#% 993960
#% 993961
#% 993969
#% 1015278
#% 1015279
#% 1015293
#! Quantile computation has many applications including data mining and financial data analysis. It has been shown that an \epsilon{\hbox{-}}{\rm{approximate}} summary can be maintained so that, given a quantile query (\phi, \epsilon), the data item at rank \lceil \phi N \rceil may be approximately obtained within the rank error precision \epsilon N over all N data items in a data stream or in a sliding window. However, scalable online processing of massive continuous quantile queries with different \phi and \epsilon poses a new challenge because the summary is continuously updated with new arrivals of data items. In this paper, first we aim to dramatically reduce the number of distinct query results by grouping a set of different queries into a cluster so that they can be processed virtually as a single query while the precision requirements from users can be retained. Second, we aim to minimize the total query processing costs. Efficient algorithms are developed to minimize the total number of times for reprocessing clusters and to produce the minimum number of clusters, respectively. The techniques are extended to maintain near-optimal clustering when queries are registered and removed in an arbitrary fashion against whole data streams or sliding windows. In addition to theoretical analysis, our performance study indicates that the proposed techniques are indeed scalable with respect to the number of input queries as well as the number of items and the item arrival rate in a data stream.

#index 863401
#* Design and Evaluation of a Scalable and Reliable P2P Assisted Proxy for On-Demand Streaming Media Delivery
#@ Lei Guo;Songqing Chen;Xiaodong Zhang
#t 2006
#c 7
#% 309835
#% 311791
#% 330579
#% 337170
#% 340176
#% 345950
#% 345964
#% 348037
#% 399549
#% 401983
#% 470588
#% 578119
#% 578132
#% 578133
#% 615087
#% 730123
#% 745416
#% 745418
#% 805891
#% 812812
#% 963846
#% 1717268
#% 1849755
#% 1849756
#! To efficiently deliver streaming media, researchers have developed technical solutions that fall into three categories, each of which has its merits and limitations. Infrastructure-based CDNs with dedicated network bandwidths and hardware supports can provide high-quality streaming services, but at a high cost. Server-based proxies are cost-effective but not scalable due to the limited proxy capacity in storage and bandwidth, and its centralized control also brings a single point of failure. Client-based P2P networks are scalable, but do not guarantee high-quality streaming service due to the transient nature of peers. To address these limitations, we present a novel and efficient design of a scalable and reliable media proxy system assisted by P2P networks, called PROP. In the PROP system, the clients' machines in an intranet are self-organized into a structured P2P system to provide a large media storage and to actively participate in the streaming media delivery, where the proxy is also embedded as an important member to ensure the quality of streaming service. The coordination and collaboration in the system are efficiently conducted by our P2P management structure and replacement policies. Our system has the following merits: 1) It addresses both the scalability problem in centralized proxy systems and the unreliable service concern by only relying on the P2P sharing of clients. 2) The proposed content locating scheme can timely serve the demanded media data and fairly dispatch media streaming tasks in appropriate granularity across the system. 3) Based on the modeling and analysis, we propose global replacement policies for proxy and clients, which well balance the demand and supply of streaming data in the system, achieving a high utilization of peers' cache. We have comparatively evaluated our system through trace-driven simulations with synthetic workloads and with a real-life workload extracted from the media server logs in an enterprise network, which shows our design significantly improves the quality of media streaming and the system scalability.

#index 863402
#* Rights Protection for Discrete Numeric Streams
#@ Radu Sion;Mikhail Atallah;Sunil Prabhakar
#t 2006
#c 7
#% 191721
#% 332279
#% 378388
#% 379445
#% 402405
#% 643919
#% 654449
#% 726623
#% 743059
#% 745531
#% 932139
#% 933808
#% 993944
#% 993948
#% 993949
#% 993999
#% 1016190
#! Today's world of increasingly dynamic environments naturally results in more and more data being available as fast streams. Applications such as stock market analysis, environmental sensing, Web clicks, and intrusion detection are just a few of the examples where valuable data is streamed. Often, streaming information is offered on the basis of a nonexclusive, single-use customer license. One major concern, especially given the digital nature of the valuable stream, is the ability to easily record and potentially "replay” parts of it in the future. If there is value associated with such future replays, it could constitute enough incentive for a malicious customer (Mallory) to record and duplicate data segments, subsequently reselling them for profit. Being able to protect against such infringements becomes a necessity. In this work, we introduce the issue of rights protection for discrete streaming data through watermarking. This is a novel problem with many associated challenges including: operating in a finite window, single-pass, (possibly) high-speed streaming model, and surviving natural domain specific transforms and attacks (e.g., extreme sparse sampling and summarizations), while at the same time keeping data alterations within allowable bounds. We propose a solution and analyze its resilience to various types of attacks as well as some of the important expected domain-specific transforms, such as sampling and summarization. We implement a proof of concept software (wms.*) and perform experiments on real sensor data from the NASA Infrared Telescope Facility at the University of Hawaii, to assess encoding resilience levels in practice. Our solution proves to be well suited for this new domain. For example, we can recover an over 97 percent confidence watermark from a highly down-sampled (e.g., less than 8 percent) stream or survive stream summarization (e.g., 20 percent) and random alteration attacks with very high confidence levels, often above 99 percent.

#index 863403
#* A Fuzzy Approach to Partitioning Continuous Attributes for Classification
#@ Wai-Ho Au;Keith C. C. Chan;Andrew K. C. Wong
#t 2006
#c 7
#% 35065
#% 136350
#% 366958
#% 369236
#% 374537
#% 391311
#% 443880
#% 998568
#% 1010687
#! Classification is an important topic in data mining research. To better handle continuous data, fuzzy sets are used to represent interval events in the domains of continuous attributes, allowing continuous data lying on the interval boundaries to partially belong to multiple intervals. Since the membership functions of fuzzy sets can profoundly affect the performance of the models or rules discovered, the determination of membership functions or fuzzy partitioning is crucial. In this paper, we present a new method to determine the membership functions of fuzzy sets directly from data to maximize the class-attribute interdependence and, hence, improve the classification results. In other words, it forms a fuzzy partition of the input space automatically, using an information-theoretic measure to evaluate the interdependence between the class membership and an attribute as the objective function for fuzzy partitioning. To find the optimum of the measure, it employs fractional programming. To evaluate the effectiveness of the proposed method, several real-world data sets are used in our experiments. The experimental results show that this method outperforms other well-known discretization and fuzzy partitioning approaches.

#index 865730
#* Closed Constrained Gradient Mining in Retail Databases
#@ Jianyong Wang;Jiawei Han;Jian Pei
#t 2006
#c 7
#% 248785
#% 300120
#% 464989
#% 480630
#% 580588
#% 629612
#% 729933
#! Incorporating constraints into frequent itemset mining not only improves data mining efficiency, but also leads to concise and meaningful results. In this paper, a framework for closed constrained gradient itemset mining in retail databases is proposed by introducing the concept of gradient constraint into closed itemset mining. A tailored version of CLOSET+, LCLOSET, is first briefly introduced, which is designed for efficient closed itemset mining from sparse databases. Then, a newly proposed weaker but antimonotone measure, {\rm{top}}{\hbox{-}}X average measure, is proposed and can be adopted to prune search space effectively. Experiments show that a combination of LCLOSET and the {\rm{top}}{\hbox{-}}X average pruning provides an efficient approach to mining frequent closed gradient itemsets.

#index 865731
#* Fast Discovery and the Generalization of Strong Jumping Emerging Patterns for Building Compact and Accurate Classifiers
#@ Hongjian Fan;Kotagiri Ramamohanarao
#t 2006
#c 7
#% 92533
#% 136350
#% 216504
#% 232117
#% 280409
#% 290482
#% 300120
#% 310550
#% 316709
#% 342610
#% 361100
#% 376266
#% 379331
#% 380342
#% 400847
#% 411390
#% 449588
#% 466426
#% 466490
#% 466653
#% 477957
#% 480940
#% 501982
#% 564401
#% 754410
#! Classification of large data sets is an important data mining problem that has wide applications. Jumping Emerging Patterns (JEPs) are those itemsets whose supports increase abruptly from zero in one data set to nonzero in another data set. In this paper, we propose a fast, accurate, and less complex classifier based on a subset of JEPs, called Strong Jumping Emerging Patterns (SJEPs). The support constraint of SJEP removes potentially less useful JEPs while retaining those with high discriminating power. Previous algorithms based on the manipulation of border [1] as well as consEPMiner [2] cannot directly mine SJEPs. Here, we present a new tree-based algorithm for their efficient discovery. Experimental results show that: 1) the training of our classifier is typically 10 times faster than earlier approaches, 2) our classifier uses much fewer patterns than the JEP-Classifier [3] to achieve a similar (and, often, improved) accuracy, and 3) in many cases, it is superior to other state-of-the-art classification systems such as Naive Bayes, CBA, C4.5, and bagged and boosted versions of C4.5. We argue that SJEPs are high-quality patterns which possess the most differentiating power. As a consequence, they represent sufficient information for the construction of accurate classifiers. In addition, we generalize these patterns by introducing Noise-tolerant Emerging Patterns (NEPs) and Generalized Noise-tolerant Emerging Patterns (GNEPs). Our tree-based algorithms can be adopted to easily discover these variations. We experimentally demonstrate that SJEPs, NEPs, and GNEPs are extremely useful for building effective classifiers that can deal well with noise.

#index 865732
#* Dynamic Grouping Strategies Based on a Conceptual Graph for Cooperative Learning
#@ BinShyan Jong;YuLung Wu;TeYi Chan
#t 2006
#c 7
#% 106413
#% 106425
#% 465334
#% 569476
#% 589943
#% 665621
#% 1800830
#% 1801073
#% 1801574
#% 1850739
#! In large classrooms, teachers rarely have time to monitor the status of individual students. As a result, students who learn quickly thoroughly grasp the content, while students who learn slowly fall further and further behind until, in some cases, the education system gives up on them completely. Teachers thus need a system to help them obtain the status of all students and manage particular students. This study proposes a novel learning process based on the conceptual graph, a knowledge representation tool. This study adopts cooperative learning to let students conduct further studies through interaction with each other. The proposed strategy acquires and measures the knowledge structure of students. According to the knowledge structure of individual students, this study proposes dynamic-grouping and partial-regrouping to identify suitable partners for students. The proposed strategy achieves the best complementary groups for further learning stages. Evaluation results indicate that the proposed method significantly improves the learning achievement of all learners. Additionally, the questionnaire results indicate that learners respond positively to the proposed grouping strategy.

#index 865733
#* KDX: An Indexer for Support Vector Machines
#@ Navneet Panda;Edward Y. Chang
#t 2006
#c 7
#% 86950
#% 190581
#% 197394
#% 227939
#% 269213
#% 328374
#% 333929
#% 341269
#% 425048
#% 435141
#% 458379
#% 466887
#% 479462
#% 479649
#% 479973
#% 481956
#% 592108
#% 794857
#% 1858012
#% 1860547
#! Support Vector Machines (SVMs) have been adopted by many data mining and information-retrieval applications for learning a mining or query concept, and then retrieving the "{\rm{top}}{\hbox{-}}k” best matches to the concept. However, when the data set is large, naively scanning the entire data set to find the top matches is not scalable. In this work, we propose a kernel indexing strategy to substantially prune the search space and, thus, improve the performance of {\rm{top}}{\hbox{-}}k queries. Our kernel indexer (KDX) takes advantage of the underlying geometric properties and quickly converges on an approximate set of {\rm{top}}{\hbox{-}}k instances of interest. More importantly, once the kernel (e.g., Gaussian kernel) has been selected and the indexer has been constructed, the indexer can work with different kernel-parameter settings (e.g., \gamma and \sigma) without performance compromise. Through theoretical analysis and empirical studies on a wide variety of data sets, we demonstrate KDX to be very effective. An earlier version of this paper appeared in the 2005 SIAM International Conference on Data Mining [24]. This version differs from the previous submission in providing a detailed cost analysis under different scenarios, specifically designed to meet the varying needs of accuracy, speed, and space requirements, developing an approach for insertion and deletion of instances, presenting the specific computations as well as the geometric properties used in performing the same, and providing detailed algorithms for each of the operations necessary to create and use the index structure.

#index 865734
#* Efficient Classification across Multiple Database Relations: A CrossMine Approach
#@ Xiaoxin Yin;Jiawei Han;Jiong Yang;Philip S. Yu
#t 2006
#c 7
#% 99396
#% 136350
#% 376266
#% 393907
#% 396021
#% 420077
#% 420089
#% 452864
#% 458257
#% 466073
#% 479787
#% 501209
#% 729982
#% 1271968
#% 1289267
#! Relational databases are the most popular repository for structured data, and is thus one of the richest sources of knowledge in the world. In a relational database, multiple relations are linked together via entity-relationship links. Multirelational classification is the procedure of building a classifier based on information stored in multiple relations and making predictions with it. Existing approaches of Inductive Logic Programming (recently, also known as Relational Mining) have proven effective with high accuracy in multirelational classification. Unfortunately, most of them suffer from scalability problems with regard to the number of relations in databases. In this paper, we propose a new approach, called CrossMine, which includes a set of novel and powerful methods for multirelational classification, including 1) tuple ID propagation, an efficient and flexible method for virtually joining relations, which enables convenient search among different relations, 2) new definitions for predicates and decision-tree nodes, which involve aggregated information to provide essential statistics for classification, and 3) a selective sampling method for improving scalability with regard to the number of tuples. Based on these techniques, we propose two scalable and accurate methods for multirelational classification: CrossMine-Rule, a rule-based method and CrossMine-Tree, a decision-tree-based method. Our comprehensive experiments on both real and synthetic data sets demonstrate the high scalability and accuracy of the CrossMine approach.

#index 865735
#* An Exact Closed-Form Formula for d-Dimensional Quadtree Decomposition of Arbitrary Hyperrectangles
#@ Shyh-Kwei Chen
#t 2006
#c 7
#% 42091
#% 68091
#% 118213
#% 252304
#% 319508
#% 328481
#% 333938
#% 373953
#% 407995
#% 427125
#% 427199
#% 435137
#% 443128
#% 445701
#% 564133
#% 740766
#% 753434
#% 783479
#% 1725058
#! In this paper, we solve the classic problem of computing the average number of decomposed quadtree blocks (quadrants, nodes, or pieces) in quadtree decomposition for an arbitrary hyperrectangle aligned with the axes. We derive a closed-form formula for general cases. The previously known state-of-the-art solution provided a closed-form solution for special cases and utilized these formulas to derive linearly interpolated formulas for general cases individually. However, there is no exact and uniform closed-form formula that fits all cases. Contrary to the top-down counting approach used by most prior solutions, we employ a bottom-up enumeration approach to transform the problem into one that involves the Cartesian product of d multisets of successive 2's powers. Classic combinatorial enumeration techniques are applied to obtain an exact and uniform closed-form formula. The result is of theoretical interest since it is the first exact closed-form formula for arbitrary cases. Practically, it is nice to have a uniform formula for estimating the average number since a simple program can be conveniently constructed taking side lengths as inputs.

#index 865736
#* Modeling and Computing Ternary Projective Relations between Regions
#@ Eliseo Clementini;Roland Billen
#t 2006
#c 7
#% 26133
#% 39656
#% 243705
#% 260653
#% 318487
#% 320074
#% 322378
#% 362928
#% 424591
#% 481331
#% 487212
#% 489422
#% 495787
#% 518723
#% 526995
#% 549078
#% 661295
#% 1478754
#% 1671781
#! Current spatial database systems offer limited querying capabilities beyond binary topological relations. This paper introduces a model for projective relations between regions to support other qualitative spatial queries. The relations are ternary because they are based on the collinearity invariant of three points under projective geometry. The model is built on a partition of the plane into separate zones that are obtained from projective properties of two reference objects: Then, by considering the empty/nonempty intersections of a primary object with these zones, the model is able to distinguish between 34 different projective relations. Then, the paper proposes original algorithms for computing the relations under the assumption that regions of the plane are stored as vector polygons in a spatial database. These algorithms run in optimal O(n \log n) time.

#index 865737
#* A Generic Library of Problem Solving Methods for Scheduling Applications
#@ Dnyanesh G. Rajpathak;Enrico Motta;Zdenek Zdrahal;Rajkumar Roy
#t 2006
#c 7
#% 2084
#% 11720
#% 22486
#% 37909
#% 48702
#% 55936
#% 76459
#% 87035
#% 110011
#% 111054
#% 116575
#% 125634
#% 130000
#% 150866
#% 156337
#% 205176
#% 225468
#% 230386
#% 258933
#% 262696
#% 270816
#% 289332
#% 295830
#% 319244
#% 320603
#% 354383
#% 362942
#% 370526
#% 408396
#% 418892
#% 443456
#% 444996
#% 459120
#% 459129
#% 497785
#% 723404
#! In this paper, we propose a generic library of problem-solving methods for scheduling applications. Although some attempts have been made in the past at developing the libraries of scheduling problem-solvers, these only provide limited coverage. Many lack generality, as they subscribe to a particular scheduling domain. Others simply implement a particular problem-solving technique, which may be applicable only to a subset of the space of scheduling problems. In addition, most of these libraries fail to provide the required degree of depth and precision. In our approach, we subscribe to the Task-Method-Domain-Application knowledge modeling framework which provides a structured organization for the different components of the library. At the task level, we construct a generic scheduling task ontology to formalize the space of scheduling problems. At the method level, we construct a generic problem-solving model of scheduling that generalizes from the variety of approaches to scheduling problem-solving, which can be found in the literature. The generic nature of this model is demonstrated by constructing seven methods for scheduling as an alternative specialization of the model. Finally, we validated our library on a number of applications to demonstrate its generic nature and effective support for developing scheduling applications.

#index 865738
#* Fuzzy Metagraph and Its Combination with the Indexing Approach in Rule-Based Systems
#@ Zheng-Hua Tan
#t 2006
#c 7
#% 34874
#% 188440
#% 201073
#% 230023
#% 261205
#% 279003
#% 290703
#% 365250
#% 387427
#% 441055
#% 442657
#% 442719
#% 443115
#% 444881
#% 452870
#% 595075
#% 608449
#% 1780612
#% 1780693
#% 1788184
#% 1788291
#! This paper presents a graph-theoretic construct called a fuzzy metagraph (FM) with the capability of describing the relationships between sets of fuzzy elements instead of only single fuzzy elements. The algebraic structure of FM and its properties are extensively investigated. Subsequently, the FM construct is applied to rule-based systems. First, we propose FM-based knowledge representation in both graphic and algebraic format. The representation is capable of identifying dependencies across compound propositions in the rules. In the algebraic representation, the FM closure matrix is considered a precompiled rule base enabling efficient query processing. An iterative approach is presented to facilitate the construction and expansion of the FM closure matrix, which is a key for real-world applications. Next, we introduce the concept of indexing, which was originally developed for information retrieval (IR), to enable an immediate extraction of relevant entries from the FM closure matrix. The indexing approach is applied in combination with the FM closure matrix. Based on the combination, corresponding inference mechanisms are introduced to achieve instant acquisition of relevant rules over a large collection of rules. The application in rule-based systems indicates that the combination of FM and IR techniques offers advantages for the mathematical analysis of systems.

#index 865739
#* Automatic Fuzzy Ontology Generation for Semantic Web
#@ Quan Thanh Tho;Siu Cheung Hui;A. C. M. Fong;Tru Hoang Cao
#t 2006
#c 7
#% 76921
#% 197530
#% 341660
#% 346636
#% 375017
#% 384416
#% 429873
#% 445448
#% 451052
#% 452449
#% 459510
#% 463744
#% 466150
#% 538873
#% 637736
#% 642999
#% 744036
#% 756964
#% 920714
#% 1289178
#! Ontology is an effective conceptualism commonly used for the Semantic Web. Fuzzy logic can be incorporated to ontology to represent uncertainty information. Typically, fuzzy ontology is generated from a predefined concept hierarchy. However, to construct a concept hierarchy for a certain domain can be a difficult and tedious task. To tackle this problem, this paper proposes the FOGA (Fuzzy Ontology Generation frAmework) for automatic generation of fuzzy ontology on uncertainty information. The FOGA framework comprises the following components: Fuzzy Formal Concept Analysis, Concept Hierarchy Generation, and Fuzzy Ontology Generation. We also discuss approximating reasoning for incremental enrichment of the ontology with new upcoming data. Finally, a fuzzy-based technique for integrating other attributes of database to the ontology is proposed.

#index 865740
#* Improved Word-Aligned Binary Compression for Text Indexing
#@ Vo Ngoc Anh;Alistair Moffat
#t 2006
#c 7
#% 212665
#% 290703
#% 311799
#% 340886
#% 397151
#% 453323
#% 786632
#% 1739409
#! We present an improved compression mechanism for handling the compressed inverted indexes used in text retrieval systems, extending the word-aligned binary coding carry method. Experiments using two typical document collections show that the new method obtains superior compression to previous static codes, without penalty in terms of execution speed.

#index 871023
#* Using Emerging Patterns to Construct Weighted Decision Trees
#@ Hamad Alhammady;Kotagiri Ramamohanarao
#t 2006
#c 7
#% 272995
#% 280409
#% 290482
#% 466483
#% 501982
#% 546047
#% 607791
#% 722357
#% 785370
#! Decision trees (DTs) represent one of the most important and popular solutions to the problem of classification. They have been shown to have excellent performance in the field of data mining and machine learning. However, the problem of DTs is that they are built using data instances assigned to crisp classes. In this paper, we generalize decision trees so that they can take into account weighted classes assigned to the training data instances. Moreover, we propose a novel method for discovering weights for the training instances. Our method is based on emerging patterns (EPs). EPs are those itemsets whose supports (probabilities) in one class are significantly higher than their supports (probabilities) in the other classes. Our experimental evaluation shows that the new proposed method has good performance and excellent noise tolerance.

#index 871024
#* Power-Efficient Access-Point Selection for Indoor Location Estimation
#@ Yiqiang Chen;Qiang Yang;Jie Yin;Xiaoyong Chai
#t 2006
#c 7
#% 220544
#% 245076
#% 247246
#% 259658
#% 376266
#% 401172
#% 608160
#% 613334
#% 613383
#% 625623
#% 642363
#% 729437
#% 746038
#% 1113048
#% 1250210
#! An important goal of indoor location estimation systems is to increase the estimation accuracy while reducing the power consumption. In this paper, we present a novel algorithm known as CaDet for power-efficient location estimation by intelligently selecting the number of Access Points (APs) used for location estimation. We show that by employing machine learning techniques, CaDet is able to use a small subset of the APs in the environment to detect a client's location with high accuracy. CaDet uses a combination of information theory, clustering analysis, and a decision tree algorithm. By collecting data and testing our algorithms in a realistic WLAN environment in the computer science department area of the Hong Kong University of Science and Technology, we show that CaDet (Clustering and Decision Tree-based method) can be much higher in accuracy as compared to other methods. We also show through experiments that, by intelligently selecting APs, we are able to save the power on the client device while achieving the same level of accuracy.

#index 871025
#* Enhancing Knowledge Discovery via Association-Based Evolution of Neural Logic Networks
#@ Henry W. K. Chia;Chew Lim Tan;Sam Y. Sung
#t 2006
#c 7
#% 152934
#% 204433
#% 362240
#% 445365
#% 683563
#% 1248873
#% 1289258
#% 1777130
#! The comprehensibility aspect of rule discovery is of emerging interest in the realm of knowledge discovery in databases. Of the many cognitive and psychological factors relating the comprehensibility of knowledge, we focus on the use of human amenable concepts as a representation language in expressing classification rules. Existing work in neural logic networks (or neulonets) provides impetus for our research; its strength lies in its ability to learn and represent complex human logic in decision-making using symbolic-interpretable net rules. A novel technique is developed for neulonet learning by composing net rules using genetic programming. Coupled with a sequential covering approach for generating a list of neulonets, the straightforward extraction of human-like logic rules from each neulonet provides an alternate perspective to the greater extent of knowledge that can potentially be expressed and discovered, while the entire list of neulonets together constitute an effective classifier. We show how the sequential covering approach is analogous to association-based classification, leading to the development of an association-based neulonet classifier. Empirical study shows that associative classification integrated with the genetic construction of neulonets performs better than general association-based classifiers in terms of higher accuracies and smaller rule sets. This is due to the richness in logic expression inherent in the neulonet learning paradigm.

#index 871026
#* Comparing Subspace Clusterings
#@ Anne Patrikainen;Marina Meila
#t 2006
#c 7
#% 248792
#% 280417
#% 300131
#% 397384
#% 415678
#% 469422
#% 659967
#% 722902
#% 729918
#% 763861
#% 765439
#% 765518
#% 765549
#% 769928
#% 769947
#% 769949
#% 769957
#% 774875
#% 774878
#% 778729
#% 785360
#% 799752
#% 800529
#% 800530
#% 801683
#% 840907
#% 856734
#! We present the first framework for comparing subspace clusterings. We propose several distance measures for subspace clusterings, including generalizations of well-known distance measures for ordinary clusterings. We describe a set of important properties for any measure for comparing subspace clusterings and give a systematic comparison of our proposed measures in terms of these properties. We validate the usefulness of our subspace clustering distance measures by comparing clusterings produced by the algorithms FastDOC, HARP, PROCLUS, ORCLUS, and SSPC. We show that our distance measures can be also used to compare partial clusterings, overlapping clusterings, and patterns in binary data matrices.

#index 871027
#* Presto Authorization: A Bitmap Indexing Scheme for High-Speed Access Control to XML Documents
#@ Jong P. Yoon
#t 2006
#c 7
#% 84807
#% 128261
#% 248814
#% 273905
#% 275836
#% 331770
#% 340827
#% 342328
#% 344222
#% 344639
#% 429213
#% 450560
#% 569760
#% 664665
#% 717115
#% 725290
#% 726512
#% 742563
#% 765442
#% 765447
#! XML (eXtensible Markup Language) is fast becoming the de facto standard for information exchange over the Internet. As more and more sensitive information gets stored in the form of XML, proper access control to the XML documents becomes increasingly important. However, traditional access control methodologies that have been adapted for XML documents do not address the performance issue of access control. This paper proposes a bitmap-indexing scheme in which access control decisions can be sped up. Authorization policies of the form (subject, object, and action) are encoded as bitmaps in the same manner as XML document indexes are constructed. These two are then efficiently pipelined and manipulated for "fast” access control and "secure” retrieval of XML documents.

#index 871028
#* Semantic Segment Extraction and Matching for Internet FAQ Retrieval
#@ Chung-Hsien Wu;Jui-Feng Yeh;Yu-Sheng Lai
#t 2006
#c 7
#% 197538
#% 342398
#% 342961
#% 387427
#% 391322
#% 443476
#% 474643
#% 570627
#% 641768
#% 641963
#% 641976
#% 676170
#% 723392
#% 729622
#% 740763
#% 809937
#% 854908
#% 939836
#! This investigation presents a novel approach to semantic segment extraction and matching for retrieving information from Internet FAQs with natural language queries. Two semantic segments, the question category segment (QS) and the keyword segment (KS), are extracted from the input queries and the FAQ questions with a semiautomatically derived question-semantic grammar. A semantic matching method is presented to estimate the similarity between the semantic segments of the query and the questions in the FAQ collection. Additionally, the vector space model (VSM) is adopted to measure the similarity between the query and the answers of the QA pairs. Finally, a multistage ranking strategy is adopted to determine the optimally performing combination of similarity metrics. The experimental results illustrate that the proposed method achieves an average rank of 4.52 and a top-10 recall rate of 90.89 percent. Compared with the query-expansion method, this method improves the performance by 4.82 places in the average rank of correct answers, 25.34 percent in the top-5 recall rate, and 5.21 percent in the top-10 recall rate.

#index 871029
#* A Self-Organizing Computing Network for Decision-Making in Data Sets with a Diversity of Data Types
#@ QingXiang Wu;Martin McGinnity;David A. Bell;Girijesh Prasad
#t 2006
#c 7
#% 413060
#% 426630
#% 443880
#% 733623
#% 761280
#% 1272280
#% 1861005
#! A self-organizing computing network based on concepts of fuzzy conditions, beliefs, probabilities, and neural networks is proposed for decision-making in intelligent systems which are required to handle data sets with a diversity of data types. A sense-function with a sense-range and fuzzy edges is defined as a transfer function for connections from the input layer to the hidden layer in the network. By generating hidden cells and adjusting the parameters of the sense-functions, the network self-organizes and adapts to a training set. Computing cells in the input layer are designed as data converters so that the network can deal with both symbolic data and numeric data. Hidden computing cells in the network can be explained via fuzzy rules in a similar manner to those in fuzzy neural networks. The values in the output layer can be explained as a belief distribution over a decision space. The final decision is made by means of the winner-take-all rule. The approach was applied to a series of the benchmark data sets with a diversity of data types and comparative results obtained. Based on these results, the suitability of a range of data types for processing by different intelligent techniques was analyzed, and the results show that the proposed approach is better than other approaches for decision-making in information systems with mixed data types.

#index 871030
#* Replica Placement Algorithms for Mobile Transaction Systems
#@ Manghui Tu;Peng Li;Liangliang Xiao;I-Ling Yen;Farokh B. Bastani
#t 2006
#c 7
#% 210179
#% 225006
#% 237196
#% 245017
#% 249933
#% 309139
#% 330305
#% 342712
#% 379133
#% 433319
#% 442620
#% 442621
#% 442622
#% 443430
#% 636007
#% 660971
#% 720827
#% 783716
#! In distributed mobile systems, communication cost and disconnections are major concerns. In this paper, we address replica placement issues to achieve improved performance for systems supporting mobile transactions. We focus on handling correlated data objects and disconnections. Frequently, requests and/or transactions issued by mobile clients may access multiple data objects and should be considered together in terms of replica allocation. We discuss the replication cost model for correlated data objects and show that the problem of finding an optimal solution is NP. We further adjust the replication cost model for disconnections. A heuristic "expansion-shrinking” algorithm is developed to efficiently make replica placement decisions. The algorithm obtains near optimal solutions for the correlated data model and yields significant performance gains when disconnection is considered. Experimental studies show that the heuristic expansion-shrinking algorithm significantly outperforms the general frequency-based replication schemes.

#index 871031
#* Boosting an Associative Classifier
#@ Yanmin Sun;Yang Wang;Andrew K. C. Wong
#t 2006
#c 7
#% 235377
#% 280409
#% 300120
#% 302391
#% 443172
#% 466483
#% 481290
#% 546047
#% 578406
#% 629618
#% 637522
#% 735356
#% 856941
#% 1499573
#! Associative classification is a new classification approach integrating association mining and classification. It becomes a significant tool for knowledge discovery and data mining. However, high-order association mining is time consuming when the number of attributes becomes large. The recent development of the AdaBoost algorithm indicates that boosting simple rules could often achieve better classification results than the use of complex rules. In view of this, we apply the AdaBoost algorithm to an associative classification system for both learning time reduction and accuracy improvement. In addition to exploring many advantages of the boosted associative classification system, this paper also proposes a new weighting strategy for voting multiple classifiers.

#index 871032
#* Attribute-Level Neighbor Hierarchy Construction Using Evolved Pattern-Based Knowledge Induction
#@ Thanit Puthpongsiriporn;J. David Porter;Bopaya Bidanda;Ming-En Wang;Richard E. Billo
#t 2006
#c 7
#% 245519
#% 361612
#% 442678
#% 443133
#% 452747
#% 617860
#% 1809812
#! Neighbor knowledge construction is the foundation for the development of cooperative query answering systems capable of searching for close match or approximate answers when exact match answers are not available. This paper presents a technique for developing neighbor hierarchies at the attribute level. The proposed technique is called the evolved Pattern-based Knowledge Induction (ePKI) technique and allows construction of neighbor hierarchies for nonunique attributes based upon confidences, popularities, and clustering correlations of inferential relationships among attribute values. The technique is applicable for both categorical and numerical (discrete and continuous) attribute values. Attribute value neighbor hierarchies generated by the ePKI technique allow a cooperative query answering system to search for approximate answers by relaxing each individual query condition separately. Consequently, users can search for approximate answers even when the exact match answers do not exist in the database (i.e., searching for existing similar parts as part of the implementation of the concepts of rapid prototyping). Several experiments were conducted to assess the performance of the ePKI in constructing attribute-level neighbor hierarchies. Results indicate that the ePKI technique produces accurate neighbor hierarchies when strong inferential relationships appear among data.

#index 889076
#* Unsupervised Selection of a Finite Dirichlet Mixture Model: An MML-Based Approach
#@ Nizar Bouguila;Djemel Ziou
#t 2006
#c 7
#% 58646
#% 219847
#% 268102
#% 296375
#% 345829
#% 424804
#% 424810
#% 484604
#% 775028
#% 971183
#% 1389003
#% 1650716
#! This paper proposes an unsupervised algorithm for learning a finite Dirichlet mixture model. An important part of the unsupervised learning problem is determining the number of clusters which best describe the data. We extend the minimum message length (MML) principle to determine the number of clusters in the case of Dirichlet mixtures. Parameter estimation is done by the expectation-maximization algorithm. The resulting method is validated for one-dimensional and multidimensional data. For the one-dimensional data, the experiments concern artificial and real SAR image histograms. The validation for multidimensional data involves synthetic data and two real applications: shadow detection in images and summarization of texture image databases for efficient retrieval. A comparison with results obtained for other selection criteria is provided.

#index 889077
#* Discovering Expressive Process Models by Clustering Log Traces
#@ Gianluigi Greco;Antonella Guzzo;Luigi Pontieri;Domenico Sacca
#t 2006
#c 7
#% 203029
#% 210390
#% 248013
#% 259602
#% 274098
#% 280488
#% 310496
#% 310559
#% 310561
#% 345694
#% 346653
#% 398149
#% 459021
#% 463903
#% 466668
#% 487251
#% 488623
#% 579505
#% 631960
#% 720183
#% 733140
#% 772836
#% 796214
#% 889077
#% 983684
#% 993989
#% 1390338
#% 1709197
#% 1711212
#% 1739933
#! Process mining techniques have recently received notable attention in the literature for their ability to assist in the (re)design of complex processes by automatically discovering models that explain the events registered in some log traces provided as input. Following this line of research, the paper investigates an extension of such basic approaches, where the identification of different variants for the process is explicitly accounted for, based on the clustering of log traces. Indeed, modeling each group of similar executions with a different schema allows us to single out "conformant” models, which, specifically, minimize the number of modeled enactments that are extraneous to the process semantics. Therefore, a novel process mining framework is introduced and some relevant computational issues are deeply studied. As finding an exact solution to such an enhanced process mining problem is proven to require high computational costs, in most practical cases, a greedy approach is devised. This is founded on an iterative, hierarchical, refinement of the process model, where, at each step, traces sharing similar behavior patterns are clustered together and equipped with a specialized schema. The algorithm guarantees that each refinement leads to an increasingly sound model, thus attaining a monotonic search. Experimental results evidence the validity of the approach with respect to both effectiveness and scalability.

#index 889078
#* Orthogonal Decision Trees
#@ Hillol Kargupta;Byung-Hoon Park;Haimonti Dutta
#t 2006
#c 7
#% 156699
#% 209021
#% 280496
#% 342639
#% 345861
#% 424993
#% 733624
#! This paper introduces orthogonal decision trees that offer an effective way to construct a redundancy-free, accurate, and meaningful representation of large decision-tree-ensembles often created by popular techniques such as Bagging, Boosting, Random Forests, and many distributed and data stream mining algorithms. Orthogonal decision trees are functionally orthogonal to each other and they correspond to the principal components of the underlying function space. This paper offers a technique to construct such trees based on the Fourier transformation of decision trees and eigen-analysis of the ensemble in the Fourier representation. It offers experimental results to document the performance of orthogonal trees on the grounds of accuracy and model complexity.

#index 889079
#* Test Strategies for Cost-Sensitive Decision Trees
#@ Charles X. Ling;Victor S. Sheng;Qiang Yang
#t 2006
#c 7
#% 280437
#% 342611
#% 464639
#% 477640
#% 770791
#% 785338
#% 785413
#% 829982
#% 1250583
#% 1272369
#% 1289281
#% 1499572
#% 1673023
#! In medical diagnosis, doctors must often determine what medical tests (e.g., X-ray and blood tests) should be ordered for a patient to minimize the total cost of medical tests and misdiagnosis. In this paper, we design cost-sensitive machine learning algorithms to model this learning and diagnosis process. Medical tests are like attributes in machine learning whose values may be obtained at a cost (attribute cost), and misdiagnoses are like misclassifications which may also incur a cost (misclassification cost). We first propose a lazy decision tree learning algorithm that minimizes the sum of attribute costs and misclassification costs. Then, we design several novel "test strategies” that can request to obtain values of unknown attributes at a cost (similar to doctors' ordering of medical tests at a cost) in order to minimize the total cost for test examples (new patients). These test strategies correspond to different situations in real-world diagnoses. We empirically evaluate these test strategies, and show that they are effective and outperform previous methods. Our results can be readily applied to real-world diagnosis tasks. A case study on heart disease is given throughout the paper.

#index 889080
#* Generating Compact Redundancy-Free XML Documents from Conceptual-Model Hypergraphs
#@ Wai Yin Mok;David W. Embley
#t 2006
#c 7
#% 4797
#% 10245
#% 205246
#% 286897
#% 287223
#% 287631
#% 289424
#% 357984
#% 443490
#% 533902
#% 742566
#% 771227
#% 1015270
#% 1388057
#% 1393699
#! As XML data becomes more and more prevalent and as larger quantities of data find their way into XML documents, the need for quality XML data organization will only increase. One standard way of structuring data well is to reduce and, if possible, eliminate redundancy, while at the same time making the storage structures as compact as possible. In this paper, we present a methodology to generate XML storage structures where conforming XML documents are redundancy-free, and for most practical cases, are also fully compact. Our methodology assumes the input is a conceptual-model hypergraph. For the special case that every edge in the hypergraph is binary, we present a simple algorithm, guaranteed to always generate redundancy-free storage structures. We show, however, that generating a minimum number of redundancy-free storage structures is NP-hard. We therefore provide heuristics to guide the process and observe that these heuristics result in satisfactory solutions, which are often optimal. We then present a general algorithm for n{\hbox{-}}\rm ary edges and show that it generates redundancy-free storage structures. The general algorithm must overcome several problems that do not arise in the special case.

#index 889081
#* A Compensation-Based Approach for View Maintenance in Distributed Environments
#@ Songting Chen;Xin Zhang;Elke A. Rundensteiner
#t 2006
#c 7
#% 201928
#% 201929
#% 201930
#% 210210
#% 227947
#% 229827
#% 300141
#% 334043
#% 378409
#% 443527
#% 479452
#% 480134
#% 480645
#% 511638
#% 565261
#% 577359
#% 637833
#% 654525
#% 745537
#% 772834
#% 791180
#% 824736
#% 1015303
#! Data integration over multiple heterogeneous data sources has become increasingly important for modern applications. The integrated data is usually stored as materialized views to allow better access, performance, and high availability. In loosely coupled environments, such as the Data Grid, the data sources are autonomous. Hence, the source updates can be concurrent and cause erroneous results during view maintenance. State-of-the-art maintenance strategies apply compensating queries to correct such errors, making the restricting assumption that all source schemata remain static over time. However, in such dynamic environments, the data sources may change not only their data but also their schema. Consequently, either the maintenance queries or the compensating queries may fail. In this paper, we propose a novel framework called DyDa that overcomes these limitations and handles both source data updates and schema changes. We identify three types of maintenance anomalies, caused by either source data updates, data-preserving schema changes, or non-data-preserving schema changes. We propose a compensation algorithm to solve the first two types of anomalies. We show that the third type of anomaly is caused by the violation of dependencies between maintenance processes. Then, we propose dependency detection and correction algorithms to identify and resolve the violations. Put together, DyDa extends prior maintenance solutions to solve all types of view maintenance anomalies. The experimental results show that DyDa imposes a minimal overhead on data update processing while allowing for the extended functionality to handle concurrent schema changes.

#index 889082
#* Quality of Service Guarantee for Temporal Consistency of Real-Time Transactions
#@ Ming Xiong;Biyu Liang;Kam-Yiu Lam;Yang Guo
#t 2006
#c 7
#% 235030
#% 442985
#% 443541
#% 452735
#% 586147
#% 615561
#% 615634
#% 635887
#% 646398
#% 744763
#% 789417
#! The More-Less (ML) scheme has been shown to be an efficient approach for maintaining temporal consistency of real-time data objects. Although ML provides a deterministic guarantee in temporal consistency, the number of update transactions that can be supported in a system is limited. This is due to its use of the worst-case computation time in deriving deadlines and periods of update transactions. This paper studies the problem of temporal consistency maintenance where a certain degree of temporal inconsistency is tolerable. A suite of Statistical More-Less (SML) approaches are proposed to explore the trade-off between quality of service (QoS) of temporal consistency and the number of supported transactions. It begins with a base-line algorithm, SML-BA, which provides the requested QoS of temporal consistency. Then, SML with Optimization (SML-OPT) is proposed to further improve the QoS by better utilizing the excess processor capacity. Finally, SML-OPT is enhanced with a Slack Reclaiming scheme (SML-SR). The reclaimed slacks are used to process jobs whose required computation time is larger than the guaranteed computation time. Simulation experiments are conducted to compare the performance of these schemes (SML-BA, SML-OPT, and SML-SR) together with the deterministic More-Less and Half-Half schemes. The results show that the SML schemes are effective in trading the schedulability of transactions for the QoS guaranteed. Moreover, SML-SR performs best and offers a significant QoS improvement over SML-BA and SML-OPT.

#index 889083
#* An Energy-Efficient and Access Latency Optimized Indexing Scheme for Wireless Data Broadcast
#@ Yuxia Yao;Xueyan Tang;Ee-Peng Lim;Aixin Sun
#t 2006
#c 7
#% 201897
#% 235940
#% 247246
#% 248768
#% 269631
#% 274199
#% 430426
#% 442626
#% 443127
#% 443263
#% 452850
#% 452871
#% 575109
#% 720828
#% 729623
#% 730648
#% 731800
#% 755203
#% 784513
#% 844271
#% 849817
#! Data broadcast is an attractive data dissemination method in mobile environments. To improve energy efficiency, existing air indexing schemes for data broadcast have focused on reducing tuning time only, i.e., the duration that a mobile client stays active in data accesses. On the other hand, existing broadcast scheduling schemes have aimed at reducing access latency through nonflat data broadcast to improve responsiveness only. Not much work has addressed the energy efficiency and responsiveness issues concurrently. This paper proposes an energy-efficient indexing scheme called MHash that optimizes tuning time and access latency in an integrated fashion. MHash reduces tuning time by means of hash-based indexing and enables nonflat data broadcast to reduce access latency. The design of hash function and the optimization of bandwidth allocation are investigated in depth to refine MHash. Experimental results show that, under skewed access distribution, MHash outperforms state-of-the-art air indexing schemes and achieves access latency close to optimal broadcast scheduling.

#index 889084
#* Transparent Decision Support Using Statistical Reasoning and Fuzzy Inference
#@ Andrew Hamilton-Wright;Daniel W. Stashuk
#t 2006
#c 7
#% 223647
#% 248044
#% 335215
#% 443172
#% 641102
#% 1788658
#! A framework for the development of a decision support system (DSS) that exhibits uncommonly transparent rule-based inference logic is introduced. A DSS is constructed by marrying a statistically based fuzzy inference system (FIS) with a user interface, allowing drill-down exploration of the underlying statistical support, providing transparent access to both the rule-based inference as well as the underlying statistical basis for the rules. The FIS is constructed through a "Pattern Discovery” based analysis of training data. Such an analysis yields a rule base characterized by simple explanations for any rule or data division in the extracted knowledge base. The reliability of a fuzzy inference is well predicted by a confidence measure that determines the probability of a correct suggestion by examination of values produced within the inference calculation. The combination of these components provides a means of constructing decision support systems that exhibit a degree of transparency beyond that commonly observed in supervised-learning-based methods. A prototype DSS is analyzed in terms of its workflow and usability, outlining the insight derived through use of the framework. This is demonstrated by considering a simple synthetic data example and a more interesting real-world example application with the goal of characterizing patients with respect to risk of heart disease. Specific input data samples and corresponding output suggestions created by the system are presented and discussed. The means by which the suggestions made by the system may be used in a larger decision context is evaluated.

#index 889085
#* Sentence Similarity Based on Semantic Nets and Corpus Statistics
#@ Yuhua Li;David McLean;Zuhair A. Bandar;James D. O'Shea;Keeley Crockett
#t 2006
#c 7
#% 179717
#% 198058
#% 272325
#% 325502
#% 388500
#% 389155
#% 452869
#% 740763
#% 762603
#% 813994
#% 1272053
#% 1275285
#% 1697983
#! Sentence similarity measures play an increasingly important role in text-related research and applications in areas such as text mining, Web page retrieval, and dialogue systems. Existing methods for computing sentence similarity have been adopted from approaches used for long text documents. These methods process sentences in a very high-dimensional space and are consequently inefficient, require human input, and are not adaptable to some application domains. This paper focuses directly on computing the similarity between very short texts of sentence length. It presents an algorithm that takes account of semantic information and word order information implied in the sentences. The semantic similarity of two sentences is calculated using information from a structured lexical database and from corpus statistics. The use of a lexical database enables our method to model human common sense knowledge and the incorporation of corpus statistics allows our method to be adaptable to different domains. The proposed method can be used in a variety of applications that involve text knowledge representation and discovery. Experiments on two sets of selected sentence pairs demonstrate that the proposed method provides a similarity measure that shows a significant correlation to human intuition.

#index 889086
#* Robust Rule-Based Prediction
#@ Jiuyong Li
#t 2006
#c 7
#% 209021
#% 269634
#% 300120
#% 577277
#% 791179
#! This paper studies a problem of robust rule-based classification, i.e., making predictions in the presence of missing values in data. This study differs from other missing value handling research in that it does not handle missing values but builds a rule-based classification model to tolerate missing values. Based on a commonly used rule-based classification model, we characterize the robustness of a hierarchy of rule sets as k{\hbox{-}}{\rm{optimal}} rule sets with the decreasing size corresponding to the decreasing robustness. We build classifiers based on k{\hbox{-}}{\rm{optimal}} rule sets and show experimentally that they are more robust than some benchmark rule-based classifiers, such as C4.5rules and CBA. We also show that the proposed approach is better than two well-known missing value handling methods for missing values in test data.

#index 889087
#* EIC Editorial: TKDE Editorial Board Changes
#@ Xindong Wu
#t 2006
#c 7

#index 889088
#* A New Text Categorization Technique Using Distributional Clustering and Learning Logic
#@ Hisham Al-Mubaid;Syed A. Umair
#t 2006
#c 7
#% 116149
#% 190581
#% 260001
#% 262059
#% 266215
#% 266292
#% 280817
#% 280819
#% 280866
#% 340903
#% 344447
#% 376266
#% 458379
#% 465747
#% 465754
#% 482864
#% 574135
#% 722930
#% 722934
#% 722935
#! Text categorization is continuing to be one of the most researched NLP problems due to the ever-increasing amounts of electronic documents and digital libraries. In this paper, we present a new text categorization method that combines the distributional clustering of words and a learning logic technique, called Lsquare, for constructing text classifiers. The high dimensionality of text in a document has not been fruitful for the task of categorization, for which reason, feature clustering has been proven to be an ideal alternative to feature selection for reducing the dimensionality. We, therefore, use distributional clustering method (IB) to generate an efficient representation of documents and apply Lsquare for training text classifiers. The method was extensively tested and evaluated. The proposed method achieves higher or comparable classification accuracy and {\rm F}_1 results compared with SVM on exact experimental settings with a small number of training documents on three benchmark data sets WebKB, 20Newsgroup, and Reuters-21578. The results prove that the method is a good choice for applications with a limited amount of labeled training data. We also demonstrate the effect of changing training size on the classification performance of the learners.

#index 889089
#* Adaptive Clustering for Multiple Evolving Streams
#@ Bi-Ru Dai;Jen-Wei Huang;Mi-Yen Yeh;Ming-Syan Chen
#t 2006
#c 7
#% 293720
#% 310500
#% 342600
#% 378388
#% 379445
#% 466083
#% 575972
#% 594012
#% 659972
#% 727900
#% 769888
#% 769927
#% 800176
#% 800496
#% 810007
#% 993958
#% 1015261
#% 1015262
#% 1016200
#! In the data stream environment, the patterns generated at different time instances are different due to data evolution. As time progresses, the behavior and members of clusters usually change. Hence, clustering continuous data streams allows us to observe the changes of group behavior. In order to support flexible clustering requirements, we devise in this paper a Clustering on Demand framework, abbreviated as COD framework, to dynamically cluster multiple data streams. While providing a general framework of clustering on multiple data streams, the COD framework has two advantageous features, namely, one data scan for online statistics collection and compact multiresolution approximations, which are designed to address, respectively, the time and the space constraints in a data stream environment. The COD framework consists of two phases, i.e., the online maintenance phase and the offline clustering phase. The online maintenance phase provides an efficient mechanism to maintain summary hierarchies of data streams with multiple resolutions in time linear in both the number of streams and the number of data points in each stream. On the other hand, an adaptive clustering algorithm is devised for the offline phase to retrieve approximations of desired substreams from summary hierarchies according to clustering queries. We propose two summarization techniques, based on wavelet and regression analyses, to construct the summary hierarchies. The regression-based summary hierarchy approximates the data stream more precisely and provides better clustering results, at the cost of slightly longer time than and twice the storage space as the wavelet-based one. An adaptive version of COD framework is designed to make a selection between a wavelet-based model and a regression-based model for building the summary hierarchy. By the adaptive COD, we can obtain clustering results with almost the same quality as the regression-based COD while using much less storage space for the summary hierarchy. As shown in the complexity analyses and also validated by our empirical studies, the COD framework performs very efficiently in the data stream environment while producing clustering results of very high quality.

#index 889090
#* Multidimensional Vector Regression for Accurate and Low-Cost Location Estimation in Pervasive Computing
#@ Jeffrey Junfeng Pan;James T. Kwok;Qiang Yang;Yiqiang Chen
#t 2006
#c 7
#% 339218
#% 401172
#% 613334
#% 613383
#% 722809
#% 722818
#% 746038
#% 777786
#% 777788
#% 797049
#% 797050
#% 800186
#% 819757
#% 1250174
#% 1250210
#% 1289567
#! In this paper, we present an algorithm for multidimensional vector regression on data that are highly uncertain and nonlinear, and then apply it to the problem of indoor location estimation in a wireless local area network (WLAN). Our aim is to obtain an accurate mapping between the signal space and the physical space without requiring too much human calibration effort. This location estimation problem has traditionally been tackled through probabilistic models trained on manually labeled data, which are expensive to obtain. In contrast, our algorithm adopts Kernel Canonical Correlation Analysis (KCCA) to build a nonlinear mapping between the signal-vector space and the physical location space by transforming data in both spaces into their canonical features. This allows the pairwise similarity of samples in both spaces to be maximally correlated using kernels. We use a Gaussian kernel to adapt to the noisy characteristics of signal strengths and a Matérn kernel to sense the changes in physical locations. By using real data collected in an 802.11 wireless LAN environment, we achieve accurate location estimation for pervasive computing while requiring a much smaller set of labeled training data than previous methods.

#index 889091
#* The Optimality of Allocation Methods for Bounded Disagreement Search Queries: The Possible and the Impossible
#@ Khaled A. S. Abdel-Ghaffar;Amr El Abbadi
#t 2006
#c 7
#% 45766
#% 135559
#% 271202
#% 286962
#% 299983
#% 344424
#% 386006
#% 443200
#% 465027
#% 563486
#% 565263
#% 637794
#% 715140
#% 733594
#% 1809814
#! Data Allocation on multiple I/O devices manifests itself in many computing systems, both centralized and distributed. Data is partitioned on multiple I/O devices and clients issue various types of queries to retrieve relevant information. In this paper, we derive necessary and sufficient conditions for a data allocation method to be optimal for two important types of queries: partial match and bounded disagreement search queries. We formally define these query types and derive the optimality conditions based on coding-theoretic arguments. Although these conditions are fairly strict, we show how to construct good allocation methods for practical realistic situations. Not only are the response times bounded by a small value, but also the identification of the relevant answer set is efficient.

#index 889092
#* On the Signature Tree Construction and Analysis
#@ Yibin Chen
#t 2006
#c 7
#% 1921
#% 3036
#% 6806
#% 209691
#% 249989
#% 268473
#% 287715
#% 288578
#% 442959
#% 737363
#! Advanced database application areas, such as computer aided design, office automation, digital libraries, data-mining, as well as hypertext and multimedia systems, need to handle complex data structures with set-valued attributes, which can be represented as bit strings, called signatures. A set of signatures can be stored in a file, called a signature file. In this paper, we propose a new method to organize a signature file into a tree structure, called a signature tree, to speed up the signature file scanning and query evaluation. In addition, the average time complexity of searching a signature tree is analyzed and how to maintain a signature tree on disk is discussed. We also conducted experiments, which show that the approach of signature trees provides a promising index structure.

#index 889093
#* Efficient, Energy Conserving Transaction Processing in Wireless Data Broadcast
#@ SangKeun Lee;Chong-Sun Hwang;Masaru Kitsuregawa
#t 2006
#c 7
#% 172876
#% 175253
#% 201897
#% 259658
#% 273893
#% 274199
#% 279165
#% 286967
#% 316788
#% 430426
#% 442620
#% 442622
#% 443127
#% 443224
#% 443263
#% 443365
#% 452850
#% 479961
#% 487722
#% 511499
#% 553995
#% 554731
#% 613372
#% 631861
#% 632025
#% 632067
#% 635819
#% 661499
#% 727673
#! Broadcasting in wireless mobile computing environments is an effective technique to disseminate information to a massive number of clients equipped with powerful, battery operated devices. To conserve the usage of energy, which is a scarce resource, the information to be broadcast must be organized so that the client can selectively tune in at the desired portion of the broadcast. In this paper, the efficient, energy conserving transaction processing in mobile broadcast environments is examined with widely accepted approaches to indexed data organizations suited for a single item retrieval. The basic idea is to share the index information on multiple data items based on the predeclaration technique. The analytical and simulation studies have been performed to evaluate the effectiveness of our methodology, showing that predeclaration-based transaction processing with selective tuning ability can provide a significant performance improvement of battery life, while retaining a low access time. Tolerance to access failures during transaction processing is also described.

#index 889094
#* Reverse Nearest Neighbor Search in Metric Spaces
#@ Yufei Tao;Man Lung Yiu;Nikos Mamoulis
#t 2006
#c 7
#% 86950
#% 201876
#% 235114
#% 248017
#% 287466
#% 300136
#% 300163
#% 300183
#% 411758
#% 465009
#% 479462
#% 480812
#% 527328
#% 730019
#% 731409
#% 1016191
#! Given a set {\cal D} of objects, a reverse nearest neighbor (RNN) query returns the objects o in {\cal D} such that o is closer to a query object q than to any other object in {\cal D}, according to a certain similarity metric. The existing RNN solutions are not sufficient because they either 1) rely on precomputed information that is expensive to maintain in the presence of updates or 2) are applicable only when the data consists of "Euclidean objects” and similarity is measured using the L_2 norm. In this paper, we present the first algorithms for efficient RNN search in generic metric spaces. Our techniques require no detailed representations of objects, and can be applied as long as their mutual distances can be computed and the distance metric satisfies the triangle inequality. We confirm the effectiveness of the proposed methods with extensive experiments.

#index 889095
#* A Tree-Based Data Perturbation Approach for Privacy-Preserving Data Mining
#@ Xiao-Bai Li;Sumit Sarkar
#t 2006
#c 7
#% 1868
#% 67453
#% 300184
#% 317313
#% 340475
#% 574170
#% 575969
#% 577233
#% 740764
#! Due to growing concerns about the privacy of personal information, organizations that use their customers' records in data mining activities are forced to take actions to protect the privacy of the individuals. A frequently used disclosure protection method is data perturbation. When used for data mining, it is desirable that perturbation preserves statistical relationships between attributes, while providing adequate protection for individual confidential data. To achieve this goal, we propose a kd-tree based perturbation method, which recursively partitions a data set into smaller subsets such that data records within each subset are more homogeneous after each partition. The confidential data in each final subset are then perturbed using the subset average. An experimental study is conducted to show the effectiveness of the proposed method.

#index 889096
#* Privacy-Preserving Computation of Bayesian Networks on Vertically Partitioned Data
#@ Zhiqiang Yang;Rebecca N. Wright
#t 2006
#c 7
#% 300184
#% 333876
#% 340976
#% 393264
#% 446010
#% 487496
#% 564438
#% 568869
#% 577233
#% 577289
#% 616944
#% 654448
#% 727904
#% 729930
#% 743280
#% 769943
#% 769962
#% 785414
#% 823389
#% 884531
#% 963800
#% 993988
#% 1386180
#% 1722551
#! Traditionally, many data mining techniques have been designed in the centralized model in which all data is collected and available in one central site. However, as more and more activities are carried out using computers and computer networks, the amount of potentially sensitive data stored by business, governments, and other parties increases. Different parties often wish to benefit from cooperative use of their data, but privacy regulations and other privacy concerns may prevent the parties from sharing their data. Privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data need not be revealed. In this paper, we present privacy-preserving protocols for a particular data mining task: learning a Bayesian network from a database vertically partitioned among two parties. In this setting, two parties owning confidential databases wish to learn the Bayesian network on the combination of their databases without revealing anything else about their data to each other. We present an efficient and privacy-preserving protocol to construct a Bayesian network on the parties' joint data.

#index 889097
#* A Knowledge-Based Software Life-Cycle Framework for the Incorporation of Multicriteria Analysis in Intelligent User Interfaces
#@ Katerina Kabassi;Maria Virvou
#t 2006
#c 7
#% 243177
#% 248125
#% 262224
#% 278252
#% 358412
#% 424015
#% 429835
#% 482523
#% 516284
#% 541258
#% 1386316
#% 1389383
#% 1719097
#! Decision-making theories aiming at solving decision problems that involve multiple criteria have often been incorporated in knowledge-based systems for the improvement of these systems' reasoning process. However, multicriteria analysis has not been used adequately in intelligent user interfaces, even though user-computer interaction is, by nature, multicriteria-based. The actual process of incorporating multicriteria analysis into an intelligent user interface is neither clearly defined nor adequately described in the literature. It involves many experimental studies throughout the software life-cycle. Moreover, each multicriteria decision-making theory requires different kinds of experiments for the criteria to be determined and then for the proper respective weight of each criterion to be specified. In our research, we address the complex issue of developing intelligent user interfaces that are based on multicriteria decision-making theories. In particular, we present and discuss a software life-cycle framework that is appropriate for the development of such user interfaces. The life-cycle framework is called MBIUI. Given the fact, that very little has been reported in the literature about the required experimental studies, their participants and the appropriate life-cycle phase during which the experimental studies should take place, MBIUI provides useful insight for future developments of intelligent user interfaces that incorporate multicriteria theories. One significant advantage of MBIUI is that it provides a unifying life-cycle framework that may be used for the application of many different multicriteria decision-making theories. In the paper, we discuss the incorporation features of four distinct multicriteria theories: TOPSIS, SAW, MAUT, and DEA. Furthermore, we give detailed specifications of the experiments that should take place and reveal their similarities and differences with respect to the theories.

#index 889098
#* Mobile Advertising in Capacitated Wireless Networks
#@ Arvind K. Tripathi;Suresh K. Nair
#t 2006
#c 7
#% 360833
#% 379122
#% 401191
#% 401195
#% 571877
#% 640705
#% 1848755
#! The growing number of mobile subscribers has attracted firms to invent newer strategies to reach prospective customers in innovative but nonintrusive ways. While customer mobility creates an opportunity to reach them at desired times and locations, in practice, real-time ad deliveries are difficult due to the size of the ad-delivery decision problem. This research aims at analyzing and developing a decision policy for delivering ads on mobile devices such as cell phones. We look at the ad delivery problem from the perspective of an advertising firm, which delivers ads on behalf of its clients (merchants) to mobile customer using a wireless carrier's infrastructure. We formulate the mobile ad delivery problem as a Markov decision process (MDP) model. The ad delivery policy depends on the customer's desire and willingness to receive ads, their real-time locations, historical mobility patterns, available network capacity, and fee structure agreed upon for ad deliveries. We also develop a fast heuristic to solve larger size problems. We test our heuristic against an upper bound we developed and analyze performance using simulation.

#index 889099
#* Feature Reduction via Generalized Uncorrelated Linear Discriminant Analysis
#@ Jieping Ye;Ravi Janardan;Qi Li;Haesun Park
#t 2006
#c 7
#% 49252
#% 80995
#% 190581
#% 200694
#% 212689
#% 224113
#% 235342
#% 581716
#% 729437
#% 829010
#% 1010920
#% 1828409
#% 1828410
#! High-dimensional data appear in many applications of data mining, machine learning, and bioinformatics. Feature reduction is commonly applied as a preprocessing step to overcome the curse of dimensionality. Uncorrelated Linear Discriminant Analysis (ULDA) was recently proposed for feature reduction. The extracted features via ULDA were shown to be statistically uncorrelated, which is desirable for many applications. In this paper, an algorithm called ULDA/QR is proposed to simplify the previous implementation of ULDA. Then, the ULDA/GSVD algorithm is proposed, based on a novel optimization criterion, to address the singularity problem which occurs in undersampled problems, where the data dimension is larger than the sample size. The criterion used is the regularized version of the one in ULDA/QR. Surprisingly, our theoretical result shows that the solution to ULDA/GSVD is independent of the value of the regularization parameter. Experimental results on various types of data sets are reported to show the effectiveness of the proposed algorithm and to compare it with other commonly used feature reduction algorithms.

#index 889100
#* A Joinless Approach for Mining Spatial Colocation Patterns
#@ Jin Soung Yoo;Shashi Shekhar
#t 2006
#c 7
#% 273685
#% 342635
#% 466644
#% 479797
#% 527021
#% 527171
#% 629708
#% 630974
#% 784296
#% 844416
#! Spatial colocations represent the subsets of features which are frequently located together in geographic space. Colocation pattern discovery presents challenges since spatial objects are embedded in a continuous space, whereas classical data is often discrete. A large fraction of the computation time is devoted to identifying the instances of colocation patterns. We propose a novel joinless approach for efficient colocation pattern mining. The joinless colocation mining algorithm uses an instance-lookup scheme instead of an expensive spatial or an instance join operation for identifying colocation instances. We prove the joinless algorithm is correct and complete in finding colocation rules. We also describe a partial join approach for a spatial data set often clustered in neighborhood areas. We provide the algebraic cost models to characterize the performance dominance zones of the joinless method and the partial join method with a current join-based colocation mining method, and compare their computational complexities. In the experimental evaluation, using synthetic and real-world data sets, our methods performed more efficiently than the join-based method and show more scalability in dense data.

#index 889101
#* Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization
#@ Min-Ling Zhang;Zhi-Hua Zhou
#t 2006
#c 7
#% 67565
#% 92148
#% 136350
#% 235377
#% 252009
#% 260001
#% 311034
#% 318412
#% 344447
#% 361100
#% 400985
#% 430756
#% 465754
#% 466240
#% 642996
#% 770783
#% 934582
#% 1388992
#! In multilabel learning, each instance in the training set is associated with a set of labels and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, this problem is addressed in the way that a neural network algorithm named BP-MLL, i.e., Backpropagation for Multilabel Learning, is proposed. It is derived from the popular Backpropogation algorithm through employing a novel error function capturing the characteristics of multilabel learning, i.e., the labels belonging to an instance should be ranked higher than those not belonging to that instance. Applications to two real-world multilabel learning problems, i.e., functional genomics and text categorization, show that the performance of BP-MLL is superior to that of some well-established multilabel learning algorithms.

#index 889102
#* Achieving Communication Efficiency through Push-Pull Partitioning of Semantic Spaces to Disseminate Dynamic Information
#@ Amitabha Bagchi;Amitabh Chaudhary;Michael T. Goodrich;Chen Li;Michal Shmueli-Scheuer
#t 2006
#c 7
#% 43163
#% 123074
#% 210190
#% 227885
#% 299982
#% 330682
#% 333947
#% 333969
#% 333995
#% 344720
#% 360716
#% 399550
#% 428605
#% 443235
#% 443262
#% 443263
#% 480128
#% 481916
#% 571216
#% 659573
#% 993975
#% 993978
#! Many database applications that need to disseminate dynamic information from a server to various clients can suffer from heavy communication costs. Data caching at a client can help mitigate these costs, particularly when individual {\rm PUSH}{\hbox{-}}{\rm PULL} decisions are made for the different semantic regions in the data space. The server is responsible for notifying the client about updates in the {\rm PUSH} regions. The client needs to contact the server for queries that ask for data in the {\rm PULL} regions. We call the idea of partitioning the data space into {\rm PUSH}{\hbox{-}}{\rm PULL} regions to minimize communication cost data gerrymandering. In this paper, we present solutions to technical challenges in adopting this simple but powerful idea. We give a provably optimal-cost dynamic programming algorithm for gerrymandering on a single query attribute. We propose a family of efficient heuristics for gerrymandering on multiple query attributes. We handle the dynamic case in which the workloads of queries and updates evolve over time. We validate our methods through extensive experiments on real and synthetic data sets.

#index 889103
#* An Effective and Efficient Exact Match Retrieval Scheme for Symbolic Image Database Systems Based on Spatial Reasoning: A Logarithmic Search Time Approach
#@ P. Punitha;D. S. Guru
#t 2006
#c 7
#% 23998
#% 68781
#% 78243
#% 100673
#% 124680
#% 129689
#% 140649
#% 170852
#% 181409
#% 183568
#% 186230
#% 202322
#% 228108
#% 245508
#% 277596
#% 334741
#% 443133
#% 443213
#% 443242
#% 443458
#% 443543
#% 679325
#% 738842
#% 787725
#% 824954
#% 1215203
#! In this paper, a novel method of representing symbolic images in a symbolic image database (SID) invariant to image transformations that is useful for exact match retrieval is presented. The relative spatial relationships existing among the components present in an image are perceived with respect to the direction of reference [15] and preserved by a set of triples. A distinct and unique key is computed for each distinct triple. The mean and standard deviation of the set of keys computed for a symbolic image are stored along with the total number of keys as the representatives of the corresponding image. The proposed exact match retrieval scheme is based on a modified binary search technique and, thus, requires O(log n) search time in the worst case, where n is the total number of symbolic images in the SID. An extensive experimentation on a large database of 22,630 symbolic images is conducted to corroborate the superiority of the model. The effectiveness of the proposed representation scheme is tested with standard testbed images.

#index 889104
#* Segmenting Customers from Population to Individuals: Does 1-to-1 Keep Your Customers Forever?
#@ Tianyi Jiang;Alexander Tuzhilin
#t 2006
#c 7
#% 17185
#% 136350
#% 229931
#% 234992
#% 290482
#% 307109
#% 310488
#% 341700
#% 420118
#% 629695
#% 727886
#% 727917
#% 739411
#% 739634
#% 829389
#% 840892
#% 1650665
#! There have been various claims made in the marketing community about the benefits of 1-to-1 marketing versus traditional customer segmentation approaches and how much they can improve understanding of customer behavior. However, few rigorous studies exist that systematically compare these approaches. In this paper, we conducted such a study and compared the predictive performance of aggregate, segmentation, and 1-to-1 marketing approaches across a broad range of experimental settings, such as multiple segmentation levels, multiple real-world marketing data sets, multiple dependent variables, different types of classifiers, different segmentation techniques, and different predictive measures. Our experiments show that both 1-to-1 and segmentation approaches significantly outperform aggregate modeling. Reaffirming anecdotal evidence of the benefits of 1-to-1 marketing, our experiments show that the 1-to-1 approach also dominates the segmentation approach for the frequently transacting customers. However, our experiments also show that segmentation models taken at the best granularity levels dominate 1-to-1 models when modeling customers with little transactional data using effective clustering methods. In addition, the peak performance of segmentation models are reached at the finest granularity levels, skewed towards the 1-to-1 case. This finding adds support for the microsegmentation approach and suggests that 1-to-1 marketing may not always be the best solution.

#index 889105
#* A Multiresolution Terrain Model for Efficient Visualization Query Processing
#@ Kai Xu;Xiaofang Zhou;Xuemin Lin;Heng Tao Shen;Ke Deng
#t 2006
#c 7
#% 2115
#% 86950
#% 137887
#% 153260
#% 164360
#% 213525
#% 213975
#% 232949
#% 259608
#% 342504
#% 349484
#% 349737
#% 379053
#% 411694
#% 413736
#% 427121
#% 427148
#% 448673
#% 451857
#% 462503
#% 480830
#% 510675
#% 565463
#% 590646
#% 607812
#% 631993
#% 632104
#% 662874
#% 745459
#! Multiresolution Triangular Mesh (MTM) models are widely used to improve the performance of large terrain visualization by replacing the original model with a simplified one. MTM models, which consist of both original and simplified data, are commonly stored in spatial database systems due to their size. The relatively slow access speed of disks makes data retrieval the bottleneck of such terrain visualization systems. Existing spatial access methods proposed to address this problem rely on main-memory MTM models, which leads to significant overhead during query processing. In this paper, we approach the problem from a new perspective and propose a novel MTM called direct mesh that is designed specifically for secondary storage. It supports available indexing methods natively and requires no modification to MTM structure. Experiment results, which are based on two real-world data sets, show an average performance improvement of 5-10 times over the existing methods.

#index 889106
#* Fuzzy Sets Defined on a Hierarchical Domain
#@ Rallou Thomopoulos;Patrice Buche;Ollivier Haemmerle
#t 2006
#c 7
#% 273
#% 10186
#% 54459
#% 465914
#% 561923
#% 566787
#% 748600
#% 862114
#% 1788481
#! This paper presents a new type of fuzzy sets, called "Hierarchical Fuzzy Sets,” that apply when the considered domain of values is not "flat,” but contains values that are more specific than others according to the "kind of” relation. We study the properties of such fuzzy sets, that can be defined in a short way on a part of the hierarchy, or exhaustively (by their "closure”) on the whole hierarchy. We show that hierarchical fuzzy sets form equivalence classes in regard to their closures and that each class has a particular representative called "minimal fuzzy set.” We propose a use of this minimal fuzzy set for query enlargement purposes and, thus, present a methodology for hierarchical fuzzy set generalization. We finally present an experimental evaluation of the theoretical results described in the paper, in a practical application.

#index 889107
#* A Survey of Web Information Extraction Systems
#@ Chia-Hui Chang;Mohammed Kayed;Moheb Ramzy Girgis;Khaled F. Shaalan
#t 2006
#c 7
#% 248808
#% 266216
#% 271065
#% 275915
#% 275917
#% 278109
#% 283136
#% 287202
#% 300288
#% 330784
#% 331772
#% 397605
#% 424931
#% 442981
#% 451356
#% 464825
#% 480824
#% 496886
#% 551870
#% 577319
#% 632051
#% 643004
#% 654469
#% 729939
#% 729978
#% 730038
#% 765409
#% 765411
#% 769890
#% 788941
#% 797999
#% 804821
#% 805845
#% 805846
#% 805847
#% 815342
#% 1290067
#% 1683908
#% 1782837
#! The Internet presents a huge amount of useful information which is usually formatted for its users, which makes it difficult to extract relevant data from various sources. Therefore, the availability of robust, flexible Information Extraction (IE) systems that transform the Web pages into program-friendly structures such as a relational database will become a great necessity. Although many approaches for data extraction from Web pages have been developed, there has been limited effort to compare such tools. Unfortunately, in only a few cases can the results generated by distinct tools be directly compared since the addressed extraction tasks are different. This paper surveys the major Web data extraction approaches and compares them in three dimensions: the task domain, the automation degree, and the techniques used. The criteria of the first dimension explain why an IE system fails to handle some Web sites of particular structures. The criteria of the second dimension classify IE systems based on the techniques used. The criteria of the third dimension measure the degree of automation for IE systems. We believe these criteria provide qualitatively measures to evaluate various IE approaches.

#index 889108
#* On Weight Design of Maximum Weighted Likelihood and an Extended EM Algorithm
#@ Zhenyue Zhang;Yiu-ming Cheung
#t 2006
#c 7
#% 256648
#% 775651
#% 813967
#% 832762
#% 959438
#! The recent Maximum Weighted Likelihood (MWL) [18], [19] has provided a general learning paradigm for density-mixture model selection and learning, in which weight design, however, is a key issue. This paper will therefore explore such a design, and through which a heuristic extended Expectation-Maximization (X-EM) algorithm is presented accordingly. Unlike the EM algorithm [1], the X-EM algorithm is able to perform model selection by fading the redundant components out from a density mixture, meanwhile estimating the model parameters appropriately. The numerical simulations demonstrate the efficacy of our algorithm.

#index 889109
#* Class Noise Handling for Effective Cost-Sensitive Learning by Cost-Guided Iterative Classification Filtering
#@ Xingquan Zhu;Xindong Wu
#t 2006
#c 7
#% 160852
#% 209021
#% 280437
#% 565543
#% 727925
#% 785369
#% 866324
#% 1272369
#! Recent research in machine learning, data mining, and related areas has produced a wide variety of algorithms for cost-sensitive (CS) classification, where instead of maximizing the classification accuracy, minimizing the misclassification cost becomes the objective. These methods often assume that their input is quality data without conflict or erroneous values, or the noise impact is trivial, which is seldom the case in real-world environments. In this paper, we propose a Cost-guided Iterative Classification Filter (CICF) to identify noise for effective CS learning. Instead of putting equal weights on handling noise in all classes in existing efforts, CICF puts more emphasis on expensive classes, which makes it attractive in dealing with data sets with a large cost-ratio. Experimental results and comparative studies indicate that the existence of noise may seriously corrupt the performance of the underlying CS learners and by adopting the proposed CICF algorithm, we can significantly reduce the misclassification cost of a CS classifier in noisy environments.

#index 902448
#* Discovering Frequent Graph Patterns Using Disjoint Paths
#@ Ehud Gudes;Solomon Eyal Shimony;Natalia Vanetik
#t 2006
#c 7
#% 262071
#% 273922
#% 378391
#% 397359
#% 431105
#% 443194
#% 443514
#% 466644
#% 479465
#% 481290
#% 619154
#% 629646
#% 629708
#% 654524
#% 727845
#% 729938
#% 731608
#% 745514
#% 766204
#% 769907
#% 772830
#% 893371
#% 944956
#% 1289345
#% 1390146
#! Whereas data mining in structured data focuses on frequent data values, in semistructured and graph data mining, the issue is frequent labels and common specific topologies. Here, the structure of the data is just as important as its content. We study the problem of discovering typical patterns of graph data, a task made difficult because of the complexity of required subtasks, especially subgraph isomorphism. In this paper, we propose a new Apriori-based algorithm for mining graph data, where the basic building blocks are relatively large, disjoint paths. The algorithm is proven to be sound and complete. Empirical evidence shows practical advantages of our approach for certain categories of graphs.

#index 902449
#* Discovering Frequent Closed Partial Orders from Strings
#@ Jian Pei;Haixun Wang;Jian Liu;Ke Wang;Jianyong Wang;Philip S. Yu
#t 2006
#c 7
#% 152934
#% 280409
#% 310515
#% 329537
#% 397383
#% 397632
#% 413550
#% 420063
#% 459006
#% 459021
#% 463903
#% 464996
#% 479971
#% 504526
#% 577256
#% 579314
#% 600539
#% 727882
#% 727913
#% 729922
#% 729933
#% 729938
#% 745492
#% 745515
#% 769910
#% 772836
#% 844401
#! Mining knowledge about ordering from sequence data is an important problem with many applications, such as bioinformatics, Web mining, network management, and intrusion detection. For example, if many customers follow a partial order in their purchases of a series of products, the partial order can be used to predict other related customers' future purchases and develop marketing campaigns. Moreover, some biological sequences (e.g., microarray data) can be clustered based on the partial orders shared by the sequences. Given a set of items, a total order of a subset of items can be represented as a string. A string database is a multiset of strings. In this paper, we identify a novel problem of mining frequent closed partial orders from strings. Frequent closed partial orders capture the nonredundant and interesting ordering information from string databases. Importantly, mining frequent closed partial orders can discover meaningful knowledge that cannot be disclosed by previous data mining techniques. However, the problem of mining frequent closed partial orders is challenging. To tackle the problem, we develop Frecpo (for Frequent closed partial order), a practically efficient algorithm for mining the complete set of frequent closed partial orders from large string databases. Several interesting pruning techniques are devised to speed up the search. We report an extensive performance study on both real data sets and synthetic data sets to illustrate the effectiveness and the efficiency of our approach.

#index 902450
#* Learning Contextual Dependency Network Models for Link-Based Classification
#@ Yonghong Tian;Qiang Yang;Tiejun Huang;Charles X. Ling;Wen Gao
#t 2006
#c 7
#% 79440
#% 248810
#% 266215
#% 420495
#% 430761
#% 464449
#% 577217
#% 722754
#% 727834
#% 727912
#% 729982
#% 731611
#% 743489
#% 769942
#% 785353
#% 850430
#% 882041
#% 1279354
#% 1387536
#% 1650403
#% 1650767
#% 1718470
#! Links among objects contain rich semantics that can be very helpful in classifying the objects. However, many irrelevant links can be found in real-world link data such as Web pages. Often, these noisy and irrelevant links do not provide useful and predictive information for categorization. It is thus important to automatically identify which links are most relevant for categorization. In this paper, we present a contextual dependency network (CDN) model for classifying linked objects in the presence of noisy and irrelevant links. The CDN model makes use of a dependency function that characterizes the contextual dependencies among linked objects. In this way, CDNs can differentiate the impacts of the related objects on the classification and consequently reduce the effect of irrelevant links on the classification. We show how to learn the CDN model effectively and how to use the Gibbs inference framework over the learned model for collective classification of multiple linked objects. The experiments show that the CDN model demonstrates relatively high robustness on data sets containing irrelevant links.

#index 902451
#* Access Structures for Angular Similarity Queries
#@ Tan Apaydin;Hakan Ferhatosmanoglu
#t 2006
#c 7
#% 86950
#% 109206
#% 169805
#% 183360
#% 227924
#% 237187
#% 245787
#% 248796
#% 283716
#% 285932
#% 291942
#% 294834
#% 309129
#% 342656
#% 397398
#% 406493
#% 427199
#% 479462
#% 479649
#% 480133
#% 597321
#% 631955
#% 632035
#% 737451
#% 1390194
#% 1854303
#% 1855293
#! Angular similarity measures have been utilized by several database applications to define semantic similarity between various data types such as text documents, time-series, images, and scientific data. Although similarity searches based on Euclidean distance have been extensively studied in the database community, processing of angular similarity searches has been relatively untouched. Problems due to a mismatch in the underlying geometry as well as the high dimensionality of the data make current techniques either inapplicable or their use results in poor performance. This brings up the need for effective indexing methods for angular similarity queries. We first discuss how to efficiently process such queries and propose effective access structures suited to angular similarity measures. In particular, we propose two classes of access structures, namely, Angular-sweep and Cone-shell, which perform different types of quantization based on the angular orientation of the data objects. We also develop query processing algorithms that utilize these structures as dense indices. The proposed techniques are shown to be scalable with respect to both dimensionality and the size of the data. Our experimental results on real data sets from various applications show two to three orders of magnitude of speedup over the current techniques.

#index 902452
#* On Mining Instance-Centric Classification Rules
#@ Jianyong Wang;George Karypis
#t 2006
#c 7
#% 136350
#% 152934
#% 169719
#% 260001
#% 280436
#% 280488
#% 300120
#% 318412
#% 329598
#% 340905
#% 413590
#% 458257
#% 458379
#% 461909
#% 466483
#% 481290
#% 481949
#% 629642
#% 729941
#% 735356
#% 765413
#% 769889
#% 810064
#! Many studies have shown that rule-based classifiers perform well in classifying categorical and sparse high-dimensional databases. However, a fundamental limitation with many rule-based classifiers is that they find the rules by employing various heuristic methods to prune the search space and select the rules based on the sequential database covering paradigm. As a result, the final set of rules that they use may not be the globally best rules for some instances in the training database. To make matters worse, these algorithms fail to fully exploit some more effective search space pruning methods in order to scale to large databases. In this paper, we present a new classifier, HARMONY, which directly mines the final set of classification rules. HARMONY uses an instance-centric rule-generation approach and it can assure that, for each training instance, one of the highest-confidence rules covering this instance is included in the final rule set, which helps in improving the overall accuracy of the classifier. By introducing several novel search strategies and pruning methods into the rule discovery process, HARMONY also has high efficiency and good scalability. Our thorough performance study with some large text and categorical databases has shown that HARMONY outperforms many well-known classifiers in terms of both accuracy and computational efficiency and scales well with regard to the database size.

#index 902453
#* Design and Performance Evaluation of Broadcast Algorithms for Time-Constrained Data Retrieval
#@ Yu-Chi Chung;Chao-Chun Chen;Chiang Lee
#t 2006
#c 7
#% 201897
#% 259634
#% 274200
#% 279165
#% 281441
#% 290747
#% 309463
#% 331031
#% 430426
#% 442624
#% 464065
#% 482107
#% 536178
#% 564239
#% 615031
#% 622460
#% 632025
#% 632067
#% 635989
#% 778364
#! We refer "time-constrained services” to those requests that have to be replied to within a certain client-expected time duration. If the answer cannot reach the client within this expected time, the value of the information may seriously degrade or even become useless. On-demand channels may not be able to handle all time-constrained services without degrading the performance. How to handle these services in broadcast channels becomes crucial to balance the load of wireless systems. In this paper, we study this problem and find the minimum number of broadcast channels required for such a task. Also, we propose solutions for this problem when the available channels are insufficient. Our performance result reveals that only a moderate number of channels is required to promote these time-constrained services.

#index 902454
#* Hierarchical Indexing Structure for Efficient Similarity Search in Video Retrieval
#@ Hong Lu;Beng Chin Ooi;Heng Tao Shen;Xiangyang Xue
#t 2006
#c 7
#% 194009
#% 249321
#% 249322
#% 264161
#% 342828
#% 422978
#% 435081
#% 437405
#% 443259
#% 451653
#% 479649
#% 479973
#% 480632
#% 522743
#% 565235
#% 632035
#% 729437
#% 745496
#% 810069
#% 1775143
#% 1775156
#% 1775166
#% 1857639
#% 1857658
#% 1857906
#% 1858015
#! With the rapid increase in both centralized video archives and distributed WWW video resources, content-based video retrieval is gaining its importance. To support such applications efficiently, content-based video indexing must be addressed. Typically, each video is represented by a sequence of frames. Due to the high dimensionality of frame representation and the large number of frames, video indexing introduces an additional degree of complexity. In this paper, we address the problem of content-based video indexing and propose an efficient solution, called the Ordered VA-File (OVA-File) based on the VA-file. OVA-File is a hierarchical structure and has two novel features: 1) partitioning the whole file into slices such that only a small number of slices are accessed and checked during k Nearest Neighbor (kNN) search and 2) efficient handling of insertions of new vectors into the OVA-File, such that the average distance between the new vectors and those approximations near that position is minimized. To facilitate a search, we present an efficient approximate kNN algorithm named Ordered VA-LOW (OVA-LOW) based on the proposed OVA-File. OVA-LOW first chooses possible OVA-Slices by ranking the distances between their corresponding centers and the query vector, and then visits all approximations in the selected OVA-Slices to work out approximate kNN. The number of possible OVA-Slices is controlled by a user-defined parameter \delta. By adjusting \delta, OVA-LOW provides a trade-off between the query cost and the result quality. Query by video clip consisting of multiple frames is also discussed. Extensive experimental studies using real video data sets were conducted and the results showed that our methods can yield a significant speed-up over an existing VA-file-based method and iDistance with high query result quality. Furthermore, by incorporating temporal correlation of video content, our methods achieved much more efficient performance.

#index 902455
#* Incremental Processing of Continual Range Queries over Moving Objects
#@ Kun-Lung Wu;Shyh-Kwei Chen;Philip S. Yu
#t 2006
#c 7
#% 13041
#% 68091
#% 206915
#% 252304
#% 273706
#% 295512
#% 299979
#% 300173
#% 300174
#% 315005
#% 415957
#% 427199
#% 442615
#% 461923
#% 464847
#% 480473
#% 564133
#% 576115
#% 654478
#% 740766
#% 765452
#% 765453
#% 765454
#% 773482
#% 800571
#% 800572
#% 810048
#% 846190
#% 863399
#% 975260
#% 993955
#% 1015297
#% 1015320
#% 1016193
#! Efficient processing of continual range queries over moving objects is critically important in providing location-aware services and applications. A set of continual range queries, each defining the geographical region of interest, can be periodically (re)evaluated to locate moving objects that are currently within individual query boundaries. We study a new query indexing method, called CES-based indexing, for incremental processing of continual range queries over moving objects. A set of containment-encoded squares (CES) are predefined, each with a unique ID. CESs are virtual constructs (VC) used to decompose query regions and to store indirectly precomputed search results. Compared with a prior VC-based approach, the number of VCs visited in a search operation is reduced from (4L^{2}-1)/3 to \log(L)+1, where L is the maximal side length of a VC. Search time is hence significantly lowered. Moreover, containment encoding among the CESs makes it easy to identify all those VCs that need not be visited during an incremental query (re)evaluation. We study the performance of CES-based indexing and compare it with a prior VC-based approach.

#index 902456
#* Decentralized Assignment Reasoning Using Collaborative Local Mediation
#@ Kiam Tian Seow;Kwang Mong Sim
#t 2006
#c 7
#% 116
#% 25185
#% 162305
#% 378936
#% 643099
#% 773232
#% 1275312
#% 1386317
#! The collaborative linear assignment problem (CLAP) is a recent framework being developed to provide an intellectual basis for investigating uncluttered agent-based solutions for a fundamental class of combinatorial assignment (or allocation) applications. One key motivation of the research on CLAP is the hope that it can shed new light on adopting agent approaches for solving traditional combinatorial problems in general. To accommodate the various levels of control on agent sociability, typically different application-specific solutions to CLAP are required. In this paper, we take an architectural perspective, classifying solutions according to three typical control structures, namely, centralized, distributed, and decentralized. Existing work focuses mainly on centralized and distributed systems. In this paper, based on the Multi-Agent Assignment Algorithm ({\rm MA}^{3}) used for distributed systems, we propose a new mechanism for a totally decentralized architecture. This proposed mechanism incorporates a novel idea called collaborative Local Mediation (LM), therefore, we term this mechanism {\rm MA}^{3}{\hbox{-}}{\rm{LM}}. We prove that the decentralized {\rm MA}^{3}{\hbox{-}}{\rm{LM}} does not increase the worst-case reasoning complexity when compared to its partially decentralized counterpart. An example illustrates the new mechanism, with emphasis on how it performs collaborative local mediation.

#index 902457
#* Some Effective Techniques for Naive Bayes Text Classification
#@ Sang-Bum Kim;Kyoung-Soo Han;Hae-Chang Rim;Sung Hyon Myaeng
#t 2006
#c 7
#% 127850
#% 165111
#% 246831
#% 260001
#% 280817
#% 311027
#% 311034
#% 458369
#% 458379
#% 458389
#% 465754
#% 465895
#% 565531
#% 779943
#! While naive Bayes is quite effective in various data mining tasks, it shows a disappointing result in the automatic text classification problem. Based on the observation of naive Bayes for the natural language text, we found a serious problem in the parameter estimation process, which causes poor results in text classification domain. In this paper, we propose two empirical heuristics: per-document text normalization and feature weighting method. While these are somewhat ad hoc methods, our proposed naive Bayes text classifier performs very well in the standard benchmark collections, competing with state-of-the-art text classifiers based on a highly complex learning method such as SVM.

#index 902458
#* Regression Cubes with Lossless Compression and Aggregation
#@ Yixin Chen;Guozhu Dong;Jiawei Han;Jian Pei;Benjamin W. Wah;Jianyong Wang
#t 2006
#c 7
#% 190679
#% 210182
#% 223781
#% 227880
#% 273916
#% 333925
#% 333926
#% 333931
#% 420053
#% 428155
#% 459025
#% 477968
#% 480156
#% 480628
#% 480630
#% 480820
#% 481611
#% 481951
#% 832569
#% 846209
#% 993958
#! As OLAP engines are widely used to support multidimensional data analysis, it is desirable to support in data cubes advanced statistical measures, such as regression and filtering, in addition to the traditional simple measures such as count and average. Such new measures will allow users to model, smooth, and predict the trends and patterns of data. Existing algorithms for simple distributive and algebraic measures are inadequate for efficient computation of statistical measures in a multidimensional space. In this paper, we propose a fundamentally new class of measures, compressible measures, in order to support efficient computation of the statistical models. For compressible measures, we compress each cell into an auxiliary matrix with a size independent of the number of tuples. We can then compute the statistical measures for any data cell from the compressed data of the lower-level cells without accessing the raw data. Time- and space-efficient lossless aggregation formulae are derived for regression and filtering measures. Our analytical and experimental studies show that the resulting system, regression cube, substantially reduces the memory usage and the overall response time for statistical analysis of multidimensional data.

#index 902459
#* Multi-Output Regularized Feature Projection
#@ Shipeng Yu;Kai Yu;Volker Tresp;Hans-Peter Kriegel
#t 2006
#c 7
#% 190581
#% 200694
#% 235342
#% 266281
#% 266426
#% 269226
#% 722809
#% 743284
#% 763698
#% 763708
#! Dimensionality reduction by feature projection is widely used in pattern recognition, information retrieval, and statistics. When there are some outputs available (e.g., regression values or classification results), it is often beneficial to consider supervised projection, which is based not only on the inputs, but also on the target values. While this applies to a single-output setting, we are more interested in applications with multiple outputs, where several tasks need to be learned simultaneously. In this paper, we introduce a novel projection approach called Multi-Output Regularized feature Projection (MORP), which preserves the information of input features and, meanwhile, captures the correlations between inputs/outputs and (if applicable) between multiple outputs. This is done by introducing a latent variable model on the joint input-output space and minimizing the reconstruction errors for both inputs and outputs. It turns out that the mappings can be found by solving a generalized eigenvalue problem and are ready to extend to nonlinear mappings. Prediction accuracy can be greatly improved by using the new features since the structure of outputs is explored. We validate our approach in two applications. In the first setting, we predict users' preferences for a set of paintings. The second is concerned with image and text categorization where each image (or document) may belong to multiple categories. The proposed algorithm produces very encouraging results in both settings.

#index 902460
#* Structured Data Extraction from the Web Based on Partial Tree Alignment
#@ Yanhong Zhai;Bing Liu
#t 2006
#c 7
#% 271065
#% 273925
#% 289193
#% 289335
#% 330784
#% 348146
#% 413572
#% 480648
#% 480824
#% 577319
#% 643004
#% 654469
#% 729978
#% 754078
#% 754108
#% 763066
#% 765411
#% 805845
#% 805846
#% 805847
#% 1683893
#! This paper studies the problem of structured data extraction from arbitrary Web pages. The objective of the proposed research is to automatically segment data records in a page, extract data items/fields from these records, and store the extracted data in a database. Existing methods addressing the problem can be classified into three categories. Methods in the first category provide some languages to facilitate the construction of data extraction systems. Methods in the second category use machine learning techniques to learn wrappers (which are data extraction programs) from human labeled examples. Manual labeling is time-consuming and is hard to scale to a large number of sites on the Web. Methods in the third category are based on the idea of automatic pattern discovery. However, multiple pages that conform to a common schema are usually needed as the input. In this paper, we propose a novel and effective technique (called DEPTA) to perform the task of Web data extraction automatically. The method consists of two steps: 1) identifying individual records in a page and 2) aligning and extracting data items from the identified records. For step 1, a method based on visual information and tree matching is used to segment data records. For step 2, a novel partial alignment technique is proposed. This method aligns only those data items in a pair of records that can be aligned with certainty, making no commitment on the rest of the items. Experimental results obtained using a large number of Web pages from diverse domains show that the proposed two-step technique is highly effective.

#index 902461
#* Cache-Conscious Automata for XML Filtering
#@ Bingsheng He;Qiong Luo;Byron Choi
#t 2006
#c 7
#% 67164
#% 274595
#% 300194
#% 321250
#% 479819
#% 480296
#% 480821
#% 526938
#% 570879
#% 659987
#% 731408
#% 800591
#% 993947
#% 1015288
#% 1015289
#! Hardware cache behavior is an important factor in the performance of memory-resident, data-intensive systems such as XML filtering engines. A key data structure in several recent XML filters is the automaton, which is used to represent the long-running XML queries in the main memory. In this paper, we study the cache performance of automaton-based XML filtering through analytical modeling and system measurement. Furthermore, we propose a cache-conscious automaton organization technique, called the hot buffer, to improve the locality of automaton state transitions. Our results show that 1) our cache performance model for XML filtering automata is highly accurate and 2) the hot buffer improves the cache performance as well as the overall performance of automaton-based XML filtering.

#index 902462
#* Continuous Skyline Queries for Moving Objects
#@ Zhiyong Huang;Hua Lu;Beng Chin Ooi;Anthony K. H. Tung
#t 2006
#c 7
#% 201876
#% 273706
#% 282343
#% 285932
#% 287466
#% 288976
#% 289148
#% 299979
#% 300174
#% 378405
#% 397377
#% 480671
#% 554738
#% 574283
#% 654478
#% 654480
#% 806212
#% 993954
#% 1015297
#% 1016193
#! The literature on skyline algorithms has so far dealt mainly with queries of static query points over static data sets. With the increasing number of mobile service applications and users, however, the need for continuous skyline query processing has become more pressing. A continuous skyline query involves not only static dimensions, but also the dynamic one. In this paper, we examine the spatiotemporal coherence of the problem and propose a continuous skyline query processing strategy for moving query points. First, we distinguish the data points that are permanently in the skyline and use them to derive a search bound. Second, we investigate the connection between the spatial positions of data points and their dominance relationship, which provides an indication of where to find changes in the skyline and how to maintain the skyline continuously. Based on the analysis, we propose a kinetic-based data structure and an efficient skyline query processing algorithm. We concisely analyze the space and time costs of the proposed method and conduct an extensive experiment to evaluate the method. To the best of our knowledge, this is the first work on continuous skyline query processing.

#index 902463
#* Logic and Computational Complexity for Boolean Information Retrieval
#@ Manolis Koubarakis;Spiros Skiadopoulos;Christos Tryfonopoulos
#t 2006
#c 7
#% 186336
#% 228800
#% 237053
#% 267451
#% 307470
#% 340911
#% 387427
#% 429749
#% 443052
#% 504581
#% 508414
#% 543556
#% 644201
#% 657754
#% 723452
#% 750867
#% 754116
#% 766446
#% 818243
#% 1709393
#% 1712586
#! We study the complexity of query satisfiability and entailment for the Boolean Information Retrieval models {\cal WP} and {\cal AWP} using techniques from propositional logic and computational complexity. {\cal WP} and {\cal AWP} can be used to represent and query textual information under the Boolean model using the concept of attribute with values of type text, the concept of word, and word proximity constraints. Variations of {\cal WP} and {\cal AWP} are in use in most deployed digital libraries using the Boolean model, text extenders for relational database systems (e.g., Oracle 10g), search engines, and P2P systems for information retrieval and filtering.

#index 902464
#* New Algorithm for Computing Cube on Very Large Compressed Data Sets
#@ Weili Wu;Hong Gao;Jianzhong Li
#t 2006
#c 7
#% 2248
#% 210182
#% 211575
#% 333926
#% 397388
#% 420053
#% 465033
#% 479450
#% 479812
#% 481951
#% 482082
#% 511660
#% 654446
#% 1016174
#! Data compression is an effective technique to improve the performance of data warehouses. Since cube operation represents the core of online analytical processing in data warehouses, it is a major challenge to develop efficient algorithms for computing cube on compressed data warehouses. To our knowledge, very few cube computation techniques have been proposed for compressed data warehouses to date in the literature. This paper presents a novel algorithm to compute cubes on compressed data warehouses. The algorithm operates directly on compressed data sets without the need of first decompressing them. The algorithm is applicable to a large class of mapping complete data compression methods. The complexity of the algorithm is analyzed in detail. The analytical and experimental results show that the algorithm is more efficient than all other existing cube algorithms. In addition, a heuristic algorithm to generate an optimal plan for computing cube is also proposed.

#index 902465
#* IPSS: A Hybrid Approach to Planning and Scheduling Integration
#@ Maria Dolores Rodriguez-Moreno;Angelo Oddi;Daniel Borrajo;Amedeo Cesta
#t 2006
#c 7
#% 421314
#% 618512
#% 1272014
#% 1272016
#% 1272311
#% 1272333
#% 1289205
#! Recently, the areas of planning and scheduling in Artificial Intelligence (AI) have witnessed a big push toward their integration in order to solve complex problems. These problems require both reasoning on which actions are to be performed as well as their precedence constraints (planning) and the reasoning with respect to temporal constraints (e.g., duration, precedence, and deadline); those actions should satisfy the resources they use (scheduling). This paper describes ipss (Integrated Planning and Scheduling System), a domain independent solver that integrates an AI planner that synthesizes courses of actions with constraint-based techniques that reason based upon time and resources. ipss is able to manage not only simple precedence constraints, but also more complex temporal requirements (as the Allen primitives) and multicapacity resource usage/consumption. The solver is evaluated against a set of problems characterized by the use of multiple agents (or multiple resources) that have to perform tasks with some temporal restrictions in the order of the tasks or some constraints in the availability of the resources. Experiments show how the integrated reasoning approach improves plan parallelism and gains better makespans than some state-of-the-art planners where multiple agents are represented as additional fluents in the problem operators. It also shows that ipss is suitable for solving real domains (i.e., workflow problems) because it is able to impose temporal windows on the goals or set a maximum makespan, features that most of the planners do not yet incorporate.

#index 902466
#* Approaches to Multisensor Data Fusion in Target Tracking: A Survey
#@ Duncan Smith;Sameer Singh
#t 2006
#c 7
#% 278986
#% 414174
#% 444055
#% 639291
#% 721148
#% 751031
#% 751053
#% 793871
#% 991033
#! The tracking of objects using distributed multiple sensors is an important field of work in the application areas of autonomous robotics, military applications, and mobile systems. In this survey, we review a number of computationally intelligent methods that are used for developing robust tracking schemes through sensor data fusion. The survey discusses the application of the various algorithms at different layers of the JDL model and highlights the weaknesses and strengths of the approaches in the context of different applications.

#index 902467
#* Generating Queries with Cardinality Constraints for DBMS Testing
#@ Nicolas Bruno;Surajit Chaudhuri;Dilys Thomas
#t 2006
#c 7
#% 210190
#% 479656
#% 824744
#% 1016216
#! Good testing coverage of novel database techniques, such as multidimensional histograms or changes in the execution engine, is a complex problem. In this work, we argue that this task requires generating query instances, not randomly, but based on a given set of constraints. Specifically, obtaining query instances that satisfy cardinality constraints on their subexpressions is an important challenge. We show that this problem is inherently hard, and develop heuristics that effectively find approximate solutions.

#index 902468
#* An Overlay Subscription Network for Live Internet TV Broadcast
#@ Ying Cai;Jianming Zhou
#t 2006
#c 7
#% 173593
#% 212727
#% 236747
#% 261877
#% 309516
#% 340168
#% 341920
#% 345959
#% 345964
#% 443443
#% 722158
#% 754144
#% 1863125
#! We propose a framework, called Overlay Subscription Network (OSN), for live Internet TV broadcast, where a subscriber can choose to watch at any time. This framework allows the source server to incrementally build a topology graph that contains the network connections not only from the server to each subscriber, but also among the subscribers themselves. With such a topology graph in place, we consider efficient overlay multicast for scalable OSN services. We first show that idling nodes, which do not receive video data for their own playback, can actually be used for data forwarding to significantly reduce the cost of overlay multicast. In light of this observation, we then propose a novel overlay multicast technique that distinguishes itself from existing schemes with these three aspects. First, the proposed technique is centered on the topology graph and can take advantage of the actual network connections among the subscribing nodes. Second, the new scheme is able to find and incorporate appropriate idling nodes in multicast to reduce network traffic. Third, with our approach, a node can be used in multiple multicast trees for data forwarding to improve the overall system performance. We evaluate the performance of the proposed technique through simulation. Our extensive studies show that the proposed framework has the potential to enable the Internet, a vehicle up to date mainly for transferring text and image data, for large-scale and cost-effective TV broadcast.

#index 913783
#* Duplicate Record Detection: A Survey
#@ Ahmed K. Elmagarmid;Panagiotis G. Ipeirotis;Vassilios S. Verykios
#t 2007
#c 7
#% 58593
#% 120648
#% 120649
#% 121278
#% 122671
#% 126332
#% 131061
#% 170649
#% 199537
#% 229828
#% 232117
#% 248801
#% 249989
#% 251405
#% 252011
#% 255137
#% 269217
#% 300176
#% 310516
#% 310533
#% 311027
#% 312072
#% 312166
#% 314740
#% 328186
#% 333679
#% 333943
#% 340887
#% 348164
#% 350103
#% 397369
#% 398003
#% 420072
#% 462352
#% 463445
#% 465747
#% 466892
#% 480496
#% 480499
#% 480654
#% 547438
#% 577238
#% 577247
#% 577263
#% 577309
#% 577522
#% 596205
#% 654454
#% 654467
#% 659991
#% 765463
#% 765548
#% 768939
#% 769877
#% 770844
#% 788090
#% 800590
#% 843716
#% 870896
#% 922779
#% 993980
#% 1016182
#% 1016219
#! Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area.

#index 913784
#* A Distribution-Index-Based Discretizer for Decision-Making with Symbolic AI Approaches
#@ QingXiang Wu;David A. Bell;Girijesh Prasad;Thomas Martin McGinnity
#t 2007
#c 7
#% 315438
#% 376266
#% 443510
#% 449588
#% 733623
#% 758424
#% 793440
#% 1408579
#! When symbolic AI approaches are applied to handle continuous valued attributes, there is a requirement to transform the continuous attribute values to symbolic data. In this paper, a novel distribution-index-based discretizer is proposed for such a transformation. Based on definitions of dichotomic entropy and a compound distributional index, a simple criterion is applied to discretize continuous attributes adaptively. The dichotomic entropy indicates the homogeneity degree of the decision value distribution, and is applied to determine the best splitting point. The compound distributional index combines both the homogeneity degrees of attribute value distributions and the decision value distribution, and is applied to determine which interval should be split further; thus, a potentially improved solution of the discretization problem can be found efficiently. Based on multiple reducts in rough set theory, a multiknowledge approach can attain high decision accuracy for information systems with a large number of attributes and missing values. In this paper, our discretizer is combined with the multiknowledge approach to further improve decision accuracy for information systems with continuous attributes. Experimental results on benchmark data sets show that the new discretizer can improve not only the multiknowledge approach, but also the naïve Bayes classifier and the C5.0 tree.

#index 913785
#* Hiding Sensitive Association Rules with Limited Side Effects
#@ Yi-Hung Wu;Chia-Ming Chiang;Arbee L. P. Chen
#t 2007
#c 7
#% 152934
#% 232136
#% 300184
#% 333876
#% 359132
#% 428404
#% 443350
#% 443448
#% 443463
#% 481290
#% 488478
#% 539744
#% 577234
#% 577239
#% 577289
#% 586838
#% 635220
#% 664070
#% 729418
#% 740764
#% 742048
#% 751578
#% 993988
#! Data mining techniques have been widely used in various applications. However, the misuse of these techniques may lead to the disclosure of sensitive information. Researchers have recently made efforts at hiding sensitive association rules. Nevertheless, undesired side effects, e.g., nonsensitive rules falsely hidden and spurious rules falsely generated, may be produced in the rule hiding process. In this paper, we present a novel approach that strategically modifies a few transactions in the transaction database to decrease the supports or confidences of sensitive rules without producing the side effects. Since the correlation among rules can make it impossible to achieve this goal, in this paper, we propose heuristic methods for increasing the number of hidden sensitive rules and reducing the number of modified entries. The experimental results show the effectiveness of our approach, i.e., undesired side effects are avoided in the rule hiding process. The results also report that in most cases, all the sensitive rules are hidden without spurious rules falsely generated. Moreover, the good scalability of our approach in terms of database size and the influence of the correlation among rules on rule hiding are observed.

#index 913786
#* Online Random Shuffling of Large Database Tables
#@ Christopher Jermaine
#t 2007
#c 7
#% 77967
#% 190611
#% 208047
#% 210173
#% 227883
#% 273910
#% 280406
#% 387508
#% 397370
#% 464204
#% 464705
#% 466908
#% 479470
#% 479974
#% 480953
#% 481304
#% 503535
#% 722802
#% 722920
#% 735358
#% 765424
#% 765425
#% 992831
#% 1016146
#! Many applications require a randomized ordering of input data. Examples include algorithms for online aggregation, data mining, and various randomized algorithms. Most existing work seems to assume that accessing the records from a large database in a randomized order is not a difficult problem. However, it turns out to be extremely difficult in practice. Using existing methods, randomization is either extremely expensive at the front end (as data are loaded), or at the back end (as data are queried). This paper presents a simple file structure which supports both efficient, online random shuffling of a large database, as well as efficient online sampling or randomization of the database when it is queried. The key innovation of our method is the introduction of a small degree of carefully controlled, rigorously monitored nonrandomness into the file.

#index 913787
#* Extracting Actionable Knowledge from Decision Trees
#@ Qiang Yang;Jie Yin;Charles Ling;Rong Pan
#t 2007
#c 7
#% 136350
#% 190581
#% 215664
#% 217812
#% 280437
#% 286669
#% 309208
#% 342611
#% 388922
#% 400985
#% 402373
#% 408396
#% 443082
#% 443086
#% 443313
#% 466086
#% 466268
#% 481290
#% 577237
#% 629609
#% 629717
#% 727833
#% 727852
#% 796197
#% 819448
#% 844380
#% 864403
#% 1086611
#% 1289281
#! Most data mining algorithms and tools stop at discovered customer models, producing distribution information on customer profiles. Such techniques, when applied to industrial problems such as customer relationship management (CRM), are useful in pointing out customers who are likely attritors and customers who are loyal, but they require human experts to postprocess the discovered knowledge manually. Most of the postprocessing techniques have been limited to producing visualization results and interestingness ranking, but they do not directly suggest actions that would lead to an increase in the objective function such as profit. In this paper, we present novel algorithms that suggest actions to change customers from an undesired status (such as attritors) to a desired one (such as loyal) while maximizing an objective function: the expected net profit. These algorithms can discover cost-effective actions to transform customers from undesirable classes to desirable ones. The approach we take integrates data mining and decision making tightly by formulating the decision making problems directly on top of the data mining results in a postprocessing step. To improve the effectiveness of the approach, we also present an ensemble of decision trees which is shown to be more robust when the training data changes. Empirical tests are conducted on both a realistic insurance application domain and UCI benchmark data.

#index 913788
#* Incremental Evaluation of Sliding-Window Queries over Data Streams
#@ Thanaa M. Ghanem;Moustafa A. Hammad;Mohamed F. Mokbel;Walid G. Aref;Ahmed K. Elmagarmid
#t 2007
#c 7
#% 13016
#% 116082
#% 201929
#% 248795
#% 273911
#% 300179
#% 378388
#% 578391
#% 578560
#% 654462
#% 654508
#% 654510
#% 726621
#% 726622
#% 745434
#% 765436
#% 771230
#% 801694
#% 810063
#% 993948
#% 1015278
#% 1015324
#% 1016156
#% 1016157
#! Two research efforts have been conducted to realize sliding-window queries in data stream management systems, namely, query reevaluation and incremental evaluation. In the query reevaluation method, two consecutive windows are processed independently of each other. On the other hand, in the incremental evaluation method, the query answer for a window is obtained incrementally from the answer of the preceding window. In this paper, we focus on the incremental evaluation method. Two approaches have been adopted for the incremental evaluation of sliding-window queries, namely, the input-triggered approach and the negative tuples approach. In the input-triggered approach, only the newly inserted tuples flow in the query pipeline and tuple expiration is based on the timestamps of the newly inserted tuples. On the other hand, in the negative tuples approach, tuple expiration is separated from tuple insertion where a tuple flows in the pipeline for every inserted or expired tuple. The negative tuples approach avoids the unpredictable output delays that result from the input-triggered approach. However, negative tuples double the number of tuples through the query pipeline, thus reducing the pipeline bandwidth. Based on a detailed study of the incremental evaluation pipeline, we classify the incremental query operators into two classes according to whether an operator can avoid the processing of negative tuples or not. Based on this classification, we present several optimization techniques over the negative tuples approach that aim to reduce the overhead of processing negative tuples while avoiding the output delay of the query answer. A detailed experimental study, based on a prototype system implementation, shows the performance gains over the input-triggered approach of the negative tuples approach when accompanied with the proposed optimizations.

#index 913789
#* Resource Discovery in a European Spatial Data Infrastructure
#@ Paul C. Smits;Anders Friis-Christensen
#t 2007
#c 7
#% 86528
#% 204430
#% 212683
#% 290703
#% 438548
#% 445169
#% 452869
#% 773826
#% 773897
#% 1069164
#% 1778472
#% 1778583
#! The geospatial community is moving toward distributed databases and Web services by following the general developments in information and communication technology. The sharing of resources across multiple information communities raises the need of new technologies that support resource discovery and information retrieval. This paper investigates if a common ontology is desirable and feasible for information retrieval in a European Spatial Data Infrastructure. It does so by reviewing relevant literature and proposes an approach for the automatic updating of existing ontologies, designed to facilitate access to multilingual descriptors of geospatial resources. We demonstrate by means of a prototype of an experimental system that the proposed approach is feasible. The experimental system is unique because it integrates a gazetteer, the EuroVoc multilingual vocabulary, the GEMET multilingual thesaurus, an automatic concept space generator, and graph matching into one system. Based on our study, we conclude that it will be impractical to rely only on one common ontology for resource discovery. We also conclude that the approach of using human-created ontologies in combination with automatic concept space generation and associative retrieval is a powerful means to the discovery of geospatial resources. In the absence of a consistent use of semantic Web technologies, a centralized approach to indexing of metadata is required, which has consequences for architectural choices.

#index 913790
#* Random Sampling for Continuous Streams with Arbitrary Updates
#@ Yufei Tao;Xiang Lian;Dimitris Papadias;Marios Hadjieleftheriou
#t 2007
#c 7
#% 1331
#% 164361
#% 178991
#% 210188
#% 248812
#% 273682
#% 273908
#% 273909
#% 300195
#% 333955
#% 379444
#% 411355
#% 464987
#% 654463
#% 654486
#% 765424
#% 765425
#% 765426
#% 765453
#% 808428
#% 824653
#% 1015280
#% 1015328
#% 1016153
#% 1016154
#% 1016156
#! The existing random sampling methods have at least one of the following disadvantages: they 1) are applicable only to certain update patterns, 2) entail large space overhead, or 3) incur prohibitive maintenance cost. These drawbacks prevent their effective application in stream environments (where a relation is updated by a large volume of insertions and deletions that may arrive in any order), despite the considerable success of random sampling in conventional databases. Motivated by this, we develop several fully dynamic algorithms for obtaining random samples from individual relations, and from the join result of two tables. Our solutions can handle any update pattern with small space and computational overhead. We also present an in-depth analysis that provides valuable insight into the characteristics of alternative sampling strategies and leads to precision guarantees. Extensive experiments validate our theoretical findings and demonstrate the efficiency of our techniques in practice.

#index 913791
#* Computing Iceberg Cubes by Top-Down and Bottom-Up Integration: The StarCubing Approach
#@ Dong Xin;Jiawei Han;Xiaolei Li;Zheng Shao;Benjamin W. Wah
#t 2007
#c 7
#% 210182
#% 227880
#% 236410
#% 248785
#% 259995
#% 273916
#% 280448
#% 300120
#% 333925
#% 397388
#% 420053
#% 420141
#% 459025
#% 462204
#% 464706
#% 479450
#% 479476
#% 479646
#% 481290
#% 481951
#% 654446
#% 660006
#% 810029
#% 818916
#% 864391
#% 993958
#% 993996
#% 1016173
#% 1016174
#! Data cube computation is one of the most essential but expensive operations in data warehousing. Previous studies have developed two major approaches, top-down versus bottom-up. The former, represented by the MultiWay Array Cube (called the MultiWay) algorithm [30], aggregates simultaneously on multiple dimensions; however, it cannot take advantage of a priori pruning [2] when computing iceberg cubes (cubes that contain only aggregate cells whose measure values satisfy a threshold, called the iceberg condition). The latter, represented by BUC [6] , computes the iceberg cube bottom-up and facilitates a priori pruning. BUC explores fast sorting and partitioning techniques; however, it does not fully explore multidimensional simultaneous aggregation. In this paper, we present a new method, Star-Cubing, that integrates the strengths of the previous two algorithms and performs aggregations on multiple dimensions simultaneously. It utilizes a star-tree structure, extends the simultaneous aggregation methods, and enables the pruning of the group-bys that do not satisfy the iceberg condition. Our performance study shows that Star-Cubing is highly efficient and outperforms the previous methods.

#index 913792
#* A Flexible Content Adaptation System Using a Rule-Based Approach
#@ Jiang He;Tong Gao;Wei Hao;I-Ling Yen;Farokh Bastani
#t 2007
#c 7
#% 170375
#% 218253
#% 297600
#% 308616
#% 308742
#% 309736
#% 330780
#% 330783
#% 342962
#% 345354
#% 364906
#% 396987
#% 428369
#% 433361
#% 577321
#% 589982
#% 762874
#% 789503
#% 832354
#% 899262
#% 1112856
#% 1775058
#% 1775085
#% 1775262
#% 1780827
#% 1788058
#% 1788077
#% 1855372
#% 1898231
#! Content adaptation is an important technique for mobile devices. Existing content adaptation systems have been developed with specific adaptation goals. In this paper, we present an extensible content adaptation system, Xadaptor. We take a rule-based approach to facilitate extensible, systematic, and adaptive content adaptation. It integrates adaptation mechanisms for various content types and organizes them into the rule base. Rules are invoked based on the individual client information. We classify HTML page objects into structure, content, and pointer objects. Existing content adaptation techniques mainly focus on content objects and do not consider adaptation for structure and pointer objects. In Xadaptor, novel adaptation techniques for the structure object HTML table have been developed. We use fuzzy logic to model the adaptation quality and guide the adaptation decision. To demonstrate the feasibility of our approach, we have implemented a prototype system. Experimental studies show that Xadaptor is capable of on-the-fly content adaptation and is easily extensible.

#index 913793
#* Editorial: Revisiting the (Machine) Semantic Web: The Missing Layers for the Human Semantic Web
#@ Gottfried Vossen;Miltiadis Lytras;Nick Koudas
#t 2007
#c 7
#% 936403
#! First Page of the Article

#index 913794
#* Bottom-Up Extraction and Trust-Based Refinement of Ontology Metadata
#@ Paolo Ceravolo;Ernesto Damiani;Marco Viviani
#t 2007
#c 7
#% 36672
#% 40313
#% 118771
#% 188062
#% 198016
#% 222206
#% 224592
#% 230386
#% 279123
#% 306885
#% 379449
#% 413620
#% 445448
#% 511913
#% 529190
#% 577367
#% 589388
#% 641975
#% 644330
#% 668675
#% 799891
#% 813966
#% 853572
#% 1346779
#% 1388160
#% 1713836
#% 1788206
#% 1788934
#! We present a way of building ontologies that proceeds in a bottom-up fashion, defining concepts as clusters of concrete XML objects. Our rough bottom-up ontologies are based on simple relations like association and inheritance, as well as on value restrictions, and can be used to enrich and update existing upper ontologies. Then, we show how automatically generated assertions based on our bottom-up ontologies can be associated with a flexible degree of trust by nonintrusively collecting user feedback in the form of implicit and explicit votes. Dynamic trust-based views on assertions automatically filter out imprecisions and substantially improve metadata quality in the long run.

#index 913795
#* Mining Generalized Associations of Semantic Relations from Textual Web Content
#@ Tao Jiang;Ah-Hwee Tan;Ke Wang
#t 2007
#c 7
#% 2298
#% 152934
#% 198058
#% 280512
#% 300120
#% 350914
#% 379333
#% 384416
#% 392618
#% 445309
#% 466331
#% 481290
#% 481754
#% 481758
#% 481779
#% 577214
#% 705443
#% 729933
#% 742368
#% 748722
#% 756964
#% 785396
#% 786523
#! Traditional text mining techniques transform free text into flat bags of words representation, which does not preserve sufficient semantics for the purpose of knowledge discovery. In this paper, we present a two-step procedure to mine generalized associations of semantic relations conveyed by the textual content of Web documents. First, RDF (Resource Description Framework) metadata representing semantic relations are extracted from raw text using a myriad of natural language processing techniques. The relation extraction process also creates a term taxonomy in the form of a sense hierarchy inferred from WordNet. Then, a novel generalized association pattern mining algorithm (GP-Close) is applied to discover the underlying relation association patterns on RDF metadata. For pruning the large number of redundant overgeneralized patterns in relation pattern search space, the GP-Close algorithm adopts the notion of generalization closure for systematic overgeneralization reduction. The efficacy of our approach is demonstrated through empirical experiments conducted on an online database of terrorist activities.

#index 913796
#* A Taxonomy Learning Method and Its Application to Characterize a Scientific Web Community
#@ Paola Velardi;Alessandro Cucchiarelli;Michael Petit
#t 2007
#c 7
#% 41347
#% 151777
#% 238621
#% 350914
#% 406493
#% 452869
#% 756964
#% 766458
#% 786523
#% 814007
#% 815295
#% 816189
#% 843739
#% 854663
#% 939847
#! The need to extract and manage domain-specific taxonomies has become increasingly relevant in recent years. A taxonomy is a form of business intelligence used to integrate information, reduce semantic heterogeneity, describe emergent communities and interest groups, and facilitate communication between information systems. We present a semiautomated strategy to extract domain-specific taxonomies from Web documents and its application to model a Network of Excellence in the emerging research field of enterprise interoperability.

#index 913797
#* RDFS(FA): Connecting RDF(S) and OWL DL
#@ Jeff Z. Pan;Ian Horrocks
#t 2007
#c 7
#% 140407
#% 198058
#% 330715
#% 478260
#% 519552
#% 561740
#% 577304
#% 665859
#% 869664
#% 913801
#% 1289438
#% 1374370
#% 1388076
#% 1655420
#% 1702405
#! Semantic Web (SW) languages are supposed to be compatible with each other in a meaningful way, so as to facilitate machine understanding. Recent research, however, shows that the semantics of the standard SW annotation language RDF (as well as its ontological extension RDFS) and that of the standard SW ontology language OWL DL are not compatible with each other. This paper investigates some issues behind this incompatibility and proposes a novel modification of RDF(S) as a firm semantic foundation for many of the latest Description Logics-based SW ontology languages, including OWL DL. Furthermore, the bidirectional one-to-one mapping between RDFS(FA) axioms in strata 0-2 and OWL DL axioms has been established, which enables RDFS(FA)-agents and OWL DL-agents to communicate with each other more easily. As a result, the introduction of RDFS(FA) clarifies the vision of the Semantic Web and solidifies RDF(S)'s proposed role as the base of the Semantic Web.

#index 913798
#* Introducing Time into RDF
#@ Claudio Gutierrez;Carlos A. Hurtado;Alejandro Vaisman
#t 2007
#c 7
#% 213959
#% 299489
#% 361445
#% 462230
#% 464720
#% 480129
#% 480827
#% 509845
#% 577347
#% 665630
#% 743023
#% 787631
#% 801677
#% 1015307
#% 1016147
#% 1702401
#! The Resource Description Framework (RDF) is a metadata model and language recommended by the W3C. This paper presents a framework to incorporate temporal reasoning into RDF, yielding temporal RDF graphs. We present a semantics for these kinds of graphs which includes the notion of temporal entailment and a syntax to incorporate this framework into standard RDF graphs, using the RDF vocabulary plus temporal labels. We give a characterization of temporal entailment in terms of RDF entailment and show that the former does not yield extra asymptotic complexity with respect to nontemporal RDF graphs. We also discuss temporal RDF graphs with anonymous timestamps, providing a theoretical framework for the study of temporal anonymity. Finally, we sketch a temporal query language for RDF, along with complexity results for query evaluation that show that the time dimension preserves the tractability of answers.

#index 913799
#* Interoperability Support between MPEG-7/21 and OWL in DS-MIRF
#@ Chrisa Tsinaraki;Panagiotis Polydoros;Stavros Christodoulakis
#t 2007
#c 7
#% 445570
#% 534148
#% 737454
#% 864922
#% 1112383
#% 1112566
#% 1389554
#! In this paper, we focus on interoperable semantic multimedia services that are offered in open environments such as the Internet. The use of well-accepted standards is of paramount importance for interoperability support in open environments. In addition, the semantic description of multimedia content utilizing domain ontologies is very useful for indexing, query specification, retrieval, filtering, user interfaces, and knowledge extraction from audiovisual material. With the MPEG-7 and MPEG-21 standards dominating the multimedia content and service description domain and OWL dominating the ontology description languages, it is important to establish a framework that allows these standards to interoperate. We describe here the DS-MIRF Framework, a software engineering framework that facilitates the development of knowledge-based multimedia applications such as multimedia information retrieval, filtering, browsing, interaction, knowledge extraction, segmentation, and content description. DS-MIRF supports interoperability of OWL with the MPEG-7/21 so that domain and application ontologies expressed in OWL can be transparently integrated with MPEG-7/21 metadata. This allows applications that recognize and use the constructs provided by MPEG-7/21 to make use of domain and application ontologies, resulting in more effective retrieval and user interaction with the audiovisual material. We also present a retrieval evaluation methodology and comparative retrieval results.

#index 913800
#* DR-Prolog: A System for Defeasible Reasoning with Rules and Ontologies on the Semantic Web
#@ Grigoris Antoniou;Antonis Bikakis
#t 2007
#c 7
#% 1146
#% 103705
#% 167542
#% 208197
#% 244373
#% 330234
#% 340563
#% 346442
#% 383293
#% 429214
#% 464934
#% 519567
#% 529674
#% 577305
#% 577336
#% 657371
#% 778288
#% 785575
#% 795329
#% 796785
#% 834129
#% 858817
#% 904768
#% 1374387
#% 1374389
#! Nonmonotonic rule systems are expected to play an important role in the layered development of the Semantic Web. Defeasible reasoning is a direction in nonmonotonic reasoning that is based on the use of rules that may be defeated by other rules. It is a simple, but often more efficient approach than other nonmonotonic rule systems for reasoning with incomplete and inconsistent information. This paper reports on the implementation of a system for defeasible reasoning on the Web. The system 1) is syntactically compatible with RuleML, 2) features strict and defeasible rules, priorities, and two kinds of negation, 3) is based on a translation to logic programming with declarative semantics, 4) is flexible and adaptable to different intuitions within defeasible reasoning, and 5) can reason with rules, RDF, RDF Schema, and (parts of) OWL ontologies.

#index 913801
#* A Flexible Ontology Reasoning Architecture for the Semantic Web
#@ Jeff Z. Pan
#t 2007
#c 7
#% 478260
#% 496265
#% 509706
#% 519438
#% 561709
#% 561740
#% 577335
#% 665859
#% 671280
#% 1269453
#% 1289174
#% 1289422
#% 1388076
#% 1702405
#% 1726001
#! Knowledge-based systems in the Semantic Web era can make use of the power of the Semantic Web languages and technologies, in particular those related to ontologies. Recent research has shown that user-defined data types are very useful for Semantic Web and ontology applications. The W3C Semantic Web Best Practices and Development Working Group has set up a task force to address this issue. Very recently, OWL-Eu and OWL-E, two decidable extensions of the W3C standard ontology language OWL DL, have been proposed to support customized data types and customized data type predicates, respectively. In this paper, we propose a flexible reasoning architecture for these two expressive Semantic Web ontology languages and describe our prototype implementation of the reasoning architecture, based on the well-known FaCT DL reasoner, which witnesses the two key flexibility features of our proposed architecture: 1) It allows users to define their own data types and data type predicates based on built-in ones and 2) new data type reasoners can be added into the architecture without having to change the concept reasoner.

#index 913802
#* An Adaptation of the Vector-Space Model for Ontology-Based Information Retrieval
#@ Pablo Castells;Miriam Fernandez;David Vallet
#t 2007
#c 7
#% 229247
#% 232703
#% 348181
#% 406493
#% 413603
#% 420521
#% 434033
#% 445309
#% 459490
#% 577373
#% 719454
#% 754095
#% 771571
#% 782761
#% 1374367
#% 1671766
#% 1702425
#% 1742130
#! Semantic search has been one of the motivations of the Semantic Web since it was envisioned. We propose a model for the exploitation of ontology-based knowledge bases to improve search over large document repositories. In our view of Information Retrieval on the Semantic Web, a search engine returns documents rather than, or in addition to, exact values in response to user queries. For this purpose, our approach includes an ontology-based scheme for the semiautomatic annotation of documents and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with conventional keyword-based retrieval to achieve tolerance to knowledge base incompleteness. Experiments are shown where our approach is tested on corpora of significant scale, showing clear improvements with respect to keyword-based search.

#index 913803
#* A Relation-Based Search Engine in Semantic Web
#@ Yufei Li;Yuan Wang;Xiaotao Huang
#t 2007
#c 7
#% 268087
#% 290830
#% 445449
#% 445512
#% 773851
#% 783560
#% 1768592
#! With the development of the Web, an information "Big Bang” has taken place on the Internet. Search engines have become one of the most helpful tools for obtaining useful information from the Internet. However, instead of caring about the semantics of information, the machine on the current Web cares about the location and display of information only. Because of this shortcoming of the current Web, the search results by even the most popular search engines cannot produce satisfactory results. The development of the next generation Web, Semantic Web, will turn the situation around completely. This paper proposes a prototype relation-based search engine, "OntoLook,” which has been implemented in a virtual Semantic Web environment in our lab. We also present its system architecture and analyze the key algorithm.

#index 913804
#* A Semantic Web-Based Approach to Knowledge Management for Grid Applications
#@ Liming Chen;Nigel R. Shadbolt;Carole A. Goble
#t 2007
#c 7
#% 261139
#% 310424
#% 374223
#% 438596
#% 728757
#% 778646
#% 779881
#% 799913
#% 826185
#% 885468
#% 928357
#% 1112382
#% 1782947
#! Knowledge has become increasingly important to support intelligent process automation and collaborative problem solving in large-scale science over the Internet. This paper addresses distributed knowledge management, its approach and methodology, in the context of Grid application. We start by analyzing the nature of Grid computing and its requirements for knowledge support; then, we discuss knowledge characteristics and the challenges for knowledge management on the Grid. A Semantic Web-based approach is proposed to tackle the six challenges of the knowledge lifecycle—namely, those of acquiring, modeling, retrieving, reusing, publishing, and maintaining knowledge. To facilitate the application of the approach, a systematic methodology is conceived and designed to provide a general implementation guideline. We use a real-world Grid application, the GEODISE project, as a case study in which the core Semantic Web technologies such as ontologies, semantic enrichment, and semantic reasoning are used for knowledge engineering and management. The case study has been fully implemented and deployed through which the evaluation and validation for the approach and methodology have been performed.

#index 913805
#* A Requirements Driven Framework for Benchmarking Semantic Web Knowledge Base Systems
#@ Yuanbo Guo;Abir Qasem;Zhengxiang Pan;Jeff Heflin
#t 2007
#c 7
#% 41903
#% 159250
#% 385629
#% 482070
#% 519567
#% 1374374
#% 1655406
#% 1655434
#! A key challenge for the Semantic Web is to acquire the capability to effectively query large knowledge bases. As there will be several competing systems, we need benchmarks that will objectively evaluate these systems. Development of effective benchmarks in an emerging domain is a challenging endeavor. In this paper, we propose a requirements driven framework for developing benchmarks for Semantic Web Knowledge Base Systems (SW KBSs). In this paper, we make two major contributions. First, we provide a list of requirements for SW KBS benchmarks. This can serve as an unbiased guide to both the benchmark developers and personnel responsible for systems acquisition and benchmarking. Second, we provide an organized collection of techniques and tools needed to develop such benchmarks. In particular, the collection contains a detailed guide for generating benchmark workload, defining performance metrics, and interpreting experimental results.

#index 913806
#* From Wrapping to Knowledge
#@ Jose L. Arjona;Rafael Corchuelo;David Ruiz;Miguel Toro
#t 2007
#c 7
#% 248808
#% 275915
#% 300288
#% 331772
#% 397176
#% 424302
#% 431536
#% 445512
#% 459484
#% 459490
#% 655350
#% 754104
#% 762609
#% 762610
#% 772300
#% 805872
#% 810073
#% 870898
#% 870899
#% 870902
#% 1374367
#! One the most challenging problems for Enterprise Information Integration is to deal with heterogeneous information sources on the Web. The reason is that they usually provide information that is in human-readable form only, which makes it difficult for a software agent to understand it. Current solutions build on the idea of annotating the information with semantics. If the information is unstructured, proposals such as S-CREAM, MnM, or Armadillo may be effective enough since they rely on using natural language processing techniques; furthermore, their accuracy can be improved by using redundant information on the Web, as C-PANKOW has proved recently. If the information is structured and closely related to a back-end database, Deep Annotation ranges among the most effective proposals, but it requires the information providers to modify their applications; if Deep Annotation is not applicable, the easiest solution consists of using a wrapper and transforming its output into annotations. In this paper, we prove that this transformation can be automated by means of an efficient, domain-independent algorithm. To the best of our knowledge, this is the first attempt to devise and formalize such a systematic, general solution.

#index 913807
#* A Component Model and Infrastructure for a Fluid Web
#@ Andre Santanche;Claudia Bauzer Medeiros
#t 2007
#c 7
#% 86478
#% 301177
#% 480433
#% 484414
#% 527086
#% 527089
#% 564068
#% 570900
#% 728758
#% 769426
#% 771139
#% 824964
#% 825659
#% 832089
#% 1112587
#% 1712544
#% 1775352
#% 1775353
#! The Web is evolving from a space for publication/consumption of documents to an environment for collaborative work, where digital content can travel and be replicated, adapted, decomposed, fusioned, and transformed. We call this the Fluid Web perspective. This view requires a thorough revision of the typical document-oriented approach that permeates content management on the Web. This paper presents our solution for the Fluid Web, which allows moving from the document-oriented to a content-oriented perspective, where "content” can be any digital object. The solution is based on two axes: a self-descriptive unit to encapsulate any kind of content artifact—the Digital Content Component (DCC) and a Fluid Web infrastructure that provides management and deployment of DCCs through the Web, and whose goal is to support collaboration on the Web. Designed to be reused and adapted, DCCs encapsulate data and software using a single structure, thus allowing homogeneous composition and processing of any digital content, executable or not. These properties are exploited by our Fluid Web infrastructure, which supports DCC multilevel annotation and discovery mechanisms, configuration management, and version control. Our work extensively explores taxonomic ontologies and Semantic Web standards, which serve as a semantic bridge, unifying DCC management vocabularies, and improving DCC description/indexing/discovery. DCCs and infrastructure have been implemented and are illustrated by means of a running example, for a scientific application.

#index 975019
#* The Google Similarity Distance
#@ Rudi L. Cilibrasi;Paul M. B. Vitanyi
#t 2007
#c 7
#% 115608
#% 198055
#% 234979
#% 420077
#% 577214
#% 735137
#% 769896
#% 794511
#% 816185
#% 849867
#% 883475
#% 1558464
#% 1809406
#% 1815364
#% 1815525
#! Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers, the equivalent of "society” is "database,” and the equivalent of "use” is "a way to search the database.” We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts, we use the World Wide Web (WWW) as the database, and Google as the search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the WWW using Google page counts. The WWW is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87 percent with the expert crafted WordNet categories.

#index 975020
#* K-Means+ID3: A Novel Method for Supervised Anomaly Detection by Cascading K-Means Clustering and ID3 Decision Tree Learning Methods
#@ Shekhar R. Gaddam;Vir V. Phoha;Kiran S. Balagani
#t 2007
#c 7
#% 251145
#% 280181
#% 376266
#% 442588
#% 630995
#% 633259
#% 725307
#% 729437
#% 769628
#% 796197
#% 821870
#% 841801
#% 862157
#% 1756868
#% 1781125
#% 1781556
#! In this paper, we present "K-Means+ID3,” a method to cascade k-Means clustering and the ID3 decision tree learning methods for classifying anomalous and normal activities in a computer network, an active electronic circuit, and a mechanical mass-beam system. The k-Means clustering method first partitions the training instances into k clusters using Euclidean distance similarity. On each cluster, representing a density region of normal or anomaly instances, we build an ID3 decision tree. The decision tree on each cluster refines the decision boundaries by learning the subgroups within the cluster. To obtain a final decision on classification, the decisions of the k-Means and ID3 methods are combined using two rules: 1) the Nearest-neighbor rule and 2) the Nearest-consensus rule. We perform experiments on three data sets: 1) Network Anomaly Data (NAD), 2) Duffing Equation Data (DED), and 3) Mechanical System Data (MSD), which contain measurements from three distinct application domains of computer networks, an electronic circuit implementing a forced Duffing Equation, and a mechanical system, respectively. Results show that the detection accuracy of the K-Means+ID3 method is as high as 96.24 percent at a false-positive-rate of 0.03 percent on NAD; the total accuracy is as high as 80.01 percent on MSD and 79.9 percent on DED.

#index 975021
#* Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation
#@ Francois Fouss;Alain Pirotte;Jean-Michel Renders;Marco Saerens
#t 2007
#c 7
#% 51641
#% 51647
#% 63833
#% 74120
#% 129521
#% 228171
#% 238372
#% 280510
#% 290830
#% 313959
#% 342687
#% 387427
#% 418262
#% 464615
#% 593047
#% 616105
#% 729936
#% 731604
#% 734590
#% 734593
#% 743284
#% 769887
#% 823388
#% 832334
#% 840965
#% 1390190
#% 1784711
#% 1970771
#! This work presents a new perspective on characterizing the similarity between elements of a database or, more generally, nodes of a weighted and undirected graph. It is based on a Markov-chain model of random walk through the database. More precisely, we compute quantities (the average commute time, the pseudoinverse of the Laplacian matrix of the graph, etc.) that provide similarities between any pair of nodes, having the nice property of increasing when the number of paths connecting those elements increases and when the "length” of paths decreases. It turns out that the square root of the average commute time is a Euclidean distance and that the pseudoinverse of the Laplacian matrix is a kernel matrix (its elements are inner products closely related to commute times). A principal component analysis (PCA) of the graph is introduced for computing the subspace projection of the node vectors in a manner that preserves as much variance as possible in terms of the Euclidean commute-time distance. This graph PCA provides a nice interpretation to the "Fiedler vector,” widely used for graph partitioning. The model is evaluated on a collaborative-recommendation task where suggestions are made about which movies people should watch based upon what they watched in the past. Experimental results on the MovieLens database show that the Laplacian-based similarities perform well in comparison with other methods. The model, which nicely fits into the so-called "statistical relational learning” framework, could also be used to compute document or word similarities, and, more generally, it could be applied to machine-learning and pattern-recognition tasks involving a relational database.

#index 975022
#* Efficient Dissemination of Transaction-Consistent Data in Broadcast Environments
#@ Cheng-Ru Young;Ge-Ming Chiu
#t 2007
#c 7
#% 9241
#% 32884
#% 201897
#% 227885
#% 273893
#% 310282
#% 315669
#% 442622
#% 464065
#% 471523
#% 480941
#% 487722
#% 589259
#% 615361
#% 615596
#% 659573
#% 661499
#% 664067
#% 727673
#% 753022
#% 810654
#% 839510
#% 1700460
#% 1706158
#! In this paper, we present a novel protocol for disseminating data in broadcast environments such that view consistency, a useful correctness criterion for broadcast environments, is guaranteed. Our protocol is based on concurrency control information that is constructed by the server and is broadcasted at the beginning of each broadcast cycle. The concurrency control information mainly captures read-from relations among update transactions. A salient feature of the protocol is that the concurrency control information is small in size, but precise enough for reducing unnecessary abortion of mobile transactions. The small-sized concurrency control information implies low communication overhead on broadcasting system. In addition, the computation overheads imposed by the algorithm on the server and the clients are low. We also address the reliability issue of wireless communication and the incorporation of a prefetching mechanism into our protocol. Simulation results demonstrate the superiority of our protocol in comparison with existing methods. Furthermore, we have extended our protocol to deal with local view consistency which requires that all mobile transactions submitted by the same client observe the same serial order of update transactions.

#index 975023
#* HybMig: A Hybrid Approach to Dynamic Plan Migration for Continuous Queries
#@ Yin Yang;Jurgen Kramer;Dimitris Papadias;Bernhard Seeger
#t 2007
#c 7
#% 248795
#% 273910
#% 300167
#% 397353
#% 726621
#% 765437
#% 765497
#% 788215
#% 800502
#% 810016
#% 878299
#% 1015278
#% 1015296
#% 1016169
#% 1016170
#% 1016208
#% 1016210
#! In data stream environments, the initial plan of a long-running query may gradually become inefficient due to changes of the data characteristics. In this case, the query optimizer will generate a more efficient plan based on the current statistics. The online transition from the old to the new plan is called dynamic plan migration. In addition to correctness, an effective technique for dynamic plan migration should achieve the following objectives: 1) minimize the memory and CPU overhead of the migration, 2) reduce the duration of the transition, and 3) maintain a steady output rate. The only known solutions for this problem are the moving states (MS) and parallel track (PT) strategies, which have some serious shortcomings related to the above objectives. Motivated by these shortcomings, we first propose HybMig, which combines the merits of MS and PT and outperforms both in every aspect. As a second step, we extend PT, MS, and HybMig to the general problem of migration, where both the new and the old plans are treated as black boxes.

#index 975024
#* Reverse Nearest Neighbors Search in Ad Hoc Subspaces
#@ Man Lung Yiu;Nikos Mamoulis
#t 2007
#c 7
#% 121114
#% 273891
#% 286258
#% 287466
#% 300163
#% 333854
#% 339614
#% 397378
#% 420515
#% 465009
#% 479486
#% 480132
#% 480661
#% 495433
#% 527328
#% 571056
#% 643566
#% 654478
#% 659255
#% 659993
#% 730019
#% 765467
#% 800510
#% 824697
#% 864463
#% 993999
#% 1016191
#! Given an object q, modeled by a multidimensional point, a reverse nearest neighbors (RNN) query returns the set of objects in the database that have q as their nearest neighbor. In this paper, we study an interesting generalization of the RNN query, where not all dimensions are considered, but only an ad hoc subset thereof. The rationale is that 1) the dimensionality might be too high for the result of a regular RNN query to be useful, 2) missing values may implicitly define a meaningful subspace for RNN retrieval, and 3) analysts may be interested in the query results only for a set of (ad hoc) problem dimensions (i.e., object attributes). We consider a suitable storage scheme and develop appropriate algorithms for projected RNN queries, without relying on multidimensional indexes. Given the significant cost difference between random and sequential data accesses, our algorithms are based on applying sequential accesses only on the projected atomic values of the data at each dimension, to progressively derive a set of RNN candidates. Whether these candidates are actual RNN results is then validated via an optimized refinement step. In addition, we study variants of the projected RNN problem, including RkNN search, bichromatic RNN, and RNN retrieval for the case where sequential accesses are not possible. Our methods are experimentally evaluated with real and synthetic data.

#index 975025
#* Propositional Logic Constraint Patterns and Their Use in UML-Based Conceptual Modeling and Analysis
#@ James P. Davis;Ronald D. Bonnell
#t 2007
#c 7
#% 1810
#% 83029
#% 92387
#% 102371
#% 115390
#% 157322
#% 235012
#% 237269
#% 261177
#% 262278
#% 264771
#% 321680
#% 352990
#% 362518
#% 386381
#% 408526
#% 462502
#% 527549
#% 527553
#% 527708
#% 527716
#% 535046
#% 556167
#% 556174
#% 558147
#% 737455
#% 740777
#% 753307
#% 1013898
#% 1395385
#% 1395393
#% 1801137
#! An important conceptual modeling activity in the development of database, object-oriented and agent-oriented systems is the capture and expression of domain constraints governing underlying data and object states. UML is increasingly used for capturing conceptual models, as it supports conceptual modeling of arbitrary domains, and has extensible notation allowing capture of invariant constraints both in the class diagram notation and in the separately denoted OCL syntax. However, a need exists for increased formalism in constraint capture that does not sacrifice ease of use for the analyst. In this paper, we codify a set of invariant patterns formalized here for capturing a rich category of propositional constraints on class diagrams. We use tools of Boolean logic to set out the distinction between these patterns, applying them in modeling by way of example. We use graph notation to systematically uncover constraints hidden in the diagrams. We present data collected from applications across different domains, supporting the importance of "pattern-finding” for n{\hbox{-}}{\rm{variable}} propositional constraints using general graph theoretic methods. This approach enriches UML-based conceptual modeling for greater completeness, consistency, and correctness by formalizing the syntax and semantics of these constraint patterns, which has not been done in a comprehensive manner before now.

#index 975026
#* Negative Samples Analysis in Relevance Feedback
#@ Dacheng Tao;Xuelong Li;Stephen J. Maybank
#t 2007
#c 7
#% 80995
#% 190581
#% 219845
#% 341269
#% 345848
#% 479788
#% 589960
#% 593491
#% 646276
#% 733364
#% 770787
#% 780807
#% 836846
#% 1502474
#% 1775495
#% 1854912
#% 1857498
#% 1860977
#! Recently, relevance feedback (RF) in content-based image retrieval (CBIR) has been implemented as an online binary classifier to separate the positive samples from the negative samples, where both sets of samples are labeled by the user. In many applications, it is reasonable to assume that all the positive samples are alike and thus that the region of the feature space occupied by the positive samples can be described by a single hypersurface. However, for the negative samples, previous RF methods either treat each one of the negative samples as an isolated point or assume the whole negative set can be described by a single convex hypersurface. In this paper, we argue that these treatments of the negative samples are not sound. Our belief is all positive samples are included in a set and the negative samples split into a small number of subsets, each one of which has a simple distribution. Therefore, we first cluster the negative samples into several groups; for each such negative group, we build a marginal convex machine (MCM) subclassifier between it and the single positive group which results in a series of subclassifiers. These subclassifiers are then incorporated into a biased MCM (BMCM) for RF. Experiments were carried out to prove the advantages of BMCM-based RF over previous methods for RF.

#index 975027
#* Evaluating Variable-Length Markov Chain Models for Analysis of User Web Navigation Sessions
#@ Jose Borges;Mark Levene
#t 2007
#c 7
#% 268184
#% 312874
#% 452396
#% 571605
#% 728195
#% 729886
#% 739634
#% 755395
#% 758427
#% 830834
#% 836144
#% 947832
#% 1673556
#! Markov models have been widely used to represent and analyze user Web navigation data. In previous work, we have proposed a method to dynamically extend the order of a Markov chain model and a complimentary method for assessing the predictive power of such a variable-length Markov chain. Herein, we review these two methods and propose a novel method for measuring the ability of a variable-length Markov model to summarize user Web navigation sessions up to a given length. Although the summarization ability of a model is important to enable the identification of user navigation patterns, the ability to make predictions is important in order to foresee the next link choice of a user after following a given trail so as, for example, to personalize a Web site. We present an extensive experimental evaluation providing strong evidence that prediction accuracy increases linearly with summarization ability.

#index 975028
#* Discovery of Periodic Patterns in Spatiotemporal Sequences
#@ Huiping Cao;Nikos Mamoulis;David W. Cheung
#t 2007
#c 7
#% 310542
#% 329537
#% 342642
#% 452847
#% 458857
#% 463903
#% 464839
#% 464986
#% 480156
#% 480473
#% 480817
#% 481290
#% 527319
#% 566128
#% 629677
#% 631924
#% 631926
#% 729960
#% 745486
#% 769899
#% 813978
#! In many applications that track and analyze spatiotemporal data, movements obey periodic patterns; the objects follow the same routes (approximately) over regular time intervals. For example, people wake up at the same time and follow more or less the same route to their work everyday. The discovery of hidden periodic patterns in spatiotemporal data could unveil important information to the data analyst. Existing approaches for discovering periodic patterns focus on symbol sequences. However, these methods cannot directly be applied to a spatiotemporal sequence because of the fuzziness of spatial locations in the sequence. In this paper, we define the problem of mining periodic patterns in spatiotemporal data and propose an effective and efficient algorithm for retrieving maximal periodic patterns. In addition, we study two interesting variants of the problem. The first is the retrieval of periodic patterns that are frequent only during a continuous subinterval of the whole history. The second problem is the discovery of periodic patterns, whose instances may be shifted or distorted. We demonstrate how our mining technique can be adapted for these variants. Finally, we present a comprehensive experimental evaluation, where we show the effectiveness and efficiency of the proposed techniques.

#index 975029
#* Quality-Aware Sampling and Its Applications in Incremental Data Mining
#@ Kun-Ta Chuang;Keng-Pei Lin;Ming-Syan Chen
#t 2007
#c 7
#% 1331
#% 18658
#% 210173
#% 232768
#% 248790
#% 248792
#% 280406
#% 290482
#% 300132
#% 316709
#% 342689
#% 379444
#% 420079
#% 420137
#% 451052
#% 566870
#% 614619
#% 729915
#% 729932
#% 729959
#% 742562
#% 1016146
#% 1016154
#% 1700144
#! We explore in this paper a novel sampling algorithm, referred to as algorithm PAS (standing for Proportion Approximation Sampling), to generate a high-quality online sample with the desired sample rate. The sampling quality refers to the consistency between the population proportion and the sample proportion of each categorical value in the database. Note that the state-of-the-art sampling algorithm to preserve the sampling quality has to examine the population proportion of each categorical value in a pilot sample a priori and is thus not applicable to incremental mining applications. To remedy this, algorithm PAS adaptively determines the inclusion probability of each incoming tuple in such a way that the sampling quality can be sequentially preserved while also guaranteeing the sample rate close to the user specified one. Importantly, PAS not only guarantees the proportion consistency of each categorical value but also excellently preserves the proportion consistency of multivariate statistics, which will be significantly beneficial to various data mining applications. For better execution efficiency, we further devise an algorithm, called algorithm EQAS (standing for Efficient Quality-Aware Sampling), which integrates PAS and random sampling to provide the flexibility of striking a compromise between the sampling quality and the sampling efficiency. As validated in experimental results on real and synthetic data, algorithm PAS can stably provide high-quality samples with corresponding computational overhead, whereas algorithm EQAS can flexibly generate samples with the desired balance between sampling quality and sampling efficiency. In addition, while applying the sample generated by algorithms PAS and EQAS to incremental mining applications, a significant efficiency improvement can be obtained without compromising the resulting precision, showing the prominent advantage of both proposed algorithms to be the quality-aware sampling means for incremental mining applications.

#index 975030
#* Classifier Ensembles with a Random Linear Oracle
#@ Ludmila I. Kuncheva;Juan J. Rodriguez
#t 2007
#c 7
#% 155961
#% 182686
#% 226499
#% 235377
#% 256615
#% 278042
#% 280181
#% 312728
#% 329533
#% 400847
#% 451221
#% 565528
#% 702615
#% 742990
#% 889165
#% 926881
#% 961134
#% 1042787
#% 1378443
#% 1393001
#% 1712965
#% 1777251
#% 1781125
#% 1784642
#% 1859963
#% 1860033
#% 1860899
#% 1861606
#! We propose a combined fusion-selection approach to classifier ensemble design. Each classifier in the ensemble is replaced by a miniensemble of a pair of subclassifiers with a random linear oracle to choose between the two. It is argued that this approach encourages extra diversity in the ensemble while allowing for high accuracy of the individual ensemble members. Experiments were carried out with 35 data sets from UCI and 11 ensemble models. Each ensemble model was examined with and without the oracle. The results showed that all ensemble methods benefited from the new approach, most markedly so random subspace and bagging. A further experiment with seven real medical data sets demonstrates the validity of these findings outside the UCI data collection.

#index 975031
#* MESO: Supporting Online Decision Making in Autonomic Computing Systems
#@ Eric P. Kasten;Philip K. McKinley
#t 2007
#c 7
#% 87705
#% 92546
#% 210173
#% 237387
#% 273891
#% 296375
#% 296738
#% 302383
#% 318130
#% 333933
#% 342626
#% 375388
#% 420084
#% 444569
#% 449588
#% 452359
#% 479462
#% 546061
#% 577229
#% 594477
#% 661884
#% 661886
#% 661888
#% 693091
#% 729437
#% 729929
#% 729940
#% 766247
#% 785388
#% 835018
#% 1759695
#% 1800264
#! Autonomic computing systems must be able to detect and respond to errant behavior or changing conditions with little or no human intervention. Clearly, decision making is a critical issue in such systems, which must learn how and when to invoke corrective actions based on past experience. This paper describes the design, implementation, and evaluation of MESO, a pattern classifier designed to support online, incremental learning and decision making in autonomic systems. A novel feature of MESO is its use of small agglomerative clusters, called sensitivity spheres, that aggregate similar training samples. Sensitivity spheres are partitioned into sets during the construction of a memory-efficient hierarchical data structure. This structure facilitates data compression, which is important to many autonomic systems. Results are presented demonstrating that MESO achieves high accuracy while enabling rapid incremental training and classification. A case study is described in which MESO enables a mobile computing application to learn, by imitation, user preferences for balancing wireless network packet loss and bandwidth consumption. Once trained, the application can autonomously adjust error control parameters as needed while the user roams about a wireless cell.

#index 975032
#* An Exact Data Mining Method for Finding Center Strings and All Their Instances
#@ Ruqian Lu;Caiyan Jia;Shaofang Zhang;Lusheng Chen;Hongyu Zhang
#t 2007
#c 7
#% 232136
#% 271246
#% 300120
#% 328321
#% 344981
#% 469417
#% 469571
#% 481290
#% 498509
#% 720645
#% 725189
#% 769673
#% 958602
#! Common substring problems allowing errors are known to be NP-hard. The main challenge of the problems lies in the combinatorial explosion of potential candidates. In this paper, we propose and study a Generalized Center String (GCS) problem, where not only all models (center strings) of any length, but also the positions of all their (degenerative) instances in input sequences are searched for. Inspired by frequent pattern mining techniques in data mining field, we present an exact and efficient method to solve GCS. First, a highly parallelized TRIE-like structure, consensus tree, is proposed. Based on this structure, we present three Bpriori algorithms step by step. Bpriori algorithms can solve GCS with reasonable time and/or space complexities. We have proved that GCS is fixed parameter tractable with respect to fixed symbol set size and fixed length of input sequences. Experiment results on both artificial and real data have shown the correctness of the algorithms and the validity of our complexity analysis. A comparison with some current algorithms for solving Common Approximate Substring problems is also given.

#index 975033
#* The Threshold Algorithm: From Middleware Systems to the Relational Engine
#@ Nicolas Bruno;Hui (Wendy) Wang
#t 2007
#c 7
#% 67565
#% 213981
#% 227894
#% 290747
#% 300193
#% 333854
#% 333947
#% 397378
#% 399762
#% 480330
#% 591565
#% 631988
#% 659921
#% 763882
#% 765418
#% 772847
#! The answer to a top-k query is an ordered set of tuples, where the ordering is based on how closely each tuple matches the query. In the context of middleware systems, new algorithms to answer top-k queries have been recently proposed. Among these, the Threshold Algorithm (TA) is the most well-known instance due to its simplicity and memory requirements. TA is based on an early-termination condition and can evaluate top-k queries without examining all the tuples. This top-k query model is prevalent not only over middleware systems, but also over plain relational data. In this work, we analyze the challenges that must be addressed to adapt TA to a relational database system. We show that, depending on the available indices, many alternative TA strategies can be used to answer a given query. Choosing the best alternative requires a cost model that can be seamlessly integrated with that of current optimizers. In this work, we address these challenges and conduct an extensive experimental evaluation of the resulting techniques by characterizing which scenarios can take advantage of TA-like algorithms to answer top-k queries in relational database systems.

#index 975034
#* Rank Aggregation for Automatic Schema Matching
#@ Carmel Domshlak;Avigdor Gal;Haggai Roitman
#t 2007
#c 7
#% 237184
#% 330769
#% 332166
#% 333854
#% 333990
#% 348187
#% 466815
#% 480134
#% 480645
#% 529190
#% 536351
#% 551850
#% 572314
#% 578668
#% 643566
#% 654457
#% 654459
#% 654466
#% 660001
#% 728195
#% 745483
#% 765433
#% 765540
#% 790843
#% 800005
#% 800498
#% 823368
#% 830522
#% 830524
#% 864573
#% 945908
#% 993982
#% 1671768
#% 1688252
#% 1730010
#! Schema matching is a basic operation of data integration, and several tools for automating it have been proposed and evaluated in the database community. Research in this area reveals that there is no single schema matcher that is guaranteed to succeed in finding a good mapping for all possible domains and, thus, an ensemble of schema matchers should be considered. In this paper, we introduce schema metamatching, a general framework for composing an arbitrary ensemble of schema matchers and generating a list of best ranked schema mappings. Informally, schema metamatching stands for computing a "consensus” ranking of alternative mappings between two schemata, given the "individual” graded rankings provided by several schema matchers. We introduce several algorithms for this problem, varying from adaptations of some standard techniques for general quantitative rank aggregation to novel techniques specific to the problem of schema matching, and to combinations of both. We provide a formal analysis of the applicability and relative performance of these algorithms and evaluate them empirically on a set of real-world schemata.

#index 975035
#* Efficient Revalidation of XML Documents
#@ Mukund Raghavachari;Oded Shmueli
#t 2007
#c 7
#% 70370
#% 262724
#% 404772
#% 408458
#% 413648
#% 427027
#% 465059
#% 745467
#% 791181
#% 994015
#% 1673668
#! We study the problem of schema revalidation where XML data known to conform to one schema must be validated with respect to another schema. Such revalidation algorithms have applications in schema evolution, query processing, XML-based programming languages, and other domains. We describe how knowledge of conformance to an XML Schema may be used to determine conformance to another XML Schema efficiently. We examine both the situation where an XML document is modified before it is revalidated and the situation where it is unmodified.

#index 975036
#* Knowledge and Data Engineering for e-Learning
#@ 
#t 2007
#c 7

#index 975037
#* Practical Algorithms and Lower Bounds for Similarity Search in Massive Graphs
#@ Daniel Fogaras;Balazs Racz
#t 2007
#c 7
#% 238182
#% 243166
#% 248810
#% 249321
#% 281209
#% 290830
#% 293720
#% 309747
#% 309748
#% 311808
#% 348165
#% 387427
#% 413614
#% 480328
#% 577219
#% 577273
#% 581661
#% 616528
#% 730089
#% 754090
#% 818216
#% 859116
#% 1712592
#! To exploit the similarity information hidden in the hyperlink structure of the Web, this paper introduces algorithms scalable to graphs with billions of vertices on a distributed architecture. The similarity of multistep neighborhoods of vertices are numerically evaluated by similarity functions including SimRank [1], a recursive refinement of cocitation, and PSimRank, a novel variant with better theoretical characteristics. Our methods are presented in a general framework of Monte Carlo similarity search algorithms that precompute an index database of random fingerprints, and at query time, similarities are estimated from the fingerprints. We justify our approximation method by asymptotic worst-case lower bounds: We show that there is a significant gap between exact and approximate approaches, and suggest that the exact computation, in general, is infeasible for large-scale inputs. We were the first to evaluate SimRank on real Web data. On the Stanford WebBase [2] graph of 80M pages the quality of the methods increased significantly in each refinement step until step four.

#index 975038
#* An Efficient Web Page Change Detection System Based on an Optimized Hungarian Algorithm
#@ Imad Khoury;Rami M. El-Mawas;Oussama El-Rawas;Elias F. Mounayar;Hassan Artail
#t 2007
#c 7
#% 227859
#% 316563
#% 354949
#% 424263
#% 464991
#% 642660
#% 659923
#% 737991
#% 785131
#% 806625
#! This paper describes an efficient Web page change detection system based on three optimizations that were implemented on top of the Hungarian algorithm, which we employ to compare trees that correspond to HTML Web pages. The optimizations attempt to stop the comparator algorithm that employs this O(n^{3}) algorithm before it completes all its iterations based on criteria having to do with properties of HTML and heuristics. Analysis and experimental results prove the effectiveness of these optimizations and their ability to render O(n^{2}) performance, where n denotes the number of nodes in the tree. A complete system was implemented and used to carry out the performance experiments. This system includes functionalities and interfaces for processing user requests, fetching Web pages from the Internet, allowing users to select zones in Web pages to monitor, and highlighting changes on the Web pages being monitored.

#index 975039
#* Scalable Delivery of Dynamic Content Using a Cooperative Edge Cache Grid
#@ Lakshmish Ramaswamy;Ling Liu;Arun Iyengar
#t 2007
#c 7
#% 248768
#% 256883
#% 271616
#% 281152
#% 283823
#% 397357
#% 398237
#% 399551
#% 402267
#% 443295
#% 641977
#% 661477
#% 723901
#% 729343
#% 753049
#% 762650
#% 778731
#% 789005
#% 812763
#% 813976
#% 884457
#% 963648
#% 979357
#% 994021
#% 1016166
#% 1306883
#% 1502090
#% 1849768
#! In recent years, edge computing has emerged as a popular mechanism to deliver dynamic Web content to clients. However, many existing edge cache networks have not been able to harness the full potential of edge computing technology. In this paper, we argue and experimentally demonstrate that cooperation among the individual edge caches coupled with scalable server-driven document consistency mechanisms can significantly enhance the capabilities and performance of edge cache networks in delivering fresh dynamic content. However, designing large-scale cooperative edge cache networks presents many research challenges. Toward addressing these challenges, this paper presents cooperative edge cache grid (cooperative EC grid, for short)—a large-scale cooperative edge cache network for efficiently delivering highly dynamic Web content with varying server update frequencies. The design of the cooperative EC grid focuses on the scalability and reliability of dynamic content delivery in addition to cache hit rates, and it incorporates several novel features. We introduce the concept of cache clouds as a generic framework of cooperation in large-scale edge cache networks. The architectural design of the cache clouds includes dynamic hashing-based document lookup and update protocols, which dynamically balance lookup and update loads among the caches in the cloud. We also present cooperative techniques for making the document lookup and update protocols resilient to the failures of individual caches. This paper reports a series of simulation-based experiments which show that the overheads of cooperation in the cooperative EC grid are very low, and our architecture and techniques enhance the performance of the cooperative edge networks.

#index 975040
#* Conditional Anomaly Detection
#@ Xiuyao Song;Mingxi Wu;Christopher Jermaine;Sanjay Ranka
#t 2007
#c 7
#% 169368
#% 246834
#% 300136
#% 300183
#% 309471
#% 333929
#% 345829
#% 379238
#% 457911
#% 466745
#% 570886
#% 578689
#% 731721
#% 749406
#% 769901
#! When anomaly detection software is used as a data analysis tool, finding the hardest-to-detect anomalies is not the most critical task. Rather, it is often more important to make sure that those anomalies that are reported to the user are in fact interesting. If too many unremarkable data points are returned to the user labeled as candidate anomalies, the software will soon fall into disuse. One way to ensure that returned anomalies are useful is to make use of domain knowledge provided by the user. Often, the data in question includes a set of environmental attributes whose values a user would never consider to be directly indicative of an anomaly. However, such attributes cannot be ignored because they have a direct effect on the expected distribution of the result attributes whose values can indicate an anomalous observation. This paper describes a general purpose method called conditional anomaly detection for taking such differences among attributes into account, and proposes three different expectation-maximization algorithms for learning the model that is used in conditional anomaly detection. Experiments with more than 13 different data sets compare our algorithms with several other more standard methods for outlier or anomaly detection.

#index 975041
#* Probe Minimization by Schedule Optimization: Supporting Top-K Queries with Expensive Predicates
#@ Seung-won Hwang;Kevin Chen-Chuan Chang
#t 2007
#c 7
#% 55490
#% 152940
#% 172931
#% 174161
#% 210172
#% 213981
#% 300170
#% 397378
#% 399762
#% 410276
#% 480330
#% 480819
#% 591565
#% 631988
#% 643566
#% 659993
#% 733373
#% 763882
#% 768521
#% 800511
#% 810018
#% 1016183
#! This paper addresses the problem of evaluating ranked top-k queries with expensive predicates. As major DBMSs now all support expensive user-defined predicates for Boolean queries, we believe such support for ranked queries will be even more important: First, ranked queries often need to model user-specific concepts of preference, relevance, or similarity, which call for dynamic user-defined functions. Second, middleware systems must incorporate external predicates for integrating autonomous sources typically accessible only by per-object queries. Third, ranked queries often accompany Boolean ranking conditions, which may turn predicates into expensive ones, as the index structure on the predicate built on the base table may be no longer effective in retrieving the filtered objects in order. Fourth, fuzzy joins are inherently expensive, as they are essentially user-defined operations that dynamically associate multiple relations. These predicates, being dynamically defined or externally accessed, cannot rely on index mechanisms to provide zero-time sorted output, and must instead require per-object probe to evaluate. To enable probe minimization, we develop the problem as cost-based optimization of searching over potential probe schedules. In particular, we decouple probe scheduling into object and predicate scheduling problems and develop an analytical object scheduling optimization and a dynamic predicate scheduling optimization, which combined together form a cost-effective probe schedule.

#index 975042
#* Indexing Spatio-Temporal Trajectories with Efficient Polynomial Approximations
#@ Jinfeng Ni;Chinya V. Ravishankar
#t 2007
#c 7
#% 32913
#% 76907
#% 164360
#% 213975
#% 273706
#% 296090
#% 300174
#% 411356
#% 413597
#% 427199
#% 443181
#% 443190
#% 443444
#% 452852
#% 458857
#% 480473
#% 480817
#% 527195
#% 555050
#% 617878
#% 632208
#% 765451
#% 765452
#% 765454
#% 801679
#% 824729
#% 878300
#% 1015320
#% 1015321
#% 1720756
#! Complex queries on trajectory data are increasingly common in applications involving moving objects. MBR or grid-cell approximations on trajectories perform suboptimally since they do not capture the smoothness and lack of internal area of trajectories. We describe a parametric space indexing method for historical trajectory data, approximating a sequence of movement functions with single continuous polynomial. Our approach works well, yielding much finer approximation quality than MBRs. We present the PA-tree, a parametric index that uses this method, and show through extensive experiments that PA-trees have excellent performance for offline and online spatio-temporal range queries. Compared to MVR-trees, PA-trees are an order of magnitude faster to construct and incur I/O cost for spatio-temporal range queries lower by a factor of 2-4. SETI is faster than our method for index construction and timestamp queries, but incurs twice the I/O cost for time interval queries, which are much more expensive and are the bottleneck in online processing. Therefore, the PA-tree is an excellent choice for both offline and online processing of historical trajectories.

#index 975043
#* Multiquality Data Replication in Multimedia Databases
#@ Yi-Cheng Tu;Jingfeng Yan;Gang Shen;Sunil Prabhakar
#t 2007
#c 7
#% 183436
#% 194196
#% 201933
#% 210182
#% 225006
#% 237736
#% 302731
#% 335726
#% 401173
#% 422948
#% 422965
#% 443367
#% 451550
#% 458590
#% 500722
#% 535989
#% 615647
#% 635962
#% 741416
#% 1698879
#% 1775058
#! In contrast to other database applications, multimedia data can have a wide range of quality parameters, such as spatial and temporal resolution and compression format. Users can request data with specific quality requirements due to the needs of their application or the limitations of their resources. The database can support multiple qualities by converting data from the original (high) quality to another (lower) quality to support a user's query or precompute and store multiple quality replicas of data items. On-the-fly conversion of multimedia data (such as video transcoding) is very CPU intensive and can limit the level of concurrent access supported by the database. Storing all possible replicas, on the other hand, requires unacceptable increases in storage requirements. In this paper, we address the problem of multiple-quality replica selection subject to an overall storage constraint. We establish that the problem is NP-hard and provide heuristic solutions under two different system models: Hard-Quality and Soft-Quality. Under the soft-quality model, users are willing to negotiate their quality needs, as opposed to the hard-quality system wherein users will only accept the exact quality requested. Extensive simulations show that our algorithm performs significantly better than other heuristics. Our algorithms are flexible in that they can be extended to deal with changes in query pattern.

#index 975044
#* Bayesian Networks for Knowledge-Based Authentication
#@ Ye Chen;Divakaran Liginlal
#t 2007
#c 7
#% 44876
#% 183490
#% 211044
#% 246832
#% 289771
#% 351595
#% 443356
#% 443359
#% 722915
#% 739184
#% 739899
#% 744793
#% 750863
#% 778673
#% 959049
#% 959137
#% 963791
#% 975267
#! Knowledge-based authentication (KBA) has gained prominence as a user authentication method for electronic transactions. This paper presents a Bayesian network model of KBA grounded in probabilistic reasoning and information theory. The probabilistic semantics of the model parameters naturally lead to the definitions of two key KBA metrics—guessability and memorability. The statistical modeling approach allows parameter estimation using methods such as the maximum likelihood estimator (MLE). The information-theoretic view helps to derive the closed-form solutions to estimating the guessability and guessing entropy metrics. The results related to KBA metrics and the models under different attacking strategies and factoid distributions are unified under a game-theoretic framework that yields lower and upper bounds of optimal guessability. The paper also proposes a methodology for implementing a Bayesian network-based KBA system. Further, an empirical evaluation of the relative merits of two Bayesian network structures for KBA, the Naive Bayes (NB) and the Tree Augmented Naive Bayes (TAN), confirms the hypothesis that the TAN structure is superior in terms of authentication accuracy and error rates. The results of the theoretical analysis and the empirical study provide insights into the KBA design problem and establish a foundation for future research in the KBA area.

#index 975045
#* Anonymizing Classification Data for Privacy Preservation
#@ Benjamin C. M. Fung;Ke Wang;Philip S. Yu
#t 2007
#c 7
#% 90661
#% 136350
#% 248030
#% 443463
#% 488324
#% 576762
#% 577239
#% 785363
#% 800514
#% 800515
#% 801690
#% 810011
#% 844340
#% 864406
#% 864412
#% 874989
#% 881483
#% 881497
#% 881546
#% 881551
#% 951837
#% 1719423
#! Classification is a fundamental problem in data analysis. Training a classifier requires accessing a large collection of data. Releasing person-specific data, such as customer data or patient records, may pose a threat to an individual's privacy. Even after removing explicit identifying information such as Name and SSN, it is still possible to link released records back to their identities by matching some combination of nonidentifying attributes such as \{Sex, Zip, Birthdate\}. A useful approach to combat such linking attacks, called k-anonymization [1], is anonymizing the linking attributes so that at least k released records match each value combination of the linking attributes. Previous work attempted to find an optimal k-anonymization that minimizes some data distortion metric. We argue that minimizing the distortion to the training data is not relevant to the classification goal that requires extracting the structure of predication on the "future” data. In this paper, we propose a k-anonymization solution for classification. Our goal is to find a k-anonymization, not necessarily optimal in the sense of minimizing data distortion, which preserves the classification structure. We conducted intensive experiments to evaluate the impact of anonymization on the classification on future data. Experiments on real-life data show that the quality of classification can be preserved even for highly restrictive anonymity requirements.

#index 975046
#* Call for Papers - Knowledge and Data Engineering for e-Learning
#@ 
#t 2007
#c 7

#index 975047
#* Rule Extraction from Support Vector Machines: A Sequential Covering Approach
#@ Nahla H. Barakat;Andrew P. Bradley
#t 2007
#c 7
#% 229811
#% 376266
#% 420077
#% 466086
#% 466639
#% 775603
#% 796197
#% 799039
#% 799042
#% 799043
#% 823327
#% 893461
#% 900107
#% 900566
#% 926881
#% 1378224
#% 1707788
#% 1734493
#! In this paper, we propose a novel algorithm for rule extraction from support vector machines (SVMs), termed SQRex-SVM. The proposed method extracts rules directly from the support vectors (SVs) of a trained SVM using a modified sequential covering algorithm. Rules are generated based on an ordered search of the most discriminative features, as measured by interclass separation. Rule performance is then evaluated using measured rates of true and false positives and the area under the receiver operating characteristic (ROC) curve (AUC). Results are presented on a number of commonly used data sets that show the rules produced by SQRex-SVM exhibit both improved generalization performance and smaller more comprehensible rule sets compared to both other SVM rule extraction techniques and direct rule learning techniques.

#index 975048
#* Mining Nonambiguous Temporal Patterns for Interval-Based Events
#@ Shin-Yi Wu;Yen-Liang Chen
#t 2007
#c 7
#% 310559
#% 319244
#% 329537
#% 341918
#% 399793
#% 412588
#% 413550
#% 424908
#% 443082
#% 443194
#% 459006
#% 460862
#% 463903
#% 464196
#% 464986
#% 464996
#% 502121
#% 558575
#% 570314
#% 629695
#% 630984
#% 631926
#% 665649
#% 749082
#% 789007
#% 818916
#! Previous research on mining sequential patterns mainly focused on discovering patterns from point-based event data. Little effort has been put toward mining patterns from interval-based event data, where a pair of time values is associated with each event. Kam and Fu's work [31] in 2000 identified 13 temporal relationships between two intervals. According to these temporal relationships, a new variant of temporal patterns was defined for interval-based event data. Unfortunately, the patterns defined in this manner are ambiguous, which means that the temporal relationships among events cannot be correctly represented in temporal patterns. To resolve this problem, we first define a new kind of nonambiguous temporal pattern for interval-based event data. Then, the TPrefixSpan algorithm is developed to mine the new temporal patterns from interval-based events. The completeness and accuracy of the results are also proven. The experimental results show that the efficiency and scalability of the TPrefixSpan algorithm are satisfactory. Furthermore, to show the applicability and effectiveness of temporal pattern mining, we execute experiments to discover temporal patterns from historical Nasdaq data.

#index 975049
#* Peer-to-Peer in Metric Space and Semantic Space
#@ Hai Zhuge;Xiang Li
#t 2007
#c 7
#% 31681
#% 35764
#% 160390
#% 188026
#% 268079
#% 280424
#% 282905
#% 330715
#% 340175
#% 340176
#% 344708
#% 349973
#% 429749
#% 433981
#% 452719
#% 484379
#% 505869
#% 569762
#% 571859
#% 576976
#% 608886
#% 641979
#% 660001
#% 674136
#% 754122
#% 776235
#% 791020
#% 823440
#% 824708
#% 837612
#% 928357
#% 951734
#% 1112006
#% 1303506
#% 1720303
#% 1722625
#% 1788049
#! This paper first proposes three improved gossip mechanisms by mapping links into metric space and dynamically adapting the number of selected neighbors to disseminate messages. Experiments and comparisons show that these mechanisms can improve the performance of gossip in peer-to-peer (P2P) networks. This is the effect of mapping a network into a metric space that differentiates nodes and links according to linking characteristics and controlling local information flow with knowing such differences. A further study about query routing on P2P semantic link network shows that mapping a network into a semantic space can also improve the performance. An intrinsic rule is found by experimental comparisons and analysis: The performance of a P2P network can be improved by designing an appropriate mapping from the network into metric space or semantic space. A general framework for networking with metric space and semantic space is suggested.

#index 975050
#* Adaptive Index Utilization in Memory-Resident Structural Joins
#@ Bingsheng He;Qiong Luo;Byron Choi
#t 2007
#c 7
#% 13033
#% 201883
#% 252458
#% 273910
#% 300167
#% 300194
#% 317933
#% 333940
#% 333942
#% 333949
#% 333981
#% 397375
#% 462941
#% 464233
#% 480803
#% 571294
#% 580978
#% 632100
#% 659999
#% 721346
#% 736997
#% 810046
#% 824667
#% 893112
#% 993947
#% 993953
#% 1015277
#% 1015298
#! We consider adaptive index utilization as a fine-grained problem in autonomic databases in which an existing index is dynamically determined to be used or not in query processing. As a special case, we study this problem for structural joins, the core operator in XML query processing, in the main memory. We find that index utilization is beneficial for structural joins only under certain join selectivity and distribution of matching elements. Therefore, we propose adaptive algorithms to decide whether to use an index probe or a data scan for each step of matching during the processing of a structural join operator. Our adaptive algorithms are based on the history, the look-ahead information, or both. We have developed a cost model to facilitate this adaptation and have conducted experiments with both synthetic and real-world data sets. Our results show that adaptively utilizing indexes in a structural join improves the performance by taking advantage of both sequential scans and index probes.

#index 975051
#* Continuous Nearest Neighbor Queries over Sliding Windows
#@ Kyriakos Mouratidis;Dimitris Papadias
#t 2007
#c 7
#% 86950
#% 201876
#% 287466
#% 378388
#% 427199
#% 442615
#% 465167
#% 480671
#% 527187
#% 527191
#% 527328
#% 579313
#% 654478
#% 736290
#% 765453
#% 800555
#% 800571
#% 800572
#% 806212
#% 810033
#% 810061
#% 814650
#% 849816
#% 879211
#% 993949
#% 993954
#% 1016196
#! This paper studies continuous monitoring of nearest neighbor (NN) queries over sliding window streams. According to this model, data points continuously stream in the system, and they are considered valid only while they belong to a sliding window that contains 1) the W most recent arrivals (count-based) or 2) the arrivals within a fixed interval W covering the most recent time stamps (time-based). The task of the query processor is to constantly maintain the result of long-running NN queries among the valid data. We present two processing techniques that apply to both count-based and time-based windows. The first one adapts conceptual partitioning, the best existing method for continuous NN monitoring over update streams, to the sliding window model. The second technique reduces the problem to skyline maintenance in the distance-time space and precomputes the future changes in the NN set. We analyze the performance of both algorithms and extend them to variations of NN search. Finally, we compare their efficiency through a comprehensive experimental evaluation. The skyline-based algorithm achieves lower CPU cost, at the expense of slightly larger space overhead.

#index 975052
#* Building a Multiple-Criteria Negotiation Support System
#@ Timon C. Du;Hsing-Ling Chen
#t 2007
#c 7
#% 107734
#% 252811
#% 280791
#% 287544
#% 302000
#% 320334
#% 417642
#% 487372
#% 588762
#% 591749
#% 591756
#% 659848
#% 725260
#% 734627
#% 815051
#% 912080
#% 917908
#% 1776246
#! Electronic negotiation systems have been devised to create an electronic marketplace for bargaining, auctions, reverse auctions, and exchanges between multiple buyers and sellers. Most studies of negotiation systems concentrate on negotiation process modeling and data modeling—rather than on strategies and efficiency—for a multiple-criteria decision making (MCDM) problem in which many criteria are taken into account as attributes for decision making. This study proposes an active collaboration and negotiation framework (ACNF), which is a negotiation support system that uses active documents with embedded business logics or business rules that can adapt to different collaborative strategies in a business-to-business (B2B) environment. The risk preferences of negotiators are modeled and measured by utility functions that provide mathematical tools to compute the relative value of different courses of action. The system is demonstrated, and three experiments are conducted to validate its performance. The experiments show that the negotiation process is very efficient, and the results are both close to the efficient point—or the Pareto frontier—and are fair to both negotiating parties. The framework can be used to efficiently and effectively achieve a settlement in various multiple-criteria bargaining schemes in the electronic marketplace.

#index 975053
#* Economics-Driven Data Management: An Application to the Design of Tabular Data Sets
#@ Adir Even;G. Shankaranarayanan;Paul D. Berger
#t 2007
#c 7
#% 214028
#% 242234
#% 262388
#% 322880
#% 344898
#% 375848
#% 382342
#% 385321
#% 388636
#% 442973
#% 445631
#% 452855
#% 770146
#% 923676
#% 995811
#% 1603096
#% 1603168
#! Organizational data repositories are recognized as critical resources for supporting a large variety of decision tasks and for enhancing business capabilities. As investments in data resources increase, there is also a growing concern about the economic aspects of data resources. While the technical aspects of data management are well examined, the contribution of data management to economic performance is not. Current design and implementation methodologies for data management are driven primarily by technical and functional requirements, without considering the relevant economic factors sufficiently. To address this gap, this study proposes a framework for optimizing data management design and maintenance decisions. The framework assumes that certain design characteristics of data repositories and data manufacturing processes significantly affect the utility of the data resources and the costs associated with implementing them. Modeling these effects helps identify design alternatives that maximize net-benefit, defined as the difference between utility and cost. The framework for the economic assessment of design alternatives is demonstrated for the optimal design of a large data set.

#index 975054
#* Verification of Medical Guidelines Using Background Knowledge in Task Networks
#@ Arjen Hommersom;Perry Groot;Peter J. F. Lucas;Michael Balser;Jonathan Schmitt
#t 2007
#c 7
#% 101955
#% 289236
#% 306015
#% 459503
#% 902767
#% 936869
#% 1727550
#% 1786803
#! The application of a medical guideline to the treatment of a patient's disease can be seen as the execution of tasks, sequentially or in parallel, in the face of patient data. It has been shown that many of such guidelines can be represented as a "network of tasks,” that is, as a sequence of steps that have a specific function or goal. In this paper, a novel methodology for verifying the quality of such guidelines is introduced. To investigate the quality of such guidelines, we propose to include medical background knowledge to task networks and to formalize criteria for good medical practice that a guideline should comply with. This framework was successfully applied to a guideline dealing with the management of diabetes mellitus type 2 by using KIV.

#index 975055
#* Credible Case-Based Inference Using Similarity Profiles
#@ Eyke Hullermeier
#t 2007
#c 7
#% 5182
#% 59336
#% 92533
#% 129212
#% 168280
#% 176887
#% 229931
#% 229972
#% 257885
#% 405727
#% 444007
#% 446658
#% 458669
#% 458703
#% 496264
#% 543918
#% 566639
#% 742781
#% 1269395
#% 1275276
#% 1389746
#% 1706008
#% 1706031
#! In this paper, we propose a method for retrieving promising candidate solutions in case-based problem solving. Our method, referred to as credible case-based inference, makes use of so-called similarity profiles as a formal model of the key hypothesis underlying case-based reasoning (CBR), namely, the assumption that similar problems have similar solutions. Proceeding from this formalization, it becomes possible to derive theoretical properties of the corresponding inference scheme in a rigorous way. In particular, it can be shown that, under mild technical conditions, a set of candidates covers the true solution with high probability. Thus, the approach supports an important subtask in CBR, namely, to generate potential solutions for a new target problem in a sound manner and hence contributes to the methodical foundations of CBR. Due to its generality, it can be employed for different types of performance tasks and can easily be integrated in existing CBR systems.

#index 975056
#* Rough-Fuzzy C-Medoids Algorithm and Selection of Bio-Basis for Amino Acid Sequence Analysis
#@ Pradipta Maji;Sankar K. Pal
#t 2007
#c 7
#% 263321
#% 366687
#% 452843
#% 760809
#% 778546
#% 944855
#% 1378398
#% 1781720
#% 1788198
#% 1796844
#% 1860401
#% 1861451
#! In most pattern recognition algorithms, amino acids cannot be used directly as inputs since they are nonnumerical variables. They, therefore, need encoding prior to input. In this regard, bio-basis function maps a nonnumerical sequence space to a numerical feature space. It is designed using an amino acid mutation matrix. One of the important issues for the bio-basis function is how to select the minimum set of bio-bases with maximum information. In this paper, we describe an algorithm, termed as rough-fuzzy c{\hbox{-}}{\rm{medoids}} (RFCMdd) algorithm, to select the most informative bio-bases. It is comprised of a judicious integration of the principles of rough sets, fuzzy sets, the c{\hbox{-}}{\rm{medoids}} algorithm, and the amino acid mutation matrix. While the membership function of fuzzy sets enables efficient handling of overlapping partitions, the concept of lower and upper bounds of rough sets deals with uncertainty, vagueness, and incompleteness in class definition. The concept of crisp lower bound and fuzzy boundary of a class, introduced in RFCMdd, enables efficient selection of the minimum set of the most informative bio-bases. Some new indices are introduced for evaluating quantitatively the quality of selected bio-bases. The effectiveness of the proposed algorithm, along with a comparison with other algorithms, has been demonstrated on different types of protein data sets.

#index 982751
#* Efficient Monitoring Algorithm for Fast News Alerts
#@ Ka Cheung Sia;Junghoo Cho;Hyun-Kyu Cho
#t 2007
#c 7
#% 297191
#% 300139
#% 300179
#% 309746
#% 330604
#% 330682
#% 333938
#% 344154
#% 348136
#% 348137
#% 397355
#% 443298
#% 480296
#% 577224
#% 577302
#% 577369
#% 577371
#% 640706
#% 643011
#% 643012
#% 731406
#% 754107
#% 805879
#% 818213
#% 818243
#% 879620
#% 993974
#% 1015259
#% 1715596
#! Recently, there has been a dramatic increase in the use of XML data to deliver information over the Web. Personal Weblogs, news Web sites, and discussion forums are now publishing RSS feeds for their subscribers to retrieve new postings. As the popularity of personal Weblogs and RSS feeds grows rapidly, RSS aggregation services and blog search engines have appeared, which try to provide a central access point for simpler access and discovery of new content from a large number of diverse RSS sources. In this paper, we study how the RSS aggregation services should monitor the data sources to retrieve new content quickly using minimal resources and to provide its subscribers with fast news alerts. We believe that the change characteristics of RSS sources and the general user access behavior pose distinct requirements that make this task significantly different from the traditional index refresh problem for Web search engines. Our studies on a collection of 10,000 RSS feeds reveal some general characteristics of the RSS feeds and show that, with proper resource allocation and scheduling, the RSS aggregator provides news alerts significantly faster than the best existing approach.

#index 982752
#* Top-k Monitoring in Wireless Sensor Networks
#@ Minji Wu;Jianliang Xu;Xueyan Tang;Wang-Chien Lee
#t 2007
#c 7
#% 309433
#% 333854
#% 333969
#% 480330
#% 631988
#% 654443
#% 654482
#% 654488
#% 712040
#% 745412
#% 745442
#% 755707
#% 763882
#% 768521
#% 783740
#% 788219
#% 800503
#% 800509
#% 805466
#% 812755
#% 822531
#% 824654
#% 824704
#% 832568
#% 864455
#% 864530
#% 870325
#% 874984
#% 931466
#% 1015264
#% 1016178
#% 1016183
#! Top-k monitoring is important to many wireless sensor applications. This paper exploits the semantics of top-k query and proposes an energy-efficient monitoring approach called FILA. The basic idea is to install a filter at each sensor node to suppress unnecessary sensor updates. Filter setting and query reevaluation upon updates are two fundamental issues to the correctness and efficiency of the FILA approach. We develop a query reevaluation algorithm that is capable of handling concurrent sensor updates. In particular, we present optimization techniques to reduce the probing cost. We design a skewed filter setting scheme, which aims to balance energy consumption and prolong network lifetime. Moreover, two filter update strategies, namely, eager and lazy, are proposed to favor different application scenarios. We also extend the algorithms to several variants of top-k query, that is, order-insensitive, approximate, and value monitoring. The performance of the proposed FILA approach is extensively evaluated using real data traces. The results show that FILA substantially outperforms the existing TAG-based approach and range caching approach in terms of both network lifetime and energy consumption under various network configurations.

#index 982753
#* Discovering and Exploiting Causal Dependencies for Robust Mobile Context-Aware Recommenders
#@ Ghim-Eng Yap;Ah-Hwee Tan;Hwee-Hwa Pang
#t 2007
#c 7
#% 17144
#% 67866
#% 124010
#% 170649
#% 220706
#% 220709
#% 234992
#% 264857
#% 277480
#% 301259
#% 414514
#% 420054
#% 449588
#% 643688
#% 724579
#% 753435
#% 779868
#% 781774
#% 801785
#% 814375
#% 829992
#% 840583
#% 849811
#% 926881
#% 1499473
#% 1650569
#% 1712951
#% 1719326
#! Acquisition of context poses unique challenges to mobile context-aware recommender systems. The limited resources in these systems make minimizing their context acquisition a practical need, and the uncertainty in the mobile environment makes missing and erroneous context inputs a major concern. In this paper, we propose an approach based on Bayesian networks (BNs) for building recommender systems that minimize context acquisition. Our learning approach iteratively trims the BN-based context model until it contains only the minimal set of context parameters that are important to a user. In addition, we show that a two-tiered context model can effectively capture the causal dependencies among context parameters, enabling a recommender system to compensate for missing and erroneous context inputs. We have validated our proposed techniques on a restaurant recommendation data set and a Web page recommendation data set. In both benchmark problems, the minimal sets of context can be reliably discovered for the specific users. Furthermore, the learned Bayesian network consistently outperforms the J4.8 decision tree in overcoming both missing and erroneous context inputs to generate significantly more accurate predictions.

#index 982754
#* A Note on Linear Time Algorithms for Maximum Error Histograms
#@ Sudipto Guha;Kyuseok Shim
#t 2007
#c 7
#% 2115
#% 43163
#% 201921
#% 210190
#% 274152
#% 287391
#% 338425
#% 397389
#% 399763
#% 411554
#% 479648
#% 481266
#% 689389
#% 801684
#% 866990
#% 1016154
#! Histograms and Wavelet synopses provide useful tools in query optimization and approximate query answering. Traditional histogram construction algorithms, e.g., V-Optimal, use error measures which are the sums of a suitable function, e.g., square, of the error at each point. Although the best-known algorithms for solving these problems run in quadratic time, a sequence of results have given us a linear time approximation scheme for these algorithms. In recent years, there have been many emerging applications where we are interested in measuring the maximum (absolute or relative) error at a point. We show that this problem is fundamentally different from the other traditional {\rm{non}}{\hbox{-}}\ell_\infty error measures and provide an optimal algorithm that runs in linear time for a small number of buckets. We also present results which work for arbitrary weighted maximum error measures.

#index 982755
#* The Concentration of Fractional Distances
#@ Damien Francois;Vincent Wertz;Michel Verleysen
#t 2007
#c 7
#% 41590
#% 80995
#% 227856
#% 227939
#% 237187
#% 249321
#% 280452
#% 296738
#% 332094
#% 342827
#% 342828
#% 435141
#% 443396
#% 451653
#% 465017
#% 479649
#% 480132
#% 480307
#% 481460
#% 481956
#% 487887
#% 631963
#% 632011
#% 632035
#% 722151
#% 731404
#% 789225
#% 1016130
#% 1377635
#% 1715623
#% 1739420
#% 1815582
#% 1857498
#! Nearest neighbor search and many other numerical data analysis tools most often rely on the use of the euclidean distance. When data are high dimensional, however, the euclidean distances seem to concentrate; all distances between pairs of data elements seem to be very similar. Therefore, the relevance of the euclidean distance has been questioned in the past, and fractional norms (Minkowski-like norms with an exponent less than one) were introduced to fight the concentration phenomenon. This paper justifies the use of alternative distances to fight concentration by showing that the concentration is indeed an intrinsic property of the distances and not an artifact from a finite sample. Furthermore, an estimation of the concentration as a function of the exponent of the distance and of the distribution of the data is given. It leads to the conclusion that, contrary to what is generally admitted, fractional norms are not always less concentrated than the euclidean norm; a counterexample is given to prove this claim. Theoretical arguments are presented, which show that the concentration phenomenon can appear for real data that do not match the hypotheses of the theorems, in particular, the assumption of independent and identically distributed variables. Finally, some insights about how to choose an optimal metric are given.

#index 982756
#* Enhancing the Effectiveness of Clustering with Spectra Analysis
#@ Wenyuan Li;Wee-Keong Ng;Ying Liu;Kok-Leong Ong
#t 2007
#c 7
#% 14511
#% 224113
#% 290830
#% 296738
#% 313959
#% 338443
#% 342621
#% 420083
#% 430746
#% 438137
#% 466675
#% 479791
#% 577296
#% 594009
#% 773681
#! For many clustering algorithms, such as K-Means, EM, and CLOPE, there is usually a requirement to set some parameters. Often, these parameters directly or indirectly control the number of clusters, that is, k, to return. In the presence of different data characteristics and analysis contexts, it is often difficult for the user to estimate the number of clusters in the data set. This is especially true in text collections such as Web documents, images, or biological data. In an effort to improve the effectiveness of clustering, we seek the answer to a fundamental question: How can we effectively estimate the number of clusters in a given data set? We propose an efficient method based on spectra analysis of eigenvalues (not eigenvectors) of the data set as the solution to the above. We first present the relationship between a data set and its underlying spectra with theoretical and experimental results. We then show how our method is capable of suggesting a range of k that is well suited to different analysis contexts. Finally, we conclude with further empirical results to show how the answer to this fundamental question enhances the clustering process for large text collections.

#index 982757
#* Efficient Computation of Iceberg Cubes by Bounding Aggregate Functions
#@ Xiuzhen Zhang;Pauline Lienhua Chou;Guozhu Dong
#t 2007
#c 7
#% 227880
#% 248785
#% 273916
#% 333925
#% 420053
#% 464989
#% 479450
#% 480154
#% 481290
#% 481951
#% 631970
#% 796200
#% 1015294
#! The iceberg cubing problem is to compute the multidimensional group-by partitions that satisfy given aggregation constraints. Pruning unproductive computation for iceberg cubing when nonantimonotone constraints are present is a great challenge because the aggregate functions do not increase or decrease monotonically along the subset relationship between partitions. In this paper, we propose a novel bound prune cubing (BP-Cubing) approach for iceberg cubing with nonantimonotone aggregation constraints. Given a cube over n dimensions, an aggregate for any group-by partition can be computed from aggregates for the most specific n--dimensional partitions (MSPs). The largest and smallest aggregate values computed this way become the bounds for all partitions in the cube. We provide efficient methods to compute tight bounds for base aggregate functions and, more interestingly, arithmetic expressions thereof, from bounds of aggregates over the MSPs. Our methods produce tighter bounds than those obtained by previous approaches. We present iceberg cubing algorithms that combine bounding with efficient aggregation strategies. Our experiments on real-world and artificial benchmark data sets demonstrate that BP-Cubing algorithms achieve more effective pruning and are several times faster than state-of-the-art iceberg cubing algorithms and that BP-Cubing achieves the best performance with the top-down cubing approach.

#index 982758
#* Efficient Approximate Query Processing in Peer-to-Peer Networks
#@ Benjamin Arai;Gautam Das;Dimitrios Gunopulos;Vana Kalogeraki
#t 2007
#c 7
#% 248821
#% 283833
#% 299989
#% 303694
#% 333955
#% 340175
#% 340176
#% 397351
#% 465162
#% 479804
#% 505869
#% 654486
#% 656792
#% 723903
#% 731091
#% 745498
#% 754109
#% 765424
#% 765425
#% 768512
#% 770901
#% 783948
#% 910588
#% 1015281
#% 1722237
#! Peer-to-peer (P2P) databases are becoming prevalent on the Internet for distribution and sharing of documents, applications, and other digital media. The problem of answering large-scale ad hoc analysis queries, for example, aggregation queries, on these databases poses unique challenges. Exact solutions can be time consuming and difficult to implement, given the distributed and dynamic nature of P2P databases. In this paper, we present novel sampling-based techniques for approximate answering of ad hoc aggregation queries in such databases. Computing a high-quality random sample of the database efficiently in the P2P environment is complicated due to several factors: the data is distributed (usually in uneven quantities) across many peers, within each peer, the data is often highly correlated, and, moreover, even collecting a random sample of the peers is difficult to accomplish. To counter these problems, we have developed an adaptive two-phase sampling approach based on random walks of the P2P graph, as well as block-level sampling techniques. We present extensive experimental evaluations to demonstrate the feasibility of our proposed solution.

#index 982759
#* SPEX: Streamed and Progressive Evaluation of XPath
#@ Dan Olteanu
#t 2007
#c 7
#% 310488
#% 340582
#% 369818
#% 465061
#% 480296
#% 487257
#% 570880
#% 576108
#% 654477
#% 659995
#% 733593
#% 800620
#% 801685
#% 809253
#% 864465
#% 927031
#% 976789
#% 993939
#% 993950
#% 1015373
#! Streams are preferable over data stored in memory in contexts where data is too large or volatile, or a standard approach to data processing based on storing is too time or space consuming. Emerging applications such as publish-subscribe systems, data monitoring in sensor networks, financial and traffic monitoring, and routing of MPEG-7 call for querying streams. In many such applications, XML streams are arguably more appropriate than flat streams, for they convey (possibly unbounded) unranked ordered trees with labeled nodes. However, the flexibility enabled by XML streams in data modeling makes query evaluation different from traditional settings and challenging. This paper describes SPEX, a streamed and progressive evaluation of XML Path Language (XPath). SPEX compiles queries into networks of simple and independent transducers and processes XML streams with polynomial combined complexity. This makes SPEX especially suitable for implementation on devices with low memory and simple logic as used, for example, in mobile computing.

#index 982760
#* Hot Topic Extraction Based on Timeline Analysis and Multidimensional Sentence Modeling
#@ Kuan-Yu Chen;Luesak Luesukprasert;Seng-cho T. Chou
#t 2007
#c 7
#% 118771
#% 278107
#% 287196
#% 309096
#% 340995
#% 413593
#% 482511
#% 577220
#% 577297
#% 735078
#% 766444
#% 766460
#% 815278
#! With the vast amount of digitized textual materials now available on the Internet, it is almost impossible for people to absorb all pertinent information in a timely manner. To alleviate the problem, we present a novel approach for extracting hot topics from disparate sets of textual documents published in a given time period. Our technique consists of two steps. First, hot terms are extracted by mapping their distribution over time. Second, based on the extracted hot terms, key sentences are identified and then grouped into clusters that represent hot topics by using multidimensional sentence vectors. The results of our empirical tests show that this approach is more effective in identifying hot topics than existing methods.

#index 982761
#* A Family of Directional Relation Models for Extended Objects
#@ Spiros Skiadopoulos;Nikos Sarkas;Timos Sellis;Manolis Koubarakis
#t 2007
#c 7
#% 26168
#% 181229
#% 201880
#% 270714
#% 329895
#% 333684
#% 409857
#% 463425
#% 481283
#% 489430
#% 526851
#% 526998
#% 549078
#% 560268
#% 589627
#% 590627
#% 710974
#% 741459
#% 806735
#% 837603
#% 1275343
#! In this paper, we introduce a family of expressive models for qualitative spatial reasoning with directions. The proposed family is based on the cognitive plausible cone-based model. We formally define the directional relations that can be expressed in each model of the family. Then, we use our formal framework to study two interesting problems: computing the inverse of a directional relation and composing two directional relations. For the composition operator, in particular, we concentrate on two commonly used definitions, namely, consistency-based and existential composition. Our formal framework allows us to prove that our solutions are correct. The presented solutions are handled in a uniform manner and apply to all of the models of the family.

#index 982762
#* Toward Exploratory Test-Instance-Centered Diagnosis in High-Dimensional Classification
#@ Charu C. Aggarwal
#t 2007
#c 7
#% 4868
#% 90661
#% 136350
#% 209623
#% 248792
#% 273891
#% 280511
#% 310517
#% 321059
#% 342613
#% 420084
#% 436509
#% 438134
#% 449566
#% 459008
#% 466744
#% 479640
#% 481290
#% 481945
#% 677078
#% 743911
#% 823378
#% 1011200
#% 1499572
#! High-dimensional data is a difficult case for most subspace-based classification methods because of the large number of combinations of dimensions, which have discriminatory power. This is because there are an exponential number of combinations of dimensions that could decide the correct class instance, and this combination could vary with data locality and test instance. Therefore, most summarized models such as decision trees and rule-based systems only aim to have a global summary of the data, which is used for classification. Because of this incompleteness, a particular classification model may be more or less suited to individual test instances. Furthermore, it may not provide sufficient insight into the most representative characteristics of a particular test instance. This is undesirable for many classification applications in which the diagnostic reasoning behind the classification of a test instance is as important as the classification process itself. In an interactive application, a user may find it more valuable to develop a diagnostic decision support method, which can reveal significant classification behaviors of exemplar records. Such an approach has the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification methods. In this paper, we propose the Subspace Decision Path (SD-Path) method, which provides the user with the ability to interactively explore a small number of nodes of a hierarchical decision process so that the most significant classification characteristics for a given test instance are revealed. In addition, the SD-Path method can provide enormous interpretability by constructing views of the data in which the different classes are clearly separated out. Even in difficult cases where the classification behavior of the test instance is ambiguous, the SD-Path method provides a diagnostic understanding of the characteristics, which results in this ambiguity. Therefore, this method combines the abilities of the human and the computer in creating an effective diagnostic tool for instance-centered high-dimensional classification.

#index 982763
#* An Entropy Weighting k-Means Algorithm for Subspace Clustering of High-Dimensional Sparse Data
#@ Liping Jing;Michael K. Ng;Joshua Zhexue Huang
#t 2007
#c 7
#% 248792
#% 252400
#% 273891
#% 280417
#% 296738
#% 300131
#% 333941
#% 397384
#% 480307
#% 481281
#% 580509
#% 659967
#% 715706
#% 765518
#% 778729
#% 800188
#% 800529
#% 837604
#% 1676115
#% 1684730
#% 1707873
#! This paper presents a new k-means type algorithm for clustering high-dimensional objects in subspaces. In high-dimensional data, clusters of objects often exist in subspaces rather than in the entire space. For example, in text clustering, clusters of documents of different topics are categorized by different subsets of terms or keywords. The keywords for one cluster may not occur in the documents of other clusters. This is a data sparsity problem faced in clustering high-dimensional data. In the new algorithm, we extend the k{\hbox{-}}{\rm{means}} clustering process to calculate a weight for each dimension in each cluster and use the weight values to identify the subsets of important dimensions that categorize different clusters. This is achieved by including the weight entropy in the objective function that is minimized in the k{\hbox{-}}{\rm{means}} clustering process. An additional step is added to the k{\hbox{-}}{\rm{means}} clustering process to automatically compute the weights of all dimensions in each cluster. The experiments on both synthetic and real data have shown that the new algorithm can generate better clustering results than other subspace clustering algorithms. The new algorithm is also scalable to large data sets.

#index 982764
#* Frequent Closed Sequence Mining without Candidate Maintenance
#@ Jianyong Wang;Jiawei Han;Chun Li
#t 2007
#c 7
#% 310559
#% 329537
#% 338609
#% 397383
#% 413550
#% 459006
#% 463903
#% 464839
#% 477791
#% 479971
#% 481290
#% 501994
#% 577256
#% 629623
#% 629644
#% 727913
#% 729933
#% 729953
#% 778732
#% 823384
#% 835872
#% 844306
#% 844401
#% 863436
#% 871454
#! Previous studies have presented convincing arguments that a frequent pattern mining algorithm should not mine all frequent patterns but only the closed ones because the latter leads to not only a more compact yet complete result set but also better efficiency. However, most of the previously developed closed pattern mining algorithms work under the candidate maintenance-and-test paradigm, which is inherently costly in both runtime and space usage when the support threshold is low or the patterns become long. In this paper, we present BIDE, an efficient algorithm for mining frequent closed sequences without candidate maintenance. It adopts a novel sequence closure checking scheme called BI-Directional Extension and prunes the search space more deeply compared to the previous algorithms by using the BackScan pruning method. A thorough performance study with both sparse and dense, real, and synthetic data sets has demonstrated that BIDE significantly outperforms the previous algorithm: It consumes an order(s) of magnitude less memory and can be more than an order of magnitude faster. It is also linearly scalable in terms of database size.

#index 982765
#* Maintaining Strong Cache Consistency for the Domain Name System
#@ Xin Chen;Haining Wang;Shansi Ren;Xiaodong Zhang
#t 2007
#c 7
#% 65498
#% 129086
#% 188026
#% 232771
#% 249968
#% 309451
#% 344398
#% 344408
#% 402267
#% 443295
#% 449070
#% 496158
#% 577363
#% 580991
#% 615735
#% 721150
#% 725358
#% 770898
#% 770899
#% 771771
#% 781683
#% 781685
#% 963597
#% 963673
#% 963887
#% 979357
#! Effective caching in the Domain Name System (DNS) is critical to its performance and scalability. Existing DNS only supports weak cache consistency by using the Time-to-Live (TTL) mechanism, which functions reasonably well in normal situations. However, maintaining strong cache consistency in DNS as an indispensable exceptional handling mechanism has become more and more demanding for three important objectives: 1) to quickly respond and handle exceptions such as sudden and dramatic Internet failures caused by natural and human disasters, 2) to adapt increasingly frequent changes of Internet Protocol (IP) addresses due to the introduction of dynamic DNS techniques for various stationed and mobile devices on the Internet, and 3) to provide fine-grain controls for content delivery services to timely balance server load distributions. With agile adaptation to various exceptional Internet dynamics, strong DNS cache consistency improves the availability and reliability of Internet services. In this paper, we first conduct extensive Internet measurements to quantitatively characterize DNS dynamics. Then, we propose a proactive DNS cache update protocol (DNScup), running as middleware in DNS name servers, to provide strong cache consistency for DNS. The core of DNScup is an optimal lease scheme, called dynamic lease, to keep track of the local DNS name servers. We compare dynamic lease with other existing lease schemes through theoretical analysis and trace-driven simulations. Based on the DNS Dynamic Update protocol, we build a DNScup prototype with minor modifications to the current DNS implementation. Our system prototype demonstrates the effectiveness of DNScup and its easy and incremental deployment on the Internet.

#index 982766
#* Efficient Skyline and Top-k Retrieval in Subspaces
#@ Yufei Tao;Xiaokui Xiao;Jian Pei
#t 2007
#c 7
#% 86950
#% 213981
#% 287466
#% 300180
#% 318703
#% 465167
#% 466425
#% 480671
#% 481956
#% 654480
#% 733373
#% 800555
#% 806212
#% 810024
#% 824670
#% 824671
#% 824672
#% 824704
#% 864451
#% 864453
#% 875011
#% 875012
#% 941785
#% 993954
#% 1688273
#! Skyline and top-k queries are two popular operations for preference retrieval. In practice, applications that require these operations usually provide numerous candidate attributes, whereas, depending on their interests, users may issue queries regarding different subsets of the dimensions. The existing algorithms are inadequate for subspace skyline/top-k search because they have at least one of the following defects: 1) They require scanning the entire database at least once, 2) they are optimized for one subspace but incur significant overhead for other subspaces, or 3) they demand expensive maintenance cost or space consumption. In this paper, we propose a technique SUBSKY, which settles both types of queries by using purely relational technologies. The core of SUBSKY is a transformation that converts multidimensional data to one-dimensional (1D) values. These values are indexed by a simple B-tree, which allows us to answer subspace queries by accessing a fraction of the database. SUBSKY entails low maintenance overhead, which equals the cost of updating a traditional B-tree. Extensive experiments with real data confirm that our technique outperforms alternative solutions significantly in both efficiency and scalability.

#index 982767
#* A Method for Estimating the Precision of Placename Matching
#@ Martin Doerr;Manos Papagelis
#t 2007
#c 7
#% 254216
#% 266102
#% 287222
#% 312166
#% 420072
#% 481607
#% 729913
#% 744539
#% 1041075
#! Information in digital libraries and information systems frequently refers to locations or objects in geographic space. Digital gazetteers are commonly employed to match the referred placenames with actual locations in information integration and data cleaning procedures. This process may fail due to missing information in the gazetteer, multiple matches, or false positive matches. We have analyzed the cases of success and reasons for failure of the mapping process to a gazetteer. Based on these, we present a statistical model that permits estimating 1) the completeness of a gazetteer with respect to the specific target area and application, 2) the expected precision and recall of one-to-one mappings of source placenames to the gazetteer, 3) the semantic inconsistency that remains in one-to-one mappings, and 4) the degree to which the precision and recall are improved under knowledge of the identity of higher levels in a hierarchy of places. The presented model is based on statistical analysis of the mapping process of a large set of placenames itself and does not require any other background data. The statistical model assumes that a gazetteer is populated by a stochastic process. The paper discusses how future work could take deviations from this assumption into account. The method has been applied to a real case.

#index 982768
#* Ontology-Based Service Representation and Selection
#@ Murat Şensoy;Pınar Yolum
#t 2007
#c 7
#% 176887
#% 378940
#% 378981
#% 379191
#% 438669
#% 490920
#% 607817
#% 636340
#% 643249
#% 729437
#% 773234
#% 778664
#% 821845
#% 823936
#% 890346
#% 927337
#% 1784807
#! Selecting the right parties to interact with is a fundamental problem in open and dynamic environments. The problem is amplified when the number of interacting parties is high, and the parties' reasons for selecting others vary. We examine the problem of service selection in an e-commerce setting where consumer agents cooperate to identify service providers that would satisfy their service needs the most. Previous approaches to service selection are usually based on capturing and exchanging the ratings of consumers to providers. Rating-based approaches have two major weaknesses. 1) ratings are given in a particular context. Even though the context is crucial for interpreting the ratings correctly, the rating-based approaches do not provide the means to represent the context explicitly. 2) The satisfaction criteria of the rater is unknown. Without knowing the expectation of the rater, it is almost impossible to make sense of a rating. We deal with these two weaknesses in two steps. First, we extend a classical rating-based approach by adding a representation of context. This addition improves the accuracy of selected service providers only when two consumers with the same service request are assumed to be satisfied with the same service. Next, we replace ratings with detailed experiences of consumers. The experiences are represented with an ontology that can capture the requested service and the received service in detail. When a service consumer decides to share her experiences with a second service consumer, the receiving consumer evaluates the experience by using her own context and satisfaction criteria. By sharing experiences rather than ratings, the service consumers can model service providers more accurately and, thus, can select service providers that are better suited for their needs.

#index 982769
#* On Three Types of Covering-Based Rough Sets
#@ William Zhu;Fei-Yue Wang
#t 2007
#c 7
#% 8380
#% 217733
#% 263167
#% 268136
#% 268148
#% 273085
#% 347878
#% 350051
#% 366687
#% 443346
#% 500368
#% 500399
#% 581220
#% 610057
#% 641961
#% 653544
#% 737974
#% 792811
#% 796207
#% 808029
#% 826147
#% 914739
#% 916700
#% 942344
#% 949379
#% 972291
#% 1408544
#% 1408545
#% 1408546
#% 1665790
#% 1665791
#% 1670444
#% 1670445
#% 1678493
#% 1696088
#% 1696148
#% 1731181
#% 1787856
#% 1788479
#! Rough set theory is a useful tool for data mining. It is based on equivalence relations and has been extended to covering-based generalized rough set. This paper studies three kinds of covering generalized rough sets for dealing with the vagueness and granularity in information systems. First, we examine the properties of approximation operations generated by a covering in comparison with those of the Pawlak's rough sets. Then, we propose concepts and conditions for two coverings to generate an identical lower approximation operation and an identical upper approximation operation. After the discussion on the interdependency of covering lower and upper approximation operations, we address the axiomization issue of covering lower and upper approximation operations. In addition, we study the relationships between the covering lower approximation and the interior operator and also the relationships between the covering upper approximation and the closure operator. Finally, this paper explores the relationships among these three types of covering rough sets.

#index 982770
#* Localized Outlying and Boundary Data Detection in Sensor Networks
#@ Weili Wu;Xiuzhen Cheng;Min Ding;Kai Xing;Fang Liu;Ping Deng
#t 2007
#c 7
#% 401232
#% 727847
#% 728011
#% 729437
#% 729855
#% 731484
#% 737944
#% 737946
#% 807555
#% 1016178
#% 1190454
#! This paper targets the identification of outlying sensors (that is, outlying reading sensors) and the detection of the reach of events in sensor networks. Typical applications include the detection of the transportation front line of some vegetation or animalcule's growth over a certain geographical region. We propose and analyze two novel algorithms for outlying sensor identification and event boundary detection. These algorithms are purely localized and, thus, scale well to large sensor networks. Their computational overhead is low, since only simple numerical operations are involved. Simulation results indicate that these algorithms can clearly detect the event boundary and can identify outlying sensors with a high accuracy and a low false alarm rate when as many as 20 percent sensors report outlying readings. Our work is exploratory in that the proposed algorithms can accept any kind of scalar values as inputs—a dramatic improvement over existing work, which takes only 0/1 decision predicates. Therefore, our algorithms are generic. They can be applied as long as "events” can be modeled by numerical numbers. Though designed for sensor networks, our algorithms can be applied to the outlier detection and regional data analysis in spatial data mining.

#index 1013603
#* Distributed Nearest Neighbor-Based Condensation of Very Large Data Sets
#@ Fabrizio Angiulli;Gianluigi Folino
#t 2007
#c 7
#% 5182
#% 92533
#% 220037
#% 229943
#% 252304
#% 307100
#% 382327
#% 420138
#% 428413
#% 465912
#% 657817
#% 722807
#% 760777
#% 803575
#% 818916
#% 840838
#% 875957
#% 1656017
#% 1860566
#% 1861143
#! In this work, PFCNN, a distributed method for computing a consistent subset of very large data set for the nearest neighbor classification rule is presented. In order to cope with the communication overhead typical of distributed environments and to reduce memory requirements, different variants of the basic PFCNN method are introduced. An analysis of spatial cost, CPU cost, and communication overhead is accomplished for all the algorithms. Experimental results, performed on both synthetic and real very large data sets, revealed that these methods can be profitably applied to enormous collections of data. Indeed, they scale-up well and are efficient in memory consumption, confirming the theoretical analysis, and achieve noticeable data reduction and good classification accuracy. To the best of our knowledge, this is the first distributed algorithm for computing a training set consistent subset for the nearest neighbor rule.

#index 1013604
#* Maximal Biclique Subgraphs and Closed Pattern Pairs of the Adjacency Matrix: A One-to-One Correspondence and Mining Algorithms
#@ Jinyan Li;Guimei Liu;Haiquan Li;Limsoon Wong
#t 2007
#c 7
#% 152934
#% 171557
#% 197751
#% 281214
#% 309749
#% 431105
#% 477661
#% 629708
#% 727845
#% 729933
#% 729938
#% 731608
#% 769907
#% 769940
#% 772830
#% 778215
#% 779960
#% 785402
#% 802053
#% 810072
#% 823347
#% 823357
#% 824931
#% 826008
#% 833025
#% 833028
#% 833120
#% 905904
#% 1738863
#! Enumerating maximal biclique subgraphs from a graph is a computationally challenging problem. In this paper, we efficiently enumerate them through the use of closed patterns of the adjacency matrix of the graph. For an undirected graph $G$ without self-loops, we prove that: (i) the number of closed patterns in the adjacency matrix of $G$ is even; and (ii) for every maximal biclique subgraph, there always exists a unique pair of closed patterns that matches the two vertex sets of the subgraph. Therefore, the problem of enumerating maximal bicliques can be solved by using efficient algorithms for mining closed patterns, which are algorithms extensively studied in the data mining field. However, this direct use of existing algorithms causes a duplicated enumeration. To achieve high efficiency, we propose an $O(mn)$ time delay algorithm for a non-duplicated enumeration, in particular for enumerating those maximal bicliques with a large size, where $m$ and $n$ are the number of edges and vertices of the graph respectively. We evaluate the high efficiency of our algorithm by comparing it to state-of-the-art algorithms on many graphs.

#index 1013605
#* Combining Subclassifiers in Text Categorization: A DST-Based Solution and a Case Study
#@ Kanoksri Sarinnapakorn;Miroslav Kubat
#t 2007
#c 7
#% 73372
#% 165111
#% 198701
#% 219052
#% 235377
#% 246832
#% 252403
#% 302391
#% 311034
#% 318412
#% 344447
#% 375017
#% 420528
#% 445319
#% 458379
#% 458696
#% 465754
#% 466240
#% 544511
#% 770783
#% 779082
#% 786633
#% 789853
#% 818236
#% 824928
#% 844057
#% 1036013
#% 1272353
#% 1388992
#! Text categorization systems often induce document classifiers from pre-classified examples by the use of machine learning techniques. The circumstance that each example-document can belong to many different classes often leads to impractically high computational costs that sometimes grow exponentially in the number of features. Looking for ways to reduce these costs, we explored the possibility of running a ``baseline induction algorithm'' separately for subsets of features, obtaining a set of classifiers to be combined. For the specific case of classifiers that return not only class labels but also confidences in these labels, we investigate here a few alternative fusion techniques, including our own mechanism that was inspired by the Dempster-Shafer Theory. The paper describes the algorithm and, in our specific case study, compares its performance to that of more traditional mechanisms.

#index 1013606
#* To Select or To Weigh: A Comparative Study of Linear Combination Schemes for SuperParent-One-Dependence Estimators
#@ Ying Yang;Geoffrey I. Webb;Jesus Cerquides;Kevin B. Korb;Janice Boughton;Kai Ming Ting
#t 2007
#c 7
#% 129987
#% 246831
#% 246832
#% 290482
#% 312728
#% 321059
#% 375636
#% 420054
#% 449566
#% 458168
#% 458259
#% 466252
#% 466583
#% 486328
#% 486797
#% 502131
#% 643688
#% 770854
#% 799040
#% 840963
#% 843342
#% 876084
#% 900940
#% 906547
#% 913833
#% 961134
#% 1269497
#% 1269504
#% 1650783
#% 1665169
#% 1674146
#% 1699581
#% 1781125
#! We conduct a large-scale comparative study on linearly combining superparent-one-dependence estimators (SPODEs), a popular family of semi-naive Bayesian classifiers. Altogether 16 model selection and weighing schemes, 58 benchmark data sets, as well as various statistical tests are employed. This paper's main contributions are three-fold. First, it formally presents each scheme's definition, rationale and time complexity; and hence can serve as a comprehensive reference for researchers interested in ensemble learning. Second, it offers bias-variance analysis for each scheme's classification error performance. Third, it identifies effective schemes that meet various needs in practice. This leads to accurate and fast classification algorithms with immediate and significant impact on real-world applications. Another important feature of our study is using a variety of statistical tests to evaluate multiple learning methods across multiple data sets.

#index 1013607
#* A Normalization Framework for Multimedia Databases
#@ Shi-Kuo Chang;Vincenzo Deufemia;Giuseppe Polese;Mario Vacca
#t 2007
#c 7
#% 36244
#% 111368
#% 176172
#% 183355
#% 197915
#% 206917
#% 213981
#% 216545
#% 249928
#% 284557
#% 287754
#% 301190
#% 329613
#% 379326
#% 406493
#% 437405
#% 442961
#% 452796
#% 479987
#% 742566
#% 771227
#% 784963
#% 808740
#% 860956
#% 923676
#% 940694
#% 1775157
#% 1855648
#% 1857840
#% 1858012
#! We present a normalization framework for designing of multimedia database schemes with reduced manipulation anomalies. To this end we introduce new extended dependencies involving different types of multimedia data. Such dependencies are based on distance functions that are used to detect semantic relationships between complex data types. Based upon these new dependencies, we have defined five multimedia normal forms. Finally, we have performed a simulation on a large image dataset to analyze the impact of the proposed framework in the context of content-based retrieval applications and in e-learning applications.

#index 1013608
#* Product Life-Cycle Metadata Modeling and Its Application with RDF
#@ Hong-Bae Jun;Dimitris Kiritsis;Paul Xirouchakis
#t 2007
#c 7
#% 122671
#% 332596
#% 392169
#% 443078
#% 445443
#% 482524
#% 523477
#% 576214
#% 728100
#% 738955
#% 824747
#% 1548403
#% 1841531
#! The whole product lifecycle consists of three phases: Beginning Of Life (BOL), Middle Of Life (MOL), and End Of Life (EOL). Although large amounts of product lifecycle data are generated over the whole product lifecycle, data flows are rather vague after BOL. Over the last decade, however, emerging Internet, wireless mobile telecommunications, and product identification technologies have created the potential to make the whole product lifecycle visible. As a result, the scope of data to be managed has expanded over the whole product lifecycle. Hence, it becomes important to describe product lifecycle meta data in a systematic manner. Although much attention has been paid to data modeling over several objects such as products and processes, modeling methodology for product lifecycle meta data is not well developed. To cope with this limitation, we develop a modeling method for product lifecycle meta data using the resource description framework (RDF). We define an RDF data model and its schema for describing and managing product lifecycle meta data. In addition, we describe how the proposed RDF model can be usefully applied to track, trace, and infer product lifecycle data with an RDF query language.

#index 1013609
#* Semantic-Aware and QoS-Aware Image Caching in Ad Hoc Networks
#@ Bo Yang;Ali R. Hurson
#t 2007
#c 7
#% 120270
#% 152934
#% 259642
#% 286949
#% 309461
#% 399548
#% 656764
#% 718329
#% 724236
#% 780688
#% 1549278
#! Caching has been widely used in the mobile environments to improve system performance. However, traditional semantic caching methodology was proposed for structural data such as 2-D location, and cannot be directly used for image data accessing: First, traditional caching relies on exact match and therefore is unsuitable for similarity-based queries. Second, the description of cached data is defined based on query context instead of data content, which leads to inefficient use of storage. Third, the description of cached data does not reflect the popularity of the data, making it inefficient in providing QoS-related services. To facilitate content-based image retrieval in mobile environments, we propose a semantic-aware image caching scheme (SAIC) in this paper. The proposed scheme can efficiently utilize the cache space and significantly reduce the cost of image retrieval. The proposed SAIC scheme is based on several innovative ideas: 1) multi-level partitioning of the semantic space, 2) association and Bayesian probability based content prediction, 3) constraint-based representation method showing the semantic similarity between images, 4) non-flooding query processing, and 5) adaptive QoS-aware cache consistency maintenance. The proposed model is introduced and through extensive simulation its behavior has been compared against two state-of-the-art caching schemes as advanced in the literature.

#index 1013610
#* Modeling Causal Reinforcement and Undermining for Efficient CPT Elicitation
#@ Yang Xiang;Ning Jia
#t 2007
#c 7
#% 44876
#% 297171
#% 359572
#% 1781505
#% 1784146
#! Representation of uncertain knowledge using a Bayesian network requires acquisition of a conditional probability table (CPT) for each variable. The CPT can be acquired by data mining or elicitation. When data are insufficient to support mining, causal modeling, such as the noisy-OR, aids elicitation by reducing the number of probability parameters to be acquired from human experts. Multiple causes can reinforce each other in producing the effect or can undermine the impact of each other. Most existing causal models do not consider causal interactions from the perspective of reinforcement or undermining. Our analysis shows that none can represent both interactions. Except the RNOR, other models also limit parameters to probabilities of single-cause events. We present the first causal model, the non-impeding noisy-AND tree, that allows encoding of both reinforcement and undermining. The model generalizes several existing models for the binary case. It supports efficient CPT acquisition by elicitating a partial ordering of causes in terms of a tree topology, plus necessary numerical parameters. It also allows incorporation of probabilities for multi-cause events.

#index 1013611
#* Preventing Location-Based Identity Inference in Anonymous Spatial Queries
#@ Panos Kalnis;Gabriel Ghinita;Kyriakos Mouratidis;Dimitris Papadias
#t 2007
#c 7
#% 67453
#% 68091
#% 86950
#% 264163
#% 300184
#% 443397
#% 443463
#% 452685
#% 576761
#% 657736
#% 800515
#% 801690
#% 801784
#% 810011
#% 812797
#% 812799
#% 824726
#% 843877
#% 863155
#% 864412
#% 874989
#% 893151
#% 907397
#% 911803
#% 956531
#% 993955
#% 1022265
#% 1409348
#% 1676510
#% 1729021
#! The increasing trend of embedding positioning capabilities (e.g., GPS) in mobile devices facilitates the widespread use of Location Based Services. For such applications to succeed, privacy and confidentiality are essential. Existing privacy-enhancing techniques rely on encryption to safeguard communication channels, and on pseudonyms to protect user identities. Nevertheless, the query contents may disclose the physical location of the user. In this paper, we present a framework for preventing Location-based identity inference of users who issue spatial queries to Location Based Services. We propose transformations based on the well-established K-anonymity concept to compute exact answers for range and nearest neighbor search, without revealing the query source. Our methods optimize the entire process of anonymizing the requests and processing the transformed spatial queries. Extensive experimental studies suggest that the proposed techniques are applicable to real-life scenarios with numerous mobile users.

#index 1013612
#* Mining Multimedia Streams in Large-Scale Distributed Environments
#@ 
#t 2007
#c 7

#index 1013613
#* Top-Down Parameter-Free Clustering of High-Dimensional Categorical Data
#@ Eugenio Cesario;Giuseppe Manco;Riccardo Ortale
#t 2007
#c 7
#% 36672
#% 114667
#% 210173
#% 248790
#% 248792
#% 280419
#% 287285
#% 310516
#% 314054
#% 316481
#% 329531
#% 342594
#% 397597
#% 413618
#% 420081
#% 420144
#% 424809
#% 443531
#% 451052
#% 466073
#% 466425
#% 478774
#% 570885
#% 577296
#% 722904
#% 754412
#% 765518
#% 769896
#% 770826
#% 789003
#% 794518
#% 800532
#% 826918
#% 857458
#! A parameter-free, fully-automatic approach to clustering high-dimensional categorical data is proposed. The technique is based on a two-phase iterative procedure, which attempts to improve the overall quality of the whole partition. In the first phase, cluster assignments are given, and a new cluster is added to the partition by choosing and splitting a low-quality cluster. In the second phase, the number of clusters is fixed, and an attempt to optimize cluster assignments is done. On the basis of such features, the algorithm attempts to improve the overall quality of the whole partition and finds clusters in the data, whose number is naturally established on the basis of the inherent features of the underlying dataset, rather than being previously specified. Furthermore, the approach is parametric to the notion of cluster quality: here, a cluster is defined as a set of tuples exhibiting a sort of homogeneity. We show how a suitable notion of cluster homogeneity can be defined in the context of high dimensional categorical data, from which an effective instance of the proposed clustering scheme immediately follows. Experiments on both synthetic and real data prove that the devised algorithm scales linearly and achieves nearly-optimal results in terms of compactness and separation.

#index 1013614
#* TKDE Guidelines for Survey Papers
#@ Christopher Clifton;Xindong Wu;Cristos Faloutsos
#t 2007
#c 7

#index 1013615
#* Fast Nearest Neighbor Condensation for Large Data Sets Classification
#@ Fabrizio Angiulli
#t 2007
#c 7
#% 5182
#% 92533
#% 229943
#% 252304
#% 307100
#% 420138
#% 428413
#% 465912
#% 627398
#% 840838
#% 1861143
#! This work has two main objectives, namely, to introduce a novel algorithm, called the Fast Condensed Nearest Neighbor (FCNN) rule, for computing a training set consistent subset for the nearest neighbor decision rule, and to show that condensation algorithms for the nearest neighbor rule can be applied to huge collections of data. The FCNN rule has some interesting properties: it is order independent, its worst case time complexity is quadratic but often with a small constant pre-factor, and it is likely to select points very close to the decision boundary. Furthermore, its structure allows for the triangular inequality to be effectively exploited to reduce the computational effort. The FCNN rule outperformed even here enhanced variants of existing competence preservation methods both in terms of learning speed and learning scaling behavior, and often in terms of the size of the model, while it guaranteed the same prediction accuracy. Furthermore, it was three order of magnitude faster than hybrid instance-based learning algorithms on the MNIST and MIT Face databases and computed a model of accuracy comparable to that of methods incorporating a noise filtering pass.

#index 1013616
#* Semisupervised Regression with Cotraining-Style Algorithms
#@ Zhi-Hua Zhou;Ming Li
#t 2007
#c 7
#% 116165
#% 209021
#% 252011
#% 283180
#% 311027
#% 316509
#% 400985
#% 466095
#% 466263
#% 466888
#% 516767
#% 565545
#% 748550
#% 765552
#% 785551
#% 811376
#% 815908
#% 816079
#% 832574
#% 840938
#% 875962
#% 879447
#% 915312
#% 1269479
#% 1269777
#% 1289496
#% 1785052
#! The traditional setting of supervised learning requires a large amount of labeled training examples in order to achieve good generalization. However, in many practical applications, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning has attracted much attention. Previous research on semi-supervised learning mainly focuses on semi-supervised classification. Although regression is almost as important as classification, semi-supervised regression is largely understudied. In particular, although co-training is a main paradigm in semi-supervised learning, few works has been devoted to co-training style semi-supervised regression algorithms. In this paper, a co-training style semi-supervised regression algorithm, i.e. Coreg, is proposed. This algorithm uses two regressors each labels the unlabeled data for the other regressor, where the confidence in labeling an unlabeled example is estimated through the amount of reduction in mean square error over the labeled neighborhood of that example. Analysis and experiments show that Coreg can effectively exploit unlabeled data to improve regression estimates.

#index 1013617
#* Graph-Based Analysis of Human Transfer Learning Using a Game Testbed
#@ Diane J. Cook;Lawrence B. Holder;G. Michael Youngblood
#t 2007
#c 7
#% 65654
#% 369349
#% 431105
#% 445369
#% 629708
#% 643990
#% 731612
#% 769951
#% 823852
#% 841960
#% 1274925
#! The ability to transfer knowledge learned in one environment in order to improve performance in a different environment is one of the hallmarks of human intelligence. Insights into human transfer learning help us to design computer-based agents that can better adapt to new environments without the need for substantial reprogramming. In this paper we study the transfer of knowledge by humans playing various scenarios in a graphically realistic urban setting which are specifically designed to test various levels of transfer. We determine the amount and type of transfer that is being performed based on the performance of human trained and untrained players. In addition, we use a graph-based relational learning algorithm to extract patterns from player graphs. These analyses reveal that indeed humans are transferring knowledge from one set of games to another and the amount and type of transfer varies according to player experience and scenario complexity. The results of this analysis help us understand the nature of human transfer in such environments and shed light on how we might endow computer-based agents with similar capabilities. The game simulator and human data collection also represent a significant testbed in which other AI capabilities can be tested and compared to human performance.

#index 1013618
#* Evaluating Universal Quantification in XML
#@ Shurug Al-Khalifa;Bin Liu;H. V. Jagadish
#t 2007
#c 7
#% 136740
#% 189638
#% 321468
#% 458823
#% 462783
#% 463871
#% 463894
#% 482103
#% 499336
#% 570875
#% 659999
#% 993953
#! Queries posed to database systems often involve Universal Quantification. Such queries are typically expensive to evaluate. While they can be handled by basic access methods, for selection, grouping, etc., new access methods specifically tailored to evaluate universal quantification can greatly decrease the computational cost. In this paper, we study the efficient evaluation of universal quantification in an XML database. Specifically, we develop a small taxonomy of universal quantification types, and define a family of algorithms suitable for handling each. We experimentally demonstrate the performance benefits of the new family of algorithms.

#index 1013619
#* Efficient and Scalable Algorithms for Inferring Likely Invariants in Distributed Systems
#@ Guofei Jiang;Haifeng Chen;Kenji Yoshihira
#t 2007
#c 7
#% 36698
#% 70370
#% 120702
#% 190611
#% 209891
#% 263985
#% 314924
#% 331886
#% 342655
#% 387427
#% 411099
#% 438371
#% 584891
#% 630984
#% 674157
#% 782456
#% 813979
#% 835188
#% 869395
#% 910886
#% 963866
#% 1142429
#! Distributed systems generate large amount of monitoring data such as log files to track their operational status. However, it is hard to correlate such monitoring data effectively across distributed systems and along observation time for system management. In previous work, we proposed a concept named flow intensity to measure the intensity with which internal monitoring data reacts to the volume of user requests. We calculated flow intensity measurements from monitoring data and proposed an algorithm to automatically search constant relationships between flow intensities measured at various points across distributed systems. If such relationships hold all the time, we regard them as invariants of the underlying systems. Invariants can be used to characterize complex systems and support various system management tasks. However, the computational complexity of previous invariant search algorithm is high so that it may not scale well in large systems with thousands of measurements. In this paper, we propose two efficient but approximate algorithms for inferring invariants in large-scale systems. The computational complexity of new randomized algorithms is significantly reduced and experimental results from a real system are also included to demonstrate the accuracy and efficiency of our new algorithms.

#index 1013620
#* Wildcard Search in Structured Peer-to-Peer Networks
#@ Yuh-Jzer Joung;Li-Wei Yang
#t 2007
#c 7
#% 67210
#% 67565
#% 340175
#% 340176
#% 349973
#% 496147
#% 496291
#% 612643
#% 636008
#% 636009
#% 653826
#% 745354
#% 765673
#% 768511
#% 768539
#% 770901
#% 793888
#% 800519
#% 812773
#% 839334
#% 840583
#% 855458
#% 888195
#% 963596
#% 1180870
#% 1180871
#% 1386265
#% 1722231
#% 1725999
#% 1850764
#! We address wildcard search in structured peer-to-peer (P2P) networks, which, to our knowledge, has not yet been explored in the literature. We begin by presenting an approach based on some well-known techniques in information retrieval (IR), and discuss why it is not appropriate in a distributed environment. We then present a simple and novel technique to index objects for wildcard search in a fully decentralized manner, along with some search strategies to retrieve objects. Our index scheme, as opposed to a traditional IR approach, can achieve quite balanced loads, avoid hop-spots and single point of failure, reduce storage and maintenance costs, and offer some ranking mechanisms for matching objects. We use the CD records collected in FreeDB (http://freedb.org) as experimental dataset to evaluate our scheme. The results confirm that our index scheme is very effective in balancing the load. Moreover, search efficiency depends on the information given in a query: the more the information, the higher the performance.

#index 1013621
#* A Logical Formulation of Probabilistic Spatial Databases
#@ Austin Parker;V. S. Subrahmanian;John Grant
#t 2007
#c 7
#% 1436
#% 33376
#% 73571
#% 102767
#% 144840
#% 166497
#% 209725
#% 235023
#% 259487
#% 323126
#% 340699
#% 417773
#% 442830
#% 443429
#% 462486
#% 480102
#% 480934
#% 527793
#% 561913
#% 808858
#% 824728
#! There are numerous applications where there isuncertainty over space and time. Examples of such uncertaintyarise in vehicle tracking systems where we are not always surewhere a vehicle is now (or may be in the future), cell and satellitephone applications where we are not sure exactly where a phonemay be, and so on. In this paper, we propose the concept of aSpatial PrObabilistic Temporal (SPOT ) database which containsstatements of the form "Object O is in spatial region R at sometime t with some probability in the interval [L, U]." We definethe syntax and a declarative semantics for SPOT databasesbased on a mix of logic and linear programming, as well asquery algebra. We show alternative implementations of some ofthese query algebra operators when the SPOT database has adisjointness property. Though the declarative semantics of SPOTdatabases is rooted in linear programming, we have found veryefficient algorithms that do not use linear programming methods.We report on experiments we have conducted that show that thesystem scales to large numbers of SPOT atoms as well as tofairly fine temporal and spatial granularity.

#index 1013622
#* Approximate Query Processing in Cube Streams
#@ Ming-Jyh Hsieh;Ming-Syan Chen;Philip S. Yu
#t 2007
#c 7
#% 115996
#% 168862
#% 211575
#% 227866
#% 259995
#% 273902
#% 273903
#% 379445
#% 397389
#% 397426
#% 458843
#% 460862
#% 480465
#% 480628
#% 631923
#% 889089
#% 907519
#% 1818266
#! Data cubes have become important components in most data warehouse systems and Decision-Support-Systems. In such systems, users usually pose very complex queries to the Online Analytical Processing (OLAP) system, and systems usually have to deal with a huge amounts of data because of the large dimensionality of the sets; thus approximating query processing has emerged as a viable solution. Specifically, the applications of cube streams handle multidimensional data sets in a continuous manner in contrast to traditional cube approximation. Such an application collects data events for cube streams on-line and generates snapshots with limited resources and keeps the approximated information in a synopsis memory for further analysis. Compared to OLAP applications, applications of cube streams are subject to many more resource constraints on both the processing time and the memory and cannot be dealt with by existing methods due to the limited resources. In this paper, we propose the DAWA algorithm, which is a hybrid algorithm of Dct for Data and the discrete WAvelet transform, to approximate cube streams. Our algorithm combines the advantages of the high compression rate of DWT and the low memory cost of DCT. Consequently, DAWA requires much smaller working buffer and outperforms both DWT-based and DCT-based methods in execution efficiency. Also, it is shown that DAWA provides a good solution for approximate query processing of cube streams with a small working buffer and a short execution time. The optimality of the DAWA algorithm is theoretically proved and empirically demonstrated by our experiments.

#index 1013623
#* Optimal Scheduling and Placement of Internet Banner Advertisements
#@ Subodha Kumar;Milind Dawande;Vijay Mookerjee
#t 2007
#c 7
#% 246087
#% 408396
#% 416622
#% 445224
#% 739574
#% 766020
#% 770014
#! The increasing popularity of the world wide web has made it an attractive medium for advertisers. As more advertisers place internet advertisements (hereafter also called "ads), it has become important for web site owners to maximize revenue through the optimal selection and placement of these ads. Unlike most previous research, we consider a hybrid pricing model where the price advertisers pay is a function of (i) the number of exposures of the ad and (ii) the number of clicks on the ad. The problem is to find an ad schedule to maximize web site revenue under a hybrid pricing model. We formulate two versions of the problem: static and dynamic, and propose a variety of efficient solution techniques that provide near-optimal solutions. In the dynamic version, the schedule of ads is changed based on individual user click behavior. We show - using a theoretical proof under special circumstances and an experimental demonstration under general conditions - that a schedule that adapts to user click behavior consistently outperforms one that does not. We also demonstrate that to benefit from observing user click behavior, the associated probability parameter need not be estimated accurately.

#index 1013624
#* Semisupervised Query Expansion with Minimal Feedback
#@ Masayuki Okabe;Seiji Yamada
#t 2007
#c 7
#% 92696
#% 109223
#% 169806
#% 232645
#% 309088
#% 342707
#% 348154
#% 466263
#% 577301
#% 643001
#% 731620
#% 770851
#% 818207
#% 840426
#% 879584
#% 907516
#% 939974
#% 1289346
#! Query expansion is an information retrieval technique in which new query terms are selected to improve search performance. Although useful terms can be extracted from documents whose relevance is already known, it is difficult to get enough of such feedback from a user in actual use. We propose a query expansion method that performs well even if a user makes practically minimum effort, that is, chooses only a single relevant document. To improve searches in these conditions, we made two refinements to a well-known query expansion method. One uses transductive learning to obtain pseudo relevant documents, thereby increasing the total number of source documents from which expansion terms can be extracted. The other is a modified parameter estimation method that aggregates the predictions of multiple learning trials to sort candidate terms for expansion by importance. Experimental results show that our method outperforms traditional methods, and is comparable to a state of the art method.

#index 1013625
#* Failure Detection in Large-Scale Internet Services by Principal Subspace Mapping
#@ Haifeng Chen;Guofei Jiang;Kenji Yoshihira
#t 2007
#c 7
#% 120702
#% 149246
#% 212673
#% 224113
#% 275453
#% 278781
#% 295146
#% 302406
#% 310552
#% 352219
#% 546061
#% 731721
#% 731722
#% 769920
#% 784551
#% 820391
#% 820393
#% 823414
#% 869395
#% 874186
#% 963474
#% 963866
#% 1142429
#! Fast and accurate failure detection is becoming essential in managing large scale Internet services. This paper proposes a novel detection approach based on the subspace mapping between system inputs and internal measurements. By exploring these contextual dependencies, our detector can initiate repair actions accurately, increasing the availability of system. While a classical statistical method, the canonical correlation analysis (CCA), is presented in the paper to achieve subspace mapping, we also propose a more advanced technique, the principal canonical correlation analysis (PCCA), to improve the performance of CCA based detector. PCCA extracts a principal subspace from internal measurements that is not only highly correlated with the inputs, but also a significant representative of original measurements. Experimental results on a J2EE based web application demonstrate that such property of PCCA is especially beneficial to failure detection tasks.

#index 1013626
#* Community Mining from Signed Social Networks
#@ Bo Yang;William Cheung;Jiming Liu
#t 2007
#c 7
#% 74120
#% 214673
#% 281214
#% 281251
#% 290830
#% 313959
#% 438553
#% 460812
#% 577367
#% 736155
#% 754098
#% 765548
#% 1675858
#! Many complex systems in the real world can be modeled as signed social networks that contain both positive and negative relations. Algorithms for mining social networks have been developed in the past, however most of them were designed primarily for networks containing only positive relations and thus not suitable for signed networks. In this work, we propose a new algorithm, called FEC, to mine signed social networks so that both positive within-group relations and negative between-group relations are dense. FEC considers both the sign and the density of relations as the clustering attributes, making itself effective for not only signed networks but also conventional social networks including only positive relations. Also, FEC adopts an agent-based heuristic that makes the algorithm efficient (in linear time with respect to the size of a network) and capable of giving nearly optimal solutions. FEC depends on only one parameter whose value can easily be set, and requires no prior knowledge on hidden community structures. The effectiveness and efficacy of FEC have been demonstrated through a set of rigorous experiments involving both benchmark and randomly-generated signed networks.

#index 1013627
#* EIC Editorial: 2007 TKDE Editorial Board Changes
#@ Xindong Wu
#t 2007
#c 7

#index 1013628
#* An Evaluation of the Robustness of MTS for Imbalanced Data
#@ Chao-Ton Su;Yu-Hsiang Hsiao
#t 2007
#c 7
#% 345823
#% 390324
#% 641965
#% 765519
#% 765521
#% 765522
#% 765524
#% 772849
#% 813970
#% 881980
#% 998622
#% 1271973
#% 1502470
#! In classification problems, class imbalance problem will cause bias on the training of classifiers, and will result in the lower sensitivity of detecting the minority class examples. Mahalabobis-Taguchi System (MTS) is a diagnosis and forecasting technique for multivariate data. MTS establishes a classifier by constructing a continuous measurement scale rather than directly learning from the training set. Therefore, it is expected that the construction of an MTS model will not be influenced by data distribution, and this property is helpful to overcome the class imbalance problem. To verify the robustness of MTS for imbalanced data, this study compares MTS with several popular classification techniques. The results indicate that MTS is the most robust technique to deal with the classification problem on imbalanced data. In addition, this study develops a "probabilistic thresholding method" to determine the classification threshold for MTS, and it obtains a good performance. Finally, MTS is employed to analyze the RF inspection process of mobile phone manufacture. The data collected from the RF inspection process is typically an imbalanced type. Implementation results show that the inspection attributes are significantly reduced and that the RF inspection process can also maintain high inspection accuracy.

#index 1013629
#* Clustering over Multiple Evolving Streams by Events and Correlations
#@ Mi-Yen Yeh;Bi-Ru Dai;Ming-Syan Chen
#t 2007
#c 7
#% 32926
#% 280408
#% 310500
#% 342600
#% 378388
#% 443392
#% 466506
#% 594012
#% 659972
#% 701402
#% 705649
#% 744027
#% 765403
#% 800496
#% 881938
#% 889089
#% 993961
#% 1015261
#% 1869561
#! In applications of multiple data streams such as stock market trading and sensor network data analysis, the clusters of streams change at different time because of the data evolution. The information of evolving cluster is valuable to support corresponding online decisions. In this paper, we present a framework for Clustering Over Multiple Evolving sTreams by CORrelations and Events, which, abbreviated as COMETCORE, monitors the distribution of clusters over multiple data streams based on their correlation. Instead of directly clustering the multiple data streams periodically, COMET-CORE applies efficient cluster split and merge processes only when significant cluster evolution happens. Accordingly, we devise an event detection mechanism to signal the cluster adjustments. The coming streams are smoothed as sequences of end points by employing piecewise linear approximation. At the time when end points are generated, weighted correlations between streams are updated. End points are good indicators of significant change in streams, and this is a main cause of cluster evolution event. When an event occurs, through split and merge operations we can report the latest clustering results. As shown in our experimental studies, COMET-CORE can be performed effectively with good clustering quality.

#index 1013630
#* Efficiently Querying Large XML Data Repositories: A Survey
#@ Gang Gou;Rada Chirkova
#t 2007
#c 7
#% 58365
#% 236416
#% 291299
#% 333981
#% 340144
#% 340914
#% 345742
#% 396733
#% 397358
#% 397359
#% 397360
#% 397366
#% 397375
#% 406493
#% 411554
#% 428146
#% 458836
#% 479465
#% 479806
#% 479956
#% 480296
#% 480489
#% 480656
#% 504574
#% 528124
#% 562456
#% 570875
#% 570876
#% 576108
#% 598374
#% 654442
#% 654450
#% 654451
#% 654452
#% 654453
#% 654476
#% 654477
#% 654493
#% 659995
#% 659999
#% 660000
#% 722530
#% 731408
#% 742563
#% 745461
#% 745468
#% 745477
#% 765405
#% 765406
#% 765407
#% 765408
#% 765466
#% 765488
#% 766417
#% 770338
#% 772025
#% 783546
#% 783547
#% 783787
#% 791182
#% 800523
#% 800534
#% 800535
#% 803121
#% 810036
#% 810046
#% 810052
#% 810118
#% 814648
#% 814651
#% 824663
#% 824667
#% 824668
#% 824673
#% 824674
#% 838518
#% 838542
#% 864462
#% 864465
#% 884628
#% 993939
#% 993950
#% 993953
#% 1015268
#% 1015273
#% 1015274
#% 1015276
#% 1015277
#% 1015298
#% 1015338
#% 1016135
#% 1016148
#% 1016150
#% 1016224
#% 1721248
#! Extensible Markup Language (XML) is emerging as a de facto standard for information exchange among various applications on the World-Wide Web. There has been a growing need for developing high-performance techniques to query large XML data repositories efficiently. One important problem in XML query processing is twig pattern matching , that is, finding in an XML data tree D all matches that satisfy a specified twig (or path) query pattern Q. In this survey we review, classify, and compare major techniques for twig pattern matching.Specifically, we consider two classes of major XML queryprocessing techniques: the relational approach and the native approach. The relational approach directly utilizes existing relational database systems to store and query XML data, which enables the use of all important techniques that have been developed for relational databases, while in the native approach, specialized storage and query-processing systems tailored for XML data are developed from scratch to further improve XML query performance. As implied by existing work, XML data querying and management are developing in the direction of integrating the relational approach with the native approach, which could result in higher query-processing performance and also significantly reduce system-reengineering costs.

#index 1013631
#* Using Ranked Nodes to Model Qualitative Judgments in Bayesian Networks
#@ Norman E. Fenton;Martin Neil;Jose Galan Caballero
#t 2007
#c 7
#% 89748
#% 183490
#% 307915
#% 441933
#% 443356
#% 443357
#% 443358
#% 743403
#% 743441
#% 763237
#% 939163
#% 1650335
#% 1650644
#% 1650731
#% 1650734
#% 1650791
#% 1650799
#% 1786724
#! Although Bayesian Nets (BNs) are increasingly being used to solve real world risk problems, their use is still constrained by the difficulty of constructing the node probability tables (NPTs). A key challenge is to construct relevant NPTs using the minimal amount of expert elicitation, recognising that it is rarely cost-effective to elicit complete sets of probability values. We describe a simple approach to defining NPTs for a large class of commonly occurring nodes (called ranked nodes). The approach is based on the doubly truncated Normal distribution with a central tendency that is invariably a type of weighted function of the parent nodes. In extensive real-world case studies we have found that this approach is sufficient for generating the NPTs of a very large class of nodes. We describe one such case study for validation purposes. The approach has been fully automated in a commercial tool, called AgenaRisk, and is thus accessible to all types of domain experts. We believe this work represents a useful contribution to BN research and technology since its application makes the difference between being able to build realistic BN models and not.

#index 1013632
#* GrubJoin: An Adaptive, Multi-Way, Windowed Stream Join with Time Correlation-Aware CPU Load Shedding
#@ Bugra Gedik;Kun-Lung Wu;Philip S. Yu;Ling Liu
#t 2007
#c 7
#% 554
#% 273908
#% 479654
#% 654444
#% 765436
#% 788216
#% 810037
#% 812805
#% 838409
#% 853011
#% 864468
#% 875006
#% 1015278
#% 1015280
#% 1016156
#% 1016158
#% 1712538
#! Tuple dropping, though commonly used for loadshedding in most data stream operations, is generally inadequatefor multi-way, windowed stream joins. The join output rate canbe unnecessarily reduced because tuple dropping fails to exploitthe time correlations likely to exist among interrelated streams.In this paper, we introduce GrubJoin - an adaptive, multi-way,windowed stream join that effectively performs time correlationawareCPU load shedding. GrubJoin maximizes the output rateby achieving near-optimal window harvesting, which picks onlythe most profitable segments of individual windows for the join.Due mainly to the combinatorial explosion of possible multi-wayjoin sequences involving different window segments, GrubJoinfaces unique challenges that do not exist for binary joins, suchas determining the optimal window harvesting configurationin a time efficient manner and learning the time correlationsamong the streams without introducing overhead. To tacklethese challenges, we formalize window harvesting as an optimizationproblem, develop greedy heuristics to determine nearoptimalwindow harvesting configurations and use approximationtechniques to capture the time correlations. Our experimentalresults show that GrubJoin is vastly superior to tuple droppingwhen time correlations exist and is equally effective when timecorrelations are nonexistent.

#index 1013633
#* MULS: A General Framework of Providing Multilevel Service Quality in Sequential Data Broadcasting
#@ Hao-Ping Hung;Ming-Syan Chen
#t 2007
#c 7
#% 201897
#% 259634
#% 279486
#% 290747
#% 310775
#% 342694
#% 430425
#% 442624
#% 443127
#% 452850
#% 480500
#% 481777
#% 511499
#% 575109
#% 628205
#% 632067
#% 720829
#% 772837
#% 812809
#% 814346
#% 814348
#% 874283
#! In recent years, data broadcasting becomes a promising technique to design a mobile information system with power conservation, high scalability and high bandwidth utilization. In many applications, the query issued by a mobile client corresponds to multiple items which should be accessed in a sequential order. In this paper, we study the scheduling approach in such a sequential data broadcasting environment. Explicitly, we propose a general framework referred to as MULS (standing for MUlti-Level Service) for an information system. There are two primary stages in MULS: on-line scheduling and optimization procedure. In the first stage, we propose an On- Line Scheduling algorithm (denoted by OLS) to allocate the data items into multiple channels. As for the second stage, we devise an optimization procedure SCI, standing for Sampling with Controlled Iteration, to enhance the quality of broadcast programs generated by algorithm OLS. Procedure SCI is able to strike a compromise between effectiveness and efficiency by tuning the control parameters. According to the experimental results, we show that algorithm OLS with procedure SCI outperforms the approaches in prior works prominently in both effectiveness (i.e., the average access time of mobile users) and efficiency (i.e., the complexity of the scheduling algorithm). Therefore, by cooperating algorithm OLS with procedure SCI, the proposed MULS framework is able to generate broadcast programs with flexibility of providing different service qualities under different requirements of effectiveness and efficiency: in the dynamic environment in which the access patterns and information contents change rapidly, the parameters used in SCI will perform online scheduling with satisfactory service quality. As for the static environment in which the query profile and the database are updated infrequently, larger values of parameters are helpful to generate an optimized broadcast program, indicating the advantageous feature of MULS.

#index 1013634
#* Efficient Process of Top-k Range-Sum Queries over Multiple Streams with Minimized Global Error
#@ Hao-Ping Hung;Kun-Ta Chuang;Ming-Syan Chen
#t 2007
#c 7
#% 248822
#% 257637
#% 273902
#% 333854
#% 333948
#% 378388
#% 399763
#% 480330
#% 577361
#% 578390
#% 632089
#% 654443
#% 654487
#% 729966
#% 742562
#% 766671
#% 800496
#% 800509
#% 801684
#% 823333
#% 824653
#% 824654
#% 824686
#% 824704
#% 824709
#% 838410
#% 866990
#% 1016153
#% 1016154
#% 1016196
#! Due to the resource limitation in the data stream environments, it has been reported that answering user queries according to the wavelet synopsis of a stream is an essential ability of a Data Stream Management System (DSMS). In the literature, recent research has been elaborated upon minimizing the local error metric of an individual stream. However, many emergent applications, such as stock marketing and sensor detection, also call for the need of recording multiple streams in a commercial DSMS. As shown in our thorough analysis and experimental studies, minimizing global error in multiple-stream environments leads to good reliability for DSMS to answer the queries; in contrast, only minimizing local error may lead to significant loss of query accuracy. As such, we first study in this paper the problem of maintaining the wavelet coefficients of multiple streams within collective memory so that the predetermined global error metric is minimized. Moreover, we also examine a promising application in the multistream environment, i.e., the queries for top-k range sum. We resolve the problem of efficient top-k query processing with minimized global error by developing a general framework. For the purposes of maintaining the wavelet coefficients and processing top-k queries, several well-designed algorithms are utilized to optimize the performance of each primary component of this general framework. We also evaluate the proposed algorithms empirically on real and simulated data streams and show that our framework can process top-k queries accurately and efficiently.

#index 1013635
#* Correction to "A Family of Directional Relation Models for Extended Objects"
#@ Spiros Skiadopoulos;Nikos Sarkas;Timos Sellis;Manolis Koubarakis
#t 2007
#c 7
#% 982761
#! In the above titled paper (ibid., vol. 19, no. 8, pp. 1116-1130, Aug 07), some information appeared incorrectly. The corrections appear here.

#index 1034710
#* Monitoring High-Dimensional Data for Failure Detection and Localization in Large-Scale Computing Systems
#@ Haifeng Chen;Guofei Jiang;Kenji Yoshihira
#t 2008
#c 7
#% 224113
#% 231932
#% 278781
#% 302406
#% 310552
#% 333929
#% 479973
#% 546061
#% 731721
#% 731722
#% 769920
#% 820391
#% 820393
#% 963474
#% 1142429
#! It is a major challenge to process the high dimensional measurements for failure detection and localization in large scale computing systems. However, it is observed that in information systems those measurements are usually located in a low dimensional structure that is embedded in the high dimensional space. From this perspective, a novel approach is proposed in this paper to model the geometry of underlying data generation and detect anomalies based on that model. We consider both linear and nonlinear data generation models. Two statistics, the Hotelling $T^2$ and the squared prediction error ($SPE$), are used to reflect data variations within and outside the model. We track the probabilistic density of extracted statistics to monitor the system's health. After a failure has been detected, a localization process is also proposed to find the most suspicious attributes related to the failure. Experimental results on both synthetic data and a real e-commerce application demonstrate the effectiveness of our approach in detecting and localizing failures in computing systems.

#index 1034711
#* Neural-Based Learning Classifier Systems
#@ Hai H. Dam;Hussein A. Abbass;Chris Lokan;Xin Yao
#t 2008
#c 7
#% 61477
#% 96699
#% 114994
#% 356892
#% 369236
#% 379334
#% 465704
#% 477725
#% 490832
#% 551901
#% 566829
#% 689505
#% 710643
#% 723672
#% 811782
#% 827526
#% 840766
#% 1022821
#% 1670538
#% 1777140
#% 1861209
#! UCS, a sUpervised Classifier System, is an accuracy-based evolutionary learning classifier system for data mining classification tasks. UCS works through two stages: exploration and exploitation. During the exploration phase, a population of rules is evolved in order to represent a complete solution of the target problem. The exploitation phase is responsible for applying the rule-based knowledge obtained in the first phase when the system is exposed to unseen data. The representation of a rule in UCS as a univariate classification rule can be easily seen in a symbolic form, which is easy for a human to understand and comprehend (i.e. expressive power). However, the system may generate a large number of rules to cover the input space. Artificial neural networks normally provide a more compact and accurate representation. However, it is not a straightforward task to understand the network. In this paper, we propose a novel way to incorporate neural networks into UCS. The approach offers a good compromise between compactness, expressiveness, and accuracy. By using a simple artificial neural network as the classifier's action, we obtain smaller/compact population size, better generalization, while maintaining a reasonable level of expressiveness. We also apply negative correlation learning (NCL) during the training of the resultant neural network ensemble. NCL is shown to improve the generalization of the ensemble.

#index 1034712
#* Efficient Similarity Search over Future Stream Time Series
#@ Xiang Lian;Lei Chen
#t 2008
#c 7
#% 58646
#% 78792
#% 86950
#% 132676
#% 172949
#% 232122
#% 248797
#% 248798
#% 285711
#% 333941
#% 378388
#% 397380
#% 413606
#% 452844
#% 460862
#% 462231
#% 466507
#% 477479
#% 480146
#% 480631
#% 564263
#% 577275
#% 617886
#% 629640
#% 654456
#% 654497
#% 659936
#% 659971
#% 662750
#% 729943
#% 765136
#% 765403
#% 765451
#% 800496
#% 810049
#% 874982
#% 976788
#% 993961
#% 993965
#% 1016191
#% 1016195
#% 1809954
#% 1860826
#! With the advance of hardware and communication technologies, stream time series is gaining ever-increasing attention due to its importance in many applications, such as financial data processing, network monitoring, web click-stream analysis, sensor data mining and anomaly detection. For all these applications, an efficient and effective similarity search over stream data is essential. Even though many approaches have been proposed for searching through archived data, because of the unique characteristics of the stream, for example, data are frequently updated and real-time response is required, traditional methods may not work in these stream scenarios. Especially, for the cases where the arrival of data is often delayed for various reasons, for example, the communication congestion or batch processing and so on, queries on such incomplete time series or even future time series may result in inaccuracy using the traditional approaches. Therefore, in this paper we propose three approaches, polynomial, DFT and probabilistic, to predict the unknown values that have not arrived at the system and answer the queries based on the predicated data. We also present efficient indexes, that is, a multidimensional hash index and B+-tree, to facilitate the prediction and similarity search on future time series, respectively. Extensive experiments demonstrate the efficiency and effectiveness of our methods in terms of I/O, prediction and query accuracy

#index 1034713
#* SRDA: An Efficient Algorithm for Large-Scale Discriminant Analysis
#@ Deng Cai;Xiaofei He;Jiawei Han
#t 2008
#c 7
#% 80995
#% 212689
#% 224113
#% 235342
#% 252304
#% 317525
#% 317535
#% 341596
#% 729437
#% 769912
#% 791402
#% 829010
#% 881501
#% 983940
#% 997140
#% 1019136
#% 1117001
#% 1117038
#% 1828410
#! Linear Discriminant Analysis (LDA) has been a popular method for extracting features which preserve class separability. It has been widely used in many fields of information processing. However, the computation of LDA involves dense matrices eigen-decomposition which can be computationally expensive both in time and memory. Specifically, LDA has $O(mnt+t^3)$ time complexity and requires $O(mn+mt+nt)$ memory, where $m$ is the number of samples, $n$ is the number of features and $t=\min(m,n)$. When both $m$ and $n$ are large, it is infeasible to apply LDA. In this paper, we propose a novel algorithm for discriminant analysis, called {\em Spectral Regression Discriminant Analysis} (SRDA). By using spectral graph analysis, SRDA casts discriminant analysis into a regression framework which facilitates both efficient computation and the use of regularization techniques. Specifically, SRDA only needs to solve a set of regularized least squares problems and there is no eigenvector computation involved, which is a huge save of both time and memory. Our theoretical analysis shows that SRDA can be computed with $O(ms)$ time and $O(ms)$ memory, where $s (\leq n)$ is the average number of non-zero features in each sample. Extensive experimental results on four real world data sets demonstrate the effectiveness and efficiency of our algorithm.

#index 1034714
#* Label Propagation through Linear Neighborhoods
#@ Fei Wang;Changshui Zhang
#t 2008
#c 7
#% 36672
#% 190581
#% 236651
#% 252011
#% 311027
#% 449588
#% 466263
#% 565545
#% 593047
#% 876068
#% 902655
#% 961218
#! In many practical data mining applications such as text classification, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms have aroused considerable interests from the data mining and machine learning fields. In recent years, graph based semi-supervised learning has been becoming one of the most active research area in semi-supervised learning community. In this paper, a novel graph based semi-supervised learning approach is proposed based on a linear neighborhood model, which assumes that each data point can be linearly reconstructed from its neighborhood. Our algorithm, named Linear Neighborhood Propagation (LNP), can propagate the labels from the labeled points to the whole dataset using these linear neighborhoods with sufficient smoothness. Theoretical analysis of the properties of LNP are presented in this paper. Furthermore, we also derive an easy way to extend LNP to out-of-sample data. Promising experimental results are presented for synthetic data, digit and text classification tasks.

#index 1034715
#* Discovering Frequent Agreement Subtrees from Phylogenetic Data
#@ Sen Zhang;Jason T. L. Wang
#t 2008
#c 7
#% 193902
#% 243682
#% 256623
#% 316034
#% 466644
#% 481290
#% 727845
#% 727909
#% 729418
#% 729938
#% 731608
#% 745511
#% 778214
#% 789011
#% 813989
#% 814196
#% 883408
#% 885359
#% 944956
#% 1015260
#! We study a new data mining problem concerning the discovery of frequent agreement subtrees (FASTs) from a set of phylogenetic trees. A phylogenetic tree, or phylogeny, is an unordered tree in which the order among siblings is unimportant. Furthermore, each leaf in the tree has a label representing a taxon (species or organism) name whereas internal nodes are unlabeled. The tree may have a root, representing the common ancestor of all species in the tree, or may be unrooted. An unrooted phylogeny arises due to the lack of sufficient evidence to infer a common ancestor of the taxa in the tree. The FAST problem addressed here is a natural extension of the MAST (maximum agreement subtree) problem widely studied in the computational phylogenetics community. The paper establishes a framework for tackling the FAST problem for both rooted and unrooted phylogenetic trees using data mining techniques. We first develop a novel canonical form for rooted trees together with a phylogeny-aware tree expansion scheme for generating candidate subtrees level by level. Then we present an efficient heuristic to find all frequent agreement subtrees in a given set of rooted trees, through an Apriori-like approach. We show the correctness and completeness of the proposed method. Finally we discuss extensions of the techniques to unrooted trees. Experimental results demonstrate that the proposed methods work well, capable of finding interesting patterns in both synthetic data and real phylogenetic trees.

#index 1034716
#* Maximal Subspace Coregulated Gene Clustering
#@ Yuhai Zhao;Jeffrey Xu Yu;Guoren Wang;Lei Chen;Bin Wang;Ge Yu
#t 2008
#c 7
#% 397382
#% 464996
#% 469422
#% 727882
#% 727908
#% 762824
#% 765135
#% 765518
#% 769919
#% 773685
#% 804395
#% 810066
#% 833094
#% 864476
#% 897247
#! Clustering is a popular technique for analyzing microarray datasets, with n genes and m experimental conditions. As explored by biologists, there is a real need to identify co-regulated gene clusters, which includes both positive/negative regulated gene clusters. The existing pattern-based and tendency-based clustering approaches cannot be directly applied to find such co-regulated gene clusters, because they are designed for finding positive regulated gene clusters. In this paper, in order to cluster co-regulated genes, we propose a coding scheme which allows us to cluster two genes into the same cluster if they have the same code, where two genes that have the same code can be either positive or negative regulated. Based on the coding scheme, we propose a new algorithm to find maximal subspace co-regulated gene clusters with new pruning techniques. A maximal subspace co-regulated gene cluster clusters a set of genes on a condition sequence such that the cluster is not included in any other subspace co-regulated gene clusters. We conduct extensive experimental studies. Our approach can effectively and efficiently find maximal subspace co-regulated gene clusters. In addition, our approach outperforms the existing approaches to finding positive regulated gene clusters.

#index 1034717
#* An Exploratory Study of Database Integration Processes
#@ Joerg Evermann
#t 2008
#c 7
#% 22948
#% 34621
#% 55294
#% 248882
#% 251252
#% 307632
#% 314740
#% 315025
#% 332166
#% 379119
#% 442787
#% 442861
#% 442886
#% 443408
#% 452859
#% 462644
#% 488766
#% 551850
#% 572314
#% 641044
#% 654458
#% 660001
#% 723412
#% 726627
#% 728755
#% 734653
#% 735938
#% 737519
#% 740771
#% 829178
#% 859697
#% 933329
#% 1134881
#! One of the central problems of database integration is schema matching, the identification of similar data elements in two or more databases or other data sources. Existing definitions of "similarity" in this context vary greatly. As a result, schema matching has given rise to large number of heuristics software tools. However, the empirical understanding of this process in humans is very limited, so little guidance can be offered to the further development of heuristics and tool. This paper presents an exploratory study of the similarity judgement process in humans, employing a process tracing methodology. The similarity judgements of twelve data integration professionals on a range of integration problems are recorded and analyzed. Implications for future empirical and applied research in this area are discussed.

#index 1034718
#* Watermarking Relational Databases Using Optimization-Based Techniques
#@ Mohamed Shehab;Elisa Bertino;Arif Ghafoor
#t 2008
#c 7
#% 114994
#% 263455
#% 288197
#% 356519
#% 369236
#% 416929
#% 539761
#% 576109
#% 728264
#% 781917
#% 784512
#% 803781
#% 928872
#% 993944
#! Proving ownership rights on outsourced relational databases is a crucial issue in today internet-based application environment and in many content distribution applications. In this paper, we present a mechanism for proof of ownership based on the secure embedding of a robust imperceptible watermark in relational data. We formulate the watermarking of relational databases as a constrained optimization problem, and discuss efficient techniques to solve the optimization problem and to handle the constraints. Our watermarking technique is resilient to watermark synchronization errors because it uses a partitioning approach that does not require marker tuples. Our approach overcomes a major weakness in previously proposed watermarking techniques. Watermark decoding is based on a threshold-based technique characterized by an optimal threshold that minimizes the probability of decoding errors. We implemented a proof of concept implementation of our watermarking technique and showed by experimental results that our technique is resilient to tuple deletion, alteration and insertion attacks.

#index 1034719
#* Visualization of Ontologies to Specify Semantic Descriptions of Services
#@ Jose Samper;Vicente R. Tomas;Eduardo Carrillo;Rogerio P.  C. do Nascimento
#t 2008
#c 7
#% 445446
#% 519428
#% 529352
#% 646294
#% 655355
#% 824112
#% 965519
#! The present paper describes the main characteristics and components of the tool developed for integrating the definition of profiles for semantic web services. This tool is based on the languages DAML-S and OWL-S and it includes the ontology visualization and consistency verification that specify the concepts a web service interacts with. Starting from a service description interpreted by a computer and by the means used for accessing the service, it is possible to find out which software agents use the service. The tool can generate information in different formats, such as the SVG format proposed by the W3C, for representing graphic information based on XML. The tool was developed in Java language and is being used for the visualization of ontologies and for the semantic description of services in traffic information systems that the LISITT group (Laboratorio Integrado de Sistemas Inteligentes y Tecnologías de la información de Tráfico) developed at the Robotics Institute at the University of Valencia.

#index 1034720
#* TKDE 2007 Reviewers List
#@ 
#t 2008
#c 7

#index 1034721
#* Eliciting Consumer Preferences Using Robust Adaptive Choice Questionnaires
#@ Jacob Abernethy;Theodoros Evgeniou;Olivier Toubia;Jean-Philippe Vert
#t 2008
#c 7
#! We propose a framework for designing adaptive choice-based conjoint questionnaires that are robust to response error. It is developed based on a combination of experimental design and statistical learning theory principles. We implement and test a specific case of this framework using Regularization Networks. We also formalize within this framework the polyhedral methods recently proposed in marketing. We use simulations as well as an online market research experiment with 500 participants to compare the proposed method to benchmark methods. Both experiments show that the proposed adaptive questionnaires outperform existing ones in most cases. This work also indicates the potential of using machine learning methods in marketing.

#index 1034722
#* A Lazy Approach to Associative Classification
#@ Elena Baralis;Silvia Chiusano;Paolo Garza
#t 2008
#c 7
#! Associative classification is a promising technique to build accurate classifiers. However, in large or correlated datasets, association rule mining may yield huge rule sets. Hence, several pruning techniques have been proposed to select a small subset of high quality rules. We argue that rule pruning should be reduced to a minimum, since the availability of a "rich" rule set may improve the accuracy of the classifier. The L^3 associative classifier is built by means of a lazy pruning technique which discards exclusively rules that only misclassify training data. Classification of unlabeled data is performed in two steps. A small subset of high quality rules is first considered. When this set is not able to classify the data, a larger rule set is exploited. This second set includes rules usually discarded by previous approaches. To cope with the need of mining large rule sets and efficiently use them for classification, a compact form is proposed to represent a complete rule set in a space-efficient way and without information loss. An extensive experimental evaluation on real and synthetic datasets shows that L^3 improves the classification accuracy with respect to previous approaches.

#index 1034723
#* On Modularity Clustering
#@ Ulrik Brandes;Daniel Delling;Marco Gaertler;Robert Gorke;Martin Hoefer;Zoran Nikoloski;Dorothea Wagner
#t 2008
#c 7
#! Modularity is a recently introduced quality measure for graph clusterings. It has immediately received considerable attention in several disciplines, and in particular in the complex systems literature, although its properties are not well understood. We study the problem of finding clusterings with maximum modularity, thus providing theoretical foundations for past and present work based on this measure. More precisely, we prove the conjectured hardness of maximizing modularity both in the general case and with the restriction to cuts, and give an Integer Linear Programming formulation. This is complemented by first insights into the behavior and performance of the commonly applied greedy agglomaration approach.

#index 1034724
#* Learning a Maximum Margin Subspace for Image Retrieval
#@ Xiaofei He;Deng Cai;Jiawei Han
#t 2008
#c 7
#! One of the fundamental problems in Content-Based Image Retrieval (CBIR) has been the gap been low level visual features and high level semantic concepts. To narrow down this gap, relevance feedback is introduced into image retrieval. With the user provided information, a classifier can be learned to discriminate between positive and negative examples. However, in real world applications, the number of user feedbacks is usually too small comparing to the dimensionality of the image space. Thus, a situation of overfitting may occur. In order to cope with the high dimensionality, we propose a novel supervised method for dimensionality reduction called Maximum Margin Projection (MMP). MMP aims to maximize the margin between positive and negative examples at each local neighborhood. Different from traditional dimensionality reduction algorithms such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) which effectively see only the global Euclidean structure, MMP is designed for discovering the local manifold structure. Therefore, MMP is likely to be more suitable for image retrieval where nearest neighbor search is usually involved. After projecting the images into a lower dimensional subspace, the relevant images get closer to the query image, thus the retrieval performance can be enhanced. The experimental results on a large image database demonstrates the effectiveness and efficiency of our proposed algorithm.

#index 1034725
#* A Web Usage Mining Framework for Mining Evolving User Profiles in Dynamic Web Sites
#@ Olfa Nasraoui;Maha Soliman;Esin Saka;Antonio Badia;Richard Germain
#t 2008
#c 7
#! In this paper, we present a complete framework and findings in mining web usage patterns from Web log files of a real website that has all the challenging aspects of real life web usage mining, including evolving user profiles and external data describing an ontology of the web content. Even though the website under study is part of a non-profit organization that does not "sell" any products, it was crucial to understand "who" the users were, "what" they looked at, and "how their interests changed with time", all of which are important questions in Customer Relationship Management (CRM). Hence, we present an approach to discover and track evolving user profiles. We also describe how to enrich the discovered user profiles with explicit information need that is inferred from search queries extracted from Web log data. Profiles are also enriched with other domain specific information facets that give a panoramic view of the discovered mass usage modes. An objective validation strategy is also used to assess the quality of the mined profiles, and in particular, their adaptability in the face of evolving user behavior.

#index 1034726
#* Incremental Maintenance of Online Summaries Over Multiple Streams
#@ Fatih Altiparmak;Ertem Tuncel;Hakan Ferhatosmanoglu
#t 2008
#c 7
#! We propose a novel predictive quantization (PQ) based approach for online summarization of multiple time varying data streams. A synopsis over a sliding window of most recent entries is computed in one pass and dynamically updated in constant time. The correlation between consecutive data elements is effectively taken into account without the need for preprocessing. We extend PQ to multiple streams and propose structures for real-time summarization and querying of a massive number of streams. Queries on any subsequence of a sliding window over multiple streams are processed in real-time. We examine each component of the proposed approach, prediction and quantization, separately and investigate the space-accuracy trade off for synopsis generation. Complementing the theoretical optimality of PQ based approaches, we show that the proposed technique, even for very short prediction windows, significantly outperforms the current techniques for a wide variety of query types on both synthetic and real data sets.

#index 1034727
#* A Cost-Based Approach to Adaptive Resource Management in Data Stream Systems
#@ Michael Cammert;Jurgen Kramer;Bernhard Seeger;Sonny Vaupel
#t 2008
#c 7
#! Data stream management systems need to control their resources adaptively since stream characteristics as well as query workload vary over time. In this paper we investigate an approach to adaptive resource management for continuous sliding window queries that adjusts window sizes and time granularities to keep resource usage within bounds. These two novel techniques differ from standard load shedding approaches based on sampling as they ensure exact query answers for given user-defined Quality of Service specifications, even under query re-optimization. In order to quantify the effects of both techniques on the various operations in a query plan, we develop an appropriate cost model for estimating operator resource allocation in terms of memory usage and processing costs. A thorough experimental study not only validates the accuracy of our cost model but also demonstrates the efficacy and scalability of the proposed techniques.

#index 1034728
#* Online Index Recommendations for High-Dimensional Databases Using Query Workloads
#@ Michael Gibas;Guadalupe Canahuate;Hakan Ferhatosmanoglu
#t 2008
#c 7
#! High-dimensional databases pose a challenge withrespect to efficient access. High-dimensional indexes do notwork because of the oft-cited "curse of dimensionality'. However, users are usually interested in querying data over a relativelysmall subset of the entire attribute set at a time. A potential solution is to use lower dimensional indexes that accurately represent the user access patterns. Query response using physical database design developed based on a static snapshot of the query workload may significantly degrade if the query patterns change.To address these issues, we introduce a parameterizable technique to recommend indexes based on index types frequently used forhigh-dimensional data sets and to dynamically adjust indexesas the underlying query workload changes. We incorporate aquery pattern change detection mechanism to determine when the access patterns have changed enough to warrant change inthe physical database design. By adjusting analysis parameters,we trade off analysis speed against analysis resolution. We perform experiments with a number of data sets, query sets, and parameters to show the effect that varying these characteristics has on analysis results.

#index 1034729
#* Semantic Ideation Learning for Agent-Based E-Brainstorming
#@ Soe-Tsyr Yuan;Yen-Chuan Chen
#t 2008
#c 7
#! Brainstorming has been a solution that helps organizations to generate creative ideas through teamwork and collaboration. However, by far, the role of information technology in brainstorming is merely like an assistant that passively supports the progression of brainstorming sessions instead of proactively engaging in the sessions. This research combines human's unique association thinking with the intelligent agent technique, devising an automated decision agent called Semantic Ideation Learning Agent (SILA) that can represent a session participant to actively participate in brainstorming. SILAs are grounded on the three association capabilities of human's thinking (similarity, contiguity, contrast). Moreover, a Collective Brainstorming Decision System (CBDS) is built to furnish an environment where SILAs can learn and share their knowledge with each other. We have successfully integrated CBDS into an intelligent care project (iCare) for the purpose of innovated e-service recommendation. The evaluation results are fairly promising.

#index 1034730
#* Distributed Suffix Tree Overlay for Peer-to-Peer Search
#@ Hai Zhuge;Liang Feng
#t 2008
#c 7
#! Establishing an appropriate semantic overlay on Peer-to-Peer networks to obtain both semantic ability and scalability is a challenge. Current DHT-based P2P networks are limited in their ability to support semantic search. This paper proposes the DST (Distributed Suffix Tree) overlay as the intermediate layer between the DHT overlay and the semantic overlay. The DST overlay supports search of keyword sequences. Its time cost is sub-linear with the length of the keyword sequences. Using a common interface, the DST overlay is independent of the variation of the underlying DHT overlays. Analysis and experiments show that DST-based search is fast, load-balanced, and useful in realizing accurate content search on large networks.

#index 1034731
#* Mining Multimedia Streams in Large-Scale Distributed Environments
#@ 
#t 2008
#c 7

#index 1035127
#* DryadeParent, An Efficient and Robust Closed Attribute Tree Mining Algorithm
#@ Alexandre Termier;Marie-Christine Rousset;Michele Sebag;Kouzou Ohara;Takashi Washio;Hiroshi Motoda
#t 2008
#c 7
#! In this paper, we present a new tree mining algorithm, DryadeParent, based on the hooking principle first introduced in Dryade. In the experiments, we demonstrate that the branching factor and depth of the frequent patterns to find are key factors of complexity for tree mining algorithms, even if often overlooked in previous work. We show that DryadeParent outperforms the current fastest algorithm, CMTreeMiner, by orders of magnitude on datasets where the frequent patterns have a high branching factor.

#index 1035128
#* Using Incremental PLSI for Threshold-Resilient Online Event Analysis
#@ Tzu-Chuan Chou;Meng Chang Chen
#t 2008
#c 7
#! The goal of on-line event analysis is to detect events and their associated documents in real-time from a continuous stream of documents generated by multiple information sources. Existing approaches (e.g., window-based, decay function, and adaptive threshold methods) incorporate the temporal relations of documents into traditional text categorization methods for event analysis. However, these methods suffer from the threshold dependence problem, i.e., their performance is only acceptable for a narrow range of thresholds; thus, it is difficult to designate an appropriate threshold in advance. In this paper, we propose a threshold resilient algorithm, called Incremental Probabilistic Latent Semantic Indexing (IPLSI), which can capture the storyline development of an event without the threshold dependence problem. The IPLSI algorithm is theoretically sound and more efficient than naïve PLSI approaches. The results of the performance evaluation based on the TDT 4 corpus show that the proposed algorithm reduces the error tradeoff cost of event detection by as much as 14.51% and increases the threshold range for acceptable performance by 300% - 800%

#index 1035129
#* Efficient Similarity Search in Nonmetric Spaces with Local Constant Embedding
#@ Lei Chen;Xiang Lian
#t 2008
#c 7
#! Similarity-based search has been a key factor for many applications, such as multimedia retrieval, data mining, web search and retrieval, and so on. There are two important issues related to the the similarity search, namely the design of a distance function to measure the similarity, and improving the search efficiency. Many distance functions have been proposed that attempt to closely mimic human recognition. Unfortunately, some of these well-designed distance functions do not follow the triangle inequality, and are, therefore, non-metric. As a consequence, efficient retrieval using these non-metric distance functions becomes more challenging, since most existing index structures assume that the indexed distance functions are metric. In this paper, we address this challenging problem by proposing an efficient method, local constant embedding (LCE), which divides the data set into disjoint groups, so that the triangle inequality holds within each group by constant shifting. Furthermore, we design a pivot selection approach for the converted metric distance and create an index structure to speed up the retrieval efficiency. Extensive experiments show that, our method works well on various non-metric distance functions and improves the retrieval efficiency by an order of magnitude compared to the linear scan and existing retrieval approaches with no false dismissals.

#index 1035130
#* Materialized Sample Views for Database Approximation
#@ Shantanu Joshi;Christopher Jermaine
#t 2008
#c 7
#! We consider the problem of creating sample view of a database table. A sample view is an indexed, materialized view that permits efficient sampling from an arbitrary range query over the view. Such "sample views'' are very useful to applications that require random samples from a database: approximate query processing, online aggregation, data mining, and randomized algorithms are a few examples. Our core technical contribution is a new file organization called the ACE Tree that is suitable for organizing and indexing a sample view. One of the most important aspects of the ACE Tree is that it supports online random sampling from the view. That is, at all times, the set of records returned by the ACE Tree constitutes a statistically random sample of the database records satisfying the relational selection predicate over the view. Our paper presents experimental results that demonstrate the utility of the ACE Tree.

#index 1035131
#* Long-Term Cross-Session Relevance Feedback Using Virtual Features
#@ Peng-Yeng Yin;Bir Bhanu;Kuang-Cheng Chang;Anlei Dong
#t 2008
#c 7
#! Relevance feedback (RF) is an iterative process which refines the retrievals by utilizing the user's feedback on previously retrieved results. Traditional RF techniques use solely the short-term learning experience and do not exploit the knowledge created during cross-sessions with multiple users. In this paper, we propose a novel RF framework which facilitates the combination of short-term and long-term learning processes by integrating the traditional methods with a new technique called the virtual feature. The feedback history with all the users is digested by the system and is represented in a very efficient form as a virtual feature of the images. As such, the dissimilarity measure can be adapted dynamically depending on the estimate of the semantic relevance derived from the virtual features. Also with a dynamic database, the user's subject concepts may transit from one to another. By monitoring the changes in retrieval performance, the proposed system can automatically adapt the concepts according to the new subject concepts. The experiments are conducted on a real image database. The results manifest that the proposed framework outperforms the one that adopts a traditional within-session RF technique.

#index 1035132
#* On the Design of Distributed Object Placement and Load Balancing Strategies in Large-Scale Networked Multimedia Storage Systems
#@ Zeng Zeng;Bharadwaj Veeravalli
#t 2008
#c 7
#! In a large-scale multimedia storage system (LMSS) where client requests for different multimedia objects may have different demands, placement and replication of the objects is an important factor, as it may result in an imbalance in server loading across the system. Since replica management and load balancing is all the more a crucial issue in multimedia systems, in the literature this problem is handled by centralized servers. Each object storage server (OSS) responses the requests coming from the centralized servers independently and has no communication with other OSSs among the system. In this paper, we design a novel distributed load balancing strategy of LMSS, in which the OSSs can cooperate together to achieve a high performance. Such OSS modeled as an M/M/m system, can replicate the objects to and balance the requests among other servers to achieve an optimal average waiting time (AWT) of the requests in the system. We validate the performance of the system via rigorous simulations with respect to several influencing factors and prove that our proposed strategy is scalable, flexible and efficient for the real-life applications.

#index 1035133
#* A Lagrangian Approach for Multiple Personalized Campaigns
#@ Yong-Hyuk Kim;Yourim Yoon;Byung-Ro Moon
#t 2008
#c 7
#! Multicampaign assignment problem is a campaign model to overcome the multiple recommendation problem which occurs when conducting several personalized campaigns simultaneously. In this paper, we propose a Lagrangian method for the problem. The original problem space is transformed to another simpler one by introducing Lagrange multipliers which relax the constraints of the multicampaign assignment problem. When the Lagrangian vector is supplied, we can compute the optimal solution under this new environment in O(NK^2) time, where N and K are the numbers of customers and campaigns, respectively. This is a linear-time method when the number of campaigns is a constant. However, it is not easy to find a Lagrangian vector in exact accord with given problem constraints. We thus combine the Lagrangian method with a genetic algorithm to find good feasible solutions. We verify the effectiveness of our evolutionary Lagrangian approach in both theoretical and experimental views of points. The suggested Lagrangian approach is practically attractive for large-scale real-world problems.

#index 1035134
#* A Rule-Based Object-Oriented OWL Reasoner
#@ Georgios Meditskos;Nick Bassiliades
#t 2008
#c 7
#! In this paper we describe O-DEVICE, a memory-based knowledge base system for reasoning and querying OWL ontologies by implementing RDF/OWL entailments in the form of production rules in order to apply the formal semantics of the language. Our approach is based on a transformation procedure of OWL ontologies into an Object-Oriented schema and the application of inference production rules over the generated objects in order to implement the various semantics of OWL. In order to enhance the performance of the system, we introduce a dynamic approach of generating production rules for ABOX reasoning and an incremental approach of loading ontologies. O-DEVICE is built over the CLIPS production rule system, using the object-oriented language COOL to model and handle ontology concepts and RDF resources. One of the contributions of our work is that we enable a well-known and efficient production rule system to handle OWL ontologies. We argue that although native OWL rule reasoners may process ontology information faster, they lack some of the key features that rule systems offer, such as the efficient manipulation of the information through complex rule programs. We present a comparison of our system with other OWL reasoners, showing that O-DEVICE can constitute a practical rule environment for ontology manipulation.

#index 1035135
#* Beyond Single-Page Web Search Results
#@ Ramakrishna Varadarajan;Vagelis Hristidis;Tao Li
#t 2008
#c 7
#! Given a user keyword query, current Web search engines return a list of individual web pages ranked by their "goodness" with respect to the query. Thus the basic unit for search and retrieval is an individual page, even though information on a topic is often spread across multiple pages. This degrades the quality of search results especially for long or uncorrelated (multi-topic) queries (in which individual keywords rarely occur together in the same document) where a single page is unlikely to satisfy the user's information need. We propose a technique that given a keyword query, on-the-fly generates new pages, called composed pages, which contain all query keywords. The composed pages are generated by extracting and stitching together relevant pieces from hyperlinked Web pages, and retaining links to the original Web pages. To rank the composed pages we consider both the hyperlink structure of the original pages, as well as the associations between the keywords within each page. Furthermore, we present and experimentally evaluate heuristic algorithms to efficiently generate the top composed pages. The quality of our method is compared to current approaches using user surveys. Finally, we also show how our techniques can be used to perform query-specific summarization of web pages.

#index 1035136
#* Characterizing the MVDs That Hold in a Connected Subtree of a Join Tree: A Correction
#@ Wai Yin Mok
#t 2008
#c 7
#! Given an acyclic database scheme R and a connected subtree T of a join tree for R, this paper provides a characterization of the set of multivalued dependencies that hold over T with respect to R. The characterization provided in this paper corrects a result in one of our previous papers published in this journal.

#index 1035137
#* The Efficacy of Commutativity-Based Semantic Locking in a Real-World Application
#@ Paul Wu;Alan Fekete;Uwe Rohm
#t 2008
#c 7
#! While the dominant approach to persistent storage in practice is to use a relational DBMS, there are some specialist applications that rely on object stores. The performance of these applications depends on the efficiency of the object store's concurrency control mechanism. Today's predominant concurrency control mechanism is strict two-phase object locking. In the 80s, an interesting alternative was developed: commutativity-based semantic locking. In theory, it can outperform traditional locking schemes in certain scenarios with appropriate commutativity potential. In this paper, we study the real-world performance of different locking strategies in a particular industrial application from the telecommunications sector. We compare object-based locking and commutativity-based semantic locking. We found that, in this application, semantic locking performs equally to, but no better than, object locking in reasonable mixes of real-world transactions, and that it only outperforms in a deliberately contrived mix.

#index 1035138
#* Call for Papers: Mining Multimedia Streams in Large-Scale Distributed Environments
#@ 
#t 2008
#c 7

#index 1038715
#* Mining Weighted Association Rules without Preassigned Weights
#@ Ke Sun;Fengshan Bai
#t 2008
#c 7
#! Association rule mining is a key issue in data mining. However the classical models ignore the difference between the transactions; and the weighted association rule mining does not work on databases with only binary attributes. In this paper, we introduce a new measure wsupport, which does not require pre-assigned weights. It takes the quality of transactions into consideration, using link-based models. A fast miming algorithm is given and a large amount of experimental results is presented.

#index 1038716
#* A Framework for Mining Sequential Patterns from Spatio-Temporal Event Data Sets
#@ Yan Huang;Liqin Zhang;Pusheng Zhang
#t 2008
#c 7
#! Given a large spatio-temporal database of events, where each event consists of the following fields: event-ID, time, location, event-type, mining spatio-temporal sequential patterns is to identify significant event type sequences. Such spatio-temporal sequential patterns are crucial to investigate spatial and temporal evolutions of phenomena in many application domains. Recent literatures have explored the sequential patterns on transaction data and trajectory analysis on moving objects. However, these methods can not be directly applied to mining sequential patterns from a large number of spatio-temporal events. Two major research challenges are still remaining: (i) the definition of significance measures for spatio-temporal sequential patterns to avoid spurious ones; (ii) the algorithmic design under the significance measures which may not guarantee the downward closure property. In this paper, we propose a sequence index as the significance measure for spatio-temporal sequential patterns, which is meaningful due to its interpretability using spatial statistics. We propose a novel algorithm called Slicing-STS-Miner to tackle the algorithmic design challenges using the spatial sequence index which does not preserve the downward closure property. We compare the proposed algorithm with a simple algorithm called STS-Miner that utilizes the weak monotone property of the sequence index. Performance evaluations using both synthetic and real world datasets shows that the Slicing-STS-Miner is an order of magnitude faster than STS-Miner for large datasets.

#index 1038717
#* A Study of the Neighborhood Counting Similarity
#@ Hui Wang;Fionn Murtagh
#t 2008
#c 7
#! A novel similarity, neighborhood counting measure, was recently proposed which counts the number of neighborhoods of a pair of data points. This similarity can handle numerical and categorical attributes in a conceptually uniform way, can be calculated efficiently through a simple formula, and gives good performance when tested in the framework of k-nearest neighbor classifier. In particular it consistently outperforms a combination of the classical Euclidean distance and Hamming distance. This measure was also shown to be related to a probability formalism, G probability, which is induced from a target probability function P. It was however unclear how G is related to P, especially for classification. Therefore it was not possible to explain some characteristic features of the neighborhood counting measure. In this paper we show that G is a linear function of P, and G-based Bayes classification is equivalent to P-based Bayes classification. We also show that the k-nearest neighbor classifier, when weighted by the neighborhood counting measure, is in fact an approximation of the G-based Bayes classifier, and furthermore, the P-based Bayes classifier. Additionally we show that the neighborhood counting measure remains unchanged when binary attributes are treated as categorical or numerical data. This is a feature that is not shared by other distance measures, to the best of our knowledge. This study provides a theoretical insight into the neighborhood counting measure.

#index 1038718
#* Clustering of Count Data Using Generalized Dirichlet Multinomial Distributions
#@ Nizar Bouguila
#t 2008
#c 7
#! In this paper we examine the problem of count data clustering. We analyze this problem using finite mixtures of distributions. The multinomial and the multinomial Dirichlet distributions are widely accepted to model count data. We show that these two distributions cannot be the best choice in all the applications and we propose another model called the multinomial generalized Dirichlet distribution (MGDD) that is the composition of the generalized Dirichlet distribution and the multinomial, in the same way that the multinomial Dirichlet distribution (MDD) is the composition of the Dirichlet and the multinomial. The estimation of the parameters and the determination of the number of components in our model are based on the deterministic annealing expectation-maximization (DAEM) approach and the minimum description length (MDL) criterion, respectively. We compare our method to standard approaches such as multinomial and multinomial Dirichlet mixtures to show its merits. The comparison involves different applications such as spatial color image databases indexing, handwritten digit recognition, and text documents clustering.

#index 1038719
#* Distributed Identification of Top-l Inner Product Elements and its Application in a Peer-to-Peer Network
#@ Kamalika Das;Kanishka Bhaduri;Kun Liu;Hillol Kargupta
#t 2008
#c 7
#! Inner product computation is an important primitive used in many techniques for feature dependency detection, distance computation, clustering and correlation computation among others. Recently, peer-to-peer networks are getting increasing attention in various applications involving distributed file sharing, sensor networks, and mobile ad hoc networks. Efficient identification of top few inner product entries from the entire inner product matrix of features in a distributed peer-to-peer network is a challenging problem since centralizing the data from all the nodes in a synchronous, communication efficient manner may not be an option. This paper deals with the problem of identifying significant inner products among features in a peer-to-peer environment where different nodes observe a different set of data. It uses an ordinal framework to develop probabilistic algorithms to find top-l elements in the inner product matrix. These l inner product entries are important in making crucial decisions about dependency or relatedness between feature pairs, important for a number of data mining applications. In this paper we present experimental results demonstrating accurate and scalable performance of this algorithm for large peer-to-peer networks and also describe a real-world application for this algorithm.

#index 1038720
#* Semisupervised Clustering with Metric Learning using Relative Comparisons
#@ Nimit Kumar;Krishna Kummamuru
#t 2008
#c 7
#! Semi-supervised clustering algorithms partition a given data set using limited supervision from the user. The success of these algorithms depend on the type of supervision and also on the kind of dissimilarity measure used while creating partitions of the space. This paper proposes a clustering algorithm that uses supervision in terms of relative comparisons, viz., x is closer to y than to z. The proposed clustering algorithm simultaneously learns the underlying dissimilarity measure while finding compact clusters in the given data set using relative comparisons. Through our experimental studies on high-dimensional textual data sets, we demonstrate that the proposed algorithm achieves higher accuracy and is more robust than similar algorithms using pairwise constraints for supervision.

#index 1038721
#* Adaptive Broadcasting for Similarity Queries in Wireless Content Delivery Systems
#@ Wei Wang;Chinya V. Ravishankar
#t 2008
#c 7
#! We present a new adaptive and energy-efficient broadcast dissemination model that supports flexible responses to client requests. In current broadcast dissemination models, clients must specify precisely what documents they require, and servers disseminate exactly those documents. This approach can be impractical, since in practice, clients may know the characteristics of the documents, but not the document names or IDs. In our model, clients specify the required document using attributes, and servers broadcast documents that match client requests at a prespecified level of similarity. A single document may satisfy several clients, so the server broadcasts a minimal set of documents that achieves a desired level of satisfaction in the client population. We introduce a mechanism for the server to obtain randomized feedback from clients to adapt its broadcast program to client needs. Finally, the server integrates a selective tune-in scheme based on approximate index matching to allow clients to conserve energy. Our simulation results show that our model captures client interest patterns efficiently and accurately and scales very well with the number of clients, while reducing overall client average waiting times. The selective tune-in scheme can considerably reduce the consumption of client energy with moderate waiting time overhead.

#index 1038722
#* Building toward Capability Specifications of Web Services Based on an Environment Ontology
#@ Puwei Wang;Zhi Jin;Lin Liu;Guangjun Cai
#t 2008
#c 7
#! An automated Web service discovery requires Web service capability specifications of a high precision. Semantic-based approaches are inherently more precise than conventional keyword-based approaches. This paper proposes to build capability specifications of Web services based on an Environment Ontology, main concepts of which are the environment entities in a particular application domain and their interactions. For each environment entity, there is a tree-like hierarchical state machine modeling the effects which are to be achieved by the Web services on this environment entity. The proposed approach is based on the assumption that the Web service capability specifications, built on the effects of the environment entities, are more accessible and observable. Algorithms for constructing the domain environment ontology and the matchmaking between the Web service capability specifications are presented to show how the Web service discovery is supported. An example on Travel Service is given to illustrate this proposed approach.

#index 1038723
#* Localized Co-Occurrence Model for Fast Approximate Search in 3D Structure Databases
#@ Zi Huang;Heng Tao Shen;Xiaofang Zhou
#t 2008
#c 7
#! Similarity search for $3$D structure data sets is fundamental to many database applications such as molecular biology, image registration and computer aided design. However, it is well known that computing structural similarity is extremely expensive due to high exponential time complexity of structure similarity measures. As the structure databases keep growing rapidly, real-time search from large structure databases becomes problematic. In this paper, we present a novel statistical model, multi-resolution \textit{Localized Co-occurrence Model} (LCM), to approximately measure the similarity between the two $3$D structures in linear time complexity for fast retrieval. LCM could capture both distribution characteristics and spatial structure of $3$D data by localizing the point co-occurrence relationship within a predefined neighborhood system. A novel structure query processing method called \textit{iBound} is also proposed for further computational reduction. iBound avoids a large amount of expensive computation at higher resolution LCMs. By superposing two LCMs, their largest common substructure can also be found quickly. Finally, our experiment results prove the effectiveness and efficiency of our methods.

#index 1038724
#* Watermill: An Optimized Fingerprinting System for Databases under Constraints
#@ Julien Lafaye;David Gross-Amblard;Camelia Constantin;Meryem Guerrouani
#t 2008
#c 7
#! his paper presents a watermarking/fingerprinting system for relational databases. It features a built-in declarative language to specify usability constraints that watermarked datasets must comply with. For a subset of these constraints, namely weight-independent constraints, we propose a novel watermarking strategy which consists of translating them into an integer linear program. We show two watermarking strategies: an exhaustive one based on integer linear programming constraint solving and a scalable pairing heuristic. Fingerprinting applications, for which several distinct watermarks need to be computed, benefit from the reduced computation time of our method that precomputes the watermarks only once. Moreover we show that our method enables practical collusion-secure fingerprinting since the precomputed watermarks are based on binary alterations located at exactly the same positions. The paper includes an in-depth analysis of false hits and false misses occurrence probabilities for the detection algorithm. Experiments performed on our open source software Watermill assess the watermark robustness against common attacks, and show that our method outperforms the existing ones concerning the watermark embedding speed.

#index 1038725
#* A Similarity Metric for Retrieval of Compressed Objects: Application for Mining Satellite Image Time Series
#@ Lionel Gueguen;Mihai Datcu
#t 2008
#c 7
#! This paper adresses the problem of building an index of compressed object data-base. We introduce an informational similarity measure based on coding length of two part codes. Then, we present a methodology to compress the data-base by taking into account inter-object redundancies and by using the informational similarity measure. The method produces an index included in the code of the data-volume. This index is built such that it contains the minimal sufficient information to discriminate the data-volume objects. Then, we present an optimal two part coder for compressing spatio-temporal events contained in Satellite Image Time Series (SITS). The two part coder allows us to measure similarity and, then, to derive an optimal index of SITS spatio-temporal events. The resulting index is representative of the SITS information content and enables queries based on information content.

#index 1068963
#* A Niching Memetic Algorithm for Simultaneous Clustering and Feature Selection
#@ Weiguo Sheng;Xiaohui Liu;Mike Fairhurst
#t 2008
#c 7
#! Clustering is inherently a difficult task, and is made even more difficult when the selection of relevant features is also an issue. In this paper we propose an approach for simultaneous clustering and feature selection using a niching memetic algorithm. Our approach (which we call NMA_CFS) makes feature selection an integral part of the global clustering search procedure and attempts to overcome the problem of identifying less promising locally optimal solutions in both clustering and feature selection, without making any a priori assumption about the number of clusters. Within the NMA_CFS procedure, a variable composite representation is devised to encode both feature selection and cluster centers with different numbers of clusters. Further, local search operations are introduced to refine feature selection and cluster centers encoded in the chromosomes. Finally, a niching method is integrated to preserve the population diversity and prevent premature convergence. In an experimental evaluation we demonstrate the effectiveness of the proposed approach and compare it with other related approaches, using both synthetic and real data.

#index 1068964
#* Cluster Kernels: Resource-Aware Kernel Density Estimators over Streaming Data
#@ Christoph Heinz;Bernhard Seeger
#t 2008
#c 7
#! A variety of real-world applications heavily relies on an adequate analysis of transient data streams. Due to the rigid processing requirements of data streams, common analysis techniques as known from data mining are not directly applicable. A fundamental building block of many data mining and analysis approaches is density estimation. It provides a well-defined estimation of a continuous data distribution, a fact, which makes its adaptation to data streams desirable. A convenient method for density estimation utilizes kernels. The computational complexity of kernel density estimation, however, renders its application to data streams impossible. In this paper, we tackle this problem and propose our Cluster Kernel approach which provides continuously computed kernel density estimators over streaming data. Not only do Cluster Kernels meet the rigid processing requirements of data streams, they also allocate only a constant amount of memory, even with the opportunity to adapt it dynamically to changing system resources. For this purpose, we develop an intelligent merge scheme for Cluster Kernels and utilize continuously collected local statistics to resample already processed data. We focus on Cluster Kernels for one-dimensional data streams, but also address the multi-dimensional case. We validate the efficacy of Cluster Kernels for a variety of real-world data streams in an extensive experimental study.

#index 1068965
#* Ranked Reverse Nearest Neighbor Search
#@ Ken C. K. Lee;Baihua Zheng;Wang-Chien Lee
#t 2008
#c 7
#! Given a set of data points P and a query point q in a multidimensional space, Reverse Nearest Neighbor (RNN) query finds data points in P whose nearest neighbors are q. Reverse k-Nearest Neighbor (RkNN) query (where k ≥ 1) generalizes RNN query to find data points whose kNNs include q. For RkNN query semantics, q is said to have influence to all those answer data points. The degree of q's influence on a data point p (∈ P) is denoted by κp where q is the κp-th NN of p. We introduce a new variant of RNN query, namely, Ranked Reverse Nearest Neighbor (RRNN) query, that retrieves t data points most influenced by q, i.e., the t data points having the smallest κ's with respect to q. To answer this RRNN query efficiently, we propose two novel algorithms, κ-Counting and κ-Browsing that are applicable to both monochromatic and bichromatic scenarios and are able to deliver results progressively. Through an extensive performance evaluation, we validate that the two proposed RRNN algorithms are superior to solutions derived from algorithms designed for RkNN query.

#index 1068966
#* Simultaneous Pattern and Data Clustering for Pattern Cluster Analysis
#@ Andrew K. C. Wong;Gary C. L. Li
#t 2008
#c 7
#! In data mining and knowledge discovery, pattern discovery extracts previously unknown regularities in the data and is a useful tool for categorical data analysis. However, the number of patterns discovered is often overwhelming. It is difficult and time-consuming to 1) interpret the discovered patterns and 2) use them to further analyze the data set. To overcome these problems, this paper proposes a new method that clusters patterns and their associated data simultaneously. When patterns are clustered, the data containing the patterns are also clustered; and the relation between patterns and data is made explicit. Such an explicit relation allows the user on the one hand to further analyze each pattern cluster via its associated data cluster, and on the other hand to interpret why a data cluster is formed via its corresponding pattern cluster. Since the effectiveness of clustering mainly depends on the distance measure, several distance measures between patterns and their associated data are proposed. Their relationships to the existing common ones are discussed. Once pattern clusters and their associated data clusters are obtained, each of them can be further analyzed individually. To evaluate the effectiveness of the proposed approach, experimental results on synthetic and real data are reported.

#index 1068967
#* Analyzing and Managing Role-Based Access Control Policies
#@ Karsten Sohr;Michael Drouineaud;Gail-Joon Ahn;Martin Gogolla
#t 2008
#c 7
#! Today more and more security-relevant data is stored on computer systems; security-critical business processes are mapped to their digital counterparts. This situation applies to various domains such as health care industry, digital government, and financial service institutes requiring that different security requirements must be fulfilled. Authorisation constraints can help the policy architect design and express higher-level organisational rules. Although the importance of authorisation constraints has been addressed in the literature, there does not exist a systematic way to verify and validate authorisation constraints. In this paper, we specify both non-temporal and history-based authorisation constraints in the Object Constraint Language (OCL) and first-order linear temporal logic (LTL). Based upon these specifications, we attempt to formally verify role-based access control policies with the help of a theorem prover and to validate policies with the USE system, a validation tool for OCL constraints. We also describe an authorisation engine, which supports the enforcement of authorisation constraints.

#index 1068968
#* Integrating Data Warehouses with Web Data: A Survey
#@ Juan Manuel Perez Martinez;Rafael Berlanga;Maria Jose Aramburu;Torben Bach Pedersen
#t 2008
#c 7
#! This paper surveys the most relevant research on combining Data Warehouse (DW) and Web data. It studies the XML technologies that are currently being used to integrate, store, query and retrieve web data, and their application to DWs. The paper reviews different DW distributed architectures and the use of XML languages as an integration tool in these systems. It also introduces the problem of dealing with semi-structured data in a DW. It studies Web data repositories, the design of multidimensional databases for XML data sources and the XML extensions of On-Line Analytical Processing techniques. The paper addresses the application of information retrieval technology in a DW to exploit text-rich documents collections. The authors hope that the paper will help to discover the main limitations and opportunities that offer the combination of the DW and the Web fields, as well as, to identify open research lines.

#index 1068969
#* Chaotic Time Series Prediction Using a Neuro-Fuzzy System with Time-Delay Coordinates
#@ Jun Zhang;Henry Shu-Hung Chung;W. -L. Lo
#t 2008
#c 7
#! This paper presents an investigation into the use of the time delay coordinate embedding technique in the multi-input-multi-output-adaptive-network-based fuzzy inference system (MANFIS) for chaotic time series prediction. The inputs of the MANFIS are embedded-phase-space (EPS) vectors preprocessed from the time series under test while the output time series is extracted from the EPS vectors. With such EPS preprocessing, the prediction accuracy of the MANFIS is found to be significantly improved. The proposed system will be tested with a periodic and the Mackey-Glass chaotic time series by comparing the prediction accuracy with and without EPS preprocessing. A moving root-mean-square error is used to monitor the error along the prediction horizon and to tune the membership functions in the MANFIS.

#index 1068970
#* Molecular Verification of Rule-Based Systems Based on DNA Computation
#@ Chung-Wei Yeh;Chih-Ping Chu
#t 2008
#c 7
#! Various graphic techniques have been developed to analyze structural errors in rule-based systems that utilize inference (propositional) logic rules. Four typical errors in rule-based systems are: redundancy (numerous rule sets resulting in the same conclusion); circularity (a rule leading back to itself); incompleteness (deadends or a rule set conclusion leading to unreachable goals); and inconsistency (rules conflicting with each other). This study presents a new DNA-based computing algorithm mainly based upon Adleman's DNA operations. It can be used to detect such errors. There are three phases to this molecular solution: rule-to-DNA transformation design, solution space generation, and rule verification. We first encode individual rules using relatively short DNA strands, and then generate all possible rule paths by the directed joining of such short strands to form longer strands. We then conduct the verification algorithm to detect errors. The potential of applying this proposed DNA computation algorithm to rule verification is promising given the operational time complexity of O(n*q), in which n denotes the number of fact clauses in the rule base and q is the number of rules with longest inference chain.

#index 1068971
#* Meshing Streaming Updates with Persistent Data in an Active Data Warehouse
#@ Neoklis Polyzotis;Spiros Skiadopoulos;Panos Vassiliadis;Alkis Simitsis;Nils Frantzell
#t 2008
#c 7
#! An active warehouse is refreshed on-line and thus achieves a higher consistency between the stored information and the latest data updates. The need for on-line warehouse refreshment introduces several challenges in the implementation of data warehouse transformations. In this article, we focus on a frequently encountered operation in this context, namely, the join of a fast stream $S$ of source updates with a disk-based relation $R$, under the constraint of limited memory. This operation lies at the core of several common transformations, such as, surrogate key assignment, duplicate detection or identification of newly inserted tuples. We propose a specialized join algorithm, termed MeshJoin , that compensates for the difference in the access cost of the two join inputs by (a) relying entirely on fast sequential scans of $R$, and (b) sharing the I/O cost of accessing $R$ across multiple tuples of $S$. We detail the MeshJoin algorithm and develop a systematic cost model that enables tuning MeshJoin based on the available memory and the desired throughput. We present an experimental study that validates the performance of MeshJoin on synthetic and real-life data. Our results verify the effectiveness of MeshJoin and demonstrate its advantages over existing join algorithms.

#index 1068972
#* Streaming Time Series Summarization Using User-Defined Amnesic Functions
#@ Themis Palpanas;Michail Vlachos;Eamonn Keogh;Dimitrios Gunopulos
#t 2008
#c 7
#! The past decade has seen a wealth of research on time series representations. The vast majority of research has concentrated on representations that are calculated in batch mode and represent each value with approximately equal fidelity. However, the increasing deployment of mobile devices and real time sensors has brought home the need for representations that can be incrementally updated, and can approximate the data with fidelity proportional to its age. The latter property allows us to answer queries about the recent past with greater precision, since in many domains recent information is more useful than older information. We call such representations amnesic. While there has been previous work on amnesic representations, the class of amnesic functions possible was dictated by the representation itself. In this work, we introduce a novel representation of time series that can represent arbitrary, user-specified amnesic functions. We propose online algorithms for our representation, and discuss their properties. Finally, we perform an extensive empirical evaluation on 40 datasets, and show that our approach can efficiently maintain a high quality amnesic approximation.

#index 1068973
#* Editorial: TKDE Editorial Board Changes
#@ Xindong Wu
#t 2008
#c 7

#index 1081542
#* Bagging with Adaptive Costs
#@ Yi Zhang;W. Nick Street
#t 2008
#c 7
#! Ensemble methods have proved to be highly effective in improving the performance of base learners under most circumstances. In this paper, we propose a new algorithm that combines the merits of some existing techniques, namely bagging, arcing and stacking. The basic structure of the algorithm resembles bagging. However, the misclassification cost of each training point is repeatedly adjusted according to its observed out-of-bag vote margin. In this way, the method gains the advantage of arcing - building the classifier the ensemble needs - without fixating on potentially noisy points. Computational experiments show that this algorithm performs consistently better than bagging and arcing with linear and nonlinear base classifiers. In view of the characteristics of bacing, a hybrid ensemble learning strategy, which combines bagging and different versions of bacing, is proposed and studied empirically.

#index 1081543
#* Explaining Classifications For Individual Instances
#@ Marko Robnik-Šikonja;Igor Kononenko
#t 2008
#c 7
#! We present a method for explaining predictions for individual instances. The presented approach is general and can be used with all classification models that output probabilities. It is based on decomposition of a model's predictions on individual contributions of each attribute. Our method works for so called black box models such as support vector machines, neural networks, and nearest neighbor algorithms as well as for ensemble methods, such as boosting and random forests. We demonstrate that the generated explanations closely follow the learned models and present a visualization technique which shows the utility of our approach and enables the comparison of different prediction methods.

#index 1081544
#* Feature Extraction and Uncorrelated Discriminant Analysis for High-Dimensional Data
#@ Wen-Hui Yang;Dao-Qing Dai;Hong Yan
#t 2008
#c 7
#! High-dimensional data and small sample size problem occur in many modern pattern classification applications, such as face recognition and gene expression data analysis. To deal with such data, an important step is the dimensionality reduction. Principal component analysis(PCA) and between-group analysis(BGA) are two commonly used methods and various extensions exist. The principle of these two approaches comes from their best approximation. From a pattern recognition perspective we show that PCA based on total-scatter matrix preserves linear separability and BGA based on between-scatter matrix retains only the distances between class centroid. Moreover we propose a novel uncorrelated discriminant analysis (UDA) algorithm. It combines rank preserving dimensionality reduction and constraint discriminant analysis, and serves as a simple and complete solution for small sample size problem. We conduct a series of comparative study on face images and gene expression data sets to evaluate UDA in terms of classification accuracy and robustness.

#index 1081545
#* Hierarchical Clustering of Time-Series Data Streams
#@ Pedro Pereira Rodrigues;João Gama;Joao Pedroso
#t 2008
#c 7
#! This paper presents and analyzes an incremental system for clustering streaming time series. The Online Divisive-Agglomerative Clustering (ODAC) system continuously maintains a tree-like hierarchy of clusters that evolves with data. ODAC uses a top-down strategy. The splitting criterion is a correlation-based dissimilarity measure among time series, splitting each node by the farthest pair of streams, which defines the diameter of the cluster. In stationary environments expanding the structure leads to a decrease in the diameters of the clusters. The system uses a merge operator, which agglomerates two sibling clusters, in order to react to changes in the correlation structure between time series. The split and merge operators are triggered in response to changes in the diameters of existing clusters. The system is designed to process thousands of data streams that flow at high-rate. The main features of the system include update time and memory consumption that do not depend on the number of examples in the stream. Moreover, the time and memory required to process an example decreases whenever the cluster structure expands. Experimental results on artificial and real data assess the processing qualities of the system, suggesting competitive performance on clustering streaming time series, exploring also its ability to deal with concept drift.

#index 1081546
#* Improving Bayesian Network Structure Learning with Mutual Information-Based Node Ordering in the K2 Algorithm
#@ Xue-Wen Chen;Gopalakrishna Anantha;Xiaotong Lin
#t 2008
#c 7
#! Structure learning of Bayesian networks is a well-researched but computationally hard task. We present an algorithm that integrates an information theory-based approach and a scoring function-based approach for learning structures of Bayesian networks. Our algorithm also makes use of basic Bayesian network concepts like d-separation and Markov independence. We show that the proposed algorithm is capable of handling networks with a large number of variables. We present the applicability of the proposed algorithm on four standard network datasets and also compare its performance and computational efficiency with other standard structure learning methods. The experimental results show that our method can efficiently and accurately identify complex network structures from data.

#index 1081547
#* Text Clustering with Feature Selection by Using Statistical Data
#@ Yanjun Li;Congnan Luo;Soon M. Chung
#t 2008
#c 7
#! Feature selection is an important method for improving the efficiency and accuracy of text categorization algorithms by removing redundant and irrelevant terms from the corpus. In this paper, we propose a new supervised feature selection method, named CHIR, which is based on the Chi-square statistic and new statistical data that can measure the positive term-category dependency. We also propose a new text clustering algorithm TCFS, which stands for Text Clustering with Feature Selection. TCFS can incorporate CHIR to identify relevant features (i.e., terms) iteratively, and the clustering becomes a learning process. We compared TCFS and the k-means clustering algorithm in combination with different feature selection methods for various real data sets. Our experimental results show that TCFS with CHIR has better clustering accuracy in terms of the F-measure and the purity.

#index 1081548
#* Time-Aware Web Users' Clustering
#@ Sophia G. Petridou;Vassiliki A. Koutsonikola;Athena I. Vakali;Georgios I. Papadimitriou
#t 2008
#c 7
#! Web users clustering is a crucial task for mining information related to users needs and preferences. Up to now, popular clustering approaches build clusters based on usage patterns derived from users' page preferences. This paper emphasizes the need to discover similarities in users' accessing behavior with respect to the time locality of their navigational acts. In this context, we present two time aware clustering approaches for tuning and binding the page and time visiting criteria. The two tracks of the proposed algorithms define clusters with users that show similar visiting behavior at the same time period, by varying the priority given to page or time visiting. The proposed algorithms are evaluated using both synthetic and real datasets and the experimentation has shown that the new clustering schemes result in enriched clusters compared to those created by the conventional non-time aware users clustering approaches. These clusters contain users exhibiting similar access behavior not only in terms of their page preferences but also of their access time.

#index 1081549
#* Effective Generation of Data Broadcast Schedules with Different Allocation Numbers for Multiple Wireless Channels
#@ Song-Yi Yi;Seunghoon Nam;Sungwon Jung
#t 2008
#c 7
#! Existing methods of scheduling data items over multiple wireless broadcast channels focus on the assignment of a data item to a channel. But data items are not allocated more than once per broadcast cycle to a single channel. Our scheme considers the numbers of copies of a data item that should be allocated in the context of the channel assignment problem, and aims to reduce the average data access time by allocating a popular data item more than once per cycle to the channel to which it is assigned. The number of times that each data item is allocated reflects its access probability. Simulation results show that our scheme reduces the average expected delay, especially when there are few channels.

#index 1081550
#* A Scrambling Method for Fingerprint Positioning Based on Temporal Diversity and Spatial Dependency
#@ Sheng-Po Kuo;Yu-Chee Tseng
#t 2008
#c 7
#! Signal strength fluctuation is one of the major problems in a fingerprint-based localization system. To alleviate this problem, we propose a scrambling method to exploit temporal diversity and spatial dependency of collected signal samples. We present how to apply these properties to enhance the positioning accuracy of several existing schemes. Simulation studies and experimental results show that the scrambling method can greatly improve positioning accuracy, especially when the tracked object has some degree of mobility.

#index 1081551
#* Location Fingerprinting In A Decorrelated Space
#@ Shih-Hau Fang;Tsung-Nan Lin;Po-Chiang Lin
#t 2008
#c 7
#! We present a novel approach to the indoor localization in wireless environments. The main contribution of this paper is four folds: (a) we show that, by projecting the measured signal into a decorrelated signal space, the accuracy is improved since the cross-correlation between each AP is reduced. (b) We demonstrate that this novel approach achieves a more efficient information compaction and provides a better scheme to reduce the online computation. The drawback of AP selection techniques is overcomed since we reduce the dimensionality by combing features. Each component in the decorrelated space is the linear combination of all APs. Therefore a more efficient mechanism is provided to utilize information of all APs while reducing the computational complexity. (c) Experimental results show that the size of training samples can be greatly reduced. That is, fewer human efforts are required for developing the system. (d) We carry out comparisons between RSS and three decorrelated spaces including Discrete Cosine Transform, principal component analysis (PCA), and independent component analysis in this paper. Two AP selection criteria proposed in literature, MaxMean and InfoGain are also compared. Testing on a realistic WLAN environment, we find that PCA achieves the best performance on the location fingerprinting task.

#index 1081552
#* On Graph Features of Semantic Web Schemas
#@ Yannis Theoharis;Yannis Tzitzikas;Dimitris Kotzinos;Vassilis Christophides
#t 2008
#c 7
#! In this paper, we measure and analyze the graph features of Semantic Web (SW) schemas with focus on power-law degree distributions. Our main finding is that the majority of SW schemas with a significant number of properties (resp. classes) approximate a power-law for total-degree (resp. number of subsumed classes) distribution. Moreover, our analysis revealed some emerging conceptual modeling practices of SW schema developers, namely: a) each schema has a few focal classes that have been analyzed in detail (i.e., having numerous properties and subclasses) which are further connected with focal classes defined in other schemas, b) the class subsumption hierarchies are mostly unbalanced (i.e., some branches are deep and heavy, while others are shallow and light), c) most properties have as domain/range classes that are located highly at the class subsumption hierarchies and d) the number of recursive/multiple properties is significant. The knowledge of these features is essential for guiding synthetic SW schema generation, which is an important step towards benchmarking SW repositories and query languages implementations.

#index 1081553
#* Analyzing the Structure and Evolution of Massive Telecom Graphs
#@ Amit Anil Nanavati;Rahul Singh;Dipanjan Chakraborty;Koustuv Dasgupta;Sougata Mukherjea;Gautam Das;Siva Gurumurthy;Anupam Joshi
#t 2008
#c 7
#! With ever growing competition in telecommunications markets, operators have to increasingly rely on business intelligence to offer the right incentives to their customers. Existing approaches for telecom business intelligence have almost solely focused on the individual behavior of customers. In this paper, we use the Call Detail Records of a mobile operator to construct Call graphs, that is, graphs induced by people calling each other. We determine the structural properties of these graphs and also introduce the {\sl Treasure-Hunt} model to describe the shape of mobile call graphs. We also determine how the structure of these call graphs evolve over time. Finally, since Short Messaging Service (SMS) is becoming a preferred mode of communication among many sections of the society we also study the properties of the SMS graph. Our analysis indicates several interesting similarities as well as differences between the SMS graph and the corresponding call graph. We believe that our analysis techniques can allow telecom operators to better understand the social behavior of their customers, and potentially provide major insights for designing effective incentives.

#index 1081573
#* Grid Service Discovery with Rough Sets
#@ Maozhen Li;Bin Yu;Omer Rana;Zidong Wang
#t 2008
#c 7
#! The computational grid is evolving as a service-oriented computing infrastructure that facilitates resource sharing and large-scale problem solving over the Internet. Service discovery becomes an issue of vital importance in utilising grid facilities. This paper presents ROSSE, a Rough sets based search engine for grid service discovery. Building on Rough sets theory, ROSSE is novel in its capability to deal with uncertainty of properties when matching services. In this way, ROSSE can discover the services that are most relevant to a service query from a functional point of view. Since functionally matched services may have distinct non-functional properties related to Quality of Service (QoS), ROSSE introduces a QoS model to further filter matched services with their QoS values to maximise user satisfaction in service discovery. ROSSE is evaluated in terms of its accuracy and efficiency in discovery of computing services.

#index 1081574
#* C-TREND: Temporal Cluster Graphs for Identifying and Visualizing Trends in Multiattribute Transactional Data
#@ Gediminas Adomavicius;Jesse Bockstedt
#t 2008
#c 7
#! Organizations and firms are capturing increasingly more data about their customers, suppliers, competitors, and business environment. Most of this data is multi-attribute (multi-dimensional) and temporal in nature. Data mining and business intelligence techniques are often used to discover patterns in such data; however, mining temporal relationships typically is a complex task. We propose a new data analysis and visualization technique for representing trends in multi-attribute temporal data using a clustering-based approach. We introduce C-TREND, a system that implements the temporal cluster graph construct, which maps multi-attribute temporal data to a two-dimensional directed graph that identifies trends in dominant data types over time. In this paper, we present our temporal clustering-based technique, discuss its algorithmic implementation and performance, demonstrate applications of the technique by analyzing data on wireless networking technologies and baseball batting statistics, and introduce a set of metrics for further analysis of discovered trends.

#index 1081575
#* Algorithms for Storytelling
#@ Deept Kumar;Naren Ramakrishnan;Richard F. Helm;Malcolm Potts
#t 2008
#c 7
#! We formulate a new data mining problem called storytelling as a generalization of redescription mining. In traditional redescription mining, we are given a set of objects and a collection of subsets defined over these objects. The goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects. Storytelling, on the other hand, aims to explicitly relate object sets that are disjoint (and hence, maximally dissimilar) by finding a chain of (approximate) redescriptions between the sets. This problem finds applications in bioinformatics, for instance, where the biologist is trying to relate a set of genes expressed in one experiment to another set, implicated in a different pathway. We outline an efficient storytelling implementation that embeds the CARTwheels redescription mining algorithm in an A* search procedure, using the former to supply next move operators on search branches to the latter. This approach is practical and effective for mining large datasets and, at the same time, exploits the structure of partitions imposed by the given vocabulary. Three application case studies are presented: a study of word overlaps in large English dictionaries, exploring connections between genesets in a bioinformatics dataset, and relating publications in the PubMed index of abstracts.

#index 1081576
#* An Efficient Clustering Scheme to Exploit Hierarchical Data in Network Traffic Analysis
#@ Abdun Naser Mahmood;Christopher Leckie;Parampalli Udaya
#t 2008
#c 7
#! There is significant interest in the data mining and network management communities about the need to improve existing techniques for clustering multi-variate network traffic flow records so that we can quickly infer underlying traffic patterns. In this paper we investigate the use of clustering techniques to identify interesting traffic patterns from network traffic data in an efficient manner. We develop a framework to deal with mixed type attributes including numerical, categorical and hierarchical attributes for a one-pass hierarchical clustering algorithm. We demonstrate the improved accuracy and efficiency of our approach in comparison to previous work on clustering network traffic.

#index 1081577
#* A Signature-Based Indexing Method for Efficient Content-Based Retrieval of Relative Temporal Patterns
#@ Edi Winarko;John F. Roddick
#t 2008
#c 7
#! A number of algorithms have been proposed for the discovery of temporal patterns. However, since the number of generated patterns can be large, selecting which patterns to analyze can be non-trivial. There is thus a need for algorithms and tools that can assist in the selection of discovered patterns so that subsequent analysis can be performed in an efficient and, ideally, interactive manner. In this paper, we propose a signature-based indexing method, to optimise the storage and retrieval of a large collection of relative temporal patterns.

#index 1081578
#* Bounded Approximation: A New Criterion for Dimensionality Reduction Approximation in Similarity Search
#@ Khanh Vu;Kien A. Hua;Hao Cheng;Sheau-Dong Lang
#t 2008
#c 7
#! We examine the problem of efficient distance-based similarity search over high-dimensional data. A promising approach to this problem is to reduce dimensions and allow fast approximation. Conventional reduction approaches, however, entail a significant shortcoming: the approximation volume extends across the dataspace, which causes over-estimation of retrieval sets and impairs performance. This paper focuses on a new criterion for dimensionality reduction methods: bounded approximation. We show that this requirement can be accomplished by a novel non-linear transformation scheme that extracts two important parameters from the data. We devise two approximation formulations, rectangular and spherical range search, each corresponding to a closed volume around the original search sphere. We discuss in detail how to derive tight bounds for the parameters and to prove further results, as well as highlighting insights into the problems and our proposed solutions. To demonstrate the benefits of the new criterion, we study the effects of (un)boundedness on approximation performance, including selectivity, error toleration, and efficiency. Extensive experiments confirm the superiority of this technique over recent state-of-the-art schemes.

#index 1081579
#* Hardware-Enhanced Association Rule Mining with Hashing and Pipelining
#@ Ying-Hsiang Wen;Jen-Wei Huang;Ming-Syan Chen
#t 2008
#c 7
#! Generally speaking, to implement Apriori-based association rule mining in hardware, one has to load candidate itemsets and a database into the hardware. Too many candidate itemsets and a large database would create a performance bottleneck. In this paper, we propose a HAsh-based and PiPelIned architecture (abbreviated as HAPPI) for hardware-enhanced association rule mining. We apply the pipeline methodology in the HAPPI architecture to compare itemsets with the database and collect useful information for reducing the number of candidate itemsets and items in the database simultaneously. When the database is fed into the hardware, candidate itemsets are compared with the items in the database to find frequent itemsets. At the same time, trimming information is collected from each transaction. Therefore, we can effectively reduce the frequency of loading the database into the hardware. As such, HAPPI solves the bottleneck problem in Apriori-based hardware schemes. We also derive some properties to investigate the performance of this hardware implementation. As shown by the experiment results, HAPPI significantly outperforms the previous hardware approach in terms of execution cycles.

#index 1081580
#* Truth Discovery with Multiple Conflicting Information Providers on the Web
#@ Xiaoxin Yin;Jiawei Han;Philip S. Yu
#t 2008
#c 7
#! The world-wide web has become the most important information source for most of us. Unfortunately, there is no guarantee for the correctness of information on the web. Moreover, different web sites often provide conflicting information on a subject, such as different specifications for the same product. In this paper we propose a new problem called Veracity, i.e., conformity to truth, which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites. We design a general framework for the Veracity problem, and invent an algorithm called TruthFinder, which utilizes the relationships between web sites and their information, i.e., a web site is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy web sites. An iterative method is used to infer the trustworthiness of web sites and the correctness of information from each other. Our experiments show that TruthFinder successfully finds true facts among conflicting information, and identifies trustworthy web sites better than the popular search engines.

#index 1081581
#* Probabilistic Group Nearest Neighbor Queries in Uncertain Databases
#@ Xiang Lian;Lei Chen
#t 2008
#c 7
#! The importance of query processing over uncertain data has recently arisen due to its wide usage in many real-world applications. In the context of uncertain databases, previous work have studied many query types such as nearest neighbor query, range query, top-$k$ query, skyline query, and similarity join. In this paper, we focus on another important query, namely probabilistic group nearest neighbor query (PGNN), in the uncertain database, which also has many applications. Specifically, given a set, Q, of query points, a PGNN query retrieves data objects that minimize the aggregate distance (e.g. sum, min, and max) to query set Q. Due to the inherent uncertainty of data objects, previous techniques to answer group nearest neighbor query (GNN) cannot be directly applied to our PGNN problem. Motivated by this, we propose effective pruning methods, namely spatial pruning and probabilistic pruning, to reduce the PGNN search space, which can be seamlessly integrated into our PGNN query procedure. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approach, in terms of the wall clock time and the speed-up ratio against linear scan.

#index 1081582
#* Evolutionary Optimization of File Assignment for a Large-Scale Video-on-Demand System
#@ Jun Guo;Yi Wang;Kit-Sang Tang;Sammy Chan;Eric W. M. Wong;Peter Taylor;Moshe Zukerman
#t 2008
#c 7
#! We present a genetic algorithm to tackle a file assignment problem for a large scale video-on-demand system. The file assignment problem is to find the optimal replication and allocation of movie files to disks, so that the request blocking probability is minimized subject to capacity constraints. We adopt a divide-and-conquer strategy, where the entire solution space of file assignments is divided into subspaces. Each subspace is an exclusive set of solutions sharing a common file replication instance. This allows us to utilize a greedy file allocation method to find a sufficiently good quality heuristic solution within each subspace. Two performance indices are further designed to measure the quality of the heuristic solution on 1) its assignment of multi-copy movies and 2) its assignment of single-copy movies. We demonstrate that these techniques together with ad hoc population handling methods enable genetic algorithms to operate in a significantly reduced search space, and achieve good quality file assignments in a computationally efficient way.

#index 1112737
#* Continuous Clustering of Moving Objects
#@ Christian S. Jensen;Dan Lin;Beng Chin Ooi
#t 2007
#c 7
#! This paper considers the problem of efficiently maintaining a clustering of a dynamic set of data points that move continuously in two-dimensional Euclidean space. This problem has received little attention and introduces new challenges to clustering. The paper proposes a new scheme that is capable of incrementally clustering moving objects. This proposal employs a notion of object dissimilarity that considers object movement across a period of time, and it employs clustering features that can be maintained efficiently in incremental fashion. In the proposed scheme, a quality measure for incremental clusters is used for identifying clusters that are not compact enough after certain insertions and deletions. An extensive experimental study shows that the new scheme performs significantly faster than traditional ones that frequently rebuild clusters. The study also shows that the new scheme is effective in preserving the quality of moving-object clusters.

#index 1112738
#* Compressed Hierarchical Mining of Frequent Closed Patterns from Dense Data Sets
#@ Liping Ji;Kian-Lee Tan;Anthony Tung
#t 2007
#c 7
#! This paper addresses the problem of finding frequent closed patterns (FCPs) from very dense datasets. We introduce two compressed hierarchical FCP mining algorithms C-Miner and B-Miner. The two algorithms compress the original mining space, hierarchically partition the whole mining task into independent subtasks and mine each subtask progressively. The two algorithms adopt different task-partitioning strategies: CMiner partitions the mining task based on Compact Matrix Division whereas B-Miner partitions the task based on Base Rows Projection. The compressed hierarchical mining algorithms enhance the mining efficiency and facilitate a progressive refinement of results. Moreover, because the subtasks can be mined independently, C-Miner and B-Miner can be readily parallelized without incurring significant communication overhead. We have implemented C-Miner and B-Miner, and our performance study on synthetic datasets and real dense microarray datasets shows their effectiveness over existing schemes. We also report experimental results on parallel versions of these two methods.

#index 1112739
#* WhiteWater: Distributed Processing of Fast Streams
#@ Ioana Stanoi;George Mihaila;Themis Palpanas;Christian Lang
#t 2007
#c 7
#! Monitoring systems today often involve continuous queries over streaming data, in a distributed collaborative fashion. The distribution of query operators over a network of processors, and their processing sequence, form a query configuration with inherent constraints on the throughput it can support. In this paper we discuss the implications of measuring and optimizing for output throughput, and its limitations. We propose to use instead the more granular input throughput and a version of throughput measure, the profiled input throughput, that is focused on matching the expected behavior of the input streams. We show how to evaluate a query configuration based on profiled input throughput, and that the problem of finding the optimal configuration is NP-hard. Furthermore, we describe how to overcome the complexity limitation by adapting hill-climbing heuristics to reduce the search space of configurations. We show experimentally that the approach used is not only efficient but also effective.

#index 1112740
#* Using Punctuation Schemes to Characterize Strategies for Querying over Data Streams
#@ Peter Tucker;David Maier;Tim Sheard;Paul Stephens
#t 2007
#c 7
#! Many systems and strategies have been proposed for processing non-terminating data streams. Each approach has advantages and disadvantages, including the kinds of queries that can be executed. We present a framework for characterizing the kinds of queries that can be executed over streams based on a notion of compact sets from topology. We first apply our framework to queries over punctuated data streams. Previous work on punctuations focused primarily on the behavior of individual query operators. We use our framework to determine if an entire query can benefit from punctuations available from stream sources. We then consider other common strategies proposed in the literature for executing queries over streams, and we discuss how our framework can characterize the kinds of queries each strategy can answer.

#index 1112741
#* A Generalized Associative Petri Net for Reasoning
#@ Dong-Her Shih;Hsiu-Sen Chiang;Binshan Lin
#t 2007
#c 7
#! The application of fuzzy Petri Nets to the development of intelligent systems has received an increasing attention recently. However, the fuzzy production rules are rather limited if the determination of certainty factor for each proposition is subjective. Unfortunately, this is the case for many existing fuzzy Petri nets. This paper proposes a generalized associative Petri net model with associative degree and knowledge representation of a rule-based system. Based on the generalized associative Petri net model, an efficient reasoning algorithm is proposed. The ontology mapping and the associative reasoning algorithm are described formally in details. An example of malicious email reasoning is also included as an illustration.

#index 1112742
#* On the Optimal Robot Routing Problem in Wireless Sensor Networks
#@ Bo Yuan;Maria Orlowska;Shazia Sadiq
#t 2007
#c 7
#! Given a set of sparsely distributed sensors in the Euclidean plane, a mobile robot is required to visit all sensors to download the data and finally return to its base. The effective range of each sensor is specified by a disk and the robot must at least reach the boundary to start communication. The primary goal of optimization in this scenario is to minimize the traveling distance by the robot. This problem can be regarded as a special case of the Traveling Salesman Problem with Neighborhoods (TSPN), which is known to be NP-hard. In this paper, we present a novel TSPN algorithm for this class of TSPN, which can yield significantly improved results compared to the latest approximation algorithm.

#index 1112743
#* On the Customization of Components: A Rule-Based Approach
#@ Jia Zhou;Kendra Cooper;Hui Ma;I-Ling Yen
#t 2007
#c 7
#! Realizing the quality of service (QoS) requirements for a software system continues to be an important and challenging issue in software engineering. A software system may need to be updated, or reconfigured, to provide modified QoS capabilities. These changes can occur at development time or at run-time. In component based software engineering, software systems are built by composing components. When the QoS requirements change, there is a need to reconfigure the components. Unfortunately, many components are not designed to be reconfigurable, especially in terms of QoS capabilities. It is often labor-intensive and error prone work to reconfigure the components, as developers need to manually check and modify the source code. Furthermore, the work requires experienced, senior developers, which makes it costly. The limitations motivate the development of a new rule-based, semi-automated component parameterization technique that performs code analysis to identify and adapt parameters and changes components into reconfigurable ones. Compared with a number of alternative QoS adaptation approaches, the proposed rule-based technique has advantages in terms of flexibility, extensibility and efficiency. The adapted components support the reconfiguration of potential QoS trade-offs among time, space, quality, etc. The proposed rule-based technique has been successfully applied to two substantial libraries of components. The F-measure or balanced F-score results for the validation are excellent: 94%.

#index 1112744
#* Topic Signature Language Models for Ad hoc Retrieval
#@ Xiaohua Zhou;Xiaohua Hu;Xiaodan Zhang
#t 2007
#c 7
#! Semantic smoothing, which incorporates synonym and sense information into the language models, is effective and potentially significant to improve retrieval performance. The previously implemented semantic smoothing models, such as the translation model, have shown good experimental results. However, these models are unable to incorporate contextual information. To overcome this limitation, we propose a novel context-sensitive semantic smoothing method that decomposes a document into a set of weighted context-sensitive topic signatures and then translate those topic signatures into query terms. The language model with such a context-sensitive semantic smoothing is referred to as the topic signature language model. In detail, we implement two types of topic signatures depending on whether ontology exists in the application domain. One is the ontology-based concept and the other the multiword phrase. The translation probabilities from each topic signature to individual terms are estimated through the EM algorithm. Document models based on topic signature translation are then derived. The new smoothing method is evaluated on TREC 2004/2005 Genomics Track with ontology-based concepts, and TREC Ad hoc Track (Disk 1, 2 and 3) with multiword phrases. Both experiments show significant improvements over the two-stage language model as well as the language model with context-insensitive semantic smoothing.

#index 1112745
#* Discovering Frequent Generalized Episodes When Events Persist for Different Durations
#@ Srivatsan Laxman;P. Sastry;K. Unnikrishnan
#t 2007
#c 7
#! This paper is concerned with the framework of frequent episode discovery in event sequences. A new temporal pattern, called the generalized episode, is defined which extends this framework by incorporating event duration constraints explicitly into the pattern?s definition. This new formalism facilitates extension of the technique of episodes discovery to applications where data appears as a sequence of events that persist for different durations (rather than being instantaneous). We present efficient algorithms for episode discovery in this new framework. Through extensive simulations we show the expressive power of the new formalism. We also show how the duration constraint possibilities can be used as a design choice to properly focus the episode discovery process. Finally, we briefly discuss some interesting results obtained on data from manufacturing plants of General Motors .

#index 1112746
#* A Low-Granularity Classifier for Data Streams with Concept Drifts and Biased Class Distribution
#@ Peng Wang;Haixun Wang;Xiaochen Wu;Wei Wang;Baile Shi
#t 2007
#c 7
#! Many applications track streaming data for actionable alerts, which may include, for example, network intrusions, transaction frauds, biosurveilence abnormalities, etc. Some stream classification models are built for this purpose. Due to concept drifts, maintaining a model's up-to-dateness has become one of the most challenging tasks in mining data streams. State of the art approaches, including both the incrementally updated classifiers and the ensemble classifiers, have proved that model update is a very costly process. In this paper, we show that reducing model granularity reduces update cost, as models of fine granularity enable us to efficiently pinpoint local components in the model that are affected by the concept drift. It also enables us to derive new model components to reflect the current data distribution, thus avoiding expensive updates on a global scale. Furthermore, those actionable alerts being monitored are usually rare occurring. The existing stream classifiers cannot handle this problem. We address this problem and show that the low granularity classifier handles rare events on stream data with ease. Experiments on real and synthetic data show that our approach is able to maintain good prediction accuracy at a fraction of model updating cost of state of the art approaches.

#index 1112747
#* Engineering a Policy-Based System for Federated Healthcare Databases
#@ Rafae Bhatti;Arjmand Samuel;Mohamed Eltabakh;Haseeb Amjad;Arif Ghafoor
#t 2007
#c 7
#! Policy-based management for federated healthcare systems have recently gained increasing attention due to strict privacy and disclosure rules. While the work on privacy languages and enforcement mechanisms, such as Hippocratic databases, has advanced our understanding of designing privacy-preserving policies for healthcare databases, the need to integrate these policies in practical healthcare framework is becoming acute. Additionally, while most work in this area has been organization-oriented, dealing with exchange of information between healthcare organizations (such as referrals), the requirements for the emerging area of personal healthcare information management have so far not been adequately addressed. These shortcomings arise from the lack of a sophisticated policy specification language and enforcement architecture that can capture the requirement for (i) integration of privacy and disclosure policies with well-known healthcare standards used in the industry in order to specify the precise requirements of a practical healthcare system, and (ii) provision of ubiquitous healthcare services to patients using the same infrastructure that enables federated healthcare management for organizations. In this paper, we have designed a policy-based system to mitigate these concerns. One, we have designed our disclosure and privacy policies using a requirements specification based on a set of use cases for the Clinical Document Architecture (CDA) standard proposed by the community. Two, we present a context-aware policy specification language which allows encoding of CDA-based requirements use-cases into privacy and disclosure policy rules. We have shown that our policy specification language is effective in terms of handling a variety of expressive constraints on CDA-encoded document contents. Our language enables specification of privacy-aware access control for federated healthcare information across organizational boundaries, while the use of contextual constraints allows the incorporation of user and environment context in the access control mechanism for personal healthcare information management. Moreover, the declarative syntax of the policy rules makes the policy adaptable to changes in privacy regulations or patient preferences. We also present an enforcement architecture for the federated healthcare framework proposed in this paper.

#index 1113088
#* Moving E-Commerce with PIVOTS: Private Information Viewing Offering Total Safety
#@ Angelos Yannopoulos;Yiannis Stavroulas;Theodora A. Varvarigou
#t 2004
#c 7
#! We present a technology which enables wide-scale deployment of a unique and powerful data access and search tool. This tool makes it possible for content owners on the Internet to offer end-users the ability to search, assess, or otherwise examine their nondisclosable private data. The end-users may use arbitrary, distrusted assessment algorithms and receive the results which are produced. These results take the form of a meaningful digest, e.g., a useful statistic, a relevance assessment, etc. There exists some controllable danger of information theft which, however, is quantitatively dependent on system parameters and can be tuned accordingly. We consider a broad class of practical applications where brute force attacks exist which are easier and cheaper to implement than an attack through appropriately configured PIVOTS, making our system, effectively, very trustworthy. The actual creation of the search algorithm may be performed by the end-users themselves in specialized applications, or by third-party search-engine operators who offer their services to end-users. Sophisticated data mining technologies which are very often not directly available to content owners can thus be exploited to mine even private data collections. Users discovering interesting items in private data sets may purchase this specific data, but other desirable exploitation possibilities are also analyzed. A novel business model is presented and its potential for great impact on e-commerce is explored. Three diverse and important real-world applications are presented in conclusion.

#index 1113089
#* Performance Analysis of R*-Trees with Arbitrary Node Extents
#@ Yufei Tao;Dimitris Papadias
#t 2004
#c 7
#! Existing analysis for R-trees is inadequate for several traditional and emerging applications including, for example, temporal, spatio-temporal, and multimedia databases because it is based on the assumption that the extents of a node are identical on all dimensions, which is not satisfied in these domains. In this paper, we propose analytical models that can accurately predict R*-tree performance without this assumption. Our derivation is based on the novel concept of extent regression function, which computes the node extents as a function of the number of node splits. Detailed experimental evaluation reveals that the proposed models are accurate, even in cases where previous methods fail completely.

#index 1113090
#* Skyline Index for Time Series Data
#@ Quanzhong Li;Ines Fernando Vega Lopez;Bongki Moon
#t 2004
#c 7
#! We have developed a new indexing strategy that helps overcome the curse of dimensionality for time series data. Our proposed approach, called Skyline Index, adopts new Skyline Bounding Regions (SBR) to approximate and represent a group of time series data according to their collective shape. Skyline bounding regions allow us to define a distance function that tightly lower bounds the distance between a query and a group of time series data. In an extensive performance study, we investigate the impact of different distance functions by various dimensionality reduction and indexing techniques on the performance of similarity search, including index pages accessed, data objects fetched, and overall query processing time. In addition, we show that, for k{\hbox{-}}{\rm{nearest}} neighbor queries, the proposed Skyline index approach can be coupled with the state of the art dimensionality reduction techniques such as Adaptive Piecewise Constant Approximation (APCA) and improve its performance by up to a factor of 3.

#index 1113091
#* THESUS, a Closer View on Web Content Management Enhanced with Link Semantics
#@ Iraklis Varlamis;Michalis Vazirgiannis;Maria Halkidi;Benjamin Nguyen
#t 2004
#c 7
#! With the unstoppable growth of the World Wide Web, the great success of Web Search Engines, such as Google and Alta-Vista, users now turn to the Web whenever looking for information. However, many users are neophytes when it comes to computer science, yet they are often specialists of a certain domain. These users would like to add more semantics to guide their search through World Wide Web material, whereas currently most search features are based on raw lexical content. We show in this paper how the use of the incoming links of a page can be used efficiently to classify a page in a concise manner. This enhances the browsing and querying of Web pages. In this article, we focus on the tools needed in order to manage the links and their semantics. We further process these links using a hierarchy of concepts, akin to an ontology, and a thesaurus. This work is demonstrated by an prototype system, called THESUS, that organizes thematic Web documents into semantic clusters. Our contributions in this paper are the following: 1) a model and language to exploit link semantics information, 2) the THESUS prototype system, 3) its innovative aspects and algorithms, more specifically, the novel similarity measure between Web documents applied to different clustering schemes (DB-Scan and COBWEB), and 4) a thorough experimental evaluation proving the value of our approach.

#index 1113092
#* On Computing Mobile Agent Routes for Data Fusion in Distributed Sensor Networks
#@ Qishi Wu;Nageswara S. V. Rao;Jacob Barhen;S. Sitharama Iyengar;Vijay K. Vaishnavi;Hairong Qi;Krishnendu Chakrabarty
#t 2004
#c 7
#! The problem of computing a route for a mobile agent that incrementally fuses the data as it visits the nodes in a distributed sensor network is considered. The order of nodes visited along the route has a significant impact on the quality and cost of fused data, which, in turn, impacts the main objective of the sensor network, such as target classification or tracking. We present a simplified analytical model for a distributed sensor network and formulate the route computation problem in terms of maximizing an objective function, which is directly proportional to the received signal strength and inversely proportional to the path loss and energy consumption. We show this problem to be NP-complete and propose a genetic algorithm to compute an approximate solution by suitably employing a two-level encoding scheme and genetic operators tailored to the objective function. We present simulation results for networks with different node sizes and sensor distributions, which demonstrate the superior performance of our algorithm over two existing heuristics, namely, local closest first and global closest first methods.

#index 1113093
#* NeC4.5: Neural Ensemble Based C4.5
#@ Zhi-Hua Zhou;Yuan Jiang
#t 2004
#c 7
#! Decision tree is with good comprehensibility while neural network ensemble is with strong generalization ability. In this paper, these merits are integrated into a novel decision tree algorithm NeC4.5. This algorithm trains a neural network ensemble at first. Then, the trained ensemble is employed to generate a new training set through replacing the desired class labels of the original training examples with those output from the trained ensemble. Some extra training examples are also generated from the trained ensemble and added to the new training set. Finally, a C4.5 decision tree is grown from the new training set. Since its learning results are decision trees, the comprehensibility of NeC4.5 is better than that of neural network ensemble. Moreover, experiments show that the generalization ability of NeC4.5 decision trees can be better than that of C4.5 decision trees.

#index 1113094
#* Steganographic Schemes for File System and B-Tree
#@ HweeHwa Pang;Kian-Lee Tan;Xuan Zhou
#t 2004
#c 7
#! While user access control and encryption can protect valuable data from passive observers, these techniques leave visible ciphertexts that are likely to alert an active adversary to the existence of the data. This paper introduces StegFD, a steganographic file driver that securely hides user-selected files in a file system so that, without the corresponding access keys, an attacker would not be able to deduce their existence. Unlike other steganographic schemes proposed previously, our construction satisfies the prerequisites of a practical file system in ensuring the integrity of the files and maintaining efficient space utilization. We also propose two schemes for implementing steganographic B-trees within a StegFD volume. We have completed an implementation on Linux, and results of the experiment confirm that StegFD achieves an order of magnitude improvements in performance and/or space utilization over the existing schemes.

#index 1113095
#* Dimensionality Reduction and Similarity Computation by Inner-Product Approximations
#@ Omer Egecioglu;Hakan Ferhatosmanoglu;Umit Ogras
#t 2004
#c 7
#! As databases increasingly integrate different types of information such as multimedia, spatial, time-series, and scientific data, it becomes necessary to support efficient retrieval of multidimensional data. Both the dimensionality and the amount of data that needs to be processed are increasing rapidly. Reducing the dimension of the feature vectors to enhance the performance of the underlying technique is a popular solution to the infamous curse of dimensionality. We expect the techniques to have good quality of distance measures when the similarity distance between two feature vectors is approximated by some notion of distance between two lower-dimensional transformed vectors. Thus, it is desirable to develop techniques resulting in accurate approximations to the original similarity distance. In this paper, we investigate dimensionality reduction techniques that directly target minimizing the errors made in the approximations. In particular, we develop dynamic techniques for efficient and accurate approximation of similarity evaluations between high-dimensional vectors based on inner-product approximations. Inner-product, by itself, is used as a distance measure in a wide area of applications such as document databases. A first order approximation to the inner-product is obtained from the Cauchy-Schwarz inequality. We extend this idea to higher order power symmetric functions of the multidimensional points. We show how to compute fixed coefficients that work as universal weights based on the moments of the probability density function of the data set. We also develop a dynamic model to compute the universal coefficients for data sets whose distribution is not known. Our experiments on synthetic and real data sets show that the similarity between two objects in high-dimensional space can be accurately approximated by a significantly lower-dimensional representation.

#index 1113096
#* Constrained Cascade Generalization of Decision Trees
#@ Huimin Zhao;Sudha Ram
#t 2004
#c 7
#! While decision tree techniques have been widely used in classification applications, a shortcoming of many decision tree inducers is that they do not learn intermediate concepts, i.e., at each node, only one of the original features is involved in the branching decision. Combining other classification methods, which learn intermediate concepts, with decision tree inducers can produce more flexible decision boundaries that separate different classes, potentially improving classification accuracy. We propose a generic algorithm for cascade generalization of decision tree inducers with the maximum cascading depth as a parameter to constrain the degree of cascading. Cascading methods proposed in the past, i.e., loose coupling and tight coupling, are strictly special cases of this new algorithm. We have empirically evaluated the proposed algorithm using logistic regression and C4.5 as base inducers on 32 UCI data sets and found that neither loose coupling nor tight coupling is always the best cascading strategy and that the maximum cascading depth in the proposed algorithm can be tuned for better classification accuracy. We have also empirically compared the proposed algorithm and ensemble methods such as bagging and boosting and found that the proposed algorithm performs marginally better than bagging and boosting on the average.

#index 1113097
#* Building a Large and Efficient Hybrid Peer-to-Peer Internet Caching System
#@ Li Xiao;Xiaodong Zhang;Artur Andrzejak;Songqing Chen
#t 2004
#c 7
#! Proxy hit ratios tend to decrease as the demand and supply of Web contents are becoming more diverse. By case studies, we quantitatively confirm this trend and observe significant document duplications among a proxy and its client browsers' caches. One reason behind this trend is that the client/server Web caching model does not support direct resource sharing among clients, causing the Web contents and the network bandwidths among clients to be relatively underutilized. To address these limits and improve Web caching performance, we have extensively enhanced and deployed our browsers-aware framework, a peer-to-peer Web caching management scheme. We make the browsers and their proxy share the contents to exploit the neglected but rich data locality in browsers and reduce document duplications among the proxy and browsers' caches to effectively utilize the Web contents and network bandwidth among clients. The objective of our scheme is to improve the scalability of proxy-based caching both in the number of connected clients and in the diversity of Web documents. In this paper, we show that building such a caching system with considerations of sharing contents among clients, minimizing document duplications, and achieving data integrity and communication anonymity is not only feasible but also highly effective.

#index 1113098
#* Data Structure for Association Rule Mining: T-Trees and P-Trees
#@ Frans Coenen;Paul Leng;Shakil Ahmed
#t 2004
#c 7
#! Two new structures for Association Rule Mining (ARM), the T-tree, and the P-tree, together with associated algorithms, are described. The authors demonstrate that the structures and algorithms offer significant advantages in terms of storage and execution time.

#index 1113099
#* Time Series Classification Using Gaussian Mixture Models of Reconstructed Phase Spaces
#@ Richard J. Povinelli;Michael T. Johnson;Andrew C. Lindgren;Jinjin Ye
#t 2004
#c 7
#! A new signal classification approach is presented that is based upon modeling the dynamics of a system as they are captured in a reconstructed phase space. The modeling is done using full covariance Gaussian Mixture Models of time domain signatures, in contrast with current and previous work in signal classification that is typically focused on either linear systems analysis using frequency content or simple nonlinear machine learning models such as artificial neural networks. The proposed approach has strong theoretical foundations based on dynamical systems and topological theorems, resulting in a signal reconstruction, which is asymptotically guaranteed to be a complete representation of the underlying system, given properly chosen parameters. The algorithm automatically calculates these parameters to form appropriate reconstructed phase spaces, requiring only the number of mixtures, the signals, and their class labels as input. Three separate data sets are used for validation, including motor current simulations, electrocardiogram recordings, and speech waveforms. The results show that the proposed method is robust across these diverse domains, significantly outperforming the time delay neural network used as a baseline.

#index 1114487
#* Sensor-Based Abnormal Human-Activity Detection
#@ Jie Yin;Qiang Yang;Jeffrey Junfeng Pan
#t 2008
#c 7
#! With the availability of affordable sensors and sensor networks, sensor-based human activity recognition has attracted much attention in artificial intelligence and ubiquitous computing. In this paper, we present a novel two-phase approach for detecting abnormal activities based on wireless sensors attached to a human body. Detecting abnormal activities is a particular important task in security monitoring and healthcare applications of sensor networks, among many others. Traditional approaches to this problem suffer from a high false positive rate, particularly when the collected sensor data are biased towards normal data while the abnormal events are rare. Therefore, there is a lack of training data for many traditional data mining methods to be applied. To solve this problem, our approach first employs a one-class support vector machine (SVM) that is trained on commonly available normal activities, which filters out the activities that have a very high probability of being normal. We then derive abnormal activity models from a general normal model via a kernel nonlinear regression (KNLR) to reduce false positive rate in an unsupervised manner. We show that our approach provides a good tradeoff between abnormality detection rate and false alarm rate, and allows abnormal activity models to be automatically derived without the need to explicitly label the abnormal training data, which are scarce. We demonstrate the effectiveness of our approach using real data collected from a sensor network that is deployed in a realistic setting.

#index 1114488
#* Biometric Authentication for Border Control Applications
#@ Taekyoung Kwon;Hyeonjoon Moon
#t 2008
#c 7
#! For homeland and national security, the deployment of multi-modal biometrics is desirable with e-passports, but is hindered by privacy infringement and high cost implementation. We propose an authentication methodology which combines multi-modal biometrics and cryptographic mechanisms for border control applications. Specifically, we accommodate face and fingerprint without a mandatory requirement of (tamper-resistant) smart-card-level devices. We could imprint a (publicly readable) bar codes on the passport with regard to easier world-wide deployment. Additionally, we present a solution based on certification and key management method to control the validity of passports within the current PKI technology paradigm.

#index 1114489
#* A Thin-Plate Spline Calibration Model For Fingerprint Sensor Interoperability
#@ Arun Ross;Rohan Nadgir
#t 2008
#c 7
#! Biometric sensor interoperability refers to the ability of a system to compensate for the variability introduced in the biometric data of an individual due to the deployment of different sensors. Poor inter-sensor performance has been reported in different biometric domains including fingerprint, face, iris and speech. In the context of fingerprints, variations are observed in the acquired images due to differences in sensor resolution, scanning area, sensing technology, etc. which impact the feature set extracted from these images. The inability of a fingerprint matcher to compensate for these variations introduced in fingerprints acquired using different sensors results in inferior inter-sensor performance. In this work it is demonstrated that a simple non-linear calibration scheme, based on Thin Plate Splines (TPS), is sufficient to facilitate sensor interoperability in the context of fingerprints. In the proposed technique, the variation between the images acquired using two different sensors is modeled using non-linear distortions. The proposed calibration model is tested on the MSU dataset comprising of fingerprint images obtained using two different sensor technologies: an optical Digital Biometrics (DBI) sensor and a solid-state capacitive VERIDICOM (VERI) sensor. Experiments indicate that the proposed calibration scheme improves the inter-sensor Genuine Accept Rate (GAR) by ~35% to ~40% at a False Accept Rate (FAR) of 0.01%.

#index 1114490
#* Inference of Security Hazards from Event Composition Based on Incomplete or Uncertain Information
#@ Segev Wasserkrug;Avigdor Gal;Opher Etzion
#t 2008
#c 7
#! In many security-related contexts, a quick recognition of security hazards is required. Such recognition is challenging, since available information sources are often insufficient to infer the occurrence of hazards with certainty. This requires that the recognition of security hazard is carried out using inference based on patterns of occurrences distributed over space and time. The two main existing approaches to the inference of security hazards are a) custom-coded solutions, which are tailored to specific patterns, and cannot respond quickly to changes in the patterns of occurrences used for inference, and b) approaches based on direct statistical inferencing techniques, such as regression, which do not enable combining various kinds of evidence regarding the same hazard. In this work, we introduce a more generic formal framework which overcomes the aforementioned deficiencies, together with a case study illustrating the detection of DoS attacks.

#index 1114491
#* Contraflow Transportation Network Reconfiguration for Evacuation Route Planning
#@ Sangho Kim;Shashi Shekhar;Manki Min
#t 2008
#c 7
#! Given a transportation network having source nodes with evacuees and destination nodes, we want to find a contraflow network configuration, i.e., ideal direction for each edge, to minimize evacuation time. Contraflow is considered a potential remedy to reduce congestion during evacuations in the context of homeland security and natural disasters (e.g., hurricanes). This problem is computationally challenging because of the very large search space and the expensive calculation of evacuation time on a given network. To our knowledge, this paper presents the first macroscopic approaches for the solution of contraflow network reconfiguration incorporating road capacity constraints, multiple sources, congestion factor, and scalability. We formally define the contraflow problem based on graph theory and provide a framework of computational workload to classify our approaches. A Greedy heuristic is designed to produce high quality solutions with significant performance. A Bottleneck Relief heuristic is developed to deal with large numbers of evacuees. We evaluate the proposed approaches both analytically and experimentally using real world datasets. Experimental results show that our contraflow approaches can reduce evacuation time by 40% or more.

#index 1114492
#* On Bandwidth-Efficient Data Broadcast
#@ De-Nien Yang;Ming-Syan Chen
#t 2008
#c 7
#! In this paper, we leverage network coding to reduce the bandwidth consumption for data broadcast. We use the concept of mixing in a different way from the traditional network coding. Traditional network coding mixes all data items together. However, we mix each data item from only some data items according to the queried and stored data of receivers. Therefore, each receiver in our approach is required to receive fewer coded data to decode the required information, and the sender thereby is able to broadcast fewer data items. We formulate an optimization problem with integer-linear programming to minimize the bandwidth consumption in data broadcast. We prove that the problem is NP-hard and design an approximation algorithm that can be implemented in the data server. In addition, we show that different mixings of data items lead to different decoding costs for receivers. We design an algorithm to optimally code the data items with the minimum decoding cost.

#index 1114493
#* Ring Signature with Weak Linkability and Its Applications
#@ Ik Rae Jeong;Jeong Ok Kwon;Dong Hoon Lee
#t 2008
#c 7
#! We suggest a linkable ring signature scheme providing strong anonymity and weak linkability. We show that our linkable ring signature scheme can be used to construct a selectively linkable ring signature scheme, an efficient convertible (verifiable) ring signature scheme, and an efficient deductible ring signature scheme.

#index 1114494
#* Guest Editors' Introduction: Special Section on Intelligence and Security Informatics
#@ Daniel Dajun Zeng;Hsinchun Chen;Fei-Yue Wang;Hillol Kargupta
#t 2008
#c 7
#! The 12 papers in this special section focus on intelligence and security informatics. They are summarized here.

#index 1114495
#* Protection of Database Security via Collaborative Inference Detection
#@ Yu Chen;Wesley W. Chu
#t 2008
#c 7
#! Malicious users can exploit the correlation among data to infer sensitive information from a series of seemingly innocuous data accesses. Thus, we develop an inference violation detection system to protect sensitive data content. Based on data dependency, database schema and semantic knowledge, we con-structed a semantic inference model (SIM) that represents the possible inference channels from any at-tribute to the pre-assigned sensitive attributes. The SIM is then instantiated to a semantic inference graph (SIG) for query-time inference violation detection. For a single user case, when a user poses a query, the detection system will examine his/her past query log and calculate the probability of inferring sensitive information. The query request will be denied if the inference probability exceeds the pre-specified threshold. For multi-user cases, the users may share their query answers to increase the inference prob-ability. Therefore, we develop a model to evaluate collaborative inference based on the query sequences of collaborators and their task-sensitive collaboration levels. Experimental studies reveal that information authoritativeness, communication fidelity and honesty in collaboration are three key factors that affect the level of achievable collaboration. An example is given to illustrate the use of the proposed technique to prevent multiple collaborative users from deriving sensitive information via inference.

#index 1114496
#* Privacy Protection Against Malicious Adversaries in Distributed Information Sharing Systems
#@ Nan Zhang;Wei Zhao
#t 2008
#c 7
#! We address issues related to sharing information in a distributed system consisting of autonomous entities, each of which holds a private database. We consider threats from malicious adversaries that can deviate from the designated protocol and change their input databases. We classify malicious adversaries into two widely existing subclasses, namely weakly and strongly malicious adversaries, and propose protocols that can effectively and efficiently protect privacy against malicious adversaries.

#index 1114497
#* Efficient Remote Data Possession Checking in Critical Information Infrastructures
#@ Francesc Sebé;Josep Domingo-Ferrer;Antoni Martinez-Balleste;Yves Deswarte;Jean-Jacques Quisquater
#t 2008
#c 7
#! Checking data possession in networked information systems such as those related to critical infrastructures (power facilities, airports, data vaults, defense systems, etc.) is a matter of crucial importance. Remote data possession checking protocols permit to check that a remote server can access an uncorrupted file in such a way that the verifier does not need to know beforehand the entire file that is being verified. Unfortunately, current protocols only allow a limited number of successive verifications or are impractical from the computational point of view. In this paper, we present a new remote data possession checking protocol such that: i) it allows an unlimited number of file integrity verifications; ii) its maximum running time can be chosen at set-up time and traded off against storage at the verifier.

#index 1114498
#* Discovering and Explaining Abnormal Nodes in Semantic Graphs
#@ Shou-de Lin;Hans Chalupsky
#t 2008
#c 7
#! An important problem in the area of homeland security is to identify suspicious entities in large datasets. Although there are methods from knowledge discovery and data mining (KDD) focusing on finding anomalies in numerical datasets, there has been little work aimed at discovering suspicious instances in large and complex semantic graphs whose nodes are richly connected with many different types of links. In this paper, we describe a novel, domain independent and unsupervised framework to identify such instances. Besides discovering suspicious instances, we believe that to complete the process, a system has to convince the users by providing understandable explanations for its findings. Therefore, in the second part of the paper we describe several explanation mechanisms to automatically generate human understandable explanations for the discovered results. To evaluate our discovery and explanation systems, we perform experiments on several different semantic graphs. The results show that our discovery system outperforms the state-of-the-art unsupervised network algorithms used to analyze the 9/11 terrorist network by a large margin. Additionally, the human study we conducted demonstrates that our explanation system, which provides natural language explanations for its findings, allowed human subjects to perform complex data analysis in a much more efficient and accurate manner.

#index 1114499
#* Mining Impact-Targeted Activity Patterns in Imbalanced Data
#@ Longbing Cao;Yanchang Zhao;Chengqi Zhang
#t 2008
#c 7
#! Impact-targeted activities are rare but lead to significant impact on the society, e.g., isolated terrorism activities may lead to a disastrous event threatening national security. Similar issues can also be seen in many other areas. Therefore, it is important to identify such particular activities before they lead to significant impact to the world. However, it is challenging to mine impact-targeted activity patterns due to its imbalanced structure. This paper develops techniques for discovering such activity patterns. First, the complexities of mining imbalanced impact-targeted activities are analyzed.We then discuss strategies for constructing impact-targeted activity sequences. Algorithms are developed to mine frequent positive-impact (P → T) and negative-impact (P → $(\bar{T})$) oriented activity patterns, sequential impact-contrasted activity patterns (P is frequently associated with both pattern P → T and P → $(\bar{T})$) in separated data sets), and sequential impact-reversed activity patterns (both P → T and PQ → $(\bar{T})$) are frequent). Activity impact modelling is also studied to quantify pattern impact on business outcomes. Social security debt-related activity data is used to test the proposed approaches. The outcomes show that they are promising for ISI applications to identify impact-targeted activity patterns in imbalanced data.

#index 1114500
#* Detecting Word Substitutions in Text
#@ SW. Fong;D. Roussinov;D. B. Skillicorn
#t 2008
#c 7
#! Searching for words on a watchlist is one way in which large-scale surveillance of communication can be done, for example in intelligence and counterterrorism settings. One obvious defense is to replace words that might attract attention to a message with other, more innocuous, words. For example, the sentence the attack will be tomorrow" might be altered to the complex will be tomorrow", since 'complex'is a word whose frequency is close to that of 'attack'. Such substitutions are readily detectable by humans since they do not make sense. We address the problem of detecting such substitutions automatically, by looking for discrepancies between words and their contexts, and using only syntactic information. We define a set of measures, each of which is quite weak, but which together produce per-sentence detection rates around 90% with false positive rates around 10%. Rules for combining persentence detection into per-message detection can reduce the false positive and false negative rates for messages to practical levels. We test the approach using sentences from the Enron email and Brown corpora, representing informal and formal text respectively.

#index 1114501
#* A Statistical Language Modeling Approach to Online Deception Detection
#@ Lina Zhou;Yongmei Shi;Dongsong Zhang
#t 2008
#c 7
#! Online deception is disrupting our daily life, organizational process, and even national security. Existing approaches to online deception detection follow a traditional paradigm by using a set of cues as antecedents for deception detection, which may be hindered by ineffective cue identification. Motivated by the strength of statistical language models (SLMs) in capturing the dependency of words in text without explicit feature extraction, we developed SLMs to detect online deception. We also addressed the data sparsity problem in building SLMs in general and in deception detection in specific using smoothing and vocabulary pruning techniques. The developed SLMs were evaluated empirically with diverse datasets. The results showed that the proposed SLM approach to deception detection outperformed a state-of-the-art text categorization method as well as traditional feature-based methods.

#index 1117697
#* Competitor Mining with the Web
#@ Shenghua Bao;Rui Li;Yong Yu;Yunbo Cao
#t 2008
#c 7
#! This paper is concerned with the problem of mining competitors from the web automatically. Nowadays the fierce competition in the market necessitates every company not only to know which companies are its primary competitors, but also in which fields the company's rivals compete with itself and what its competitors' strength is in a specific competitive domain. The task of competitor mining that we address in the paper includes mining all the information such as competitors, competing fields and competitors' strength. A novel algorithm called CoMiner is proposed, which tries to conduct a web-scale mining in a domain-independent manner. The CoMiner algorithm consists of three parts: 1) given an input entity, extracting a set of comparative candidates and then ranking them according to comparability; 2) extracting the fields in which the given entity and its competitors play against each other; 3) identifying and summarizing the competitive evidence that details the competitors' strength. As for evaluation, a prototype system implementing the CoMiner algorithm is presented. An evaluation data set consisting of 70 entities is constructed. 728 competitors and 3,640 competitive fields with 6,381 competitive evidences are discovered with the prototype. The experimental results show that the proposed algorithm is highly effective.

#index 1117698
#* Kernel Uncorrelated and Regularized Discriminant Analysis: A Theoretical and Computational Study
#@ Shuiwang Ji;Jieping Ye
#t 2008
#c 7
#! Linear and kernel discriminant analysis are popular approaches for supervised dimensionality reduction. Uncorrelated and regularized discriminant analysis have been proposed to overcome the singularity problem encountered by classical discriminant analysis. In this paper, we study the properties of kernel uncorrelated and regularized discriminant analysis, called KUDA and KRDA, respectively. In particular, we show that under a mild condition, both linear and kernel uncorrelated discriminant analysis project samples in the same class to a common vector in the dimensionality-reduced space. This implies that uncorrelated discriminant analysis may suffer from the overfitting problem if there are a large number of samples in each class. We show that as the regularization parameter in KRDA tends to zero, KRDA approaches KUDA. This shows that KUDA is a special case of KRDA, and that regularization can be applied to overcome the overfitting problem in uncorrelated discriminant analysis. As the performance of KRDA depends on the value of the regularization parameter, we show that the matrix computations involved in KRDA can be simplified, so that a large number of candidate values can be crossvalidated efficiently. Finally, we conduct experiments to evaluate the proposed theories and algorithms.

#index 1117699
#* Mixed-Drove Spatiotemporal Co-Occurrence Pattern Mining
#@ Mete Celik;Shashi Shekhar;James P. Rogers;James A. Shine
#t 2008
#c 7
#! Mixed-drove spatio-temporal co-occurrence patterns (MDCOPs) represent subsets of two or more different object-types whose instances are often located in spatial and temporal proximity. Discovering MDCOPs is an important problem with many applications such as identifying tactics in battlefields, games, and predator-prey interactions. However, mining MDCOPs is computationally very expensive because the interest measures are computationally complex, datasets are larger due to the archival history, and the set of candidate patterns is exponential in the number of object-types. We propose a monotonic composite interest measure for discovering MDCOPs and novel MDCOP mining algorithms. Analytical results show that the proposed algorithms are correct and complete. Experimental results also show that the proposed methods are computationally more efficient than naive alternatives.

#index 1117700
#* Rotational Linear Discriminant Analysis Technique for Dimensionality Reduction
#@ Alok Sharma;Kuldip K. Paliwal
#t 2008
#c 7
#! The linear discriminant analysis (LDA) technique is very popular in pattern recognition for dimensionality reduction. It is a supervised learning technique that finds a linear transformation such that the overlapping between the classes is minimum for the projected feature vectors in the reduced feature space. This overlapping, if present, adversely affects the classification performance. In this paper, we introduce prior to dimensionality-reduction transformation an additional rotational transform that rotates the feature vectors in the original feature space around their respective class centroids in such a way that the overlapping between the classes in the reduced feature space is further minimized. As a result, the classification performance significantly improves which is demonstrated using several data corpuses.

#index 1117701
#* The Discrete Basis Problem
#@ Pauli Miettinen;Taneli Mielikäinen;Aristides Gionis;Gautam Das;Heikki Mannila
#t 2008
#c 7
#! Matrix decomposition methods represent a data matrix as a product of two factor matrices: one containing basis vectors that represent meaningful concepts in the data, and another describing how the observed data can be expressed as combinations of the basis vectors. Decomposition methods have been studied extensively, but many methods return real-valued matrices. Interpreting real-valued factor matrices is hard if the original data is Boolean. In this paper, we describe a matrix decomposition formulation for Boolean data, the Discrete Basis Problem. The problem seeks for a Boolean decomposition of a binary matrix, thus allowing the user to easily interpret the basis vectors. We also describe a variation of the problem, the Discrete Basis Partitioning Problem. We show that both problems are NP-hard. For the Discrete Basis Problem, we give a simple greedy algorithm for solving it; for the Discrete Basis Partitioning Problem we show how it can be solved using existing methods. We present experimental results for the greedy algorithm and compare it against other, well known methods. Our algorithm gives intuitive basis vectors, but its reconstruction error is usually larger than with the real-valued methods. We discuss about the reasons for this behavior.

#index 1117702
#* DiSC: Benchmarking Secure Chip DBMS
#@ Nicolas Anciaux;Luc Bouganim;Philippe Pucheral;Patrick Valduriez
#t 2008
#c 7
#! Secure chips, e.g. present in smart cards, USB dongles, i-buttons, are now ubiquitous in applications with strong security requirements. And they require embedded data management techniques. However, secure chips have severe hardware constraints which make traditional database techniques irrelevant. The main problem faced by secure chip DBMS designers is to be able to assess various design choices and trade-offs for different applications. Our solution is to use a benchmark for secure chip DBMS in order to (1) compare different database techniques, (2) predict the limits of on-chip applications, and (3) provide co-design hints. In this paper, we propose DiSC (Data management in Secure Chip), a benchmark which matches these three objectives. This work benefits from our long experience in developing and tuning data management techniques for the smart card. To validate DiSC, we compare the behavior of candidate data management techniques thanks to a cycle-accurate smart card simulator. Finally, we show the applicability of DiSC to future designs involving new hardware platforms and new database techniques.

#index 1117703
#* Random Walks to Identify Anomalous Free-Form Spatial Scan Windows
#@ Vandana P. Janeja;Vijayalakshmi Atluri
#t 2008
#c 7
#! Often, it is required to identify anomalous windows reflecting unusual rate of occurrence of a specific event of interest. Spatial scan statistic approach moves scan window over the region and computes the statistic of a parameter(s) of interest, and identifies anomalous windows. While this approach has been successfully employed, earlier proposals suffer from two limitations: (i) In general, the scan window is regular shaped (e.g., circle, rectangle) identifying anomalous windows of fixed shapes only. However, the region of anomaly is not necessarily regular shaped. Recent proposals to identify windows of irregular shapes identify windows larger than the true anomalies, or penalize large windows. (ii) These techniques account for autocorrelation among spatial data, but not spatial heterogeneity often resulting in inaccurate anomalous windows. We propose a random walk based Free-Form Spatial Scan Statistic (FS3). We construct a Weighted Delaunay Nearest Neighbor graph (WDNN) to capture spatial autocorrelation and heterogeneity. Using random walks we identify natural free-form scan windows, not restricted to a predefined shape and prove that they are not random. FS3 on real datasets has shown that it identifies more refined anomalous windows with better likelihood ratio of it being an anomaly as compared to earlier spatial scan statistic approaches.

#index 1117704
#* Schema Matching Using Interattribute Dependencies
#@ Jaewoo Kang;Jeffrey F. Naughton
#t 2008
#c 7
#! Schema matching is one of the key challenges in information integration. It is a labor-intensive and time-consuming process. To alleviate the problem, many automated solutions have been proposed. Most of the existing solutions mainly rely upon textual similarity of the data to be matched. However, there exist instances of the schema matching problem for which they do not even apply. Such problem instances typically arise when the column names in the schemas and the data in the columns are opaque or very difficult to interpret. In our previous work [36] we proposed a two-step technique to address this problem. In the first step, we measure the dependencies between attributes within tables using an information-theoretic measure and construct a dependency graph for each table capturing the dependencies among attributes. In the second step, we find matching node pairs across the dependency graphs by running a graph matching algorithm. In our previous work, we experimentally validated the accuracy of the approach. One remaining challenge is the computational complexity of the graph matching problem in the second step. In this paper we extend the previous work by improving the second phase of the algorithm incorporating efficient approximation algorithms into the framework.

#index 1117705
#* Toward Managing Uncertain Spatial Information for Situational Awareness Applications
#@ Yiming Ma;Dmitri V. Kalashnikov;Sharad Mehrotra
#t 2008
#c 7
#! Situational awareness (SA) applications monitor the real world and the entities therein to support tasks such as rapid decision-making, reasoning, and analysis. Raw input about unfolding events may arrive from variety of sources in the form of sensor data, video streams, human observations, and so on, from which events of interest are extracted. Location is one of the most important attributes of events, useful for a variety of SA tasks. In this article, we consider the problem of reaching situation awareness from textual input. We propose an approach to probabilistically model and represent (potentially uncertain) event locations described by human reporters in the form of free text. We analyze several types of spatial queries of interest in SA applications. We design techniques to store and index the models, to support the efficient processing of queries. Our extensive experimental evaluation over real and synthetic datasets demonstrates the effectiveness and efficiency of our approaches.

#index 1117706
#* Knowledge Reuse Enhancement with Motional Visual Representation
#@ Jiang-Liang Hou;Alice W. -J. Tsai
#t 2008
#c 7
#! The growing complexity of information and documents has made it difficult for knowledge receivers to understand digital contents, therefore, multiple knowledge representation schemes are required for enterprise knowledge services. Traditional schemes for explicit knowledge representation within enterprise and academic circles are primarily text-oriented and thus, a great deal of time and effort are required for knowledge receivers to understand the contents, especially for motion knowledge. In order to enhance knowledge reuse with motion knowledge extraction, representation, and visualization, this research focuses on the development of a motion knowledge representation and visualization (MKRV) model for Chinese documents with three modules, namely the automatic thesaurus definition (ATD) module, the target sentence extraction and formatting (TSEF) module, and the motion knowledge visualization (MKV) module. Moreover, based on the proposed model, a Motion Knowledge Representation and Management System (MKRMS) is established. A real world case of computer assembly is also applied in order to verify the feasibility of the proposed model. The verification results show that the system could achieve a high performance level with a small amount of training data.

#index 1117732
#* Affect Analysis of Web Forums and Blogs Using Correlation Ensembles
#@ Ahmed Abbasi;Hsinchun Chen;Sven Thoms;Tianjun Fu
#t 2008
#c 7
#! Analysis of affective intensities in computer mediated communication is important in order to allow a better understanding of online users' emotions and preferences. Despite considerable research on textual affect classification, it is unclear which features and techniques are most effective. In this study we compared several feature representations for affect analysis, including learned n-grams and various automatically and manually crafted affect lexicons. We also proposed the support vector regression correlation ensemble (SVRCE) method for enhanced classification of affect intensities. SVRCE uses an ensemble of classifiers each trained using a feature subset tailored towards classifying a single affect class. The ensemble is combined with affect correlation information to enable better prediction of emotive intensities. Experiments were conducted on four test beds encompassing web forums, blogs, and online stories. The results revealed that learned n-grams were more effective than lexicon based affect representations. The findings also indicated that SVRCE outperformed comparison techniques, including Pace regression, semantic orientation, and WordNet models. Ablation testing showed that the improved performance of SVRCE was attributable to its use of feature ensembles as well as affect correlation information. A brief case study was conducted to illustrate the utility of the features and techniques for affect analysis of large archives of online discourse.

#index 1117733
#* Anonymization by Local Recoding in Data with Attribute Hierarchical Taxonomies
#@ Jiuyong Li;Raymond Chi-Wing Wong;Ada Wai-Chee Fu;Jian Pei
#t 2008
#c 7
#! Individual privacy will be at risk if a published data set is not properly de-identified. k-anonymity is a major technique to de-identify a data set. Among a number of k-anonymisation schemes, local recoding methods are promising for minimising the distortion of a k-anonymity view. This paper addresses two major issues in local recoding k-anonymisation in attribute hierarchical taxonomies. Firstly, we define a proper distance metric to achieve local recoding generalisation with small distortion. Secondly, we propose a means to control the inconsistency of attribute domains in a generalised view by local recoding. We show experimentally that our proposed local recoding method based on the proposed distance metric produces higher quality k-anonymity tables in three quality measures than a global recoding anonymisation method, Incognito, and a multidimensional recoding anonymisation method, Multi. The proposed inconsistency handling method is able to balance distortion and consistency of a generalised view.

#index 1117734
#* Automatic Website Summarization by Image Content: A Case Study with Logo and Trademark Images
#@ Evdoxios Baratis;Euripides G. M. Petrakis;Evangelos E. Milios
#t 2008
#c 7
#! Image-based abstraction (or summarization) of a Web site is the process of extracting the most characteristic (or important) images from it. The criteria for measuring the importance of images in Web sites are based on their frequency of occurrence, characteristics of their content and Web link information. As a case study, this work focuses on logo and trademark images. These are important characteristic signs of corporate Web sites or of products presented there. The proposed method incorporates machine learning for distinguishing logo and trademarks from images of other categories (e.g., landscapes, faces). Because the same logo or trademark may appear many times in various forms within the same Web site, duplicates are detected and only unique logo and trademark images are extracted. These images are then ranked by importance taking frequency of occurrence, image content and Web link information into account. The most important logos and trademarks are finally selected to form the image-based summary of a Web site. Evaluation results of the method on real Web sites are also presented. The method has been implemented and integrated into a fully automated image-based summarization system which is accessible on the Web (www.intelligence.tuc.gr/websummarization)

#index 1117735
#* Continuous k-Means Monitoring over Moving Objects
#@ Zhenjie Zhang;Yin Yang;Anthony K. H. Tung;Dimitris Papadias
#t 2008
#c 7
#! Given a dataset P, a k-means query returns k points in space (called centers), such that the average squared distance between each point in P and its nearest center is minimized. Since this problem is NP-hard, several approximate algorithms have been proposed and used in practice. In this paper, we study continuous k-means computation at a server that monitors a set of moving objects. Re-evaluating k-means every time there is an object update imposes a heavy burden on the server (for computing the centers from scratch) and the clients (for continuously sending location updates). We overcome these problems with a novel approach that significantly reduces the computation and communication costs, while guaranteeing that the quality of the solution, with respect to the re-evaluation approach, is bounded by a user-defined tolerance. The proposed method assigns each moving object a threshold (i.e., range) such that the object sends a location update only when it crosses the range boundary. First, we develop an efficient technique for maintaining the k-means. Then, we present mathematical formulae and algorithms for deriving the individual thresholds. Finally, we justify our performance claims with extensive experiments.

#index 1117736
#* Efficient Phrase-Based Document Similarity for Clustering
#@ Hung Chim;Xiaotie Deng
#t 2008
#c 7
#! In this paper, we propose a phrase-based document similarity to compute the pair-wise similarities of documents based on the Suffix Tree Document (STD) model. By mapping each node in the suffix tree of STD model into a unique feature term in the Vector Space Document (VSD) model, the phrase-based document similarity naturally inherits the term tf-idf weighting scheme in computing the document similarity with phrases. We apply the phrase-based document similarity to the group-average Hierarchical Agglomerative Clustering (HAC) algorithm and develop a new document clustering approach. Our evaluation experiments indicate that, the new clustering approach is very effective on clustering the documents of two standard document benchmark corpora OHSUMED and RCV1. The quality of the clustering results significantly surpass the results of traditional single-word \textit{tf-idf} similarity measure in the same HAC algorithm, especially in large document data sets. Furthermore, by studying the property of STD model, we conclude that the feature vector of phrase terms in the STD model can be considered as an expanded feature vector of the traditional single-word terms in the VSD model. This conclusion sufficiently explains why the phrase-based document similarity works much better than the single-word tf-idf similarity measure.

#index 1117737
#* IDD: A Supervised Interval Distance-Based Method for Discretization
#@ Francisco J. Ruiz;Cecilio Angulo;Núria Agell
#t 2008
#c 7
#! This article introduces a new method for supervised discretization based on interval distances by using a novel concept of neighbourhood in the target's space. The method proposed takes into consideration the order of the class attribute, when this exists, so that it can be used with ordinal discrete classes as well as continuous classes, in the case of regression problems. The method has proved to be very efficient in terms of accuracy and faster than the most commonly supervised discretization methods used in the literature. It is illustrated through several examples and a comparison with other standard discretization methods is performed for three public data sets by using two different learning tasks: a decision tree algorithm and SVM for regression.

#index 1117738
#* On Combining Neuro-Fuzzy Architectures with the Rough Set Theory to Solve Classification Problems with Incomplete Data
#@ Robert Nowicki
#t 2008
#c 7
#! This paper presents a new approach to fuzzy classification in the case of missing features. The rough set theory is incorporated into neuro-fuzzy structures and the rough-neurofuzzy classifier is derived. The architecture of the classifier is determined by the MICOG (modified indexed center of gravity) defuzzification method. The structure of the classifier is presented in a general form which includes both the Mamdani approach and the logical approach - based on the genuine fuzzy aplications. A theorem, which allows to determine the structures of a roughneuro-fuzzy classifiers based on the MICOG defuzzification, is given and proven. Specific rough-neuro-fuzzy structures based on the Larsen rule, the Reichenbach and the Kleene-Dienes implications are given in details. In the experiments it is shown that the classifier with the Dubois-Prade fuzzy implication is characterized by the best performance in the case of missing features

#index 1117739
#* Recommendation Method for Improving Customer Lifetime Value
#@ Tomoharu Iwata;Kazumi Saito;Takeshi Yamada
#t 2008
#c 7
#! It is important for online stores to improve Customer Lifetime Value (LTV) if they are to increase their profits. Conventional recommendation methods suggest items that best coincide with user's interests to maximize the purchase probability, and this does not necessarily help to improve LTV. We present a novel recommendation method that maximizes the probability of the LTV being improved, which can apply to both of measured and subscription services. Our method finds frequent purchase patterns among high LTV users, and recommends items for a new user that simulate the found patterns. Using survival analysis techniques, we efficiently extract information from log data to find the patterns. Furthermore, we infer a user's interests from purchase histories based on maximum entropy models, and use these interests to improve the recommendations. Since a higher LTV is the result of greater user satisfaction, our method benefits users as well as online stores. We evaluate our method using two sets of real log data for measured and subscription services.

#index 1117740
#* Text Document Preprocessing with the Bayes Formula for Classification Using the Support Vector Machine
#@ Dino Isa;Lam H. Lee;V. P. Kallimani;R. RajKumar
#t 2008
#c 7
#! This work implements an enhanced hybrid classification method through the utilization of the naïve Bayes classifier and the Support Vector Machine (SVM). In this project, the Bayes formula was used to vectorize (as opposed to classify) a document according to a probability distribution reflecting the probable categories that the document may belong to. The Bayes formula gives a range of probabilities to which the document can be assigned according to a pre determined set of topics such as those found in the "20 newsgroups" dataset for instance. Using this probability distribution as the vectors to represent the document, the SVM can then be used to classify the documents on a multi - dimensional level. The effects of an inadvertent dimensionality reduction caused by classifying using only the highest probability using the naïve Bayes classifier can be overcome using the SVM by employing all the probability values associated with every category for each document. This method can be used for any dataset and shows a significant reduction in training time as compared to the LSquare method and significant improvement in classification accuracy when compared to pure naïve Bayes systems and also the TF-IDF/SVM hybrids.

#index 1117741
#* Cooperative Media Data Streaming with Scalable Video Coding
#@ Hui Guo;K. -T. Lo
#t 2008
#c 7
#! Live peer-to-peer (P2P) streaming has become a promising approach for broadcasting non-interactive media content from a server to a large number of interested clients. However, it still faces many challenges such as high churn rate of peer clients, uplink bandwidth constraints of participating peers, and heterogeneity of client throuput capacities. This paper presents a new P2P network called LSONet, a collaborative peer-to-peer streaming framework for scalable layer-encoded bit streams. The contributions are the combination of the advantages of both layered conding and mesh-based packet exchange. With layered coding, it overcomes overlay bandwidth limitatioins and heterogeneity of client capacities. With mesh based overlay streaming, it can better handle peer churns, as compared to tree-based solutions. For achieving these targets, this paper employs a gossip-based data-driven scheme for partnership formation, and proposes two algorithms, optimized transmission policy (OTP) and graceful degradation scheme (GDS), for multi-layers allocation. The proposed system is completely self-organizing, and in a fully distributed fashion. Extensive simulations show that LSONet achieves higher quality of service by peer-assisted streaming and layered video coding. Also, through comparison, results show that the system outperforms some previous schemes in resource utilization and is more robust and resilient for nodes departure, which demonstrate that it is well-suited for quality adaptive live streaming applications.

#index 1117742
#* GossipTrust for Fast Reputation Aggregation in Peer-to-Peer Networks
#@ Runfang Zhou;Kai Hwang;Min Cai
#t 2008
#c 7
#! Abstract In peer-to-peer (P2P) networks, reputation aggregation and ranking are the most time-consuming and space-demanding operations. This paper proposes a new gossip protocol for fast score aggregation. We developed a Bloom filter architecture for efficient score ranking. These techniques do not require any secure hashing or fast lookup mechanism, thus are applicable to both unstructured and structured P2P networks. We report the design principles and performance results of a simulated GossipTrust reputation system. Randomized gossiping with effective use of power nodes enables light-weight aggregation and fast dissemination of global scores in O(log2 n) time steps, where n is the P2P network size. The Gossip-based protocol is designed to tolerate dynamic peer joining and departure, as well as to avoid possible peer collusions. The scheme has a considerably low gossiping message overhead, i.e. O(nlog2 n) messages for n nodes. Bloom filters demand at most 512 KB memory per node for a 10,000-node network. We evaluate the performance of GossipTrust with distributed P2P file-sharing and parameter-sweeping applications. The simulation results demonstrate that GossipTrust has small aggregation time, low memory demand, and high ranking accuracy. These results suggest promising advantages of using the GossipTrust system for trusted P2P applications.

#index 1117743
#* A General Model for Sequential Pattern Mining with a Progressive Database
#@ Jen-Wei Huang;Chi-Yao Tseng;Jian-Chih Ou;Ming-Syan Chen
#t 2008
#c 7
#! To capture the dynamic nature of data addition and deletion, we propose a general model of sequential pattern mining with a progressive database while the data in the database may be static, inserted or deleted. In addition, we present a progressive algorithm Pisa, standing for Progressive mIning of Sequential pAtterns, to progressively discover sequential patterns in defined time period of interest. The period of interest is a sliding window continuously advancing as the time goes by. Pisa utilizes a progressive sequential tree to efficiently maintain the latest data sequences, discover the complete set of up-to-date sequential patterns, and delete obsolete data and patterns accordingly. The height of the sequential pattern tree proposed is bounded by the length of period of interest, thereby effectively limiting the memory space required by Pisa that is significantly smaller than the memory needed by alternative methods. Note that the sequential pattern mining with a static database and with an incremental database are special cases of the progressive sequential pattern mining. By changing Start time and End time of the period of interest, Pisa can easily deal with a static database or an incremental database as well. Complexity of algorithms proposed is analyzed.

#index 1119125
#* A Point Symmetry-Based Clustering Technique for Automatic Evolution of Clusters
#@ Sanghamitra Bandyopadhyay;Sriparna Saha
#t 2008
#c 7
#! In this article, a new symmetry based genetic clustering algorithm is proposed which automatically evolves the number of clusters as well as the proper partitioning from a data set. Strings comprise both real numbers and the don't care symbol in order to encode a variable number of clusters. Here, assignment of points to different clusters are done based on a point symmetry based distance rather than the Euclidean distance. A newly proposed point symmetry based cluster validity index, {\em Sym}-index, is used as a measure of the validity of the corresponding partitioning. The algorithm is therefore able to detect both convex and non-convex clusters irrespective of their sizes and shapes as long as they possess the point symmetry property. Kd-tree based nearest neighbor search is used to reduce the complexity of computing point symmetry based distance. A proof on the convergence property of variable string length GA with point symmetry based distance clustering (VGAPS-clustering) technique is also provided. The effectiveness of VGAPS-clustering compared to variable string length Genetic K-means algorithm (GCUK-clustering) and one recently developed weighted sum validity function based hybrid niching genetic algorithm (HNGA-clustering) is demonstrated for nine artificial and five real-life data sets.

#index 1119126
#* On Data Labeling for Clustering Categorical Data
#@ Hung-Leng Chen;Kun-Ta Chuang;Ming-Syan Chen
#t 2008
#c 7
#! Sampling has been recognized as an important technique to improve the efficiency of clustering. However, with sampling applied, those points which are not sampled will not have their labels after the normal process. Although there is a straightforward approach in the numerical domain, the problem of how to allocate those unlabeled data points into proper clusters remains as a challenging issue in the categorical domain. In this paper, a mechanism named MAximal Resemblance Data Labeling (abbreviated as MARDL) is proposed to allocate each unlabeled data point into the corresponding appropriate cluster based on the novel categorical clustering representative, namely, N-Nodeset Importance Representative(abbreviated as NNIR), which represents clusters by the importance of the combinations of attribute values. MARDL has two advantages: (1) MARDL exhibits high execution efficiency; (2) MARDL can achieve high intra-cluster similarity and low inter-cluster similarity, which are regarded as the most important properties of clusters, thus benefiting the analysis of cluster behaviors. MARDL is empirically validated on real and synthetic data sets, and is shown to be not only more efficient than prior methods but also attaining results of better quality.

#index 1119127
#* Mining Loosely Structured Motifs from Biological Data
#@ Fabio Fassetti;Gianluigi Greco;Giorgio Terracina
#t 2008
#c 7
#! The discovery of information encoded in biological sequences is assuming a prominent role in identifying genetic diseases and in deciphering biological mechanisms. This information is usually encoded in patterns frequently occurring in the sequences, also called motifs. In fact, motif discovery has received much attention in the literature, and several algorithms have already been proposed, which are specifically tailored to deal with motifs exhibiting some kinds of "regular structure". Motivated by biological observations, this paper focuses on the mining of loosely structured motifs, i.e., of more general kinds of motif where several "exceptions" may be tolerated in pattern repetitions. To this end, an algorithm exploiting data structures conceived to efficiently handle pattern variabilities is presented and analyzed. Furthermore, a randomized variant with linear time and space complexity is introduced, and a theoretical guarantee on its performances is proven. Both algorithms have been implemented and tested on real data sets. Despite the ability of mining very complex kinds of pattern, performance results evidence a genome-wide applicability of the proposed techniques.

#index 1119128
#* Bias and Controversy in Evaluation Systems
#@ Hady W. Lauw;Ee-Peng Lim;Ke Wang
#t 2008
#c 7
#! Evaluation is prevalent in real-life. With the advent of Web 2.0, online evaluation has become an important feature in many applications that involve information (e.g., video, photo, audio) sharing, and social networking (e.g., blogging). In these evaluation settings, a set of reviewers assign scores to a set of objects. As part of evaluation analysis, we want to obtain fair reviews for all the given objects. However, the reality is that reviewers may deviate in their scores assigned to the same object, due to the potential "bias" of reviewers or "controversy" objects. The statistical approach of averaging deviations to determine bias and controversy assumes that all reviewers and objects should be given equal weight. In this paper, we look beyond this assumption and propose an approach based on the following observations: (1) evaluation is "subjective", as reviewers and objects have varying bias and controversy respectively, and (2) bias and controversy are mutually dependent. These observations underlie our proposed reinforcement-based model to determine bias and controversy simultaneously. Our approach also quantifies "evidence", which reveals the degree of confidence with which bias and controversy has been derived. This model is shown to be effective by experiments on real-life and synthetic datasets.

#index 1119129
#* Personalized Concept-Based Clustering of Search Engine Queries
#@ Kenneth Wai-Ting Leung;Wilfred Ng;Dik Lun Lee
#t 2008
#c 7
#! The exponential growth of information on the Web has introduced new challenges for building effective search engines. A major problem of web search is that search queries are usually short and ambiguous, and thus are insufficient for specifying the precise user needs. To alleviate this problem, some search engines suggest terms that are semantically related to the submitted queries so that users can choose from the suggestions the ones that reflect their information needs. In this paper, we introduce an effective approach that captures the user's conceptual preferences in order to provide personalized query suggestions. We achieve this goal with two new strategies. First, we develop online techniques that extract concepts from the web-snippets of the search result returned from a query and use the concepts to identify related queries for that query. Second, we propose a new two-phase personalized agglomerative clustering algorithm that is able to generate personalized query clusters. To the best of the authors' knowledge, no previous work has addressed personalization for query suggestions. To evaluate the effectiveness of our technique, a Google middleware was developed for collecting clickthrough data to conduct experimental evaluation. Experimental results show that our approach has better precision and recall than the existing query clustering methods.

#index 1119130
#* Agglomerative Fuzzy K-Means Clustering Algorithm with Selection of Number of Clusters
#@ Mark Junjie Li;Michael K. Ng;Yiu-ming Cheung;Joshua Zhexue Huang
#t 2008
#c 7
#! In this paper, we present an agglomerative fuzzy $k$-means clustering algorithm for numerical data, an extension to the standard fuzzy $k$-means algorithm by introducing a penalty term to the objective function to make the clustering process not sensitive to the initial cluster centers. The new algorithm can produce more consistent clustering results from different sets of initial clusters centers. Combined with cluster validation techniques, the new algorithm can determine the number of clusters in a data set, which is a well known problem in $k$-means clustering. Experimental results on synthetic data sets (2 to 5 dimensions, 500 to 5000 objects and 3 to 7 clusters), the BIRCH two-dimensional data set of 20000 objects and 100 clusters, and the WINE data set of 178 objects, 17 dimensions and 3 clusters from UCI, have demonstrated the effectiveness of the new algorithm in producing consistent clustering results and determining the correct number of clusters in different data sets, some with overlapping inherent clusters.

#index 1119131
#* Using Context to Improve Predictive Modeling of Customers in Personalization Applications
#@ Cosimo Palmisano;Alexander Tuzhilin;Michele Gorgoglione
#t 2008
#c 7
#! The idea that context is important when predicting customer behavior has been maintained by scholars in marketing and data mining. However, no systematic study measuring how much the contextual information really matters in building customer models in personalization applications have been done before. In this paper we study how important the contextual information is when predicting customer behavior and how to use it when building customer models. It is done by conducting an empirical study across a wide range of experimental conditions. The experimental results show that context does matter when modeling the behavior of individual customers and that it is possible to infer the context from the existing data with reasonable accuracy in certain cases. It is also shown that significant performance improvements can be achieved if the context is "cleverly" modeled, as described in the paper. These findings have significant implications for data miners and marketers. They show that contextual information does matter in personalization applications and companies have different opportunities to both make context valuable for improving predictive performance of customers' behavior and decreasing the costs of gathering contextual information.

#index 1119132
#* Web People Search via Connection Analysis
#@ Dmitri V. Kalashnikov;Zhaoqi Chen;Sharad Mehrotra;Rabia Nuray-Turan
#t 2008
#c 7
#! Nowadays, searches for webpages of a person with a given name constitute a notable fraction of queries to web search engines. Such a query would normally return webpages related to several namesakes, who happened to have the queried name, leaving the burden of disambiguating and collecting pages relevant to a particular person (from among the namesakes) on the user. In this article we develop a Web People Search approach that clusters webpages based on their association to different people. Our method exploits a variety of semantic information extracted from Web pages, such as named entities and hyperlinks, to disambiguate among namesakes referred to on the Web pages. We demonstrate the effectiveness of our approach by testing the efficacy of the disambiguation algorithms and its impact on person search.

#index 1119133
#* Modeling Image Data for Effective Indexing and Retrieval in Large General Image Databases
#@ Xiaoyan Li;Lidan Shou;Gang Chen;Tianlei Hu;Jinxiang Dong
#t 2008
#c 7
#! In this paper, we propose an image semantic model based on the knowledge and criteria in the field of linguistics and taxonomy. Our work bridges the "semantic gap" by seamlessly exploiting the synergy of both visual feature processing and semantic relevance computation in a new way, and provides improved query efficiency and effectiveness for large general image databases. Our main contributions are as follows: We design novel data structures, namely a Lexical Hierarchy, an Image-Semantic Hierarchy, and a number of Atomic Semantic Domains, to capture the semantics and the features of the database, and to provide the indexing scheme. We present a novel image query algorithm based on the proposed structures. In addition, we propose a novel term expansion mechanism to improve the lexical processing. Our extensive experiments indicate that our proposed techniques are effective in achieving high run-time performance with improved retrieval accuracy. The experiments also show that the proposed method has good scalability.

#index 1147643
#* EIC Editorial: Introducing the New Editor-in-Chief and Four New Associate Editors
#@ Xindong Wu
#t 2009
#c 7

#index 1147644
#* 2008 TKDE Reviewers List
#@ 
#t 2009
#c 7

#index 1147645
#* Unsupervised Multiway Data Analysis: A Literature Survey
#@ Evrim Acar;Bülent Yener
#t 2009
#c 7
#! Two-way arrays or matrices are often not enough to represent all the information in the data and standard two-way analysis techniques commonly applied on matrices may fail to find the underlying structures in multi-modal datasets. Multiway data analysis has recently become popular as an exploratory analysis tool in discovering the structures in higher-order datasets, where data have more than two modes. We provide a review of significant contributions in the literature on multiway models, algorithms as well as their applications in diverse disciplines including chemometrics, neuroscience, social network analysis, text mining and computer vision.

#index 1147646
#* Comparing Scores Intended for Ranking
#@ Narayan L. Bhamidipati;Sankar K. Pal
#t 2009
#c 7
#! Often ranking is performed on the the basis of some scores available for each item. The existing practice for comparing scoring functions is to compare the induced rankings by one of the multitude of rank comparison methods available in the literature. We suggest that it may be better to compare the underlying scores themselves. To this end, a generalized Kendall distance is defined, which takes into consideration not only the final ordering that the two schemes produce, but also at the spacing between pairs of scores. This is shown to be equivalent to comparing the scores after fusing with another set of scores, making it theoretically interesting. A top k version of the score comparison methodology is also provided. Experimental results clearly show the advantages score comparison has over rank comparison.

#index 1147647
#* Online Skyline Analysis with Dynamic Preferences on Nominal Attributes
#@ Raymond Chi-Wing Wong;Jian Pei;Ada Wai-Chee Fu;Ke Wang
#t 2009
#c 7
#! The importance of skyline analysis has been well recognized in multi-criteria decision making applications. All of the previous studies assume a fixed order on the attributes in question. However, in some applications, users may be interested in skylines with respect to various total or partial orders on nominal attributes. In this paper, we identify and tackle the problem of online skyline analysis with dynamic preferences on nominal attributes. We investigate how changes of orders in attributes lead to changes of skylines. We address two novel types of interesting queries: a viewpoint query returns with respect to which orders a point is (or is not) in the skylines and an order-based skyline query retrieves the skyline with respect to a specific order. We develop two methods systematically and report an extensive performance study using both synthetic and real data sets to verify their effectiveness and efficiency.

#index 1147648
#* Self-Learning Disk Scheduling
#@ Yu Zhang;Bharat Bhargava
#t 2009
#c 7
#! Performance of disk I/O schedulers is affected by many factors, such as workloads, file systems, and disk systems. Disk scheduling performance can be improved by tuning scheduler parameters, such as the length of read timers. Scheduler performance tuning is mostly done manually. To automate this process, we propose four self-learning disk scheduling schemes: Change-sensing Round-Robin, Feedback Learning, Per-request Learning, and Two-layer Learning. Experiments show that the novel Two-layer Learning Scheme performs best. It integrates the workload-level and request-level learning algorithms. It employs feedback learning techniques to analyze workloads, change scheduling policy, and tune scheduling parameters automatically. We discuss schemes to choose features for workload learning, divide and recognize workloads, generate training data, and integrate machine learning algorithms into the Two-layer Learning Scheme. We conducted experiments to compare the accuracy, performance, and overhead of five machine learning algorithms: Decision Tree, Logistic Regression, Naïve Bayes, Neural Network, and Support Vector Machine Algorithms. Experiments with real-world and synthetic workloads show that self-learning disk scheduling can adapt to a wide variety of workloads, file systems, disk systems, and user preferences. It outperforms existing disk schedulers by as much as 15.8% while consuming less than 3%-5% of CPU time.

#index 1147649
#* Discriminative Training of the Hidden Vector State Model for Semantic Parsing
#@ Deyu Zhou;Yulan He
#t 2009
#c 7
#! In this paper, we discuss how discriminative training can be applied to the Hidden Vector State (HVS) model in different task domains. The HVS model is a discrete Hidden Markov Model (HMM) in which each HMM state represents the state of a push-down automaton with a finite stack size. In previous applications, Maximum Likelihood estimation (MLE) is used to derive the parameters of the HVS model. However, MLE makes a number of assumptions and unfortunately some of these assumptions do not hold. Discriminative training, without making such assumptions, can improve the performance of the HVS model. Experiments have been conducted in two domains: the travel domain for the semantic parsing task using the DARPA Communicator data and the ATIS data, and the bioinformatics domain for the information extraction task using the GENIA corpus. The results demonstrate modest improvements of the performance of the HVS model using discriminative training. In the travel domain, discriminative training of the HVS model gives a relative error reduction rate of 31% in F-measure when compared with MLE on the DARPA Communicator data and 9% on the ATIS data. In the bioinformatics domain, a relative error reduction rate of 4% in F-measure is achieved on the GENIA corpus.

#index 1147650
#* Efficient Range Query Processing in Peer-to-Peer Systems
#@ Dongsheng Li;Jiannong Cao;Xicheng Lu;Keith C.  C. Chen
#t 2009
#c 7
#! With the increasing popularity of the peer-to-peer (P2P) computing paradigm, many general range query schemes for distributed hash table (DHT)-based P2P systems have been proposed in recent years. Although those schemes can provide range query capability without modifying the underlying DHTs, they have the query delay depending on both the scale of the system and the size of the query space or the specific query, and thus cannot guarantee to return the query results in a bounded delay. In this paper, we propose Armada, an efficient range query processing scheme to support delay-bounded single-attribute and multiple-attribute range queries. It is the first delay-bounded general range query scheme on constant-degree DHTs, and can return the results for any range query within 2logN hops in a P2P system with N peers. Results of analysis and simulations show that the average delay in Armada is less than logN, and the average message cost of single-attribute range queries is about logN+2n 2 (n is the number of peers that intersect with the query). These results are very close to the lower bounds on delay and message cost of range queries over constant-degree DHTs.

#index 1147651
#* SPOT Databases: Efficient Consistency Checking and Optimistic Selection in Probabilistic Spatial Databases
#@ Austin Parker;Guillaume Infantes;John Grant;V. S. Subrahmanian
#t 2009
#c 7
#! Spatial PrObabilistic Temporal (SPOT) databases are a paradigm for reasoning with probabilistic statements about where objects are now or in the future. They express statements of the form "Object O is in spatial region R at time t with some probability in the interval [L,U]." Past work on SPOT databases uses selection operators returning SPOT atoms entailed by the SPOT database - we call this "cautious" selection. In this paper, we study several problems. First, we introduce the notion of "optimistic" selection queries that return sets of SPOT atoms consistent with, rather than entailed by, the SPOT database. We then develop an approach to scaling SPOT databases that has three main contributions: (i) We substantially reduce the size of past work's linear programs via variable elimination. (ii) We rigorously prove how one can prune the space searched in optimistic selection. (iii) We build an efficient index to execute optimistic selection queries over SPOT databases. Our approach is superior to past work in two major respects: first, it makes fewer assumptions than all past works on this topic except [30]. Second, the experiments - some using real world ship movement data - show substantially better performance than achieved in [30].

#index 1147652
#* Efficient Evaluation of Probabilistic Advanced Spatial Queries on Existentially Uncertain Data
#@ Man Lung Yiu;Nikos Mamoulis;Xiangyuan Dai;Yufei Tao;Michail Vaitis
#t 2009
#c 7
#! We study the problem of answering spatial queries in databases where objects exist with some uncertainty and they are associated with an existential probability. The goal of a thresholding probabilistic spatial query is to retrieve the objects that qualify the spatial predicates with probability that exceeds a threshold. Accordingly, a ranking probabilistic spatial query selects the objects with the highest probabilities to qualify the spatial predicates. We propose adaptations of spatial access methods and search algorithms for probabilistic versions of range queries, nearest neighbors, spatial skylines, and reverse nearest neighbors and conduct an extensive experimental study, which evaluates the effectiveness of proposed solutions.

#index 1147653
#* A Relation-Based Page Rank Algorithm for Semantic Web Search Engines
#@ Fabrizio Lamberti;Andrea Sanna;Claudio Demartini
#t 2009
#c 7
#! With the tremendous growth of information available to end users through the Web, search engines come to play ever a more critical role. Nevertheless, because of their general purpose approach, it is always less uncommon that obtained result sets provide a burden of useless pages. Next generation Web architecture, represented by Semantic Web, provides the layered architecture possibly allowing to overcome this limitation. Several search engines have been proposed, which allow to increase information retrieval accuracy by exploiting a key content of Semantic Web resources, that is relations. However, in order to rank results, most of the existing solutions need to work on the whole annotated knowledge base. In this paper we propose a relation-based page rank algorithm to be used in conjunction with Semantic Web search engines that simply relies on information which could be extracted from user query and annotated resource. Relevance is measured as the probability that retrieved resource actually contains those relations whose existence was assumed by the user at the time of query definition.

#index 1147654
#* CDNs Content Outsourcing via Generalized Communities
#@ Dimitrios Katsaros;George Pallis;Konstantinos Stamos;Athena Vakali;Antonis Sidiropoulos;Yannis Manolopoulos
#t 2009
#c 7
#! Content Distribution Networks (CDNs) balance costs and quality in services related to content delivery. Devising an efficient content outsourcing policy is crucial since, based on such policies, CDN providers can provide client-tailored content, improve performance, and result in significant economical gains. Earlier content outsourcing approaches may often prove ineffective since they drive prefetching decisions by assuming knowledge of content popularity statistics, which are not always available and are extremely volatile. This work addresses this issue, by proposing a novel self-adaptive technique under a CDN framework on which outsourced content is identified with no a-priori knowledge of (earlier) request statistics. This is employed by using a structure-based approach identifying coherent clusters of "correlated" Web server content objects, the so-called Web page communities. These communities are the core outsourcing unit and in this paper a detailed simulation experimentation has shown that the proposed technique is robust and effective in reducing user-perceived latency as compared with competing approaches, i.e., two communities-based approaches, Web caching, and non-CDN.

#index 1147655
#* TKDE 20(12) (December 2008) EIC Editorial: State of the Transactions
#@ Xindong Wu
#t 2008
#c 7

#index 1147656
#* Quantitative Inference by Qualitative Semantic Knowledge Mining with Bayesian Model Averaging
#@ Rui Chang;Martin Stetter;Wilfried Brauer
#t 2008
#c 7
#! In this paper, we consider the problem of performing quantitative Bayesian inference and model averaging based on a set of qualitative statements about relationships. Statements are transformed into parameter constraints which are imposed onto a set of Bayesian networks. Recurrent relationship structures are resolved by unfolding in time to Dynamic Bayesian networks. The approach enables probabilistic inference by model averaging, i.e. it allows to predict probabilistic quantities from a set of qualitative constraints without probability assignment on the model parameters. Model averaging is performed by Monte Carlo integration techniques. The method is applied to a problem in a molecular medical context: We show how the rate of breast cancer metastasis formation can be predicted based solely on a set of qualitative biological statements about the involvement of proteins in metastatic processes.

#index 1147657
#* Efficient Correlation Search from Graph Databases
#@ Yiping Ke;James Cheng;Wilfred Ng
#t 2008
#c 7
#! Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects. However, research on correlation mining from graph databases is still lacking despite the proliferation of graph data in recent years. We propose a new problem of correlation mining from graph databases, called Correlated Graph Search (CGS). CGS adopts Pearson's correlation coefficient to take into account the occurrence distributions of graphs. However, the problem poses significant challenges, since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential. We derive two necessary conditions that set bounds on the occurrence probability of a candidate in the database. With this result, we devise an efficient algorithm that mines the candidate set from a much smaller projected database and thus a significantly smaller set of candidates is obtained. Three heuristic rules are further developed to refine the candidate set. We also make use of the bounds to directly answer high-support queries without mining the candidates. Experimental results justify the efficiency of our algorithm. Finally, we generalize the CGS problem and show that our algorithm provides a general solution to most of the existing correlation measures.

#index 1147658
#* Novel Online Methods for Time Series Segmentation
#@ Xiaoyan Liu;Zhenjiang Lin;Huaiqing Wang
#t 2008
#c 7
#! To efficiently and effectively mine massive amounts of data in the time series, approximate representation of the data is one of the most commonly used strategies. Piecewise Linear Approximation is such an approach, which represents a time series by dividing it into segments and approximating each segment with a straight line. In this paper, we first propose a new segmentation criterion that improves computing efficiency. Based on this criterion, two novel online piecewise linear segmentation methods are developed, the feasible space window method and the stepwise feasible space window method. The former usually produces much fewer segments and is faster and more reliable in the running time than other methods. The latter can reduce the representation error with fewer segments. It achieves the best overall performance on the segmentation results compared with other methods. Extensive experiments on a variety of real-world time series have been conducted to demonstrate the advantages of our methods.

#index 1147659
#* Scalable Filtering of Multiple Generalized-Tree-Pattern Queries over XML Streams
#@ Songting Chen;Hua-Gang Li;Jun'ichi Tatemura;Wang-Pin Hsiung;Divyakant Agrawal;K. Selçuk Candan
#t 2008
#c 7
#! An XML publish/subscribe system needs to filter a large number of queries over XML streams. Most existing systems only consider filtering the simple XPath statements. In this paper, we focus on filtering of the more complex Generalized-Tree-Pattern (GTP) queries. Our filtering mechanism is based on a novel Tree-of-Path (TOP) encoding scheme, which compactly represents the path matches for the entire document. First, we show that the TOP encodings can be efficiently produced via a shared bottom-up path matching. Second, with the aid of this TOP encoding, we can 1) achieve polynomial time and space complexity for post processing, 2) avoid redundant predicate evaluations, 3) allow an efficient duplicate-free and merge join-based algorithm for merging multiple encoded path matches and 4) simplify the processing of GTP queries. Overall our approach maximizes the sharing opportunity across queries by exploiting the suffix as well as prefix sharing. At the same time, our TOP encodings allow efficient post processing for GTP queries. Extensive performance studies show that our GFilter solution not only achieves significantly better filtering performance than state-of-the-art algorithms, but also is capable of efficiently filtering the more complex GTP queries.

#index 1147660
#* Computation and Monitoring of Exclusive Closest Pairs
#@ Leong Hou U;Nikos Mamoulis;Man Lung Yiu
#t 2008
#c 7
#! Given two datasets $A$ and $B$, their exclusive closest pairs (ECP) join is a one-to-one assignment of objects from the two datasets, such that (i) the closest pair $(a,b)$ in $A \times B$ is in the result and (ii) the remaining pairs are determined by removing objects $a,b$ from $A,B$ respectively, and recursively searching for the next closest pair. A real application of exclusive closest pairs is the computation of (car, parking slot) assignments. This paper introduces the problem and proposes several solutions that solve it in main-memory, exploiting space partitioning. In addition, we define a dynamic version of the problem, where the objective is to continuously monitor the ECP join solution, in an environment where the joined datasets change positions and content. Finally, we study an extended form of the query, where objects in one of the two joined sets (e.g., parking slots) have a capacity constraint, allowing them to match with multiple objects from the other set (e.g., cars). We show how our techniques can be extended for this variant and compare them with a previous solution to this problem. Experimental results on a system prototype demonstrate the efficiency and applicability of the proposed algorithms.

#index 1147661
#* Toward the Optimal Itinerary-Based KNN Query Processing in Mobile Sensor Networks
#@ Shan-Hung Wu;Kun-Ta Chuang;Chung-Min Chen;Ming-Syan Chen
#t 2008
#c 7
#! Current approaches to K Nearest Neighbor (KNN) search in mobile sensor networks require certain kind of indexing support. Creation and maintenance of these index structures, to reflect the network dynamics due to sensor node mobility, may result in long query response time and low battery efficiency, thus limiting their practical use. We propose a maintenance-free, itinerary-based approach called Density-aware Itinerary KNN query processing (DIKNN). The DIKNN divides the search area into multiple cone-shape areas centered at the query point. It then performs a query dissemination and response collection itinerary in each of the cone-shape areas in parallel. The design of the DIKNN scheme takes into account several challenging issues such as the tradeoff between degree of parallelism and network interference on query response time, and the dynamic adjustment of the search radius (in terms of number of hops) according to spatial irregularity or mobility of sensor nodes. This model is validated by extensive simulations. The simulation results show that DIKNN yields substantially better performance and scalability over previous work, both as k increases and as the sensor node mobility increases.

#index 1147662
#* Efficient Processing of Top-k Queries in Uncertain Databases with x-Relations
#@ Ke Yi;Feifei Li;George Kollios;Divesh Srivastava
#t 2008
#c 7
#! This work introduces new algorithms for processing top-$k$ queries in uncertain databases, under the generally adopted model of x-relations. An x-relation consists of a number of x-tuples, and each x-tuple randomly instantiates into one tuple from one or more alternatives. Soliman et al.~\cite{soliman07} first introduced the problem of top-$k$ query processing in uncertain databases and proposed various algorithms to answer such queries. Under the x-relation model, our new results significantly improve the state of the art, in terms of both running time and memory usage. In the single-alternative case, our new algorithms are 2 to 3 orders of magnitude faster than the previous algorithms. In the multi-alternative case, the improvement is even more dramatic: while the previous algorithms have exponential complexity in both time and space, our algorithms run in near linear or low polynomial time. Our study covers both types of top-$k$ queries proposed in \cite{soliman07}. We provide both the theoretical analysis and an extensive experimental evaluation to demonstrate the superiority of the new approaches over existing solutions.

#index 1147663
#* Deriving Protocol Models from Imperfect Service Conversation Logs
#@ Hamid R. Motahari-Nezhad;Régis Saint-Paul;Boualem Benatallah;Fabio Casati
#t 2008
#c 7
#! Understanding the business (interaction) protocol supported by a service is very important for both clients and service providers: it allows developers to know how to write clients that interact with a service, and it allows development tools and runtime middleware to deliver functionality that simplifies the service development lifecycle. It also greatly facilitates the monitoring, visualization, and aggregation of interaction data. This paper presents an approach for discovering protocol definitions from real-world service interaction logs. It first describes the challenges in protocol discovery in such a context. Then, it presents a novel discovery algorithm, which is widely applicable, robust to different kinds of imperfections often present in realworld service logs, and able to derive protocols of small sizes, also thanks to heuristics. As finding the most precise and the smallest model is algorithmically not feasible from imperfect service logs, finally, the paper presents an approach to refine the discovered protocol via user interaction, to compensate for possible imprecision introduced in the discovered model. The approach has been implemented and experimental results show its viability on both synthetic and real-world datasets.

#index 1147664
#* Nonthreshold-Based Event Detection for 3D Environment Monitoring in Sensor Networks
#@ Mo Li;Yunhao Liu;Lei Chen
#t 2008
#c 7
#! Event detection is a crucial task for wireless sensor network applications, especially environment monitoring. Existing approaches for event detection are mainly based on some predefined threshold values, and thus are often inaccurate and incapable of capturing complex events. For example, in coal mine monitoring scenarios, gas leakage or water osmosis can hardly be described by the overrun of specified attribute thresholds, but some complex pattern in the full-scale view of the environmental data. To address this issue, we propose a non-threshold based approach for the real 3D sensor monitoring environment. We employ energy-efficient methods to collect a time series of data maps from the sensor network and detect complex events through matching the gathered data to spatio-temporal data patterns. Finally, we conduct trace driven simulations to prove the efficacy and efficiency of this approach on detecting events of complex phenomena from real-life records.

#index 1147665
#* A Virtual Ring Method for Building Small-World Structured P2P Overlays
#@ Hai Zhuge;Xiaoping Sun
#t 2008
#c 7
#! This paper presents a general virtual ring method to design and analyze small-world structured P2P networks on the base topologies embedded in ID spaces with distance metric. Its basic idea is to abstract a virtual ring from the base topology according to the distance metric, then build small-world long links in the virtual ring and map the links back onto the real network to construct the small-world routing tables for achieving logarithmic greedy routing efficiency. Four properties are proposed to characterize the base topologies that can be turned into small-world by the virtual ring method. The virtual ring method is applied to the base topologies of d-torus with Manhattan distance, high dimensional d-torus base topologies, and other base topologies including the unbalanced d-torus and the ring topology with tree distance. Theoretical analysis and simulation experiments demonstrate the efficiency and the resilience of the proposed overlays.

#index 1164162
#* Learning Image-Text Associations
#@ Tao Jiang;Ah-Hwee Tan
#t 2009
#c 7
#! Web information fusion can be defined as the problem of collating and tracking information related to specific topics on the World Wide Web. Whereas most existing work on web information fusion has focused on text-based multidocument summarization, this paper concerns the topic of image and text association, a cornerstone of cross-media web information fusion. Specifically, we present two learning methods for discovering the underlying associations between images and texts based on small training data sets. The first method based on vague transformation measures the information similarity between the visual features and the textual features through a set of predefined domain-specific information categories. Another method uses a neural network to learn direct mapping between the visual and textual features by automatically and incrementally summarizing the associated features into a set of information templates. Despite their distinct approaches, our experimental results on a terrorist domain document set show that both methods are capable of learning associations between images and texts from a small training data set.

#index 1164163
#* Cost-Based Predictive Spatiotemporal Join
#@ Wook-Shin Han;Jaehwa Kim;Byung Suk Lee;Yufei Tao;Ralf Rantzau;Volker Markl
#t 2009
#c 7
#! A predictive spatiotemporal join finds all pairs of moving objects satisfying a join condition on future time and space. In this paper, we present CoPST, the first and foremost algorithm for such a join using two spatiotemporal indexes. In a predictive spatiotemporal join, the bounding boxes of the outer index are used to perform window searches on the inner index, and these bounding boxes enclose objects with increasing laxity over time. CoPST constructs globally tightened bounding boxes âon the flyâ to perform window searches during join processing, thus significantly minimizing overlap and improving the join performance. CoPST adapts gracefully to large-scale databases, by dynamically switching between main-memory buffering and disk-based buffering, through a novel probabilistic cost model. Our extensive experiments validate the cost model and show its accuracy for realistic data sets. We also showcase the superiority of CoPST over algorithms adapted from state-of-the-art spatial join algorithms, by a speedup of up to an order of magnitude.

#index 1164164
#* Decompositional Rule Extraction from Support Vector Machines by Active Learning
#@ David Martens;Bart Baesens;Tony Van Gestel
#t 2009
#c 7
#! Support vector machines (SVMs) are currently state-of-the-art for the classification task and, generally speaking, exhibit good predictive performance due to their ability to model nonlinearities. However, their strength is also their main weakness, as the generated nonlinear models are typically regarded as incomprehensible black-box models. In this paper, we propose a new Active Learning-Based Approach (ALBA) to extract comprehensible rules from opaque SVM models. Through rule extraction, some insight is provided into the logics of the SVM model. ALBA extracts rules from the trained SVM model by explicitly making use of key concepts of the SVM: the support vectors, and the observation that these are typically close to the decision boundary. Active learning implies the focus on apparent problem areas, which for rule induction techniques are the regions close to the SVM decision boundary where most of the noise is found. By generating extra data close to these support vectors that are provided with a class label by the trained SVM model, rule induction techniques are better able to discover suitable discrimination rules. This performance increase, both in terms of predictive accuracy as comprehensibility, is confirmed in our experiments where we apply ALBA on several publicly available data sets.

#index 1164165
#* Multiclass MTS for Simultaneous Feature Selection and Classification
#@ Chao-Ton Su;Yu-Hsiang Hsiao
#t 2009
#c 7
#! Multiclass Mahalanobis-Taguchi system (MMTS), the extension of MTS, is developed for simultaneous multiclass classification and feature selection. In MMTS, the multiclass measurement scale is constructed by establishing an individual Mahalanobis space for each class. To increase the validity of the measurement scale, the Gram-Schmidt process is performed to mutually orthogonalize the features and eliminate the multicollinearity. The important features are identified using the orthogonal arrays and the signal-to-noise ratio, and are then used to construct a reduced model measurement scale. The contribution of each important feature to classification is also derived according to the effect gain to develop a weighted Mahalanobis distance which is finally used as the distance metric for the classification of MMTS. Using the reduced model measurement scale, an unknown example will be classified into the class with minimum weighted Mahalanobis distance considering only the important features. For evaluating the effectiveness of MMTS, a numerical experiment is implemented, and the results show that MMTS outperforms other well-known algorithms not only on classification accuracy but also on feature selection efficiency. Finally, a real case about gestational diabetes mellitus is studied, and the results indicate the practicality of MMTS in real-world applications.

#index 1164166
#* k-Anonymization with Minimal Loss of Information
#@ Aristides Gionis;Tamir Tassa
#t 2009
#c 7
#! The technique of k-anonymization allows the releasing of databases that contain personal information while ensuring some degree of individual privacy. Anonymization is usually performed by generalizing database entries. We formally study the concept of generalization, and propose three information-theoretic measures for capturing the amount of information that is lost during the anonymization process. The proposed measures are more general and more accurate than those that were proposed by Meyerson and Williams [23] and Aggarwal et al. [1]. We study the problem of achieving k-anonymity with minimal loss of information. We prove that it is NP-hard and study polynomial approximations for the optimal solution. Our first algorithm gives an approximation guarantee of O(\ln k) for two of our measures as well as for the previously studied measures. This improves the best-known O(k)-approximation in [1]. While the previous approximation algorithms relied on the graph representation framework, our algorithm relies on a novel hypergraph representation that enables the improvement in the approximation ratio from O(k) to O(\ln k). As the running time of the algorithm is O(n^{2k}), we also show how to adapt the algorithm in [1] in order to obtain an O(k)-approximation algorithm that is polynomial in both n and k.

#index 1164167
#* BMQ-Processor: A High-Performance Border-Crossing Event Detection Framework for Large-Scale Monitoring Applications
#@ Jinwon Lee;Seungwoo Kang;Youngki Lee;Sang Jeong Lee;Junehwa Song
#t 2009
#c 7
#! In this paper, we present BMQ-Processor, a high-performance Border-Crossing Event (BCE) detection framework for large-scale monitoring applications. We first characterize a new query semantics, namely, Border Monitoring Query (BMQ), which is useful for BCE detection in many monitoring applications. It monitors the values of data streams and reports them only when data streams cross the borders of its range. We then propose BMQ-Processor to efficiently handle a large number of BMQs over a high volume of data streams. BMQ-Processor efficiently processes BMQs in a shared and incremental manner. It develops and operates over a novel stateful query index, achieving a high level of scalability over continuous data updates. Also, it utilizes the locality embedded in data streams and greatly accelerates successive BMQ evaluations. We present data structures and algorithms to support 1D as well as multidimensional BMQs. We show that the semantics of border monitoring can be extended toward more advanced ones and build region transition monitoring as a sample case. Lastly, we demonstrate excellent processing performance and low storage cost of BMQ-Processor through extensive analysis and experiments.

#index 1164168
#* Privacy-Preserving Kth Element Score over Vertically Partitioned Data
#@ Jaideep Vaidya;Christopher W. Clifton
#t 2009
#c 7
#! Given a large integer data set shared vertically by two parties, we consider the problem of securely computing a score separating the k{\rm th} and the (k + 1){\rm th} element. An efficient secure protocol is developed to compute such a score while revealing little additional information. The proposed protocol is implemented using the Fairplay system and experimental results are reported. We show a real application of this protocol as a component used in the secure processing of top-k queries over vertically partitioned data.

#index 1164169
#* Semantic Access to Multichannel M-Services
#@ Xu Yang;Athman Bouguettaya
#t 2009
#c 7
#! M-services provide mobile users wireless access to Web services. In this paper, we present a novel infrastructure for supporting M-services in wireless broadcast systems. The proposed infrastructure provides a generic framework for mobile users to look up, access, and execute Web services over wireless broadcast channels. Access efficiency is an important issue in wireless broadcast systems. We discuss different semantics that have impact on the access efficiency for composite M-services. A multiprocess workflow is proposed for effectively accessing composite M-services from multiple broadcast channels based on these semantics. We also present and compare different broadcast channel organizations for M-services and wireless data. Analytical models are provided for these channel organizations. Practical studies are presented to demonstrate the impact of different semantics and channel organizations on the access efficiency.

#index 1164170
#* Online Scheduling Sequential Objects with Periodicity for Dynamic Information Dissemination
#@ Chih-Lin Hu;Ming-Syan Chen
#t 2009
#c 7
#! The scalability of data broadcasting has been manifested by prior studies on the base of the traditional data management systems where data objects, mapped to a pair of state and value in the database, are independent, persistent, and static against simple queries. However, many modern information applications spread dynamic data objects and process complex queries for retrieving multiple data objects. Particularly, the information servers dynamically generate data objects that are dependent and can be associated into a complete response against complex queries. Accordingly, the study in this paper considers the problem of scheduling dynamic broadcast data objects in a clients-providers-servers system from the standpoint of data association, dependency, and dynamics. Since the data broadcast problem is NP-hard, we derive the lower and the upper bounds of the mean service access time. In light of the theoretical analyses, we further devise a deterministic algorithm with several gain measure functions for the approximation of schedule optimization. The experimental results show that the proposed algorithm is able to generate a dynamic broadcast schedule and also minimize the mean service access time to the extent of being very close to the theoretical optimum.

#index 1164171
#* Storing and Indexing Spatial Data in P2P Systems
#@ Verena Kantere;Spiros Skiadopoulos;Timos Sellis
#t 2009
#c 7
#! The peer-to-peer (P2P) paradigm has become very popular for storing and sharing information in a totally decentralized manner. At first, research focused on P2P systems that host 1D data. Nowadays, the need for P2P applications with multidimensional data has emerged, motivating research on P2P systems that manage such data. The majority of the proposed techniques are based either on the distribution of centralized indexes or on the reduction of multidimensional data to one dimension. Our goal is to create from scratch a technique that is inherently distributed and also maintains the multidimensionality of data. Our focus is on structured P2P systems that share spatial information. We present SpatialP2P, a totally decentralized indexing and searching framework that is suitable for spatial data. SpatialP2P supports P2P applications in which spatial information of various sizes can be dynamically inserted or deleted, and peers can join or leave. The proposed technique preserves well locality and directionality of space.

#index 1177859
#* A Generic Local Algorithm for Mining Data Streams in Large Distributed Systems
#@ Ran Wolff;Kanishka Bhaduri;Hillol Kargupta
#t 2009
#c 7
#! In a large network of computers or wireless sensors, each of the components (henceforth, peers) has some data about the global state of the system. Much of the system's functionality such as message routing, information retrieval and load sharing relies on modeling the global state. We refer to the outcome of the function (e.g., the load experienced by each peer) as the \emph{model} of the system. Since the state of the system is constantly changing, it is necessary to keep the models up-to-date. Computing global data mining models e.g. decision trees, $k$-means clustering in large distributed systems may be very costly due to the scale of the system and due to communication cost, which may be high. The cost further increases in a dynamic scenario when the data changes rapidly. In this paper we describe a two step approach for dealing with these costs. First, we describe a highly efficient \emph{local} algorithm which can be used to monitor a wide class of data mining models. Then, we use this algorithm as a feedback loop for the monitoring of complex functions of the data such as its $k$-means clustering. The theoretical claims are corroborated with a thorough experimental analysis.

#index 1177860
#* Compression and Aggregation for Logistic Regression Analysis in Data Cubes
#@ Ruibin Xi;Nan Lin;Yixin Chen
#t 2009
#c 7
#! Logistic regression is an important technique for analyzing and predicting data with categorical attributes. In this paper, We consider supporting online analytical processing (OLAP) of logistic regression analysis for multi-dimensional data in a data cube where it is expensive in time and space to build logistic regression models for each cell from the raw data. We propose a novel scheme to compress the data in such a way that we can reconstruct logistic regression models to answer any OLAP query without accessing the raw data. Based on a first-order approximation to the maximum likelihood estimating equations, we develop a compression scheme that compresses each base cell into a small compressed data block with essential information to support the aggregation of logistic regression models. Aggregation formulae for deriving high-level logistic regression models from lower level component cells are given. We prove that the compression is nearly lossless in the sense that the aggregated estimator deviates from the true model by an error that is bounded and approaches to zero when the data size increases. The results show that the proposed compression and aggregation scheme can make feasible OLAP of logistic regression in a data cube.

#index 1177861
#* Mining Projected Clusters in High-Dimensional Spaces
#@ Mohamed Bouguessa;Shengrui Wang
#t 2009
#c 7
#! Clustering high-dimensional data has been a major challenge due to the inherent sparsity of the points. Most existing clustering algorithms become substantially inefficient if the required similarity measure is computed between data points in the full-dimensional space. To address this problem, a number of projected clustering algorithms have been proposed. However, most of them encounter difficulties when clusters hide in subspaces with very low dimensionality. These challenges motivate our effort to propose a robust partitional distance-based projected clustering algorithm. The algorithm consists of three phases. The first phase performs attribute relevance analysis by detecting dense and sparse regions and their location in each attribute. Starting from the results of the first phase, the goal of the second phase is to eliminate outliers, while the third phase aims to discover clusters in different subspaces. The clustering process is based on the K-means algorithm, with the computation of distance restricted to subsets of attributes where object values are dense. Our algorithm is capable of detecting projected clusters of low dimensionality embedded in a high-dimensional space and avoids the computation of the distance in the full-dimensional space. The suitability of our proposal has been demonstrated through an empirical study using synthetic and real datasets.

#index 1177862
#* On the Design and Applicability of Distance Functions in High-Dimensional Data Space
#@ Chih-Ming Hsu;Ming-Syan Chen
#t 2009
#c 7
#! Effective distance functions in high dimensional data space are very important in solutions for many data mining problems. Recent research has shown that if the Pearson variation of the distance distribution converges to zero with increasing dimensionality, the distance function will become unstable (or meaningless) in high dimensional space, even with the commonly used $L_p$ metric in the Euclidean space. This result has spawned many studies the along the same lines. However, the necessary condition for unstability of a distance function, which is required for function design, remains unknown. In this paper, we shall prove that several important conditions are in fact equivalent to unstability. Based on these theoretical results, we employ some effective and valid indices for testing the stability of a distance function. In addition, this theoretical analysis inspires us that unstable phenomena are rooted in variation of the distance distribution. To demonstrate the theoretical results, we design a meaningful distance function, called the Shrinkage-Divergence Proximity (SDP), based on a given distance function. It is shown empirically that the SDP significantly outperforms other measures in terms of stability in high dimensional data space, and is thus more suitable for distance-based clustering applications.

#index 1177863
#* A Pure Nash Equilibrium-Based Game Theoretical Method for Data Replication across Multiple Servers
#@ Samee Ullah Khan;Ishfaq Ahmad
#t 2009
#c 7
#! This paper proposes a non-cooperative game based technique to replicate data objects across a distributed system of multiple servers in order to reduce user perceived Web access delays. In the proposed technique computational agents represent servers and compete with each other to optimize the performance of their servers. The optimality of a non-cooperative game is typically described by Nash equilibrium, which is based on spontaneous and non-deterministic strategies. However, Nash equilibrium may or may not guarantee system-wide performance. Furthermore, there can be multiple Nash equilibria, making it difficult to decide which one is the best. In contrast, the proposed technique uses the notion of pure Nash equilibrium, which if achieved, guarantees stable optimal performance. In the proposed technique, agents use deterministic strategies that work in conjunction with their self-interested nature but ensure system-wide performance enhancement. In general, the existence of a pure Nash equilibrium is hard to achieve, but we prove the existence of such equilibrium in the proposed technique. The proposed technique is also experimentally compared against some well-known conventional replica allocation methods, such as branch and bound, greedy, and genetic algorithms.

#index 1177864
#* Optimal Lot Sizing Policies For Sequential Online Auctions
#@ Arvind K. Tripathi;Suresh K. Nair;Gilbert G. Karuga
#t 2009
#c 7
#! This study proposes methods for determining the optimal lot sizes for sequential auctions that are conducted to sell sizable quantities of an item. These auctions are fairly common in business to consumer (B2C) auctions. In these auctions, the tradeoff for the auctioneer is between the alacrity with which funds are received, and the amount of funds collected by the faster clearing of inventory using larger lot sizes. Observed bids in these auctions impact the auctioneer's decision on lot sizes in future auctions. We first present a goal programming approach for estimating the bid distribution for the bidder population from the observed bids, readily available in these auctions. We then develop models to compute optimal lot sizes for both stationary and non-stationary bid distributions. For stationary bid distribution, we present closed form solutions and structural results. Our findings show that the optimal lot size increases with inventory holding costs and number of bidders. Our model for non-stationary bid distribution captures the inter-auction dynamics such as the number of bidders, their bids, past winning bids, and lot size. We use simulated data to test the robustness of our model.

#index 1177865
#* Progressive Parametric Query Optimization
#@ Pedro Bizarro;Nicolas Bruno;David J. DeWitt
#t 2009
#c 7
#! Commercial applications usually rely on pre-compiled parameterized procedures to interact with a database. Unfortunately, executing a procedure with a set of parameters different from those used at compilation time may be arbitrarily sub-optimal. Parametric query optimization (PQO) attempts to solve this problem by exhaustively determining the optimal plans at each point of the parameter space at compile time. However, PQO is likely not cost-effective if the query is executed infrequently or if it is executed with values only within a subset of the parameter space. In this paper we propose instead to progressively explore the parameter space and build a parametric plan during several executions of the same query. We introduce algorithms that, as parametric plans are populated, are able to frequently bypass the optimizer but still execute optimal or near-optimal plans.

#index 1177866
#* IMine: Index Support for Item Set Mining
#@ Elena Baralis;Tania Cerquitelli;Silvia Chiusano
#t 2009
#c 7
#! This paper presents the IMine index, a general and compact structure which provides tight integration of itemset extraction in a relational DBMS. Since no constraint is enforced during the index creation phase, IMine provides a complete representation of the original database. To reduce the I/O cost, data accessed together during the same extraction phase are clustered on the same disk block. The IMine index structure can be efficiently exploited by different itemset extraction algorithms. In particular, IMine data access methods currently support the FP-growth and LCM v.2 algorithms, but they can straightforwardly support the enforcement of various constraint categories. The IMine index has been integrated into the PostgreSQL DBMS and exploits its physical level access methods. Experiments, run for both sparse and dense data distributions, show the efficiency of the proposed index and its linear scalability also for large datasets. Itemset mining supported by the IMine index shows performance always comparable with, and sometimes better than, state of the art algorithms accessing data on flat file.

#index 1177867
#* Multiscale Representations for Fast Pattern Matching in Stream Time Series
#@ Xiang Lian;Lei Chen;Jeffrey Xu Yu;Jinsong Han;Jian Ma
#t 2009
#c 7
#! Similarity-based time series retrieval has been a subject of long term study due to its wide usage in many applications, such as financial data analysis and weather data forecasting. Its original task was to find those time series similar to a pattern time series data, where both the pattern and data time series are static. Recently, with an increasing demand on stream data management, similarity-based stream time series retrieval has raised new research issues due to its unique requirements during the stream processing, such as one-pass search and fast response. In this paper, we address the problem of matching both static and dynamic patterns over stream time series data. We will develop a novel multi-scale representation, called multi-scale segment mean (MSM), for stream time series data, which can be incrementally computed and thus perfectly adapted to the stream characteristics. Most importantly, we propose a novel multi-step filtering mechanism, SS, over the multi-scale representation. Analysis indicates that the mechanism can greatly prune the search space and thus offer fast response. Furthermore, batching processing optimization, the dynamic case where patterns are also from stream time series, and pattern matching over future stream time series are also discussed. Extensive experiments show the proposed scheme can efficiently filter out false candidates and detect patterns.

#index 1177868
#* Histogram-Based Global Load Balancing in Structured Peer-to-Peer Systems
#@ Quang Hieu Vu;Beng Chin Ooi;Martin Rinard;Kian-Lee Tan
#t 2009
#c 7
#! Over the pass few years, peer-to-peer (P2P) systems have rapidly grown in popularity and become a dominant means for sharing resources. In these systems, load balancing is a key challenge because nodes are often heterogeneous. While several load balancing schemes have been proposed in the literature, these solutions are typically ad-hoc, heuristic-based and localized. In this paper, we present a general framework, HiGLOB, for global load balancing in structured P2P systems. Each node in HiGLOB has two key components: (1) A histogram manager maintains a histogram that reflects a global view of the distribution of the load in the system, and (2) A load-balancing manager that redistributes the load whenever the node becomes over or under loaded. We exploit the routing metadata to partition the P2P network into non-overlapping regions corresponding to the histogram buckets. We propose mechanisms to keep the cost of constructing and maintaining the histograms low. We further show that our scheme can control and bound the amount of load imbalance across the system. Finally, we demonstrate the effectiveness of HiGLOB by instantiating it over three existing structured P2P systems: Chord, Skip Graph and BATON. Our experimental results indicate that our approach works well in practice.

#index 1177869
#* Automatically Determining the Number of Clusters in Unlabeled Data Sets
#@ Liang Wang;Christopher Leckie;Kotagiri Ramamohanarao;James Bezdek
#t 2009
#c 7
#! Clustering is a popular tool for exploratory data analysis. One of the major problems in cluster analysis is the determination of the number of clusters in unlabeled data, which is a basic input for most clustering algorithms. In this paper we investigate a new method called DBE (Dark Block Extraction) for automatically estimating the number of clusters in unlabeled data sets, which is based on an existing algorithm for Visual Assessment of cluster Tendency (VAT) of a data set, using several common image and signal processing techniques. Basic steps include: 1) Generating a VAT image of an input dissimilarity matrix; 2) Performing image segmentation on the VAT image to obtain a binary image, followed by directional morphological filtering; 3) Applying a distance transform to the filtered binary image and projecting the pixel values onto the main diagonal axis of the image to form a projection signal; 4) Smoothing the projection signal, computing its first-order derivative, and then detecting major peaks and valleys in the resulting signal to decide the number of clusters. Our new DBE method is nearly "automatic", depending on just one easy-to-set parameter. Several numerical and real-world examples are presented to illustrate the effectiveness of DBE.

#index 1177870
#* Efficient Processing of Metric Skyline Queries
#@ Lei Chen;Xiang Lian
#t 2009
#c 7
#! Skyline query is of great importance in many applications, such as multi-criteria decision making and business planning. In particular, a skyline point is a data object in the database whose attribute vector is not dominated by that of any other objects. Previous methods to retrieve skyline points usually assume static data objects in the database (i.e. their attribute vectors are fixed), whereas several recent work focus on skyline queries with dynamic attributes. In this paper, we propose a novel variant of skyline queries, namely metric skyline, whose dynamic attributes are defined in the metric space (i.e. not limited to the Euclidean space). We illustrate an efficient and effective pruning mechanism to answer metric skyline queries through a metric index. Most importantly, we formalize the query performance of the metric skyline query in terms of the pruning power, by a cost model, in light of which we construct an optimized metric index aiming to maximize the pruning power of metric skyline queries. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed pruning techniques as well as the constructed index in answering metric skyline queries.

#index 1177871
#* On the Effect of Location Uncertainty in Spatial Querying
#@ Elias Frentzos;Kostas Gratsias;Yannis Theodoridis
#t 2009
#c 7
#! An emerging topic in the field of spatial data management is the handling of location uncertainty of spatial objects, mainly due to inaccurate measurements. The literature on location uncertainty so far has focused on modifying traditional spatial search algorithms in order to handle the impact of objects' location uncertainty in query results. In this paper, we present the first, to the best of our knowledge, theoretical analysis that estimates the average number of false hits introduced in the results of rectangular range queries in the case of data points uniformly distributed in 2D space. Then, we relax the original distribution assumptions showing how to deal with arbitrarily distributed data points and more realistic location uncertainty distributions. The accuracy of the results of our analytical approach is demonstrated through an extensive experimental study using various synthetic and real datasets. Our proposal can be directly employed in spatial database systems in order to provide users with the accuracy of spatial query results based only on known dataset and query parameters.

#index 1177872
#* Distributed Skyline Retrieval with Low Bandwidth Consumption
#@ Lin Zhu;Yufei Tao;Shuigeng Zhou
#t 2009
#c 7
#! We consider skyline computation when the underlying dataset is horizontally partitioned onto geographically distant servers \color{red} that are connected to the Internet. \color{black} The existing solutions are not suitable for our problem, because they have at least one of the following drawbacks: (i) applicable only to distributed systems adopting vertical partitioning or restricted horizontal partitioning, (ii) effective only when each server has limited computing and communication abilities, and (iii) optimized only for skyline search in subspaces but inefficient in the full space. This paper proposes an algorithm, called {\em feedback-based distributed skyline} (FDS), to support arbitrary horizontal partitioning. \color{red} FDS aims at minimizing the network bandwidth, measured in the number of tuples transmitted over the network. \color{black} The core of FDS is a novel feedback-driven mechanism, where the coordinator iteratively transmits certain feedback to each participant. Participants can leverage such information to prune a large amount of local data, which otherwise would need to be sent to the coordinator. Extensive experimentation confirms that FDS significantly outperforms alternative approaches in both effectiveness and progressiveness.

#index 1177873
#* Improving Personalization Solutions through Optimal Segmentation of Customer Bases
#@ Tianyi Jiang;Alexander Tuzhilin
#t 2009
#c 7
#! On the Web, where the search costs are low and the competition is just a mouse click away, it is crucial to segment the customers intelligently in order to offer more personalized products and services to them. Traditionally, customer segmentation is achieved using statistics-based methods that compute a set of statistics from the customer data and group customers into segments by applying distance-based clustering algorithms in the space of these statistics. In this paper, we present a direct grouping based approach to computing customer segments that groups customers in terms of optimally combining transactional data of several customers to build a predictive model of customer behavior for each group. We consider customer segmentation as a combinatorial optimization problem of finding the best partitioning of the customer base into disjoint groups and show that finding an optimal customer partition is NP-hard. We propose several suboptimal direct grouping segmentation methods, empirically compares them against traditional statistics-based hierarchical and affinity propagation based segmentation, and 1-to-1 methods across multiple experimental conditions. We show that the best direct grouping method builds mostly small sized customer segments and significantly dominates the statistics-based and 1-to-1 approaches across most of the experimental conditions, while still being computationally tractable.

#index 1177874
#* Effective and Efficient Query Processing for Video Subsequence Identification
#@ Heng Tao Shen;Jie Shao;Zi Huang;Xiaofang Zhou
#t 2009
#c 7
#! Content-based video retrieval has been well investigated. However, despite the importance, few studies on video subsequence identification, which is to find the similar content to a short query clip from a long video sequence, have been published. This paper presents a graph transformation and matching approach to this problem, with extension to identify the occurrence of potentially different ordering, alignment or length due to content editing. With a batch query algorithm to retrieve similar frames, the mapping relationship between the query and the database video is first represented by a bipartite graph. The densely matched parts along the long sequence are then extracted, followed by a filter-and-refine search strategy to prune some irrelevant subsequences. During the filtering stage, Maximum Size Matching (MSM) is deployed for each subgraph constructed by the query and candidate subsequence to obtain a smaller set of candidates. During the refinement stage, Sub-Maximum Similarity Matching (SMSM) is devised to identify the subsequence, according to a robust video similarity model which incorporates visual content, temporal order, frame alignment and length information. The performance studies conducted on a long and diverse video recording validate our approach is promising in terms of both search accuracy and speed.

#index 1177875
#* semQA: SPARQL with Idempotent Disjunction
#@ E. Patrick Shironoshita;Yves R. Jean-Mary;Ray M. Bradley;Mansur R. Kabuka
#t 2009
#c 7
#! The SPARQL LeftJoin abstract operator is not distributive over Union; this limits the algebraic manipulation of graph patterns, which in turn restricts the ability to create query plans for distributed processing or query optimization. In this paper, we present semQA, an algebraic extension for the SPARQL query language for RDF, which overcomes this issue by transforming graph patterns through the use of an idempotent disjunction operator Or as a substitute for Union. This permits the application of a set of equivalences that transform a query into distinct forms. We further present an algorithm to derive the solution set of the original query from the solution set of a query where Union has been substituted by Or. We also analyze the combined complexity of SPARQL, proving it to be NP-complete. It is also shown that the SPARQL query language is not, in the general case, fixed-parameter tractable. Experimental results are presented to validate the query evaluation methodology presented in this paper against the SPARQL standard, to corroborate the complexity analysis, and to illustrate the gains in processing cost reduction that can be obtained through the application of semQA.

#index 1177876
#* Detecting, Assessing and Monitoring Relevant Topics in Virtual Information Environments
#@ Jörg Ontrup;Helge Ritter;Sören W. Scholz;Ralf Wagner
#t 2009
#c 7
#! The ability to assess the relevance of topics and related sources in information-rich environments is a key to success when scanning business environments. This paper introduces a hybrid system to support managerial information gathering. The system is made up of three components: (1) a hierarchical hyperbolic SOM for structuring the information environment and visualizing the intensity of news activity with respect to identified topics, (2) a spreading activation network for the selection of the most relevant information sources with respect to an already existing knowledge infrastructure, and (3) measures of interestingness for association rules as well as statistical testing facilitates the monitoring of already identified topics. Embedding the system by a framework describing three modes of human information seeking behavior endorses an active organization, exploration and selection of information that matches the needs of decision makers in all stages of the information gathering process. By applying our system in the domain of the hotel industry we demonstrate how typical information gathering tasks are supported. Moreover, we present an empirical study investigating the effectiveness and efficiency of the visualization framework of our system.

#index 1177877
#* Distributional Features for Text Categorization
#@ Xiao-Bing Xue;Zhi-Hua Zhou
#t 2009
#c 7
#! Text categorization is the task of assigning predefined categories to natural language text. With the widely used 'bag of words' representation, previous researches usually assign a word with values such that whether this word appears in the document concerned or how frequently this word appears. Although these values are useful for text categorization, they have not fully expressed the abundant information contained in the document. This paper explores the effect of other types of values, which express the distribution of a word in the document. These novel values assigned to a word are called {\it distributional features}, which include the compactness of the appearances of the word and the position of the first appearance of the word. The proposed distributional features are exploited by a {\it tfidf} style equation and different features are combined using ensemble learning techniques. Experiments show that the distributional features are useful for text categorization. In contrast to using the traditional term frequency values solely, including the distributional features requires only a little additional cost, while the categorization performance can be significantly improved. Further analysis shows that the distributional features are especially useful when documents are long and the writing style is casual.

#index 1177878
#* The Development of Fuzzy Rough Sets with the Use of Structures and Algebras of Axiomatic Fuzzy Sets
#@ Xiaodong Liu;Witold Pedrycz;Tianyou Chai;Mingli Song
#t 2009
#c 7
#! The notion of a rough set was originally proposed by Pawlak underwent a number of extensions and generalizations. Dubois and Prade (1990) introduced fuzzy rough sets which involve the use of rough sets and fuzzy sets within a single framework. Radzikowska and Kerre (2002) proposed a broad family of fuzzy rough sets, referred to as ( t)-fuzzy rough sets which are determined by some implication operator (implicator), and a certain t-norm. In order to describe the linguistically represented concepts coming from data available in some information system, the concept of fuzzy rough sets are redefined and further studied in the setting of the Axiomatic Fuzzy Set (AFS) theory. Compared with the ( t)-fuzzy rough sets, the advantages of AFS fuzzy rough sets are twofold. They can be directly applied to data analysis present in any information system without resorting to the details concerning the choice of the implication, t-norm and a similarity relation S. Furthermore such rough approximations of fuzzy concepts come with a well-defined semantics and therefore offer a sound interpretation. Some examples are included to illustrate the effectiveness of the proposed construct. It is shown that the AFS fuzzy rough sets provide a far higher flexibility and effectiveness in comparison with rough sets and some of their generalizations.

#index 1189215
#* A Survey of Uncertain Data Algorithms and Applications
#@ Charu C. Aggarwal;Philip S. Yu
#t 2009
#c 7
#! In recent years, a number of indirect data collection methodologies have lead to the proliferation of uncertain data. Such data points are often represented in the form of a probabilistic function, since the corresponding deterministic value is not known. This increases the challenge of mining and managing uncertain data, since the precise behavior of the underlying data is no longer known. In this paper, we provide a survey of uncertain data mining and management applications. In the field of uncertain data management, we will examine traditional methods such as join processing, query processing, selectivity estimation, OLAP queries, and indexing. In the field of uncertain data mining, we will examine traditional mining problems such as classification and clustering. We will also examine a general transform based technique for mining uncertain data. We discuss the models for uncertain data, and how they can be leveraged in a variety of applications. We discuss different methodologies to process and mine uncertain data in a variety of forms.

#index 1189216
#* Adapted One-versus-All Decision Trees for Data Stream Classification
#@ Sattar Hashemi;Ying Yang;Zahra Mirzamomen;Mohammadreza Kangavari
#t 2009
#c 7
#! One versus all (OVA) decision trees learn k individual binary classifiers, each one to distinguish the instances of a single class from the instances of all other classes. Thus OVA is different from existing data stream classification schemes whose majority use multiclass classifiers, each one to discriminate among all the classes. This paper advocates some outstanding advantages of OVA for data stream classification. First, there is low error correlation and hence high diversity among OVA's component classifiers, which leads to high classification accuracy. Second, OVA is adept at accommodating new class labels that often appear in data streams. However, there also remain many challenges to deploy traditional OVA for classifying data streams. First, as every instance is fed to all component classifiers, OVA is known as an inefficient model. Second, OVA's classification accuracy is adversely affected by the imbalanced class distribution in data streams. This paper addresses those key challenges and consequently proposes a new OVA scheme that is adapted for data stream classification. Theoretical analysis and empirical evidence reveal that the adapted OVA can offer faster training, faster updating and higher classification accuracy than many existing popular data stream classification algorithms.

#index 1189217
#* Bayes Vector Quantizer for Class-Imbalance Problem
#@ Claudia Diamantini;Domenico Potena
#t 2009
#c 7
#! The class-imbalance problem is the problem of learning a classification rule from data that are skewed in favor of one class. On these datasets traditional learning techniques tend to overlook the less numerous class, at the advantage of the majority class. However, the minority class is often the most interesting one for the task at hand. For this reason, the class-imbalance problem has received increasing attention in the last few years. In the present paper we point the attention of the reader to a learning algorithm for the minimization of the average misclassification risk. In contrast to some popular class-imbalance learning methods, this method has its roots in statistical decision theory. A particular interesting characteristic is that when class distributions are unknown, the method can work by resorting to stochastic gradient algorithm. We study the behavior of this algorithm on imbalanced datasets, demonstrating that this principled approach allows to obtain better classification performances compared to the principal methods proposed in the literature.

#index 1189218
#* Catching the Trend: A Framework for Clustering Concept-Drifting Categorical Data
#@ Hung-Leng Chen;Ming-Syan Chen;Su-Chen Lin
#t 2009
#c 7
#! Although the problem of clustering numerical time-evolving data is well-explored, the problem of clustering categorical time-evolving data remains as a challenge issue. In this paper, we propose a generalized clustering framework which utilizes existing clustering algorithms and adopts sliding window technique to detect if there is a drifting-concept or not in the incoming sliding window. The framework is composed of two algorithms: Drifting Concept Detecting (abbreviated as DCD) algorithm detecting the changes of cluster distributions between the current sliding window and the last clustering result, and Cluster Relationship Analysis (abbreviated as CRA) algorithm analyzing the relationship between clustering results at different time. In DCD, the concept is said to drift if quite a large number of outliers are found in the current sliding window, or if quite a large number of clusters are varied in the ratio of data points. The drifted sliding window will perform re-clustering to capture the recent concept. In CRA, a visualizing method is devised to facilitate the observation of the evolving clustering results. The framework is validated on real and synthetic data sets, and is shown to not only accurately detect the drifting-concepts but also attain clustering results of better quality.

#index 1189219
#* GridVideo: A Practical Example of Nonscientific Application on the Grid
#@ Dario Bruneo;Giuseppe Iellamo;Giuseppe Minutoli;Antonio Puliafito
#t 2009
#c 7
#! Starting from 1990s and until now, Grid computing has been mainly used in scientific laboratories. Only in the last few years it is evolving into a business-innovating technology that is driving commercial adoption. In this paper we describe GridVideo, a Grid-based multimedia application for the distributed tailoring and streaming of media files. The objective is to show, starting from a real experience, how Grid technologies can be used for the development of non-scientific applications. Relevant performance aspects are analyzed, regarding both user-oriented (in terms of responsiveness) and provider-oriented (in terms of system efficiency) requirements. Different multimedia data dissemination strategies have been analyzed and an innovative technique, based on the Fibonacci series, is proposed. To respond to the stringent Quality of Service (QoS) requirements, typical of soft real-time applications, a reservation-based architecture is presented. Such architecture is able to manage the Grid resource allocation, thus enabling the provisioning of advanced services with different QoS levels. Technical and practical problems encountered during the development are discussed and a thorough performance evaluation of the developed prototype is presented.

#index 1189220
#* Hierarchically Distributed Peer-to-Peer Document Clustering and Cluster Summarization
#@ Khaled M. Hammouda;Mohamed S. Kamel
#t 2009
#c 7
#! In distributed data mining, adopting a flat node distribution model can affect scalability. To address the problem of modularity, flexibility and scalability, we propose a Hierarchically-distributed Peer-to-Peer (HP2PC) architecture and clustering algorithm. The architecture is based on a multi-layer overlay network of peer neighborhoods. Supernodes, which act as representatives of neighborhoods, are recursively grouped to form higher level neighborhoods. Within a certain level of the hierarchy, peers cooperate within their respective neighborhoods to perform P2P clustering. Using this model, we can partition the clustering problem in a modular way across neighborhoods, solve each part individually using a distributed K-means variant, then successively combine clusterings up the hierarchy where increasingly more global solutions are computed. In addition, for document clustering applications, we summarize the distributed document clusters using a distributed keyphrase extraction algorithm, thus providing interpretation of the clusters. Results show decent speedup, reaching 165 times faster than centralized clustering for a 250-node simulated network, with comparable clustering quality to the centralized approach. We also provide comparison to the P2P K-means algorithm and show that HP2PC accuracy is better for typical hierarchy heights. Results for distributed cluster summarization match those of their centralized counterparts with up to 88% accuracy.

#index 1189221
#* Exact Knowledge Hiding through Database Extension
#@ Aris Gkoulalas-Divanis;Vassilios S. Verykios
#t 2009
#c 7
#! In this paper, we propose a novel, exact border-based approach that provides an optimal solution for the hiding of sensitive frequent itemsets by (i) minimally extending the original database by a synthetically generated database part - the database extension, (ii) formulating the creation of the database extension as a constraint satisfaction problem, (iii) mapping the constraint satisfaction problem to an equivalent binary integer programming problem, (iv) exploiting underutilized synthetic transactions to proportionally increase the support of non-sensitive itemsets, (v) minimally relaxing the constraint satisfaction problem to provide an approximate solution close to the optimal one when an ideal solution does not exist, and (vi) by using a partitioning in the universe of the items to increase the efficiency of the proposed hiding algorithm. Extending the original database for sensitive itemset hiding is proved to provide optimal solutions to an extended set of hiding problems compared to previous approaches and to provide solutions of higher quality. Moreover, the application of binary integer programming enables the simultaneous hiding of the sensitive itemsets and thus allows for the identification of globally optimal solutions.

#index 1189222
#* GLIP: A Concurrency Control Protocol for Clipping Indexing
#@ Chang-Tien Lu;Jing Dai;Ying Jin;Janak Mathuria
#t 2009
#c 7
#! Multidimensional databases are now beginning to be used in a wide range of applications. To meet this fast-growing demand, the R-tree family is being applied to support fast access to multidimensional data, for which the R+-tree exhibits outstanding search performance. In order to support efficient concurrent access in multi-user environments, concurrency control mechanisms for multidimensional indexing have been proposed. However, these mechanisms cannot be directly applied to the R+-tree because an object in the R+-tree may be indexed in multiple leaves. This paper proposes a concurrency control protocol for R-tree variants with object clipping, namely, Granular Locking for clIPping indexing (GLIP), dubbed an R+-tree variant, the Zero-overlap R+-tree (ZR+-tree). To the best of our knowledge, GLIP is the first concurrency control approach designed specifically for the R+-tree and its variants. The proposed GLIP supports efficient concurrent operations on R+-trees with serializable isolation, consistency, and deadlock-free. Experiment results on both real and synthetic data sets validated the effectiveness and efficiency of the proposed concurrent access framework.

#index 1189223
#* Fast Query Point Movement Techniques for Large CBIR Systems
#@ Danzhou Liu;Kien A. Hua;Khanh Vu;Ning Yu
#t 2009
#c 7
#! Target search in content-based image retrieval (CBIR) systems refers to finding a specific (target) image such as a particular registered logo or a specific historical photograph. Existing techniques, designed around query refinement based on relevance feedback, suffer from slow convergence, and do not guarantee to find intended targets. To address these limitations, we propose several efficient query point movement methods. We prove that our approach is able to reach any given target image with fewer iterations in the worst and average cases. We propose a new index structure and query processing technique to improve retrieval effectiveness and efficiency. We also consider strategies to minimize the effects of users' inaccurate relevance feedback. Extensive experiments in simulated and realistic environments show that our approach significantly reduces the number of required iterations and improves overall retrieval performance. The experimental results also confirm that our approach can always retrieve intended targets even with poor selection of initial query points.

#index 1189224
#* Schema Vacuuming in Temporal Databases
#@ John F. Roddick
#t 2009
#c 7
#! Temporal databases facilitate the support of historical information by providing functions for indicating the intervals during which a tuple was applicable (along one or more temporal dimensions). Because data are never deleted, only superceded, temporal databases are inherently append-only resulting, over time, in a large historical sequence of database states. Data vacuuming in temporal databases allows for this sequence to be shortened by strategically, and irrevocably, deleting obsolete data. Schema versioning allows users to maintain a history of database schemata without compromising the semantics of the data or the ability to view data through historical schemata. While the techniques required for data vacuuming in temporal databases have been relatively well covered, the associated area of vacuuming schemata has received less attention. This paper discusses this issue and proposes a mechanism that fits well with existing methods for data vacuuming and schema versioning.

#index 1189225
#* The Subgraph Similarity Problem
#@ Lorenzo De Nardo;Francesco Ranzato;Francesco Tapparo
#t 2009
#c 7
#! Similarity is a well known weakening of bisimilarity where one system is required to simulate the other and vice versa. It has been shown that the subgraph bisimilarity problem, a variation of the subgraph isomorphism problem where isomorphism is weakened to bisimilarity, is NP-complete. We show that the subgraph similarity problem and some related variations thereof still remain NP-complete.

#index 1209653
#* Guest Editors' Introduction: Knowledge and Data Engineering for E-Learning
#@ Qing Li;Rynson W. H. Lau;Dennis McLeod;Jiming Liu
#t 2009
#c 7
#! The 13 papers in this special issue focus on knowledge and data engineering for e-learning. Some of these papers were recommended submissions from the best ranked papers presented at the Sixth International Conference on Web-Based Learning (ICWL '07), held in August 2007 in Edinburgh, United Kingdom.

#index 1209654
#* Clustering and Sequential Pattern Mining of Online Collaborative Learning Data
#@ Dilhan Perera;Judy Kay;Irena Koprinska;Kalina Yacef;Osmar R. Zaïane
#t 2009
#c 7
#! Group work is widespread in education. The growing use of online tools supporting group work generates huge amounts of data. We aim to exploit this data to support mirroring: presenting useful high-level views of information about the group, together with desired patterns characterizing the behavior of strong groups. The goal is to enable the groups and their facilitators to see relevant aspects of the group's operation and provide feedback if these are more likely to be associated with positive or negative outcomes and indicate where the problems are. We explore how useful mirror information can be extracted via a theory-driven approach and a range of clustering and sequential pattern mining. The context is a senior software development project where students use the collaboration tool TRAC. We extract patterns distinguishing the better from the weaker groups and get insights in the success factors. The results point to the importance of leadership and group interaction, and give promising indications if they are occurring. Patterns indicating good individual practices were also identified. We found that some key measures can be mined from early data. The results are promising for advising groups at the start and early identification of effective and poor practices, in time for remediation.

#index 1209655
#* Monitoring Online Tests through Data Visualization
#@ Gennaro Costagliola;Vittorio Fuccella;Massimiliano Giordano;Giuseppe Polese
#t 2009
#c 7
#! We present an approach and a system to let tutors monitor several important aspects related to online tests, such as learner behavior and test quality. The approach includes the logging of important data related to learner interaction with the system during the execution of online tests and exploits data visualization to highlight information useful to let tutors review and improve the whole assessment process. We have focused on the discovery of behavioral patterns of learners and conceptual relationships among test items. Furthermore, we have led several experiments in our faculty in order to assess the whole approach. In particular, by analyzing the data visualization charts, we have detected several previously unknown test strategies used by the learners. Last, we have detected several correlations among questions, which gave us useful feedbacks on the test quality.

#index 1209656
#* Communities and Emerging Semantics in Semantic Link Network: Discovery and Learning
#@ Hai Zhuge
#t 2009
#c 7
#! The World Wide Web provides plentiful contents for Web-based learning, but its hyperlink-based architecture connects Web resources for browsing freely rather than for effective learning. To support effective learning, an e-learning system should be able to discover and make use of the semantic communities and the emerging semantic relations in a dynamic complex network of learning resources. Previous graph-based community discovery approaches are limited in ability to discover semantic communities. This paper first suggests the Semantic Link Network (SLN), a loosely coupled semantic data model that can semantically link resources and derive out implicit semantic links according to a set of relational reasoning rules. By studying the intrinsic relationship between semantic communities and the semantic space of SLN, approaches to discovering reasoning-constraint, rule-constraint, and classification-constraint semantic communities are proposed. Further, the approaches, principles, and strategies for discovering emerging semantics in dynamic SLNs are studied. The basic laws of the semantic link network motion are revealed for the first time. An e-learning environment incorporating the proposed approaches, principles, and strategies to support effective discovery and learning is suggested.

#index 1209657
#* Toward a Fuzzy Domain Ontology Extraction Method for Adaptive e-Learning
#@ Raymond Y. K. Lau;Dawei Song;Yuefeng Li;Terence C. H. Cheung;Jin-Xing Hao
#t 2009
#c 7
#! With the widespread applications of electronic learning (e-Learning) technologies to education at all levels, increasing number of online educational resources and messages are generated from the corresponding e-Learning environments. Nevertheless, it is quite difficult, if not totally impossible, for instructors to read through and analyze the online messages to predict the progress of their students on the fly. The main contribution of this paper is the illustration of a novel concept map generation mechanism which is underpinned by a fuzzy domain ontology extraction algorithm. The proposed mechanism can automatically construct concept maps based on the messages posted to online discussion forums. By browsing the concept maps, instructors can quickly identify the progress of their students and adjust the pedagogical sequence on the fly. Our initial experimental results reveal that the accuracy and the quality of the automatically generated concept maps are promising. Our research work opens the door to the development and application of intelligent software tools to enhance e-Learning.

#index 1209658
#* Open Smart Classroom: Extensible and Scalable Learning System in Smart Space Using Web Service Technology
#@ Yue Suo;Naoki Miyata;Hiroki Morikawa;Toru Ishida;Yuanchun Shi
#t 2009
#c 7
#! Real-time interactive virtual classroom with teleeducation experience is an important approach in distance learning. However, most current systems fail to meet new challenges in extensibility and scalability, which mainly lie with three issues. First, an open system architecture is required to better support the integration of increasing human-computer interfaces and personal mobile devices in the classroom. Second, the learning system should facilitate opening its interfaces, which will help easy deployment that copes with different circumstances and allows other learning systems to talk to each other. Third, problems emerge on binding existing systems of classrooms together in different places or even different countries such as tackling systems intercommunication and distant intercultural learning in different languages. To address these issues, we build a prototype application called Open Smart Classroom built on our software infrastructure based on the multiagent system architecture using Web Service technology in Smart Space. Besides the evaluation of the extensibility and scalability of the system, an experiment connecting two Open Smart Classrooms deployed in different countries is also undertaken, which demonstrates the influence of these new features on the educational effect. Interesting and optimistic results obtained show a significant research prospect for developing future distant learning systems.

#index 1209659
#* NNexus: An Automatic Linker for Collaborative Web-Based Corpora
#@ James Gardner;Aaron Krowne;Li Xiong
#t 2009
#c 7
#! In this paper, we introduce Noosphere Networked Entry eXtension and Unification System (NNexus), a generalization of the automatic linking engine of Noosphere (at PlanetMath.org) and the first system that automates the process of linking disparate "encyclopedia” entries into a fully connected conceptual network. The main challenges of this problem space include: 1) linking quality (correctly identifying which terms to link and which entry to link to with minimal effort on the part of users), 2) efficiency and scalability, and 3) generalization to multiple knowledge bases and web-based information environment. We present the NNexus approach that utilizes subject classification and other metadata to address these challenges. We also present evaluation results demonstrating the effectiveness and efficiency of the approach and discuss ongoing and future directions of research.

#index 1209660
#* Effective Collaboration with Information Sharing in Virtual Universities
#@ Hua Wang;Yanchun Zhang;Jinli Cao
#t 2009
#c 7
#! A global education system, as a key area in future IT, has fostered developers to provide various learning systems with low cost. While a variety of e-learning advantages has been recognized for a long time and many advances in e-learning systems have been implemented, the needs for effective information sharing in a secure manner have to date been largely ignored, especially for virtual university collaborative environments. Information sharing of virtual universities usually occurs in broad, highly dynamic network-based environments, and formally accessing the resources in a secure manner poses a difficult and vital challenge. This paper aims to build a new rule-based framework to identify and address issues of sharing in virtual university environments through role-based access control (RBAC) management. The framework includes a role-based group delegation granting model, group delegation revocation model, authorization granting, and authorization revocation. We analyze various revocations and the impact of revocations on role hierarchies. The implementation with XML-based tools demonstrates the feasibility of the framework and authorization methods. Finally, the current proposal is compared with other related work.

#index 1209661
#* Interactive Correction and Recommendation for Computer Language Learning and Training
#@ Claus Pahl;Claire Kenny
#t 2009
#c 7
#! Active learning and training is a particularly effective form of education. In various domains, skills are equally important to knowledge. We present an automated learning and skills training system for a database programming environment that promotes procedural knowledge acquisition and skills training. The system provides meaningful knowledge-level feedback such as correction of student solutions and personalized guidance through recommendations. Specifically, we address automated synchronous feedback and recommendations based on personalized performance assessment. At the core of the tutoring system is a pattern-based error classification and correction component that analyzes student input in order to provide immediate feedback and in order to diagnose student weaknesses and suggest further study material. A syntax-driven approach based on grammars and syntax trees provides the solution for a semantic analysis technique. Syntax tree abstractions and comparison techniques based on equivalence rules and pattern matching are specific approaches.

#index 1209662
#* Subontology-Based Resource Management for Web-Based e-Learning
#@ Zhaohui Wu;Yuxin Mao;Huajun Chen
#t 2009
#c 7
#! Recent advances in Web and information technologies have resulted in many e-learning resources. There is an emerging requirement to manage and reuse relevant resources together to achieve on-demand e-learning in the Web. Ontologies have become a key technology for enabling semantic-driven resource management. We argue that to meet the requirements of semantic-based resource management for Web-based e-learning, one should go beyond using domain ontologies statically. In this paper, we provide a semantic mapping mechanism to integrate e-learning databases by using ontology semantics. Heterogeneous e-learning databases can be integrated under a mediated ontology. Taking into account the locality of resource reuse, we propose to represent context-specific portions from the whole ontology as subontologies. We present a subontology-based approach for resource reuse by using an evolutionary algorithm. We also conduct simulation experiments to evaluate the approach with a traditional Chinese medicine e--learning scenario and obtain promising results.

#index 1209663
#* Enhancing Learning Objects with an Ontology-Based Memory
#@ Amal Zouaq;Roger Nkambou
#t 2009
#c 7
#! The reusability in learning objects has always been a hot issue. However, we believe that current approaches to e-Learning failed to find a satisfying answer to this concern. This paper presents an approach that enables capitalization of existing learning resources by first creating "content metadata” through text mining and natural language processing and second by creating dynamically learning knowledge objects, i.e., active, adaptable, reusable, and independent learning objects. The proposed model also suggests integrating explicitly instructional theories in an on-the-fly composition process of learning objects. Semantic Web technologies are used to satisfy such an objective by creating an ontology-based organizational memory able to act as a knowledge base for multiple training environments.

#index 1209664
#* Providing Flexible Process Support to Project-Centered Learning
#@ Stefano Ceri;Florian Daniel;Maristella Matera;Alessandro Raffio
#t 2009
#c 7
#! While business process definition is becoming more and more popular as an instrument for describing human activities, there is a growing need for software tools supporting business process abstractions to help users organize and monitor their desktop work. Tools are most effective when they embed some knowledge about the process, e.g., in terms of the typical activities required by the process, so that users can execute the activities without having to define them. Tools must be lightweight and flexible, so as to enable users to create or change the process as soon as there is a new need. In this article, we first describe an application-independent approach to flexible process support by discussing the abstractions required for modeling, creating, enacting, and modifying flexible processes. Then, we show our approach at work in the context of project-centered learning. In this application, learners are challenged to perform concrete tasks in order to master specific subjects; in doing so, they have to conduct significant projects and cope with realistic (or even real-life) working conditions and scenarios. Often, students are geographically dispersed or under severe timing constraints, because these activities intertwine with their normal university activity. As a result, they need communication technology in order to interact and workflow technology in order to organize their work. The developed platform provides a comprehensible, e-learning-specific set of activities and process templates, which can be combined through a simple Web interface into project-centered collaboration processes. We discuss how the general paradigm of flexible processes was adapted to the learning concept, implemented, and experienced by students.

#index 1209665
#* Learning in an Ambient Intelligent World: Enabling Technologies and Practices
#@ Xiang Li;Ling Feng;Lizhu Zhou;Yuanchun Shi
#t 2009
#c 7
#! The rapid evolution of information and communication technology opens a wide spectrum of opportunities to change our surroundings into an Ambient Intelligent (AmI) world. AmI is a vision of future information society, where people are surrounded by a digital environment that is sensitive to their needs, personalized to their requirements, anticipatory of their behavior, and responsive to their presence. It emphasizes on greater user friendliness, user empowerment, and more effective service support, with an aim to bring information and communication technology to everyone, every home, every business, and every school, thus improving the quality of human life. AmI unprecedentedly enhances learning experiences by endowing the users with the opportunities of learning in context, a breakthrough from the traditional education settings. In this survey paper, we examine some major characteristics of an AmI learning environment. To deliver a feasible and effective solution to ambient learning, we overview a few latest developed enabling technologies in context awareness and interactive learning. Associated practices are meanwhile reported. We also describe our experience in designing and implementing a smart class prototype, which allows teachers to simultaneously instruct both local and remote students in a context-aware and natural way.

#index 1209666
#* An Implementation of the CORDRA Architecture Enhanced for Systematic Reuse of Learning Objects
#@ Freya H. Lin;Timothy K. Shih;Won Kim
#t 2009
#c 7
#! The Sharable Content Object Reference Model (SCORM) specification defines metadata of learning objects, which are used as the elementary reusable components in distance learning. The Content Object Repository Discovery and Registration/Resolution Architecture (CORDRA) specification provides a common architecture for the resolution, discovery, and sharing of these learning objects. These two specifications together define standardized ways in which learning objects can be discovered and reused by content designers. However, the current CORDRA and the definition of objects in SCORM only allow an object to be copied, updated, and reorganized in a new content aggregation, which is used as a delivery package to end users. This paper proposes a revised CORDRA architecture and a reusability mechanism to make instruction design easier. In particular, it proposes a structure called a reusability tree for tracking the history of reuse of learning objects in CORDRA. This paper also defines the notions of similarity, diversity, and relevancy of learning objects to make it easier for users to precisely search for and reuse learning objects.

#index 1209667
#* On the Expressive Power of the Relational Algebra on Finite Sets of Relation Pairs
#@ George H. L. Fletcher;Marc Gyssens;Jan Paredaens;Dirk Van Gucht
#t 2009
#c 7
#! We give a language-independent characterization of the expressive power of the relational algebra on finite sets of source-target relation instance pairs. The associated decision problem is shown to be cograph-isomorphism hard and in coNP. The main result is also applied in providing a new characterization of the generic relational queries.

#index 1246151
#* Efficient Skyline Computation in Structured Peer-to-Peer Systems
#@ Bin Cui;Lijiang Chen;Linhao Xu;Hua Lu;Guojie Song;Quanqing Xu
#t 2009
#c 7
#! An increasing number of large-scale applications exploit peer-to-peer network architecture to provide highly scalable and flexible services. Among these applications, data management in peer-to-peer systems is one of the interesting domains. In this paper, we investigate the multidimensional skyline computation problem on a structured peer-to-peer network. In order to achieve low communication cost and quick response time, we utilize the iMinMax(\theta ) method to transform high-dimensional data to one-dimensional value and distribute the data in a structured peer-to-peer network called BATON. Thereafter, we propose a progressive algorithm with adaptive filter technique for efficient skyline computation in this environment. We further discuss some optimization techniques for the algorithm, and summarize the key principles of our algorithm into a query routing protocol with detailed analysis. Finally, we conduct an extensive experimental evaluation to demonstrate the efficiency of our approach.

#index 1246152
#* ANGEL: Enhancing the Utility of Generalization for Privacy Preserving Publication
#@ Yufei Tao;Hekang Chen;Xiaokui Xiao;Shuigeng Zhou;Donghui Zhang
#t 2009
#c 7
#! Generalization is a well-known method for privacy preserving data publication. Despite its vast popularity, it has several drawbacks such as heavy information loss, difficulty of supporting marginal publication, and so on. To overcome these drawbacks, we develop ANGEL,1 a new anonymization technique that is as effective as generalization in privacy protection, but is able to retain significantly more information in the microdata. ANGEL is applicable to any monotonic principles (e.g., l-diversity, t-closeness, etc.), with its superiority (in correlation preservation) especially obvious when tight privacy control must be enforced. We show that ANGEL lends itself elegantly to the hard problem of marginal publication. In particular, unlike generalization that can release only restricted marginals, our technique can be easily used to publish any marginals with strong privacy guarantees.

#index 1246153
#* A Divide-and-Conquer Approach for Minimum Spanning Tree-Based Clustering
#@ Xiaochun Wang;Xiali Wang;D. Mitchell Wilkes
#t 2009
#c 7
#! Due to their ability to detect clusters with irregular boundaries, minimum spanning tree-based clustering algorithms have been widely used in practice. However, in such clustering algorithms, the search for nearest neighbor in the construction of minimum spanning trees is the main source of computation and the standard solutions take O(N^{2}) time. In this paper, we present a fast minimum spanning tree-inspired clustering algorithm, which, by using an efficient implementation of the cut and the cycle property of the minimum spanning trees, can have much better performance than O(N^{2}).

#index 1246154
#* Determining Attributes to Maximize Visibility of Objects
#@ Muhammed Miah;Gautam Das;Vagelis Hristidis;Heikki Mannila
#t 2009
#c 7
#! In recent years, there has been significant interest in the development of ranking functions and efficient top-k retrieval algorithms to help users in ad hoc search and retrieval in databases (e.g., buyers searching for products in a catalog). We introduce a complementary problem: How to guide a seller in selecting the best attributes of a new tuple (e.g., a new product) to highlight so that it stands out in the crowd of existing competitive products and is widely visible to the pool of potential buyers. We develop several formulations of this problem. Although the problems are NP-complete, we give several exact and approximation algorithms that work well in practice. One type of exact algorithms is based on Integer Programming (IP) formulations of the problems. Another class of exact methods is based on maximal frequent item set mining algorithms. The approximation algorithms are based on greedy heuristics. A detailed performance study illustrates the benefits of our methods on real and synthetic data.

#index 1246155
#* Discovery of Structural and Functional Features in RNA Pseudoknots
#@ Qingfeng Chen;Yi-Ping Phoebe Chen
#t 2009
#c 7
#! An RNA pseudoknot consists of nonnested double-stranded stems connected by single-stranded loops. There is increasing recognition that RNA pseudoknots are one of the most prevalent RNA structures and fulfill a diverse set of biological roles within cells, and there is an expanding rate of studies into RNA pseudoknotted structures as well as increasing allocation of function. These not only produce valuable structural data but also facilitate an understanding of structural and functional characteristics in RNA molecules. PseudoBase is a database providing structural, functional, and sequence data related to RNA pseudoknots. To capture the features of RNA pseudoknots, we present a novel framework using quantitative association rule mining to analyze the pseudoknot data. The derived rules are classified into specified association groups regarding structure, function, and category of RNA pseudoknots. The discovered association rules assist biologists in filtering out significant knowledge of structure-function and structure-category relationships. A brief biological interpretation to the relationships is presented, and their potential correlations with each other are highlighted.

#index 1246156
#* Predicting Missing Items in Shopping Carts
#@ Kasun Wickramaratna;Miroslav Kubat;Kamal Premaratne
#t 2009
#c 7
#! Existing research in association mining has focused mainly on how to expedite the search for frequently co-occurring groups of items in “shopping cart” type of transactions; less attention has been paid to methods that exploit these “frequent itemsets” for prediction purposes. This paper contributes to the latter task by proposing a technique that uses partial information about the contents of a shopping cart for the prediction of what else the customer is likely to buy. Using the recently proposed data structure of itemset trees (IT-trees), we obtain, in a computationally efficient manner, all rules whose antecedents contain at least one item from the incomplete shopping cart. Then, we combine these rules by uncertainty processing techniques, including the classical Bayesian decision theory and a new algorithm based on the Dempster-Shafer (DS) theory of evidence combination.

#index 1246157
#* Predictive Ensemble Pruning by Expectation Propagation
#@ Huanhuan Chen;Peter Tiňo;Xin Yao
#t 2009
#c 7
#! An ensemble is a group of learners that work together as a committee to solve a problem. The existing ensemble learning algorithms often generate unnecessarily large ensembles, which consume extra computational resource and may degrade the generalization performance. Ensemble pruning algorithms aim to find a good subset of ensemble members to constitute a small ensemble, which saves the computational resource and performs as well as, or better than, the unpruned ensemble. This paper introduces a probabilistic ensemble pruning algorithm by choosing a set of “sparse” combination weights, most of which are zeros, to prune the ensemble. In order to obtain the set of sparse combination weights and satisfy the nonnegative constraint of the combination weights, a left-truncated, nonnegative, Gaussian prior is adopted over every combination weight. Expectation propagation (EP) algorithm is employed to approximate the posterior estimation of the weight vector. The leave-one-out (LOO) error can be obtained as a by-product in the training of EP without extra computation and is a good indication for the generalization error. Therefore, the LOO error is used together with the Bayesian evidence for model selection in this algorithm. An empirical study on several regression and classification benchmark data sets shows that our algorithm utilizes far less component learners but performs as well as, or better than, the unpruned ensemble. Our results are very competitive compared with other ensemble pruning algorithms.

#index 1246158
#* Rough Cluster Quality Index Based on Decision Theory
#@ Pawan Lingras;Min Chen;Duoqian Miao
#t 2009
#c 7
#! Quality of clustering is an important issue in application of clustering techniques. Most traditional cluster validity indices are geometry-based cluster quality measures. This paper proposes a cluster validity index based on the decision-theoretic rough set model by considering various loss functions. Experiments with synthetic, standard, and real-world retail data show the usefulness of the proposed validity index for the evaluation of rough and crisp clustering. The measure is shown to help determine optimal number of clusters, as well as an important parameter called threshold in rough clustering. The experiments with a promotional campaign for the retail data illustrate the ability of the proposed measure to incorporate financial considerations in evaluating quality of a clustering scheme. This ability to deal with monetary values distinguishes the proposed decision-theoretic measure from other distance-based measures. The proposed validity index can also be extended for evaluating other clustering algorithms such as fuzzy clustering.

#index 1246159
#* A Communication Perspective on Automatic Text Categorization
#@ Marta Capdevila;Oscar W. Marquez Florez
#t 2009
#c 7
#! The basic concern of a Communication System is to transfer information from its source to a destination some distance away. Textual documents also deal with the transmission of information. Particularly, from a text categorization system point of view, the information encoded by a document is the topic or category it belongs to. Following this initial intuition, a theoretical framework is developed where Automatic Text Categorization (ATC) is studied under a Communication System perspective. Under this approach, the problematic indexing feature space dimensionality reduction has been tackled by a two-level supervised scheme, implemented by a noisy terms filtering and a subsequent redundant terms compression. Gaussian probabilistic categorizers have been revisited and adapted to the concomitance of sparsity in ATC. Experimental results pertaining to 20 Newsgroups and Reuters-21578 collections validate the theoretical approaches. The noise filter and redundancy compressor allows an aggressive term vocabulary reduction (reduction factor greater than 0.99) with a minimum loss (lower than 3 percent) and, in some cases, gain (greater than 4 percent) of final classification accuracy. The adapted Gaussian Naive Bayes classifier reaches classification results similar to those obtained by state-of-the-art Multinomial Naive Bayes (MNB) and Support Vector Machines (SVMs).

#index 1246160
#* Active Integrity Constraints for Database Consistency Maintenance
#@ Luciano Caroprese;Sergio Greco;Ester Zumpano
#t 2009
#c 7
#! This paper introduces active integrity constraints (AICs), an extension of integrity constraints for consistent database maintenance. An active integrity constraint is a special constraint whose body contains a conjunction of literals which must be false and whose head contains a disjunction of update actions representing actions (insertions and deletions of tuples) to be performed if the constraint is not satisfied (that is its body is true). The AICs work in a domino-like manner as the satisfaction of one AIC may trigger the violation and therefore the activation of another one. The paper also introduces founded repairs, which are minimal sets of update actions that make the database consistent, and are specified and “supported” by active integrity constraints. The paper presents: 1) a formal declarative semantics allowing the computation of founded repairs and 2) a characterization of this semantics obtained by rewriting active integrity constraints into disjunctive logic rules, so that founded repairs can be derived from the answer sets of the derived logic program. Finally, the paper studies the computational complexity of computing founded repairs.

#index 1246161
#* Lanczos Vectors versus Singular Vectors for Effective Dimension Reduction
#@ Jie Chen;Yousef Saad
#t 2009
#c 7
#! This paper takes an in-depth look at a technique for computing filtered matrix-vector (mat-vec) products which are required in many data analysis applications. In these applications, the data matrix is multiplied by a vector and we wish to perform this product accurately in the space spanned by a few of the major singular vectors of the matrix. We examine the use of the Lanczos algorithm for this purpose. The goal of the method is identical with that of the truncated singular value decomposition (SVD), namely to preserve the quality of the resulting mat-vec product in the major singular directions of the matrix. The Lanczos-based approach achieves this goal by using a small number of Lanczos vectors, but it does not explicitly compute singular values/vectors of the matrix. The main advantage of the Lanczos-based technique is its low cost when compared with that of the truncated SVD. This advantage comes without sacrificing accuracy. The effectiveness of this approach is demonstrated on a few sample applications requiring dimension reduction, including information retrieval and face recognition. The proposed technique can be applied as a replacement to the truncated SVD technique whenever the problem can be formulated as a filtered mat-vec multiplication.

#index 1246162
#* Multirelational k-Anonymity
#@ Mehmet Ercan Nergiz;Christopher Clifton;Ahmet Erhan Nergiz
#t 2009
#c 7
#! k-Anonymity protects privacy by ensuring that data cannot be linked to a single individual. In a k-anonymous data set, any identifying information occurs in at least k tuples. Much research has been done to modify a single-table data set to satisfy anonymity constraints. This paper extends the definitions of k-anonymity to multiple relations and shows that previously proposed methodologies either fail to protect privacy or overly reduce the utility of the data in a multiple relation setting. We also propose two new clustering algorithms to achieve multirelational anonymity. Experiments show the effectiveness of the approach in terms of utility and efficiency.

#index 1246163
#* Olex: Effective Rule Learning for Text Categorization
#@ Pasquale Rullo;Veronica Lucia Policicchio;Chiara Cumbo;Salvatore Iiritano
#t 2009
#c 7
#! This paper describes Olex, a novel method for the automatic induction of rule-based text classifiers. Olex supports a hypothesis language of the form "if T_{1} or \cdots or T_{n} occurs in document d, and none of T_{n + 1}, \ldots T_{n + m} occurs in d, then classify d under category c,” where each T_{i} is a conjunction of terms. The proposed method is simple and elegant. Despite this, the results of a systematic experimentation performed on the Reuters-21578, the Ohsumed, and the ODP data collections show that Olex provides classifiers that are accurate, compact, and comprehensible. A comparative analysis conducted against some of the most well-known learning algorithms (namely, Naive Bayes, Ripper, C4.5, SVM, and Linear Logistic Regression) demonstrates that it is more than competitive in terms of both predictive accuracy and efficiency.

#index 1246164
#* Ranking and Suggesting Popular Items
#@ Milan Vojnovi&#263//;James Cruise;Dinan Gunawardena;Peter Marbach
#t 2009
#c 7
#! We consider the problem of ranking the popularity of items and suggesting popular items based on user feedback. User feedback is obtained by iteratively presenting a set of suggested items, and users selecting items based on their own preferences either from this suggestion set or from the set of all possible items. The goal is to quickly learn the true popularity ranking of items (unbiased by the made suggestions), and suggest true popular items. The difficulty is that making suggestions to users can reinforce popularity of some items and distort the resulting item ranking. The described problem of ranking and suggesting items arises in diverse applications including search query suggestions and tag suggestions for social tagging systems. We propose and study several algorithms for ranking and suggesting popular items, provide analytical results on their performance, and present numerical results obtained using the inferred popularity of tags from a month-long crawl of a popular social bookmarking service. Our results suggest that lightweight, randomized update rules that require no special configuration parameters provide good performance.

#index 1246165
#* Similarity-Profiled Temporal Association Mining
#@ Jin Soung Yoo;Shashi Shekhar
#t 2009
#c 7
#! Given a time stamped transaction database and a user-defined reference sequence of interest over time, similarity-profiled temporal association mining discovers all associated item sets whose prevalence variations over time are similar to the reference sequence. The similar temporal association patterns can reveal interesting relationships of data items which co-occur with a particular event over time. Most works in temporal association mining have focused on capturing special temporal regulation patterns such as cyclic patterns and calendar scheme-based patterns. However, our model is flexible in representing interesting temporal patterns using a user-defined reference sequence. The dissimilarity degree of the sequence of support values of an item set to the reference sequence is used to capture how well its temporal prevalence variation matches the reference pattern. By exploiting interesting properties such as an envelope of support time sequence and a lower bounding distance for early pruning candidate item sets, we develop an algorithm for effectively mining similarity-profiled temporal association patterns. We prove the algorithm is correct and complete in the mining results and provide the computational analysis. Experimental results on real data as well as synthetic data show that the proposed algorithm is more efficient than a sequential method using a traditional support-pruning scheme.

#index 1246166
#* Optimal-Location-Selection Query Processing in Spatial Databases
#@ Yunjun Gao;Baihua Zheng;Gencai Chen;Qing Li
#t 2009
#c 7
#! This paper introduces and solves a novel type of spatial queries, namely, Optimal-Location-Selection (OLS) search, which has many applications in real life. Given a data object set D_A, a target object set D_B, a spatial region R, and a critical distance d_c in a multidimensional space, an OLS query retrieves those target objects in D_B that are outside R but have maximal optimality. Here, the optimality of a target object b \in D_B located outside R is defined as the number of the data objects from D_A that are inside R and meanwhile have their distances to b not exceeding d_c. When there is a tie, the accumulated distance from the data objects to b serves as the tie breaker, and the one with smaller distance has the better optimality. In this paper, we present the optimality metric, formalize the OLS query, and propose several algorithms for processing OLS queries efficiently. A comprehensive experimental evaluation has been conducted using both real and synthetic data sets to demonstrate the efficiency and effectiveness of the proposed algorithms.

#index 1246167
#* Evaluating the Effectiveness of Personalized Web Search
#@ Zhicheng Dou;Ruihua Song;Ji-Rong Wen;Xiaojie Yuan
#t 2009
#c 7
#! Although personalized search has been under way for many years and many personalization algorithms have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users and under different search contexts. In this paper, we study this problem and provide some findings. We present a large-scale evaluation framework for personalized search based on query logs and then evaluate five personalized search algorithms (including two click-based ones and three topical-interest-based ones) using 12-day query logs of Windows Live Search. By analyzing the results, we reveal that personalized Web search does not work equally well under various situations. It represents a significant improvement over generic Web search for some queries, while it has little effect and even harms query performance under some situations. We propose click entropy as a simple measurement on whether a query should be personalized. We further propose several features to automatically predict when a query will benefit from a specific personalization algorithm. Experimental results show that using a personalization algorithm for queries selected by our prediction model is better than using it simply for all queries.

#index 1246168
#* Extracting Takagi-Sugeno Fuzzy Rules with Interpretable Submodels via Regularization of Linguistic Modifiers
#@ Shang-Ming Zhou;John Q. Gan
#t 2009
#c 7
#! In this paper, a method for constructing Takagi-Sugeno (TS) fuzzy system from data is proposed with the objective of preserving TS submodel comprehensibility, in which linguistic modifiers are suggested to characterize the fuzzy sets. A good property held by the proposed linguistic modifiers is that they can broaden the cores of fuzzy sets while contracting the overlaps of adjoining membership functions (MFs) during identification of fuzzy systems from data. As a result, the TS submodels identified tend to dominate the system behaviors by automatically matching the global model (GM) in corresponding subareas, which leads to good TS model interpretability while producing distinguishable input space partitioning. However, the GM accuracy and model interpretability are two conflicting modeling objectives, improving interpretability of fuzzy models generally degrades the GM performance of fuzzy models, and vice versa. Hence, one challenging problem is how to construct a TS fuzzy model with not only good global performance but also good submodel interpretability. In order to achieve a good tradeoff between GM performance and submodel interpretability, a regularization learning algorithm is presented in which the GM objective function is combined with a local model objective function defined in terms of an extended index of fuzziness of identified MFs. Moreover, a parsimonious rule base is obtained by adopting a QR decomposition method to select the important fuzzy rules and reduce the redundant ones. Experimental studies have shown that the TS models identified by the suggested method possess good submodel interpretability and satisfactory GM performance with parsimonious rule bases.

#index 1246169
#* Development of a Software Engineering Ontology for Multisite Software Development
#@ Pornpit Wongthongtham;Elizabeth Chang;Tharam Dillon;Ian Sommerville
#t 2009
#c 7
#! This paper aims to present an ontology model of software engineering to represent its knowledge. The fundamental knowledge relating to software engineering is well described in the textbook entitled Software Engineering by Sommerville that is now in its eighth edition [1] and the white paper, Software Engineering Body of Knowledge (SWEBOK), by the IEEE [2] upon which software engineering ontology is based. This paper gives an analysis of what software engineering ontology is, what it consists of, and what it is used for in the form of usage example scenarios. The usage scenarios presented in this paper highlight the characteristics of the software engineering ontology. The software engineering ontology assists in defining information for the exchange of semantic project information and is used as a communication framework. Its users are software engineers sharing domain knowledge as well as instance knowledge of software engineering.

#index 1246170
#* RiMOM: A Dynamic Multistrategy Ontology Alignment Framework
#@ Juanzi Li;Jie Tang;Yi Li;Qiong Luo
#t 2009
#c 7
#! Ontology alignment identifies semantically matching entities in different ontologies. Various ontology alignment strategies have been proposed; however, few systems have explored how to automatically combine multiple strategies to improve the matching effectiveness. This paper presents a dynamic multistrategy ontology alignment framework, named RiMOM. The key insight in this framework is that similarity characteristics between ontologies may vary widely. We propose a systematic approach to quantitatively estimate the similarity characteristics for each alignment task and propose a strategy selection method to automatically combine the matching strategies based on two estimated factors. In the approach, we consider both textual and structural characteristics of ontologies. With RiMOM, we participated in the 2006 and 2007 campaigns of the Ontology Alignment Evaluation Initiative (OAEI). Our system is among the top three performers in benchmark data sets.

#index 1246171
#* Batch Mode Active Learning with Applications to Text Categorization and Image Retrieval
#@ Steven C. H. Hoi;Rong Jin;Michael R. Lyu
#t 2009
#c 7
#! Most machine learning tasks in data classification and information retrieval require manually labeled data examples in the training stage. The goal of active learning is to select the most informative examples for manual labeling in these learning tasks. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be inefficient, since the classification model has to be retrained for every acquired labeled example. It is also inappropriate for the setup of information retrieval tasks where the user's relevance feedback is often provided for the top K retrieved items. In this paper, we present a framework for batch mode active learning, which selects a number of informative examples for manual labeling in each iteration. The key feature of batch mode active learning is to reduce the redundancy among the selected examples such that each example provides unique information for model updating. To this end, we employ the Fisher information matrix as the measurement of model uncertainty, and choose the set of unlabeled examples that can efficiently reduce the Fisher information of the classification model. We apply our batch mode active learning framework to both text categorization and image retrieval. Promising results show that our algorithms are significantly more effective than the active learning approaches that select unlabeled examples based only on their informativeness for the classification model.

#index 1246172
#* Information-Theoretic Distance Measures for Clustering Validation: Generalization and Normalization
#@ Ping Luo;Hui Xiong;Guoxing Zhan;Junjie Wu;Zhongzhi Shi
#t 2009
#c 7
#! This paper studies the generalization and normalization issues of information-theoretic distance measures for clustering validation. Along this line, we first introduce a uniform representation of distance measures, defined as quasi-distance, which is induced based on a general form of conditional entropy. The quasi-distance possesses three properties: symmetry, the triangle law, and the minimum reachable. These properties ensure that the quasi-distance naturally lends itself as the external measure for clustering validation. In addition, we observe that the ranges of the distance measures are different when they apply for clustering validation on different data sets. Therefore, when comparing the performances of clustering algorithms on different data sets, distance normalization is required to equalize ranges of the distance measures. A critical challenge for distance normalization is to obtain the ranges of a distance measure when a data set is provided. To that end, we theoretically analyze the computation of the maximum value of a distance measure for a data set. Finally, we compare the performances of the partition clustering algorithm K-means on various real-world data sets. The experiments show that the normalized distance measures have better performance than the original distance measures when comparing clusterings of different data sets. Also, the normalized Shannon distance has the best performance among four distance measures under study.

#index 1246173
#* Learning from Imbalanced Data
#@ Haibo He;Edwardo A. Garcia
#t 2009
#c 7
#! With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.

#index 1246174
#* Nonlinear Dimensionality Reduction with Local Spline Embedding
#@ Shiming Xiang;Feiping Nie;Changshui Zhang;Chunxia Zhang
#t 2009
#c 7
#! This paper presents a new algorithm for Nonlinear Dimensionality Reduction (NLDR). Our algorithm is developed under the conceptual framework of compatible mapping. Each such mapping is a compound of a tangent space projection and a group of splines. Tangent space projection is estimated at each data point on the manifold, through which the data point itself and its neighbors are represented in tangent space with local coordinates. Splines are then constructed to guarantee that each of the local coordinates can be mapped to its own single global coordinate with respect to the underlying manifold. Thus, the compatibility between local alignments is ensured. In such a work setting, we develop an optimization framework based on reconstruction error analysis, which can yield a global optimum. The proposed algorithm is also extended to embed out of samples via spline interpolation. Experiments on toy data sets and real-world data sets illustrate the validity of our method.

#index 1246175
#* Patch Alignment for Dimensionality Reduction
#@ Tianhao Zhang;Dacheng Tao;Xuelong Li;Jie Yang
#t 2009
#c 7
#! Spectral analysis-based dimensionality reduction algorithms are important and have been popularly applied in data mining and computer vision applications. To date many algorithms have been developed, e.g., principal component analysis, locally linear embedding, Laplacian eigenmaps, and local tangent space alignment. All of these algorithms have been designed intuitively and pragmatically, i.e., on the basis of the experience and knowledge of experts for their own purposes. Therefore, it will be more informative to provide a systematic framework for understanding the common properties and intrinsic difference in different algorithms. In this paper, we propose such a framework, named "patch alignment,” which consists of two stages: part optimization and whole alignment. The framework reveals that 1) algorithms are intrinsically different in the patch optimization stage and 2) all algorithms share an almost identical whole alignment stage. As an application of this framework, we develop a new dimensionality reduction algorithm, termed Discriminative Locality Alignment (DLA), by imposing discriminative information in the part optimization stage. DLA can 1) attack the distribution nonlinearity of measurements; 2) preserve the discriminative ability; and 3) avoid the small-sample-size problem. Thorough empirical studies demonstrate the effectiveness of DLA compared with representative dimensionality reduction algorithms.

#index 1246176
#* Visible Reverse k-Nearest Neighbor Query Processing in Spatial Databases
#@ Yunjun Gao;Baihua Zheng;Gencai Chen;Wang-Chien Lee;Ken C.  K. Lee;Qing Li
#t 2009
#c 7
#! Reverse nearest neighbor (RNN) queries have a broad application base such as decision support, profile-based marketing, resource allocation, etc. Previous work on RNN search does not take obstacles into consideration. In the real world, however, there are many physical obstacles (e.g., buildings) and their presence may affect the visibility between objects. In this paper, we introduce a novel variant of RNN queries, namely, visible reverse nearest neighbor (VRNN) search, which considers the impact of obstacles on the visibility of objects. Given a data set P, an obstacle set O, and a query point q in a 2D space, a VRNN query retrieves the points in P that have q as their visible nearest neighbor. We propose an efficient algorithm for VRNN query processing, assuming that P and O are indexed by R-trees. Our techniques do not require any preprocessing and employ half-plane property and visibility check to prune the search space. In addition, we extend our solution to several variations of VRNN queries, including: 1) visible reverse k-nearest neighbor (VRkNN) search, which finds the points in P that have q as one of their k visible nearest neighbors; 2) \delta-VRkNN search, which handles VRkNN retrieval with the maximum visible distance \delta constraint; and 3) constrained VRkNN (CVRkNN) search, which tackles the VRkNN query with region constraint. Extensive experiments on both real and synthetic data sets have been conducted to demonstrate the efficiency and effectiveness of our proposed algorithms under various experimental settings.

#index 1246177
#* Result Merging Technique for Answering XPath Query over XSLT Transformed Data
#@ Sven Groppe;Jingua Groppe;Dirk Muller
#t 2009
#c 7
#! Caching stores the results of previously answered queries in order to answer succeeding queries faster by reusing these results. We propose two different approaches for using caches of XSLT transformed XML data in order to answer queries. The first approach checks whether or not a current query Q can be directly answered from the result of a previously answered query Qi stored in the cache. The new query is otherwise submitted to the source over the network, the answer of the query is determined, transmitted back to the client, and stored in the cache. The second approach determines only the intersection Q−Qi and integrates the result of Q−-Qi into the previous results in the cache, which requires applying a numbering scheme for the output of the XSLT stylesheet. We show by experimental results that the second approach can significantly speed up the answering time in comparison to the first approach, but is not significantly slower in few worst cases than the second approach.

#index 1246178
#* Optimization Techniques for Reactive Network Monitoring
#@ Ahmet Bulut;Nick Koudas;Anand Meka;Ambuj K. Singh;Divesh Srivastava
#t 2009
#c 7
#! We develop a framework for minimizing the communication overhead of monitoring global system parameters in IP networks and sensor networks. A global system predicate is defined as a conjunction of the local properties of different network elements. A typical example is to identify the time windows when the outbound traffic from each network element exceeds a predefined threshold. Our main idea is to optimize the scheduling of local event reporting across network elements for a given network traffic load and local event frequencies. The system architecture consists of N distributed network elements coordinated by a central monitoring station. Each network element monitors a set of local properties and the central station is responsible for identifying the status of global parameters registered in the system. We design an optimal algorithm, the Partition and Rank (PAR) scheme, when the local events are independent; whereas, when they are dependent, we show that the problem is NP-complete and develop two efficient heuristics: the PAR for dependent events (PAR-D) and Adaptive (Ada) algorithms, which adapt well to changing network conditions, and outperform the current state of the art techniques in terms of communication cost.

#index 1260389
#* Approximate Distributed K-Means Clustering over a Peer-to-Peer Network
#@ Souptik Datta;Chris Giannella;Hillol Kargupta
#t 2009
#c 7
#! Data intensive Peer-to-Peer (P2P) networks are finding increasing number of applications. Data mining in such P2P environments is a natural extension. However, common monolithic data mining architectures do not fit well in such environments since they typically require centralizing the distributed data which is usually not practical in a large P2P network. Distributed data mining algorithms that avoid large-scale synchronization or data centralization offer an alternate choice. This paper considers the distributed K-means clustering problem where the data and computing resources are distributed over a large P2P network. It offers two algorithms which produce an approximation of the result produced by the standard centralized K-means clustering algorithm. The first is designed to operate in a dynamic P2P network that can produce clusterings by “local” synchronization only. The second algorithm uses uniformly sampled peers and provides analytical guarantees regarding the accuracy of clustering on a P2P network. Empirical results show that both the algorithms demonstrate good performance compared to their centralized counterparts at the modest communication cost.

#index 1260390
#* Automating the Design and Construction of Query Forms
#@ Magesh Jayapandian;H. V. Jagadish
#t 2009
#c 7
#! One of the simplest ways to query a database is through a form where a user can fill in relevant information and obtain desired results by submitting the form. Designing good forms is a nontrivial manual task, and the designer needs a sound understanding of both the data organization and the querying needs. Furthermore, form design usually has conflicting goals: each form should be simple and easy to understand, while collectively, the interface must support as many queries as possible. In this paper, we present a framework for generating forms in an automatic and principled way, given a database and a sample query workload. We design a tunable clustering algorithm for establishing form structure based on multiple “similar” queries, which includes a mechanism for extending forms to support future “similar” queries. The algorithm is adaptive and can incrementally adjust forms to reflect the most current querying trends. We have implemented our form generation system on a real database and evaluated it on a comprehensive set of query loads and database schemas. We observe that our system generates a modest number of forms for large and diverse query loads even after placing a strict bound on form complexity.

#index 1260391
#* Distributed View Divergence Control of Data Freshness in Replicated Database Systems
#@ Takao Yamashita
#t 2009
#c 7
#! In this paper, we propose a distributed method to control the view divergence of data freshness for clients in replicated database systems whose facilitating or administrative roles are equal. Our method provides data with statistically defined freshness to clients when updates are initially accepted by any of the replicas, and then, asynchronously propagated among the replicas that are connected in a tree structure. To provide data with freshness specified by clients, our method selects multiple replicas using a distributed algorithm so that they statistically receive all updates issued up to a specified time before the present time. We evaluated by simulation the distributed algorithm to select replicas for the view divergence control in terms of controlled data freshness, time, message, and computation complexity. The simulation showed that our method achieves more than 36.9 percent improvement in data freshness compared with epidemic-style update propagation.

#index 1260392
#* estMax: Tracing Maximal Frequent Item Sets Instantly over Online Transactional Data Streams
#@ Ho Jin Woo;Won Suk Lee
#t 2009
#c 7
#! Frequent item set mining is one of the most challenging issues for descriptive data mining. In general, its resulting set tends to produce a large number of frequent item sets. To represent them in a more compact notation, closed or maximal frequent item sets are often used but finding such item sets over online transactional data streams is not easy due to the requirements of a data stream. For this purpose, this paper proposes a method of tracing the set of MFIs instantly over an online data stream. The method, namely estMax, maintains the set of frequent item sets by a prefix tree and extracts all MFIs without any additional superset/subset checking mechanism. Upon processing a new transaction, those frequent item sets that are matched maximally by the transaction are newly marked in their corresponding nodes of the prefix tree as candidates for MFIs. At the same time, if any subset of a newly marked item set has been already marked as a candidate MFI by a previous transaction, it is cleared as well. By employing this additional step, it is possible to extract the set of MFIs at any moment. The performance of the estMax method is comparatively analyzed by a series of experiments to identify its various characteristics.

#index 1260393
#* General Cost Models for Evaluating Dimensionality Reduction in High-Dimensional Spaces
#@ Xiang Lian;Lei Chen
#t 2009
#c 7
#! Similarity search usually encounters a serious problem in the high-dimensional space, known as the “curse of dimensionality.” In order to speed up the retrieval efficiency, most previous approaches reduce the dimensionality of the entire data set to a fixed lower value before building indexes (referred to as global dimensionality reduction (GDR)). More recent works focus on locally reducing the dimensionality of data to different values (called the local dimensionality reduction (LDR)). In addition, random projection is proposed as an approximate dimensionality reduction (ADR) technique to answer the approximate similarity search instead of the exact one. However, so far little work has formally evaluated the effectiveness and efficiency of GDR, LDR, and ADR for the range query. Motivated by this, in this paper, we propose general cost models for evaluating the query performance over the reduced data sets by GDR, LDR, and ADR, in light of which we introduce a novel (A)LDR method, Partitioning based on RANdomized Search (PRANS). It can achieve high retrieval efficiency with the guarantee of optimality given by the formal models. Finally, a {\rm B}^{+}-tree index is constructed over the reduced partitions for fast similarity search. Extensive experiments validate the correctness of our cost models on both real and synthetic data sets and demonstrate the efficiency and effectiveness of the proposed PRANS method.

#index 1260394
#* A Novel Bayes Model: Hidden Naive Bayes
#@ Liangxiao Jiang;Harry Zhang;Zhihua Cai
#t 2009
#c 7
#! Because learning an optimal Bayesian network classifier is an NP-hard problem, learning-improved naive Bayes has attracted much attention from researchers. In this paper, we summarize the existing improved algorithms and propose a novel Bayes model: hidden naive Bayes (HNB). In HNB, a hidden parent is created for each attribute which combines the influences from all other attributes. We experimentally test HNB in terms of classification accuracy, using the 36 UCI data sets selected by Weka, and compare it to naive Bayes (NB), selective Bayesian classifiers (SBC), naive Bayes tree (NBTree), tree-augmented naive Bayes (TAN), and averaged one-dependence estimators (AODE). The experimental results show that HNB significantly outperforms NB, SBC, NBTree, TAN, and AODE. In many data mining applications, an accurate class probability estimation and ranking are also desirable. We study the class probability estimation and ranking performance, measured by conditional log likelihood (CLL) and the area under the ROC curve (AUC), respectively, of naive Bayes and its improved models, such as SBC, NBTree, TAN, and AODE, and then compare HNB to them in terms of CLL and AUC. Our experiments show that HNB also significantly outperforms all of them.

#index 1260395
#* Reducing Redundancy in Subspace Clustering
#@ Yi-Hong Chu;Yi-Ju Chen;De-Nian Yang;Ming-Syan Chen
#t 2009
#c 7
#! In this paper, we first study an important but unsolved dilemma in the literature of subspace clustering, which is referred to as “information overlapping-data coverage” challenge. Current solutions of subspace clustering usually invoke a grid-based Apriori-like procedure to identify dense regions and construct subspace clusters afterward. Due to the nature of monotonicity property in Apriori-like procedures, it is inherent that if a region is identified as dense, all its projected regions are also identified as dense, causing overlapping/redundant clustering information to be inevitably reported to users when generating clusters from such highly correlated regions. However, naive methods to filter redundant clusters will incur a challenging problem in the other side of the dilemma, called the “data coverage” issue. Note that two clusters may have highly correlated dense regions but their data members could be highly different to each other. Arbitrarily removing one of them may lose the coverage of data with clustering information, thus likely reporting an incomplete and biased clustering result. In this paper, therefore, we further propose an innovative algorithm, called "NOnRedundant Subspace Cluster mining” (NORSC), to efficiently discover a succinct collection of subspace clusters while also maintaining the required degree of data coverage. NORSC not only avoids generating the redundant clusters with most of the contained data covered by higher dimensional clusters to resolve the information overlapping problem but also limits the information loss to cope with the data coverage problem. As shown by our experimental results, NORSC is very effective in identifying a concise and small set of subspace clusters, while incurring time complexity in orders of magnitude better than that of previous works.

#index 1260396
#* Granular Computing and Knowledge Reduction in Formal Contexts
#@ Wei-Zhi Wu;Yee Leung;Ju-Sheng Mi
#t 2009
#c 7
#! Granular computing and knowledge reduction are two basic issues in knowledge representation and data mining. Granular structure of concept lattices with application in knowledge reduction in formal concept analysis is examined in this paper. Information granules and their properties in a formal context are first discussed. Concepts of a granular consistent set and a granular reduct in the formal context are then introduced. Discernibility matrices and Boolean functions are, respectively, employed to determine granular consistent sets and calculate granular reducts in formal contexts. Methods of knowledge reduction in a consistent formal decision context are also explored. Finally, knowledge hidden in such a context is unraveled in the form of compact implication rules.

#index 1260397
#* Large Margin Feature Weighting Method via Linear Programming
#@ Bo Chen;Hongwei Liu;Jing Chai;Zheng Bao
#t 2009
#c 7
#! The problem of feature selection is a difficult combinatorial task in machine learning and of high practical relevance. In this paper, we consider feature selection method for multimodally distributed data, and present a large margin feature weighting method for k-nearest neighbor (kNN) classifiers. The method learns the feature weighting factors by minimizing a cost function, which aims at separating different classes by large local margins and pulling closer together points from the same class, based on using as few features as possible. The consequent optimization problem can be efficiently solved by Linear Programming. Finally, the proposed approach is assessed through a series of experiments with UCI and microarray data sets, as well as a more specific and challenging task, namely, radar high-resolution range profiles (HRRP) automatic target recognition (ATR). The experimental results demonstrate the effectiveness of the proposed algorithms.

#index 1260398
#* Learning Heuristics for the Superblock Instruction Scheduling Problem
#@ Tyrel Russell;Abid M. Malik;Michael Chase;Peter van Beek
#t 2009
#c 7
#! Modern processors have multiple pipelined functional units and can issue more than one instruction per clock cycle. This places a burden on the compiler to schedule the instructions to take maximum advantage of the underlying hardware. Superblocks—a straight-line sequence of code with a single entry point and multiple possible exit points—are a commonly used scheduling region within compilers. Superblock scheduling is NP-complete, and is done suboptimally in production compilers using a greedy algorithm coupled with a heuristic. The heuristic is usually handcrafted, a potentially time-consuming process. In this paper, we show that supervised machine learning techniques can be used to semiautomate the construction of heuristics for superblock scheduling. In our approach, labeled training data were produced using an optimal superblock scheduler. A decision tree learning algorithm was then used to induce a heuristic from the training data. The automatically constructed decision tree heuristic was compared against the best previously proposed, handcrafted heuristics for superblock scheduling on the SPEC 2000 and MediaBench benchmark suites. On these benchmark suites, the decision tree heuristic reduced the number of superblocks that were not optimally scheduled by up to 38 percent, and led to improved performance on some architectural models and competitive performance on others.

#index 1286793
#* A Dynamic Discretization Approach for Constructing Decision Trees with a Continuous Label
#@ Hsiao-Wei Hu;Yen-Liang Chen;Kwei Tang
#t 2009
#c 7
#! In traditional decision (classification) tree algorithms, the label is assumed to be a categorical (class) variable. When the label is a continuous variable in the data, two possible approaches based on existing decision tree algorithms can be used to handle the situations. The first uses a data discretization method in the preprocessing stage to convert the continuous label into a class label defined by a finite set of nonoverlapping intervals and then applies a decision tree algorithm. The second simply applies a regression tree algorithm, using the continuous label directly. These approaches have their own drawbacks. We propose an algorithm that dynamically discretizes the continuous label at each node during the tree induction process. Extensive experiments show that the proposed method outperforms the preprocessing approach, the regression tree approach, and several nontree-based algorithms.

#index 1286794
#* A Novel Density-Based Clustering Framework by Using Level Set Method
#@ Xiao-Feng Wang;De-Shuang Huang
#t 2009
#c 7
#! In this paper, a new density-based clustering framework is proposed by adopting the assumption that the cluster centers in data space can be regarded as target objects in image space. First, the level set evolution is adopted to find an approximation of cluster centers by using a new initial boundary formation scheme. Accordingly, three types of initial boundaries are defined so that each of them can evolve to approach the cluster centers in different ways. To avoid the long iteration time of level set evolution in data space, an efficient termination criterion is presented to stop the evolution process in the circumstance that no more cluster centers can be found. Then, a new effective density representation called level set density (LSD) is constructed from the evolution results. Finally, the valley seeking clustering is used to group data points into corresponding clusters based on the LSD. The experiments on some synthetic and real data sets have demonstrated the efficiency and effectiveness of the proposed clustering framework. The comparisons with DBSCAN method, OPTICS method, and valley seeking clustering method further show that the proposed framework can successfully avoid the overfitting phenomenon and solve the confusion problem of cluster boundary points and outliers.

#index 1286795
#* A Taxonomy of Similarity Mechanisms for Case-Based Reasoning
#@ Padraig Cunningham
#t 2009
#c 7
#! Assessing the similarity between cases is a key aspect of the retrieval phase in case-based reasoning (CBR). In most CBR work, similarity is assessed based on feature value descriptions of cases using similarity metrics, which use these feature values. In fact, it might be said that this notion of a feature value representation is a defining part of the CBR worldview—it underpins the idea of a problem space with cases located relative to each other in this space. Recently, a variety of similarity mechanisms have emerged that are not founded on this feature space idea. Some of these new similarity mechanisms have emerged in CBR research and some have arisen in other areas of data analysis. In fact, research on kernel-based learning is a rich source of novel similarity representations because of the emphasis on encoding domain knowledge in the kernel function. In this paper, we present a taxonomy that organizes these new similarity mechanisms and more established similarity mechanisms in a coherent framework.

#index 1286796
#* Efficient Similarity Join over Multiple Stream Time Series
#@ Xiang Lian;Lei Chen
#t 2009
#c 7
#! Similarity join (SJ) in time-series databases has a wide spectrum of applications such as data cleaning and mining. Specifically, an SJ query retrieves all pairs of (sub)sequences from two time-series databases that \varepsilon-match with each other, where \varepsilon is the matching threshold. Previous work on this problem usually considers static time-series databases, where queries are performed either on disk-based multidimensional indexes built on static data or by nested loop join (NLJ) without indexes. SJ over multiple stream time series, which continuously outputs pairs of similar subsequences from stream time series, strongly requires low memory consumption, low processing cost, and query procedures that are themselves adaptive to time-varying stream data. These requirements invalidate the existing approaches in static databases. In this paper, we propose an efficient and effective approach to perform SJ among multiple stream time series incrementally. In particular, we present a novel method, Adaptive Radius-based Search (ARES), which can answer the similarity search without false dismissals and is seamlessly integrated into SJ processing. Most importantly, we provide a formal cost model for ARES, based on which ARES can be adaptive to data characteristics, achieving the minimum number of refined candidate pairs, and thus, suitable for stream processing. Furthermore, in light of the cost model, we utilize space-efficient synopses that are constructed for stream time series to further reduce the candidate set. Extensive experiments demonstrate the efficiency and effectiveness of our proposed approach.

#index 1286797
#* Evaluating the Generation of Domain Ontologies in the Knowledge Puzzle Project
#@ Amal Zouaq;Roger Nkambou
#t 2009
#c 7
#! One of the goals of the Knowledge Puzzle Project is to automatically generate a domain ontology from plain text documents and use this ontology as the domain model in computer-based education. This paper describes the generation procedure followed by TEXCOMON, the Knowledge Puzzle Ontology Learning Tool, to extract concept maps from texts. It also explains how these concept maps are exported into a domain ontology. Data sources and techniques deployed by TEXCOMON for ontology learning from texts are briefly described herein. Then, the paper focuses on evaluating the generated domain ontology and advocates the use of a three-dimensional evaluation: structural, semantic, and comparative. Based on a set of metrics, structural evaluations consider ontologies as graphs. Semantic evaluations rely on human expert judgment, and finally, comparative evaluations are based on comparisons between the outputs of state-of-the-art tools and those of new tools such as TEXCOMON, using the very same set of documents in order to highlight the improvements of new techniques. Comparative evaluations performed in this study use the same corpus to contrast results from TEXCOMON with those of one of the most advanced tools for ontology generation from text. Results generated by such experiments show that TEXCOMON yields superior performance, especially regarding conceptual relation learning.

#index 1286798
#* Managing Frequent Updates in R-Trees for Update-Intensive Applications
#@ MoonBae Song;Hiroyuki Kitagawa
#t 2009
#c 7
#! Managing frequent updates is greatly important in many update-intensive applications, such as location-aware services, sensor networks, and stream databases. In this paper, we present an R-tree-based index structure (called {\rm R}^{\rm{{sb}}}-tree, R-tree with semibulk loading) for efficiently managing frequent updates from massive moving objects. The concept of semibulk loading is exploiting a small in-memory buffer to defer, buffer, and group the incoming updates and bulk-insert these updates simultaneously. With a reasonable memory overhead (typically only 1 percent of the whole data set), the proposed approach far outperforms the previous works in terms of update and query performance as well in a realistic environment. In order to further increase buffer hit ratio for the proposed approach, a new page-replacement policy that exploits the level of buffered node is proposed. Furthermore, we introduce the concept of deferring threshold ratio (dtr) that simply enables deferring CPU- and I/O-intensive operations such as node splits and removals. Extensive experimental evaluation reveals that the proposed approach is far more efficient than previous approaches for managing frequent updates under various settings.

#index 1286799
#* Nonlinear Dimension Reduction with Kernel Sliced Inverse Regression
#@ Yi-Ren Yeh;Su-Yun Huang;Yuh-Jye Lee
#t 2009
#c 7
#! Sliced inverse regression (SIR) is a renowned dimension reduction method for finding an effective low-dimensional linear subspace. Like many other linear methods, SIR can be extended to nonlinear setting via the “kernel trick.” The main purpose of this paper is two-fold. We build kernel SIR in a reproducing kernel Hilbert space rigorously for a more intuitive model explanation and theoretical development. The second focus is on the implementation algorithm of kernel SIR for fast computation and numerical stability. We adopt a low-rank approximation to approximate the huge and dense full kernel covariance matrix and a reduced singular value decomposition technique for extracting kernel SIR directions. We also explore kernel SIR's ability to combine with other linear learning algorithms for classification and regression including multiresponse regression. Numerical experiments show that kernel SIR is an effective kernel tool for nonlinear dimension reduction and it can easily combine with other linear algorithms to form a powerful toolkit for nonlinear data analysis.

#index 1286800
#* Performance of a Finite-State Machine Implementation of Iterative Cluster Labeling on Desktop and Mobile Computing Platforms
#@ Matthew L. Aldridge;Michael W. Berry
#t 2009
#c 7
#! In this paper, we present an efficient finite-state machine implementation of the Hoshen-Kopelman cluster identification algorithm using the nearest-eight neighborhood rule suitable to applications such as computer modeling for landscape ecology. The implementation presented in this study was tested using both actual land cover maps, as well as randomly generated data similar to those in the original presentation of the Hoshen-Kopelman algorithm for percolation analysis. The finite-state machine implementation clearly outperformed a straightforward adaptation of the original Hoshen-Kopelman algorithm on either data type. Research was also conducted to explore the finite-state machine's performance on a Palm mobile computing device, and while it was competitive, it did not exceed the performance of the straightforward Hoshen-Kopelman implementation. However, a discussion of why this was the case is provided along with a possible remedy for future hardware designs.

#index 1286801
#* The Cyclic Model Analysis on Sequential Patterns
#@ Ding-An Chiang;Cheng-Tzu Wang;Shao-Ping Chen;Chun-Chi Chen
#t 2009
#c 7
#! Sequential pattern mining has been used to predict various aspects of customer buying behavior for a long time. Discovered sequence reveals the chronological relation between items and provides valuable information to aid in developing marketing strategies. Nevertheless, we can hardly know whether the buying is cyclic and how long the interval between the two consecutive items in the sequential pattern is. To solve this problem, in this paper, data mining skills and the fundamentals of statistics are combined to develop a set of algorithms to unearth the cyclic properties of discovered sequential patterns. The algorithms, coupled with the sequential pattern mining process, constitute a thorough scheme to analyze and predict likely consumer behavior. The proposed algorithms are implemented and applied to test against real data collected from a consumer goods company. The experimental results illustrate how the model can be used to predict likely purchases within a certain time frame. Consequently, marketing professionals can execute campaigns to favorably impact customers' behaviors.

#index 1286802
#* URBE: Web Service Retrieval Based on Similarity Evaluation
#@ Pierluigi Plebani;Barbara Pernici
#t 2009
#c 7
#! In this work, we present Uddi Registry By Example (Urbe), a novel approach for Web service retrieval based on the evaluation of similarity between Web service interfaces. Our approach assumes that the Web service interfaces are defined with Web Service Description Language (WSDL) and the algorithm combines the analysis of their structures and the analysis of the terms used inside them. The higher the similarity, the less are the differences among their interfaces. As a consequence, Urbe is useful when we need to find a Web service suitable to replace an existing one that fails. Especially in autonomic systems, this situation is very common since we need to ensure the self-management, the self-configuration, the self-optimization, the self-healing, and the self-protection of the application that is based on the failed Web service. A semantic-oriented variant of the approach is also proposed, where we take advantage of annotations semantically enriching WSDL specifications. Semantic Annotation for WSDL (SAWSDL) is adopted as a language to annotate a WSDL description. The Urbe approach has been implemented in a prototype that extends a Universal Description, Discovery and Integration (UDDI) compliant Web service registry.

#index 1286803
#* Approximation Bounds for Minimum Information Loss Microaggregation
#@ Michael Laszlo;Sumitra Mukherjee
#t 2009
#c 7
#! The NP-hard microaggregation problem seeks a partition of data points into groups of minimum specified size k, so as to minimize the sum of the squared euclidean distances of every point to its group's centroid. One recent heuristic provides an {\rm O}(k^3) guarantee for this objective function and an {\rm O}(k^2) guarantee for a version of the problem that seeks to minimize the sum of the distances of the points to its group's centroid. This paper establishes approximation bounds for another microaggregation heuristic, providing better approximation guarantees of {\rm O}(k^2) for the squared distance measure and {\rm O}(k) for the distance measure.

#index 1326691
#* State of the Transactions Editorial
#@ Beng Chin Ooi
#t 2010
#c 7

#index 1326692
#* Anonymous Query Processing in Road Networks
#@ Kyriakos Mouratidis;Man Lung Yiu
#t 2010
#c 7
#! The increasing availability of location-aware mobile devices has given rise to a flurry of location-based services (LBSs). Due to the nature of spatial queries, an LBS needs the user position in order to process her requests. On the other hand, revealing exact user locations to a (potentially untrusted) LBS may pinpoint their identities and breach their privacy. To address this issue, spatial anonymity techniques obfuscate user locations, forwarding to the LBS a sufficiently large region instead. Existing methods explicitly target processing in the euclidean space and do not apply when proximity to the users is defined according to network distance (e.g., driving time through the roads of a city). In this paper, we propose a framework for anonymous query processing in road networks. We design location obfuscation techniques that: 1) provide anonymous LBS access to the users and 2) allow efficient query processing at the LBS side. Our techniques exploit existing network database infrastructure, requiring no specialized storage schemes or functionalities. We experimentally compare alternative designs in real road networks and demonstrate the effectiveness of our techniques.

#index 1326693
#* Density Conscious Subspace Clustering for High-Dimensional Data
#@ Yi-Hong Chu;Jen-Wei Huang;Kun-Ta Chuang;De-Nian Yang;Ming-Syan Chen
#t 2010
#c 7
#! Instead of finding clusters in the full feature space, subspace clustering is an emergent task which aims at detecting clusters embedded in subspaces. Most of previous works in the literature are density-based approaches, where a cluster is regarded as a high-density region in a subspace. However, the identification of dense regions in previous works lacks of considering a critical problem, called "the density divergence problem” in this paper, which refers to the phenomenon that the region densities vary in different subspace cardinalities. Without considering this problem, previous works utilize a density threshold to discover the dense regions in all subspaces, which incurs the serious loss of clustering accuracy (either recall or precision of the resulting clusters) in different subspace cardinalities. To tackle the density divergence problem, in this paper, we devise a novel subspace clustering model to discover the clusters based on the relative region densities in the subspaces, where the clusters are regarded as regions whose densities are relatively high as compared to the region densities in a subspace. Based on this idea, different density thresholds are adaptively determined to discover the clusters in different subspace cardinalities. Due to the infeasibility of applying previous techniques in this novel clustering model, we also devise an innovative algorithm, referred to as DENCOS (DENsity COnscious Subspace clustering), to adopt a divide-and-conquer scheme to efficiently discover clusters satisfying different density thresholds in different subspace cardinalities. As validated by our extensive experiments on various data sets, DENCOS can discover the clusters in all subspaces with high quality, and the efficiency of DENCOS outperformes previous works.

#index 1326694
#* Development of a Bayesian Framework for Determining Uncertainty in Receiver Operating Characteristic Curve Estimates
#@ David R. Parker;Steven C. Gustafson;Mark E. Oxley;Timothy D. Ross
#t 2010
#c 7
#! This research uses a Bayesian framework to develop probability densities for the receiver operating characteristic (ROC) curve. The ROC curve is a discrimination metric that may be used to quantify how well a detection system classifies targets and nontargets. The degree of uncertainty in ROC curve formulation is a concern that previous research has not adequately addressed. This research formulates a probability density for the ROC curve and characterizes its uncertainty using confidence bands. Methods for the generation and characterization of the probability densities of the ROC curve are specified and demonstrated, where the initial analysis employs beta densities to model target and nontarget samples of detection system output. For given target and nontarget data, given functional forms of the data densities (such as beta density forms) and given prior densities of the form parameters, the methods developed here provide exact performance metric probability densities.

#index 1326695
#* Learning with Positive and Unlabeled Examples Using Topic-Sensitive PLSA
#@ Ke Zhou;Xue Gui-Rong;Qiang Yang;Yong Yu
#t 2010
#c 7
#! It is often difficult and time-consuming to provide a large amount of positive and negative examples for training a classification system in many applications such as information retrieval. Instead, users often find it easier to indicate just a few positive examples of what he or she likes, and thus, these are the only labeled examples available for the learning system. A large amount of unlabeled data are easier to obtain. How to make use of the positive and unlabeled data for learning is a critical problem in machine learning and information retrieval. Several approaches for solving this problem have been proposed in the past, but most of these methods do not work well when only a small amount of labeled positive data are available. In this paper, we propose a novel algorithm called Topic-Sensitive pLSA to solve this problem. This algorithm extends the original probabilistic latent semantic analysis (pLSA), which is a purely unsupervised framework, by injecting a small amount of supervision information from the user. The supervision from users is in the form of indicating which documents fit the users' interests. The supervision is encoded into a set of constraints. By introducing the penalty terms for these constraints, we propose an objective function that trades off the likelihood of the observed data and the enforcement of the constraints. We develop an iterative algorithm that can obtain the local optimum of the objective function. Experimental evaluation on three data corpora shows that the proposed method can improve the performance especially only with a small amount of labeled positive data.

#index 1326696
#* LIGHT: A Query-Efficient Yet Low-Maintenance Indexing Scheme over DHTs
#@ Yuzhe Tang;Shuigeng Zhou;Jianliang Xu
#t 2010
#c 7
#! DHT is a widely used building block for scalable P2P systems. However, as uniform hashing employed in DHTs destroys data locality, it is not a trivial task to support complex queries (e.g., range queries and k-nearest-neighbor queries) in DHT-based P2P systems. In order to support efficient processing of such complex queries, a popular solution is to build indexes on top of the DHT. Unfortunately, existing over-DHT indexing schemes suffer from either query inefficiency or high maintenance cost. In this paper, we propose LIGhtweight Hash Tree (LIGHT)—a query-efficient yet low-maintenance indexing scheme. LIGHT employs a novel naming mechanism and a tree summarization strategy for graceful distribution of its index structure. We show through analysis that it can support various complex queries with near-optimal performance. Extensive experimental results also demonstrate that, compared with state of the art over-DHT indexing schemes, LIGHT saves 50-75 percent of index maintenance cost and substantially improves query performance in terms of both response time and bandwidth consumption. In addition, LIGHT is designed over generic DHTs and hence can be easily implemented and deployed in any DHT-based P2P system.

#index 1326697
#* MILD: Multiple-Instance Learning via Disambiguation
#@ Wu-Jun Li;Dit Yan yeung
#t 2010
#c 7
#! In multiple-instance learning (MIL), an individual example is called an instance and a bag contains a single or multiple instances. The class labels available in the training set are associated with bags rather than instances. A bag is labeled positive if at least one of its instances is positive; otherwise, the bag is labeled negative. Since a positive bag may contain some negative instances in addition to one or more positive instances, the true labels for the instances in a positive bag may or may not be the same as the corresponding bag label and, consequently, the instance labels are inherently ambiguous. In this paper, we propose a very efficient and robust MIL method, called Multiple-Instance Learning via Disambiguation (MILD), for general MIL problems. First, we propose a novel disambiguation method to identify the true positive instances in the positive bags. Second, we propose two feature representation schemes, one for instance-level classification and the other for bag-level classification, to convert the MIL problem into a standard single-instance learning (SIL) problem that can be solved by well-known SIL algorithms, such as support vector machine. Third, an inductive semi-supervised learning method is proposed for MIL. We evaluate our methods extensively on several challenging MIL applications to demonstrate their promising efficiency, robustness, and accuracy.

#index 1326698
#* Modeling Massive RFID Data Sets: A Gateway-Based Movement Graph Approach
#@ Hector Gonzalez;Jiawei Han;Hong Cheng;Xiaolei Li;Diego Klabjan;Tianyi Wu
#t 2010
#c 7
#! Massive Radio Frequency Identification (RFID) data sets are expected to become commonplace in supply chain management systems. Warehousing and mining this data is an essential problem with great potential benefits for inventory management, object tracking, and product procurement processes. Since RFID tags can be used to identify each individual item, enormous amounts of location-tracking data are generated. With such data, object movements can be modeled by movement graphs, where nodes correspond to locations and edges record the history of item transitions between locations. In this study, we develop a movement graph model as a compact representation of RFID data sets. Since spatiotemporal as well as item information can be associated with the objects in such a model, the movement graph can be huge, complex, and multidimensional in nature. We show that such a graph can be better organized around gateway nodes, which serve as bridges connecting different regions of the movement graph. A graph-based object movement cube can be constructed by merging and collapsing nodes and edges according to an application-oriented topological structure. Moreover, we propose an efficient cubing algorithm that performs simultaneous aggregation of both spatiotemporal and item dimensions on a partitioned movement graph, guided by such a topological structure.

#index 1326699
#* Object and Combination Shedding Schemes for Adaptive Media Workflow Execution
#@ Lina Peng;Renwei Yu;K. Selcuk Candan;Xinxin Wang
#t 2010
#c 7
#! Complex media fusion operations can be costly in terms of the time they need to process input objects. If data arrive faster to fusion nodes than the speed with which they can consume the inputs, this will result in some input objects not being processed. In this paper, we develop load shedding mechanisms which take into consideration both data quality and expensive nature of media fusion operators. In particular, we present quality assessment models for objects and multistream fusion operators and highlight that such quality assessments may impose partial orders on objects. We highlight that the most effective load control approach for fusion operators involves shedding of (not the individual input objects but) combinations of objects. Yet, identifying suitable combinations of objects in real time will not be possible if efficient combination selection algorithms do not exist. We develop efficient combination selection schemes for scenarios with different quality assessment and target characteristics. We first develop efficient combination-based load shedding when the fusion operator has unambiguously monotone semantics. We then extend this to the more general ambiguously monotone case and present experimental results that show the performance gains using quality-aware combination-based load shedding strategies under the various fusion scenarios.

#index 1326700
#* Aging Bloom Filter with Two Active Buffers for Dynamic Sets
#@ MyungKeun Yoon
#t 2010
#c 7
#! A Bloom filter is a simple but powerful data structure that can check membership to a static set. As Bloom filters become more popular for network applications, a membership query for a dynamic set is also required. Some network applications require high-speed processing of packets. For this purpose, Bloom filters should reside in a fast and small memory, SRAM. In this case, due to the limited memory size, stale data in the Bloom filter should be deleted to make space for new data. Namely the Bloom filter needs aging like LRU caching. In this paper, we propose a new aging scheme for Bloom filters. The proposed scheme utilizes the memory space more efficiently than double buffering, the current state of the art. We prove theoretically that the proposed scheme outperforms double buffering. We also perform experiments on real Internet traces to verify the effectiveness of the proposed scheme.

#index 1326701
#* The Dynamic Bloom Filters
#@ Deke Guo;Jie Wu;Honghui Chen;Ye Yuan;Xueshan Luo
#t 2010
#c 7
#! A Bloom filter is an effective, space-efficient data structure for concisely representing a set, and supporting approximate membership queries. Traditionally, the Bloom filter and its variants just focus on how to represent a static set and decrease the false positive probability to a sufficiently low level. By investigating mainstream applications based on the Bloom filter, we reveal that dynamic data sets are more common and important than static sets. However, existing variants of the Bloom filter cannot support dynamic data sets well. To address this issue, we propose dynamic Bloom filters to represent dynamic sets, as well as static sets and design necessary item insertion, membership query, item deletion, and filter union algorithms. The dynamic Bloom filter can control the false positive probability at a low level by expanding its capacity as the set cardinality increases. Through comprehensive mathematical analysis, we show that the dynamic Bloom filter uses less expected memory than the Bloom filter when representing dynamic sets with an upper bound on set cardinality, and also that the dynamic Bloom filter is more stable than the Bloom filter due to infrequent reconstruction when addressing dynamic sets without an upper bound on set cardinality. Moreover, the analysis results hold in stand-alone applications, as well as distributed applications.

#index 1326702
#* Bayesian Classifiers Programmed in SQL
#@ Carlos Ordonez;Sasi K. Pitchaimalai
#t 2010
#c 7
#! The Bayesian classifier is a fundamental classification technique. In this work, we focus on programming Bayesian classifiers in SQL. We introduce two classifiers: Naive Bayes and a classifier based on class decomposition using K-means clustering. We consider two complementary tasks: model computation and scoring a data set. We study several layouts for tables and several indexing alternatives. We analyze how to transform equations into efficient SQL queries and introduce several query optimizations. We conduct experiments with real and synthetic data sets to evaluate classification accuracy, query optimizations, and scalability. Our Bayesian classifier is more accurate than Naive Bayes and decision trees. Distance computation is significantly accelerated with horizontal layout for tables, denormalization, and pivoting. We also compare Naive Bayes implementations in SQL and C++: SQL is about four times slower. Our Bayesian classifier in SQL achieves high classification accuracy, can efficiently analyze large data sets, and has linear scalability.

#index 1326703
#* Deterministic Column-Based Matrix Decomposition
#@ Xuelong Li;Yawei Pang
#t 2010
#c 7
#! In this paper, we propose a deterministic column-based matrix decomposition method. Conventional column-based matrix decomposition (CX) computes the columns by randomly sampling columns of the data matrix. Instead, the newly proposed method (termed as CX_D) selects columns in a deterministic manner, which well approximates singular value decomposition. The experimental results well demonstrate the power and the advantages of the proposed method upon three real-world data sets.

#index 1327634
#* A Reasoning System of Ternary Projective Relations
#@ Eliseo Clementini;Spiros Skiadopoulos;Roland Billen;Francesco Tarquini
#t 2010
#c 7
#! This paper introduces a reasoning system based on a previously developed model for ternary projective relations between spatial objects. The model applies to spatial objects of the kind point and region is based on basic projective invariants and takes into account the size and shape of the three objects that are involved in a relation. The reasoning system proposes a set of permutation and composition rules, which allow the inference of unknown relations from given ones.

#index 1327635
#* A Unified Framework for Providing Recommendations in Social Tagging Systems Based on Ternary Semantic Analysis
#@ Panagiotis Symeonidis;Alexandros Nanopoulos;Yannis Manolopoulos
#t 2010
#c 7
#! Social Tagging is the process by which many users add metadata in the form of keywords, to annotate and categorize items (songs, pictures, Web links, products, etc.). Social tagging systems (STSs) can provide three different types of recommendations: They can recommend 1) tags to users, based on what tags other users have used for the same items, 2) items to users, based on tags they have in common with other similar users, and 3) users with common social interest, based on common tags on similar items. However, users may have different interests for an item, and items may have multiple facets. In contrast to the current recommendation algorithms, our approach develops a unified framework to model the three types of entities that exist in a social tagging system: users, items, and tags. These data are modeled by a 3-order tensor, on which multiway latent semantic analysis and dimensionality reduction is performed using both the Higher Order Singular Value Decomposition (HOSVD) method and the Kernel-SVD smoothing technique. We perform experimental comparison of the proposed method against state-of-the-art recommendation algorithms with two real data sets (Last.fm and BibSonomy). Our results show significant improvements in terms of effectiveness measured through recall/precision.

#index 1327636
#* FiVaTech: Page-Level Web Data Extraction from Template Pages
#@ Mohammed Kayed;Chia-Hui Chang
#t 2010
#c 7
#! Web data extraction has been an important part for many Web data analysis applications. In this paper, we formulate the data extraction problem as the decoding process of page generation based on structured data and tree templates. We propose an unsupervised, page-level data extraction approach to deduce the schema and templates for each individual Deep Website, which contains either singleton or multiple data records in one Webpage. FiVaTech applies tree matching, tree alignment, and mining techniques to achieve the challenging task. In experiments, FiVaTech has much higher precision than EXALG and is comparable with other record-level extraction systems like ViPER and MSE. The experiments show an encouraging result for the test pages used in many state-of-the-art Web data extraction works.

#index 1327637
#* Dynamic Wavelet Synopses Management over Sliding Windows in Sensor Networks
#@ Ken-Hao Liu;Wei-Guang Teng;Ming-Syan Chen
#t 2010
#c 7
#! Due to the dynamic nature of data streams, a sliding window is used to generate synopses that approximate the most recent data within the retrospective horizon to answer queries or discover patterns. In this paper, we propose a dynamic scheme for wavelet synopses management in sensor networks. We define a data structure Sliding Dual Tree, abbreviated as SDT, to generate dynamic synopses that adapts to the insertions and deletions in the most recent sliding window. By exploiting the properties of Haar wavelet transform, we develop several operations to incrementally maintain SDT over consecutive time windows in a time- and space-efficient manner. These operations directly operate on the transformed time-frequency domain without the need of storing/reconstructing the original data. As shown in our thorough analysis, our SDT-based approach greatly reduces the required resources for synopses generation and maximizes the storage utilization of wavelet synopses in terms of the window length and quality measures. We also show that the approximation error of the dynamic wavelet synopses, i.e., L^{2}-norm error, can be incrementally updated. We also derive the bound of the overestimation of the approximation error due to the incremental thresholding scheme. Furthermore, the synopses can be used to answer various kinds of numerical queries such as point and distance queries. In addition, we show that our SDT can adapt to resource allocation to further enhance the overall storage utilization over time. As demonstrated by our experimental results, our proposed framework can outperform current techniques in both real and synthetic data.

#index 1327638
#* Energy- and Latency-Efficient Processing of Full-Text Searches on a Wireless Broadcast Stream
#@ Yon Dohn Chung;Sanghyun Yoo;Myoung Ho Kim
#t 2010
#c 7
#! In wireless mobile computing environments, broadcasting is an effective and scalable technique to disseminate information to a massive number of clients, wherein the energy usage and latency are considered major concerns. This paper presents an indexing scheme for the energy- and latency-efficient processing of full-text searches over the wireless broadcast data stream. Although a lot of access methods and index structures have been proposed in the past for full-text searches, all of them are targeted for data in disk storage, not wireless broadcast channels. For full-text searches on a wireless broadcast stream, we firstly introduce a naive, inverted list-style indexing method, where inverted lists are placed in front of the data on the wireless channel. In order to reduce the latency overhead, we propose a two-level indexing method which adds another level of index structure to the basic inverted list-style index. In addition, we propose a replication strategy of the index list and index tree to further improve the latency performance. We analyze the performance of the proposed indexing scheme with respect to the latency and energy usage measures, and show the optimality of index replication. The correctness of the analysis is demonstrated through simulation experiments, and the effectiveness of the proposed scheme is shown by implementing a real wireless information delivery system.

#index 1327639
#* Exploring Correlated Subspaces for Efficient Query Processing in Sparse Databases
#@ Bin Cui;Jiakui Zhao;Dongqing Yang
#t 2010
#c 7
#! Sparse data are becoming increasingly common and available in many real-life applications. However, relatively little attention has been paid to effectively model the sparse data and existing approaches such as the conventional "horizontal” and "vertical” representations fail to provide satisfactory performance for both storage and query processing, as such approaches are too rigid and generally do not consider the dimension correlations. In this paper, we propose a new approach, named HoVer, to store and conduct query for sparse data sets in an unmodified RDBMS, where HoVer stands for Horizontal representation over Vertically partitioned subspaces. According to the dimension correlations of sparse data sets, a novel mechanism has been developed to vertically partition a high-dimensional sparse data set into multiple lower-dimensional subspaces, and all the dimensions are highly correlated intrasubspace and highly unrelated intersubspace, respectively. Therefore, original data objects can be represented by the horizontal format in respective subspaces. With the novel HoVer representation, users can write SQL queries over the original horizontal view, which can be easily rewritten into queries over the subspace tables. Experiments over synthetic and real-life data sets show that our approach is effective in finding correlated subspaces and yields superior performance for the storage and query of sparse data.

#index 1327640
#* Filtering Data Streams for Entity-Based Continuous Queries
#@ Reynold Cheng;Ben Kao;Alan Kwan;Sunil Prabhakar;Yicheng Tu
#t 2010
#c 7
#! The idea of allowing query users to relax their correctness requirements in order to improve performance of a data stream management system (e.g., location-based services and sensor networks) has been recently studied. By exploiting the maximum error (or tolerance) allowed in query answers, algorithms for reducing the use of system resources have been developed. In most of these works, however, query tolerance is expressed as a numerical value, which may be difficult to specify. We observe that in many situations, users may not be concerned with the actual value of an answer, but rather which object satisfies a query (e.g., "who is my nearest neighbor?”). In particular, an entity-based query returns only the names of objects that satisfy the query. For these queries, it is possible to specify a tolerance that is "nonvalue-based.” In this paper, we study fraction-based tolerance, a type of nonvalue-based tolerance, where a user specifies the maximum fractions of a query answer that can be false positives and false negatives. We develop fraction-based tolerance for two major classes of entity-based queries: 1) nonrank-based query (e.g., range queries) and 2) rank-based query (e.g., k-nearest-neighbor queries). These definitions provide users with an alternative to specify the maximum tolerance allowed in their answers. We further investigate how these definitions can be exploited in a distributed stream environment. We design adaptive filter algorithms that allow updates be dropped conditionally at the data stream sources without affecting the overall query correctness. Extensive experimental results show that our protocols reduce the use of network and energy resources significantly.

#index 1327641
#* Optimization of Linear Recursive Queries in SQL
#@ Carlos Ordonez
#t 2010
#c 7
#! Recursion is a fundamental computation mechanism which has been incorporated into the SQL language. This work focuses on the optimization of linear recursive queries in SQL. Query optimization is studied with two important graph problems: computing the transitive closure of a graph and getting the power matrix of its adjacency matrix. We present SQL implementations for two fundamental algorithms: Seminaive and Direct. Five query optimizations are studied: 1) Storage and indexing; 2) early selection; 3) early evaluation of nonrecursive joins; 4) pushing duplicate elimination; and 5) pushing aggregation. Experiments compare both evaluation algorithms and systematically evaluate the impact of optimizations with large input tables. Optimizations are evaluated on four types of graphs: binary trees, lists, cyclic graphs, and complete graphs, going from the best to worst case. In general, Seminaive is faster than Direct, except for complete graphs. Storing and indexing rows by vertex and pushing aggregation work well on trees, lists, and cyclic graphs. Pushing duplicate elimination is essential for complete graphs, but slows computation for acyclic graphs. Early selection with equality predicates significantly accelerates computation for all types of graphs.

#index 1327642
#* Structural and Role-Oriented Web Service Discovery with Taxonomies in OWL-S
#@ Georgios Meditskos;Nick Bassiliades
#t 2010
#c 7
#! In this paper, we describe and evaluate a Web service discovery framework using OWL-S advertisements, combined with the distinction between service and Web service of the WSMO Discovery Framework. More specifically, we follow the Web service discovery model, which is based on abstract and lightweight semantic Web service descriptions, using the Service Profile ontology of OWL-S. Our goal is to determine fast an initial set of candidate Web services for a specific request. This set can then be used in more fine-grained discovery approaches, based on richer Web service descriptions. Our Web service matchmaking algorithm extends object-based matching techniques used in Structural Case-based Reasoning, allowing 1) the retrieval of Web services not only based on subsumption relationships, but exploiting also the structural information of OWL ontologies and 2) the exploitation of Web services classification in Profile taxonomies, performing domain-dependent discovery. Furthermore, we describe how the typical paradigm of Profile input/output annotation with ontology concepts can be extended, allowing ontology roles to be considered as well. We have implemented our framework in the OWLS-SLR system, which we extensively evaluate and compare to the OWLS-MX matchmaker.

#index 1327643
#* Uninterpreted Schema Matching with Embedded Value Mapping under Opaque Column Names and Data Values
#@ Anuj Jaiswal;David J. Miller;Prasenjit Mitra
#t 2010
#c 7
#! Schema matching and value mapping across two heterogenous information sources are critical tasks in applications involving data integration, data warehousing, and federation of databases. Before data can be integrated from multiple tables, the columns and the values appearing in the tables must be matched. The complexity of the problem grows quickly with the number of data attributes/columns to be matched and due to multiple semantics of data values. Traditional research has tackled schema matching and value mapping independently. We propose a novel method that optimizes embedded value mappings to enhance schema matching in the presence of opaque data values and column names. In this approach, the fitness objective for matching a pair of attributes from two schemas depends on the value mapping function for each of the two attributes. Suitable fitness objectives include the euclidean distance measure, which we use in our experimental study, as well as relative (cross) entropy. We propose a heuristic local descent optimization strategy that uses sorting and two-opt switching to jointly optimize value mappings and attribute matches. Our experiments show that our proposed technique outperforms earlier uninterpreted schema matching methods, and thus, should form a useful addition to a suite of (semi) automated tools for resolving structural heterogeneity.

#index 1327644
#* Locating XML Documents in a Peer-to-Peer Network Using Distributed Hash Tables
#@ Praveen R. Rao;Bongki Moon
#t 2009
#c 7
#! One of the key challenges in a peer-to-peer (P2P) network is to efficiently locate relevant data sources across a large number of participating peers. With the increasing popularity of the extensible markup language (XML) as a standard for information interchange on the Internet, XML is commonly used as an underlying data model for P2P applications to deal with the heterogeneity of data and enhance the expressiveness of queries. In this paper, we address the problem of efficiently locating relevant XML documents in a P2P network, where a user poses queries in a language such as XPath. We have developed a new system called psiX that runs on top of an existing distributed hashing framework. Under the psiX system, each XML document is mapped into an algebraic signature that captures the structural summary of the document. An XML query pattern is also mapped into a signature. The query's signature is used to locate relevant document signatures. Our signature scheme supports holistic processing of query patterns without breaking them into multiple path queries and processing them individually. The participating peers in the network collectively maintain a collection of distributed hierarchical indexes for the document signatures. Value indexes are built to handle numeric and textual values in XML documents. These indexes are used to process queries with value predicates. Our experimental study on PlanetLab demonstrates that psiX provides an efficient location service in a P2P network for a wide variety of XML documents.

#index 1327645
#* Pooling for Combination of Multilevel Forecasts
#@ Silvia Riedel;Bogdan Gabrys
#t 2009
#c 7
#! In this paper, we provide a theoretical analysis of effects of applying different forecast diversification methods on the structure of the forecast error covariance matrices and decomposed forecast error components based on the bias-variance-Bayes error decomposition of James and Hastie. We express the "diversity” of different forecasts in relation to different error components and propose a measure in order to quantify it. We illustrate and discuss typical inhomogeneities frequently occurring in the forecast error covariance matrices and show that previously proposed pooling based only on error variances cannot fully exploit the complementary information present in a set of diverse forecasts to be combined. If covariance values could be reliably calculated, they could be taken into account during the pooling process. We study the difficult case in which covariance information cannot be measured properly and propose a novel simplified representation of the covariance matrix, which is only based on knowledge about the forecast generation process. Finally, we propose a new pooling approach that avoids inhomogeneities in the forecast error covariance matrix by considering the information contained in the simplified covariance representation and compare it with the error-variance-based pooling approach introduced by Aiolfi and Timmermann. Applying our approach more than once leads to the generation of multistep and multilevel forecast combination structures, which have generated significantly improved forecasts in our previous extensive experimental work; the summary of which is also provided.

#index 1327646
#* Privacy-Preserving Tuple Matching in Distributed Databases
#@ Yingpeng Sang;Hong Shen;Hui Tian
#t 2009
#c 7
#! We address the problems of Privacy-Preserving Duplicate Tuple Matching (PPDTM) and Privacy-Preserving Threshold Attributes Matching (PPTAM) in the scenario of a horizontally partitioned database among N parties, where each party holds a private share of the database's tuples and all tuples have the same set of attributes. In PPDTM, each party determines whether its tuples have any duplicate on other parties' private databases. In PPTAM, each party determines whether all attribute values of each tuple appear at least a threshold number of times in the attribute unions. We propose protocols for the two problems using additive homomorphic cryptosystem based on the subgroup membership assumption, e.g., Paillier's and ElGamal's schemes. By analysis on the total numbers of modular exponentiations, modular multiplications and communication bits, with a reduced computation cost which dominates the total cost, by trading off communication cost, our PPDTM protocol for the semihonest model is superior to the solution derivable from existing techniques in total cost. Our PPTAM protocol is superior in both computation and communication costs. The efficiency improvements are achieved mainly by using random numbers instead of random polynomials as existing techniques for perturbation, without causing successful attacks by polynomial interpolations. We also give detailed constructions on the required zero-knowledge proofs and extend our two protocols to the malicious model, which were previously unknown.

#index 1327647
#* Tuning On-Air Signatures for Balancing Performance and Confidentiality
#@ Baihua Zheng;Wang-Chien Lee;Peng Liu;Dik Lun Lee;Xuhua Ding
#t 2009
#c 7
#! In this paper, we investigate the trade off between performance and confidentiality in signature-based air indexing schemes for wireless data broadcast. Two metrics, namely, false drop probability and false guess probability, are defined to quantify the filtering efficiency and confidentiality loss of a signature scheme. Our analysis reveals that false drop probability and false guess probability share a similar trend as the tuning parameters of a signature scheme change and it is impossible to achieve a low false drop probability and a high false guess probability simultaneously. In order to balance the performance and confidentiality, we perform an analysis to provide a guidance for parameter settings of the signature schemes to meet different system requirements. In addition, we propose the jump pointer technique and the XOR signature scheme to further improve the performance and confidentiality. A comprehensive simulation has been conducted to validate our findings.

#index 1327648
#* Local Kernel Regression Score for Selecting Features of High-Dimensional Data
#@ Yiu-ming Cheung;Hong Zeng
#t 2009
#c 7
#! In general, irrelevant features of high-dimensional data will degrade the performance of an inference system, e.g., a clustering algorithm or a classifier. In this paper, we therefore present a Local Kernel Regression (LKR) scoring approach to evaluate the relevancy of features based on their capabilities of keeping the local configuration in a small patch of data. Accordingly, a score index featuring applicability to both of supervised learning and unsupervised learning is developed to identify the relevant features within the framework of local kernel regression. Experimental results show the efficacy of the proposed approach in comparison with the existing methods.

#index 1327649
#* Manufacturing-Oriented Discrete Process Modeling Approach Using the Predicate Logic
#@ Wang Zhenwei;Hui Li
#t 2009
#c 7
#! Part machining is a discrete manufacturing process. In order to evaluate the manufacturing process, an intelligent modeling method based on the first-order predicate logic is proposed. First, the basic predicate formula is defined according to the machining method, and the predicate and variables are illustrated in detail. Thus, the process representation is completed. Second, to construct the process model, the modeling element is put forward, which includes three nodes. Components of modeling element are, respectively, discussed, as well as the mapping relationship between modeling element and predicate. After the definition of modeling predicate formula, five basic inference rules are established. Consequently, the manufacturing process model is constructed. Third, on the basis of the process model, the process simulation is carried out to evaluate the manufacturing performances, such as the production efficiency, the utilization rate of machining equipment, the production bottleneck, etc. Finally, a case study is conducted to explain this modeling method.

#index 1327650
#* Clustering with Local and Global Regularization
#@ Fei Wang;Changshui Zhang;Tao Li
#t 2009
#c 7
#! Clustering is an old research topic in data mining and machine learning. Most of the traditional clustering methods can be categorized as local or global ones. In this paper, a novel clustering method that can explore both the local and global information in the data set is proposed. The method, Clustering with Local and Global Regularization (CLGR), aims to minimize a cost function that properly trades off the local and global costs. We show that such an optimization problem can be solved by the eigenvalue decomposition of a sparse symmetric matrix, which can be done efficiently using iterative methods. Finally, the experimental results on several data sets are presented to show the effectiveness of our method.

#index 1327651
#* A Model-Based Approach for Discrete Data Clustering and Feature Weighting Using MAP and Stochastic Complexity
#@ Nizar Bouguila
#t 2009
#c 7
#! In this paper, we consider the problem of unsupervised discrete feature selection/weighting. Indeed, discrete data are an important component in many data mining, machine learning, image processing, and computer vision applications. However, much of the published work on unsupervised feature selection has concentrated on continuous data. We propose a probabilistic approach that assigns relevance weights to discrete features that are considered as random variables modeled by finite discrete mixtures. The choice of finite mixture models is justified by its flexibility which has led to its widespread application in different domains. For the learning of the model, we consider both Bayesian and information-theoretic approaches through stochastic complexity. Experimental results are presented to illustrate the feasibility and merits of our approach on a difficult problem which is clustering and recognizing visual concepts in different image data. The proposed approach is successfully applied also for text clustering.

#index 1327652
#* Continuous K-Means Monitoring with Low Reporting Cost in Sensor Networks
#@ Ming Hua;Man Ki Lau;Jian Pei;Kui Wu
#t 2009
#c 7
#! In this paper, we study an interesting problem: continuously monitoring k-means clustering of sensor readings in a large sensor network. Given a set of sensors whose readings evolve over time, we want to maintain the k-means of the readings continuously. The optimization goal is to reduce the reporting cost in the network, that is, let as few sensors as possible report their current readings to the data center in the course of maintenance. To tackle the problem, we propose the reading reporting tree, a hierarchical data collection, and analysis framework. Moreover, we develop several reporting cost-effective methods using reading reporting trees in continuous k-means monitoring. First, a uniform sampling method using a reading reporting tree can achieve good quality approximation of k-means. Second, we propose a reporting threshold method which can guarantee the approximation quality. Last, we explore a lazy approach which can reduce the intermediate computation substantially. We conduct a systematic simulation evaluation using synthetic data sets to examine the characteristics of the proposed methods.

#index 1327653
#* Discovering Transitional Patterns and Their Significant Milestones in Transaction Databases
#@ Qian Wan;Aijun An
#t 2009
#c 7
#! A transaction database usually consists of a set of time-stamped transactions. Mining frequent patterns in transaction databases has been studied extensively in data mining research. However, most of the existing frequent pattern mining algorithms (such as Apriori and FP-growth) do not consider the time stamps associated with the transactions. In this paper, we extend the existing frequent pattern mining framework to take into account the time stamp of each transaction and discover patterns whose frequency dramatically changes over time. We define a new type of patterns, called transitional patterns, to capture the dynamic behavior of frequent patterns in a transaction database. Transitional patterns include both positive and negative transitional patterns. Their frequencies increase/decrease dramatically at some time points of a transaction database. We introduce the concept of significant milestones for a transitional pattern, which are time points at which the frequency of the pattern changes most significantly. Moreover, we develop an algorithm to mine from a transaction database the set of transitional patterns along with their significant milestones. Our experimental studies on real-world databases illustrate that mining positive and negative transitional patterns is highly promising as a practical and useful approach for discovering novel and interesting knowledge from large databases.

#index 1327654
#* Efficient Tree Structures for High Utility Pattern Mining in Incremental Databases
#@ Chowdhury Farhan Ahmed;Syed Khairuzzaman Tanbeer;Byeong-Soo Jeong;Young-Koo Lee
#t 2009
#c 7
#! Recently, high utility pattern (HUP) mining is one of the most important research issues in data mining due to its ability to consider the nonbinary frequency values of items in transactions and different profit values for every item. On the other hand, incremental and interactive data mining provide the ability to use previous data structures and mining results in order to reduce unnecessary calculations when a database is updated, or when the minimum threshold is changed. In this paper, we propose three novel tree structures to efficiently perform incremental and interactive HUP mining. The first tree structure, Incremental HUP Lexicographic Tree ({\rm IHUP}_{{\rm {L}}}-Tree), is arranged according to an item's lexicographic order. It can capture the incremental data without any restructuring operation. The second tree structure is the IHUP Transaction Frequency Tree ({\rm IHUP}_{{\rm {TF}}}-Tree), which obtains a compact size by arranging items according to their transaction frequency (descending order). To reduce the mining time, the third tree, IHUP-Transaction-Weighted Utilization Tree ({\rm IHUP}_{{\rm {TWU}}}-Tree) is designed based on the TWU value of items in descending order. Extensive performance analyses show that our tree structures are very efficient and scalable for incremental and interactive HUP mining.

#index 1327655
#* Join of Multiple Data Streams in Sensor Networks
#@ Xianjin Zhu;Himanshu Gupta;Bin Tang
#t 2009
#c 7
#! Sensor networks are multihop wireless networks of resource-constrained sensor nodes used to realize high-level collaborative sensing tasks. To query or access data generated by the sensor nodes, the sensor network can be viewed as a distributed database. In this paper, we develop algorithms for communication-efficient implementation of join of multiple (two or more) data streams in a sensor network. The distributed implementation of join in sensor networks is particularly challenging due to unique characteristics of the sensor networks such as limited memory and battery energy on individual nodes, arbitrary and dynamic network topology, multihop communication, and unreliable infrastructure. One of our proposed approaches, viz., the Perpendicular Approach (PA), is load balanced, and in fact, incurs near-optimal communication cost for the special case of binary joins in grid networks under the assumption of uniform generation of tuples across the network. We compare the performance of our designed approaches through extensive simulations on the ns2 simulator, and show that PA results in substantially prolonging the network lifetime compared to other approaches, especially for joins involving spatial constraints.

#index 1364944
#* Ensemble Rough Hypercuboid Approach for Classifying Cancers
#@ Jin-Mao Wei;Shu-Qin Wang;Xiao-Jie Yuan
#t 2010
#c 7
#! Cancer classification is the critical basis for patient-tailored therapy. Conventional histological analysis tends to be unreliable because different tumors may have similar appearance. The advances in microarray technology make individualized therapy possible. Various machine learning methods can be employed to classify cancer tissue samples based on microarray data. However, few methods can be elegantly adopted for generating accurate and reliable as well as biologically interpretable rules. In this paper, we introduce an approach for classifying cancers based on the principle of minimal rough fringe. For training rough hypercuboid classifiers from gene expression data sets, the method dynamically evaluates all available genes and sifts the genes with the smallest implicit regions as the dimensions of implicit hypercuboids. An unseen object is predicted to be a certain class if it falls within the corresponding class hypercuboid. Based upon the method, ensemble rough hypercuboid classifiers are subsequently constructed. Experimental results on some open cancer gene expression data sets show that the proposed method is capable of generating accurate and interpretable rules compared with some other machine learning methods. Hence, it is a feasible way of classifying cancer tissues in biomedical applications.

#index 1364945
#* k-Anonymity in the Presence of External Databases
#@ Dimitris Sacharidis;Kyriakos Mouratidis;Dimitris Papadias
#t 2010
#c 7
#! The concept of k-anonymity has received considerable attention due to the need of several organizations to release microdata without revealing the identity of individuals. Although all previous k-anonymity techniques assume the existence of a public database (PD) that can be used to breach privacy, none utilizes PD during the anonymization process. Specifically, existing generalization algorithms create anonymous tables using only the microdata table (MT) to be published, independently of the external knowledge available. This omission leads to high information loss. Motivated by this observation, we first introduce the concept of k-join-anonymity (KJA), which permits more effective generalization to reduce the information loss. Briefly, KJA anonymizes a superset of MT, which includes selected records from PD. We propose two methodologies for adapting k-anonymity algorithms to their KJA counterparts. The first generalizes the combination of MT and PD, under the constraint that each group should contain at least 1 tuple of MT (otherwise, the group is useless and discarded). The second anonymizes MT, and then, refines the resulting groups using PD. Finally, we evaluate the effectiveness of our contributions with an extensive experimental evaluation using real and synthetic data sets.

#index 1364946
#* PAM: An Efficient and Privacy-Aware Monitoring Framework for Continuously Moving Objects
#@ Haibo Hu;Jianliang Xu;Dik Lun Lee
#t 2010
#c 7
#! Efficiency and privacy are two fundamental issues in moving object monitoring. This paper proposes a privacy-aware monitoring (PAM) framework that addresses both issues. The framework distinguishes itself from the existing work by being the first to holistically address the issues of location updating in terms of monitoring accuracy, efficiency, and privacy, particularly, when and how mobile clients should send location updates to the server. Based on the notions of safe region and most probable result, PAM performs location updates only when they would likely alter the query results. Furthermore, by designing various client update strategies, the framework is flexible and able to optimize accuracy, privacy, or efficiency. We develop efficient query evaluation/reevaluation and safe region computation algorithms in the framework. The experimental results show that PAM substantially outperforms traditional schemes in terms of monitoring accuracy, CPU cost, and scalability while achieving close-to-optimal communication cost.

#index 1364947
#* Ranked Query Processing in Uncertain Databases
#@ Xiang Lian;Lei Chen
#t 2010
#c 7
#! Recently, many new applications, such as sensor data monitoring and mobile device tracking, raise up the issue of uncertain data management. Compared to "certain” data, the data in the uncertain database are not exact points, which, instead, often reside within a region. In this paper, we study the ranked queries over uncertain data. In fact, ranked queries have been studied extensively in traditional database literature due to their popularity in many applications, such as decision making, recommendation raising, and data mining tasks. Many proposals have been made in order to improve the efficiency in answering ranked queries. However, the existing approaches are all based on the assumption that the underlying data are exact (or certain). Due to the intrinsic differences between uncertain and certain data, these methods are designed only for ranked queries in certain databases and cannot be applied to uncertain case directly. Motivated by this, we propose novel solutions to speed up the probabilistic ranked query (PRank) with monotonic preference functions over the uncertain database. Specifically, we introduce two effective pruning methods, spatial and probabilistic pruning, to help reduce the PRank search space. A special case of PRank with linear preference functions is also studied. Then, we seamlessly integrate these pruning heuristics into the PRank query procedure. Furthermore, we propose and tackle the PRank query processing over the join of two distinct uncertain databases. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approaches in answering PRank queries, in terms of both wall clock time and the number of candidates to be refined.

#index 1364948
#* Spectral Anonymization of Data
#@ Thomas A. Lasko;Staal A. Vinterbo
#t 2010
#c 7
#! The goal of data anonymization is to allow the release of scientifically useful data in a form that protects the privacy of its subjects. This requires more than simply removing personal identifiers from the data because an attacker can still use auxiliary information to infer sensitive individual information. Additional perturbation is necessary to prevent these inferences, and the challenge is to perturb the data in a way that preserves its analytic utility. No existing anonymization algorithm provides both perfect privacy protection and perfect analytic utility. We make the new observation that anonymization algorithms are not required to operate in the original vector-space basis of the data, and many algorithms can be improved by operating in a judiciously chosen alternate basis. A spectral basis derived from the data's eigenvectors is one that can provide substantial improvement. We introduce the term spectral anonymization to refer to an algorithm that uses a spectral basis for anonymization, and give two illustrative examples. We also propose new measures of privacy protection that are more general and more informative than existing measures, and a principled reference standard with which to define adequate privacy protection.

#index 1364949
#* ViDE: A Vision-Based Approach for Deep Web Data Extraction
#@ Wei Liu;Xiaofeng Meng;Weiyi Meng
#t 2010
#c 7
#! Deep Web contents are accessed by queries submitted to Web databases and the returned data records are enwrapped in dynamically generated Web pages (they will be called deep Web pages in this paper). Extracting structured data from deep Web pages is a challenging problem due to the underlying intricate structures of such pages. Until now, a large number of techniques have been proposed to address this problem, but all of them have inherent limitations because they are Web-page-programming-language-dependent. As the popular two-dimensional media, the contents on Web pages are always displayed regularly for users to browse. This motivates us to seek a different way for deep Web data extraction to overcome the limitations of previous works by utilizing some interesting common visual features on the deep Web pages. In this paper, a novel vision-based approach that is Web-page-programming-language-independent is proposed. This approach primarily utilizes the visual features on the deep Web pages to implement deep Web data extraction, including data record extraction and data item extraction. We also propose a new evaluation measure revision to capture the amount of human effort needed to produce perfect extraction. Our experiments on a large set of Web databases show that the proposed vision-based approach is highly effective for deep Web data extraction.

#index 1364950
#* A Distance Measure Approach to Exploring the Rough Set Boundary Region for Attribute Reduction
#@ Neil Parthalain;Qiang Shen;Richard Jensen
#t 2010
#c 7
#! Feature Selection (FS) or Attribute Reduction techniques are employed for dimensionality reduction and aim to select a subset of the original features of a data set which are rich in the most useful information. The benefits of employing FS techniques include improved data visualization and transparency, a reduction in training and utilization times and potentially, improved prediction performance. Many approaches based on rough set theory up to now, have employed the dependency function, which is based on lower approximations as an evaluation step in the FS process. However, by examining only that information which is considered to be certain and ignoring the boundary region, or region of uncertainty, much useful information is lost. This paper examines a rough set FS technique which uses the information gathered from both the lower approximation dependency value and a distance metric which considers the number of objects in the boundary region and the distance of those objects from the lower approximation. The use of this measure in rough set feature selection can result in smaller subset sizes than those obtained using the dependency function alone. This demonstrates that there is much valuable information to be extracted from the boundary region. Experimental results are presented for both crisp and real-valued data and compared with two other FS techniques in terms of subset size, runtimes, and classification accuracy.

#index 1364951
#* A General Framework of Time-Variant Bandwidth Allocation in the Data Broadcasting Environment
#@ Chung-Hua Chu;Hao-Ping Hung;Ming-Syan Chen
#t 2010
#c 7
#! Data broadcast is an advanced technique to realize large scalability and bandwidth utilization in a mobile computing environment. In this environment, the channel bandwidth of each channel is variant with time in real cases. However, traditional schemes do not consider time-variant bandwidth of each channel to schedule data items. Therefore, the above drawback degrades the performance in generating broadcast programs. In this paper, we address the problem of generating a broadcast program to disseminate data via multiple channels of time-variant bandwidth. In view of the characteristics of time-variant bandwidth, we propose an algorithm using adaptive allocation on time-variant bandwidth to generate the broadcast program to avoid the above drawback to minimize average waiting time. Experimental results show that our approach is able to generate the broadcast programs with high quality and is very efficient in a data broadcasting environment with the time-variant bandwidth.

#index 1364952
#* Efficient Multidimensional Suppression for K-Anonymity
#@ Slava Kisilevich;Lior Rokach;Yuval Elovici;Bracha Shapira
#t 2010
#c 7
#! Many applications that employ data mining techniques involve mining data that include private and sensitive information about the subjects. One way to enable effective data mining while preserving privacy is to anonymize the data set that includes private information about subjects before being released for data mining. One way to anonymize data set is to manipulate its content so that the records adhere to k-anonymity. Two common manipulation techniques used to achieve k-anonymity of a data set are generalization and suppression. Generalization refers to replacing a value with a less specific but semantically consistent value, while suppression refers to not releasing a value at all. Generalization is more commonly applied in this domain since suppression may dramatically reduce the quality of the data mining results if not properly used. However, generalization presents a major drawback as it requires a manually generated domain hierarchy taxonomy for every quasi-identifier in the data set on which k-anonymity has to be performed. In this paper, we propose a new method for achieving k-anonymity named K-anonymity of Classification Trees Using Suppression (kACTUS). In kACTUS, efficient multidimensional suppression is performed, i.e., values are suppressed only on certain records depending on other attribute values, without the need for manually produced domain hierarchy trees. Thus, in kACTUS, we identify attributes that have less influence on the classification of the data records and suppress them if needed in order to comply with k-anonymity. The kACTUS method was evaluated on 10 separate data sets to evaluate its accuracy as compared to other k-anonymity generalization- and suppression-based methods. Encouraging results suggest that kACTUS' predictive performance is better than that of existing k-anonymity algorithms. Specifically, on average, the accuracies of TDS, TDR, and kADET are lower than kACTUS in 3.5, 3.3, and 1.9 percent, respectively, despite their usage of manually defined domain trees. The accuracy gap is increased to 5.3, 4.3, and 3.1 percent, respectively, when no domain trees are used.

#index 1364953
#* Beyond Redundancies: A Metric-Invariant Method for Unsupervised Feature Selection
#@ Yuexian Hou;Peng Zhang;Tingxu Yan;Wenjie Li;Dawei Song
#t 2010
#c 7
#! A fundamental goal of unsupervised feature selection is denoising, which aims to identify and reduce noisy features that are not discriminative. Due to the lack of information about real classes, denoising is a challenging task. The noisy features can disturb the reasonable distance metric and result in unreasonable feature spaces, i.e., the feature spaces in which common clustering algorithms cannot effectively find real classes. To overcome the problem, we make a primary observation that the relevance of features is intrinsic and independent of any metric scaling on the feature space. This observation implies that feature selection should be invariant, at least to some extent, with respect to metric scaling. In this paper, we clarify the necessity of considering the metric invariance in unsupervised feature selection and propose a novel model incorporating metric invariance. Our proposed method is motivated by the following observations: if the statistic that guides the unsupervised feature selection process is invariant with respect to possible metric scaling, the solution of this model will also be invariant. Hence, if a metric-invariant model can distinguish discriminative features from noisy ones in a reasonable feature space, it will also work on the unreasonable counterpart transformed from the reasonable one by metric scaling. A theoretical justification of the metric invariance of our proposed model is given and the empirical evaluation demonstrates its promising performance.

#index 1364954
#* Constrained Dimensionality Reduction Using a Mixed-Norm Penalty Function with Neural Networks
#@ Huiwen Zeng;H. J. Trussell
#t 2010
#c 7
#! Reducing the dimensionality of a classification problem produces a more computationally-efficient system. Since the dimensionality of a classification problem is equivalent to the number of neurons in the first hidden layer of a network, this work shows how to eliminate neurons on that layer and simplify the problem. In the cases where the dimensionality cannot be reduced without some degradation in classification performance, we formulate and solve a constrained optimization problem that allows a trade-off between dimensionality and performance. We introduce a novel penalty function and combine it with bilevel optimization to solve the constrained problem. The performance of our method on synthetic and applied problems is superior to other known penalty functions such as weight decay, weight elimination, and Hoyer's function. An example of dimensionality reduction for hyperspectral image classification demonstrates the practicality of the new method. Finally, we show how the method can be extended to multilayer and multiclass neural network problems.

#index 1380961
#* A Game Theoretic Approach for Simultaneous Compaction and Equipartitioning of Spatial Data Sets
#@ Upavan Gupta;Nagarajan Ranganathan
#t 2010
#c 7
#! Data and object clustering techniques are used in a wide variety of scientific applications such as biology, pattern recognition, information systems, etc. Traditionally, clustering methods have focused on optimizing a single metric, however, several multidisciplinary applications such as robot team deployment, ad hoc networks, facility location, etc., require the simultaneous examination of multiple metrics during clustering. In this paper, we propose a novel approach for spatial data clustering based on the concepts of microeconomic theory, which can simultaneously optimize both the compaction and the equipartitioning objectives. The algorithm models a multistep, normal form game consisting of randomly initialized clusters as players that compete for the allocation of data objects from resource locations. A Nash-equilibrium-based methodology is used to derive solutions that are socially fair for all the players. After each step, the clusters are updated using the KMeans algorithm, and the process is repeated until the stopping criteria are satisfied. Extensive simulations were performed on several real data sets as well as artificially synthesized data sets to evaluate the efficacy of the algorithm. Experimental results indicate that the proposed algorithm yields significantly better results as compared to the traditional algorithms. Further, the proposed algorithm yields a high value of fairness, a metric that indicates the quality of the solution in terms of simultaneous optimization of the objectives. Also, the sensitivity of the various design parameters on the performance of our algorithm is analyzed and reported.

#index 1380962
#* A Nonsupervised Learning Framework of Human Behavior Patterns Based on Sequential Actions
#@ Sang Wan Lee;Yong Soo Kim;Zeungnam Bien
#t 2010
#c 7
#! In designing autonomous service systems such as assistive robots for the aged and the disabled, discovery and prediction of human actions are important and often crucial. Patterns of human behavior, however, involve ambiguity, uncertainty, complexity, and inconsistency caused by physical, logical, and emotional factors, and thus their modeling and recognition are known to be difficult. In this paper, a nonsupervised learning framework of human behavior patterns is suggested in consideration of human behavioral characteristics. Our approach consists of two steps. In the first step, a meaningful structure of data is discovered by using Agglomerative Iterative Bayesian Fuzzy Clustering (AIBFC) with a newly proposed cluster validity index. In the second step, the sequence of actions is learned on the basis of the structure discovered in the first step and by utilizing the proposed Fuzzy-state Q--learning (FSQL) process. These two learning steps are incorporated in an amalgamated framework, AIBFC-FSQL, which is capable of learning human behavior patterns in a nonsupervised manner and predicting subsequent human actions. Through a number of simulations with typical benchmark data sets, we show that the proposed learning method outperforms several well-known methods. We further conduct experiments with two challenging real-world databases to demonstrate its usefulness from a practical perspective.

#index 1380963
#* Duplicate-Insensitive Order Statistics Computation over Data Streams
#@ Ying Zhang;Xuemin Lin;Yidong Yuan;Masaru Kitsuregawa;Xiaofang Zhou;Jeffrey Wu Yu
#t 2010
#c 7
#! Duplicates in data streams may often be observed by the projection on a subspace and/or multiple recordings of objects. Without the uniqueness assumption on observed data elements, many conventional aggregates computation problems need to be further investigated due to their duplication-sensitive nature. In this paper, we present novel, space-efficient, one-scan algorithms to continuously maintain duplicate-insensitive order sketches so that rank-based queries can be approximately processed with a relative rank error guarantee \epsilon in the presence of data duplicates. Besides the space efficiency, the proposed algorithms are time-efficient and highly accurate. Moreover, our techniques may be immediately applied to the heavy hitter problem against distinct elements and to the existing fault-tolerant distributed communication techniques. A comprehensive performance study demonstrates that our algorithms can support real-time computation against high-speed data streams.

#index 1380964
#* Filter-Based Data Partitioning for Training Multiple Classifier Systems
#@ Rozita A. Dara;Masoud Makrehchi;Mohamed S. Kamel
#t 2010
#c 7
#! Data partitioning methods such as bagging and boosting have been extensively used in multiple classifier systems. These methods have shown a great potential for improving classification accuracy. This study is concerned with the analysis of training data distribution and its impact on the performance of multiple classifier systems. In this study, several feature-based and class-based measures are proposed. These measures can be used to estimate statistical characteristics of the training partitions. To assess the effectiveness of different types of training partitions, we generated a large number of disjoint training partitions with distinctive distributions. Then, we empirically assessed these training partitions and their impact on the performance of the system by utilizing the proposed feature-based and class-based measures. We applied the findings of this analysis and developed a new partitioning method called “Clustering, Declustering, and Selection” (CDS). This study presents a comparative analysis of several existing data partitioning methods including our proposed CDS approach.

#index 1380965
#* Learning to Adapt Web Information Extraction Knowledge and Discovering New Attributes via a Bayesian Approach
#@ Tak-Lam Wong;Wai Lam
#t 2010
#c 7
#! This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites. Our approach aims at automatically adapting the information extraction knowledge previously learned from a source Web site to a new unseen site, at the same time, discovering previously unseen attributes. Two kinds of text-related clues from the source Web site are considered. The first kind of clue is obtained from the extraction pattern contained in the previously learned wrapper. The second kind of clue is derived from the previously extracted or collected items. A generative model for the generation of the site-independent content information and the site-dependent layout format of the text fragments related to attribute values contained in a Web page is designed to harness the uncertainty involved. Bayesian learning and expectation-maximization (EM) techniques are developed under the proposed generative model for identifying new training data for learning the new wrapper for new unseen sites. Previously unseen attributes together with their semantic labels can also be discovered via another EM-based Bayesian learning based on the generative model. We have conducted extensive experiments from more than 30 real-world Web sites in three different domains to demonstrate the effectiveness of our framework.

#index 1380966
#* Efficient Algorithm for Localized Support Vector Machine
#@ Haibin Cheng;Pang-Ning Tan;Rong Jin
#t 2010
#c 7
#! This paper presents a framework called Localized Support Vector Machine (LSVM) for classifying data with nonlinear decision surfaces. Instead of building a sophisticated global model from the training data, LSVM constructs multiple linear SVMs, each of which is designed to accurately classify a given test example. A major limitation of this framework is its high computational cost since a unique model must be constructed for each test example. To overcome this limitation, we propose an efficient implementation of LSVM, termed Profile SVM (PSVM). PSVM partitions the training examples into clusters and builds a separate linear SVM model for each cluster. Our empirical results show that 1) LSVM and PSVM outperform nonlinear SVM for all 20 of the evaluated data sets and 2) PSVM achieves comparable performance as LSVM in terms of model accuracy but with significant computational savings. We also demonstrate the efficacy of the proposed approaches in terms of classifying data with spatial and temporal dependencies.

#index 1380967
#* Probabilistic Reverse Nearest Neighbor Queries on Uncertain Data
#@ Muhammad Aamir Cheema;Xuemin Lin;Wei Wang;Wenjie Zhang;Jian Pei
#t 2010
#c 7
#! Uncertain data are inherent in various important applications and reverse nearest neighbor (RNN) query is an important query type for many applications. While many different types of queries have been studied on uncertain data, there is no previous work on answering RNN queries on uncertain data. In this paper, we formalize probabilistic reverse nearest neighbor query that is to retrieve the objects from the uncertain data that have higher probability than a given threshold to be the RNN of an uncertain query object. We develop an efficient algorithm based on various novel pruning approaches that solves the probabilistic RNN queries on multidimensional uncertain data. The experimental results demonstrate that our algorithm is even more efficient than a sampling-based approximate algorithm for most of the cases and is highly scalable.

#index 1380968
#* Prospective Infectious Disease Outbreak Detection Using Markov Switching Models
#@ Hsin-Min Lu;Daniel Zeng;HsinChun Chen
#t 2010
#c 7
#! Accurate and timely detection of infectious disease outbreaks provides valuable information which can enable public health officials to respond to major public health threats in a timely fashion. However, disease outbreaks are often not directly observable. For surveillance systems used to detect outbreaks, noises caused by routine behavioral patterns and by special events can further complicate the detection task. Most existing detection methods combine a time series filtering procedure followed by a statistical surveillance method. The performance of this "two-step” detection method is hampered by the unrealistic assumption that the training data are outbreak-free. Moreover, existing approaches are sensitive to extreme values, which are common in real-world data sets. We considered the problem of identifying outbreak patterns in a syndrome count time series using Markov switching models. The disease outbreak states are modeled as hidden state variables which control the observed time series. A jump component is introduced to absorb sporadic extreme values that may otherwise weaken the ability to detect slow-moving disease outbreaks. Our approach outperformed several state-of-the-art detection methods in terms of detection sensitivity using both simulated and real-world data.

#index 1380969
#* Record Matching over Query Results from Multiple Web Databases
#@ Weifeng Su;Jiying Wang;Frederick H. Lochovsky
#t 2010
#c 7
#! Record matching, which identifies the records that represent the same real-world entity, is an important step for data integration. Most state-of-the-art record matching methods are supervised, which requires the user to provide training data. These methods are not applicable for the Web database scenario, where the records to match are query results dynamically generated on-the-fly. Such records are query-dependent and a prelearned method using training examples from previous query results may fail on the results of a new query. To address the problem of record matching in the Web database scenario, we present an unsupervised, online record matching method, UDD, which, for a given query, can effectively identify duplicates from the query result records of multiple Web databases. After removal of the same-source duplicates, the “presumed” nonduplicate records from the same source can be used as training examples alleviating the burden of users having to manually label training examples. Starting from the nonduplicate set, we use two cooperating classifiers, a weighted component similarity summing classifier and an SVM classifier, to iteratively identify duplicates in the query results from multiple Web databases. Experimental results show that UDD works well for the Web database scenario where existing supervised methods do not apply.

#index 1380970
#* The Tiled Bitmap Forensic Analysis Algorithm
#@ Kyriacos E. Pavlou;Richard T. Snodgrass
#t 2010
#c 7
#! Tampering of a database can be detected through the use of cryptographically strong hash functions. Subsequently, applied forensic analysis algorithms can help determine when, what, and perhaps ultimately who and why. This paper presents a novel forensic analysis algorithm, the Tiled Bitmap Algorithm, which is more efficient than prior forensic analysis algorithms. It introduces the notion of a candidate set (all possible locations of detected tampering(s)) and provides a complete characterization of the candidate set and its cardinality. An optimal algorithm for computing the candidate set is also presented. Finally, the implementation of the Tiled Bitmap Algorithm is discussed, along with a comparison to other forensic algorithms in terms of space/time complexity and cost. An example of candidate set generation and proofs of the theorems and lemmata and of algorithm correctness can be found in the appendix, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TKDE.2009.121.

#index 1380971
#* A Binary String Approach for Updates in Dynamic Ordered XML Data
#@ Hye-Kyeong Ko;SangKeun Lee
#t 2010
#c 7
#! To facilitate XML query processing, several labeling schemes have been proposed, in which the ancestor-descendant and parent-child relationships in XML queries can be quickly determined without accessing the original XML file. However, all of these existing schemes have to relabel the existing nodes or recalculate certain values when order-sensitive updates cause insertions, thus causing the label update cost to be high. In this paper, we propose a novel labeling scheme, called IBSL (Improved Binary String Labeling), which supports order-sensitive updates without relabeling or recalculation. In addition, we reuse the deleted labels at the same position in the XML tree. The conducted experimental results show that IBSL efficiently processes order-sensitive queries and leaf node/subtree updates.

#index 1380972
#* False Negative Problem of Counting Bloom Filter
#@ Deke Guo;Yunhao Liu;Xiangyang Li;Panlong Yang
#t 2010
#c 7
#! Bloom filter is effective, space-efficient data structure for concisely representing a data set and supporting approximate membership queries. Traditionally, researchers often believe that it is possible that a Bloom filter returns a false positive, but it will never return a false negative under well-behaved operations. By investigating the mainstream variants, however, we observe that a Bloom filter does return false negatives in many scenarios. In this work, we show that the undetectable incorrect deletion of false positive items and detectable incorrect deletion of multiaddress items are two general causes of false negative in a Bloom filter. We then measure the potential and exposed false negatives theoretically and practically. Inspired by the fact that the potential false negatives are usually not fully exposed, we propose a novel Bloom filter scheme, which increases the ratio of bits set to a value larger than one without decreasing the ratio of bits set to zero. Mathematical analysis and comprehensive experiments show that this design can reduce the number of exposed false negatives as well as decrease the likelihood of false positives. To the best of our knowledge, this is the first work dealing with both the false positive and false negative problems of Bloom filter systematically when supporting standard usages of item insertion, query, and deletion operations.

#index 1380973
#* Incremental Evaluation of Visible Nearest Neighbor Queries
#@ Sarana Nutanong;Egemen Tanin;Rui Zhang
#t 2010
#c 7
#! In many applications involving spatial objects, we are only interested in objects that are directly visible from query points. In this paper, we formulate the visible k nearest neighbor (VkNN) query and present incremental algorithms as a solution, with two variants differing in how to prune objects during the search process. One variant applies visibility pruning to only objects, whereas the other variant applies visibility pruning to index nodes as well. Our experimental results show that the latter outperforms the former. We further propose the aggregate VkNN query that finds the visible k nearest objects to a set of query points based on an aggregate distance function. We also propose two approaches to processing the aggregate VkNN query. One accesses the database via multiple VkNN queries, whereas the other issues an aggregate k nearest neighbor query to retrieve objects from the database and then re-rank the results based on the aggregate visible distance metric. With extensive experiments, we show that the latter approach consistently outperforms the former one.

#index 1380974
#* Incremental Maintenance of 2-Hop Labeling of Large Graphs
#@ Ramadhana Bramandia;Byron Choi;Wee Keong Ng
#t 2010
#c 7
#! Recent interests on xml, the Semantic Web, and Web ontology, among other topics, have sparked a renewed interest on graph-structured databases. A fundamental query on graphs is the reachability test of nodes. Recently, 2-hop labeling has been proposed to index a large collection of xml and/or graphs for efficient reachability tests. However, there has been few work on updates of 2-hop labeling. This is compounded by the fact that data may often change over time. In response to these, this paper studies incremental maintenance of 2-hop labeling. We identify the main reason for the inefficiency of updates of existing 2-hop labels. We propose three updatable 2-hop labelings, hybrids of 2-hop labeling, and their incremental maintenance algorithms. The proposed 2--hop labeling is derived from graph connectivity, as opposed to set cover which is used by most previous works. Our experimental evaluation illustrates the space efficiency and update performance of various kinds of 2-hop labelings. Our results show that our incremental maintenance algorithm can be two orders of magnitude faster than previous methods and the size of our 2-hop labeling can be comparable to existing 2-hop labeling. We conclude that there is a natural way to spare some index size for update performance in 2-hop labeling.

#index 1380975
#* Iso-Map: Energy-Efficient Contour Mapping in Wireless Sensor Networks
#@ Mo Li;Yunhao Liu
#t 2010
#c 7
#! Contour mapping is a crucial part of many wireless sensor network applications. Many efforts have been made to avoid collecting data from all the sensors in the network and producing maps at the sink, which is proven to be inefficient. The existing approaches (often aggregation based), however, suffer from heavy transmission traffic and incur large computational overheads on each sensor node. We propose Iso-Map, an energy-efficient protocol for contour mapping, which builds contour maps based solely on the reports collected from intelligently selected “isoline nodes” in wireless sensor networks. Iso-Map achieves high-quality contour mapping while significantly reducing the generated traffic from O(n) to O(\sqrt n), where n is the total number of sensor nodes in the field. The pernode computation overhead is also restrained as a constant. We conduct comprehensive trace-driven simulations to verify this protocol, and demonstrate that Iso-Map outperforms the previous approaches in the sense that it produces contour maps of high fidelity with significantly reduced energy cost.

#index 1380976
#* Parallelizing Itinerary-Based KNN Query Processing in Wireless Sensor Networks
#@ Tao-Yang Fu;Wen-Chih Peng;Wang-Chien Lee
#t 2010
#c 7
#! Wireless sensor networks have been proposed for facilitating various monitoring applications (e.g., environmental monitoring and military surveillance) over a wide geographical region. In these applications, spatial queries that collect data from wireless sensor networks play an important role. One such query is the K-Nearest Neighbor (KNN) query that facilitates collection of sensor data samples based on a given query location and the number of samples specified (i.e., K). Recently, itinerary-based KNN query processing techniques, which propagate queries and collect data along a predetermined itinerary, have been developed. Prior studies demonstrate that itinerary-based KNN query processing algorithms are able to achieve better energy efficiency than other existing algorithms developed upon tree-based network infrastructures. However, how to derive itineraries for KNN query based on different performance requirements remains a challenging problem. In this paper, we propose a Parallel Concentric-circle Itinerary-based KNN (PCIKNN) query processing technique that derives different itineraries by optimizing either query latency or energy consumption. The performance of PCIKNN is analyzed mathematically and evaluated through extensive experiments. Experimental results show that PCIKNN outperforms the state-of-the-art techniques.

#index 1380977
#* The Impact of Diversity on Online Ensemble Learning in the Presence of Concept Drift
#@ Leandro L. Minku;Allan P. White;Xin Yao
#t 2010
#c 7
#! Online learning algorithms often have to operate in the presence of concept drift (i.e., the concepts to be learned can change with time). This paper presents a new categorization for concept drift, separating drifts according to different criteria into mutually exclusive and nonheterogeneous categories. Moreover, although ensembles of learning machines have been used to learn in the presence of concept drift, there has been no deep study of why they can be helpful for that and which of their features can contribute or not for that. As diversity is one of these features, we present a diversity analysis in the presence of different types of drifts. We show that, before the drift, ensembles with less diversity obtain lower test errors. On the other hand, it is a good strategy to maintain highly diverse ensembles to obtain lower test errors shortly after the drift independent on the type of drift, even though high diversity is more important for more severe drifts. Longer after the drift, high diversity becomes less important. Diversity by itself can help to reduce the initial increase in error caused by a drift, but does not provide the faster recovery from drifts in long-term.

#index 1380978
#* Mining Predictive k-CNF Expressions
#@ Anton Dries;Luc De Raedt;Siegfried Nijssen
#t 2010
#c 7
#! We adapt Mitchell's version space algorithm for mining k-CNF formulas. Advantages of this algorithm are that it runs in a single pass over the data, is conceptually simple, can be used for missing value prediction, and has interesting theoretical properties, while an empirical evaluation on classification tasks yields competitive predictive results.

#index 1380979
#* Automatic Ontology Matching via Upper Ontologies: A Systematic Evaluation
#@ Viviana Mascardi;Angela Locoro;Paolo Rosso
#t 2010
#c 7
#! “Ontology matching” is the process of finding correspondences between entities belonging to different ontologies. This paper describes a set of algorithms that exploit upper ontologies as semantic bridges in the ontology matching process and presents a systematic analysis of the relationships among features of matched ontologies (number of simple and composite concepts, stems, concepts at the top level, common English suffixes and prefixes, and ontology depth), matching algorithms, used upper ontologies, and experiment results. This analysis allowed us to state under which circumstances the exploitation of upper ontologies gives significant advantages with respect to traditional approaches that do no use them. We run experiments with SUMO-OWL (a restricted version of SUMO), OpenCyc, and DOLCE. The experiments demonstrate that when our “structural matching method via upper ontology” uses an upper ontology large enough (OpenCyc, SUMO-OWL), the recall is significantly improved while preserving the precision obtained without upper ontologies. Instead, our “nonstructural matching method” via OpenCyc and SUMO-OWL improves the precision and maintains the recall. The “mixed method” that combines the results of structural alignment without using upper ontologies and structural alignment via upper ontologies improves the recall and maintains the F-measure independently of the used upper ontology.

#index 1380980
#* Building a Rule-Based Classifier—A Fuzzy-Rough Set Approach
#@ Suyun Zhao;E. C. C. Tsang;Degang Chen;Xizhao Wang
#t 2010
#c 7
#! The fuzzy-rough set (FRS) methodology, as a useful tool to handle discernibility and fuzziness, has been widely studied. Some researchers studied on the rough approximation of fuzzy sets, while some others focused on studying one application of FRS: attribute reduction (i.e., feature selection). However, constructing classifier by using FRS, as another application of FRS, has been less studied. In this paper, we build a rule-based classifier by using one generalized FRS model after proposing a new concept named as “consistence degree” which is used as the critical value to keep the discernibility information invariant in the processing of rule induction. First, we generalized the existing FRS to a robust model with respect to misclassification and perturbation by incorporating one controlled threshold into knowledge representation of FRS. Second, we propose a concept named as “consistence degree” and by the strict mathematical reasoning, we show that this concept is reasonable as a critical value to reduce redundant attribute values in database. By employing this concept, we then design a discernibility vector to develop the algorithms of rule induction. The induced rule set can function as a classifier. Finally, the experimental results show that the proposed rule-based classifier is feasible and effective on noisy data.

#index 1380981
#* Closing the Loop in Webpage Understanding
#@ Chunyu Yang;Yong Cao;Zaiqing Nie;Jie Zhou;Ji-Rong Wen
#t 2010
#c 7
#! The two most important tasks in information extraction from the Web are webpage structure understanding and natural language sentences processing. However, little work has been done toward an integrated statistical model for understanding webpage structures and processing natural language sentences within the HTML elements. Our recent work on webpage understanding introduces a joint model of Hierarchical Conditional Random Fields (HCRFs) and extended Semi-Markov Conditional Random Fields (Semi-CRFs) to leverage the page structure understanding results in free text segmentation and labeling. In this top-down integration model, the decision of the HCRF model could guide the decision making of the Semi-CRF model. However, the drawback of the top-down integration strategy is also apparent, i.e., the decision of the Semi-CRF model could not be used by the HCRF model to guide its decision making. This paper proposed a novel framework called WebNLP, which enables bidirectional integration of page structure understanding and text understanding in an iterative manner. We have applied the proposed framework to local business entity extraction and Chinese person and organization name extraction. Experiments show that the WebNLP framework achieved significantly better performance than existing methods.

#index 1424127
#* Introduction to the Domain-Driven Data Mining Special Section
#@ Chenqi Zhang;Philip S. Yu;David Bell
#t 2010
#c 7
#! Summary form only given. In the last decade, data mining has emerged as one of the most dynamic and lively areas in information technology. Although many algorithms and techniques for data mining have been proposed, they either focus on domain independent techniques or on very specific domain problems. A general requirement in bridging the gap between academia and business is to cater to general domain-related issues surrounding real-life applications, such as constraints, organizational factors, domain expert knowledge, domain adaption, and operational knowledge. Unfortunately, these either have not been addressed, or have not been sufficiently addressed, in current data mining research and development.Domain-Driven Data Mining (D3M) aims to develop general principles, methodologies, and techniques for modeling and merging comprehensive domain-related factors and synthesized ubiquitous intelligence surrounding problem domains with the data mining process, and discovering knowledge to support business decision-making. This paper aims to report original, cutting-edge, and state-of-the-art progress in D3M. It covers theoretical and applied contributions aiming to: 1) propose next-generation data mining frameworks and processes for actionable knowledge discovery, 2) investigate effective (automated, human and machine-centered and/or human-machined-co-operated) principles and approaches for acquiring, representing, modelling, and engaging ubiquitous intelligence in real-world data mining, and 3) develop workable and operational systems balancing technical significance and applications concerns, and converting and delivering actionable knowledge into operational applications rules to seamlessly engage application processes and systems.

#index 1424128
#* Domain-Driven Data Mining: Challenges and Prospects
#@ Longbing Cao
#t 2010
#c 7
#! Traditional data mining research mainly focus]es on developing, demonstrating, and pushing the use of specific algorithms and models. The process of data mining stops at pattern identification. Consequently, a widely seen fact is that 1) many algorithms have been designed of which very few are repeatable and executable in the real world, 2) often many patterns are mined but a major proportion of them are either commonsense or of no particular interest to business, and 3) end users generally cannot easily understand and take them over for business use. In summary, we see that the findings are not actionable, and lack soft power in solving real-world complex problems. Thorough efforts are essential for promoting the actionability of knowledge discovery in real-world smart decision making. To this end, domain-driven data mining (D^3M) has been proposed to tackle the above issues, and promote the paradigm shift from “data-centered knowledge discovery” to “domain-driven, actionable knowledge delivery.” In D^3M, ubiquitous intelligence is incorporated into the mining process and models, and a corresponding problem-solving system is formed as the space for knowledge discovery and delivery. Based on our related work, this paper presents an overview of driving forces, theoretical frameworks, architectures, techniques, case studies, and open issues of D^3M. We understand D^3M discloses many critical issues with no thorough and mature solutions available for now, which indicates the challenges and prospects for this new topic.

#index 1424129
#* Bridging Domains Using World Wide Knowledge for Transfer Learning
#@ Evan Wei Xiang;Bin Cao;Derek Hao Hu;Qiang Yang
#t 2010
#c 7
#! A major problem of classification learning is the lack of ground-truth labeled data. It is usually expensive to label new data instances for training a model. To solve this problem, domain adaptation in transfer learning has been proposed to classify target domain data by using some other source domain data, even when the data may have different distributions. However, domain adaptation may not work well when the differences between the source and target domains are large. In this paper, we design a novel transfer learning approach, called BIG (Bridging Information Gap), to effectively extract useful knowledge in a worldwide knowledge base, which is then used to link the source and target domains for improving the classification performance. BIG works when the source and target domains share the same feature space but different underlying data distributions. Using the auxiliary source data, we can extract a “bridge” that allows cross-domain text classification problems to be solved using standard semisupervised learning algorithms. A major contribution of our work is that with BIG, a large amount of worldwide knowledge can be easily adapted and used for learning in the target domain. We conduct experiments on several real-world cross-domain text classification tasks and demonstrate that our proposed approach can outperform several existing domain adaptation approaches significantly.

#index 1424130
#* Knowledge-Based Interactive Postmining of Association Rules Using Ontologies
#@ Claudia Marinica;Fabrice Guillet
#t 2010
#c 7
#! In Data Mining, the usefulness of association rules is strongly limited by the huge amount of delivered rules. To overcome this drawback, several methods were proposed in the literature such as itemset concise representations, redundancy reduction, and postprocessing. However, being generally based on statistical information, most of these methods do not guarantee that the extracted rules are interesting for the user. Thus, it is crucial to help the decision-maker with an efficient postprocessing step in order to reduce the number of rules. This paper proposes a new interactive approach to prune and filter discovered rules. First, we propose to use ontologies in order to improve the integration of user knowledge in the postprocessing task. Second, we propose the Rule Schema formalism extending the specification language proposed by Liu et al. for user expectations. Furthermore, an interactive framework is designed to assist the user throughout the analyzing task. Applying our new approach over voluminous sets of rules, we were able, by integrating domain expert knowledge in the postprocessing step, to reduce the number of rules to several dozens or less. Moreover, the quality of the filtered rules was validated by the domain expert at various points in the interactive process.

#index 1424131
#* Logic-Based Pattern Discovery
#@ Alex Tze Hiang Sim;Maria Indrawan;Samar Zutshi;Bala Srinivasan
#t 2010
#c 7
#! In the data mining field, association rules are discovered having domain knowledge specified as a minimum support threshold. The accuracy in setting up this threshold directly influences the number and the quality of association rules discovered. Often, the number of association rules, even though large in number, misses some interesting rules and the rules' quality necessitates further analysis. As a result, decision making using these rules could lead to risky actions. We propose a framework to discover domain knowledge report as coherent rules. Coherent rules are discovered based on the properties of propositional logic, and therefore, requires no background knowledge to generate them. From the coherent rules discovered, association rules can be derived objectively and directly without knowing the level of minimum support threshold required. We provide analysis of the rules compare to those discovered via the a priori.

#index 1424132
#* Asking Generalized Queries to Domain Experts to Improve Learning
#@ Jun Du;Charles X. Ling
#t 2010
#c 7
#! With the assistance of a domain expert, active learning can often select or construct fewer examples to request their labels to build an accurate classifier. However, previous works of active learning can only generate and ask specific queries. In real-world applications, the domain experts (or oracles) are often more readily to answer “generalized queries” with don't-care attributes. The power of such generalized queries is that one generalized query is often equivalent to many specific ones. However, overly general queries are not good as answers from the domain experts (or oracles) can be highly uncertain, and this makes learning difficult. In this paper, we propose a novel active learning algorithm that asks good generalized queries. We, then, extend our algorithm to construct new, hierarchical features for both nominal and numeric attributes. We demonstrate experimentally that our new method asks significantly fewer queries compared with the previous works of active learning, even when the initial labeled data set is very small, and the oracle is inaccurate in class probability estimations. Our method can be readily deployed in real-world data mining tasks where obtaining labeled examples is costly.

#index 1424133
#* Domain-Driven Classification Based on Multiple Criteria and Multiple Constraint-Level Programming for Intelligent Credit Scoring
#@ Jing He;Yanchun Zhang;Yong Shi;Guangyan Huang
#t 2010
#c 7
#! Extracting knowledge from the transaction records and the personal data of credit card holders has great profit potential for the banking industry. The challenge is to detect/predict bankrupts and to keep and recruit the profitable customers. However, grouping and targeting credit card customers by traditional data-driven mining often does not directly meet the needs of the banking industry, because data-driven mining automatically generates classification outputs that are imprecise, meaningless, and beyond users' control. In this paper, we provide a novel domain-driven classification method that takes advantage of multiple criteria and multiple constraint-level programming for intelligent credit scoring. The method involves credit scoring to produce a set of customers' scores that allows the classification results actionable and controllable by human interaction during the scoring process. Domain knowledge and experts' experience parameters are built into the criteria and constraint functions of mathematical programming and the human and machine conversation is employed to generate an efficient and precise solution. Experiments based on various data sets validated the effectiveness and efficiency of the proposed methods.

#index 1424134
#* Signaling Potential Adverse Drug Reactions from Administrative Health Databases
#@ Huidong Jin;Jie Chen;Hongxing He;Chris Kelman;Damien McAullay;Christine M. O'Keefe
#t 2010
#c 7
#! The work is motivated by real-world applications of detecting Adverse Drug Reactions (ADRs) from administrative health databases. ADRs are a leading cause of hospitalization and death worldwide. Almost all current postmarket ADR signaling techniques are based on spontaneous ADR case reports, which suffer from serious underreporting and latency. However, administrative health data are widely and routinely collected. They, especially linked together, would contain evidence of all ADRs. To signal unexpected and infrequent patterns characteristic of ADRs, we propose a domain-driven knowledge representation Unexpected Temporal Association Rule (UTAR), its interestingness measure, unexlev, and a mining algorithm MUTARA (Mining UTARs given the Antecedent). We then establish an improved algorithm, HUNT, for highlighting infrequent and unexpected patterns by comparing their ranks based on unexlev with those based on traditional leverage. Various experimental results on real-world data substantiate that both MUTARA and HUNT can signal suspected ADRs while traditional association mining techniques cannot. HUNT can reliably shortlist statistically significantly more ADRs than MUTARA (p=0.00078). HUNT, e.g., not only shortlists the drug alendronate associated with esophagitis as MUTARA does, but also shortlists alendronate with diarrhoea and vomiting for older ({\rm age} \ge 60) females. We also discuss signaling ADRs systematically by using HUNT.

#index 1424135
#* Feature Selection Using f-Information Measures in Fuzzy Approximation Spaces
#@ Pradipta Maji;Sankar K. Pal
#t 2010
#c 7
#! The selection of nonredundant and relevant features of real-valued data sets is a highly challenging problem. A novel feature selection method is presented here based on fuzzy-rough sets by maximizing the relevance and minimizing the redundancy of the selected features. By introducing the fuzzy equivalence partition matrix, a novel representation of Shannon's entropy for fuzzy approximation spaces is proposed to measure the relevance and redundancy of features suitable for real-valued data sets. The fuzzy equivalence partition matrix also offers an efficient way to calculate many more information measures, termed as f-information measures. Several f-information measures are shown to be effective for selecting nonredundant and relevant features of real-valued data sets. This paper compares the performance of different f-information measures for feature selection in fuzzy approximation spaces. Some quantitative indexes are introduced based on fuzzy-rough sets for evaluating the performance of proposed method. The effectiveness of the proposed method, along with a comparison with other methods, is demonstrated on a set of real-life data sets.

#index 1424136
#* δ-Presence without Complete World Knowledge
#@ Mehmet Ercan Nergiz;Chris Clifton
#t 2010
#c 7
#! Advances in information technology, and its use in research, are increasing both the need for anonymized data and the risks of poor anonymization. In [CHECK END OF SENTENCE], we presented a new privacy metric, \delta-presence, that clearly links the quality of anonymization to the risk posed by inadequate anonymization. It was shown that existing anonymization techniques are inappropriate for situations where \delta--presence is a good metric (specifically, where knowing an individual is in the database poses a privacy risk). This article addresses a practical problem with [CHECK END OF SENTENCE], extending to situations where the data anonymizer is not assumed to have complete world knowledge. The algorithms are evaluated in the context of a real-world scenario, demonstrating practical applicability of the approach.

#index 1424137
#* Privacy-Preserving Gradient-Descent Methods
#@ Shuguo Han;Wee Keong Ng;Li Wan;Vincent C. S. Lee
#t 2010
#c 7
#! Gradient descent is a widely used paradigm for solving many optimization problems. Gradient descent aims to minimize a target function in order to reach a local minimum. In machine learning or data mining, this function corresponds to a decision model that is to be discovered. In this paper, we propose a preliminary formulation of gradient descent with data privacy preservation. We present two approaches—stochastic approach and least square approach—under different assumptions. Four protocols are proposed for the two approaches incorporating various secure building blocks for both horizontally and vertically partitioned data. We conduct experiments to evaluate the scalability of the proposed secure building blocks and the accuracy and efficiency of the protocols for four different scenarios. The excremental results show that the proposed secure building blocks are reasonably scalable and the proposed protocols allow us to determine a better secure protocol for the applications for each scenario.

#index 1424138
#* Dynamic Dissimilarity Measure for Support-Based Clustering
#@ Daewon Lee;Jaewook Lee
#t 2010
#c 7
#! Clustering methods utilizing support estimates of a data distribution have recently attracted much attention because of their ability to generate cluster boundaries of arbitrary shape and to deal with outliers efficiently. In this paper, we propose a novel dissimilarity measure based on a dynamical system associated with support estimating functions. Theoretical foundations of the proposed measure are developed and applied to construct a clustering method that can effectively partition the whole data space. Simulation results demonstrate that clustering based on the proposed dissimilarity measure is robust to the choice of kernel parameters and able to control the number of clusters efficiently.

#index 1424139
#* Kernel Discriminant Learning for Ordinal Regression
#@ Bing-Yu Sun;Jiuyong Li;Desheng Dash Wu;Xiao-Ming Zhang;Wen-Bo Li
#t 2010
#c 7
#! Ordinal regression has wide applications in many domains where the human evaluation plays a major role. Most current ordinal regression methods are based on Support Vector Machines (SVM) and suffer from the problems of ignoring the global information of the data and the high computational complexity. Linear Discriminant Analysis (LDA) and its kernel version, Kernel Discriminant Analysis (KDA), take into consideration the global information of the data together with the distribution of the classes for classification, but they have not been utilized for ordinal regression yet. In this paper, we propose a novel regression method by extending the Kernel Discriminant Learning using a rank constraint. The proposed algorithm is very efficient since the computational complexity is significantly lower than other ordinal regression methods. We demonstrate experimentally that the proposed method is capable of preserving the rank of data classes in a projected data space. In comparison to other benchmark ordinal regression methods, the proposed method is competitive in accuracy.

#index 1446816
#* Superseding Nearest Neighbor Search on Uncertain Spatial Databases
#@ Sze Man Yuen;Yufei Tao;Xiaokui Xiao;Jian Pei;Donghui Zhang
#t 2010
#c 7
#! This paper proposes a new problem, called superseding nearest neighbor search, on uncertain spatial databases, where each object is described by a multidimensional probability density function. Given a query point q, an object is a nearest neighbor (NN) candidate if it has a nonzero probability to be the NN of q. Given two NN-candidates o_1 and o_2, o_1 supersedeso_2 if o_1 is more likely to be closer to q. An object is a superseding nearest neighbor (SNN) of q, if it supersedes all the other NN-candidates. Sometimes no object is able to supersede every other NN-candidate. In this case, we return the SNN-core—the minimum set of NN-candidates each of which supersedes all the NN-candidates outside the SNN-core. Intuitively, the SNN-core contains the best objects, because any object outside the SNN-core is worse than all the objects in the SNN-core. We show that the SNN-core can be efficiently computed by utilizing a conventional multidimensional index, as confirmed by extensive experiments.

#index 1446817
#* An UpDown Directed Acyclic Graph Approach for Sequential Pattern Mining
#@ Jinlin Chen
#t 2010
#c 7
#! Traditional pattern growth-based approaches for sequential pattern mining derive length-(k+1) patterns based on the projected databases of length-k patterns recursively. At each level of recursion, they unidirectionally grow the length of detected patterns by one along the suffix of detected patterns, which needs k levels of recursion to find a length-k pattern. In this paper, a novel data structure, UpDown Directed Acyclic Graph (UDDAG), is invented for efficient sequential pattern mining. UDDAG allows bidirectional pattern growth along both ends of detected patterns. Thus, a length-k pattern can be detected in \lfloor log_{2}k+1\rfloor levels of recursion at best, which results in fewer levels of recursion and faster pattern growth. When minSup is large such that the average pattern length is close to 1, UDDAG and PrefixSpan have similar performance because the problem degrades into frequent item counting problem. However, UDDAG scales up much better. It often outperforms PrefixSpan by almost one order of magnitude in scalability tests. UDDAG is also considerably faster than Spade and LapinSpam. Except for extreme cases, UDDAG uses comparable memory to that of PrefixSpan and less memory than Spade and LapinSpam. Additionally, the special feature of UDDAG enables its extension toward applications involving searching in large spaces.

#index 1446818
#* Bregman Divergence-Based Regularization for Transfer Subspace Learning
#@ Si Si;Dacheng Tao;Bo Geng
#t 2010
#c 7
#! The regularization principals [CHECK END OF SENTENCE] lead approximation schemes to deal with various learning problems, e.g., the regularization of the norm in a reproducing kernel Hilbert space for the ill-posed problem. In this paper, we present a family of subspace learning algorithms based on a new form of regularization, which transfers the knowledge gained in training samples to testing samples. In particular, the new regularization minimizes the Bregman divergence between the distribution of training samples and that of testing samples in the selected subspace, so it boosts the performance when training and testing samples are not independent and identically distributed. To test the effectiveness of the proposed regularization, we introduce it to popular subspace learning algorithms, e.g., principal components analysis (PCA) for cross-domain face modeling; and Fisher's linear discriminant analysis (FLDA), locality preserving projections (LPP), marginal Fisher's analysis (MFA), and discriminative locality alignment (DLA) for cross-domain face recognition and text categorization. Finally, we present experimental evidence on both face image data sets and text data sets, suggesting that the proposed Bregman divergence-based regularization is effective to deal with cross-domain learning problems.

#index 1446819
#* Closeness: A New Privacy Measure for Data Publishing
#@ Ninghui Li;Tiancheng Li;Suresh Venkatasubramanian
#t 2010
#c 7
#! The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain “identifying” attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of \ell-diversity has been proposed to address this; \ell-diversity requires that each equivalence class has at least \ell well-represented (in Section 2) values for each sensitive attribute. In this paper, we show that \ell-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. Motivated by these limitations, we propose a new notion of privacy called “closeness.” We first present the base model t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We then propose a more flexible privacy model called (n,t)-closeness that offers higher utility. We describe our desiderata for designing a distance measure between two probability distributions and present two distance measures. We discuss the rationale for using closeness as a privacy measure and illustrate its advantages through examples and experiments.

#index 1446820
#* Conic Programming for Multitask Learning
#@ Tsuyoshi Kato;Hisashi Kashima;Masashi Sugiyama;Kiyoshi Asai
#t 2010
#c 7
#! When we have several related tasks, solving them simultaneously has been shown to be more effective than solving them individually. This approach is called multitask learning (MTL). In this paper, we propose a novel MTL algorithm. Our method controls the relatedness among the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions. We apply the above idea to support vector machines and show that the optimization problem can be cast as a second-order cone program, which is convex and can be solved efficiently. The usefulness of our approach is demonstrated in ordinal regression, link prediction, and collaborative filtering, each of which can be formulated as a structured multitask problem.

#index 1446821
#* Deriving Concept-Based User Profiles from Search Engine Logs
#@ Kenneth Wai-Ting Leung;Dik Lun Lee
#t 2010
#c 7
#! User profiling is a fundamental component of any personalization applications. Most existing user profiling strategies are based on objects that users are interested in (i.e., positive preferences), but not the objects that users dislike (i.e., negative preferences). In this paper, we focus on search engine personalization and develop several concept-based user profiling methods that are based on both positive and negative preferences. We evaluate the proposed methods against our previously proposed personalized query clustering method. Experimental results show that profiles which capture and utilize both of the user's positive and negative preferences perform the best. An important result from the experiments is that profiles with negative preferences can increase the separation between similar and dissimilar queries. The separation provides a clear threshold for an agglomerative clustering algorithm to terminate and improve the overall quality of the resulting query clusters.

#index 1446822
#* Incremental and General Evaluation of Reverse Nearest Neighbors
#@ James M. Kang;Mohamed F. Mokbel;Shashi Shekhar;Tian Xia;Donghui Zhang
#t 2010
#c 7
#! This paper presents a novel algorithm for Incremental and General Evaluation of continuous Reverse Nearest neighbor queries (IGERN, for short). The IGERN algorithm is general in that it is applicable for both continuous monochromatic and bichromatic reverse nearest neighbor queries. This problem is faced in a number of applications such as enhanced 911 services and in army strategic planning. A main challenge in these problems is to maintain the most up-to-date query answers as the data set frequently changes over time. Previous algorithms for monochromatic continuous reverse nearest neighbor queries rely mainly on monitoring at the worst case of six pie regions, whereas IGERN takes a radical approach by monitoring only a single region around the query object. The IGERN algorithm clearly outperforms the state-of-the-art algorithms in monochromatic queries. We also propose a new optimization for the monochromatic IGERN to reduce the number of nearest neighbor searches. Furthermore, a filter and refine approach for IGERN (FR-IGERN) is proposed for the continuous evaluation of bichromatic reverse nearest neighbor queries which is an optimized version of our previous approach. The computational complexity of IGERN and FR-IGERN is presented in comparison to the state-of-the-art algorithms in the monochromatic and bichromatic cases. In addition, the correctness of IGERN and FR-IGERN in both the monochromatic and bichromatic cases, respectively, are proved. Extensive experimental analysis using synthetic and real data sets shows that IGERN and FR-IGERN is efficient, is scalable, and outperforms previous techniques for continuous reverse nearest neighbor queries.

#index 1446823
#* P2P Reputation Management Using Distributed Identities and Decentralized Recommendation Chains
#@ Prashant Dewan;Partha Dasgupta
#t 2010
#c 7
#! Peer-to-peer (P2P) networks are vulnerable to peers who cheat, propagate malicious code, leech on the network, or simply do not cooperate. The traditional security techniques developed for the centralized distributed systems like client-server networks are insufficient for P2P networks by the virtue of their centralized nature. The absence of a central authority in a P2P network poses unique challenges for reputation management in the network. These challenges include identity management of the peers, secure reputation data management, Sybil attacks, and above all, availability of reputation data. In this paper, we present a cryptographic protocol for ensuring secure and timely availability of the reputation data of a peer to other peers at extremely low costs. The past behavior of the peer is encapsulated in its digital reputation, and is subsequently used to predict its future actions. As a result, a peer's reputation motivates it to cooperate and desist from malicious activities. The cryptographic protocol is coupled with self-certification and cryptographic mechanisms for identity management and countering Sybil attack. We illustrate the security and the efficiency of the system analytically and by means of simulations in a completely decentralized Gnutella-like P2P network.

#index 1446824
#* Performance Comparison of the {\rm R}^{\ast}-Tree and the Quadtree for kNN and Distance Join Queries
#@ You Jung Kim;Jignesh Patel
#t 2010
#c 7
#! Multidimensional point indexing plays a critical role in a variety of data-centric applications, including image retrieval, sequence matching, and moving object database search. A common choice of indexing method for these applications is often the "ubiquitous” {\rm R}^{\ast}-tree. Choosing the right indexing method requires careful consideration of various factors such as query operations and index construction methods. In this work, we present an experimental study comparing the {\rm R}^{\ast}-tree and Quadtree using various criteria including the query operations and index construction methods. Although a variety of query operations can be performed using these index structures, previous work has largely focused only on the range search operation. We go beyond this previous work and compare the performance of these index structures using k-nearest neighbor (kNN) and distance join queries. In addition, we also consider the impact of index construction methods in evaluating these index structures. Our study sheds light on how the choice of the underlying index structure affects the performance of different query operations, and shows that the method used for constructing the index and the dynamic nature of the data set has a dramatic impact on the performance of these index structures.

#index 1446825
#* Probabilistic Topic Models for Learning Terminological Ontologies
#@ Wei Wang;Payam Mamaani Barnaghi;Andrzej Bargiela
#t 2010
#c 7
#! Probabilistic topic models were originally developed and utilized for document modeling and topic extraction in Information Retrieval. In this paper, we describe a new approach for automatic learning of terminological ontologies from text corpus based on such models. In our approach, topic models are used as efficient dimension reduction techniques, which are able to capture semantic relationships between word-topic and topic-document interpreted in terms of probability distributions. We propose two algorithms for learning terminological ontologies using the principle of topic relationship and exploiting information theory with the probabilistic topic models learned. Experiments with different model parameters were conducted and learned ontology statements were evaluated by the domain experts. We have also compared the results of our method with two existing concept hierarchy learning methods on the same data set. The study shows that our method outperforms other methods in terms of recall and precision measures. The precision level of the learned ontology is sufficient for it to be deployed for the purpose of browsing, navigation, and information search and retrieval in digital libraries.

#index 1464038
#* Guest Editor's Introduction to the Special Section on the IEEE International Conference on Data Engineering
#@ Dik Lee;Raymond Ng;Yannis Ioannidis
#t 2010
#c 7
#! The eight papers in this special section were selected from the 93 long papers presented at the 25th IEEE International Conference on Data Engineering (ICDE 2009), held in Shanghai, China, on 29 March-2 April 2009.

#index 1464039
#* Towards an Effective XML Keyword Search
#@ Zhifeng Bao;Jiaheng Lu;Tok Wang Ling;Bo Chen
#t 2010
#c 7
#! Inspired by the great success of information retrieval (IR) style keyword search on the web, keyword search on XML has emerged recently. The difference between text database and XML database results in three new challenges: 1) Identify the user search intention, i.e., identify the XML node types that user wants to search for and search via. 2) Resolve keyword ambiguity problems: a keyword can appear as both a tag name and a text value of some node; a keyword can appear as the text values of different XML node types and carry different meanings; a keyword can appear as the tag name of different XML node types with different meanings. 3) As the search results are subtrees of the XML document, new scoring function is needed to estimate its relevance to a given query. However, existing methods cannot resolve these challenges, thus return low result quality in term of query relevance. In this paper, we propose an IR-style approach which basically utilizes the statistics of underlying XML data to address these challenges. We first propose specific guidelines that a search engine should meet in both search intention identification and relevance oriented ranking for search results. Then, based on these guidelines, we design novel formulae to identify the search for nodes and search via nodes of a query, and present a novel XML TF*IDF ranking strategy to rank the individual matches of all possible search intentions. To complement our result ranking framework, we also take the popularity into consideration for the results that have comparable relevance scores. Lastly, extensive experiments have been conducted to show the effectiveness of our approach.

#index 1464040
#* Continuous Subgraph Pattern Search over Certain and Uncertain Graph Streams
#@ Lei Chen;Changliang Wang
#t 2010
#c 7
#! Search over graph databases has attracted much attention recently due to its usefulness in many fields, such as the analysis of chemical compounds, intrusion detection in network traffic data, and pattern matching over users' visiting logs. However, most of the existing works focus on search over static graph databases, while in many real applications, graphs are changing over time. In this paper, we investigate a new problem on continuous subgraph pattern search under the situation where multiple target graphs are constantly changing in a stream style, namely, the subgraph pattern search over graph streams. Obviously, the proposed problem is a continuous join between query patterns and graph streams where the join predicate is the existence of subgraph isomorphism. Due to the NP-completeness of subgraph isomorphism checking, to achieve the real-time monitoring of the existence of certain subgraph patterns, we would like to avoid using subgraph isomorphism verification to find the exact query-stream subgraph isomorphic pairs but to offer an approximate answer that could report all probable pairs without missing any actual answer pairs. Therefore, we propose a lightweight yet effective feature structure called Node-Neighbor Tree to filter out false candidate query-stream pairs. To reduce the computational cost, we propose a novel idea, projecting the feature structures into a numerical vector space and conducting dominant relationship checking in the projected space. We design two methods to efficiently verify dominant relationships, and thus, answer the subgraph search over graph streams efficiently. In addition to answering queries over certain graph streams, we propose a novel problem, detecting the appearance of subgraph patterns over uncertain graph streams with high probability (i.e., larger than the probability threshold specified by users). To address this problem, we not only extend the proposed solutions for certain graphs streams, but also propose a new pruning technique by utilizing the probability threshold. We substantiate our methods with extensive experiments on both certain and uncertain graph streams.

#index 1464041
#* Multimodal Fusion for Video Search Reranking
#@ Shikui Wei;Yao Zhao;Zhenfeng Zhu;Nan Liu
#t 2010
#c 7
#! Analysis on click-through data from a very large search engine log shows that users are usually interested in the top-ranked portion of returned search results. Therefore, it is crucial for search engines to achieve high accuracy on the top-ranked documents. While many methods exist for boosting video search performance, they either pay less attention to the above factor or encounter difficulties in practical applications. In this paper, we present a flexible and effective reranking method, called CR-Reranking, to improve the retrieval effectiveness. To offer high accuracy on the top-ranked results, CR-Reranking employs a cross-reference (CR) strategy to fuse multimodal cues. Specifically, multimodal features are first utilized separately to rerank the initial returned results at the cluster level, and then all the ranked clusters from different modalities are cooperatively used to infer the shots with high relevance. Experimental results show that the search quality, especially on the top-ranked results, is improved significantly.

#index 1464042
#* Projective Distribution of XQuery with Updates
#@ Ying Zhang;Nan Tang;Peter Boncz
#t 2010
#c 7
#! We investigate techniques to automatically decompose any XQuery query—including updating queries specified by the XQuery Update Facility (XQUF)—into subqueries, that can be executed near their data sources, i.e., function-shipping. The main challenge addressed here is to ensure that the decomposed queries properly respect XML node identity and preserve structural properties, when (parts of) XML nodes are sent over the network, effectively copying them. We start by precisely characterizing the conditions, under which pass-by-value parameter passing causes semantic differences between remote execution of an XQuery expression and its local execution. We then formulate a conservative strategy that effectively avoids decomposition in such cases. To broaden the possibilities of query distribution, we extend the pass-by-value semantics to a pass-by-fragment semantics, which keeps better track of node identities and structural properties. The pass-by-fragment semantics is subsequently refined to a pass-by-projection semantics by means of a novel runtime XML projection technique, which safely eliminates most semantic differences between the local and remote execution of an XQuery expression, and strongly reduces message sizes. Finally, we discuss how these techniques can be used for updating queries, both under the standard W3C XQUF specification, as well as under an extended semantics that allows to update remote documents. The proposed techniques are implemented in XRPC, a simple yet efficient XQuery extension that enables function-shipping by adding a Remote Procedure Call mechanism to XQuery. Experiments on MonetDB/XQuery establish the performance potential of our XQuery decomposition techniques.

#index 1464043
#* Adaptive Join Operators for Result Rate Optimization on Streaming Inputs
#@ Mihaela Bornea;Vasilis Vassalos;Yannis Kotidis;Antonios Deligiannakis
#t 2010
#c 7
#! Adaptive join algorithms have recently attracted a lot of attention in emerging applications where data are provided by autonomous data sources through heterogeneous network environments. Their main advantage over traditional join techniques is that they can start producing join results as soon as the first input tuples are available, thus, improving pipelining by smoothing join result production and by masking source or network delays. In this paper, we first propose Double Index NEsted-loops Reactive join (DINER), a new adaptive two-way join algorithm for result rate maximization. DINER combines two key elements: an intuitive flushing policy that aims to increase the productivity of in-memory tuples in producing results during the online phase of the join, and a novel reentrant join technique that allows the algorithm to rapidly switch between processing in-memory and disk-resident tuples, thus, better exploiting temporary delays when new data are not available. We then extend the applicability of the proposed technique for a more challenging setup: handling more than two inputs. Multiple Index NEsted-loop Reactive join (MINER) is a multiway join operator that inherits its principles from DINER. Our experiments using real and synthetic data sets demonstrate that DINER outperforms previous adaptive join algorithms in producing result tuples at a significantly higher rate, while making better use of the available memory. Our experiments also shows that in the presence of multiple inputs, MINER manages to produce a high percentage of early results, outperforming existing techniques for adaptive multiway join.

#index 1464044
#* Maintaining Recursive Views of Regions and Connectivity in Networks
#@ Mengmeng Liu;Nicholas E. Taylor;Wenchao Zhou;Zachary G. Ives;Boon Thau Loo
#t 2010
#c 7
#! The data management community has recently begun to consider declarative network routing and distributed acquisition: e.g., sensor networks that execute queries about contiguous regions, declarative networks that maintain shortest paths, and distributed and peer-to-peer stream systems that detect transitive relationships among data at the distributed sources. In each case, the fundamental operation is to maintain a view over dynamic network state. This view is typically distributed, recursive, and may contain aggregation, e.g., describing shortest paths or least costly paths. Surprisingly, solutions to computing such views are often domain-specific, expensive, and incomplete. We recast the problem as incremental recursive view maintenance given distributed streams of updates to tuples: new stream data becomes insert operations and tuple expirations become deletions. We develop techniques to maintain compact information about tuple derivability or data provenance. We complement this with techniques to reduce communication: aggregate selections to prune irrelevant aggregation tuples, provenance-aware operators that determine when tuples are no longer derivable and remove them from the view, and shipping operators that reduce the information being propagated while still maintaining correct answers. We validate our work in a distributed setting with sensor and network router queries, showing significant gains in communication overhead without sacrificing performance.

#index 1464045
#* Histograms and Wavelets on Probabilistic Data
#@ Graham Cormode;Minos Garofalakis
#t 2010
#c 7
#! There is a growing realization that uncertain information is a first-class citizen in modern database management. As such, we need techniques to correctly and efficiently process uncertain data in database systems. In particular, data reduction techniques that can produce concise, accurate synopses of large probabilistic relations are crucial. Similar to their deterministic relation counterparts, such compact probabilistic data synopses can form the foundation for human understanding and interactive data exploration, probabilistic query planning and optimization, and fast approximate query processing in probabilistic database systems. In this paper, we introduce definitions and algorithms for building histogram- and Haar wavelet-based synopses on probabilistic data. The core problem is to choose a set of histogram bucket boundaries or wavelet coefficients to optimize the accuracy of the approximate representation of a collection of probabilistic tuples under a given error metric. For a variety of different error metrics, we devise efficient algorithms that construct optimal or near optimal size B histogram and wavelet synopses. This requires careful analysis of the structure of the probability distributions, and novel extensions of known dynamic-programming-based techniques for the deterministic domain. Our experiments show that this approach clearly outperforms simple ideas, such as building summaries for samples drawn from the data distribution, while taking equal or less time.

#index 1464046
#* Query Processing Using Distance Oracles for Spatial Networks
#@ Jagan Sankaranarayanan;Hanan Samet
#t 2010
#c 7
#! The popularity of location-based services and the need to do real-time processing on them has led to an interest in performing queries on transportation networks, such as finding shortest paths and finding nearest neighbors. The challenge here is that the efficient execution of spatial operations usually involves the computation of distance along a spatial network instead of "as the crow flies,” which is not simple. Techniques are described that enable the determination of the network distance between any pair of points (i.e., vertices) with as little as O(n) space rather than having to store the n^2 distances between all pairs. This is done by being willing to expend a bit more time to achieve this goal such as O(\log n) instead of O(1), as well as by accepting an error \varepsilon in the accuracy of the distance that is provided. The strategy that is adopted reduces the space requirements and is based on the ability to identify groups of source and destination vertices for which the distance is approximately the same within some \varepsilon. The reductions are achieved by introducing a construct termed a distance oracle that yields an estimate of the network distance (termed the \varepsilon-approximate distance) between any two vertices in the spatial network. The distance oracle is obtained by showing how to adapt the well-separated pair technique from computational geometry to spatial networks. Initially, an \varepsilon-approximate distance oracle of size O({n\over \varepsilon^2} ) is used that is capable of retrieving the approximate network distance in O(\log n) time using a B-tree. The retrieval time can be theoretically reduced further to O(1) time by proposing another \varepsilon-approximate distance oracle of size O({n \log n\over \varepsilon^2} ) that uses a hash table. Experimental results indicate that the proposed technique is scalable and can be applied to sufficiently large road networks. For example, a 10-percent-approximate oracle (\varepsilon = 0.1) on a large network yielded an average error of 0.9 percent with 90 percent of the answers having an error of 2 percent or less and an average retrieval time of 68 \mu{\rm seconds}. The fact that the network distance can be approximated by one value is used to show how a number of spatial queries can be formulated using appropriate SQL constructs and a few built-in primitives. The result is that these operations can be executed on almost any modern database with no modifications, while taking advantage of the existing query optimizers and query processing strategies.

#index 1464047
#* BinRank: Scaling Dynamic Authority-Based Search Using Materialized Subgraphs
#@ Heasoo Hwang;Andrey Balmin;Berthold Reinwald;Erik Nijkamp
#t 2010
#c 7
#! Dynamic authority-based keyword search algorithms, such as ObjectRank and personalized PageRank, leverage semantic link information to provide high quality, high recall search in databases, and the Web. Conceptually, these algorithms require a query-time PageRank-style iterative computation over the full graph. This computation is too expensive for large graphs, and not feasible at query time. Alternatively, building an index of precomputed results for some or all keywords involves very expensive preprocessing. We introduce BinRank, a system that approximates ObjectRank results by utilizing a hybrid approach inspired by materialized views in traditional query processing. We materialize a number of relatively small subsets of the data graph in such a way that any keyword query can be answered by running ObjectRank on only one of the subgraphs. BinRank generates the subgraphs by partitioning all the terms in the corpus based on their co-occurrence, executing ObjectRank for each partition using the terms to generate a set of random walk starting points, and keeping only those objects that receive non-negligible scores. The intuition is that a subgraph that contains all objects and links relevant to a set of related terms should have all the information needed to rank objects with respect to one of these terms. We demonstrate that BinRank can achieve subsecond query execution time on the English Wikipedia data set, while producing high-quality search results that closely approximate the results of ObjectRank on the original graph. The Wikipedia link graph contains about 10^8 edges, which is at least two orders of magnitude larger than what prior state of the art dynamic authority-based search systems have been able to demonstrate. Our experimental evaluation investigates the trade-off between query execution time, quality of the results, and storage requirements of BinRank.

#index 1464048
#* Guest Editors' Introduction: Special Section on Mining Large Uncertain and Probabilistic Databases
#@ Reynold Cheng;Michael Chau;Minos Garofalakis;Jeffrey Xu Yu
#t 2010
#c 7
#! The four papers in this special section were selected from 23 submissions and represent recent advances in the mining of uncertain databases. The works present new techniques for mining patterns, clustering, and ranking on uncertain data.

#index 1464049
#* Mining Frequent Subgraph Patterns from Uncertain Graph Data
#@ Zhaonian Zou;Jianzhong Li;Hong Gao;Shuo Zhang
#t 2010
#c 7
#! In many real applications, graph data is subject to uncertainties due to incompleteness and imprecision of data. Mining such uncertain graph data is semantically different from and computationally more challenging than mining conventional exact graph data. This paper investigates the problem of mining uncertain graph data and especially focuses on mining frequent subgraph patterns on an uncertain graph database. A novel model of uncertain graphs is presented, and the frequent subgraph pattern mining problem is formalized by introducing a new measure, called expected support. This problem is proved to be NP-hard. An approximate mining algorithm is proposed to find a set of approximately frequent subgraph patterns by allowing an error tolerance on expected supports of discovered subgraph patterns. The algorithm uses efficient methods to determine whether a subgraph pattern can be output or not and a new pruning method to reduce the complexity of examining subgraph patterns. Analytical and experimental results show that the algorithm is very efficient, accurate, and scalable for large uncertain graph databases. To the best of our knowledge, this paper is the first one to investigate the problem of mining frequent subgraph patterns from uncertain graph data.

#index 1464050
#* An Information-Theoretic Foundation for the Measurement of Discrimination Information
#@ Di Cai
#t 2010
#c 7
#! Hitherto, it has not been easy to interpret the meaning of the amount of discrimination information conveyed in a term rationally and explicitly within practical application contexts; it has not been simple to introduce the concept of the extent of semantic relatedness between two terms meaningfully and successfully into scientific discussions. This study is part of an attempt to do this. We attempt to answer two important questions: 1) What is the discrimination information conveyed by a term and how to measure it? 2) What is the relatedness between two terms and how to estimate it? We focus on the first question and present an in-depth investigation into the discrimination measures based on several information measures, which are widely used in a variety of applications. The relatedness measures are then naturally defined according to the individual discrimination measures. Some key points are made for clarifying potential problems arising from using the relatedness measures, and solutions are suggested. Two example applications in the contexts of text mining and information retrieval are provided. The aim of this study, of which this paper forms part, is to establish a unified theoretical framework, with measurement of discrimination information (MDI) at the core, for achieving effective measurement of semantic relatedness (MSR). Due to its generality, our method can be expected to be a useful tool with a wide range of application areas.

#index 1464051
#* Completely Lazy Learning
#@ Eric K. Garcia;Sergey Feldman;Maya R. Gupta;Santosh Srivastava
#t 2010
#c 7
#! Local classifiers are sometimes called lazy learners because they do not train a classifier until presented with a test sample. However, such methods are generally not completely lazy because the neighborhood size k (or other locality parameter) is usually chosen by cross validation on the training set, which can require significant preprocessing and risks overfitting. We propose a simple alternative to cross validation of the neighborhood size that requires no preprocessing: instead of committing to one neighborhood size, average the discriminants for multiple neighborhoods. We show that this forms an expected estimated posterior that minimizes the expected Bregman loss with respect to the uncertainty about the neighborhood choice. We analyze this approach for six standard and state-of-the-art local classifiers, including discriminative adaptive metric kNN (DANN), a local support vector machine (SVM-KNN), hyperplane distance nearest neighbor (HKNN), and a new local Bayesian quadratic discriminant analysis (local BDA). The empirical effectiveness of this technique versus cross validation is confirmed with experiments on seven benchmark data sets, showing that similar classification performance can be attained without any training.

#index 1464052
#* Clustering Uncertain Data Using Voronoi Diagrams and R-Tree Index
#@ Ben Kao;Sau Dan Lee;Foris K. F. Lee;David W. Cheung;Wai-Shing Ho
#t 2010
#c 7
#! We study the problem of clustering uncertain objects whose locations are described by probability density functions (pdfs). We show that the UK-means algorithm, which generalizes the k-means algorithm to handle uncertain objects, is very inefficient. The inefficiency comes from the fact that UK-means computes expected distances (EDs) between objects and cluster representatives. For arbitrary pdfs, expected distances are computed by numerical integrations, which are costly operations. We propose pruning techniques that are based on Voronoi diagrams to reduce the number of expected distance calculations. These techniques are analytically proven to be more effective than the basic bounding-box-based technique previously known in the literature. We then introduce an R-tree index to organize the uncertain objects so as to reduce pruning overheads. We conduct experiments to evaluate the effectiveness of our novel techniques. We show that our techniques are additive and, when used in combination, significantly outperform previously known methods.

#index 1464053
#* Scalable Probabilistic Similarity Ranking in Uncertain Databases
#@ Thomas Bernecker;Hans-Peter Kriegel;Nikos Mamoulis;Matthias Renz;Andreas Zuefle
#t 2010
#c 7
#! This paper introduces a scalable approach for probabilistic top-k similarity ranking on uncertain vector data. Each uncertain object is represented by a set of vector instances that is assumed to be mutually exclusive. The objective is to rank the uncertain data according to their distance to a reference object. We propose a framework that incrementally computes for each object instance and ranking position, the probability of the object falling at that ranking position. The resulting rank probability distribution can serve as input for several state-of-the-art probabilistic ranking models. Existing approaches compute this probability distribution by applying the Poisson binomial recurrence technique of quadratic complexity. In this paper, we theoretically as well as experimentally show that our framework reduces this to a linear-time complexity while having the same memory requirements, facilitated by incremental accessing of the uncertain vector instances in increasing order of their distance to the reference object. Furthermore, we show how the output of our method can be used to apply probabilistic top-k ranking for the objects, according to different state-of-the-art definitions. We conduct an experimental evaluation on synthetic and real data, which demonstrates the efficiency of our approach.

#index 1464054
#* Effectively Indexing the Uncertain Space
#@ Ying Zhang;Xuemin Lin;Wenjie Zhang;Jianmin Wang;Qianlu Lin
#t 2010
#c 7
#! With the rapid development of various optical, infrared, and radar sensors and GPS techniques, there are a huge amount of multidimensional uncertain data collected and accumulated everyday. Recently, considerable research efforts have been made in the field of indexing, analyzing, and mining uncertain data. As shown in a recent book [CHECK END OF SENTENCE] on uncertain data, in order to efficiently manage and mine uncertain data, effective indexing techniques are highly desirable. Based on the observation that the existing index structures for multidimensional data are sensitive to the size or shape of uncertain regions of uncertain objects and the queries, in this paper, we introduce a novel R-Tree-based inverted index structure, named UI-Tree, to efficiently support various queries including range queries, similarity joins, and their size estimation, as well as top-k range query, over multidimensional uncertain objects against continuous or discrete cases. Comprehensive experiments are conducted on both real data and synthetic data to demonstrate the efficiency of our techniques.

#index 1464055
#* Credibility: How Agents Can Handle Unfair Third-Party Testimonies in Computational Trust Models
#@ Jianshu Weng;Zhiqi Shen;Chunyan Miao;Angela Goh;Cyril Leung
#t 2010
#c 7
#! Usually, agents within multiagent systems represent different stakeholders that have their own distinct and sometimes conflicting interests and objectives. They would behave in such a way so as to achieve their own objectives, even at the cost of others. Therefore, there are risks in interacting with other agents. A number of computational trust models have been proposed to manage such risk. However, the performance of most computational trust models that rely on third-party recommendations as part of the mechanism to derive trust is easily deteriorated by the presence of unfair testimonies. There have been several attempts to combat the influence of unfair testimonies. Nevertheless, they are either not readily applicable since they require additional information which is not available in realistic settings, or ad hoc as they are tightly coupled with specific trust models. Against this background, a general credibility model is proposed in this paper. Empirical studies have shown that the proposed credibility model is more effective than related work in mitigating the adverse influence of unfair testimonies.

#index 1464056
#* Flexible Frameworks for Actionable Knowledge Discovery
#@ Longbing Cao;Yanchang Zhao;Huaifeng Zhang;Dan Luo;Chengqi Zhang;E. K. Park
#t 2010
#c 7
#! Most data mining algorithms and tools stop at the mining and delivery of patterns satisfying expected technical interestingness. There are often many patterns mined but business people either are not interested in them or do not know what follow-up actions to take to support their business decisions. This issue has seriously affected the widespread employment of advanced data mining techniques in greatly promoting enterprise operational quality and productivity. In this paper, we present a formal view of actionable knowledge discovery (AKD) from the system and decision-making perspectives. AKD is a closed optimization problem-solving process from problem definition, framework/model design to actionable pattern discovery, and is designed to deliver operable business rules that can be seamlessly associated or integrated with business processes and systems. To support such processes, we correspondingly propose, formalize, and illustrate four types of generic AKD frameworks: Postanalysis-based AKD, Unified-Interestingness-based AKD, Combined-Mining-based AKD, and Multisource Combined-Mining-based AKD (MSCM-AKD). A real-life case study of MSCM-based AKD is demonstrated to extract debt prevention patterns from social security data. Substantial experiments show that the proposed frameworks are sufficiently general, flexible, and practical to tackle many complex problems and applications by extracting actionable deliverables for instant decision making.

#index 1464057
#* Managing Multidimensional Historical Aggregate Data in Unstructured P2P Networks
#@ Filippo Furfaro;Giuseppe M. Mazzeo;Andrea Pugliese
#t 2010
#c 7
#! A P2P-based framework supporting the extraction of aggregates from historical multidimensional data is proposed, which provides efficient and robust query evaluation. When a data population is published, data are summarized in a synopsis, consisting of an index built on top of a set of subsynopses (storing compressed representations of distinct data portions). The index and the subsynopses are distributed across the network, and suitable replication mechanisms taking into account the query workload and network conditions are employed that provide the appropriate coverage for both the index and the subsynopses.

#index 1464058
#* Personalizing Web Directories with the Aid of Web Usage Data
#@ Dimitrios Pierrakos;George Paliouras
#t 2010
#c 7
#! This paper presents a knowledge discovery framework for the construction of Community Web Directories, a concept that we introduced in our recent work, applying personalization to Web directories. In this context, the Web directory is viewed as a thematic hierarchy and personalization is realized by constructing user community models on the basis of usage data. In contrast to most of the work on Web usage mining, the usage data that are analyzed here correspond to user navigation throughout the Web, rather than a particular Web site, exhibiting as a result a high degree of thematic diversity. For modeling the user communities, we introduce a novel methodology that combines the users' browsing behavior with thematic information from the Web directories. Following this methodology, we enhance the clustering and probabilistic approaches presented in previous work and also present a new algorithm that combines these two approaches. The resulting community models take the form of Community Web Directories. The proposed personalization methodology is evaluated both on a specialized artificial and a general-purpose Web directory, indicating its potential value to the Web user. The experiments also assess the effectiveness of the different machine learning techniques on the task.

#index 1464059
#* An Efficient Concept-Based Mining Model for Enhancing Text Clustering
#@ Shady Shehata;Fakhri Karray;Mohamed Kamel
#t 2010
#c 7
#! Most of the common techniques in text mining are based on the statistical analysis of a term, either word or phrase. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes more to the meaning of its sentences than the other term. Thus, the underlying text mining model should indicate terms that capture the semantics of text. In this case, the mining model can capture terms that present the concepts of the sentence, which leads to discovery of the topic of the document. A new concept-based mining model that analyzes terms on the sentence, document, and corpus levels is introduced. The concept-based mining model can effectively discriminate between nonimportant terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed mining model consists of sentence-based concept analysis, document-based concept analysis, corpus-based concept-analysis, and concept-based similarity measure. The term which contributes to the sentence semantics is analyzed on the sentence, document, and corpus levels rather than the traditional analysis of the document only. The proposed model can efficiently find significant matching concepts between documents, according to the semantics of their sentences. The similarity between documents is calculated based on a new concept-based similarity measure. The proposed similarity measure takes full advantage of using the concept analysis measures on the sentence, document, and corpus levels in calculating the similarity between documents. Large sets of experiments using the proposed concept-based mining model on different data sets in text clustering are conducted. The experiments demonstrate extensive comparison between the concept-based analysis and the traditional analysis. Experimental results demonstrate the substantial enhancement of the clustering quality using the sentence-based, document-based, corpus-based, and combined approach concept analysis.

#index 1464060
#* Adaptive Subspace Symbolization for Content-Based Video Detection
#@ Xiangmin Zhou;Xiaofang Zhou;Lei Chen;Yanfeng Shu;Athman Bouguettaya;John A. Taylor
#t 2010
#c 7
#! Efficiently and effectively identifying similar videos is an important and nontrivial problem in content-based video retrieval. This paper proposes a subspace symbolization approach, namely SUDS, for content-based retrieval on very large video databases. The novelty of SUDS is that it explores the data distribution in subspaces to build a visual dictionary with which the videos are processed by deriving the string matching techniques with two-step data simplification. Specifically, we first propose an adaptive approach, called VLP, to extract a series of dominant subspaces of variable lengths from the whole visual feature space without the constraint of dimension consecutiveness. A stable visual dictionary is built by clustering the video keyframes over each dominant subspace. A compact video representation model is developed by transforming each keyframe into a word that is a series of symbols in the dominant subspaces, and further each video into a series of words. Then, we present an innovative similarity measure called CVE, which adopts a complementary information compensation scheme based on the visual features and sequence context of videos. Finally, an efficient two-layered index strategy with a number of query optimizations is proposed to facilitate video retrieval. The experimental results demonstrate the high effectiveness and efficiency of SUDS.

#index 1464061
#* Combating the Small Sample Class Imbalance Problem Using Feature Selection
#@ Mike Wasikowski;Xue-wen Chen
#t 2010
#c 7
#! The class imbalance problem is encountered in real-world applications of machine learning and results in a classifier's suboptimal performance. Researchers have rigorously studied the resampling, algorithms, and feature selection approaches to this problem. No systematic studies have been conducted to understand how well these methods combat the class imbalance problem and which of these methods best manage the different challenges posed by imbalanced data sets. In particular, feature selection has rarely been studied outside of text classification problems. Additionally, no studies have looked at the additional problem of learning from small samples. This paper presents a first systematic comparison of the three types of methods developed for imbalanced data classification problems and of seven feature selection metrics evaluated on small sample data sets from different applications. We evaluated the performance of these metrics using area under the receiver operating characteristic (AUC) and area under the precision-recall curve (PRC). We compared each metric on the average performance across all problems and on the likelihood of a metric yielding the best performance on a specific problem. We examined the performance of these metrics inside each problem domain. Finally, we evaluated the efficacy of these metrics to see which perform best across algorithms. Our results showed that signal-to-noise correlation coefficient (S2N) and Feature Assessment by Sliding Thresholds (FAST) are great candidates for feature selection in most applications, especially when selecting very small numbers of features.

#index 1464062
#* Enhanced Visual Analysis for Cluster Tendency Assessment and Data Partitioning
#@ Liang Wang;Xin Geng;James Bezdek;Christopher Leckie;Ramamohanarao Kotagiri
#t 2010
#c 7
#! Visual methods have been widely studied and used in data cluster analysis. Given a pairwise dissimilarity matrix {\schmi D} of a set of n objects, visual methods such as the VAT algorithm generally represent {\schmi D} as an n\times n image {\rm I}(\tilde{{\schmi D}}) where the objects are reordered to reveal hidden cluster structure as dark blocks along the diagonal of the image. A major limitation of such methods is their inability to highlight cluster structure when {\schmi D} contains highly complex clusters. This paper addresses this limitation by proposing a Spectral VAT algorithm, where {\schmi D} is mapped to {\schmi D}^{\prime } in a graph embedding space and then reordered to {{\tilde{\schmi D}^{\prime }}} using the VAT algorithm. A strategy for automatic determination of the number of clusters in {\rm I}({\tilde{{\schmi D}^{\prime }}}) is then proposed, as well as a visual method for cluster formation from {\rm I}({\tilde{{\schmi D}^{\prime }}}) based on the difference between diagonal blocks and off-diagonal blocks. A sampling-based extended scheme is also proposed to enable visual cluster analysis for large data sets. Extensive experimental results on several synthetic and real-world data sets validate our algorithms.

#index 1464063
#* Enriching One Taxonomy Using Another
#@ L. Venkata Subramaniam;Amit Anil Nanavati;Sougata Mukherjea
#t 2010
#c 7
#! Taxonomies, representing hierarchical data, are a key knowledge source in multiple disciplines. Information processing across taxonomies is not possible unless they are appropriately merged for commonalities and differences. For taxonomy merging, the first task is to identify common concepts between the taxonomies. Then, these common concepts along with their associated concepts in the two taxonomies need to be integrated. Doing this in a conflict-free manner is a challenging task and generally requires human intervention. In this paper, we explore the possibility of asymmetrically merging one taxonomy into another automatically. Given one or more source taxonomies and a destination taxonomy, modeled as directed acyclic graphs, we present intuitive algorithms that merge relevant portions of the source taxonomies into the destination taxonomy. We prove that our algorithms are conflict-free, information lossless, and scalable. We also define precision and recall measures for evaluating enriched taxonomies, such as {\rm T_A}, the result of merging two taxonomies, with {\rm T_I}, the ideal merger. Our experiments indicate the effectiveness of our approach.

#index 1464064
#* Heuristic Approaches for the Quartet Method of Hierarchical Clustering
#@ Sergio Consoli;Kenneth Darby-Dowman;Gijs Geleijnse;Jan Korst;Steffen Pauws
#t 2010
#c 7
#! Given a set of objects and their pairwise distances, we wish to determine a visual representation of the data. We use the quartet paradigm to compute a hierarchy of clusters of the objects. The method is based on an NP-hard graph optimization problem called the Minimum Quartet Tree Cost problem. This paper presents and compares several heuristic approaches to approximate the optimal hierarchy. The performance of the algorithms is tested through extensive computational experiments and it is shown that the Reduced Variable Neighborhood Search heuristic is the most effective approach to the problem, obtaining high-quality solutions in short computational running times.

#index 1464065
#* Nearest Surrounder Queries
#@ Ken C.  K. Lee;Wang-Chien Lee;Hong Va Leong
#t 2010
#c 7
#! In this paper, we present a new type of spatial queries called Nearest Surrounder (NS) queries. An NS query determines the nearest polygon-shaped spatial objects (referred to as nearest surrounder objects) and their orientations with respect to a query point from an object set. Besides, we derive two NS query variants, namely, multitier NS (m-NS) queries and angle-constrained NS (ANS) queries. An m-NS query searches multiple layers of NS objects for the same range of angles from a query point. An ANS query searches for NS objects within a specified range of angles. To evaluate NS queries and their variants, we explore angle-based and distance-based bound properties of polygons, and devise two efficient algorithms, namely, Sweep and Ripple, based on R-tree. The algorithms access objects in an order according to their orientations and distances with respect to a given query point, respectively. They are efficient as they can finish a search with one index lookup. Besides, they can progressively deliver a query result. Through empirical studies, we evaluate the proposed algorithms and report their performance for both synthetic and real object sets.

#index 1464066
#* Non-Negative Matrix Factorization for Semisupervised Heterogeneous Data Coclustering
#@ Yanhua Chen;Lijun Wang;Ming Dong
#t 2010
#c 7
#! Coclustering heterogeneous data has attracted extensive attention recently due to its high impact on various important applications, such us text mining, image retrieval, and bioinformatics. However, data coclustering without any prior knowledge or background information is still a challenging problem. In this paper, we propose a Semisupervised Non-negative Matrix Factorization (SS-NMF) framework for data coclustering. Specifically, our method computes new relational matrices by incorporating user provided constraints through simultaneous distance metric learning and modality selection. Using an iterative algorithm, we then perform trifactorizations of the new matrices to infer the clusters of different data types and their correspondence. Theoretically, we prove the convergence and correctness of SS-NMF coclustering and show the relationship between SS-NMF with other well-known coclustering models. Through extensive experiments conducted on publicly available text, gene expression, and image data sets, we demonstrate the superior performance of SS-NMF for heterogeneous data coclustering.

#index 1464067
#* The Context and the SitBAC Models for Privacy Preservation—An Experimental Comparison of Model Comprehension and Synthesis
#@ Dizza Beimel;Mor Peleg
#t 2010
#c 7
#! Situation-Based Access Control (SitBAC) is a conceptual model for representing access control policies of healthcare organizations by characterizing situations of access to patient data. The SitBAC model enables formal representation of access situations as an ontology of concepts (Patient, Data Requestor, EHR, Task, and Response) along with their attributes and relationships. A competing access control model is the Contextual Role-Based Access Control (Context) model. The Context model uses logical expressions (rules) that specify contextual authorizations (i.e., characteristics of access requests that are available at access time). Open questions that relate to formal representation of scenarios involving access to patient data are: 1) which of the two models yields a formal representation that is easier to comprehend; 2) which of the two models facilitates the synthesis of correct models, and how does the task complexity affect the performance of comprehension and synthesis. In this study, we address these questions through a controlled experiment. The results of the experiment suggest that while there are no differences between the two models when it comes to comprehending or synthesizing simple scenarios of data access, for complex scenarios, there is a significant advantage to the SitBAC model in terms of both comprehension and synthesis.

#index 1464068
#* A Survey on Transfer Learning
#@ Sinno Jialin Pan;Qiang Yang
#t 2010
#c 7
#! A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.

#index 1512987
#* Coclustering Multiple Heterogeneous Domains: Linear Combinations and Agreements
#@ Gianluigi Greco;Antonella Guzzo;Luigi Pontieri
#t 2010
#c 7
#! The high-order coclustering problem, i.e., the problem of simultaneously clustering heterogeneous types of domain, has become an active research area in the last few years, due to the notable impact it has on several application scenarios. This problem is generally faced by optimizing a weighted combination of functions measuring the quality of coclustering over each pair of domains, where weights are chosen based on the supposed reliability/relevance of their correlation. However, little knowledge is likely to be available, in practice, in order to set these weights in a definite and precise manner. And, more importantly, it might even be conceptually unclear whether to prefer a weighing scheme over others, in those cases where functions encode contrasting goals so that improving the quality for a pair of domains leads to a deterioration for other pairs. The aim of this paper is precisely to shed light on the impact of weighting schemes on techniques based on linear combinations of pairwise objective functions, and to define an approach that overcomes the above problems by looking for an agreement—intuitively, a kind of compromise—among the various domains, thereby getting rid of the need to define an appropriate weighting scheme. Two algorithms performing coclustering on "star-structured” domains, based on linear combinations and agreements, respectively, have been designed within an information-theoretic framework. Results from a thorough experimentation, on both synthetic and real data, are discussed, in order to assess the effectiveness of the approaches and to get more insight into their actual behavior.

#index 1512988
#* Hiding Sequential and Spatiotemporal Patterns
#@ Osman Abul;Francesco Bonchi;Fosca Giannotti
#t 2010
#c 7
#! The process of discovering relevant patterns holding in a database was first indicated as a threat to database security by O'Leary in [CHECK END OF SENTENCE]. Since then, many different approaches for knowledge hiding have emerged over the years, mainly in the context of association rules and frequent item sets mining. Following many real-world data and application demands, in this paper, we shift the problem of knowledge hiding to contexts where both the data and the extracted knowledge have a sequential structure. We define the problem of hiding sequential patterns and show its NP-hardness. Thus, we devise heuristics and a polynomial sanitization algorithm. Starting from this framework, we specialize it to the more complex case of spatiotemporal patterns extracted from moving objects databases. Finally, we discuss a possible kind of attack to our model, which exploits the knowledge of the underlying road network, and enhance our model to protect from this kind of attack. An exhaustive experiential analysis on real-world data sets shows the effectiveness of our proposal.

#index 1512989
#* Cross-Domain Learning from Multiple Sources: A Consensus Regularization Perspective
#@ Fuzhen Zhuang;Ping Luo;Hui Xiong;Yuhong Xiong;Qing He;Zhongzhi Shi
#t 2010
#c 7
#! Classification across different domains studies how to adapt a learning model from one domain to another domain which shares similar data characteristics. While there are a number of existing works along this line, many of them are only focused on learning from a single source domain to a target domain. In particular, a remaining challenge is how to apply the knowledge learned from multiple source domains to a target domain. Indeed, data from multiple source domains can be semantically related, but have different data distributions. It is not clear how to exploit the distribution differences among multiple source domains to boost the learning performance in a target domain. To that end, in this paper, we propose a consensus regularization framework for learning from multiple source domains to a target domain. In this framework, a local classifier is trained by considering both local data available in one source domain and the prediction consensus with the classifiers learned from other source domains. Moreover, we provide a theoretical analysis as well as an empirical study of the proposed consensus regularization framework. The experimental results on text categorization and image classification problems show the effectiveness of this consensus regularization learning method. Finally, to deal with the situation that the multiple source domains are geographically distributed, we also develop the distributed version of the proposed algorithm, which avoids the need to upload all the data to a centralized location and helps to mitigate privacy concerns.

#index 1512990
#* Effective Determination of Mobile Agent Itineraries for Data Aggregation on Sensor Networks
#@ Charalampos Konstantopoulos;Aristides Mpitziopoulos;Damianos Gavalas;Grammati Pantziou
#t 2010
#c 7
#! A key feature of wireless sensor networks (WSNs) is the collaborative processing, where the correlation existing over the local data of sensor nodes (SNs) is exploited so that the total data volume can be reduced (data aggregation). The use of Mobile Agents (MAs), i.e., software entities able of migrating among nodes and resuming execution naturally, fits in this scenario; the local data of an SN can be combined with the data collected by an MA from other SNs in a way that depends on the specific program code of the MA. In this paper, we consider the problem of calculating near-optimal routes for MAs that incrementally aggregate the data as they visit the nodes in a distributed sensor network. Our algorithm follows a greedy-like approach always selecting the next node to be included in an itinerary in such a way that the cost of the so far formed itineraries is kept minimum at each step. Simulation results confirm the high effectiveness of the proposed algorithm as well as its performance gain over alternative approaches. Also, with the use of proper data structures, the computational complexity of the algorithm is kept low as it is formally proved in the paper.

#index 1512991
#* Efficient Routing of Subspace Skyline Queries over Highly Distributed Data
#@ Akrivi Vlachou;Christos Doulkeridis;Yannis Kotidis;Michalis Vazirgiannis
#t 2010
#c 7
#! Data generation increases at highly dynamic rates, making its storage, processing, and update costs at one central location excessive. The P2P paradigm emerges as a powerful model for organizing and searching large data repositories distributed over independent sources. Advanced query operators, such as skyline queries, are necessary in order to help users handle the huge amount of available data. A skyline query retrieves the set of nondominated data points in a multidimensional data set. Skyline query processing in P2P networks poses inherent challenges and demands nontraditional techniques, due to the distribution of content and the lack of global knowledge. Relying on a superpeer architecture, we propose a threshold-based algorithm, called SKYPEER and its variants, for efficient computation of skyline points in arbitrary subspaces, while reducing both computational time and volume of transmitted data. Furthermore, we address the problem of routing skyline queries over the superpeer network and we propose an efficient routing mechanism, namely SKYPEER^+, which further improves the performance by reducing the number of contacted superpeers. Finally, we provide an extensive experimental evaluation showing that our approach performs efficiently and provides a viable solution when a large degree of distribution is required.

#index 1512992
#* Instinct-Based Mating in Genetic Algorithms Applied to the Tuning of 1-NN Classifiers
#@ Thiago Quirino;Miroslav Kubat;Nicholas J. Bryan
#t 2010
#c 7
#! The behavior of the genetic algorithm (GA), a popular approach to search and optimization problems, is known to depend, among other factors, on the fitness function formula, the recombination operator, and the mutation operator. What has received less attention is the impact of the mating strategy that selects the chromosomes to be paired for recombination. Existing GA implementations mostly choose them probabilistically, according to their fitness function values, but we show that more sophisticated mating strategies can not only accelerate the search, but perhaps even improve the quality of the GA-generated solution. In our implementation, we took inspiration from the "opposites-attract” principle that is so common in nature. As a testbed, we chose the problem of 1-NN classifier tuning where genetic solutions have been employed before, and are thus well-understood by the research community. We propose three "instinct-based” mating strategies and experimentally investigate their behaviors.

#index 1512993
#* Statistical Model Computation with UDFs
#@ Carlos Ordonez
#t 2010
#c 7
#! Statistical models are generally computed outside a DBMS due to their mathematical complexity. We introduce techniques to efficiently compute fundamental statistical models inside a DBMS exploiting User-Defined Functions (UDFs). Specifically, we study the computation of linear regression, PCA, clustering, and Naive Bayes. Two summary matrices on the data set are mathematically shown to be essential for all models: the linear sum of points and the quadratic sum of cross products of points. We consider two layouts for the input data set: horizontal and vertical. We first introduce efficient SQL queries to compute summary matrices and score the data set. Based on the SQL framework, we introduce UDFs that work in a single table scan: aggregate UDFs to compute summary matrices for all models and a set of primitive scalar UDFs to score data sets. Experiments compare UDFs and SQL queries (running inside the DBMS) with C++ (analyzing exported files). In general, UDFs are faster than SQL queries and not much slower than C++. Considering export times, C++ is slower than UDFs and SQL queries. Statistical models based on precomputed summary matrices are computed in a few seconds. UDFs scale linearly and only require one table scan, highlighting their efficiency.

#index 1512994
#* Multiobjective Neural Network Ensembles Based on Regularized Negative Correlation Learning
#@ Huanhuan Chen;Xin Yao
#t 2010
#c 7
#! Negative Correlation Learning (NCL) [CHECK END OF SENTENCE], [CHECK END OF SENTENCE] is a neural network ensemble learning algorithm which introduces a correlation penalty term to the cost function of each individual network so that each neural network minimizes its mean-square-error (MSE) together with the correlation. This paper describes NCL in detail and observes that the NCL corresponds to training the entire ensemble as a single learning machine that only minimizes the MSE without regularization. This insight explains that NCL is prone to overfitting the noise in the training set. The paper analyzes this problem and proposes the multiobjective regularized negative correlation learning (MRNCL) algorithm which incorporates an additional regularization term for the ensemble and uses the evolutionary multiobjective algorithm to design ensembles. In MRNCL, we define the crossover and mutation operators and adopt nondominated sorting algorithm with fitness sharing and rank-based fitness assignment. The experiments on synthetic data as well as real-world data sets demonstrate that MRNCL achieves better performance than NCL, especially when the noise level is nontrivial in the data set. In the experimental discussion, we give three reasons why our algorithm outperforms others.

#index 1512995
#* Voting Systems with Trust Mechanisms in Cyberspace: Vulnerabilities and Defenses
#@ Qinyuan Feng;Yan Lindsay Sun;Ling Liu;Yafei Yang;Yafei Dai
#t 2010
#c 7
#! With the popularity of voting systems in cyberspace, there is growing evidence that current voting systems can be manipulated by fake votes. This problem has attracted many researchers working on guarding voting systems in two areas: relieving the effect of dishonest votes by evaluating the trust of voters, and limiting the resources that can be used by attackers, such as the number of voters and the number of votes. In this paper, we argue that powering voting systems with trust and limiting attack resources are not enough. We present a novel attack named as Reputation Trap (RepTrap). Our case study and experiments show that this new attack needs much less resources to manipulate the voting systems and has a much higher success rate compared with existing attacks. We further identify the reasons behind this attack and propose two defense schemes accordingly. In the first scheme, we hide correlation knowledge from attackers to reduce their chance to affect the honest voters. In the second scheme, we introduce robustness-of-evidence, a new metric, in trust calculation to reduce their effect on honest voters. We conduct extensive experiments to validate our approach. The results show that our defense schemes not only can reduce the success rate of attacks but also significantly increase the amount of resources an adversary needs to launch a successful attack.

#index 1512996
#* XCDSearch: An XML Context-Driven Search Engine
#@ Kamal Taha;Ramez Elmasri
#t 2010
#c 7
#! We present in this paper, a context-driven search engine called XCDSearch for answering XML Keyword-based queries as well as Loosely Structured queries, using a stack-based sort-merge algorithm. Most current research is focused on building relationships between data elements based solely on their labels and proximity to one another, while overlooking the contexts of the elements, which may lead to erroneous results. Since a data element is generally a characteristic of its parent, its context is determined by its parent. We observe that we could treat each set of elements consisting of a parent and its children data elements as one unified entity, and then use a stack-based sort-merge algorithm employing context-driven search techniques for determining the relationships between the different unified entities. We evaluated XCDSearch experimentally and compared it with five other search engines. The results showed marked improvement.

#index 1512997
#* Efficient Mining of Large Maximal Bicliques from 3D Symmetric Adjacency Matrix
#@ S. Selvan;R V Nataraj
#t 2010
#c 7
#! In this paper, we address the problem of mining large maximal bicliques from a three-dimensional Boolean symmetric adjacency matrix. We propose CubeMiner-MBC algorithm which enumerates all the maximal bicliques satisfying the user-specified size constraints. Our algorithm enumerates all bicliques with less memory in depth first manner and does not store the previously computed patterns in the main memory for duplicate detection. To efficiently prune duplicate patterns, we have proposed a subtree pruning technique which reduces the total number of nodes that are processed and also reduces the total number of duplicate patterns that are generated. We have also incorporated several optimizations for efficient cutter generation and closure checking. Experiments involving several synthetic data sets show that our algorithm takes less running time than CubeMiner algorithm.

#index 1512998
#* Using Proximity Search to Estimate Authority Flow
#@ Vagelis Hristidis;Yannis Papakonstantinou;Ramakrishna Varadarajan
#t 2010
#c 7
#! Authority flow and proximity search have been used extensively in measuring the association between entities in data graphs, ranging from the web to relational and XML databases. These two ranking factors have been used and studied separately in the past. In addition to their semantic differences, a key advantage of proximity search is the existence of efficient execution algorithms. In contrast, due to the complexity of calculating the authority flow, current systems only use precomputed authority flows in runtime. This limitation prohibits authority flow to be used more effectively as a ranking factor. In this paper, we present a comparative analysis of the two ranking factors. We present an efficient approximation of authority flow based on proximity search. We analytically estimate the approximation error and how this affects the ranking of the results of a query.

#index 1513031
#* Efficient Lazy Evaluation of Rule-Based Programs
#@ Peter Van Weert
#t 2010
#c 7
#! Thirty years after Forgy's seminal dissertation, Rete remains the de facto standard matching algorithm. Despite promising research results, alternative algorithms such as TREAT and LEAPS have had little impact on modern production rule engines. Constraint Handling Rules (CHR) is a high-level, declarative programming language, similar to production rules. In recent years, CHR has increasingly been used in a wide range of general purpose applications. State-of-the-art CHR systems use LEAPS-like lazy matching, and implement a large body of novel program analyses and optimization techniques to further improve performance. While obviously related, CHR and production rules research have mostly evolved independently from each other. With this paper, we aim to foster cross fertilization of implementation techniques. We provide a lucid, comprehensive overview of CHR's rule evaluation methodology, and survey recent contributions to the field of lazy matching. Our empirical evaluation confirms that Rete-based engines would surely benefit from incorporating similar techniques and optimizations.

#index 1513032
#* A Configurable Rete-OO Engine for Reasoning with Different Types of Imperfect Information
#@ Davide Sottara;Paola Mello;Mark Proctor
#t 2010
#c 7
#! The RETE algorithm is a very efficient option for the development of a rule-based system, but it supports only boolean, first order logic. Many real-world contexts, instead, require some degree of vagueness or uncertainty to be handled in a robust and efficient manner, imposing a trade-off between the number of rules and the cases that can be handled with sufficient accuracy. Thus, in the first part of the paper, an extension of RETE networks is proposed, capable of handling a more general inferential process, which actually includes several types of schemes for reasoning with imperfect information. In particular, the architecture depends on a number of configuration parameters which could be set by the user, individually or as a whole for the entire rule base. The second part, then, shows how an appropriate combination of parameters can be used to emulate some of the most common, specialized engines: 3-valued logic, classical certainty factors, fuzzy, many-valued logic and Bayesian networks.

#index 1513033
#* Integrated Rule-Based Learning and Inference
#@ Ioannis Hatzilygeroudis;Jim Prentzas
#t 2010
#c 7
#! Neurules are a kind of integrated rules integrating neurocomputing and production rules. Each neurule is represented as an adaline unit. Thus, the corresponding neurule base consists of a number of autonomous adaline units (neurules). In this paper, we present the construction process and the inference mechanism of neurules and explore their generalization capabilities. The construction process, which also implements corresponding learning algorithm, creates neurules from a given empirical data set. The inference mechanism of neurules is integrated in its nature; it combines neurocomputing with symbolic processes. It is also interactive, i.e., it interacts with the user asking him/her to provide values for some variables necessary to carry on inference. As shown via experiments, the neurules' integrated inference mechanism is more efficient than the inference mechanism used in connectionist expert systems. Furthermore, neurules generalize much better than their constituent neural component (adaline unit) and are comparable to the backpropagation neural net (BPNN).

#index 1513034
#* A Deductive Spreadsheet System for End Users
#@ Marcelo Tallis;Robert M. Balzer
#t 2010
#c 7
#! We exploit the spreadsheet metaphor to make deductive problem-solving methods available to the vast population of spreadsheet end users. In particular, we show how the function-based problem-solving capabilities of spreadsheets can be extended to include logical deductive methods in a way that is consistent with the existing spreadsheet "look and feel.” We also show a spreadsheet-based framework for authoring logic implication rules. This framework was conceived with the objective of reproducing many of the characteristics that make spreadsheet programming accessible to end users. In the proposed framework, rule authors describe the semantics of a binary relation by constructing a functional spreadsheet model that computes the image of that binary relation. This model is subsequently translated into a collection of logic implication rules. We implemented this deductive spreadsheet system on top of Microsoft Excel and adopting the World Wide Web Consortium (W3C) standard ontology language {\rm OWL} {+} {\rm SWRL} formalisms.

#index 1513035
#* A Novel Combination of Answer Set Programming with Description Logics for the Semantic Web
#@ Thomas Lukasiewicz
#t 2010
#c 7
#! We present a novel combination of disjunctive programs under the answer set semantics with description logics for the Semantic Web. The combination is based on a well-balanced interface between disjunctive programs and description logics, which guarantees the decidability of the resulting formalism without assuming syntactic restrictions. We show that the new formalism has very nice semantic properties. In particular, it faithfully extends both disjunctive programs and description logics. Furthermore, we describe algorithms for reasoning in the new formalism, and we give a precise picture of its computational complexity. We also define the well-founded semantics for the normal case, where normal programs are combined with tractable description logics, and we explore its semantic and computational properties. In particular, we show that the well-founded semantics approximates the answer set semantics. We also describe algorithms for the problems of consistency checking and literal entailment under the well-founded semantics, and we give a precise picture of their computational complexity. As a crucial property, in the normal case, consistency checking and literal entailment under the well-founded semantics are both tractable in the data complexity, and even first-order rewritable (and thus can be done in LogSpace in the data complexity) in a special case that is especially useful for representing mappings between ontologies.

#index 1513036
#* A Guide to the Basic Logic Dialect for Rule Interchange on the Web
#@ Harold Boley;Michael Kifer
#t 2010
#c 7
#! The W3C Rule Interchange Format (RIF) is a forthcoming standard for exchanging rules among different systems and developing intelligent rule-based applications for the Semantic Web. The RIF architecture is conceived as a family of languages, called dialects. A RIF dialect is a rule-based language with an XML syntax and a well-defined semantics. The RIF Basic Logic Dialect (RIF-BLD) semantically corresponds to a Horn rule language with equality. RIF-BLD has a number of syntactic extensions with respect to traditional textbook Horn logic, which include F-logic frames and predicates with named arguments. RIF-BLD is also well integrated with the relevant Web standards. It provides Internationalized Resource Identifiers (IRIs), XML Schema datatypes, and is aligned with RDF and OWL. This paper is a guide to the essentials of RIF-BLD, its syntax, semantics, and XML serialization. At the same time, some important RIF-BLD features are omitted due to the space limitations, including datatypes, built-ins, and the integration with RDF and OWL.

#index 1513037
#* Dictionary-Based Compression for Long Time-Series Similarity
#@ Willis Lang;Michael Morse;Jignesh M. Patel
#t 2010
#c 7
#! Long time-series data sets are common in many domains, especially scientific domains. Applications in these fields often require comparing trajectories using similarity measures. Existing methods perform well for short time series but their evaluation cost degrades rapidly for longer time series. In this work, we develop a new time-series similarity measure called the Dictionary Compression Score (DCS) for determining time-series similarity. We also show that this method allows us to accurately and quickly calculate similarity for both short and long time series. We use the well-known Kolmogorov Complexity in information theory and the Lempel-Ziv compression framework as a basis to calculate similarity scores. We show that off-the-shelf compressors do not fair well for computing time-series similarity. To address this problem, we developed a novel dictionary-based compression technique to compute time-series similarity. We also develop heuristics to automatically identify suitable parameters for our method, thus, removing the task of parameter tuning found in other existing methods. We have extensively compared DCS with existing similarity methods for classification. Our experimental evaluation shows that for long time-series data sets, DCS is accurate, and it is also significantly faster than existing methods.

#index 1513038
#* From t-Closeness-Like Privacy to Postrandomization via Information Theory
#@ David Rebollo-Monedero;Jordi Forne;Josep Domingo-Ferrer
#t 2010
#c 7
#! t-Closeness is a privacy model recently defined for data anonymization. A data set is said to satisfy t-closeness if, for each group of records sharing a combination of key attributes, the distance between the distribution of a confidential attribute in the group and the distribution of the attribute in the entire data set is no more than a threshold t. Here, we define a privacy measure in terms of information theory, similar to t-closeness. Then, we use the tools of that theory to show that our privacy measure can be achieved by the postrandomization method (PRAM) for masking in the discrete case, and by a form of noise addition in the general case.

#index 1513039
#* Unsupervised Semantic Similarity Computation between Terms Using Web Documents
#@ Elias Iosif;Alexandros Potamianos
#t 2010
#c 7
#! In this work, Web-based metrics that compute the semantic similarity between words or terms are presented and compared with the state of the art. Starting from the fundamental assumption that similarity of context implies similarity of meaning, relevant Web documents are downloaded via a Web search engine and the contextual information of words of interest is compared (context-based similarity metrics). The proposed algorithms work automatically, do not require any human-annotated knowledge resources, e.g., ontologies, and can be generalized and applied to different languages. Context-based metrics are evaluated both on the Charles-Miller data set and on a medical term data set. It is shown that context-based similarity metrics significantly outperform co-occurrence-based metrics, in terms of correlation with human judgment, for both tasks. In addition, the proposed unsupervised context-based similarity computation algorithms are shown to be competitive with the state-of-the-art supervised semantic similarity algorithms that employ language-specific knowledge resources. Specifically, context-based metrics achieve correlation scores of up to 0.88 and 0.74 for the Charles-Miller and medical data sets, respectively. The effect of stop word filtering is also investigated for word and term similarity computation. Finally, the performance of context-based term similarity metrics is evaluated as a function of the number of Web documents used and for various feature weighting schemes.

#index 1513040
#* Guest Editors' Introduction: Rule Representation, Interchange, and Reasoning in Distributed, Heterogeneous Environments
#@ Nick Bassiliades;Guido Governatori;Adrian Paschke;Jurgen Dix
#t 2010
#c 7
#! The eight papers in this special section focus on the state-of-the-art approaches, solutions, and applications in the area of rule representation, reasoning, and interchange in the context of distributed, (partially) open, heterogeneous environments, such as the semantic Web, intelligent multiagent systems, event-driven architectures. and service-oriented computing.

#index 1513041
#* Defeasible Contextual Reasoning with Arguments in Ambient Intelligence
#@ Antonis Bikakis;Grigoris Antoniou
#t 2010
#c 7
#! The imperfect nature of context in Ambient Intelligence environments and the special characteristics of the entities that possess and share the available context information render contextual reasoning a very challenging task. The accomplishment of this task requires formal models that handle the involved entities as autonomous logic-based agents and provide methods for handling the imperfect and distributed nature of context. This paper proposes a solution based on the Multi-Context Systems paradigm in which local context knowledge of ambient agents is encoded in rule theories (contexts), and information flow between agents is achieved through mapping rules that associate concepts used by different contexts. To handle imperfect context, we extend Multi-Context Systems with nonmonotonic features: local defeasible theories, defeasible mapping rules, and a preference ordering on the system contexts. On top of this model, we have developed an argumentation framework that exploits context and preference information to resolve potential conflicts caused by the interaction of ambient agents through the mappings, and a distributed algorithm for query evaluation.

#index 1513042
#* A Rule-Based Trust Negotiation System
#@ Piero Bonatti;J. L. De Coi;Daniel Olmedilla;Luigi Sauro
#t 2010
#c 7
#! Open distributed environments, such as the World Wide Web, facilitate information sharing but provide limited support to the protection of sensitive information and resources. Trust negotiation (TN) frameworks have been proposed as a better solution for open environments, in which parties may get in touch and interact without being previously known to each other. In this paper, we illustrate Protune, a rule-based TN system. By describing Protune, we will illustrate the advantages that arise from an advanced rule-based approach in terms of deployment efforts, user friendliness, communication efficiency, and interoperability. The generality and technological feasibility of Protune's approach are assessed through an extensive analysis and experimental evaluations.

#index 1520200
#* Data Leakage Detection
#@ Panagiotis Papadimitriou;Hector Garcia-Molina
#t 2011
#c 7
#! We study the following problem: A data distributor has given sensitive data to a set of supposedly trusted agents (third parties). Some of the data are leaked and found in an unauthorized place (e.g., on the web or somebody's laptop). The distributor must assess the likelihood that the leaked data came from one or more agents, as opposed to having been independently gathered by other means. We propose data allocation strategies (across the agents) that improve the probability of identifying leakages. These methods do not rely on alterations of the released data (e.g., watermarks). In some cases, we can also inject “realistic but fake” data records to further improve our chances of detecting leakage and identifying the guilty party.

#index 1520201
#* A Dual Framework and Algorithms for Targeted Online Data Delivery
#@ Haggai Roitman;Avigdor Gal;Louiqa Raschid
#t 2011
#c 7
#! A variety of emerging online data delivery applications challenge existing techniques for data delivery to human users, applications, or middleware that are accessing data from multiple autonomous servers. In this paper, we develop a framework for formalizing and comparing pull-based solutions and present dual optimization approaches. The first approach, most commonly used nowadays, maximizes user utility under the strict setting of meeting a priori constraints on the usage of system resources. We present an alternative and more flexible approach that maximizes user utility by satisfying all users. It does this while minimizing the usage of system resources. We discuss the benefits of this latter approach and develop an adaptive monitoring solution Satisfy User Profiles (SUPs). Through formal analysis, we identify sufficient optimality conditions for SUP. Using real (RSS feeds) and synthetic traces, we empirically analyze the behavior of SUP under varying conditions. Our experiments show that we can achieve a high degree of satisfaction of user utility when the estimations of SUP closely estimate the real event stream, and has the potential to save a significant amount of system resources. We further show that SUP can exploit feedback to improve user utility with only a moderate increase in resource utilization.

#index 1520202
#* Classification Using Streaming Random Forests
#@ Hanady Abdulsalam;David B. Skillicorn;Patrick Martin
#t 2011
#c 7
#! We consider the problem of data stream classification, where the data arrive in a conceptually infinite stream, and the opportunity to examine each record is brief. We introduce a stream classification algorithm that is online, running in amortized {\cal O}(1) time, able to handle intermittent arrival of labeled records, and able to adjust its parameters to respond to changing class boundaries (“concept drift”) in the data stream. In addition, when blocks of labeled data are short, the algorithm is able to judge internally whether the quality of models updated from them is good enough for deployment on unlabeled records, or whether further labeled records are required. Unlike most proposed stream-classification algorithms, multiple target classes can be handled. Experimental results on real and synthetic data show that accuracy is comparable to a conventional classification algorithm that sees all of the data at once and is able to make multiple passes over it.

#index 1520203
#* Coupling Logical Analysis of Data and Shadow Clustering for Partially Defined Positive Boolean Function Reconstruction
#@ Marco Muselli;Enrico Ferrari
#t 2011
#c 7
#! The problem of reconstructing the and-or expression of a partially defined positive Boolean function (pdpBf) is solved by adopting a novel algorithm, denoted by LSC, which combines the advantages of two efficient techniques, Logical Analysis of Data (LAD) and Shadow Clustering (SC). The kernel of the approach followed by LAD consists in a breadth-first enumeration of all the prime implicants whose degree is not greater than a fixed maximum d. In contrast, SC adopts an effective heuristic procedure for retrieving the most promising logical products to be included in the resulting and-or expression. Since the computational cost required by LAD prevents its application even for relatively small dimensions of the input domain, LSC employs a depth-first approach, with asymptotically linear memory occupation, to analyze the prime implicants having degree not greater than d. In addition, the theoretical analysis proves that LSC presents almost the same asymptotic time complexity as LAD. Extensive simulations on artificial benchmarks validate the good behavior of the computational cost exhibited by LSC, in agreement with the theoretical analysis. Furthermore, the pdpBf retrieved by LSC always shows a better performance, in terms of complexity and accuracy, with respect to those obtained by LAD.

#index 1520204
#* Decision Trees for Uncertain Data
#@ Smith Tsang;Ben Kao;Kevin Y. Yip;Wai-Shing Ho;Sau Dan Lee
#t 2011
#c 7
#! Traditional decision tree classifiers work with data whose values are known and precise. We extend such classifiers to handle data with uncertain information. Value uncertainty arises in many applications during the data collection process. Example sources of uncertainty include measurement/quantization errors, data staleness, and multiple repeated measurements. With uncertainty, the value of a data item is often represented not by one single value, but by multiple values forming a probability distribution. Rather than abstracting uncertain data by statistical derivatives (such as mean and median), we discover that the accuracy of a decision tree classifier can be much improved if the "complete information” of a data item (taking into account the probability density function (pdf)) is utilized. We extend classical decision tree building algorithms to handle data tuples with uncertain values. Extensive experiments have been conducted which show that the resulting classifiers are more accurate than those using value averages. Since processing pdfs is computationally more costly than processing single values (e.g., averages), decision tree construction on uncertain data is more CPU demanding than that for certain data. To tackle this problem, we propose a series of pruning techniques that can greatly improve construction efficiency.

#index 1520205
#* Efficient Periodicity Mining in Time Series Databases Using Suffix Trees
#@ Faras Rasheed;Mohammed Alshalalfa;Reda Alhajj
#t 2011
#c 7
#! Periodic pattern mining or periodicity detection has a number of applications, such as prediction, forecasting, detection of unusual activities, etc. The problem is not trivial because the data to be analyzed are mostly noisy and different periodicity types (namely symbol, sequence, and segment) are to be investigated. Accordingly, we argue that there is a need for a comprehensive approach capable of analyzing the whole time series or in a subsection of it to effectively handle different types of noise (to a certain degree) and at the same time is able to detect different types of periodic patterns; combining these under one umbrella is by itself a challenge. In this paper, we present an algorithm which can detect symbol, sequence (partial), and segment (full cycle) periodicity in time series. The algorithm uses suffix tree as the underlying data structure; this allows us to design the algorithm such that its worst-case complexity is O(k . n^2), where k is the maximum length of periodic pattern and n is the length of the analyzed portion (whole or subsection) of the time series. The algorithm is noise resilient; it has been successfully demonstrated to work with replacement, insertion, deletion, or a mixture of these types of noise. We have tested the proposed algorithm on both synthetic and real data from different domains, including protein sequences. The conducted comparative study demonstrate the applicability and effectiveness of the proposed algorithm; it is generally more time-efficient and noise-resilient than existing algorithms.

#index 1520206
#* Exploring Application-Level Semantics for Data Compression
#@ Hsiao-Ping Tsai;De-Nian Yang;Ming-Syan Chen
#t 2011
#c 7
#! Natural phenomena show that many creatures form large social groups and move in regular patterns. However, previous works focus on finding the movement patterns of each single object or all objects. In this paper, we first propose an efficient distributed mining algorithm to jointly identify a group of moving objects and discover their movement patterns in wireless sensor networks. Afterward, we propose a compression algorithm, called 2P2D, which exploits the obtained group movement patterns to reduce the amount of delivered data. The compression algorithm includes a sequence merge and an entropy reduction phases. In the sequence merge phase, we propose a Merge algorithm to merge and compress the location data of a group of moving objects. In the entropy reduction phase, we formulate a Hit Item Replacement (HIR) problem and propose a Replace algorithm that obtains the optimal solution. Moreover, we devise three replacement rules and derive the maximum compression ratio. The experimental results show that the proposed compression algorithm leverages the group movement patterns to reduce the amount of delivered data effectively and efficiently.

#index 1520207
#* Missing Value Estimation for Mixed-Attribute Data Sets
#@ Xiaofeng Zhu;Shichao Zhang;Zhi Jin;Zili Zhang;Zhuoming Xu
#t 2011
#c 7
#! Missing data imputation is a key issue in learning from incomplete data. Various techniques have been developed with great successes on dealing with missing values in data sets with homogeneous attributes (their independent attributes are all either continuous or discrete). This paper studies a new setting of missing data imputation, i.e., imputing missing data in data sets with heterogeneous attributes (their independent attributes are of different types), referred to as imputing mixed-attribute data sets. Although many real applications are in this setting, there is no estimator designed for imputing mixed-attribute data sets. This paper first proposes two consistent estimators for discrete and continuous missing target values, respectively. And then, a mixture-kernel-based iterative estimator is advocated to impute mixed-attribute data sets. The proposed method is evaluated with extensive experiments compared with some typical algorithms, and the result demonstrates that the proposed approach is better than these existing imputation methods in terms of classification accuracy and root mean square error (RMSE) at different missing ratios.

#index 1520208
#* Privacy-Preserving OLAP: An Information-Theoretic Approach
#@ Nan Zhang;Wei Zhao
#t 2011
#c 7
#! We address issues related to the protection of private information in Online Analytical Processing (OLAP) systems, where a major privacy concern is the adversarial inference of private information from OLAP query answers. Most previous work on privacy-preserving OLAP focuses on a single aggregate function and/or addresses only exact disclosure, which eliminates from consideration an important class of privacy breaches where partial information, but not exact values, of private data is disclosed (i.e., partial disclosure). We address privacy protection against both exact and partial disclosure in OLAP systems with mixed aggregate functions. In particular, we propose an information-theoretic inference control approach that supports a combination of common aggregate functions (e.g., COUNT, SUM, MIN, MAX, and MEDIAN) and guarantees the level of privacy disclosure not to exceed thresholds predetermined by the data owners. We demonstrate that our approach is efficient and can be implemented in existing OLAP systems with little modification. It also satisfies the simulatable auditing model and leaks no private information through query rejections. Through performance analysis, we show that compared with previous approaches, our approach provides more effective privacy protection while maintaining a higher level of query-answer availability.

#index 1520209
#* The World in a Nutshell: Concise Range Queries
#@ Ke Yi;Xiang Lian;Feifei Li;Lei Chen
#t 2011
#c 7
#! With the advance of wireless communication technology, it is quite common for people to view maps or get related services from the handheld devices, such as mobile phones and PDAs. Range queries, as one of the most commonly used tools, are often posed by the users to retrieve needful information from a spatial database. However, due to the limits of communication bandwidth and hardware power of handheld devices, displaying all the results of a range query on a handheld device is neither communication-efficient nor informative to the users. This is simply because that there are often too many results returned from a range query. In view of this problem, we present a novel idea that a concise representation of a specified size for the range query results, while incurring minimal information loss, shall be computed and returned to the user. Such a concise range query not only reduces communication costs, but also offers better usability to the users, providing an opportunity for interactive exploration. The usefulness of the concise range queries is confirmed by comparing it with other possible alternatives, such as sampling and clustering. Unfortunately, we prove that finding the optimal representation with minimum information loss is an NP-hard problem. Therefore, we propose several effective and nontrivial algorithms to find a good approximate result. Extensive experiments on real-world data have demonstrated the effectiveness and efficiency of the proposed techniques.

#index 1537092
#* A Fast Multiple Longest Common Subsequence (MLCS) Algorithm
#@ Qingguo Wang;Dmitry Korkin;Yi Shang
#t 2011
#c 7
#! Finding the longest common subsequence (LCS) of multiple strings is an NP-hard problem, with many applications in the areas of bioinformatics and computational genomics. Although significant efforts have been made to address the problem and its special cases, the increasing complexity and size of biological data require more efficient methods applicable to an arbitrary number of strings. In this paper, we present a new algorithm for the general case of multiple LCS (or MLCS) problem, i.e., finding an LCS of any number of strings, and its parallel realization. The algorithm is based on the dominant point approach and employs a fast divide-and-conquer technique to compute the dominant points. When applied to a case of three strings, our algorithm demonstrates the same performance as the fastest existing MLCS algorithm designed for that specific case. When applied to more than three strings, our algorithm is significantly faster than the best existing sequential methods, reaching up to 2-3 orders of magnitude faster speed on large-size problems. Finally, we present an efficient parallel implementation of the algorithm. Evaluating the parallel algorithm on a benchmark set of both random and biological sequences reveals a near-linear speedup with respect to the sequential algorithm.

#index 1537093
#* A Fuzzy Self-Constructing Feature Clustering Algorithm for Text Classification
#@ Jung-Yi Jiang;Ren-Jia Liou;Shie-Jue Lee
#t 2011
#c 7
#! Feature clustering is a powerful method to reduce the dimensionality of feature vectors for text classification. In this paper, we propose a fuzzy similarity-based self-constructing algorithm for feature clustering. The words in the feature vector of a document set are grouped into clusters, based on similarity test. Words that are similar to each other are grouped into the same cluster. Each cluster is characterized by a membership function with statistical mean and deviation. When all the words have been fed in, a desired number of clusters are formed automatically. We then have one extracted feature for each cluster. The extracted feature, corresponding to a cluster, is a weighted combination of the words contained in the cluster. By this algorithm, the derived membership functions match closely with and describe properly the real distribution of the training data. Besides, the user need not specify the number of extracted features in advance, and trial-and-error for determining the appropriate number of extracted features can then be avoided. Experimental results show that our method can run faster and obtain better extracted features than other methods.

#index 1537094
#* A Generic Multilevel Architecture for Time Series Prediction
#@ Dymitr Ruta;Bogdan Gabrys;Christiane Lemke
#t 2011
#c 7
#! Rapidly evolving businesses generate massive amounts of time-stamped data sequences and cause a demand for both univariate and multivariate time series forecasting. For such data, traditional predictive models based on autoregression are often not sufficient to capture complex nonlinear relationships between multidimensional features and the time series outputs. In order to exploit these relationships for improved time series forecasting while also better dealing with a wider variety of prediction scenarios, a forecasting system requires a flexible and generic architecture to accommodate and tune various individual predictors as well as combination methods. In reply to this challenge, an architecture for combined, multilevel time series prediction is proposed, which is suitable for many different universal regressors and combination methods. The key strength of this architecture is its ability to build a diversified ensemble of individual predictors that form an input to a multilevel selection and fusion process before the final optimized output is obtained. Excellent generalization ability is achieved due to the highly boosted complementarity of individual models further enforced through cross-validation-linked training on exclusive data subsets and ensemble output postprocessing. In a sample configuration with basic neural network predictors and a mean combiner, the proposed system has been evaluated in different scenarios and showed a clear prediction performance gain.

#index 1537095
#* Efficient Relevance Feedback for Content-Based Image Retrieval by Mining User Navigation Patterns
#@ Ja-Hwung Su;Wei-Jyun Huang;Philip S. Yu;Vincent S. Tseng
#t 2011
#c 7
#! Nowadays, content-based image retrieval (CBIR) is the mainstay of image retrieval systems. To be more profitable, relevance feedback techniques were incorporated into CBIR such that more precise results can be obtained by taking user's feedbacks into account. However, existing relevance feedback-based CBIR methods usually request a number of iterative feedbacks to produce refined search results, especially in a large-scale image database. This is impractical and inefficient in real applications. In this paper, we propose a novel method, Navigation-Pattern-based Relevance Feedback (NPRF), to achieve the high efficiency and effectiveness of CBIR in coping with the large-scale image data. In terms of efficiency, the iterations of feedback are reduced substantially by using the navigation patterns discovered from the user query log. In terms of effectiveness, our proposed search algorithm NPRFSearch makes use of the discovered navigation patterns and three kinds of query refinement strategies, Query Point Movement (QPM), Query Reweighting (QR), and Query Expansion (QEX), to converge the search space toward the user's intention effectively. By using NPRF method, high quality of image retrieval on RF can be achieved in a small number of feedbacks. The experimental results reveal that NPRF outperforms other existing methods significantly in terms of precision, coverage, and number of feedbacks.

#index 1537096
#* Efficient Techniques for Online Record Linkage
#@ Debabrata Dey;Vijay Mookerjee;Dengpan Liu
#t 2011
#c 7
#! The need to consolidate the information contained in heterogeneous data sources has been widely documented in recent years. In order to accomplish this goal, an organization must resolve several types of heterogeneity problems, especially the entity heterogeneity problem that arises when the same real-world entity type is represented using different identifiers in different data sources. Statistical record linkage techniques could be used for resolving this problem. However, the use of such techniques for online record linkage could pose a tremendous communication bottleneck in a distributed environment (where entity heterogeneity problems are often encountered). In order to resolve this issue, we develop a matching tree, similar to a decision tree, and use it to propose techniques that reduce the communication overhead significantly, while providing matching decisions that are guaranteed to be the same as those obtained using the conventional linkage technique. These techniques have been implemented, and experiments with real-world and synthetic databases show significant reduction in communication overhead.

#index 1537097
#* Experience Transfer for the Configuration Tuning in Large-Scale Computing Systems
#@ Haifeng Chen;Wenxuan Zhang;Guofei Jiang
#t 2011
#c 7
#! This paper proposes a new strategy, the experience transfer, to facilitate the management of large-scale computing systems. It deals with the utilization of management experiences in one system (or previous systems) to benefit the same management task in other systems (or current systems). We use the system configuration tuning as a case application to demonstrate all procedures involved in the experience transfer including the experience representation, experience extraction, and experience embedding. The dependencies between system configuration parameters are treated as transferable experiences in the configuration tuning for two reasons: 1) because such knowledge is helpful to the efficiency of the optimal configuration search, and 2) because the parameter dependencies are typically unchanged between two similar systems. We use the Bayesian network to model configuration dependencies and present a configuration tuning algorithm based on the Bayesian network construction and sampling. As a result, after the configuration tuning is completed in the original system, we can obtain a Bayesian network as the by-product which records the dependencies between system configuration parameters. Such a network is then embedded into the tuning process in other similar systems as transferred experiences to improve the configuration search efficiency. Experimental results in a web-based system show that with the help of transferred experiences, the configuration tuning process can be significantly accelerated.

#index 1537098
#* Extended XML Tree Pattern Matching: Theories and Algorithms
#@ Jiaheng Lu;Tok Wang Ling;Zhifeng Bao;Chen Wang
#t 2011
#c 7
#! As business and enterprises generate and exchange XML data more often, there is an increasing need for efficient processing of queries on XML data. Searching for the occurrences of a tree pattern query in an XML database is a core operation in XML query processing. Prior works demonstrate that holistic twig pattern matching algorithm is an efficient technique to answer an XML tree pattern with parent-child (P-C) and ancestor-descendant (A-D) relationships, as it can effectively control the size of intermediate results during query processing. However, XML query languages (e.g., XPath and XQuery) define more axes and functions such as negation function, order-based axis, and wildcards. In this paper, we research a large set of XML tree pattern, called extended XML tree pattern, which may include P-C, A-D relationships, negation functions, wildcards, and order restriction. We establish a theoretical framework about “matching cross” which demonstrates the intrinsic reason in the proof of optimality on holistic algorithms. Based on our theorems, we propose a set of novel algorithms to efficiently process three categories of extended XML tree patterns. A set of experimental results on both real-life and synthetic data sets demonstrate the effectiveness and efficiency of our proposed theories and algorithms.

#index 1537099
#* Optimizing Resource Conflicts in Workflow Management Systems
#@ Pavlos Delias;Anastasios Doulamis;Nikolaos Doulamis;Nikolaos Matsatsinis
#t 2011
#c 7
#! Resource allocation and scheduling are fundamental issues in a Workflow Management System (WfMS). Effective resource management in WfMS should examine resource allocation together with task scheduling since these problems impose mutual constraints. Optimization of the one factor is subject to the other constraints and vice versa. Thus, an ideal algorithm should take into account not only performance metrics of the infrastructure, such as the number of resources and their utilization, but also quality criteria such as the percentage of tasks undergone violation in their temporal restrictions. In this paper, we propose an innovative algorithm which jointly optimizes the two aforementioned contradictory criteria. The algorithm, called Resource Conflicts Joint Optimization (Re.Co.Jo.Op.), minimizes resource conflicts subject to temporal constraints and simultaneously optimizes throughput or utilization subject to resources constraints. To achieve the optimization, the two factors are formulated in a matrix form and the optimal solution is found by applying concepts of the generalized eigenvalue analysis. A rough outline of an agent-based architecture is proposed to achieve runtime integration of our algorithm into a functional WfMS, while experimental results under different load environments and tasks assumption reveal the superiority of the proposed strategy than the other conventional approaches.

#index 1537100
#* Ranking Spatial Data by Quality Preferences
#@ Man Lung Yiu;Hua Lu;Nikos Mamoulis;Michail Vaitis
#t 2011
#c 7
#! A spatial preference query ranks objects based on the qualities of features in their spatial neighborhood. For example, using a real estate agency database of flats for lease, a customer may want to rank the flats with respect to the appropriateness of their location, defined after aggregating the qualities of other features (e.g., restaurants, cafes, hospital, market, etc.) within their spatial neighborhood. Such a neighborhood concept can be specified by the user via different functions. It can be an explicit circular region within a given distance from the flat. Another intuitive definition is to assign higher weights to the features based on their proximity to the flat. In this paper, we formally define spatial preference queries and propose appropriate indexing techniques and search algorithms for them. Extensive evaluation of our methods on both real and synthetic data reveals that an optimized branch-and-bound solution is efficient and robust with respect to different parameters.

#index 1537101
#* Selecting Attributes for Sentiment Classification Using Feature Relation Networks
#@ Ahmed Abbasi;Stephen France;Zhu Zhang;Hsinchun Chen
#t 2011
#c 7
#! A major concern when incorporating large sets of diverse n-gram features for sentiment classification is the presence of noisy, irrelevant, and redundant attributes. These concerns can often make it difficult to harness the augmented discriminatory potential of extended feature sets. We propose a rule-based multivariate text feature selection method called Feature Relation Network (FRN) that considers semantic information and also leverages the syntactic relationships between n-gram features. FRN is intended to efficiently enable the inclusion of extended sets of heterogeneous n-gram features for enhanced sentiment classification. Experiments were conducted on three online review testbeds in comparison with methods used in prior sentiment classification research. FRN outperformed the comparison univariate, multivariate, and hybrid feature selection methods; it was able to select attributes resulting in significantly better classification accuracy irrespective of the feature subset sizes. Furthermore, by incorporating syntactic information about n-gram relations, FRN is able to select features in a more computationally efficient manner than many multivariate and hybrid techniques.

#index 1537102
#* The CoQUOS Approach to Continuous Queries in Unstructured Overlays
#@ Lakshmish Ramaswamy;Jianxia Chen
#t 2011
#c 7
#! The current peer-to-peer (P2P) content distribution systems are constricted by their simple on-demand content discovery mechanism. The utility of these systems can be greatly enhanced by incorporating two capabilities, namely a mechanism through which peers can register their long term interests with the network so that they can be continuously notified of new data items, and a means for the peers to advertise their contents. Although researchers have proposed a few unstructured overlay-based publish-subscribe systems that provide the above capabilities, most of these systems require intricate indexing and routing schemes, which not only make them highly complex but also render the overlay network less flexible toward transient peers. This paper argues that for many P2P applications, implementing full-fledged publish-subscribe systems is an overkill. For these applications, we study the alternate continuous query paradigm, which is a best-effort service providing the above two capabilities. We present a scalable and effective middleware, called CoQUOS, for supporting continuous queries in unstructured overlay networks. Besides being independent of the overlay topology, CoQUOS preserves the simplicity and flexibility of the unstructured P2P network. Our design of the CoQUOS system is characterized by two novel techniques, namely cluster-resilient random walk algorithm for propagating the queries to various regions of the network and dynamic probability-based query registration scheme to ensure that the registrations are well distributed in the overlay. Further, we also develop effective and efficient schemes for providing resilience to the churn of the P2P network and for ensuring a fair distribution of the notification load among the peers. This paper studies the properties of our algorithms through theoretical analysis. We also report series of experiments evaluating the effectiveness and the costs of the proposed schemes.

#index 1537150
#* Anonymous Publication of Sensitive Transactional Data
#@ Gabriel Ghinita;Panos Kalnis;Yufei Tao
#t 2011
#c 7
#! Existing research on privacy-preserving data publishing focuses on relational data: in this context, the objective is to enforce privacy-preserving paradigms, such as k-anonymity and \ell-diversity, while minimizing the information loss incurred in the anonymizing process (i.e., maximize data utility). Existing techniques work well for fixed-schema data, with low dimensionality. Nevertheless, certain applications require privacy-preserving publishing of transactional data (or basket data), which involve hundreds or even thousands of dimensions, rendering existing methods unusable. We propose two categories of novel anonymization methods for sparse high-dimensional data. The first category is based on approximate nearest-neighbor (NN) search in high-dimensional spaces, which is efficiently performed through locality-sensitive hashing (LSH). In the second category, we propose two data transformations that capture the correlation in the underlying data: 1) reduction to a band matrix and 2) Gray encoding-based sorting. These representations facilitate the formation of anonymized groups with low information loss, through an efficient linear-time heuristic. We show experimentally, using real-life data sets, that all our methods clearly outperform existing state of the art. Among the proposed techniques, NN-search yields superior data utility compared to the band matrix transformation, but incurs higher computational overhead. The data transformation based on Gray code sorting performs best in terms of both data utility and execution time.

#index 1537151
#* CoFiDS: A Belief-Theoretic Approach for Automated Collaborative Filtering
#@ Thanuka L. Wickramarathne;Kamal Premaratne;Miroslav Kubat;Dushyantha Jayaweera
#t 2011
#c 7
#! Automated Collaborative Filtering (ACF) refers to a group of algorithms used in recommender systems, a research topic that has received considerable attention due to its e-commerce applications. However, existing techniques are rarely capable of dealing with imperfections in user-supplied ratings. When such imperfections (e.g., ambiguities) cannot be avoided, designers resort to simplifying assumptions that impair the system's performance and utility. We have developed a novel technique referred to as CoFiDS—Collaborative Filtering based on Dempster-Shafer belief-theoretic framework—that can represent a wide variety of data imperfections, propagate them throughout the decision-making process without the need to make simplifying assumptions, and exploit contextual information. With its DS-theoretic predictions, the domain expert can either obtain a "hard” decision or can narrow the set of possible predictions to a smaller set. With its capability to handle data imperfections, CoFiDS widens the applicability of ACF to such critical and sensitive domains as medical decision support systems and defense-related applications. We describe the theoretical foundation of the system and report experiments with a benchmark movie data set. We explore some essential aspects of CoFiDS' behavior and show that its performance compares favorably with other ACF systems.

#index 1537152
#* Collaborative Filtering with Personalized Skylines
#@ Ilaria Bartolini;Zhenjie Zhang;Dimitris Papadias
#t 2011
#c 7
#! Collaborative filtering (CF) systems exploit previous ratings and similarity in user behavior to recommend the top-k objects/records which are potentially most interesting to the user assuming a single score per object. However, in various applications, a record (e.g., hotel) maybe rated on several attributes (value, service, etc.), in which case simply returning the ones with the highest overall scores fails to capture the individual attribute characteristics and to accommodate different selection criteria. In order to enhance the flexibility of CF, we propose Collaborative Filtering Skyline (CFS), a general framework that combines the advantages of CF with those of the skyline operator. CFS generates a personalized skyline for each user based on scores of other users with similar behavior. The personalized skyline includes objects that are good on certain aspects, and eliminates the ones that are not interesting on any attribute combination. Although the integration of skylines and CF has several attractive properties, it also involves rather expensive computations. We face this challenge through a comprehensive set of algorithms and optimizations that reduce the cost of generating personalized skylines. In addition to exact skyline processing, we develop an approximate method that provides error guarantees. Finally, we propose the top-k personalized skyline, where the user specifies the required output cardinality.

#index 1537153
#* Constrained Skyline Query Processing against Distributed Data Sites
#@ Lijiang Chen;Bin Cui;Hua Lu
#t 2011
#c 7
#! The skyline of a multidimensional point set is a subset of interesting points that are not dominated by others. In this paper, we investigate constrained skyline queries in a large-scale unstructured distributed environment, where relevant data are distributed among geographically scattered sites. We first propose a partition algorithm that divides all data sites into incomparable groups such that the skyline computations in all groups can be parallelized without changing the final result. We then develop a novel algorithm framework called PaDSkyline for parallel skyline query processing among partitioned site groups. We also employ intragroup optimization and multifiltering technique to improve the skyline query processes within each group. In particular, multiple (local) skyline points are sent together with the query as filtering points, which help identify unqualified local skyline points early on a data site. In this way, the amount of data to be transmitted via network connections is reduced, and thus, the overall query response time is shortened further. Cost models and heuristics are proposed to guide the selection of a given number of filtering points from a superset. A cost-efficient model is developed to determine how many filtering points to use for a particular data site. The results of an extensive experimental study demonstrate that our proposals are effective and efficient.

#index 1537154
#* Inconsistency-Tolerant Integrity Checking
#@ Hendrik Decker;Davide Martinenghi
#t 2011
#c 7
#! All methods for efficient integrity checking require all integrity constraints to be totally satisfied, before any update is executed. However, a certain amount of inconsistency is the rule, rather than the exception in databases. In this paper, we close the gap between theory and practice of integrity checking, i.e., between the unrealistic theoretical requirement of total integrity and the practical need for inconsistency tolerance, which we define for integrity checking methods. We show that most of them can still be used to check whether updates preserve integrity, even if the current state is inconsistent. Inconsistency-tolerant integrity checking proves beneficial both for integrity preservation and query answering. Also, we show that it is useful for view updating, repairs, schema evolution, and other applications.

#index 1537155
#* Initialization and Restart in Stochastic Local Search: Computing a Most Probable Explanation in Bayesian Networks
#@ Ole J. Mengshoel;David C. Wilkins;Dan Roth
#t 2011
#c 7
#! For hard computational problems, stochastic local search has proven to be a competitive approach to finding optimal or approximately optimal problem solutions. Two key research questions for stochastic local search algorithms are: Which algorithms are effective for initialization? When should the search process be restarted? In the present work, we investigate these research questions in the context of approximate computation of most probable explanations (MPEs) in Bayesian networks (BNs). We introduce a novel approach, based on the Viterbi algorithm, to explanation initialization in BNs. While the Viterbi algorithm works on sequences and trees, our approach works on BNs with arbitrary topologies. We also give a novel formalization of stochastic local search, with focus on initialization and restart, using probability theory and mixture models. Experimentally, we apply our methods to the problem of MPE computation, using a stochastic local search algorithm known as Stochastic Greedy Search. By carefully optimizing both initialization and restart, we reduce the MPE search time for application BNs by several orders of magnitude compared to using uniform at random initialization without restart. On several BNs from applications, the performance of Stochastic Greedy Search is competitive with clique tree clustering, a state-of-the-art exact algorithm used for MPE computation in BNs.

#index 1537156
#* Load Shedding in Mobile Systems with MobiQual
#@ Bugra Gedik;Kun-Lung Wu;Ling Liu;Philip S. Yu
#t 2011
#c 7
#! In location-based, mobile continual query (CQ) systems, two key measures of quality-of-service (QoS) are: freshness and accuracy. To achieve freshness, the CQ server must perform frequent query reevaluations. To attain accuracy, the CQ server must receive and process frequent position updates from the mobile nodes. However, it is often difficult to obtain fresh and accurate CQ results simultaneously, due to 1) limited resources in computing and communication and 2) fast-changing load conditions caused by continuous mobile node movement. Hence, a key challenge for a mobile CQ system is: How do we achieve the highest possible quality of the CQ results, in both freshness and accuracy, with currently available resources? In this paper, we formulate this problem as a load shedding one, and develop MobiQual—a QoS-aware approach to performing both update load shedding and query load shedding. The design of MobiQual highlights three important features. 1) Differentiated load shedding: We apply different amounts of query load shedding and update load shedding to different groups of queries and mobile nodes, respectively. 2) Per-query QoS specification: Individualized QoS specifications are used to maximize the overall freshness and accuracy of the query results. 3) Low-cost adaptation: MobiQual dynamically adapts, with a minimal overhead, to changing load conditions and available resources. We conduct a set of comprehensive experiments to evaluate the effectiveness of MobiQual. The results show that, through a careful combination of update and query load shedding, the MobiQual approach leads to much higher freshness and accuracy in the query results in all cases, compared to existing approaches that lack the QoS-awareness properties of MobiQual, as well as the solutions that perform query-only or update-only load shedding.

#index 1537157
#* Mining Group Movement Patterns for Tracking Moving Objects Efficiently
#@ Hsiao-Ping Tsai;De-Nian Yang;Ming-Syan Chen
#t 2011
#c 7
#! Existing object tracking applications focus on finding the moving patterns of a single object or all objects. In contrast, we propose a distributed mining algorithm that identifies a group of objects with similar movement patterns. This information is important in some biological research domains, such as the study of animals' social behavior and wildlife migration. The proposed algorithm comprises a local mining phase and a cluster ensembling phase. In the local mining phase, the algorithm finds movement patterns based on local trajectories. Then, based on the derived patterns, we propose a new similarity measure to compute the similarity of moving objects and identify the local group relationships. To address the energy conservation issue in resource-constrained environments, the algorithm only transmits the local grouping results to the sink node for further ensembling. In the cluster ensembling phase, our algorithm combines the local grouping results to derive the group relationships from a global view. We further leverage the mining results to track moving objects efficiently. The results of experiments show that the proposed mining algorithm achieves good grouping quality, and the mining technique helps reduce the energy consumption by reducing the amount of data to be transmitted.

#index 1537158
#* Mining Iterative Generators and Representative Rules for Software Specification Discovery
#@ David Lo;Jinyan Li;limsoon wong;Siau-Cheng Khoo
#t 2011
#c 7
#! Billions of dollars are spent annually on software-related cost. It is estimated that up to 45 percent of software cost is due to the difficulty in understanding existing systems when performing maintenance tasks (i.e., adding features, removing bugs, etc.). One of the root causes is that software products often come with poor, incomplete, or even without any documented specifications. In an effort to improve program understanding, Lo et al. have proposed iterative pattern mining which outputs patterns that are repeated frequently within a program trace, or across multiple traces, or both. Frequent iterative patterns reflect frequent program behaviors that likely correspond to software specifications. To reduce the number of patterns and improve the efficiency of the algorithm, Lo et al. have also introduced mining closed iterative patterns, i.e., maximal patterns without any superpattern having the same support. In this paper, to technically deepen research on iterative pattern mining, we introduce mining iterative generators, i.e., minimal patterns without any subpattern having the same support. Iterative generators can be paired with closed patterns to produce a set of rules expressing forward, backward, and in-between temporal constraints among events in one general representation. We refer to these rules as representative rules. A comprehensive performance study shows the efficiency of our approach. A case study on traces of an industrial system shows how iterative generators and closed iterative patterns can be merged to form useful rules shedding light on software design.

#index 1537159
#* Straggler Identification in Round-Trip Data Streams via Newton's Identities and Invertible Bloom Filters
#@ David Eppstein;Michael T. Goodrich
#t 2011
#c 7
#! In this paper, we study the straggler identification problem, in which an algorithm must determine the identities of the remaining members of a set after it has had a large number of insertion and deletion operations performed on it, and now has relatively few remaining members. The goal is to do this in o(n) space, where n is the total number of identities. Straggler identification has applications, for example, in determining the unacknowledged packets in a high-bandwidth multicast data stream. We provide a deterministic solution to the straggler identification problem that uses only O(d\log n) bits, based on a novel application of Newton's identities for symmetric polynomials. This solution can identify any subset of d stragglers from a set of n O(\log n)-bit identifiers, assuming that there are no false deletions of identities not already in the set. Indeed, we give a lower bound argument that shows that any small-space deterministic solution to the straggler identification problem cannot be guaranteed to handle false deletions. Nevertheless, we provide a simple randomized solution, using O(d\log n\log (1/\epsilon )) bits that can maintain a multiset and solve the straggler identification problem, tolerating false deletions, where \epsilon 0 is a user-defined parameter bounding the probability of an incorrect response. This randomized solution is based on a new type of Bloom filter, which we call the invertible Bloom filter.

#index 1537160
#* Temporal Data Clustering via Weighted Clustering Ensemble with Different Representations
#@ Yun Yang;Ke Chen
#t 2011
#c 7
#! Temporal data clustering provides underpinning techniques for discovering the intrinsic structure and condensing information over temporal data. In this paper, we present a temporal data clustering framework via a weighted clustering ensemble of multiple partitions produced by initial clustering analysis on different temporal data representations. In our approach, we propose a novel weighted consensus function guided by clustering validation criteria to reconcile initial partitions to candidate consensus partitions from different perspectives, and then, introduce an agreement function to further reconcile those candidate consensus partitions to a final partition. As a result, the proposed weighted clustering ensemble algorithm provides an effective enabling technique for the joint use of different representations, which cuts the information loss in a single representation and exploits various information sources underlying temporal data. In addition, our approach tends to capture the intrinsic structure of a data set, e.g., the number of clusters. Our approach has been evaluated with benchmark time series, motion trajectory, and time-series data stream clustering tasks. Simulation results demonstrate that our approach yields favorite results for a variety of temporal data clustering tasks. As our weighted cluster ensemble algorithm can combine any input partitions to generate a clustering ensemble, we also investigate its limitation by formal analysis and empirical studies.

#index 1555376
#* A Link Analysis Extension of Correspondence Analysis for Mining Relational Databases
#@ Luh Yen;Marco Saerens;Francois Fouss
#t 2011
#c 7
#! This work introduces a link analysis procedure for discovering relationships in a relational database or a graph, generalizing both simple and multiple correspondence analysis. It is based on a random walk model through the database defining a Markov chain having as many states as elements in the database. Suppose we are interested in analyzing the relationships between some elements (or records) contained in two different tables of the relational database. To this end, in a first step, a reduced, much smaller, Markov chain containing only the elements of interest and preserving the main characteristics of the initial chain, is extracted by stochastic complementation [42]. This reduced chain is then analyzed by projecting jointly the elements of interest in the diffusion map subspace [41] and visualizing the results. This two-step procedure reduces to simple correspondence analysis when only two tables are defined, and to multiple correspondence analysis when the database takes the form of a simple star-schema. On the other hand, a kernel version of the diffusion map distance, generalizing the basic diffusion map distance to directed graphs, is also introduced and the links with spectral clustering are discussed. Several data sets are analyzed by using the proposed methodology, showing the usefulness of the technique for extracting relationships in relational databases or graphs.

#index 1555377
#* A Personalized Ontology Model for Web Information Gathering
#@ Xiaohui Tao;Yuefeng Li;Ning Zhong
#t 2011
#c 7
#! As a model for knowledge description and formalization, ontologies are widely used to represent user profiles in personalized web information gathering. However, when representing user profiles, many models have utilized only knowledge from either a global knowledge base or a user local information. In this paper, a personalized ontology model is proposed for knowledge representation and reasoning over user profiles. This model learns ontological user profiles from both a world knowledge base and user local instance repositories. The ontology model is evaluated by comparing it against benchmark models in web information gathering. The results show that this ontology model is successful.

#index 1555378
#* Answering Frequent Probabilistic Inference Queries in Databases
#@ Shaoxu Song;Lei Chen;Jeffrey Xu Yu
#t 2011
#c 7
#! Existing solutions for probabilistic inference queries mainly focus on answering a single inference query, but seldom address the issues of efficiently returning results for a sequence of frequent queries, which is more popular and practical in many real applications. In this paper, we mainly study the computation caching and sharing among a sequence of inference queries in databases. The clique tree propagation (ctp) algorithm is first introduced in databases for probabilistic inference queries. We use the materialized views to cache the intermediate results of the previous inference queries, which might be shared with the following queries, and consequently reduce the time cost. Moreover, we take the query workload into account to identify the frequently queried variables. To optimize probabilistic inference queries with ctp, we cache these frequent query variables into the materialized views to maximize the reuse. Due to the existence of different query plans, we present heuristics to estimate costs and select the optimal query plan. Finally, we present the experimental evaluation in relational databases to illustrate the validity and superiority of our approaches in answering frequent probabilistic inference queries.

#index 1555379
#* Discovering Activities to Recognize and Track in a Smart Environment
#@ Parisa Rashidi;Diane J. Cook;Lawrence B. Holder;Maureen Schmitter-Edgecombe
#t 2011
#c 7
#! The machine learning and pervasive sensing technologies found in smart homes offer unprecedented opportunities for providing health monitoring and assistance to individuals experiencing difficulties living independently at home. In order to monitor the functional health of smart home residents, we need to design technologies that recognize and track activities that people normally perform as part of their daily routines. Although approaches do exist for recognizing activities, the approaches are applied to activities that have been preselected and for which labeled training data are available. In contrast, we introduce an automated approach to activity tracking that identifies frequent activities that naturally occur in an individual's routine. With this capability, we can then track the occurrence of regular activities to monitor functional health and to detect changes in an individual's patterns and lifestyle. In this paper, we describe our activity mining and tracking approach, and validate our algorithms on data collected in physical smart environments.

#index 1555380
#* Effective Navigation of Query Results Based on Concept Hierarchies
#@ Abhijith Kashyap;Vagelis Hristidis;Michalis Petropoulos;Sotiria Tavoulari
#t 2011
#c 7
#! Search queries on biomedical databases, such as PubMed, often return a large number of results, only a small subset of which is relevant to the user. Ranking and categorization, which can also be combined, have been proposed to alleviate this information overload problem. Results categorization for biomedical databases is the focus of this work. A natural way to organize biomedical citations is according to their MeSH annotations. MeSH is a comprehensive concept hierarchy used by PubMed. In this paper, we present the BioNav system, a novel search interface that enables the user to navigate large number of query results by organizing them using the MeSH concept hierarchy. First, the query results are organized into a navigation tree. At each node expansion step, BioNav reveals only a small subset of the concept nodes, selected such that the expected user navigation cost is minimized. In contrast, previous works expand the hierarchy in a predefined static manner, without navigation cost modeling. We show that the problem of selecting the best concepts to reveal at each node expansion is NP-complete and propose an efficient heuristic as well as a feasible optimal algorithm for relatively small trees. We show experimentally that BioNav outperforms state-of-the-art categorization systems by up to an order of magnitude, with respect to the user navigation cost. BioNav for the MEDLINE database is available at http://db.cse.buffalo.edu/bionav.

#index 1555381
#* Estimating and Enhancing Real-Time Data Service Delays: Control-Theoretic Approaches
#@ Kyoung-Don Kang;Yan Zhou;Jisu Oh
#t 2011
#c 7
#! It is essential to process real-time data service requests such as stock quotes and trade transactions in a timely manner using fresh data, which represent the current real-world phenomena such as the stock market status. Users may simply leave when the database service delay is excessive. Also, temporally inconsistent data may give an outdated view of the real-world status. However, supporting the desired timeliness and freshness is challenging due to dynamic workloads. To address the problem, we present new approaches for 1) database backlog estimation, 2) fine-grained closed-loop admission control based on the backlog model, and 3) incoming load smoothing. Our backlog estimation and control-theoretic approaches aim to support the desired service delay bound without degrading the data freshness, critical for real-time data services. Specifically, we design, implement, and evaluate two feedback controllers based on linear control theory and fuzzy logic control theory, to meet the desired service delay. Workload smoothing, under overload, helps the database admit and process more transactions in a timely fashion by probabilistically reducing the burstiness of incoming data service requests. In terms of the data service delay and throughput, our closed-loop admission control and probabilistic load smoothing schemes considerably outperform several baselines in the experiments undertaken in a stock trading database testbed.

#index 1555382
#* Finding Correlated Biclusters from Gene Expression Data
#@ Wen-Hui Yang;Dao-Qing Dai;Hong Yan
#t 2011
#c 7
#! Extracting biologically relevant information from DNA microarrays is a very important task for drug development and test, function annotation, and cancer diagnosis. Various clustering methods have been proposed for the analysis of gene expression data, but when analyzing the large and heterogeneous collections of gene expression data, conventional clustering algorithms often cannot produce a satisfactory solution. Biclustering algorithm has been presented as an alternative approach to standard clustering techniques to identify local structures from gene expression data set. These patterns may provide clues about the main biological processes associated with different physiological states. In this paper, different from existing bicluster patterns, we first introduce a more general pattern: correlated bicluster, which has intuitive biological interpretation. Then, we propose a novel transform technique based on singular value decomposition so that identifying correlated-bicluster problem from gene expression matrix is transformed into two global clustering problems. The Mixed-Clustering algorithm and the Lift algorithm are devised to efficiently produce \delta-corBiclusters. The biclusters obtained using our method from gene expression data sets of multiple human organs and the yeast Saccharomyces cerevisiae demonstrate clear biological meanings.

#index 1555383
#* IR-Tree: An Efficient Index for Geographic Document Search
#@ Zhisheng Li;Ken C. K. Lee;Baihua Zheng;Wang-Chien Lee;Dik Lee;Xufa Wang
#t 2011
#c 7
#! Given a geographic query that is composed of query keywords and a location, a geographic search engine retrieves documents that are the most textually and spatially relevant to the query keywords and the location, respectively, and ranks the retrieved documents according to their joint textual and spatial relevances to the query. The lack of an efficient index that can simultaneously handle both the textual and spatial aspects of the documents makes existing geographic search engines inefficient in answering geographic queries. In this paper, we propose an efficient index, called IR-tree, that together with a top-k document search algorithm facilitates four major tasks in document searches, namely, 1) spatial filtering, 2) textual filtering, 3) relevance computation, and 4) document ranking in a fully integrated manner. In addition, IR-tree allows searches to adopt different weights on textual and spatial relevance of documents at the runtime and thus caters for a wide variety of applications. A set of comprehensive experiments over a wide range of scenarios has been conducted and the experiment results demonstrate that IR-tree outperforms the state-of-the-art approaches for geographic document searches.

#index 1555384
#* Learning Semi-Riemannian Metrics for Semisupervised Feature Extraction
#@ Wei Zhang;Zhouchen Lin;Xiaoou Tang
#t 2011
#c 7
#! Discriminant feature extraction plays a central role in pattern recognition and classification. Linear Discriminant Analysis (LDA) is a traditional algorithm for supervised feature extraction. Recently, unlabeled data have been utilized to improve LDA. However, the intrinsic problems of LDA still exist and only the similarity among the unlabeled data is utilized. In this paper, we propose a novel algorithm, called Semisupervised Semi-Riemannian Metric Map ({\rm S}^3RMM), following the geometric framework of semi-Riemannian manifolds. {\rm S}^3RMM maximizes the discrepancy of the separability and similarity measures of scatters formulated by using semi-Riemannian metric tensors. The metric tensor of each sample is learned via semisupervised regression. Our method can also be a general framework for proposing new semisupervised algorithms, utilizing the existing discrepancy-criterion-based algorithms. The experiments demonstrated on faces and handwritten digits show that {\rm S}^3RMM is promising for semisupervised feature extraction.

#index 1555385
#* TEXT: Automatic Template Extraction from Heterogeneous Web Pages
#@ Chulyun Kim;Kyuseok Shim
#t 2011
#c 7
#! World Wide Web is the most useful source of information. In order to achieve high productivity of publishing, the webpages in many websites are automatically populated by using the common templates with contents. The templates provide readers easy access to the contents guided by consistent structures. However, for machines, the templates are considered harmful since they degrade the accuracy and performance of web applications due to irrelevant terms in templates. Thus, template detection techniques have received a lot of attention recently to improve the performance of search engines, clustering, and classification of web documents. In this paper, we present novel algorithms for extracting templates from a large number of web documents which are generated from heterogeneous templates. We cluster the web documents based on the similarity of underlying template structures in the documents so that the template for each cluster is extracted simultaneously. We develop a novel goodness measure with its fast approximation for clustering and provide comprehensive analysis of our algorithm. Our experimental results with real-life data sets confirm the effectiveness and robustness of our algorithm compared to the state of the art for template detection algorithms.

#index 1555386
#* Text Clustering with Seeds Affinity Propagation
#@ Renchu Guan;Xiaohu Shi;Maurizio Marchese;Chen Yang;Yanchun Liang
#t 2011
#c 7
#! Based on an effective clustering algorithm—Affinity Propagation (AP)—we present in this paper a novel semisupervised text clustering algorithm, called Seeds Affinity Propagation (SAP). There are two main contributions in our approach: 1) a new similarity metric that captures the structural information of texts, and 2) a novel seed construction method to improve the semisupervised clustering process. To study the performance of the new algorithm, we applied it to the benchmark data set Reuters-21578 and compared it to two state-of-the-art clustering algorithms, namely, k-means algorithm and the original AP algorithm. Furthermore, we have analyzed the individual impact of the two proposed contributions. Results show that the proposed similarity metric is more effective in text clustering (F-measures ca. 21 percent higher than in the AP algorithm) and the proposed semisupervised strategy achieves both better clustering results and faster convergence (using only 76 percent iterations of the original AP). The complete SAP algorithm obtains higher F-measure (ca. 40 percent improvement over k-means and AP) and lower entropy (ca. 28 percent decrease over k-means and AP), improves significantly clustering execution time (20 times faster) in respect that k-means, and provides enhanced robustness compared with all other methods.

#index 1555387
#* Some Remarks on the Paper "semQA: SPARQL with Idempotent Disjunction”
#@ Marcelo Arenas;Claudio Gutierrez;Jorge PErez
#t 2011
#c 7
#! In the paper, “semQA: SPARQL with Idempotent Disjunction” [4], the authors study the RDF query language SPARQL. In particular, they claim that some of the results presented in [1] are not correct. In this note, we refute the claims made in [4], and actually show that some of the formal results of [4] are incorrect.

#index 1573135
#* Authenticated Multistep Nearest Neighbor Search
#@ Stavros Papadopoulos;Lixing Wang;Yin Yang;Dimitris Papadias;Panagiotis Karras
#t 2011
#c 7
#! Multistep processing is commonly used for nearest neighbor (NN) and similarity search in applications involving high-dimensional data and/or costly distance computations. Today, many such applications require a proof of result correctness. In this setting, clients issue NN queries to a server that maintains a database signed by a trusted authority. The server returns the NN set along with supplementary information that permits result verification using the data set signature. An adaptation of the multistep NN algorithm incurs prohibitive network overhead due to the transmission of false hits, i.e., records that are not in the NN set, but are nevertheless necessary for its verification. In order to alleviate this problem, we present a novel technique that reduces the size of each false hit. Moreover, we generalize our solution for a distributed setting, where the database is horizontally partitioned over several servers. Finally, we demonstrate the effectiveness of the proposed solutions with real data sets of various dimensionalities.

#index 1573136
#* Branch-and-Bound for Model Selection and Its Computational Complexity
#@ Ninad Thakoor;Jean Gao
#t 2011
#c 7
#! Branch-and-bound methods are used in various data analysis problems, such as clustering, seriation and feature selection. Classical approaches of branch-and-bound based clustering search through combinations of various partitioning possibilities to optimize a clustering cost. However, these approaches are not practically useful for clustering of image data where the size of data is large. Additionally, the number of clusters is unknown in most of the image data analysis problems. By taking advantage of the spatial coherency of clusters, we formulate an innovative branch-and-bound approach, which solves clustering problem as a model-selection problem. In this generalized approach, cluster parameter candidates are first generated by spatially coherent sampling. A branch-and-bound search is carried out through the candidates to select an optimal subset. This paper formulates this approach and investigates its average computational complexity. Improved clustering quality and robustness to outliers compared to conventional iterative approach are demonstrated with experiments.

#index 1573137
#* Intertemporal Discount Factors as a Measure of Trustworthiness in Electronic Commerce
#@ Christopher J. Hazard;Munindar P. Singh
#t 2011
#c 7
#! In multiagent interactions, such as e-commerce and file sharing, being able to accurately assess the trustworthiness of others is important for agents to protect themselves from losing utility. Focusing on rational agents in e-commerce, we prove that an agent's discount factor (time preference of utility) is a direct measure of the agent's trustworthiness for a set of reasonably general assumptions and definitions. We propose a general list of desiderata for trust systems and discuss how discount factors as trustworthiness meet these desiderata. We discuss how discount factors are a robust measure when entering commitments that exhibit moral hazards. Using an online market as a motivating example, we derive some analytical methods both for measuring discount factors and for aggregating the measurements.

#index 1573138
#* Cosdes: A Collaborative Spam Detection System with a Novel E-Mail Abstraction Scheme
#@ Chi-Yao Tseng;Pin-Chieh Sung;Ming-Syan Chen
#t 2011
#c 7
#! E-mail communication is indispensable nowadays, but the e-mail spam problem continues growing drastically. In recent years, the notion of collaborative spam filtering with near-duplicate similarity matching scheme has been widely discussed. The primary idea of the similarity matching scheme for spam detection is to maintain a known spam database, formed by user feedback, to block subsequent near-duplicate spams. On purpose of achieving efficient similarity matching and reducing storage utilization, prior works mainly represent each e-mail by a succinct abstraction derived from e-mail content text. However, these abstractions of e-mails cannot fully catch the evolving nature of spams, and are thus not effective enough in near-duplicate detection. In this paper, we propose a novel e-mail abstraction scheme, which considers e-mail layout structure to represent e-mails. We present a procedure to generate the e-mail abstraction using HTML content in e-mail, and this newly devised abstraction can more effectively capture the near-duplicate phenomenon of spams. Moreover, we design a complete spam detection system Cosdes (standing for COllaborative Spam DEtection System), which possesses an efficient near-duplicate matching scheme and a progressive update scheme. The progressive update scheme enables system Cosdes to keep the most up-to-date information for near-duplicate detection. We evaluate Cosdes on a live data set collected from a real e-mail server and show that our system outperforms the prior approaches in detection results and is applicable to the real world.

#index 1573139
#* Discovering Conditional Functional Dependencies
#@ Wenfei Fan;Floris Geerts;Jianzhong Li;Ming Xiong
#t 2011
#c 7
#! This paper investigates the discovery of conditional functional dependencies (CFDs). CFDs are a recent extension of functional dependencies (FDs) by supporting patterns of semantically related constants, and can be used as rules for cleaning relational data. However, finding quality CFDs is an expensive process that involves intensive manual effort. To effectively identify data cleaning rules, we develop techniques for discovering CFDs from relations. Already hard for traditional FDs, the discovery problem is more difficult for CFDs. Indeed, mining patterns in CFDs introduces new challenges. We provide three methods for CFD discovery. The first, referred to as CFDMiner, is based on techniques for mining closed item sets, and is used to discover constant CFDs, namely, CFDs with constant patterns only. Constant CFDs are particularly important for object identification, which is essential to data cleaning and data integration. The other two algorithms are developed for discovering general CFDs. One algorithm, referred to as CTANE, is a levelwise algorithm that extends TANE, a well-known algorithm for mining FDs. The other, referred to as FastCFD, is based on the depth-first approach used in FastFD, a method for discovering FDs. It leverages closed-item-set mining to reduce the search space. As verified by our experimental study, CFDMiner can be multiple orders of magnitude faster than CTANE and FastCFD for constant CFD discovery. CTANE works well when a given relation is large, but it does not scale well with the arity of the relation. FastCFD is far more efficient than CTANE when the arity of the relation is large; better still, leveraging optimization based on closed-item-set mining, FastCFD also scales well with the size of the relation. These algorithms provide a set of cleaning-rule discovery tools for users to choose for different applications.

#index 1573140
#* Mining Discriminative Patterns for Classifying Trajectories on Road Networks
#@ Jae-Gil Lee;Jiawei Han;Xiaolei Li;Hong Cheng
#t 2011
#c 7
#! Classification has been used for modeling many kinds of data sets, including sets of items, text documents, graphs, and networks. However, there is a lack of study on a new kind of data, trajectories on road networks. Modeling such data is useful with the emerging GPS and RFID technologies and is important for effective transportation and traffic planning. In this work, we study methods for classifying trajectories on road networks. By analyzing the behavior of trajectories on road networks, we observe that, in addition to the locations where vehicles have visited, the order of these visited locations is crucial for improving classification accuracy. Based on our analysis, we contend that (frequent) sequential patterns are good feature candidates since they preserve this order information. Furthermore, when mining sequential patterns, we propose to confine the length of sequential patterns to ensure high efficiency. Compared with closed sequential patterns, these partial (i.e., length-confined) sequential patterns allow us to significantly improve efficiency almost without losing accuracy. In this paper, we present a framework for frequent pattern-based classification for trajectories on road networks. Our comparative study over a broad range of classification approaches demonstrates that our method significantly improves accuracy over other methods in some synthetic and real trajectory data.

#index 1573141
#* Pareto-Based Dominant Graph: An Efficient Indexing Structure to Answer Top-K Queries
#@ Lei Zou;Lei Chen
#t 2011
#c 7
#! Given a record set D and a query score function F, a top-k query returns k records from D, whose values of function F on their attributes are the highest. In this paper, we investigate the intrinsic connection between top-k queries and dominant relationships between records, and based on which, we propose an efficient layer-based indexing structure, Pareto-Based Dominant Graph (DG), to improve the query efficiency. Specifically, DG is built offline to express the dominant relationship between records and top-k query is implemented as a graph traversal problem, i.e., Traveler algorithm. We prove theoretically that the size of search space (that is the number of retrieved records from the record set to answer top-k query) in our algorithm is directly related to the cardinality of skyline points in the record set (see Theorem 3). Considering I/O cost, we propose cluster-based storage schema to reduce I/O cost in Traveler algorithm. We also propose the cost estimation methods in this paper. Based on cost analysis, we propose an optimization technique, pseudorecord, to further improve the search efficiency. In order to handle the top-k query in the high-dimension record set, we also propose N-Way Traveler algorithm. In order to handle DG maintenance efficiently, we propose “Insertion” and “Deletion” algorithms for DG. Finally, extensive experiments demonstrate that our proposed methods have significant improvement over its counterparts, including both classical and state art of top-k algorithms.

#index 1573142
#* RFID Data Processing in Supply Chain Management Using a Path Encoding Scheme
#@ Chun-Hee Lee;Chin-Wan Chung
#t 2011
#c 7
#! RFID technology can be applied to a broad range of areas. In particular, RFID is very useful in the area of business, such as supply chain management. However, the amount of RFID data in such an environment is huge. Therefore, much time is needed to extract valuable information from RFID data for supply chain management. In this paper, we present an efficient method to process a massive amount of RFID data for supply chain management. We first define query templates to analyze the supply chain. We then propose an effective path encoding scheme that encodes the flows of products. However, if the flows are long, the numbers in the path encoding scheme that correspond to the flows will be very large. We solve this by providing a method that divides flows. To retrieve the time information for products efficiently, we utilize a numbering scheme for the XML area. Based on the path encoding scheme and the numbering scheme, we devise a storage scheme that can process tracking queries and path oriented queries efficiently on an RDBMS. Finally, we propose a method that translates the queries to SQL queries. Experimental results show that our approach can process the queries efficiently.

#index 1573143
#* Semantic Knowledge-Based Framework to Improve the Situation Awareness of Autonomous Underwater Vehicles
#@ Emilio Miguelanez;Pedro Patron;Keith E. Brown;Yvan R. Petillot;David M. Lane
#t 2011
#c 7
#! This paper proposes a semantic world model framework for hierarchical distributed representation of knowledge in autonomous underwater systems. This framework aims to provide a more capable and holistic system, involving semantic interoperability among all involved information sources. This will enhance interoperability, independence of operation, and situation awareness of the embedded service-oriented agents for autonomous platforms. The results obtained specifically affect the mission flexibility, robustness, and autonomy. The presented framework makes use of the idea that heterogeneous real-world data of very different type must be processed by (and run through) several different layers, to be finally available in a suited format and at the right place to be accessible by high-level decision-making agents. In this sense, the presented approach shows how to abstract away from the raw real-world data step by step by means of semantic technologies. The paper concludes by demonstrating the benefits of the framework in a real scenario. A hardware fault is simulated in a REMUS 100 AUV while performing a mission. This triggers a knowledge exchange between the status monitoring agent and the adaptive mission planner embedded agent. By using the proposed framework, both services can interchange information while remaining domain independent during their interaction with the platform. The results of this paper are readily applicable to land and air robotics.

#index 1573144
#* SwiftRule: Mining Comprehensible Classification Rules for Time Series Analysis
#@ Dominik Fisch;Thiemo Gruber;Bernhard Sick
#t 2011
#c 7
#! In this article, we provide a new technique for temporal data mining which is based on classification rules that can easily be understood by human domain experts. Basically, time series are decomposed into short segments, and short-term trends of the time series within the segments (e.g., average, slope, and curvature) are described by means of polynomial models. Then, the classifiers assess short sequences of trends in subsequent segments with their rule premises. The conclusions gradually assign an input to a class. As the classifier is a generative model of the processes from which the time series are assumed to originate, anomalies can be detected, too. Segmentation and piecewise polynomial modeling are done extremely fast in only one pass over the time series. Thus, the approach is applicable to problems with harsh timing constraints. We lay the theoretical foundations for this classifier, including a new distance measure for time series and a new technique to construct a dynamic classifier from a static one, and demonstrate its properties by means of various benchmark time series, for example, Lorenz attractor time series, energy consumption in a building, or ECG data.

#index 1573145
#* When Does Cotraining Work in Real Data?
#@ Jun Du;Charles X. Ling;Zhi-Hua Zhou
#t 2011
#c 7
#! Cotraining, a paradigm of semisupervised learning, is promised to alleviate effectively the shortage of labeled examples in supervised learning. The standard two-view cotraining requires the data set to be described by two views of features, and previous studies have shown that cotraining works well if the two views satisfy the sufficiency and independence assumptions. In practice, however, these two assumptions are often not known or ensured (even when the two views are given). More commonly, most supervised data sets are described by one set of attributes (one view). Thus, they need be split into two views in order to apply the standard two-view cotraining. In this paper, we first propose a novel approach to empirically verify the two assumptions of cotraining given two views. Then, we design several methods to split single view data sets into two views, in order to make cotraining work reliably well. Our empirical results show that, given a whole or a large labeled training set, our view verification and splitting methods are quite effective. Unfortunately, cotraining is called for precisely when the labeled training set is small. However, given small labeled training sets, we show that the two cotraining assumptions are difficult to verify, and view splitting is unreliable. Our conclusions for cotraining's effectiveness are mixed. If two views are given, and known to satisfy the two assumptions, cotraining works well. Otherwise, based on small labeled training sets, verifying the assumptions or splitting single view into two views are unreliable; thus, it is uncertain whether the standard cotraining would work or not.

#index 1573146
#* A Machine Learning Approach for Identifying Disease-Treatment Relations in Short Texts
#@ Oana Frunza;Diana Inkpen;Thomas Tran
#t 2011
#c 7
#! The Machine Learning (ML) field has gained its momentum in almost any domain of research and just recently has become a reliable tool in the medical domain. The empirical domain of automatic learning is used in tasks such as medical decision support, medical imaging, protein-protein interaction, extraction of medical knowledge, and for overall patient management care. ML is envisioned as a tool by which computer-based systems can be integrated in the healthcare field in order to get a better, more efficient medical care. This paper describes a ML-based methodology for building an application that is capable of identifying and disseminating healthcare information. It extracts sentences from published medical papers that mention diseases and treatments, and identifies semantic relations that exist between diseases and treatments. Our evaluation results for these tasks show that the proposed methodology obtains reliable outcomes that could be integrated in an application to be used in the medical care domain. The potential value of this paper stands in the ML settings that we propose and in the fact that we outperform previous results on the same data set.

#index 1573147
#* Adaptive Cluster Distance Bounding for High-Dimensional Indexing
#@ Sharadh Ramaswamy;Kenneth Rose
#t 2011
#c 7
#! We consider approaches for similarity search in correlated, high-dimensional data sets, which are derived within a clustering framework. We note that indexing by “vector approximation” (VA-File), which was proposed as a technique to combat the “Curse of Dimensionality,” employs scalar quantization, and hence necessarily ignores dependencies across dimensions, which represents a source of suboptimality. Clustering, on the other hand, exploits interdimensional correlations and is thus a more compact representation of the data set. However, existing methods to prune irrelevant clusters are based on bounding hyperspheres and/or bounding rectangles, whose lack of tightness compromises their efficiency in exact nearest neighbor search. We propose a new cluster-adaptive distance bound based on separating hyperplane boundaries of Voronoi clusters to complement our cluster based index. This bound enables efficient spatial filtering, with a relatively small preprocessing storage overhead and is applicable to euclidean and Mahalanobis similarity measures. Experiments in exact nearest-neighbor set retrieval, conducted on real data sets, show that our indexing method is scalable with data set size and data dimensionality and outperforms several recently proposed indexes. Relative to the VA-File, over a wide range of quantization resolutions, it is able to reduce random IO accesses, given (roughly) the same amount of sequential IO operations, by factors reaching 100X and more.

#index 1573148
#* Automatic Discovery of Personal Name Aliases from the Web
#@ Danushka Bollegala;Yutaka Matsuo;Mitsuru Ishizuka
#t 2011
#c 7
#! An individual is typically referred by numerous name aliases on the web. Accurate identification of aliases of a given person name is useful in various web related tasks such as information retrieval, sentiment analysis, personal name disambiguation, and relation extraction. We propose a method to extract aliases of a given personal name from the web. Given a personal name, the proposed method first extracts a set of candidate aliases. Second, we rank the extracted candidates according to the likelihood of a candidate being a correct alias of the given name. We propose a novel, automatically extracted lexical pattern-based approach to efficiently extract a large set of candidate aliases from snippets retrieved from a web search engine. We define numerous ranking scores to evaluate candidate aliases using three approaches: lexical pattern frequency, word co-occurrences in an anchor text graph, and page counts on the web. To construct a robust alias detection system, we integrate the different ranking scores into a single ranking function using ranking support vector machines. We evaluate the proposed method on three data sets: an English personal names data set, an English place names data set, and a Japanese personal names data set. The proposed method outperforms numerous baselines and previously proposed name alias extraction methods, achieving a statistically significant mean reciprocal rank (MRR) of 0.67. Experiments carried out using location names and Japanese personal names suggest the possibility of extending the proposed method to extract aliases for different types of named entities, and for different languages. Moreover, the aliases extracted using the proposed method are successfully utilized in an information retrieval task and improve recall by 20 percent in a relation-detection task.

#index 1573149
#* Automatic Enrichment of Semantic Relation Network and Its Application to Word Sense Disambiguation
#@ Myunggwon G. Hwang;Chang Choi;Pan-Koo Kim
#t 2011
#c 7
#! The most fundamental step in semantic information processing (SIP) is to construct knowledge base (KB) at the human level; that is to the general understanding and conception of human knowledge. WordNet has been built to be the most systematic and as close to the human level and is being applied actively in various works. In one of our previous research, we found that a semantic gap exists between concept pairs of WordNet and those of real world. This paper contains a study on the enrichment method to build a KB. We describe the methods and the results for the automatic enrichment of the semantic relation network. A rule based method using WordNet's glossaries and an inference method using axioms for WordNet relations are applied for the enrichment and an enriched WordNet (E-WordNet) is built as the result. Our experimental results substantiate the usefulness of E-WordNet. An evaluation by comparison with the human level is attempted. Moreover, WSD-SemNet, a new word sense disambiguation (WSD) method in which E-WordNet is applied, is proposed and evaluated by comparing it with the state-of-the-art algorithm.

#index 1573150
#* Design and Implementation of an Intrusion Response System for Relational Databases
#@ Ashish Kamra;Elisa Bertino
#t 2011
#c 7
#! The intrusion response component of an overall intrusion detection system is responsible for issuing a suitable response to an anomalous request. We propose the notion of database response policies to support our intrusion response system tailored for a DBMS. Our interactive response policy language makes it very easy for the database administrators to specify appropriate response actions for different circumstances depending upon the nature of the anomalous request. The two main issues that we address in context of such response policies are that of policy matching, and policy administration. For the policy matching problem, we propose two algorithms that efficiently search the policy database for policies that match an anomalous request. We also extend the PostgreSQL DBMS with our policy matching mechanism, and report experimental results. The experimental evaluation shows that our techniques are very efficient. The other issue that we address is that of administration of response policies to prevent malicious modifications to policy objects from legitimate users. We propose a novel Joint Threshold Administration Model (JTAM) that is based on the principle of separation of duty. The key idea in JTAM is that a policy object is jointly administered by at least k database administrator (DBAs), that is, any modification made to a policy object will be invalid unless it has been authorized by at least k DBAs. We present design details of JTAM which is based on a cryptographic threshold signature scheme, and show how JTAM prevents malicious modifications to policy objects from authorized users. We also implement JTAM in the PostgreSQL DBMS, and report experimental results on the efficiency of our techniques.

#index 1573151
#* Knowledge Discovery in Services (KDS): Aggregating Software Services to Discover Enterprise Mashups
#@ M. Brian Blake;Michael E. Nowlan
#t 2011
#c 7
#! Service mashup is the act of integrating the resulting data of two complementary software services into a common picture. Such an approach is promising with respect to the discovery of new types of knowledge. However, before service mashup routines can be executed, it is necessary to predict which services (of an open repository) are viable candidates. Similar to Knowledge Discovery in Databases (KDD), we introduce the Knowledge Discovery in Services (KDS) process that identifies mashup candidates. In this work, the KDS process is specialized to address a repository of open services that do not contain semantic annotations. In these situations, specialized techniques are required to determine equivalences among open services with reasonable precision. This paper introduces a bottom-up process for KDS that adapts to the environment of services for which it operates. Detailed experiments are discussed that evaluate KDS techniques on an open repository of services from the Internet and on a repository of services created in a controlled environment.

#index 1573152
#* Locally Consistent Concept Factorization for Document Clustering
#@ Deng Cai;Xiaofei He;Jiawei Han
#t 2011
#c 7
#! Previous studies have demonstrated that document clustering performance can be improved significantly in lower dimensional linear subspaces. Recently, matrix factorization-based techniques, such as Nonnegative Matrix Factorization (NMF) and Concept Factorization (CF), have yielded impressive results. However, both of them effectively see only the global euclidean geometry, whereas the local manifold geometry is not fully considered. In this paper, we propose a new approach to extract the document concepts which are consistent with the manifold geometry such that each concept corresponds to a connected component. Central to our approach is a graph model which captures the local geometry of the document submanifold. Thus, we call it Locally Consistent Concept Factorization (LCCF). By using the graph Laplacian to smooth the document-to-concept mapping, LCCF can extract concepts with respect to the intrinsic manifold structure and thus documents associated with the same concept can be well clustered. The experimental results on TDT2 and Reuters-21578 have shown that the proposed approach provides a better representation and achieves better clustering results in terms of accuracy and mutual information.

#index 1573153
#* Mining Cluster-Based Temporal Mobile Sequential Patterns in Location-Based Service Environments
#@ Eric Hsueh-Chan Lu;Vincent S. Tseng;Philip S. Yu
#t 2011
#c 7
#! Researches on Location-Based Service (LBS) have been emerging in recent years due to a wide range of potential applications. One of the active topics is the mining and prediction of mobile movements and associated transactions. Most of existing studies focus on discovering mobile patterns from the whole logs. However, this kind of patterns may not be precise enough for predictions since the differentiated mobile behaviors among users and temporal periods are not considered. In this paper, we propose a novel algorithm, namely, Cluster-based Temporal Mobile Sequential Pattern Mine (CTMSP-Mine), to discover the Cluster-based Temporal Mobile Sequential Patterns (CTMSPs). Moreover, a prediction strategy is proposed to predict the subsequent mobile behaviors. In CTMSP-Mine, user clusters are constructed by a novel algorithm named Cluster-Object-based Smart Cluster Affinity Search Technique (CO-Smart-CAST) and similarities between users are evaluated by the proposed measure, Location-Based Service Alignment (LBS-Alignment). Meanwhile, a time segmentation approach is presented to find segmenting time intervals where similar mobile characteristics exist. To our best knowledge, this is the first work on mining and prediction of mobile behaviors with considerations of user relations and temporal property simultaneously. Through experimental evaluation under various simulated conditions, the proposed methods are shown to deliver excellent performance.

#index 1573154
#* Classification and Novel Class Detection in Concept-Drifting Data Streams under Time Constraints
#@ Mohammad Masud;Jing Gao;Latifur Khan;Jiawei Han;Bhavani M. Thuraisingham
#t 2011
#c 7
#! Most existing data stream classification techniques ignore one important aspect of stream data: arrival of a novel class. We address this issue and propose a data stream classification technique that integrates a novel class detection mechanism into traditional classifiers, enabling automatic detection of novel classes before the true labels of the novel class instances arrive. Novel class detection problem becomes more challenging in the presence of concept-drift, when the underlying data distributions evolve in streams. In order to determine whether an instance belongs to a novel class, the classification model sometimes needs to wait for more test instances to discover similarities among those instances. A maximum allowable wait time T_c is imposed as a time constraint to classify a test instance. Furthermore, most existing stream classification approaches assume that the true label of a data point can be accessed immediately after the data point is classified. In reality, a time delay T_l is involved in obtaining the true label of a data point since manual labeling is time consuming. We show how to make fast and correct classification decisions under these constraints and apply them to real benchmark data. Comparison with state-of-the-art stream classification techniques prove the superiority of our approach.

#index 1573155
#* On Computing Farthest Dominated Locations
#@ Hua Lu;Man Lung Yiu
#t 2011
#c 7
#! In reality, spatial objects (e.g., hotels) not only have spatial locations but also have quality attributes (e.g., price, star). An object p is said to dominate another one p^{\prime }, if p is no worse than p^{\prime } with respect to every quality attribute and p is better on at least one quality attribute. Traditional spatial queries (e.g., nearest neighbor, closest pair) ignore quality attributes, whereas conventional dominance-based queries (e.g., skyline) neglect spatial locations. Motivated by these observations, we propose a novel query by combining spatial and quality attributes together meaningfully. Given a set of (competitors') spatial objects P, a set of (candidate) locations L, and a quality vector \Psi as design competence (for L), the farthest dominated location (FDL) query retrieves the location s \in L such that the distance to its nearest dominating object in P is maximized. FDL queries are suitable for various spatial decision support applications such as business planning, wild animal protection, and digital battle field systems. As FDL queries cannot be readily solved by existing techniques, we develop several efficient R-tree-based algorithms for processing FDL queries, which offer users a range of selections in terms of different indexes available on the data. We also generalize our methods to support the generic distance metric and other interesting query types. The experimental results on both real and synthetic data sets disclose the performance of those algorithms, and reveal the most efficient and scalable one among them.

#index 1573156
#* Seeking Quality of Web Service Composition in a Semantic Dimension
#@ Freddy Lecue;Nikolay Mehandjiev
#t 2011
#c 7
#! Ranking and optimization of web service compositions represent challenging areas of research with significant implications for the realization of the “Web of Services” vision. “Semantic web services” use formal semantic descriptions of web service functionality and interface to enable automated reasoning over web service compositions. To judge the quality of the overall composition, for example, we can start by calculating the semantic similarities between outputs and inputs of connected constituent services, and aggregate these values into a measure of semantic quality for the composition. This paper takes a specific interest in combining semantic and nonfunctional criteria such as quality of service (QoS) to evaluate quality in web services composition. It proposes a novel and extensible model balancing the new dimension of semantic quality (as a functional quality metric) with a QoS metric, and using them together as ranking and optimization criteria. It also demonstrates the utility of Genetic Algorithms to allow optimization within the context of a large number of services foreseen by the “Web of Services” vision. We test the performance of the overall approach using a set of simulation experiments, and discuss its advantages and weaknesses.

#index 1583279
#* A Hidden Topic-Based Framework toward Building Applications with Short Web Documents
#@ Xuan-Hieu Phan;Cam-Tu Nguyen;Dieu-Thu Le;Le-Minh Nguyen;Susumu Horiguchi;Quang-Thuy Ha
#t 2011
#c 7
#! This paper introduces a hidden topic-based framework for processing short and sparse documents (e.g., search result snippets, product descriptions, book/movie summaries, and advertising messages) on the Web. The framework focuses on solving two main challenges posed by these kinds of documents: 1) data sparseness and 2) synonyms/homonyms. The former leads to the lack of shared words and contexts among documents while the latter are big linguistic obstacles in natural language processing (NLP) and information retrieval (IR). The underlying idea of the framework is that common hidden topics discovered from large external data sets (universal data sets), when included, can make short documents less sparse and more topic-oriented. Furthermore, hidden topics from universal data sets help handle unseen data better. The proposed framework can also be applied for different natural languages and data domains. We carefully evaluated the framework by carrying out two experiments for two important online applications (Web search result classification and matching/ranking for contextual advertising) with large-scale universal data sets and we achieved significant results.

#index 1583280
#* A Web Search Engine-Based Approach to Measure Semantic Similarity between Words
#@ Danushka Bollegala;Yutaka Matsuo;Mitsuru Ishizuka
#t 2011
#c 7
#! Measuring the semantic similarity between words is an important component in various tasks on the web such as relation extraction, community mining, document clustering, and automatic metadata extraction. Despite the usefulness of semantic similarity measures in these applications, accurately measuring semantic similarity between two words (or entities) remains a challenging task. We propose an empirical method to estimate semantic similarity using page counts and text snippets retrieved from a web search engine for two words. Specifically, we define various word co-occurrence measures using page counts and integrate those with lexical patterns extracted from text snippets. To identify the numerous semantic relations that exist between two given words, we propose a novel pattern extraction algorithm and a pattern clustering algorithm. The optimal combination of page counts-based co-occurrence measures and lexical pattern clusters is learned using support vector machines. The proposed method outperforms various baselines and previously proposed web-based semantic similarity measures on three benchmark data sets showing a high correlation with human ratings. Moreover, the proposed method significantly improves the accuracy in a community mining task.

#index 1583281
#* Flexible and Efficient Resolution of Skyline Query Size Constraints
#@ Hua Lu;Christian S. Jensen;Zhenjie Zhang
#t 2011
#c 7
#! Given a set of multidimensional points, a skyline query returns the interesting points that are not dominated by other points. It has been observed that the actual cardinality (s) of a skyline query result may differ substantially from the desired result cardinality (k), which has prompted studies on how to reduce s for the case where ks. Based on these observations, the paper proposes a new approach, called skyline ordering, that forms a skyline-based partitioning of a given data set such that an order exists among the partitions. Then, set-wide maximization techniques may be applied within each partition. Efficient algorithms are developed for skyline ordering and for resolving size constraints using the skyline order. The results of extensive experiments show that skyline ordering yields a flexible framework for the efficient and scalable resolution of arbitrary size constraints on skyline queries.

#index 1583282
#* Graph Pattern Matching: A Join/Semijoin Approach
#@ Jiefeng Cheng;Jeffrey Xu Yu;Philip S. Yu
#t 2011
#c 7
#! Due to rapid growth of the Internet and new scientific/technological advances, there exist many new applications that model data as graphs, because graphs have sufficient expressiveness to model complicated structures. The dominance of graphs in real-world applications demands new graph processing techniques to access large data graphs effectively and efficiently. In this paper, we study a graph pattern matching problem, which is to find all patterns in a large data graph that match a user-given graph pattern. We propose new two-step R-join (reachability join) algorithms with a filter step (R-semijoin) and a fetch step (R-join) by utilizing a new cluster-based join index with graph codes in a relational database context. We also propose two optimization approaches to further optimize sequences of R-joins/R-semijoins. The first approach is based on R-join order selection followed by R-semijoin enhancement, and the second approach is to interleave R-joins with R-semijoins. We conducted extensive performance studies, and confirm the efficiency of our proposed new approaches.

#index 1583283
#* Higher Order Naïve Bayes: A Novel Non-IID Approach to Text Classification
#@ Murat Can Ganiz;Cibin George;William M. Pottenger
#t 2011
#c 7
#! The underlying assumption in traditional machine learning algorithms is that instances are Independent and Identically Distributed (IID). These critical independence assumptions made in traditional machine learning algorithms prevent them from going beyond instance boundaries to exploit latent relations between features. In this paper, we develop a general approach to supervised learning by leveraging higher order dependencies between features. We introduce a novel Bayesian framework for classification termed Higher Order Naïve Bayes (HONB). Unlike approaches that assume data instances are independent, HONB leverages higher order relations between features across different instances. The approach is validated in the classification domain on widely used benchmark data sets. Results obtained on several benchmark text corpora demonstrate that higher order approaches achieve significant improvements in classification accuracy over the baseline methods, especially when training data is scarce. A complexity analysis also reveals that the space and time complexity of HONB compare favorably with existing approaches.

#index 1583284
#* KEMB: A Keyword-Based XML Message Broker
#@ Guoliang Li;Jianhua Feng;Jianyong Wang;Lizhu Zhou
#t 2011
#c 7
#! This paper studies the problem of XML message brokering with user subscribed profiles of keyword queries and presents a KEyword-based XML Message Broker (KEMB) to address this problem. In contrast to traditional-path-expressions-based XML message brokers, KEMB stores a large number of user profiles, in the form of keyword queries, which capture the data requirement of users/applications, as opposed to path expressions, such as XPath/XQuery expressions. KEMB brings new challenges: 1) how to effectively identify relevant answers of keyword queries in XML data streams; and 2) how to efficiently answer large numbers of concurrent keyword queries. We adopt compact lowest common ancestors (CLCAs) to effectively identify relevant answers. We devise an automaton-based method to process large numbers of queries and devise an effective optimization strategy to enhance performance and scalability. We have implemented and evaluated KEMB on various data sets. The experimental results show that KEMB achieves high performance and scales very well.

#index 1583285
#* myOLAP: An Approach to Express and Evaluate OLAP Preferences
#@ Matteo Golfarelli;Stefano Rizzi;Paolo Biondi
#t 2011
#c 7
#! Multidimensional databases are the core of business intelligence systems. Their users express complex OLAP queries, often returning large volumes of facts, sometimes providing little or no information. Thus, expressing preferences could be highly valuable in this domain. The OLAP domain is representative of an unexplored class of preference queries, characterized by three peculiarities: preferences can be expressed on both numerical and categorical domains; they can also be expressed on the aggregation level of facts; the space on which preferences are expressed includes both elemental and aggregated facts. In this paper, we present myOLAP, an approach for expressing and evaluating OLAP preferences, devised by taking into account the three peculiarities above. We first propose a preference algebra where users are enabled to express their preferences, besides on attributes and measures, also on the aggregation level of facts, for instance, by stating that monthly data are preferred to yearly and daily data. Then, with respect to preference evaluation, we propose an algorithm called WeSt that relies on a novel graph representation where two types of domination between sets of facts may be expressed, which considerably improves efficiency. The approach is extensively tested for efficiency and effectiveness on real data, and compared against two other approaches in the literature.

#index 1583286
#* Processing of Continuous Location-Based Range Queries on Moving Objects in Road Networks
#@ Haojun Wang;Roger Zimmermann
#t 2011
#c 7
#! With the proliferation of mobile devices, an increasing number of urban users subscribe to location-based services. This trend has led to significant research interest in techniques that address two fundamental requirements: road network-based distance computation and the capability to process moving objects as points of interests. However, there exist few techniques that support both requirements simultaneously. To address these challenges, we propose a novel approach to process continuous range queries. We build on our previous work of an infrastructure that supports location-based snapshot queries on MOVing objects in road Networks (MOVNet). We introduce several significant features to enable continuous queries. The dual index structure that we proposed for MOVNet has been appropriately modified. We further appoint a number of connecting vertices in each cell and precompute the distances among them to expedite query processing. Most importantly, to alleviate the effects of frequent object updates, we introduce a Shortest-Distance-based Tree (SD-Tree). We illustrate that the network connectivity and distance information can be preserved and reused by the SD-Tree when the query point location is updated; hence, reducing the continuous query update cost. Our experimental results demonstrate that our method yields excellent performance with a very large number of moving objects.

#index 1583287
#* Random k-Labelsets for Multilabel Classification
#@ Grigorios Tsoumakas;Ioannis Katakis;Ioannis Vlahavas
#t 2011
#c 7
#! A simple yet effective multilabel learning method, called label powerset (LP), considers each distinct combination of labels that exist in the training set as a different class value of a single-label classification task. The computational efficiency and predictive performance of LP is challenged by application domains with large number of labels and training examples. In these cases, the number of classes may become very large and at the same time many classes are associated with very few training examples. To deal with these problems, this paper proposes breaking the initial set of labels into a number of small random subsets, called labelsets and employing LP to train a corresponding classifier. The labelsets can be either disjoint or overlapping depending on which of two strategies is used to construct them. The proposed method is called {\rm RA}k{\rm EL} (RAndom k labELsets), where k is a parameter that specifies the size of the subsets. Empirical evidence indicates that {\rm RA}k{\rm EL} manages to improve substantially over LP, especially in domains with large number of labels and exhibits competitive performance against other high-performing multilabel learning methods.

#index 1583288
#* Reducing the Loss of Information through Annealing Text Distortion
#@ Ana Granados;Manuel Cebrian;David Camacho;Francisco de Borja Rodriguez
#t 2011
#c 7
#! Compression distances have been widely used in knowledge discovery and data mining. They are parameter-free, widely applicable, and very effective in several domains. However, little has been done to interpret their results or to explain their behavior. In this paper, we take a step toward understanding compression distances by performing an experimental evaluation of the impact of several kinds of information distortion on compression-based text clustering. We show how progressively removing words in such a way that the complexity of a document is slowly reduced helps the compression-based text clustering and improves its accuracy. In fact, we show how the nondistorted text clustering can be improved by means of annealing text distortion. The experimental results shown in this paper are consistent using different data sets, and different compression algorithms belonging to the most important compression families: Lempel-Ziv, Statistical and Block-Sorting.

#index 1583289
#* Trace-Oriented Feature Analysis for Large-Scale Text Data Dimension Reduction
#@ Jun Yan;Ning Liu;Shuicheng Yan;Qiang Yang;Weiguo Fan;Wei Wei;Zheng Chen
#t 2011
#c 7
#! Dimension reduction for large-scale text data is attracting much attention nowadays due to the rapid growth of the World Wide Web. We can categorize those popular dimension reduction algorithms into two groups: feature extraction and feature selection algorithms. In the former, new features are combined from their original features through algebraic transformation. Though many of them have been validated to be effective, these algorithms are typically associated with high computational overhead, making them difficult to be applied on real-world text data. In the latter, subsets of features are selected directly. These algorithms are widely used in real-world tasks owing to their efficiency, but are often based on greedy strategies rather than optimal solutions. An important problem remains: it has been troublesome to integrate these two types of algorithms into a single framework, making it difficult to reap the benefits from both. In this paper, we formulate the two algorithm categories through a unified optimization framework, under which we develop a novel feature selection algorithm called Trace-Oriented Feature Analysis (TOFA). In detail, we integrate the objective functions of several state-of-the-art feature extraction algorithms into a unified one under the optimization framework, and then we propose to optimize this objective function in the solution space of feature selection algorithms for dimensionality reduction. Since the proposed objective function of TOFA integrates many prominent feature extraction algorithms' objective functions, such as unsupervised Principal Component Analysis (PCA) and supervised Maximum Margin Criterion (MMC), TOFA can handle both supervised and unsupervised problems. In addition, by tuning a weight value, TOFA is also suitable to solve semisupervised learning problems. Experimental results on several real-world data sets validate the effectiveness and efficiency of TOFA in text data for dimensionality reduction purpose.

#index 1595887
#* Guest Editors' Introduction to the Special Section on the 26th International Conference on Data Engineering
#@ Shahram Ghandeharizadeh;Gerhard Weikum;Jayant R. Haritsa
#t 2011
#c 7

#index 1595888
#* Efficient Top-k Approximate Subtree Matching in Small Memory
#@ Nikolaus Augsten;Denilson Barbosa;Michael Bohlen;Themis Palpanas
#t 2011
#c 7
#! We consider the Top-k Approximate Subtree Matching (tasm) problem: finding the k best matches of a small query tree within a large document tree using the canonical tree edit distance as a similarity measure between subtrees. Evaluating the tree edit distance for large XML trees is difficult: the best known algorithms have cubic runtime and quadratic space complexity, and, thus, do not scale. Our solution is tasm-postorder, a memory-efficient and scalable tasm algorithm. We prove an upper bound for the maximum subtree size for which the tree edit distance needs to be evaluated. The upper bound depends on the query and is independent of the document size and structure. A core problem is to efficiently prune subtrees that are above this size threshold. We develop an algorithm based on the prefix ring buffer that allows us to prune all subtrees above the threshold in a single postorder scan of the document. The size of the prefix ring buffer is linear in the threshold. As a result, the space complexity of tasm-postorder depends only on k and the query size, and the runtime of tasm-postorder is linear in the size of the document. Our experimental evaluation on large synthetic and real XML documents confirms our analytic results.

#index 1595889
#* Usher: Improving Data Quality with Dynamic Forms
#@ Kuang Chen;Harr Chen;Neil Conway;Joseph M. Hellerstein;Tapan S. Parikh
#t 2011
#c 7
#! Data quality is a critical problem in modern databases. data-entry forms present the first and arguably best opportunity for detecting and mitigating errors, but there has been little research into automatic methods for improving data quality at entry time. In this paper, we propose Usher, an end-to-end system for form design, entry, and data quality assurance. Using previous form submissions, Usher learns a probabilistic model over the questions of the form. Usher then applies this model at every step of the data-entry process to improve data quality. Before entry, it induces a form layout that captures the most important data values of a form instance as quickly as possible and reduces the complexity of error-prone questions. During entry, it dynamically adapts the form to the values being entered by providing real-time interface feedback, reasking questions with dubious responses, and simplifying questions by reformulating them. After entry, it revisits question responses that it deems likely to have been entered incorrectly by reasking the question or a reformulation thereof. We evaluate these components of Usher using two real-world data sets. Our results demonstrate that Usher can improve data quality considerably at a reduced cost when compared to current practice.

#index 1595890
#* Efficient and Accurate Discovery of Patterns in Sequence Data Sets
#@ Avrilia Floratou;Sandeep Tata;Jignesh M. Patel
#t 2011
#c 7
#! Existing sequence mining algorithms mostly focus on mining for subsequences. However, a large class of applications, such as biological DNA and protein motif mining, require efficient mining of “approximate” patterns that are contiguous. The few existing algorithms that can be applied to find such contiguous approximate pattern mining have drawbacks like poor scalability, lack of guarantees in finding the pattern, and difficulty in adapting to other applications. In this paper, we present a new algorithm called FLexible and Accurate Motif DEtector (FLAME). FLAME is a flexible suffix-tree-based algorithm that can be used to find frequent patterns with a variety of definitions of motif (pattern) models. It is also accurate, as it always finds the pattern if it exists. Using both real and synthetic data sets, we demonstrate that FLAME is fast, scalable, and outperforms existing algorithms on a variety of performance metrics. In addition, based on FLAME, we also address a more general problem, named extended structured motif extraction, which allows mining frequent combinations of motifs under relaxed constraints.

#index 1595891
#* Frequent Item Computation on a Chip
#@ Jens Teubner;Rene Muller;Gustavo Alonso
#t 2011
#c 7
#! Computing frequent items is an important problem by itself and as a subroutine in several data mining algorithms. In this paper, we explore how to accelerate the computation of frequent items using field-programmable gate arrays (FPGAs) with a threefold goal: increase performance over existing solutions, reduce energy consumption over CPU-based systems, and explore the design space in detail as the constraints on FPGAs are very different from those of traditional software-based systems. We discuss three design alternatives, each one of them exploiting different FPGA features and each one providing different performance/scalability trade-offs. An important result of the paper is to demonstrate how the inherent massive parallelism of FPGAs can improve performance of existing algorithms but only after a fundamental redesign of the algorithms. Our experimental results show that, e.g., the pipelined solution we introduce can reach more than 100 million tuples per second of sustained throughput (four times the best available results to date) by making use of techniques that are not available to CPU-based solutions. Moreover, and unlike in software approaches, the high throughput is independent of the skew of the Zipf distribution of the input and at a far lower energy cost.

#index 1595892
#* Continuous Monitoring of Distance-Based Range Queries
#@ Muhammad Aamir Cheema;Ljiljana Brankovic;Xuemin Lin;Wenjie Zhang;Wei Wang
#t 2011
#c 7
#! Given a positive value r, a distance-based range query returns the objects that lie within the distance r of the query location. In this paper, we focus on the distance-based range queries that continuously change their locations in a euclidean space. We present an efficient and effective monitoring technique based on the concept of a safe zone. The safe zone of a query is the area with a property that while the query remains inside it, the results of the query remain unchanged. Hence, the query does not need to be reevaluated unless it leaves the safe zone. Our contributions are as follows: 1) We propose a technique based on powerful pruning rules and a unique access order which efficiently computes the safe zone and minimizes the I/O cost. 2) We theoretically determine and experimentally verify the expected distance a query moves before leaving the safe zone and, for majority of queries, the expected number of guard objects. 3) Our experiments demonstrate that the proposed approach is close to optimal and is an order of magnitude faster than a naïve algorithm. 4) We also extend our technique to monitor the queries in a road network. Our algorithm is up to two order of magnitude faster than a naïve algorithm.

#index 1595893
#* Differential Privacy via Wavelet Transforms
#@ Xiaokui Xiao;Guozhang Wang;Johannes Gehrke
#t 2011
#c 7
#! Privacy-preserving data publishing has attracted considerable research interest in recent years. Among the existing solutions, \epsilon-differential privacy provides the strongest privacy guarantee. Existing data publishing methods that achieve \epsilon-differential privacy, however, offer little data utility. In particular, if the output data set is used to answer count queries, the noise in the query answers can be proportional to the number of tuples in the data, which renders the results useless. In this paper, we develop a data publishing technique that ensures \epsilon-differential privacy while providing accurate answers for range-count queries, i.e., count queries where the predicate on each attribute is a range. The core of our solution is a framework that applies wavelet transforms on the data before adding noise to it. We present instantiations of the proposed framework for both ordinal and nominal data, and we provide a theoretical analysis on their privacy and utility guarantees. In an extensive experimental study on both real and synthetic data, we show the effectiveness and efficiency of our solution.

#index 1595894
#* Monochromatic and Bichromatic Reverse Top-k Queries
#@ Akrivi Vlachou;Christos Doulkeridis;Yannis Kotidis;Kjetil Norvag
#t 2011
#c 7
#! Nowadays, most applications return to the user a limited set of ranked results based on the individual user's preferences, which are commonly expressed through top-k queries. From the perspective of a manufacturer, it is imperative that her products appear in the highest ranked positions for many different user preferences, otherwise the product is not visible to potential customers. In this paper, we define a novel query type, namely the reverse top-k query, that covers this requirement: “Given a potential product, which are the user preferences that make this product belong to the top-k query result set?.” Reverse top-k queries are essential for manufacturers to assess the impact of their products in the market based on the competition. We formally define reverse top-k queries and introduce two versions of the query, monochromatic and bichromatic. First, we provide a geometric interpretation of the monochromatic reverse top-k query to acquire an intuition of the solution space. Then, we study in detail the case of bichromatic reverse top-k query, and we propose two techniques for query processing, namely an efficient threshold-based algorithm and an algorithm based on materialized reverse top-k views. Our experimental evaluation demonstrates the efficiency of our techniques.

#index 1595895
#* Energy Time Series Forecasting Based on Pattern Sequence Similarity
#@ Francisco Martinez Alvarez;Alicia Troncoso;Jose C. Riquelme;Jesus S. Aguilar Ruiz
#t 2011
#c 7
#! This paper presents a new approach to forecast the behavior of time series based on similarity of pattern sequences. First, clustering techniques are used with the aim of grouping and labeling the samples from a data set. Thus, the prediction of a data point is provided as follows: first, the pattern sequence prior to the day to be predicted is extracted. Then, this sequence is searched in the historical data and the prediction is calculated by averaging all the samples immediately after the matched sequence. The main novelty is that only the labels associated with each pattern are considered to forecast the future behavior of the time series, avoiding the use of real values of the time series until the last step of the prediction process. Results from several energy time series are reported and the performance of the proposed method is compared to that of recently published techniques showing a remarkable improvement in the prediction.

#index 1595896
#* Integration of the HL7 Standard in a Multiagent System to Support Personalized Access to e-Health Services
#@ Pasquale De Meo;Giovanni Quattrone;Domenico Ursino
#t 2011
#c 7
#! In this paper, we present a multiagent system to support patients in search of healthcare services in an e-health scenario. The proposed system is HL7-aware in that it represents both patient and service information according to the directives of HL7, the information management standard adopted in medical context. Our system builds a profile for each patient and uses it to detect Healthcare Service Providers delivering e-health services potentially capable of satisfying his needs. In order to handle this search it can exploit three different algorithms: the first, called PPB, uses only information stored in the patient profile; the second, called DS-PPB, considers both information stored in the patient profile and similarities among the e-health services delivered by the involved providers; the third, called AB, relies on {\rm A}{\bf^*}, a popular search algorithm in Artificial Intelligence. Our system builds also a social network of patients; once a patient submits a query and retrieves a set of services relevant to him, our system applies a spreading activation technique on this social network to find other patients who may benefit from these services.

#index 1595897
#* Making Aggregation Work in Uncertain and Probabilistic Databases
#@ Raghotham Murthy;Robert Ikeda;Jennifer Widom
#t 2011
#c 7
#! We describe how aggregation is handled in the Trio system for uncertain and probabilistic data. Because “exact” aggregation in uncertain databases can produce exponentially sized results, we provide three alternatives: a low bound on the aggregate value, a high bound on the value, and the expected value. These variants return a single result instead of a set of possible results, and they are generally efficient to compute for both full-table and grouped aggregation queries. We provide formal definitions and semantics and a description of our open source implementation for single-table aggregation queries. We study the performance and scalability of our algorithms through experiments over a large synthetic data set. We also provide some preliminary results on aggregations over joins.

#index 1595898
#* Comprehensive Citation Index for Research Networks
#@ Henry H. Bi;Jianrui Wang;Dennis K.  J. Lin
#t 2011
#c 7
#! The existing Science Citation Index only counts direct citations, whereas PageRank disregards the number of direct citations. We propose a new Comprehensive Citation Index (CCI) that evaluates both direct and indirect intellectual influence of research papers, and show that CCI is more reliable in discovering research papers with far-reaching influence.

#index 1602027
#* Efficient Algorithms for Summarizing Graph Patterns
#@ Jianzhong Li;Yong Liu;Hong Gao
#t 2011
#c 7
#! We investigate the problem of summarizing frequent subgraphs by a smaller set of representative patterns. We show that some special graph patterns, called \delta\hbox{-}jumppatterns in this paper, must be representative patterns. Based on the fact, we devise two algorithms, RP-FP and RP-GD, to mine a representative set that summarizes frequent subgraphs. RP-FP derives a representative set from frequent closed subgraphs, whereas RP-GD mines a representative set from graph databases directly. Three novel heuristic strategies, Last-Succeed-First-Check, Reverse-Path-Trace, and Nephew-Representative-Based-Cover, are proposed to further improve the efficiency of RP-GD. RP-FP can provide a tight ratio bound but has heavy computation cost. RP-GD cannot provide a ratio bound guarantee but is more efficient than RP-FP. We also make use of the similarity between sibling branches in the graph pattern space to devise another much more efficient algorithm, RP-Leap, for mining a representative set that can approximately summarize frequent subgraphs. Our extensive experiments on both real and synthetic data sets verify the summarization quality and efficiency of our algorithms. To further demonstrate the interestingness of representative patterns, we study an application of representative patterns to classification. We demonstrate that the classification accuracy achieved by representative pattern-based model is no less than that achieved by closed graph pattern-based model.

#index 1602028
#* Laplacian Regularized Gaussian Mixture Model for Data Clustering
#@ Xiaofei He;Deng Cai;Yuanlong Shao;Hujun Bao;Jiawei Han
#t 2011
#c 7
#! Gaussian Mixture Models (GMMs) are among the most statistically mature methods for clustering. Each cluster is represented by a Gaussian distribution. The clustering process thereby turns to estimate the parameters of the Gaussian mixture, usually by the Expectation-Maximization algorithm. In this paper, we consider the case where the probability distribution that generates the data is supported on a submanifold of the ambient space. It is natural to assume that if two points are close in the intrinsic geometry of the probability distribution, then their conditional probability distributions are similar. Specifically, we introduce a regularized probabilistic model based on manifold structure for data clustering, called Laplacian regularized Gaussian Mixture Model (LapGMM). The data manifold is modeled by a nearest neighbor graph, and the graph structure is incorporated in the maximum likelihood objective function. As a result, the obtained conditional probability distribution varies smoothly along the geodesics of the data manifold. Experimental results on real data sets demonstrate the effectiveness of the proposed approach.

#index 1602029
#* Reasoning about Distributed Knowledge-Transforming Peer Interactions
#@ Marco Schorlemmer;Dave Robertson
#t 2011
#c 7
#! We address the problem of how to reason about properties of knowledge transformations as they occur in distributed and decentralized interactions between large and complex artifacts, such as databases, web services, and ontologies. Based on the conceptual distinction between specifications of interactions and properties of knowledge transformations that follow from these interactions, we explore a novel mixture of process calculus and property inference by connecting interaction models with knowledge transformation rules. We aim at being generic in our exploration, hence our emphasis on abstract knowledge transformations, although we exemplify it using a lightweight specification language for interaction modeling (for which an executable peer-to-peer environment already exists) and provide a formal semantics for knowledge transformation rules using the theory of institutions. Consequently, our exploration is also an example of the gain obtained by linking current state-of-the-art distributed knowledge engineering based on web services and peer-based architectures with formal methods drawn from a long tradition in algebraic specification.

#index 1602030
#* A Privacy-Preserving Remote Data Integrity Checking Protocol with Data Dynamics and Public Verifiability
#@ Zhuo Hao;Sheng Zhong;Nenghai Yu
#t 2011
#c 7
#! Remote data integrity checking is a crucial technology in cloud computing. Recently, many works focus on providing data dynamics and/or public verifiability to this type of protocols. Existing protocols can support both features with the help of a third-party auditor. In a previous work, Seb茅 et al. [1] propose a remote data integrity checking protocol that supports data dynamics. In this paper, we adapt Seb茅 et al.'s protocol to support public verifiability. The proposed protocol supports public verifiability without help of a third-party auditor. In addition, the proposed protocol does not leak any private information to third-party verifiers. Through a formal analysis, we show the correctness and security of the protocol. After that, through theoretical analysis and experimental results, we demonstrate that the proposed protocol has a good performance.

#index 1602031
#* Guest Editor's Introduction: Cloud Data Management
#@ David Lomet
#t 2011
#c 7

#index 1602032
#* Optimizing Multiway Joins in a Map-Reduce Environment
#@ Foto N. Afrati;Jeffrey D. Ullman
#t 2011
#c 7
#! Implementations of map-reduce are being used to perform many operations on very large data. We examine strategies for joining several relations in the map-reduce environment. Our new approach begins by identifying the “map-key,” the set of attributes that identify the Reduce process to which a Map process must send a particular tuple. Each attribute of the map-key gets a “share,” which is the number of buckets into which its values are hashed, to form a component of the identifier of a Reduce process. Relations have their tuples replicated in limited fashion, the degree of replication depending on the shares for those map-key attributes that are missing from their schema. We study the problem of optimizing the shares, given a fixed number of Reduce processes. An algorithm for detecting and fixing problems where a variable is mistakenly included in the map-key is given. Then, we consider two important special cases: chain joins and star joins. In each case, we are able to determine the map-key and determine the shares that yield the least replication. While the method we propose is not always superior to the conventional way of using map-reduce to implement joins, there are some important cases involving large-scale data where our method wins, including: 1) analytic queries in which a very large fact table is joined with smaller dimension tables, and 2) queries involving paths through graphs with high out-degree, such as the Web or a social network.

#index 1602033
#* MAP-JOIN-REDUCE: Toward Scalable and Efficient Data Analysis on Large Clusters
#@ David Jiang;Anthony K.  H. Tung;Gang Chen
#t 2011
#c 7
#! Data analysis is an important functionality in cloud computing which allows a huge amount of data to be processed over very large clusters. MapReduce is recognized as a popular way to handle data in the cloud environment due to its excellent scalability and good fault tolerance. However, compared to parallel databases, the performance of MapReduce is slower when it is adopted to perform complex data analysis tasks that require the joining of multiple data sets in order to compute certain aggregates. A common concern is whether MapReduce can be improved to produce a system with both scalability and efficiency. In this paper, we introduce Map-Join-Reduce, a system that extends and improves MapReduce runtime framework to efficiently process complex data analysis tasks on large clusters. We first propose a filtering-join-aggregation programming model, a natural extension of MapReduce's filtering-aggregation programming model. Then, we present a new data processing strategy which performs filtering-join-aggregation tasks in two successive MapReduce jobs. The first job applies filtering logic to all the data sets in parallel, joins the qualified tuples, and pushes the join results to the reducers for partial aggregation. The second job combines all partial aggregation results and produces the final answer. The advantage of our approach is that we join multiple data sets in one go and thus avoid frequent checkpointing and shuffling of intermediate results, a major performance bottleneck in most of the current MapReduce-based systems. We benchmark our system against Hive, a state-of-the-art MapReduce-based data warehouse on a 100-node cluster on Amazon EC2 using TPC-H benchmark. The results show that our approach significantly boosts the performance of complex analysis queries.

#index 1602034
#* Heuristics-Based Query Processing for Large RDF Graphs Using Cloud Computing
#@ Mohammad Husain;James McGlothlin;Mohammad M. Masud;Latifur Khan;Bhavani M. Thuraisingham
#t 2011
#c 7
#! Semantic web is an emerging area to augment human reasoning. Various technologies are being developed in this arena which have been standardized by the World Wide Web Consortium (W3C). One such standard is the Resource Description Framework (RDF). Semantic web technologies can be utilized to build efficient and scalable systems for Cloud Computing. With the explosion of semantic web technologies, large RDF graphs are common place. This poses significant challenges for the storage and retrieval of RDF graphs. Current frameworks do not scale for large RDF graphs and as a result do not address these challenges. In this paper, we describe a framework that we built using Hadoop to store and retrieve large numbers of RDF triples by exploiting the cloud computing paradigm. We describe a scheme to store RDF data in Hadoop Distributed File System. More than one Hadoop job (the smallest unit of execution in Hadoop) may be needed to answer a query because a single triple pattern in a query cannot simultaneously take part in more than one join in a single Hadoop job. To determine the jobs, we present an algorithm to generate query plan, whose worst case cost is bounded, based on a greedy approach to answer a SPARQL Protocol and RDF Query Language (SPARQL) query. We use Hadoop's MapReduce framework to answer the queries. Our results show that we can store large RDF graphs in Hadoop clusters built with cheap commodity class hardware. Furthermore, we show that our framework is scalable and efficient and can handle large amounts of RDF data, unlike traditional approaches.

#index 1602035
#* State Monitoring in Cloud Datacenters
#@ Shicong Meng;Ling Liu;Ting Wang
#t 2011
#c 7
#! Monitoring global states of a distributed cloud application is a critical functionality for cloud datacenter management. State monitoring requires meeting two demanding objectives: high level of correctness, which ensures zero or low error rate, and high communication efficiency, which demands minimal communication cost in detecting state updates. Most existing work follows an instantaneous model which triggers state alerts whenever a constraint is violated. This model may cause frequent and unnecessary alerts due to momentary value bursts and outliers. Countermeasures of such alerts may further cause problematic operations. In this paper, we present a WIndow-based StatE monitoring (WISE) framework for efficiently managing cloud applications. Window-based state monitoring reports alerts only when state violation is continuous within a time window. We show that it is not only more resilient to value bursts and outliers, but also able to save considerable communication when implemented in a distributed manner based on four technical contributions. First, we present the architectural design and deployment options for window-based state monitoring with centralized parameter tuning. Second, we develop a new distributed parameter tuning scheme enabling WISE to scale to much more monitoring nodes as each node tunes its monitoring parameters reactively without global information. Third, we introduce two optimization techniques, including their design rationale, correctness and usage model, to further reduce the communication cost. Finally, we provide an in-depth empirical study of the scalability of WISE, and evaluate the improvement brought by the distributed tuning scheme and the two performance optimizations. Our results show that WISE reduces communication by 50-90 percent compared with instantaneous monitoring approaches, and the improved WISE gains a clear scalability advantage over its centralized version.

#index 1602036
#* Optimal Service Pricing for a Cloud Cache
#@ Verena Kantere;Debabrata Dash;Gregory Francois;Sofia Kyriakopoulou;Anastasia Ailamaki
#t 2011
#c 7
#! Cloud applications that offer data management services are emerging. Such clouds support caching of data in order to provide quality query services. The users can query the cloud data, paying the price for the infrastructure they use. Cloud management necessitates an economy that manages the service of multiple users in an efficient, but also, resource-economic way that allows for cloud profit. Naturally, the maximization of cloud profit given some guarantees for user satisfaction presumes an appropriate price-demand model that enables optimal pricing of query services. The model should be plausible in that it reflects the correlation of cache structures involved in the queries. Optimal pricing is achieved based on a dynamic pricing scheme that adapts to time changes. This paper proposes a novel price-demand model designed for a cloud cache and a dynamic pricing scheme for queries executed in the cloud cache. The pricing solution employs a novel method that estimates the correlations of the cache services in an time-efficient manner. The experimental study shows the efficiency of the solution.

#index 1602037
#* A Pattern Mining Approach to Sensor-Based Human Activity Recognition
#@ Tao Gu;Liang Wang;Zhanqing Wu;Xianping Tao;Jian Lu
#t 2011
#c 7
#! Recognizing human activities from sensor readings has recently attracted much research interest in pervasive computing due to its potential in many applications, such as assistive living and healthcare. This task is particularly challenging because human activities are often performed in not only a simple (i.e., sequential), but also a complex (i.e., interleaved or concurrent) manner in real life. Little work has been done in addressing complex issues in such a situation. The existing models of interleaved and concurrent activities are typically learning-based. Such models lack of flexibility in real life because activities can be interleaved and performed concurrently in many different ways. In this paper, we propose a novel pattern mining approach to recognize sequential, interleaved, and concurrent activities in a unified framework. We exploit Emerging Pattern—a discriminative pattern that describes significant changes between classes of data—to identify sensor features for classifying activities. Different from existing learning-based approaches which require different training data sets for building activity models, our activity models are built upon the sequential activity trace only and can be applied to recognize both simple and complex activities. We conduct our empirical studies by collecting real-world traces, evaluating the performance of our algorithm, and comparing our algorithm with static and temporal models. Our results demonstrate that, with a time slice of 15 seconds, we achieve an accuracy of 90.96 percent for sequential activity, 88.1 percent for interleaved activity, and 82.53 percent for concurrent activity.

#index 1602038
#* A Semantics-Based Approach for Speech Annotation of Images
#@ Dmitri Kalashnikov;Sharad Mehrotra;Jie Xu;Nalini Venkatasubramanian
#t 2011
#c 7
#! Associating textual annotations/tags with multimedia content is among the most effective approaches to organize and to support search over digital images and multimedia databases. Despite advances in multimedia analysis, effective tagging remains largely a manual process wherein users add descriptive tags by hand, usually when uploading or browsing the collection, much after the pictures have been taken. This approach, however, is not convenient in all situations or for many applications, e.g., when users would like to publish and share pictures with others in real time. An alternate approach is to instead utilize a speech interface using which users may specify image tags that can be transcribed into textual annotations by employing automated speech recognizers. Such a speech-based approach has all the benefits of human tagging without the cumbersomeness and impracticality typically associated with human tagging in real time. The key challenge in such an approach is the potential low recognition quality of the state-of-the-art recognizers, especially, in noisy environments. In this paper, we explore how semantic knowledge in the form of co-occurrence between image tags can be exploited to boost the quality of speech recognition. We postulate the problem of speech annotation as that of disambiguating among multiple alternatives offered by the recognizer. An empirical evaluation has been conducted over both real speech recognizer's output as well as synthetic data sets. The results demonstrate significant advantages of the proposed approach compared to the recognizer's output under varying conditions.

#index 1633079
#* Estimating the Helpfulness and Economic Impact of Product Reviews: Mining Text and Reviewer Characteristics
#@ Anindya Ghose;Panagiotis G. Ipeirotis
#t 2011
#c 7
#! With the rapid growth of the Internet, the ability of users to create and publish content has created active electronic communities that provide a wealth of product information. However, the high volume of reviews that are typically published for a single product makes harder for individuals as well as manufacturers to locate the best reviews and understand the true underlying quality of a product. In this paper, we reexamine the impact of reviews on economic outcomes like product sales and see how different factors affect social outcomes such as their perceived usefulness. Our approach explores multiple aspects of review text, such as subjectivity levels, various measures of readability and extent of spelling errors to identify important text-based features. In addition, we also examine multiple reviewer-level features such as average usefulness of past reviews and the self-disclosed identity measures of reviewers that are displayed next to a review. Our econometric analysis reveals that the extent of subjectivity, informativeness, readability, and linguistic correctness in reviews matters in influencing sales and perceived usefulness. Reviews that have a mixture of objective, and highly subjective sentences are negatively associated with product sales, compared to reviews that tend to include only subjective or only objective information. However, such reviews are rated more informative (or helpful) by other users. By using Random Forest-based classifiers, we show that we can accurately predict the impact of reviews on sales and their perceived usefulness. We examine the relative importance of the three broad feature categories: “reviewer-related” features, “review subjectivity” features, and “review readability” features, and find that using any of the three feature sets results in a statistically equivalent performance as in the case of using all available features. This paper is the first study that integrates econometric, text mining, and predictive modeling techniques toward a more complete analysis of the information captured by user-generated online reviews in order to estimate their helpfulness and economic impact.

#index 1633080
#* Exact Top-K Queries in Wireless Sensor Networks
#@ Baljeet Malhotra;Mario A. Nascimento;Ioanis Nikolaidis
#t 2011
#c 7
#! In this paper, we consider the exact top-k query problem in wireless sensor networks, i.e., where one seeks to find the k highest reported values as well as the complete set of nodes that reported them. Our primary contribution in this context is EXTOK, a provably correct and topology-independent new filtering-based algorithm for processing exact top-k queries. As a secondary contribution we confirm a previous result of ours by showing that the efficiency of top-k query processing algorithms, including EXTOK, can be further improved by simply choosing a proper underlying logical tree topology. We examine EXTOK's performance with respect to a number of parameters and different logical tree topologies while using both synthetic and real data sets. Our simulation reveal that EXTOK consistently outperforms the current state-of-the-art algorithm by a very significant margin and regardless of the underlying logical tree topology.

#index 1633081
#* Group Enclosing Queries
#@ Feifei Li;Bin Yao;Piyush Kumar
#t 2011
#c 7
#! Given a set of points P and a query set Q, a group enclosing query (Geq) fetches the point p* â聢聢 P such that the maximum distance of p* to all points in Q is minimized. This problem is equivalent to the Min-Max case (minimizing the maximum distance) of aggregate nearest neighbor queries for spatial databases [27]. This work first designs a new exact solution by exploring new geometric insights, such as the minimum enclosing ball, the convex hull, and the furthest voronoi diagram of the query group. To further reduce the query cost, especially when the dimensionality increases, we turn to approximation algorithms. Our main approximation algorithm has a worst case \sqrt{2}-approximation ratio if one can find the exact nearest neighbor of a point. In practice, its approximation ratio never exceeds 1.05 for a large number of data sets up to six dimensions. We also discuss how to extend it to higher dimensions (up to 74 in our experiment) and show that it still maintains a very good approximation quality (still close to 1) and low query cost. In fixed dimensions, we extend the \sqrt{2}-approximation algorithm to get a (1 + Îµ)-approximate solution for the Geq problem. Both approximation algorithms have O(\log N + M) query cost in any fixed dimension, where N and M are the sizes of the data set P and query group Q. Extensive experiments on both synthetic and real data sets, up to 10 million points and 74 dimensions, confirm the efficiency, effectiveness, and scalability of the proposed algorithms, especially their significant improvement over the state-of-the-art method.

#index 1633082
#* Optimal Symbol Alignment Distance: A New Distance for Sequences of Symbols
#@ Javier Herranz;Jordi Nin;Marc Sole
#t 2011
#c 7
#! Comparison functions for sequences (of symbols) are important components of many applications, for example, clustering, data cleansing, and integration. For years, many efforts have been made to improve the performance of such comparison functions. Improvements have been done either at the cost of reducing the accuracy of the comparison, or by compromising certain basic characteristics of the functions, such as the triangular inequality. In this paper, we propose a new distance for sequences of symbols (or strings) called Optimal Symbol Alignment distance (OSA distance, for short). This distance has a very low cost in practice, which makes it a suitable candidate for computing distances in applications with large amounts of (very long) sequences. After providing a mathematical proof that the OSA distance is a real distance, we present some experiments for different scenarios (DNA sequences, record linkage, etc.), showing that the proposed distance outperforms, in terms of execution time and/or accuracy, other well-known comparison functions such as the Edit or Jaro-Winkler distances.

#index 1633083
#* Relevance-Based Retrieval on Hidden-Web Text Databases without Ranking Support
#@ Vagelis Hristidis;Yuheng Hu;Panagiotis Ipeirotis
#t 2011
#c 7
#! Many online or local data sources provide powerful querying mechanisms but limited ranking capabilities. For instance, PubMed allows users to submit highly expressive Boolean keyword queries, but ranks the query results by date only. However, a user would typically prefer a ranking by relevance, measured by an information retrieval (IR) ranking function. A naive approach would be to submit a disjunctive query with all query keywords, retrieve all the returned matching documents, and then rerank them. Unfortunately, such an operation would be very expensive due to the large number of results returned by disjunctive queries. In this paper, we present algorithms that return the top results for a query, ranked according to an IR-style ranking function, while operating on top of a source with a Boolean query interface with no ranking capabilities (or a ranking capability of no interest to the end user). The algorithms generate a series of conjunctive queries that return only documents that are candidates for being highly ranked according to a relevance metric. Our approach can also be applied to other settings where the ranking is monotonic on a set of factors (query keywords in IR) and the source query interface is a Boolean expression of these factors. Our comprehensive experimental evaluation on the PubMed database and a TREC data set show that we achieve order of magnitude improvement compared to the current baseline approaches.

#index 1633084
#* Static and Dynamic Delegation in the Role Graph Model
#@ He Wang;Sylvia Osborn
#t 2011
#c 7
#! Delegation in access control is used to deal with exceptional circumstances, when a regular user is unable to perform their normal job and delegates all or part of it to others. These situations can be anticipated and built into the security design as static delegation; however, unforseen circumstances can still occur requiring dynamic delegation to be specified at runtime. This paper presents both static and dynamic delegation in the context of the Role Graph Model. To properly capture runtime events, we add sessions to the RGM. We then introduce session-oriented, dynamic delegation, a new concept in RBAC models, using an edge-labeling method. Constraints applicable to both static and dynamic delegation are examined.

#index 1633085
#* XPath Query Relaxation through Rewriting Rules
#@ fdgdfg dfgdfg;Sergio Flesca;Filippo Furfaro
#t 2011
#c 7
#! Query relaxation is the process of weakening a query to a more general one, and it is frequently employed to support approximate query answering. In this paper, rewriting systems for a wide fragment of XPath are investigated, which accomplish query relaxation through the application of simple rewriting rules transforming navigational axes and node tests into relaxed ones. Specifically, a general yet simple form of rewriting rules is considered, which subsumes the forms adopted in several rewriting systems for approximate XPath query answering. The expressiveness of rewriting systems based on this form of rules is characterized in terms of their capability of transforming a query into every more general formulation. It is shown that traditional rewriting systems are not only incomplete w.r.t. containment, but also w.r.t. the stricter form known as containment by homomorphism. This limitation is overcome by defining a set {\cal R}^{\ast} of rewriting rules which are still of the same simple form of traditional ones, but are expressive enough to catch at least containment by homomorphism. Then, an algorithm is proposed which exploits {\cal R}^{\ast} to provide approximate answers of queries along with a measure of their approximation degree.

#index 1633086
#* A Cumulative Belief Degree-Based Approach for Missing Values in Nuclear Safeguards Evaluation
#@ Ozgur Kabak;Da Ruan
#t 2011
#c 7
#! Nuclear safeguards are a set of activities to verify that a State is living up to its international undertakings not to use nuclear programs for nuclear weapons purposes. Nuclear safeguards experts of International Atomic Energy Agency (IAEA) evaluate indicators by benefitting from several sources such as State declarations, on-site inspections, the IAEA databases, and other open sources. The IAEA expert evaluations are aggregated to make a final decision, which usually incomplete because of over 900 indicators, lack of expertise, and unavailability of information sources. In this study, a cumulative belief degree approach is introduced based on the belief structure. It is used to aggregate the incomplete expert evaluations that are represented with fuzzy linguistic terms. Moreover, a reliability index is employed to find the trustworthiness of the final result depending on the available evaluations. A numerical example illustrates the applicability of the proposed methodology.

#index 1633087
#* Alpha-Level Aggregation: A Practical Approach to Type-1 OWA Operation for Aggregating Uncertain Information with Applications to Breast Cancer Treatments
#@ Shang-Ming Zhou;Francisco Chiclana;Robert I. John;Jonathan M. Garibaldi
#t 2011
#c 7
#! Type-1 Ordered Weighted Averaging (OWA) operator provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, in which uncertain objects are modeled by fuzzy sets. The Direct Approach to performing type-1 OWA operation involves high computational overhead. In this paper, we define a type-1 OWA operator based on the \alpha-cuts of fuzzy sets. Then, we prove a Representation Theorem of type-1 OWA operators, by which type-1 OWA operators can be decomposed into a series of \alpha-level type-1 OWA operators. Furthermore, we suggest a fast approach, called Alpha-Level Approach, to implementing the type-1 OWA operator. A practical application of type-1 OWA operators to breast cancer treatments is addressed. Experimental results and theoretical analyses show that: 1) the Alpha-Level Approach with linear order complexity can achieve much higher computing efficiency in performing type-1 OWA operation than the existing Direct Approach, 2) the type-1 OWA operators exhibit different aggregation behaviors from the existing fuzzy weighted averaging (FWA) operators, and 3) the type-1 OWA operators demonstrate the ability to efficiently aggregate uncertain information with uncertain weights in solving real-world soft decision-making problems.

#index 1633088
#* Efficient Evaluation of Continuous Text Search Queries
#@ Kyriakos Mouratidis;HweeHwa Pang
#t 2011
#c 7
#! Consider a text filtering server that monitors a stream of incoming documents for a set of users, who register their interests in the form of continuous text search queries. The task of the server is to constantly maintain for each query a ranked result list, comprising the recent documents (drawn from a sliding window) with the highest similarity to the query. Such a system underlies many text monitoring applications that need to cope with heavy document traffic, such as news and email monitoring. In this paper, we propose the first solution for processing continuous text queries efficiently. Our objective is to support a large number of user queries while sustaining high document arrival rates. Our solution indexes the streamed documents in main memory with a structure based on the principles of the inverted file, and processes document arrival and expiration events with an incremental threshold-based method. We distinguish between two versions of the monitoring algorithm, an eager and a lazy one, which differ in how aggressively they manage the thresholds on the inverted index. Using benchmark queries over a stream of real documents, we experimentally verify the efficiency of our methodology; both its versions are at least an order of magnitude faster than a competitor constructed from existing techniques, with lazy being the best approach overall.

#index 1633089
#* Efficient Hidden Vector Encryption for Conjunctive Queries on Encrypted Data
#@ Jong Hwan Park
#t 2011
#c 7
#! Predicate encryption has received considerable attention in applications where private and sensitive data about users can be stored in untrusted database (DB) servers. It allows users to store encrypted data at DB servers, and yet retain the ability to search those databases without revealing anything else about the encrypted data. Hidden Vector Encryption (HVE) is a type of predicate encryption that supports the fine-grained conjunctive combination of equality queries, comparison queries, and subset queries on encrypted data. The currently known HVE schemes, which are all pairing-based, either work in composite-order groups or require a token size of O(\ell ) and O(\ell ) pairing computations for one search query with \ell conjuncts. In this paper, we present a new HVE scheme that not only works in prime-order groups but also requires a token size of O(1) and only O(1) pairing computations regardless of \ell. Our HVE construction also yields a more efficient, anonymous, identity-based encryption scheme than existing schemes, which is secure in the standard model. To achieve our goal, we introduce novel techniques for both hiding attributes in prime-order groups and reducing the number of pairing computations to O(1). Our techniques are quite general so that they can be applied to both symmetric and asymmetric bilinear maps.

#index 1633137
#* What is Unequal among the Equals? Ranking Equivalent Rules from Gene Expression Data
#@ Ruichu Cai;Anthony K.  H. Tung;Zhenjie Zhang;Zhifeng Hao
#t 2011
#c 7
#! In previous studies, association rules have been proven to be useful in classification problems over high dimensional gene expression data. However, due to the nature of such data sets, it is often the case that millions of rules can be derived such that many of them are covered by exactly the same set of training tuples and thus have exactly the same support and confidence. Ranking and selecting useful rules from such equivalent rule groups remain an interesting and unexplored problem. In this paper, we look at two interestingness measures for ranking the interestingness of rules within equivalent rule group: Max-Subrule-Conf and Min-Subrule-Conf. Based on these interestingness measures, an incremental Apriori-like algorithm is designed to select more interesting rules from the lower bound rules of the group. Moreover, we present an improved classification model to fully exploit the potential of the selected rules. Our empirical studies on our proposed methods over five gene expression data sets show that our proposals improve both the efficiency and effectiveness of the rule extraction and classifier construction over gene expression data sets.

#index 1633138
#* Approximate Aggregations in Structured P2P Networks
#@ Dalie Sun;Sai Wu;Shouxu Jiang;Jianzhong Li
#t 2011
#c 7
#! In corporate networks, daily business data are generated in gigabytes or even terabytes. It is costly to process aggregate queries in those systems. In this paper, we propose PACA, a probably approximately correct aggregate query processing scheme, for answering aggregate queries in structured Peer-to-Peer (P2P) network. PACA retrieves random samples from peers' databases and applies the samples to process queries. Instead of scanning the entire database of each peer, PACA only accesses a small random number of data. Moreover, based on the query distribution,PACA publishes a precomputed synopsis and uses the synopsis to answer future queries. Most queries are expected to be answered by the precomputed synopsis partially or fully. And the synopsis is adaptively tuned to follow the query distribution. Experiments on the PlanetLab show the effectiveness of the approach.

#index 1633139
#* Supporting Pattern-Matching Queries over Trajectories on Road Networks
#@ Gook-Pil Roh;Jong-Won Roh;Seung-Won hwang;Byoung-Kee Yi
#t 2011
#c 7
#! With the advent of ubiquitous computing, we can easily collect large-scale trajectory data, say, from moving vehicles. This paper studies pattern-matching problems for trajectory data over road networks, which complements existing efforts focusing on 1) a spatiotemporal window query for location-based service or 2) euclidean space with no restriction. In contrast, we first identify some desirable properties for pattern-matching queries to the road network trajectories. As the existing work does not fully satisfy these properties, we develop 1) trajectory representation and 2) distance metric that satisfy all the desirable properties we identified. Based on this representation and metric, we develop efficient algorithms for three types of pattern-matching queries—whole, subpattern, and reverse subpattern matching. We analytically validate the correctness of our algorithms and also empirically validate their scalability over large-scale, real-life, and synthetic trajectory data sets.

#index 1633140
#* A Survey on Graphical Methods for Classification Predictive Performance Evaluation
#@ Ronaldo C. Prati;Gustavo E.  A.  P.  A. Batista;Maria Carolina Monard
#t 2011
#c 7
#! Predictive performance evaluation is a fundamental issue in design, development, and deployment of classification systems. As predictive performance evaluation is a multidimensional problem, single scalar summaries such as error rate, although quite convenient due to its simplicity, can seldom evaluate all the aspects that a complete and reliable evaluation must consider. Due to this, various graphical performance evaluation methods are increasingly drawing the attention of machine learning, data mining, and pattern recognition communities. The main advantage of these types of methods resides in their ability to depict the trade-offs between evaluation aspects in a multidimensional space rather than reducing these aspects to an arbitrarily chosen (and often biased) single scalar measure. Furthermore, to appropriately select a suitable graphical method for a given task, it is crucial to identify its strengths and weaknesses. This paper surveys various graphical methods often used for predictive performance evaluation. By presenting these methods in the same framework, we hope this paper may shed some light on deciding which methods are more suitable to use in different situations.

#index 1633141
#* Energy Efficient Data Access in Mobile P2P Networks
#@ Kwangjin Park;Patrick Valduriez
#t 2011
#c 7
#! A fundamental problem for peer-to-peer (P2P) applications in mobile-pervasive computing environment is to efficiently identify the node that stores particular data items and download them while preserving battery power. In this paper, we propose a P2P Minimum Boundary Rectangle (PMBR, for short) which is a new spatial index specifically designed for mobile P2P environments. A node that contains desirable data item (s) can be easily identified by reading the PMBR index. Then, we propose a selective tuning algorithm, called Distributed exponential Sequence Scheme (DSS, for short), that provides clients with the ability of selective tuning of data items, thus preserving the scarce power resource. The proposed algorithm is simple but efficient in supporting linear transmission of spatial data and processing of location-aware queries. The results from theoretical analysis and experiments show that the proposed algorithm with the PMBR index is scalable and energy efficient in both range queries and nearest neighbor queries.

#index 1633142
#* Gold Standard Evaluation of Ontology Learning Methods through Ontology Transformation and Alignment
#@ Elias Zavitsanos;George Paliouras;George A. Vouros
#t 2011
#c 7
#! This paper presents a method along with a set of measures for evaluating learned ontologies against gold ontologies. The proposed method transforms the ontology concepts and their properties into a vector space representation to avoid the common string matching of concepts and properties at the lexical layer. The proposed evaluation measures exploit the vector space representation and calculate the similarity of the two ontologies (learned and gold) at the lexical and relational levels. Extensive evaluation experiments are provided, which show that these measures capture accurately the deviations from the gold ontology. The proposed method is tested using the Genia and the Lonely Planet gold ontologies, as well as the ontologies in the benchmark series of the Ontology Alignment Evaluation Initiative.

#index 1633143
#* Kernelized Fuzzy Rough Sets and Their Applications
#@ Qinghua Hu;Daren Yu;Witold Pedrycz;Degang Chen
#t 2011
#c 7
#! Kernel machines and rough sets are two classes of commonly exploited learning techniques. Kernel machines enhance traditional learning algorithms by bringing opportunities to deal with nonlinear classification problems, rough sets introduce a human-focused way to deal with uncertainty in learning problems. Granulation and approximation play a pivotal role in rough sets-based learning and reasoning. However, a way how to effectively generate fuzzy granules from data has not been fully studied so far. In this study, we integrate kernel functions with fuzzy rough set models and propose two types of kernelized fuzzy rough sets. Kernel functions are employed to compute the fuzzy T-equivalence relations between samples, thus generating fuzzy information granules in the approximation space. Subsequently fuzzy granules are used to approximate the classification based on the concepts of fuzzy lower and upper approximations. Based on the models of kernelized fuzzy rough sets, we extend the measures existing in classical rough sets to evaluate the approximation quality and approximation abilities of the attributes. We discuss the relationship between these measures and feature evaluation function ReliefF, and augment the ReliefF algorithm to enhance the robustness of these proposed measures. Finally, we apply these measures to evaluate and select features for classification problems. The experimental results help quantify the performance of the KFRS.

#index 1633144
#* Improving Classifier Performance Using Data with Different Taxonomies
#@ Tomoharu Iwata;Toshiyuki Tanaka;Takeshi Yamada;Naonori Ueda
#t 2011
#c 7
#! We propose a framework for improving classifier performance by effectively using auxiliary samples. The auxiliary samples are labeled not in terms of the target taxonomy according to which we wish to classify samples, but according to classification schemes or taxonomies that are different from the target taxonomy. Our method finds a classifier by minimizing a weighted error over the target and auxiliary samples. The weights are defined so that the weighted error approximates the expected error when samples are classified into the target taxonomy. Experiments using synthetic and text data show that our method significantly improves the classifier performance in most cases compared to conventional data augmentation methods.

#index 1633145
#* k-Anonymization in the Presence of Publisher Preferences
#@ Rinku Dewri;Indrajit Ray;Indrakshi Ray;Darrell Whitley
#t 2011
#c 7
#! Privacy constraints are typically enforced on shared data that contain sensitive personal attributes. However, owing to its adverse effect on the utility of the data, information loss must be minimized while sanitizing the data. Existing methods for this purpose modify the data only to the extent necessary to satisfy the privacy constraints, thereby asserting that the information loss has been minimized. However, given the subjective nature of information loss, it is often difficult to justify such an assertion. In this paper, we propose an interactive procedure to generate a data generalization scheme that optimally meets the preferences of the data publisher. A data publisher guides the sanitization process by specifying aspirations in terms of desired achievement levels in the objectives. A reference direction based methodology is used to investigate neighborhood solutions if the generated scheme is not acceptable. This approach draws its power from the constructive input received from the publisher about the suitability of a solution before finding a new one.

#index 1633146
#* Multiresolution Web Link Analysis Using Generalized Link Relations
#@ Laurence A. F. Park;Ramamohanarao Kotagiri
#t 2011
#c 7
#! Web link analysis methods such as PageRank, HITS, and SALSA have focused on obtaining global popularity or authority of the set of Web pages in question. Although global popularity is useful for general queries, we find that global popularity is not as useful for queries in which the global population has less knowledge of. By examining the many different communities that appear within a Web page graph, we are able to compute the popularity or authority from a specific community. Multiresolution popularity lists allow us to observe the popularity of Web pages with respect to communities at different resolutions within the Web. Multiresolution popularity lists have been shown to have high potential when compared against PageRank. In this paper, we generalize the multiresolution popularity analysis to use any form of Web page link relations. We provide results for both the PageRank relations and the In-degree relations. By utilizing the multiresolution popularity lists, we achieve a 13 percent and 25 percent improvement in mean average precision over In-degree and PageRank, respectively.

#index 1633147
#* On the Design and Analysis of the Privacy-Preserving SVM Classifier
#@ Keng-Pei Lin;Ming-Syan Chen
#t 2011
#c 7
#! The support vector machine (SVM) is a widely used tool in classification problems. The SVM trains a classifier by solving an optimization problem to decide which instances of the training data set are support vectors, which are the necessarily informative instances to form the SVM classifier. Since support vectors are intact tuples taken from the training data set, releasing the SVM classifier for public use or shipping the SVM classifier to clients will disclose the private content of support vectors. This violates the privacy-preserving requirements for some legal or commercial reasons. The problem is that the classifier learned by the SVM inherently violates the privacy. This privacy violation problem will restrict the applicability of the SVM. To the best of our knowledge, there has not been work extending the notion of privacy preservation to tackle this inherent privacy violation problem of the SVM classifier. In this paper, we exploit this privacy violation problem, and propose an approach to postprocess the SVM classifier to transform it to a privacy-preserving classifier which does not disclose the private content of support vectors. The postprocessed SVM classifier without exposing the private content of training data is called Privacy-Preserving SVM Classifier (abbreviated as PPSVC). The PPSVC is designed for the commonly used Gaussian kernel function. It precisely approximates the decision function of the Gaussian kernel SVM classifier without exposing the sensitive attribute values possessed by support vectors. By applying the PPSVC, the SVM classifier is able to be publicly released while preserving privacy. We prove that the PPSVC is robust against adversarial attacks. The experiments on real data sets show that the classification accuracy of the PPSVC is comparable to the original SVM classifier.

#index 1633148
#* Similarity Join Processing on Uncertain Data Streams
#@ Xiang Lian;Lei Chen
#t 2011
#c 7
#! Similarity join processing in the streaming environment has many practical applications such as sensor networks, object tracking and monitoring, and so on. Previous works usually assume that stream processing is conducted over precise data. In this paper, we study an important problem of similarity join processing on stream data that inherently contain uncertainty (or called uncertain data streams), where the incoming data at each time stamp are uncertain and imprecise. Specifically, we formalize this problem as join on uncertain data streams (USJ), which can guarantee the accuracy of USJ answers over uncertain data. To tackle the challenges with respect to efficiency and effectiveness such as limited memory and small response time, we propose effective pruning methods on both object and sample levels to filter out false alarms. We integrate the proposed pruning methods into an efficient query procedure that can incrementally maintain the USJ answers. Most importantly, we further design a novel strategy, namely, adaptive superset prejoin (ASP), to maintain a superset of USJ candidate pairs. ASP is in light of our proposed formal cost model such that the average USJ processing cost is minimized. We have conducted extensive experiments to demonstrate the efficiency and effectiveness of our proposed approaches.

#index 1646319
#* Guest Editors Introduction: Special Section on Keyword Search on Structured Data
#@ Surajit Chaudhuri;Yi Chen;Jeffrey Xu Yu
#t 2011
#c 7

#index 1646320
#* SPARK2: Top-k Keyword Query in Relational Databases
#@ Yi Luo;Wei Wang;Xuemin Lin;Xiaofang Zhou;Jianmin Wang;Kequi Li
#t 2011
#c 7
#! With the increasing amount of text data stored in relational databases, there is a demand for RDBMS to support keyword queries over text data. As a search result is often assembled from multiple relational tables, traditional IR-style ranking and query evaluation methods cannot be applied directly. In this paper, we study the effectiveness and the efficiency issues of answering top-k keyword query in relational database systems. We propose a new ranking formula by adapting existing IR techniques based on a natural notion of virtual document. We also propose several efficient query processing methods for the new ranking method. We have conducted extensive experiments on large-scale real databases using two popular RDBMSs. The experimental results demonstrate significant improvement to the alternative approaches in terms of retrieval effectiveness and efficiency.

#index 1646321
#* Finding Top-k Answers in Keyword Search over Relational Databases Using Tuple Units
#@ Jianhua Feng;Guoliang Li;Jianyong Wang
#t 2011
#c 7
#! Existing studies on keyword search over relational databases usually find Steiner trees composed of connected database tuples as answers. They on-the-fly identify Steiner trees by discovering rich structural relationships between database tuples, and neglect the fact that such structural relationships can be precomputed and indexed. Recently, tuple units are proposed to improve search efficiency by indexing structural relationships, and existing methods identify a single tuple unit to answer keyword queries. However, in many cases, multiple tuple units should be integrated to answer a keyword query. Thus, these methods will involve false negatives. To address this problem, in this paper, we study how to integrate multiple related tuple units to effectively answer keyword queries. To achieve a high performance, we devise two novel indexes, single-keyword-based structure-aware index and keyword-pair-based structure-aware index, and incorporate structural relationships between different tuple units into the indexes. We use the indexes to efficiently identify the answers of integrated tuple units. We develop new ranking techniques and algorithms to progressively find the top-k answers. We have implemented our method in real database systems, and the experimental results show that our approach achieves high search efficiency and result quality, and outperforms state-of-the-art methods significantly.

#index 1646322
#* Efficient Keyword-Based Search for Top-K Cells in Text Cube
#@ Bolin Ding;Bo Zhao;Cindy Xide Lin;Jiawei Han;Chengxiang Zhai;Asok Srivastava;Nikunj C. Oza
#t 2011
#c 7
#! Previous studies on supporting free-form keyword queries over RDBMSs provide users with linked structures (e.g., a set of joined tuples) that are relevant to a given keyword query. Most of them focus on ranking individual tuples from one table or joins of multiple tables containing a set of keywords. In this paper, we study the problem of keyword search in a data cube with text-rich dimension(s) (so-called text cube). The text cube is built on a multidimensional text database, where each row is associated with some text data (a document) and other structural dimensions (attributes). A cell in the text cube aggregates a set of documents with matching attribute values in a subset of dimensions. We define a keyword-based query language and an IR-style relevance model for scoring/ranking cells in the text cube. Given a keyword query, our goal is to find the top-k most relevant cells. We propose four approaches: inverted-index one-scan, document sorted-scan, bottom-up dynamic programming, and search-space ordering. The search-space ordering algorithm explores only a small portion of the text cube for finding the top-k answers, and enables early termination. Extensive experimental studies are conducted to verify the effectiveness and efficiency of the proposed approaches.

#index 1646323
#* Returning Clustered Results for Keyword Search on XML Documents
#@ Xiping Liu;Changxuan Wan;Lei Chen
#t 2011
#c 7
#! Keyword search is an effective paradigm for information discovery and has been introduced recently to query XML documents. In this paper, we address the problem of returning clustered results for keyword search on XML documents. We first propose a novel semantics for answers to an XML keyword query. The core of the semantics is the conceptually related relationship between keyword matches, which is based on the conceptual relationship between nodes in XML trees. Then, we propose a new clustering methodology for XML search results, which clusters results according to the way they match the given query. Two approaches to implement the methodology are discussed. The first approach is a conventional one which does clustering after search results are retrieved; the second one clusters search results actively, which has characteristics of clustering on the fly. The generated clusters are then organized into a cluster hierarchy with different granularities to enable users locate the results of interest easily and precisely. Experimental results demonstrate the meaningfulness of the proposed semantics as well as the efficiency of the proposed methods.

#index 1646324
#* A Fuzzy Logic Approach to Wrapping PDF Documents
#@ Sergio Flesca;Elio Masciari;Andrea Tagarelli
#t 2011
#c 7
#! The PDF format represents the de facto standard for print-oriented documents. In this paper, we address the problem of wrapping PDF documents, which raises new challenges in several contexts of text data management. Our proposal is based on a novel bottom-up hierarchical wrapping approach that exploits fuzzy logic to handle the “uncertainty” which is intrinsic to the structure and presentation of PDF documents. A PDF wrapper is defined by specifying a set of group type definitions that impose a target structure to groups of tokens containing the required information. Constraints on token groupings are formulated as fuzzy conditions, which are defined on spatial and content predicates of tokens. We define a formal semantics for PDF wrappers and propose an algorithm for wrapper evaluation working in polynomial time with respect to the size of a PDF document. The proposed approach has been implemented in a wrapper generation system that offers visual capabilities to assist the designer in specifying and evaluating a PDF wrapper. Experimental results have shown good accuracy and applicability of our system to PDF documents of various domains.

#index 1646325
#* A Spectrum-Based Framework for Quantifying Randomness of Social Networks
#@ Xiaowei Ying;Leting Wu;Xintao Wu
#t 2011
#c 7
#! Social networks tend to contain some amount of randomness and some amount of nonrandomness. The amount of randomness versus nonrandomness affects the properties of a social network. In this paper, we theoretically analyze graph randomness and present a framework which provides a series of nonrandomness measures at levels of edge, node, subgraph, and the overall graph. We show that graph nonrandomness can be obtained mathematically from the spectra of the adjacency matrix of the network. We derive the upper bound and lower bound of nonrandomness value of the overall graph. We investigate whether other graph spectra (such as Laplacian and normal spectra) could also be used to derive a nonrandomness framework. Our theoretical results showed that they are unlikely, if not impossible, to have a consistent framework to evaluate randomness. We also compare our proposed nonrandomness measures with some traditional measures such as modularity. Our theoretical and empirical studies show our proposed nonrandomness measures can characterize and capture graph randomness.

#index 1646326
#* Anomalous Window Discovery for Linear Intersecting Paths
#@ Lei Shi;Vandana P. Janeja
#t 2011
#c 7
#! The focus of this paper is to discover anomalous windows in linear intersecting paths. Anomalous windows are the contiguous groupings of data points. A linear path refers to a path represented by a line with a single dimensional spatial coordinate marking an observation point. In this paper, we propose an approach for discovering anomalous windows using a class of algorithms based on scan statistics, specifically 1) an Order invariant algorithm using Scan Statistics for Linear Intersecting Paths (SSLIP), 2) Brute force-SSLIP (BF-SSLIP), and 3) Central Brute Force—SSLIP (CBF-SSLIP). We further present two efficient variants of SSLIP: {\rm SSLIP}^\ast which employs a upper bound on the scan window size, and SSLIP-Acc, which adopts an accelerator function to speed up the scan process. The proposed approach for discovering anomalous windows along linear paths comprises the following distinct steps: 1) Cross Path Discovery: where we identify a subset of intersecting paths to be considered, 2) Anomalous Window Discovery: where we outline the various algorithms for the traversal of the cross paths to identify varying size directional windows along the paths. For identifying an anomalous window, an unusualness metric is computed, in the form of a likelihood ratio to indicate the degree of unusualness of this window with respect to the rest of the data. We identify the window with the highest likelihood ratio as our anomalous window, and 3) Monte Carlo Simulations: to ascertain whether this window is truly anomalous and not merely random occurrence, we perform hypothesis testing by computing a p-value using Monte Carlo Simulations. We present extensive experimental results in real world accident data sets for various highways with known issues (code and data available from [32], [27]). Additionally, we also perform comparisons with current approaches [18], [34] to show the efficacy of our approach. Our results show that our approach indeed is effective in identifying anomalous traffic accident windows along multiple intersecting highways.

#index 1646327
#* Materialization and Decomposition of Dataspaces for Efficient Search
#@ Shaoxu Song;Lei Chen;Mingxuan Yuan
#t 2011
#c 7
#! Dataspaces consist of large-scale heterogeneous data. The query interface of accessing tuples should be provided as a fundamental facility by practical dataspace systems. Previously, an efficient index has been proposed for queries with keyword neighborhood over dataspaces. In this paper, we study the materialization and decomposition of dataspaces, in order to improve the query efficiency. First, we study the views of items, which are materialized in order to be reused by queries. When a set of views are materialized, it leads to select some of them as the optimal plan with the minimum query cost. Efficient algorithms are developed for query planning and view generation. Second, we study the partitions of tuples for answering top-k queries. Given a query, we can evaluate the score bounds of the tuples in partitions and prune those partitions with bounds lower than the scores of top-k answers. We also provide theoretical analysis of query cost and prove that the query efficiency cannot be improved by increasing the number of partitions. Finally, we conduct an extensive experimental evaluation to illustrate the superior performance of proposed techniques.

#index 1646328
#* On Producing High and Early Result Throughput in Multijoin Query Plans
#@ Justin K. Levandoski;Mohamed E. Khalefa;Mohamed F. Mokbel
#t 2011
#c 7
#! This paper introduces an efficient framework for producing high and early result throughput in multijoin query plans. While most previous research focuses on optimizing for cases involving a single join operator, this work takes a radical step by addressing query plans with multiple join operators. The proposed framework consists of two main methods, a flush algorithm and operator state manager. The framework assumes a symmetric hash join, a common method for producing early results, when processing incoming data. In this way, our methods can be applied to a group of previous join operators (optimized for single-join queries) when taking part in multijoin query plans. Specifically, our framework can be applied by 1) employing a new flushing policy to write in-memory data to disk, once memory allotment is exhausted, in a way that helps increase the probability of producing early result throughput in multijoin queries, and 2) employing a state manager that adaptively switches operators in the plan between joining in-memory data and disk-resident data in order to positively affect the early result throughput. Extensive experimental results show that the proposed methods outperform the state-of-the-art join operators optimized for both single and multijoin query plans.

#index 1646329
#* Semantics of Ranking Queries for Probabilistic Data
#@ Jeffrey Jestes;Graham Cormode;Feifei Li;Ke Yi
#t 2011
#c 7
#! Recently, there have been several attempts to propose definitions and algorithms for ranking queries on probabilistic data. However, these lack many intuitive properties of a top-k over deterministic data. We define several fundamental properties, including exact-k, containment, unique rank, value invariance, and stability, which are satisfied by ranking queries on certain data. We argue that these properties should also be carefully studied in defining ranking queries in probabilistic data, and fulfilled by definition for ranking uncertain data for most applications. We propose an intuitive new ranking definition based on the observation that the ranks of a tuple across all possible worlds represent a well-founded rank distribution. We studied the ranking definitions based on the expectation, the median, and other statistics of this rank distribution for a tuple and derived the expected rank, median rank, and quantile rank correspondingly. We are able to prove that the expected rank, median rank, and quantile rank satisfy all these properties for a ranking query. We provide efficient solutions to compute such rankings across the major models of uncertain data, such as attribute-level and tuple-level uncertainty. Finally, a comprehensive experimental study confirms the effectiveness of our approach.

#index 1661191
#* A Framework for Learning Comprehensible Theories in XML Document Classification
#@ Jemma Wu
#t 2012
#c 7
#! XML has become the universal data format for a wide variety of information systems. The large number of XML documents existing on the web and in other information storage systems makes classification an important task. As a typical type of semistructured data, XML documents have both structures and contents. Traditional text learning techniques are not very suitable for XML document classification as structures are not considered. This paper presents a novel complete framework for XML document classification. We first present a knowledge representation method for XML documents which is based on a typed higher order logic formalism. With this representation method, an XML document is represented as a higher order logic term where both its contents and structures are captured. We then present a decision-tree learning algorithm driven by precision/recall breakeven point (PRDT) for the XML classification problem which can produce comprehensible theories. Finally, a semi-supervised learning algorithm is given which is based on the PRDT algorithm and the cotraining framework. Experimental results demonstrate that our framework is able to achieve good performance in both supervised and semi-supervised learning with the bonus of producing comprehensible learning theories.

#index 1661192
#* CoCITe—Coordinating Changes in Text
#@ Jeremy Wright;John Grothendieck
#t 2012
#c 7
#! Text streams are ubiquitous and contain a wealth of information, but are typically orders of magnitude too large in scale for comprehensive human inspection. There is a need for tools that can detect and group changes occurring within text streams and substreams, in order to find, structure, and summarize these changes for presentation to human analysts. This paper describes a procedure for efficiently finding step changes, trends, bursts, and cyclic changes affecting frequencies of words, or more general lexical items, within streams of documents which may be optionally labeled with metadata. The common phenomenon of over-dispersion is accommodated using mixture distributions. A streaming implementation is described which can process data from a continuous feed. Anomalies can be detected, grouped, and rendered visually for human comprehension.

#index 1661193
#* Effective Pattern Discovery for Text Mining
#@ Ning Zhong;Yuefeng Li;Sheng-Tang Wu
#t 2012
#c 7
#! Many data mining techniques have been proposed for mining useful patterns in text documents. However, how to effectively use and update discovered patterns is still an open research issue, especially in the domain of text mining. Since most existing text mining methods adopted term-based approaches, they all suffer from the problems of polysemy and synonymy. Over the years, people have often held the hypothesis that pattern (or phrase)-based approaches should perform better than the term-based ones, but many experiments do not support this hypothesis. This paper presents an innovative and effective pattern discovery technique which includes the processes of pattern deploying and pattern evolving, to improve the effectiveness of using and updating discovered patterns for finding relevant and interesting information. Substantial experiments on RCV1 data collection and TREC topics demonstrate that the proposed solution achieves encouraging performance.

#index 1661194
#* Efficient Processing of Uncertain Events in Rule-Based Systems
#@ Segev Wasserkrug;Avigdor Gal;Opher Etzion;Yulia Turchin
#t 2012
#c 7
#! There is a growing need for systems that react automatically to events. While some events are generated externally and deliver data across distributed systems, others need to be derived by the system itself based on available information. Event derivation is hampered by uncertainty attributed to causes such as unreliable data sources or the inability to determine with certainty whether an event has actually occurred, given available information. Two main challenges exist when designing a solution for event derivation under uncertainty. First, event derivation should scale under heavy loads of incoming events. Second, the associated probabilities must be correctly captured and represented. We present a solution to both problems by introducing a novel generic and formal mechanism and framework for managing event derivation under uncertainty. We also provide empirical evidence demonstrating the scalability and accuracy of our approach.

#index 1661195
#* Fractal-Based Intrinsic Dimension Estimation and Its Application in Dimensionality Reduction
#@ Dengyao Mo;Samuel H. Huang
#t 2012
#c 7
#! Dimensionality reduction is an important step in knowledge discovery in databases. Intrinsic dimension indicates the number of variables necessary to describe a data set. Two methods, box-counting dimension and correlation dimension, are commonly used for intrinsic dimension estimation. However, the robustness of these two methods has not been rigorously studied. This paper demonstrates that correlation dimension is more robust with respect to data sample size. In addition, instead of using a user selected distance d, we propose a new approach to capture all log-log pairs of a data set to more precisely estimate the correlation dimension. Systematic experiments are conducted to study factors that influence the computation of correlation dimension, including sample size, the number of redundant variables, and the portion of log-log plot used for calculation. Experiments on real-world data sets confirm the effectiveness of intrinsic dimension estimation with our improved method. Furthermore, a new supervised dimensionality reduction method based on intrinsic dimension estimation was introduced and validated.

#index 1661196
#* Identifying Evolving Groups in Dynamic Multimode Networks
#@ Lei Tang;Huan Liu;Jianping Zhang
#t 2012
#c 7
#! A multimode network consists of heterogeneous types of actors with various interactions occurring between them. Identifying communities in a multimode network can help understand the structural properties of the network, address the data shortage and unbalanced problems, and assist tasks like targeted marketing and finding influential actors within or between groups. In general, a network and its group structure often evolve unevenly. In a dynamic multimode network, both group membership and interactions can evolve, posing a challenging problem of identifying these evolving communities. In this work, we try to address this problem by employing the temporal information to analyze a multimode network. A temporally regularized framework and its convergence property are carefully studied. We show that the algorithm can be interpreted as an iterative latent semantic analysis process, which allows for extensions to handle networks with actor attributes and within-mode interactions. Experiments on both synthetic data and real-world networks demonstrate the efficacy of our approach and suggest its generality in capturing evolving groups in networks with heterogeneous entities and complex relationships.

#index 1661197
#* Incremental Information Extraction Using Relational Databases
#@ Luis Tari;Phan Huy Tu;Jorg Hakenberg;Yi Chen;Tran Cao Son;Graciela Gonzalez;Chitta Baral
#t 2012
#c 7
#! Information extraction systems are traditionally implemented as a pipeline of special-purpose processing modules targeting the extraction of a particular kind of information. A major drawback of such an approach is that whenever a new extraction goal emerges or a module is improved, extraction has to be reapplied from scratch to the entire text corpus even though only a small part of the corpus might be affected. In this paper, we describe a novel approach for information extraction in which extraction needs are expressed in the form of database queries, which are evaluated and optimized by database systems. Using database queries for information extraction enables generic extraction and minimizes reprocessing of data by performing incremental extraction to identify which part of the data is affected by the change of components or goals. Furthermore, our approach provides automated query generation components so that casual users do not have to learn the query language in order to perform extraction. To demonstrate the feasibility of our incremental extraction approach, we performed experiments to highlight two important aspects of an information extraction system: efficiency and quality of extraction results. Our experiments show that in the event of deployment of a new module, our incremental extraction approach reduces the processing time by 89.64 percent as compared to a traditional pipeline approach. By applying our methods to a corpus of 17 million biomedical abstracts, our experiments show that the query performance is efficient for real-time applications. Our experiments also revealed that our approach achieves high quality extraction results.

#index 1661198
#* Labeling Dynamic XML Documents: An Order-Centric Approach
#@ Liang Xu;Tok Wang Ling;Huayu Wu
#t 2012
#c 7
#! Dynamic XML labeling schemes have important applications in XML Database Management Systems. In this paper, we explore dynamic XML labeling schemes from a novel order-centric perspective. We compare the various labeling schemes proposed in the literature with a special focus on their orders of labels. We show that the order of labels fundamentally impacts the update performance of a labeling scheme and develop an order-based framework to classify and characterize XML labeling schemes. Although there are dynamic XML labeling schemes that can completely avoid relabeling, the gain in update performance all come with considerable costs such as larger label size and lower query performance, even if the XML documents are hardly updated. We introduce vector order which is the foundation of the dynamic labeling schemes we propose. Compared with previous solutions that are based on natural order or lexicographical order, vector order is a simple, yet most effective solution to process updates in XML DBMS. We show that vector order can be gracefully applied to both range-based and prefix-based labeling schemes with little overhead introduced. Moreover, vector order-based labeling schemes are not only efficient to process, but also resilient to skewed insertions. Qualitative and experimental evaluations confirm the benefits of our approach compared to previous solutions.

#index 1661199
#* Learning a Propagable Graph for Semisupervised Learning: Classification and Regression
#@ Bingbing Ni;Shuicheng Yan;Ashraf Kassim
#t 2012
#c 7
#! In this paper, we present a novel framework, called learning by propagability, for two essential data mining tasks, i.e., classification and regression. The whole learning process is driven by the philosophy that the data labels and the optimal feature representation jointly constitute a harmonic system, where the data labels are invariant with respect to the propagation on the similarity graph constructed based on the optimal feature representation. Based on this philosophy, a unified framework of learning by propagability is proposed for the purposes of both classification and regression. Specifically, this framework has three characteristics: 1) the formulation unifies the label propagation and optimal feature representation pursuing, and thus the label propagation process is enhanced by benefiting from the refined similarity graph constructed with the derived optimal feature representation instead of the original representation; 2) it unifies the formulations for supervised and semisupervised learning in both classification and regression tasks; and 3) it can directly deal with the multiclass classification problems. Extensive experiments for the classification task on UCI toy data sets, handwritten digit recognition, face recognition, and microarray recognition as well as for the regression task of human age estimation on the FG-NET aging database, all validate the effectiveness of our proposed learning framework, compared with the state-of-the-art counterparts.

#index 1661200
#* Mutual Information-Based Supervised Attribute Clustering for Microarray Sample Classification
#@ Pradipta Maji
#t 2012
#c 7
#! Microarray technology is one of the important biotechnological means that allows to record the expression levels of thousands of genes simultaneously within a number of different samples. An important application of microarray gene expression data in functional genomics is to classify samples according to their gene expression profiles. Among the large amount of genes presented in gene expression data, only a small fraction of them is effective for performing a certain diagnostic test. Hence, one of the major tasks with the gene expression data is to find groups of coregulated genes whose collective expression is strongly associated with the sample categories or response variables. In this regard, a new supervised attribute clustering algorithm is proposed to find such groups of genes. It directly incorporates the information of sample categories into the attribute clustering process. A new quantitative measure, based on mutual information, is introduced that incorporates the information of sample categories to measure the similarity between attributes. The proposed supervised attribute clustering algorithm is based on measuring the similarity between attributes using the new quantitative measure, whereby redundancy among the attributes is removed. The clusters are then refined incrementally based on sample categories. The performance of the proposed algorithm is compared with that of existing supervised and unsupervised gene clustering and gene selection algorithms based on the class separability index and the predictive accuracy of naive bayes classifier, K-nearest neighbor rule, and support vector machine on three cancer and two arthritis microarray data sets. The biological significance of the generated clusters is interpreted using the gene ontology. An important finding is that the proposed supervised attribute clustering algorithm is shown to be effective for identifying biologically significant gene clusters with excellent predictive capability.

#index 1661201
#* SPIRE: Efficient Data Inference and Compression over RFID Streams
#@ Yanming Nie;Richard Cocci;Zhao Cao;Yanlei Diao;Prashant Shenoy
#t 2012
#c 7
#! Despite its promise, RFID technology presents numerous challenges, including incomplete data, lack of location and containment information, and very high volumes. In this work, we present a novel data inference and compression substrate over RFID streams to address these challenges. Our substrate employs a time-varying graph model to efficiently capture possible object locations and interobject relationships such as containment from raw RFID streams. It then employs a probabilistic algorithm to estimate the most likely location and containment for each object. By performing such online inference, it enables online compression that recognizes and removes redundant information from the output stream of this substrate. We have implemented a prototype of our inference and compression substrate and evaluated it using both real traces from a laboratory warehouse setup and synthetic traces emulating enterprise supply chains. Results of a detailed performance study show that our data inference techniques provide high accuracy while retaining efficiency over RFID data streams, and our compression algorithm yields significant reduction in output data volume.

#index 1661202
#* Topic Mining over Asynchronous Text Sequences
#@ Xiang Wang;Xiaoming Jin;Meng-En Chen;Kai Zhang;Dou Shen
#t 2012
#c 7
#! Time stamped texts, or text sequences, are ubiquitous in real-world applications. Multiple text sequences are often related to each other by sharing common topics. The correlation among these sequences provides more meaningful and comprehensive clues for topic mining than those from each individual sequence. However, it is nontrivial to explore the correlation with the existence of asynchronism among multiple sequences, i.e., documents from different sequences about the same topic may have different time stamps. In this paper, we formally address this problem and put forward a novel algorithm based on the generative topic model. Our algorithm consists of two alternate steps: the first step extracts common topics from multiple sequences based on the adjusted time stamps provided by the second step; the second step adjusts the time stamps of the documents according to the time distribution of the topics discovered by the first step. We perform these two steps alternately and after iterations a monotonic convergence of our objective function can be guaranteed. The effectiveness and advantage of our approach were justified through extensive empirical studies on two real data sets consisting of six research paper repositories and two news article feeds, respectively.

#index 1661203
#* TSCAN: A Content Anatomy Approach to Temporal Topic Summarization
#@ Chien Chin Chen;Meng Chang Chen
#t 2012
#c 7
#! A topic is defined as a seminal event or activity along with all directly related events and activities. It is represented by a chronological sequence of documents published by different authors on the Internet. In this study, we define a task called topic anatomy, which summarizes and associates the core parts of a topic temporally so that readers can understand the content easily. The proposed topic anatomy model, called TSCAN, derives the major themes of a topic from the eigenvectors of a temporal block association matrix. Then, the significant events of the themes and their summaries are extracted by examining the constitution of the eigenvectors. Finally, the extracted events are associated through their temporal closeness and context similarity to form an evolution graph of the topic. Experiments based on the official TDT4 corpus demonstrate that the generated temporal summaries present the storylines of topics in a comprehensible form. Moreover, in terms of content coverage, coherence, and consistency, the summaries are superior to those derived by existing summarization methods based on human-composed reference summaries.

#index 1692261
#* TKDE Seeks Applications for EIC for 2013-2014 Term
#@ 
#t 2012
#c 7

#index 1692262
#* Measuring the Sky: On Computing Data Cubes via Skylining the Measures
#@ Man Lung Yiu;Eric Lo;Duncan Yung
#t 2012
#c 7
#! Data cube is a key element in supporting fast OLAP. Traditionally, an aggregate function is used to compute the values in data cubes. In this paper, we extend the notion of data cubes with a new perspective. Instead of using an aggregate function, we propose to build data cubes using the skyline operation as the “aggregate function.” Data cubes built in this way are called “group-by skyline cubes” and can support a variety of analytical tasks. Nevertheless, there are several challenges in implementing group-by skyline cubes in data warehouses: 1) the skyline operation is computational intensive, 2) the skyline operation is holistic, and 3) a group-by skyline cube contains both grouping and skyline dimensions, rendering it infeasible to precompute all cuboids in advance. This paper gives details on how to store, materialize, and query such cubes.

#index 1692263
#* On the Complexity of View Update Analysis and Its Application to Annotation Propagation
#@ Gao Cong;Wenfei Fan;Floris Geerts;Jianzhong Li;Jizhou Luo
#t 2012
#c 7
#! This paper investigates three problems identified in [1] for annotation propagation, namely, the view side-effect, source side-effect, and annotation placement problems. Given annotations entered for a tuple or an attribute in a view, these problems ask what tuples or attributes in the source have to be annotated to produce the view annotations. As observed in [1], these problems are fundamental not only for data provenance but also for the management of view updates. For an annotation attached to a single existing tuple in a view, it has been shown that these problems are often intractable even for views defined in terms of simple SPJU queries [1]. We revisit these problems by considering several dichotomies: 1) views defined in various subclasses of SPJU, versus SPJU views under a practical key preserving condition; 2) annotations attached to existing tuples in a view versus annotations on tuples to be inserted into the view; and 3) a single-tuple annotation versus a group of annotations. We provide a complete picture of intractability and tractability for the three problems in all these settings. We show that key preserving views often simplify the propagation analysis. Indeed, some problems become tractable for certain key preserving views, as opposed to the intractability of their counterparts that are not key preserving. However, group annotations often make the analysis harder. In addition, the problems have quite diverse complexity when annotations are attached to existing tuples in a view and when they are entered for tuples to be inserted into the view.

#index 1692264
#* Publishing Search Logs—A Comparative Study of Privacy Guarantees
#@ Michaela Gotz;Ashwin Machanavajjhala;Guozhang Wang;Xiaokui Xiao;Johannes Gehrke
#t 2012
#c 7
#! Search engine companies collect the “database of intentions,” the histories of their users' search queries. These search logs are a gold mine for researchers. Search engine companies, however, are wary of publishing search logs in order not to disclose sensitive information. In this paper, we analyze algorithms for publishing frequent keywords, queries, and clicks of a search log. We first show how methods that achieve variants of k-anonymity are vulnerable to active attacks. We then demonstrate that the stronger guarantee ensured by ε-differential privacy unfortunately does not provide any utility for this problem. We then propose an algorithm ZEALOUS and show how to set its parameters to achieve (ε,δ )-probabilistic privacy. We also contrast our analysis of ZEALOUS with an analysis by Korolova et al. [17] that achieves (ε′,δ′)-indistinguishability. Our paper concludes with a large experimental study using real applications where we compare ZEALOUS and previous work that achieves k-anonymity in search log publishing. Our results show that ZEALOUS yields comparable utility to k-anonymity while at the same time achieving much stronger privacy guarantees.

#index 1692265
#* Resilient Identity Crime Detection
#@ Clifton Phua;Kate Smith-Miles;Vincent Lee;Ross Gayler
#t 2012
#c 7
#! Identity crime is well known, prevalent, and costly; and credit application fraud is a specific case of identity crime. The existing nondata mining detection system of business rules and scorecards, and known fraud matching have limitations. To address these limitations and combat identity crime in real time, this paper proposes a new multilayered detection system complemented with two additional layers: communal detection (CD) and spike detection (SD). CD finds real social relationships to reduce the suspicion score, and is tamper resistant to synthetic social relationships. It is the whitelist-oriented approach on a fixed set of attributes. SD finds spikes in duplicates to increase the suspicion score, and is probe-resistant for attributes. It is the attribute-oriented approach on a variable-size set of attributes. Together, CD and SD can detect more types of attacks, better account for changing legal behavior, and remove the redundant attributes. Experiments were carried out on CD and SD with several million real credit applications. Results on the data support the hypothesis that successful credit application fraud patterns are sudden and exhibit sharp spikes in duplicates. Although this research is specific to credit application fraud detection, the concept of resilience, together with adaptivity and quality data discussed in the paper, are general to the design, implementation, and evaluation of all detection systems.

#index 1692266
#* ROAD: A New Spatial Object Search Framework for Road Networks
#@ Ken C. K. Lee;Wang-Chien Lee;Baihua Zheng;Yuan Tian
#t 2012
#c 7
#! In this paper, we present a new system framework called ROAD for spatial object search on road networks. ROAD is extensible to diverse object types and efficient for processing various location-dependent spatial queries (LDSQs), as it maintains objects separately from a given network and adopts an effective search space pruning technique. Based on our analysis on the two essential operations for LDSQ processing, namely, network traversal and object lookup, ROAD organizes a large road network as a hierarchy of interconnected regional subnetworks (called Rnets). Each Rnet is augmented with 1) shortcuts and 2) object abstracts to accelerate network traversals and provide quick object lookups, respectively. To manage those shortcuts and object abstracts, two cooperating indices, namely, Route Overlay and Association Directory are devised. In detail, we present 1) the Rnet hierarchy and several properties useful in constructing and maintaining the Rnet hierarchy, 2) the design and implementation of the ROAD framework, and 3) a suite of efficient search algorithms for single-source LDSQs and multisource LDSQs. We conduct a theoretical performance analysis and carry out a comprehensive empirical study to evaluate ROAD. The analysis and experiment results show the superiority of ROAD over the state-of-the-art approaches.

#index 1692267
#* Slicing: A New Approach for Privacy Preserving Data Publishing
#@ Tiancheng Li;Ninghui Li;Jian Zhang;Ian Molloy
#t 2012
#c 7
#! Several anonymization techniques, such as generalization and bucketization, have been designed for privacy preserving microdata publishing. Recent work has shown that generalization loses considerable amount of information, especially for high-dimensional data. Bucketization, on the other hand, does not prevent membership disclosure and does not apply for data that do not have a clear separation between quasi-identifying attributes and sensitive attributes. In this paper, we present a novel technique called slicing, which partitions the data both horizontally and vertically. We show that slicing preserves better data utility than generalization and can be used for membership disclosure protection. Another important advantage of slicing is that it can handle high-dimensional data. We show how slicing can be used for attribute disclosure protection and develop an efficient algorithm for computing the sliced data that obey the \ell-diversity requirement. Our workload experiments confirm that slicing preserves better utility than generalization and is more effective than bucketization in workloads involving the sensitive attribute. Our experiments also demonstrate that slicing can be used to prevent membership disclosure.

#index 1692268
#* A Framework for Similarity Search of Time Series Cliques with Natural Relations
#@ Bin Cui;Zhe Zhao;Wee Hyong Tok
#t 2012
#c 7
#! A Time Series Clique (TSC) consists of multiple time series which are related to each other by natural relations. The natural relations that are found between the time series depend on the application domains. For example, a TSC can consist of time series which are trajectories in video that have spatial relations. In conventional time series retrieval, such natural relations between the time series are not considered. In this paper, we formalize the problem of similarity search over a TSC database. We develop a novel framework for efficient similarity search on TSC data. The framework addresses the following issues. First, it provides a compact representation for TSC data. Second, it uses a multidimensional relation vector to capture the natural relations between the multiple time series in a TSC. Lastly, the framework defines a novel similarity measure that uses the compact representation and the relation vector. We conduct an extensive performance study, using both real-life and synthetic data sets. From the performance study, we show that our proposed framework is both effective and efficient for TSC retrieval.

#index 1692269
#* A Genetic Programming Approach to Record Deduplication
#@ Moises G. de Carvalho;Alberto H.  F. Laender;Marcos Andre Goncalves;Altigran S. da Silva
#t 2012
#c 7
#! Several systems that rely on consistent data to offer high-quality services, such as digital libraries and e-commerce brokers, may be affected by the existence of duplicates, quasi replicas, or near-duplicate entries in their repositories. Because of that, there have been significant investments from private and government organizations for developing methods for removing replicas from its data repositories. This is due to the fact that clean and replica-free repositories not only allow the retrieval of higher quality information but also lead to more concise data and to potential savings in computational time and resources to process this data. In this paper, we propose a genetic programming approach to record deduplication that combines several different pieces of evidence extracted from the data content to find a deduplication function that is able to identify whether two entries in a repository are replicas or not. As shown by our experiments, our approach outperforms an existing state-of-the-art method found in the literature. Moreover, the suggested functions are computationally less demanding since they use fewer evidence. In addition, our genetic programming approach is capable of automatically adapting these functions to a given fixed replica identification boundary, freeing the user from the burden of having to choose and tune this parameter.

#index 1692270
#* A Link-Based Cluster Ensemble Approach for Categorical Data Clustering
#@ Natthakan Iam-On;Tossapon Boongeon;Simon Garrett;Chris Price
#t 2012
#c 7
#! Although attempts have been made to solve the problem of clustering categorical data via cluster ensembles, with the results being competitive to conventional algorithms, it is observed that these techniques unfortunately generate a final data partition based on incomplete information. The underlying ensemble-information matrix presents only cluster-data point relations, with many entries being left unknown. The paper presents an analysis that suggests this problem degrades the quality of the clustering result, and it presents a new link-based approach, which improves the conventional matrix by discovering unknown entries through similarity between clusters in an ensemble. In particular, an efficient link-based algorithm is proposed for the underlying similarity assessment. Afterward, to obtain the final clustering result, a graph partitioning technique is applied to a weighted bipartite graph that is formulated from the refined matrix. Experimental results on multiple real data sets suggest that the proposed link-based method almost always outperforms both conventional clustering algorithms for categorical data and well-known cluster ensemble techniques.

#index 1692271
#* A Probabilistic Scheme for Keyword-Based Incremental Query Construction
#@ Elena Demidova;Xuan Zhou;Wolfgang Nejdl
#t 2012
#c 7
#! Databases enable users to precisely express their informational needs using structured queries. However, database query construction is a laborious and error-prone process, which cannot be performed well by most end users. Keyword search alleviates the usability problem at the price of query expressiveness. As keyword search algorithms do not differentiate between the possible informational needs represented by a keyword query, users may not receive adequate results. This paper presents IQ^P—a novel approach to bridge the gap between usability of keyword search and expressiveness of database queries. IQ^P enables a user to start with an arbitrary keyword query and incrementally refine it into a structured query through an interactive interface. The enabling techniques of IQ^P include: 1) a probabilistic framework for incremental query construction; 2) a probabilistic model to assess the possible informational needs represented by a keyword query; 3) an algorithm to obtain the optimal query construction process. This paper presents the detailed design of IQ^P, and demonstrates its effectiveness and scalability through experiments over real-world data and a user study.

#index 1692272
#* Efficiently Indexing Large Sparse Graphs for Similarity Search
#@ Guoren Wang;Bin Wang;Xiaochun Yang;Ge Yu
#t 2012
#c 7
#! The graph structure is a very important means to model schemaless data with complicated structures, such as protein-protein interaction networks, chemical compounds, knowledge query inferring systems, and road networks. This paper focuses on the index structure for similarity search on a set of large sparse graphs and proposes an efficient indexing mechanism by introducing the Q-Gram idea. By decomposing graphs to small grams (organized byκ-Adjacent Tree patterns) and pairing-up on those κ-Adjacent Tree patterns, the lower bound estimation of their edit distance can be calculated for candidate filtering. Furthermore, we have developed a series of techniques for inverted index construction and online query processing. By building the candidate set for the query graph before the exact edit distance calculation, the number of graphs need to proceed into exact matching can be greatly reduced. Extensive experiments on real and synthetic data sets have been conducted to show the effectiveness and efficiency of the proposed indexing mechanism.

#index 1692273
#* Extending Attribute Information for Small Data Set Classification
#@ Der-Chiang Li;Chiao-wen Liu
#t 2012
#c 7
#! Data quantity is the main issue in the small data set problem, because usually insufficient data will not lead to a robust classification performance. How to extract more effective information from a small data set is thus of considerable interest. This paper proposes a new attribute construction approach which converts the original data attributes into a higher dimensional feature space to extract more attribute information by a similarity-based algorithm using the classification-oriented fuzzy membership function. Seven data sets with different attribute sizes are employed to examine the performance of the proposed method. The results show that the proposed method has a superior classification performance when compared to principal component analysis (PCA), kernel principal component analysis (KPCA), and kernel independent component analysis (KICA) with a Gaussian kernel in the support vector machine (SVM) classifier.

#index 1692274
#* Feature Selection Based on Class-Dependent Densities for High-Dimensional Binary Data
#@ Kashif Javed;Haroon A. Babri;Mehreen Saeed
#t 2012
#c 7
#! Data and knowledge management systems employ feature selection algorithms for removing irrelevant, redundant, and noisy information from the data. There are two well-known approaches to feature selection, feature ranking (FR) and feature subset selection (FSS). In this paper, we propose a new FR algorithm, termed as class-dependent density-based feature elimination (CDFE), for binary data sets. Our theoretical analysis shows that CDFE computes the weights, used for feature ranking, more efficiently as compared to the mutual information measure. Effectively, rankings obtained from both the two criteria approximate each other. CDFE uses a filtrapper approach to select a final subset. For data sets having hundreds of thousands of features, feature selection with FR algorithms is simple and computationally efficient but redundant information may not be removed. On the other hand, FSS algorithms analyze the data for redundancies but may become computationally impractical on high-dimensional data sets. We address these problems by combining FR and FSS methods in the form of a two-stage feature selection algorithm. When introduced as a preprocessing step to the FSS algorithms, CDFE not only presents them with a feature subset, good in terms of classification, but also relieves them from heavy computations. Two FSS algorithms are employed in the second stage to test the two-stage feature selection idea. We carry out experiments with two different classifiers (naive Bayes' and kernel ridge regression) on three different real-life data sets (NOVA, HIVA, and GINA) of the ”Agnostic Learning versus Prior Knowledge” challenge. As a stand-alone method, CDFE shows up to about 92 percent reduction in the feature set size. When combined with the FSS algorithms in two-stages, CDFE significantly improves their classification accuracy and exhibits up to 97 percent reduction in the feature set size. We also compared CDFE against the winning entries of the challenge and found that it outperforms the best results on NOVA and HIVA while obtaining a third position in case of GINA.

#index 1692275
#* Learning Bregman Distance Functions for Semi-Supervised Clustering
#@ Lei Wu;Steven C. H. Hoi;Rong Jin;Jianke Zhu;Nenghai Yu
#t 2012
#c 7
#! Learning distance functions with side information plays a key role in many data mining applications. Conventional distance metric learning approaches often assume that the target distance function is represented in some form of Mahalanobis distance. These approaches usually work well when data are in low dimensionality, but often become computationally expensive or even infeasible when handling high-dimensional data. In this paper, we propose a novel scheme of learning nonlinear distance functions with side information. It aims to learn a Bregman distance function using a nonparametric approach that is similar to Support Vector Machines. We emphasize that the proposed scheme is more general than the conventional approach for distance metric learning, and is able to handle high-dimensional data efficiently. We verify the efficacy of the proposed distance learning method with extensive experiments on semi-supervised clustering. The comparison with state-of-the-art approaches for learning distance functions with side information reveals clear advantages of the proposed technique.

#index 1692325
#* A Multidimensional Sequence Approach to Measuring Tree Similarity
#@ Zhiwei Lin;Hui Wang;Sally McClean
#t 2012
#c 7
#! Tree is one of the most common and well-studied data structures in computer science. Measuring the similarity of such structures is key to analyzing this type of data. However, measuring tree similarity is not trivial due to the inherent complexity of trees and the ensuing large search space. Tree kernel, a state of the art similarity measurement of trees, represents trees as vectors in a feature space and measures similarity in this space. When different features are used, different algorithms are required. Tree edit distance is another widely used similarity measurement of trees. It measures similarity through edit operations needed to transform one tree to another. Without any restrictions on edit operations, the computation cost is too high to be applicable to large volume of data. To improve efficiency of tree edit distance, some approximations were introduced into tree edit distance. However, their effectiveness can be compromised. In this paper, a novel approach to measuring tree similarity is presented. Trees are represented as multidimensional sequences and their similarity is measured on the basis of their sequence representations. Multidimensional sequences have their sequential dimensions and spatial dimensions. We measure the sequential similarity by the all common subsequences sequence similarity measurement or the longest common subsequence measurement, and measure the spatial similarity by dynamic time warping. Then we combine them to give a measure of tree similarity. A brute force algorithm to calculate the similarity will have high computational cost. In the spirit of dynamic programming two efficient algorithms are designed for calculating the similarity, which have quadratic time complexity. The new measurements are evaluated in terms of classification accuracy in two popular classifiers (k-nearest neighbor and support vector machine) and in terms of search effectiveness and efficiency in k-nearest neighbor similarity search, using three different data sets from natural language processing and information retrieval. Experimental results show that the new measurements outperform the benchmark measures consistently and significantly.

#index 1692326
#* Agglomerative Mean-Shift Clustering
#@ Xiao-Tong Yuan;Bao-Gang Hu;Ran He
#t 2012
#c 7
#! Mean-Shift (MS) is a powerful nonparametric clustering method. Although good accuracy can be achieved, its computational cost is particularly expensive even on moderate data sets. In this paper, for the purpose of algorithmic speedup, we develop an agglomerative MS clustering method along with its performance analysis. Our method, namely Agglo-MS, is built upon an iterative query set compression mechanism which is motivated by the quadratic bounding optimization nature of MS algorithm. The whole framework can be efficiently implemented in linear running time complexity. We then extend Agglo-MS into an incremental version which performs comparably to its batch counterpart. The efficiency and accuracy of Agglo-MS are demonstrated by extensive comparing experiments on synthetic and real data sets.

#index 1692327
#* Answering General Time-Sensitive Queries
#@ Wisam Dakka;Luis Gravano;Panagiotis Ipeirotis
#t 2012
#c 7
#! Time is an important dimension of relevance for a large number of searches, such as over blogs and news archives. So far, research on searching over such collections has largely focused on locating topically similar documents for a query. Unfortunately, topic similarity alone is not always sufficient for document ranking. In this paper, we observe that, for an important class of queries that we call time-sensitive queries, the publication time of the documents in a news archive is important and should be considered in conjunction with the topic similarity to derive the final document ranking. Earlier work has focused on improving retrieval for “recency” queries that target recent documents. We propose a more general framework for handling time-sensitive queries and we automatically identify the important time intervals that are likely to be of interest for a query. Then, we build scoring techniques that seamlessly integrate the temporal aspect into the overall ranking mechanism. We present an extensive experimental evaluation using a variety of news article data sets, including TREC data as well as real web data analyzed using the Amazon Mechanical Turk. We examine several techniques for detecting the important time intervals for a query over a news archive and for incorporating this information in the retrieval process. We show that our techniques are robust and significantly improve result quality for time-sensitive queries compared to state-of-the-art retrieval techniques.

#index 1692328
#* BibPro: A Citation Parser Based on Sequence Alignment
#@ Chien-Chih Chen;Kai-Hsiang Yang;Chuen-Liang Chen;Jan-Ming Ho
#t 2012
#c 7
#! Dramatic increase in the number of academic publications has led to growing demand for efficient organization of the resources to meet researchers' needs. As a result, a number of network services have compiled databases from the public resources scattered over the Internet. However, publications by different conferences and journals adopt different citation styles. It is an interesting problem to accurately extract metadata from a citation string which is formatted in one of thousands of different styles. It has attracted a great deal of attention in research in recent years. In this paper, based on the notion of sequence alignment, we present a citation parser called BibPro that extracts components of a citation string. To demonstrate the efficacy of BibPro, we conducted experiments on three benchmark data sets. The results show that BibPro achieved over 90 percent accuracy on each benchmark. Even with citations and associated metadata retrieved from the web as training data, our experiments show that BibPro still achieves a reasonable performance.

#index 1692329
#* Discover Dependencies from Data—A Review
#@ Jixue Liu;Jiuyong Li;Chengfei Liu;Yongfeng Chen
#t 2012
#c 7
#! Functional and inclusion dependency discovery is important to knowledge discovery, database semantics analysis, database design, and data quality assessment. Motivated by the importance of dependency discovery, this paper reviews the methods for functional dependency, conditional functional dependency, approximate functional dependency, and inclusion dependency discovery in relational databases and a method for discovering XML functional dependencies.

#index 1692330
#* Effective and Efficient Shape-Based Pattern Detection over Streaming Time Series
#@ Yueguo Chen;Ke Chen;Mario A. Nascimento
#t 2012
#c 7
#! Existing distance measures of time series such as the euclidean distance, DTW, and EDR are inadequate in handling certain degrees of amplitude shifting and scaling variances of data items. We propose a novel distance measure of time series, Spatial Assembling Distance (SpADe), that is able to handle noisy, shifting, and scaling in both temporal and amplitude dimensions. We further apply the SpADe to the application of streaming pattern detection, which is very useful in trend-related analysis, sensor networks, and video surveillance. Our experimental results on real time series data sets show that SpADe is an effective distance measure of time series. Moreover, high accuracy and efficiency are achieved by SpADe for continuous pattern detection in streaming time series.

#index 1692331
#* Mining Low-Support Discriminative Patterns from Dense and High-Dimensional Data
#@ Gang Fang;Gaurav Pandey;Wen Wang;Manish Gupta;Michael Steinbach;Vipin Kumar
#t 2012
#c 7
#! HASH(0x295e1ec)

#index 1692332
#* On Group Nearest Group Query Processing
#@ Ke Deng;Shazia Sadiq;Xiaofang Zhou;Hu Xu;Gabriel Pui Cheong Fung;Yansheng Lu
#t 2012
#c 7
#! Given a data point set D, a query point set Q, and an integer k, the Group Nearest Group (GNG) query finds a subset \omega (\vert \omega \vert \le k) of points from D such that the total distance from all points in Q to the nearest point in \omega is not greater than any other subset \omega^{\prime } (\vert \omega^{\prime }\vert \le k) of points in D. GNG query is a partition-based clustering problem which can be found in many real applications and is NP-hard. In this paper, Exhaustive Hierarchical Combination (EHC) algorithm and Subset Hierarchial Refinement (SHR) algorithm are developed for GNG query processing. While EHC is capable to provide the optimal solution for k=2, SHR is an efficient approximate approach that combines database techniques with local search heuristic. The processing focus of our approaches is on minimizing the access and evaluation of subsets of cardinality k in D since the number of such subsets is exponentially greater than \vert D\vert. To do that, the hierarchical blocks of data points at high level are used to find an intermediate solution and then refined by following the guided search direction at low level so as to prune irrelevant subsets. The comprehensive experiments on both real and synthetic data sets demonstrate the superiority of SHR in terms of efficiency and quality.

#index 1692333
#* On the Deep Order-Preserving Submatrix Problem: A Best Effort Approach
#@ Byron J. Gao;Obi L. Griffith;Martin Ester;Hui Xiong;Qiang Zhao;Steven J. M. Jones
#t 2012
#c 7
#! HASH(0x2982ae4)

#index 1692334
#* On the Spectral Characterization and Scalable Mining of Network Communities
#@ Yang Bo;Jiming Liu;Jianfeng Feng
#t 2012
#c 7
#! Network communities refer to groups of vertices within which their connecting links are dense but between which they are sparse. A network community mining problem (or NCMP for short) is concerned with the problem of finding all such communities from a given network. A wide variety of applications can be formulated as NCMPs, ranging from social and/or biological network analysis to web mining and searching. So far, many algorithms addressing NCMPs have been developed and most of them fall into the categories of either optimization based or heuristic methods. Distinct from the existing studies, the work presented in this paper explores the notion of network communities and their properties based on the dynamics of a stochastic model naturally introduced. In the paper, a relationship between the hierarchical community structure of a network and the local mixing properties of such a stochastic model has been established with the large-deviation theory. Topological information regarding to the community structures hidden in networks can be inferred from their spectral signatures. Based on the above-mentioned relationship, this work proposes a general framework for characterizing, analyzing, and mining network communities. Utilizing the two basic properties of metastability, i.e., being locally uniform and temporarily fixed, an efficient implementation of the framework, called the LM algorithm, has been developed that can scalably mine communities hidden in large-scale networks. The effectiveness and efficiency of the LM algorithm have been theoretically analyzed as well as experimentally validated.

#index 1692335
#* Outsourced Similarity Search on Metric Data Assets
#@ Man Lung Yiu;Ira Assent;Christian S. Jensen;Panos Kalnis
#t 2012
#c 7
#! This paper considers a cloud computing setting in which similarity querying of metric data is outsourced to a service provider. The data is to be revealed only to trusted users, not to the service provider or anyone else. Users query the server for the most similar data objects to a query example. Outsourcing offers the data owner scalability and a low-initial investment. The need for privacy may be due to the data being sensitive (e.g., in medicine), valuable (e.g., in astronomy), or otherwise confidential. Given this setting, the paper presents techniques that transform the data prior to supplying it to the service provider for similarity queries on the transformed data. Our techniques provide interesting trade-offs between query cost and accuracy. They are then further extended to offer an intuitive privacy guarantee. Empirical studies with real data demonstrate that the techniques are capable of offering privacy while enabling efficient and accurate processing of similarity queries.

#index 1692336
#* Privacy Preserving Decision Tree Learning Using Unrealized Data Sets
#@ Pui Kuen Fong;Jens H. Weber-Jahnke
#t 2012
#c 7
#! Privacy preservation is important for machine learning and data mining, but measures designed to protect private information often result in a trade-off: reduced utility of the training samples. This paper introduces a privacy preserving approach that can be applied to decision tree learning, without concomitant loss of accuracy. It describes an approach to the preservation of the privacy of collected data samples in cases where information from the sample database has been partially lost. This approach converts the original sample data sets into a group of unreal data sets, from which the original samples cannot be reconstructed without the entire group of unreal data sets. Meanwhile, an accurate decision tree can be built directly from those unreal data sets. This novel approach can be applied directly to the data storage as soon as the first sample is collected. The approach is compatible with other privacy preserving approaches, such as cryptography, for extra protection.

#index 1692337
#* Subspace Similarity Search under {\rm L}_p-Norm
#@ Xiang Lian;Lei Chen
#t 2012
#c 7
#! Similarity search has been widely used in many applications such as information retrieval, image data analysis, and time-series matching. Previous work on similarity search usually consider the search problem in the full space. In this paper, however, we tackle a problem, subspace similarity search, which finds all data objects that match with a query object in the subspace instead of the original full space. In particular, the query object can specify arbitrary subspace with arbitrary number of dimensions. Due to the exponential number of possible subspaces specified by users, we introduce an efficient and effective pruning technique, which assigns scores to data objects with respect to pivots and prunes candidates via scores. We propose an effective multipivot-based method to preprocess data objects by selecting appropriate pivots, where the entire procedure is guided by a formal cost model, such that the pruning power is maximized. Then, scores of each data object are organized in sorted lists to facilitate an efficient subspace similarity search. Furthermore, many real-world application data such as image databases, time-series data, and sensory data often contain noises, which can be modeled as uncertain objects. Different from certain data, efficient query processing on uncertain data is more challenging due to its intensive computation of probability confidences. Thus, it is also crucial to answer subspace queries efficiently and effectively over uncertain objects. Specifically, we define a novel query, namely probabilistic subspace range query (PSRQ) in the uncertain database, which finds objects within a distance from a query object in any subspace with high probability. To address this query, we extend our proposed pruning techniques for precise data to that of answering PSRQ in arbitrary subspaces. Extensive experiments demonstrated the performance of our proposed approaches.

#index 1755297
#* Anomaly Detection for Discrete Sequences: A Survey
#@ Varun Chandola;Arindam Banerjee;Vipin Kumar
#t 2012
#c 7
#! This survey attempts to provide a comprehensive and structured overview of the existing research for the problem of detecting anomalies in discrete/symbolic sequences. The objective is to provide a global understanding of the sequence anomaly detection problem and how existing techniques relate to each other. The key contribution of this survey is the classification of the existing research into three distinct categories, based on the problem formulation that they are trying to solve. These problem formulations are: 1) identifying anomalous sequences with respect to a database of normal sequences; 2) identifying an anomalous subsequence within a long sequence; and 3) identifying a pattern in a sequence whose frequency of occurrence is anomalous. We show how each of these problem formulations is characteristically distinct from each other and discuss their relevance in various application domains. We review techniques from many disparate and disconnected application domains that address each of these formulations. Within each problem formulation, we group techniques into categories based on the nature of the underlying algorithm. For each category, we provide a basic anomaly detection technique, and show how the existing techniques are variants of the basic technique. This approach shows how different techniques within a category are related or different from each other. Our categorization reveals new variants and combinations that have not been investigated before for anomaly detection. We also provide a discussion of relative strengths and weaknesses of different techniques. We show how techniques developed for one problem formulation can be adapted to solve a different formulation, thereby providing several novel adaptations to solve the different problem formulations. We also highlight the applicability of the techniques that handle discrete sequences to other related areas such as online anomaly detection and time series anomaly detection.

#index 1755298
#* Creating Evolving User Behavior Profiles Automatically
#@ Jose Antonio Iglesias;Plamen Angelov;Agapito Ledezma;Araceli Sanchis
#t 2012
#c 7
#! Knowledge about computer users is very beneficial for assisting them, predicting their future actions or detecting masqueraders. In this paper, a new approach for creating and recognizing automatically the behavior profile of a computer user is presented. In this case, a computer user behavior is represented as the sequence of the commands she/he types during her/his work. This sequence is transformed into a distribution of relevant subsequences of commands in order to find out a profile that defines its behavior. Also, because a user profile is not necessarily fixed but rather it evolves/changes, we propose an evolving method to keep up to date the created profiles using an Evolving Systems approach. In this paper, we combine the evolving classifier with a trie-based user profiling to obtain a powerful self-learning online scheme. We also develop further the recursive formula of the potential of a data point to become a cluster center using cosine distance, which is provided in the Appendix. The novel approach proposed in this paper can be applicable to any problem of dynamic/evolving user behavior modeling where it can be represented as a sequence of actions or events. It has been evaluated on several real data streams.

#index 1755299
#* D-Cache: Universal Distance Cache for Metric Access Methods
#@ Tomas Skopal;Jakub Lokoc;Benjamin Bustos
#t 2012
#c 7
#! The caching of accessed disk pages has been successfully used for decades in database technology, resulting in effective amortization of I/O operations needed within a stream of query or update requests. However, in modern complex databases, like multimedia databases, the I/O cost becomes a minor performance factor. In particular, metric access methods (MAMs), used for similarity search in complex unstructured data, have been designed to minimize rather the number of distance computations than I/O cost (when indexing or querying). Inspired by I/O caching in traditional databases, in this paper we introduce the idea of distance caching for usage with MAMs—a novel approach to streamline similarity search. As a result, we present the D-cache, a main-memory data structure which can be easily implemented into any MAM, in order to spare the distance computations spent by queries/updates. In particular, we have modified two state-of-the-art MAMs to make use of D-cache—the M-tree and Pivot tables. Moreover, we present the D-file, an index-free MAM based on simple sequential search augmented by D-cache. The experimental evaluation shows that performance gain achieved due to D-cache is significant for all the MAMs, especially for the D-file.

#index 1755300
#* Efficient Fuzzy Type-Ahead Search in XML Data
#@ Jianhua Feng;Guoliang Li
#t 2012
#c 7
#! In a traditional keyword-search system over XML data, a user composes a keyword query, submits it to the system, and retrieves relevant answers. In the case where the user has limited knowledge about the data, often the user feels “left in the dark” when issuing queries, and has to use a try-and-see approach for finding information. In this paper, we study fuzzy type-ahead search in XML data, a new information-access paradigm in which the system searches XML data on the fly as the user types in query keywords. It allows users to explore data as they type, even in the presence of minor errors of their keywords. Our proposed method has the following features: 1) Search as you type: It extends Autocomplete by supporting queries with multiple keywords in XML data. 2) Fuzzy: It can find high-quality answers that have keywords matching query keywords approximately. 3) Efficient: Our effective index structures and searching algorithms can achieve a very high interactive speed. We study research challenges in this new search framework. We propose effective index structures and top-k algorithms to achieve a high interactive speed. We examine effective ranking functions and early termination techniques to progressively identify the top-k relevant answers. We have implemented our method on real data sets, and the experimental results show that our method achieves high search efficiency and result quality.

#index 1755301
#* Organizing User Search Histories
#@ Heasoo Hwang;Hady W. Lauw;Lise Getoor;Alexandros Ntoulas
#t 2012
#c 7
#! Users are increasingly pursuing complex task-oriented goals on the web, such as making travel arrangements, managing finances, or planning purchases. To this end, they usually break down the tasks into a few codependent steps and issue multiple queries around these steps repeatedly over long periods of time. To better support users in their long-term information quests on the web, search engines keep track of their queries and clicks while searching online. In this paper, we study the problem of organizing a user's historical queries into groups in a dynamic and automated fashion. Automatically identifying query groups is helpful for a number of different search engine components and applications, such as query suggestions, result ranking, query alterations, sessionization, and collaborative search. In our approach, we go beyond approaches that rely on textual similarity or time thresholds, and we propose a more robust approach that leverages search query logs. We experimentally study the performance of different techniques, and showcase their potential, especially when combined together.

#index 1755302
#* Semi-Supervised Maximum Margin Clustering with Pairwise Constraints
#@ Hong Zeng;Yiu-ming Cheung
#t 2012
#c 7
#! The pairwise constraints specifying whether a pair of samples should be grouped together or not have been successfully incorporated into the conventional clustering methods such as k-means and spectral clustering for the performance enhancement. Nevertheless, the issue of pairwise constraints has not been well studied in the recently proposed maximum margin clustering (MMC), which extends the maximum margin framework in supervised learning for clustering and often shows a promising performance. This paper therefore proposes a pairwise constrained MMC algorithm. Based on the maximum margin idea in MMC, we propose a set of effective loss functions for discouraging the violation of given pairwise constraints. For the resulting optimization problem, we show that the original nonconvex problem in our approach can be decomposed into a sequence of convex quadratic program problems via constrained concave-convex procedure (CCCP). Subsequently, we present an efficient subgradient projection optimization method to solve each convex problem in the CCCP sequence. Experiments on a number of real-world data sets show that the proposed constrained MMC algorithm is scalable and outperforms the existing constrained MMC approach as well as the typical semi-supervised clustering counterparts.

#index 1755303
#* A Framework for Personal Mobile Commerce Pattern Mining and Prediction
#@ Eric Hsueh-Chan Lu;Wang-Chien Lee;Vincent Shin-Mu Tseng
#t 2012
#c 7
#! Due to a wide range of potential applications, research on mobile commerce has received a lot of interests from both of the industry and academia. Among them, one of the active topic areas is the mining and prediction of users' mobile commerce behaviors such as their movements and purchase transactions. In this paper, we propose a novel framework, called Mobile Commerce Explorer (MCE), for mining and prediction of mobile users' movements and purchase transactions under the context of mobile commerce. The MCE framework consists of three major components: 1) Similarity Inference Model (SIM) for measuring the similarities among stores and items, which are two basic mobile commerce entities considered in this paper; 2) Personal Mobile Commerce Pattern Mine (PMCP-Mine) algorithm for efficient discovery of mobile users' Personal Mobile Commerce Patterns (PMCPs); and 3) Mobile Commerce Behavior Predictor (MCBP) for prediction of possible mobile user behaviors. To our best knowledge, this is the first work that facilitates mining and prediction of mobile users' commerce behaviors in order to recommend stores and items previously unknown to a user. We perform an extensive experimental evaluation by simulation and show that our proposals produce excellent results.

#index 1755304
#* A Query Formulation Language for the Data Web
#@ Mustafa Jarrar;Marios D. Dikaiakos
#t 2012
#c 7
#! We present a query formulation language (called MashQL) in order to easily query and fuse structured data on the web. The main novelty of MashQL is that it allows people with limited IT skills to explore and query one (or multiple) data sources without prior knowledge about the schema, structure, vocabulary, or any technical details of these sources. More importantly, to be robust and cover most cases in practice, we do not assume that a data source should have—an offline or inline—schema. This poses several language-design and performance complexities that we fundamentally tackle. To illustrate the query formulation power of MashQL, and without loss of generality, we chose the Data web scenario. We also chose querying RDF, as it is the most primitive data model; hence, MashQL can be similarly used for querying relational databases and XML. We present two implementations of MashQL, an online mashup editor, and a Firefox add on. The former illustrates how MashQL can be used to query and mash up the Data web as simple as filtering and piping web feeds; and the Firefox add on illustrates using the browser as a web composer rather than only a navigator. To end, we evaluate MashQL on querying two data sets, DBLP and DBPedia, and show that our indexing techniques allow instant user interaction.

#index 1755305
#* A Temporal Pattern Search Algorithm for Personal History Event Visualization
#@ Taowei David Wang;Amol Deshpande;Ben Shneiderman
#t 2012
#c 7
#! We present Temporal Pattern Search (TPS), a novel algorithm for searching for temporal patterns of events in historical personal histories. The traditional method of searching for such patterns uses an automaton-based approach over a single array of events, sorted by time stamps. Instead, TPS operates on a set of arrays, where each array contains all events of the same type, sorted by time stamps. TPS searches for a particular item in the pattern using a binary search over the appropriate arrays. Although binary search is considerably more expensive per item, it allows TPS to skip many unnecessary events in personal histories. We show that TPS's running time is bounded by O(m^2n lg(n)), where m is the length of (number of events) a search pattern, and n is the number of events in a record (history). Although the asymptotic running time of TPS is inferior to that of a nondeterministic finite automaton (NFA) approach (O(mn)), TPS performs better than NFA under our experimental conditions. We also show TPS is very competitive with Shift-And, a bit-parallel approach, with real data. Since the experimental conditions we describe here subsume the conditions under which analysts would typically use TPS (i.e., within an interactive visualization program), we argue that TPS is an appropriate design choice for us.

#index 1755306
#* An Efficient Formulation of the Improved Visual Assessment of Cluster Tendency (iVAT) Algorithm
#@ Timothy C. Havens;James C. Bezdek
#t 2012
#c 7
#! The VAT algorithm is a visual method for determining the possible number of clusters in, or the cluster tendency of a set of objects. The improved VAT (iVAT) algorithm uses a graph-theoretic distance transform to improve the effectiveness of the VAT algorithm for “tough” cases where VAT fails to accurately show the cluster tendency. In this paper, we present an efficient formulation of the iVAT algorithm which reduces the computational complexity of the iVAT algorithm from O(N^3) to O(N^2). We also prove a direct relationship between the VAT image and the iVAT image produced by our efficient formulation. We conclude with three examples displaying clustering tendencies in three of the Karypis data sets that illustrate the improvement offered by the iVAT transformation. We also provide a comparison of iVAT images to those produced by the Reverse Cuthill-Mckee (RCM) algorithm; our examples suggest that iVAT is superior to the RCM method of display.

#index 1755307
#* Continuous Top-k Dominating Queries
#@ Maria Kontaki;Apostolos N. Papadopoulos;Yannis Manolopoulos
#t 2012
#c 7
#! Top-k dominating queries use an intuitive scoring function which ranks multidimensional points with respect to their dominance power, i.e., the number of points that a point dominates. The k points with the best (e.g., highest) scores are returned to the user. Both top-k and skyline queries have been studied in a streaming environment, where changes to the data set are very frequent. In such an environment, continuous query processing techniques are required toward efficient monitoring of query results, since periodic query re-execution is computationally intensive, and therefore, prohibitive. This work contains the first study of continuous top-k dominating queries over data streams. In comparison to continuous top-k and skyline queries, continuous top-k dominating queries pose additional challenges. Three exact algorithms (BFA, EVA, ADA) are studied, and among them ADA, which is enhanced with additional optimization techniques, shows the best overall performance. In some cases, we are willing to trade accuracy for speed. Toward this direction, two approximate algorithms are proposed (AHBA and AMSA). AHBA offers probabilistic guarantees regarding the accuracy of the result based on the Hoeffding bound, whereas AMSA performs a more aggressive computation resulting in more efficient processing. Evaluation results, based on real-life and synthetic data sets, show the efficiency and scalability of our techniques.

#index 1755308
#* Improving Aggregate Recommendation Diversity Using Ranking-Based Techniques
#@ Gediminas Adomavicius;YoungOk Kwon
#t 2012
#c 7
#! Recommender systems are becoming increasingly important to individual users and businesses for providing personalized recommendations. However, while the majority of algorithms proposed in recommender systems literature have focused on improving recommendation accuracy (as exemplified by the recent Netflix Prize competition), other important aspects of recommendation quality, such as the diversity of recommendations, have often been overlooked. In this paper, we introduce and explore a number of item ranking techniques that can generate substantially more diverse recommendations across all users while maintaining comparable levels of recommendation accuracy. Comprehensive empirical evaluation consistently shows the diversity gains of the proposed techniques using several real-world rating data sets and different rating prediction algorithms.

#index 1755309
#* Synthesizing Ontology Alignment Methods Using the Max-Sum Algorithm
#@ Vassilis Spiliopoulos;George A. Vouros
#t 2012
#c 7
#! This paper addresses the problem of synthesizing ontology alignment methods by maximizing the social welfare within a group of interacting agents: Specifically, each agent is responsible for computing mappings concerning a specific ontology element, using a specific alignment method. Each agent interacts with other agents with whom it shares constraints concerning the validity of the mappings it computes. Interacting agents form a bipartite factor graph, composed of variable and function nodes, representing alignment decisions and utilities, respectively. Agents need to reach an agreement to the mapping of the ontology elements consistently to the semantics of specifications with respect to their mapping preferences. Addressing the synthesis problem in such a way allows us to use an extension of the max-sum algorithm to generate near-to-optimal solutions to the alignment of ontologies through local decentralized message passing. We show the potential of such an approach by synthesizing a number of alignment methods, studying their performance in the OAEI benchmark series.

#index 1755310
#* Understanding Errors in Approximate Distributed Latent Dirichlet Allocation
#@ Alexander Ihler;David Newman
#t 2012
#c 7
#! Latent Dirichlet allocation (LDA) is a popular algorithm for discovering semantic structure in large collections of text or other data. Although its complexity is linear in the data size, its use on increasingly massive collections has created considerable interest in parallel implementations. “Approximate distributed” LDA, or AD-LDA, approximates the popular collapsed Gibbs sampling algorithm for LDA models while running on a distributed architecture. Although this algorithm often appears to perform well in practice, its quality is not well understood theoretically or easily assessed on new data. In this work, we theoretically justify the approximation, and modify AD-LDA to track an error bound on performance. Specifically, we upper bound the probability of making a sampling error at each step of the algorithm (compared to an exact, sequential Gibbs sampler), given the samples drawn thus far. We show empirically that our bound is sufficiently tight to give a meaningful and intuitive measure of approximation error in AD-LDA, allowing the user to track the tradeoff between accuracy and efficiency while executing in parallel.

#index 1755322
#* A Knowledge-Driven Approach to Activity Recognition in Smart Homes
#@ Liming Chen;Chris D. Nugent;Hui Wang
#t 2012
#c 7
#! This paper introduces a knowledge-driven approach to real-time, continuous activity recognition based on multisensor data streams in smart homes. The approach goes beyond the traditional data-centric methods for activity recognition in three ways. First, it makes extensive use of domain knowledge in the life cycle of activity recognition. Second, it uses ontologies for explicit context and activity modeling and representation. Third and finally, it exploits semantic reasoning and classification for activity inferencing, thus enabling both coarse-grained and fine-grained activity recognition. In this paper, we analyze the characteristics of smart homes and Activities of Daily Living (ADL) upon which we built both context and ADL ontologies. We present a generic system architecture for the proposed knowledge-driven approach and describe the underlying ontology-based recognition process. Special emphasis is placed on semantic subsumption reasoning algorithms for activity recognition. The proposed approach has been implemented in a function-rich software system, which was deployed in a smart home research laboratory. We evaluated the proposed approach and the developed system through extensive experiments involving a number of various ADL use scenarios. An average activity recognition rate of 94.44 percent was achieved and the average recognition runtime per recognition operation was measured as 2.5 seconds.

#index 1755323
#* A Unified Probabilistic Framework for Name Disambiguation in Digital Library
#@ Jie Tang;Alvis C. M. Fong;Bo Wang;Jing Zhang
#t 2012
#c 7
#! Despite years of research, the name ambiguity problem remains largely unresolved. Outstanding issues include how to capture all information for name disambiguation in a unified approach, and how to determine the number of people K in the disambiguation process. In this paper, we formalize the problem in a unified probabilistic framework, which incorporates both attributes and relationships. Specifically, we define a disambiguation objective function for the problem and propose a two-step parameter estimation algorithm. We also investigate a dynamic approach for estimating the number of people K. Experiments show that our proposed framework significantly outperforms four baseline methods of using clustering algorithms and two other previous methods. Experiments also indicate that the number K automatically found by our method is close to the actual number.

#index 1755324
#* Using Rule Ontology in Repeated Rule Acquisition from Similar Web Sites
#@ Sangun Park;Juyoung Kang
#t 2012
#c 7
#! Inferential rules are as essential to the Semantic Web applications as ontology. Therefore, rule acquisition is also an important issue, and the Web that implies inferential rules can be a major source of rule acquisition. We expect that it will be easier to acquire rules from a site by using similar rules of other sites in the same domain rather than starting from scratch. We proposed an automatic rule acquisition procedure using a rule ontology RuleToOnto, which represents information about the rule components and their structures. The rule acquisition procedure consists of the rule component identification step and the rule composition step. We developed A* algorithm for the rule composition and we performed experiments demonstrating that our ontology-based rule acquisition approach works in a real-world application.

#index 1755325
#* Clustering with Multiviewpoint-Based Similarity Measure
#@ Duc Thang Nguyen;Lihui Chen;Chee Keong Chan
#t 2012
#c 7
#! All clustering methods have to assume some cluster relationship among the data objects that they are applied on. Similarity between a pair of objects can be defined either explicitly or implicitly. In this paper, we introduce a novel multiviewpoint-based similarity measure and two related clustering methods. The major difference between a traditional dissimilarity/similarity measure and ours is that the former uses only a single viewpoint, which is the origin, while the latter utilizes many different viewpoints, which are objects assumed to not be in the same cluster with the two objects being measured. Using multiple viewpoints, more informative assessment of similarity could be achieved. Theoretical analysis and empirical study are conducted to support this claim. Two criterion functions for document clustering are proposed based on this new measure. We compare them with several well-known clustering algorithms that use other popular similarity measures on various document collections to verify the advantages of our proposal.

#index 1755326
#* Document Clustering in Correlation Similarity Measure Space
#@ Taiping Zhang;Yuan Yan Tang;Bin Fang;Yong Xiang
#t 2012
#c 7
#! This paper presents a new spectral clustering method called correlation preserving indexing (CPI), which is performed in the correlation similarity measure space. In this framework, the documents are projected into a low-dimensional semantic space in which the correlations between the documents in the local patches are maximized while the correlations between the documents outside these patches are minimized simultaneously. Since the intrinsic geometrical structure of the document space is often embedded in the similarities between the documents, correlation as a similarity measure is more suitable for detecting the intrinsic geometrical structure of the document space than euclidean distance. Consequently, the proposed CPI method can effectively discover the intrinsic structures embedded in high-dimensional document space. The effectiveness of the new method is demonstrated by extensive experiments conducted on various data sets and by comparison with existing document clustering methods.

#index 1755327
#* Efficient Extended Boolean Retrieval
#@ Stefan Pohl;Alistair Moffat;Justin Zobel
#t 2012
#c 7
#! Extended Boolean retrieval (EBR) models were proposed nearly three decades ago, but have had little practical impact, despite their significant advantages compared to either ranked keyword or pure Boolean retrieval. In particular, EBR models produce meaningful rankings; their query model allows the representation of complex concepts in an and-or format; and they are scrutable, in that the score assigned to a document depends solely on the content of that document, unaffected by any collection statistics or other external factors. These characteristics make EBR models attractive in domains typified by medical and legal searching, where the emphasis is on iterative development of reproducible complex queries of dozens or even hundreds of terms. However, EBR is much more computationally expensive than the alternatives. We consider the implementation of the p-norm approach to EBR, and demonstrate that ideas used in the max-score and wand exact optimization techniques for ranked keyword retrieval can be adapted to allow selective bypass of documents via a low-cost screening process for this and similar retrieval models. We also propose term-independent bounds that are able to further reduce the number of score calculations for short, simple queries under the extended Boolean retrieval model. Together, these methods yield an overall saving from 50 to 80 percent of the evaluation cost on test queries drawn from biomedical search.

#index 1755328
#* Locally Discriminative Coclustering
#@ Lijun Zhang;Chun Chen;Jiajun Bu;Zhengguang Chen;Deng Cai;Jiawei Han
#t 2012
#c 7
#! Different from traditional one-sided clustering techniques, coclustering makes use of the duality between samples and features to partition them simultaneously. Most of the existing co-clustering algorithms focus on modeling the relationship between samples and features, whereas the intersample and interfeature relationships are ignored. In this paper, we propose a novel coclustering algorithm named Locally Discriminative Coclustering (LDCC) to explore the relationship between samples and features as well as the intersample and interfeature relationships. Specifically, the sample-feature relationship is modeled by a bipartite graph between samples and features. And we apply local linear regression to discovering the intrinsic discriminative structures of both sample space and feature space. For each local patch in the sample and feature spaces, a local linear function is estimated to predict the labels of the points in this patch. The intersample and interfeature relationships are thus captured by minimizing the fitting errors of all the local linear functions. In this way, LDCC groups strongly associated samples and features together, while respecting the local structures of both sample and feature spaces. Our experimental results on several benchmark data sets have demonstrated the effectiveness of the proposed method.

#index 1755329
#* Low-Rank Kernel Matrix Factorization for Large-Scale Evolutionary Clustering
#@ Lijun Wang;Manjeet Rege;Ming Dong;Yongsheng Ding
#t 2012
#c 7
#! Traditional clustering techniques are inapplicable to problems where the relationships between data points evolve over time. Not only is it important for the clustering algorithm to adapt to the recent changes in the evolving data, but it also needs to take the historical relationship between the data points into consideration. In this paper, we propose ECKF, a general framework for evolutionary clustering large-scale data based on low-rank kernel matrix factorization. To the best of our knowledge, this is the first work that clusters large evolutionary data sets by the amalgamation of low-rank matrix approximation methods and matrix factorization-based clustering. Since the low-rank approximation provides a compact representation of the original matrix, and especially, the near-optimal low-rank approximation can preserve the sparsity of the original data, ECKF gains computational efficiency and hence is applicable to large evolutionary data sets. Moreover, matrix factorization-based methods have been shown to effectively cluster high-dimensional data in text mining and multimedia data analysis. From a theoretical standpoint, we mathematically prove the convergence and correctness of ECKF, and provide detailed analysis of its computational efficiency (both time and space). Through extensive experiments performed on synthetic and real data sets, we show that ECKF outperforms the existing methods in evolutionary clustering.

#index 1755330
#* Mining Web Graphs for Recommendations
#@ Hao Ma;Irwin King;Michael R. Lyu
#t 2012
#c 7
#! As the exponential explosion of various contents generated on the Web, Recommendation techniques have become increasingly indispensable. Innumerable different kinds of recommendations are made on the Web every day, including movies, music, images, books recommendations, query suggestions, tags recommendations, etc. No matter what types of data sources are used for the recommendations, essentially these data sources can be modeled in the form of various types of graphs. In this paper, aiming at providing a general framework on mining Web graphs for recommendations, 1) we first propose a novel diffusion method which propagates similarities between different nodes and generates recommendations; 2) then we illustrate how to generalize different recommendation problems into our graph diffusion framework. The proposed framework can be utilized in many recommendation tasks on the World Wide Web, including query suggestions, tag recommendations, expert finding, image recommendations, image annotations, etc. The experimental analysis on large data sets shows the promising future of our work.

#index 1755331
#* Query Planning for Continuous Aggregation Queries over a Network of Data Aggregators
#@ Rajeev Gupta;Krithi Ramamritham
#t 2012
#c 7
#! Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some aggregation function over distributed data items, for example, to know value of portfolio for a client; or the AVG of temperatures sensed by a set of sensors. In these queries a client specifies a coherency requirement as part of the query. We present a low-cost, scalable technique to answer continuous aggregation queries using a network of aggregators of dynamic data items. In such a network of data aggregators, each data aggregator serves a set of data items at specific coherencies. Just as various fragments of a dynamic webpage are served by one or more nodes of a content distribution network, our technique involves decomposing a client query into subqueries and executing subqueries on judiciously chosen data aggregators with their individual subquery incoherency bounds. We provide a technique for getting the optimal set of subqueries with their incoherency bounds which satisfies client query's coherency requirement with least number of refresh messages sent from aggregators to the client. For estimating the number of refresh messages, we build a query cost model which can be used to estimate the number of messages required to satisfy the client specified incoherency bound. Performance results using real-world traces show that our cost-based query planning leads to queries being executed using less than one third the number of messages required by existing schemes.

#index 1755332
#* Scalable Learning of Collective Behavior
#@ Lei Tang;Xufei Wang;Huan Liu
#t 2012
#c 7
#! This study of collective behavior is to understand how individuals behave in a social networking environment. Oceans of data generated by social media like Facebook, Twitter, Flickr, and YouTube present opportunities and challenges to study collective behavior on a large scale. In this work, we aim to learn to predict collective behavior in social media. In particular, given information about some individuals, how can we infer the behavior of unobserved individuals in the same network? A social-dimension-based approach has been shown effective in addressing the heterogeneity of connections presented in social media. However, the networks in social media are normally of colossal size, involving hundreds of thousands of actors. The scale of these networks entails scalable learning of models for collective behavior prediction. To address the scalability issue, we propose an edge-centric clustering scheme to extract sparse social dimensions. With sparse social dimensions, the proposed approach can efficiently handle networks of millions of actors while demonstrating a comparable prediction performance to other nonscalable methods.

#index 1755333
#* Scalable Scheduling of Updates in Streaming Data Warehouses
#@ Lukasz Golab;Theodore Johnson;Vladislav Shkapenyuk
#t 2012
#c 7
#! We discuss update scheduling in streaming data warehouses, which combine the features of traditional data warehouses and data stream systems. In our setting, external sources push append-only data streams into the warehouse with a wide range of interarrival times. While traditional data warehouses are typically refreshed during downtimes, streaming warehouses are updated as new data arrive. We model the streaming warehouse update problem as a scheduling problem, where jobs correspond to processes that load new data into tables, and whose objective is to minimize data staleness over time (at time t, if a table has been updated with information up to some earlier time r, its staleness is t minus r). We then propose a scheduling framework that handles the complications encountered by a stream warehouse: view hierarchies and priorities, data consistency, inability to preempt updates, heterogeneity of update jobs caused by different interarrival times and data volumes among different sources, and transient overload. A novel feature of our framework is that scheduling decisions do not depend on properties of update jobs (such as deadlines), but rather on the effect of update jobs on data staleness. Finally, we present a suite of update scheduling algorithms and extensive simulation experiments to map out factors which affect their performance.

#index 1755334
#* Visual Role Mining: A Picture Is Worth a Thousand Roles
#@ Alessandro Colantonio;Roberto Di Pietro;Alberto Ocello;Nino Vincenzo Verde
#t 2012
#c 7
#! This paper offers a new role engineering approach to Role-Based Access Control (RBAC), referred to as visual role mining. The key idea is to graphically represent user-permission assignments to enable quick analysis and elicitation of meaningful roles. First, we formally define the problem by introducing a metric for the quality of the visualization. Then, we prove that finding the best representation according to the defined metric is a {\cal NP}-hard problem. In turn, we propose two algorithms: ADVISER and EXTRACT. The former is a heuristic used to best represent the user-permission assignments of a given set of roles. The latter is a fast probabilistic algorithm that, when used in conjunction with ADVISER, allows for a visual elicitation of roles even in absence of predefined roles. Besides being rooted in sound theory, our proposal is supported by extensive simulations run over real data. Results confirm the quality of the proposal and demonstrate its viability in supporting role engineering decisions.

#index 1755335
#* Weakly Supervised Joint Sentiment-Topic Detection from Text
#@ Chenghua Lin;Yulan He;Richard Everson;Stefan Ruger
#t 2012
#c 7
#! Sentiment analysis or opinion mining aims to use automated tools to detect subjective information such as opinions, attitudes, and feelings expressed in text. This paper proposes a novel probabilistic modeling framework called joint sentiment-topic (JST) model based on latent Dirichlet allocation (LDA), which detects sentiment and topic simultaneously from text. A reparameterized version of the JST model called Reverse-JST, obtained by reversing the sequence of sentiment and topic generation in the modeling process, is also studied. Although JST is equivalent to Reverse-JST without a hierarchical prior, extensive experiments show that when sentiment priors are added, JST performs consistently better than Reverse-JST. Besides, unlike supervised approaches to sentiment classification which often fail to produce satisfactory performance when shifting to other domains, the weakly supervised nature of JST makes it highly portable to other domains. This is verified by the experimental results on data sets from five different domains where the JST model even outperforms existing semi-supervised approaches in some of the data sets despite using no labeled documents. Moreover, the topics and topic sentiment detected by JST are indeed coherent and informative. We hypothesize that the JST model can readily meet the demand of large-scale sentiment analysis from the web in an open-ended fashion.

#index 1755336
#* Software Fault Prediction Using Quad Tree-Based K-Means Clustering Algorithm
#@ Partha S. Bishnu;Vandana Bhattacherjee
#t 2012
#c 7
#! Unsupervised techniques like clustering may be used for fault prediction in software modules, more so in those cases where fault labels are not available. In this paper a Quad Tree-based K-Means algorithm has been applied for predicting faults in program modules. The aims of this paper are twofold. First, Quad Trees are applied for finding the initial cluster centers to be input to the K-Means Algorithm. An input threshold parameter \delta governs the number of initial cluster centers and by varying \delta the user can generate desired initial cluster centers. The concept of clustering gain has been used to determine the quality of clusters for evaluation of the Quad Tree-based initialization algorithm as compared to other initialization techniques. The clusters obtained by Quad Tree-based algorithm were found to have maximum gain values. Second, the Quad Tree-based algorithm is applied for predicting faults in program modules. The overall error rates of this prediction approach are compared to other existing algorithms and are found to be better in most of the cases.

#index 1755373
#* A Variational Bayesian Framework for Clustering with Multiple Graphs
#@ Motoki Shiga;Hiroshi Mamitsuka
#t 2012
#c 7
#! Mining patterns in graphs has become an important issue in real applications, such as bioinformatics and web mining. We address a graph clustering problem where a cluster is a set of densely connected nodes, under a practical setting that 1) the input is multiple graphs which share a set of nodes but have different edges and 2) a true cluster cannot be found in all given graphs. For this problem, we propose a probabilistic generative model and a robust learning scheme based on variational Bayesian estimation. A key feature of our probabilistic framework is that not only nodes but also given graphs can be clustered at the same time, allowing our model to capture clusters found in only part of all given graphs. We empirically evaluated the effectiveness of the proposed framework on not only a variety of synthetic graphs but also real gene networks, demonstrating that our proposed approach can improve the clustering performance of competing methods in both synthetic and real data.

#index 1755374
#* Anónimos: An LP-Based Approach for Anonymizing Weighted Social Network Graphs
#@ Sudipto Das;Omer Egecioglu;Amr El Abbadi
#t 2012
#c 7
#! The increasing popularity of social networks has initiated a fertile research area in information extraction and data mining. Anonymization of these social graphs is important to facilitate publishing these data sets for analysis by external entities. Prior work has concentrated mostly on node identity anonymization and structural anonymization. But with the growing interest in analyzing social networks as a weighted network, edge weight anonymization is also gaining importance. We present Anónimos, a Linear Programming-based technique for anonymization of edge weights that preserves linear properties of graphs. Such properties form the foundation of many important graph-theoretic algorithms such as shortest paths problem, k-nearest neighbors, minimum cost spanning tree, and maximizing information spread. As a proof of concept, we apply Anónimos to the shortest paths problem and its extensions, prove the correctness, analyze complexity, and experimentally evaluate it using real social network data sets. Our experiments demonstrate that Anónimos anonymizes the weights, improves k-anonymity of the weights, and also scrambles the relative ordering of the edges sorted by weights, thereby providing robust and effective anonymization of the sensitive edge-weights. We also demonstrate the composability of different models generated using Anónimos, a property that allows a single anonymized graph to preserve multiple linear properties.

#index 1755375
#* Cluster-Oriented Ensemble Classifier: Impact of Multicluster Characterization on Ensemble Classifier Learning
#@ Brijesh Verma;Ashfaqur Rahman
#t 2012
#c 7
#! This paper presents a novel cluster-oriented ensemble classifier. The proposed ensemble classifier is based on original concepts such as learning of cluster boundaries by the base classifiers and mapping of cluster confidences to class decision using a fusion classifier. The categorized data set is characterized into multiple clusters and fed to a number of distinctive base classifiers. The base classifiers learn cluster boundaries and produce cluster confidence vectors. A second level fusion classifier combines the cluster confidences and maps to class decisions. The proposed ensemble classifier modifies the learning domain for the base classifiers and facilitates efficient learning. The proposed approach is evaluated on benchmark data sets from UCI machine learning repository to identify the impact of multicluster boundaries on classifier learning and classification accuracy. The experimental results and two-tailed sign test demonstrate the superiority of the proposed cluster-oriented ensemble classifier over existing ensemble classifiers published in the literature.

#index 1755376
#* DDD: A New Ensemble Approach for Dealing with Concept Drift
#@ Leandro L. Minku;Xin Yao
#t 2012
#c 7
#! Online learning algorithms often have to operate in the presence of concept drifts. A recent study revealed that different diversity levels in an ensemble of learning machines are required in order to maintain high generalization on both old and new concepts. Inspired by this study and based on a further study of diversity with different strategies to deal with drifts, we propose a new online ensemble learning approach called Diversity for Dealing with Drifts (DDD). DDD maintains ensembles with different diversity levels and is able to attain better accuracy than other approaches. Furthermore, it is very robust, outperforming other drift handling approaches in terms of accuracy when there are false positive drift detections. In all the experimental comparisons we have carried out, DDD always performed at least as well as other drift handling approaches under various conditions, with very few exceptions.

#index 1755377
#* Fast Elastic Peak Detection for Mass Spectrometry Data Mining
#@ Xin Zhang;Dennis E. Shasha;Yang Song;Jason T.  L. Wang
#t 2012
#c 7
#! We study a data mining problem concerning the elastic peak detection in 2D liquid chromatography-mass spectrometry (LC-MS) data. These data can be modeled as time series, in which the X-axis represents time points and the Y-axis represents intensity values. A peak occurs in a set of 2D LC-MS data when the sum of the intensity values in a sliding time window exceeds a user-determined threshold. The elastic peak detection problem is to locate all peaks across multiple window sizes of interest in the data set. We propose a new data structure, called a Shifted Aggregation Tree or AggTree for short, and use the data structure to find the different peaks. Our method, called PeakID, solves the elastic peak detection problem in 2D LC-MS data yielding neither false positives nor false negatives. The method works by first constructing an AggTree in a bottom-up manner from the given data set, and then searching the AggTree for the peaks in a top-down manner. We describe a state-space algorithm for finding the topology and structure of an efficient AggTree to be used by PeakID. Our experimental results demonstrate the superiority of the proposed method over other methods on both synthetic and real-world data.

#index 1755378
#* Fuzzy Orders-of-Magnitude-Based Link Analysis for Qualitative Alias Detection
#@ Qiang Shen;Tossapon Boongoen
#t 2012
#c 7
#! Alias detection has been the significant subject being extensively studied for several domain applications, especially intelligence data analysis. Many preliminary methods rely on text-based measures, which are ineffective with false descriptions of terrorists' name, date-of-birth, and address. This barrier may be overcome through link information presented in relationships among objects of interests. Several numerical link-based similarity techniques have proven effective for identifying similar objects in the Internet and publication domains. However, as a result of exceptional cases with unduly high measure, these methods usually generate inaccurate similarity descriptions. Yet, they are either computationally inefficient or ineffective for alias detection with a single-property based model. This paper presents a novel orders-of-magnitude based similarity measure that integrates multiple link properties to refine the estimation process and derive semantic-rich similarity descriptions. The approach is based on order-of-magnitude reasoning with which the theory of fuzzy set is blended to provide quantitative semantics of descriptors and their unambiguous mathematical manipulation. With such explanatory formalism, analysts can validate the generated results and partly resolve the problem of false positives. It also allows coherent interpretation and communication within a decision-making group, using this computing-with-word capability. Its performance is evaluated over a terrorism-related data set, with further generalization over publication and email data collections.

#index 1755379
#* Holistic Top-k Simple Shortest Path Join in Graphs
#@ Jun Gao;Jeffrey Yu;Huida Qiu;Xiao Jiang;Tengjiao Wang;Dongqing Yang
#t 2012
#c 7
#! Motivated by the needs such as group relationship analysis, this paper introduces a new operation on graphs, named top-k path join, which discovers the top-k simple shortest paths between two given node sets. Rather than discovering the top-k simple paths between each node pair, this paper proposes a holistic join method which answers the top-k path join by finding constrained top-k simple shortest paths between two nodes, and then devises an efficient method to handle the latter problem. Specifically, we transform the graph by encoding the precomputed shortest paths to the target node, and use the transformed graph in the candidate path searching. We show that the candidate path searching on the transformed graph not only has the same result as that on the original graph but also can be terminated much earlier with the aid of precomputed results. We also discuss two other optimization strategies, including considering the join constraint in the candidate path generation as early as possible, and pruning search space in each candidate path generation with an adaptively determined threshold. The final extensive experimental results also show that our method offers a significant performance improvement over existing ones.

#index 1755380
#* Horizontal Aggregations in SQL to Prepare Data Sets for Data Mining Analysis
#@ Carlos Ordonez;Zhibo Chen
#t 2012
#c 7
#! Preparing a data set for analysis is generally the most time consuming task in a data mining project, requiring many complex SQL queries, joining tables, and aggregating columns. Existing SQL aggregations have limitations to prepare data sets because they return one column per aggregated group. In general, a significant manual effort is required to build data sets, where a horizontal layout is required. We propose simple, yet powerful, methods to generate SQL code to return aggregated columns in a horizontal tabular layout, returning a set of numbers instead of one number per row. This new class of functions is called horizontal aggregations. Horizontal aggregations build data sets with a horizontal denormalized layout (e.g., point-dimension, observation-variable, instance-feature), which is the standard layout required by most data mining algorithms. We propose three fundamental methods to evaluate horizontal aggregations: CASE: Exploiting the programming CASE construct; SPJ: Based on standard relational algebra operators (SPJ queries); PIVOT: Using the PIVOT operator, which is offered by some DBMSs. Experiments with large tables compare the proposed query evaluation methods. Our CASE method has similar speed to the PIVOT operator and it is much faster than the SPJ method. In general, the CASE and PIVOT methods exhibit linear scalability, whereas the SPJ method does not.

#index 1755381
#* Optimizing Bloom Filter Settings in Peer-to-Peer Multikeyword Searching
#@ Hanhua Chen;Hai Jin;Lei Chen;Yunhao Liu;Lionel M. Ni
#t 2012
#c 7
#! Peer-to-Peer multikeyword searching requires distributed intersection/union operations across wide area networks, raising a large amount of traffic cost. Existing schemes commonly utilize Bloom Filters (BFs) encoding to effectively reduce the traffic cost during the intersection/union operations. In this paper, we address the problem of optimizing the settings of a BF. We show, through mathematical proof, that the optimal setting of BF in terms of traffic cost is determined by the statistical information of the involved inverted lists, not the minimized false positive rate as claimed by previous studies. Through numerical analysis, we demonstrate how to obtain optimal settings. To better evaluate the performance of this design, we conduct comprehensive simulations on TREC WT10G test collection and query logs of a major commercial web search engine. Results show that our design significantly reduces the search traffic and latency of the existing approaches.

#index 1755382
#* Manifold Adaptive Experimental Design for Text Categorization
#@ Deng Cai;Xiaofei He
#t 2012
#c 7
#! In many information processing tasks, labels are usually expensive and the unlabeled data points are abundant. To reduce the cost on collecting labels, it is crucial to predict which unlabeled examples are the most informative, i.e., improve the classifier the most if they were labeled. Many active learning techniques have been proposed for text categorization, such as {\rm SVM}_{Active} and Transductive Experimental Design. However, most of previous approaches try to discover the discriminant structure of the data space, whereas the geometrical structure is not well respected. In this paper, we propose a novel active learning algorithm which is performed in the data manifold adaptive kernel space. The manifold structure is incorporated into the kernel space by using graph Laplacian. This way, the manifold adaptive kernel space reflects the underlying geometry of the data. By minimizing the expected error with respect to the optimal classifier, we can select the most representative and discriminative data points for labeling. Experimental results on text categorization have demonstrated the effectiveness of our proposed approach.

#index 1755383
#* Mining Online Reviews for Predicting Sales Performance: A Case Study in the Movie Domain
#@ Xiaohui Yu;Yang Liu;Xiangji Huang;Aijun An
#t 2012
#c 7
#! Posting reviews online has become an increasingly popular way for people to express opinions and sentiments toward the products bought or services received. Analyzing the large volume of online reviews available would produce useful actionable knowledge that could be of economic values to vendors and other interested parties. In this paper, we conduct a case study in the movie domain, and tackle the problem of mining reviews for predicting product sales performance. Our analysis shows that both the sentiments expressed in the reviews and the quality of the reviews have a significant impact on the future sales performance of products in question. For the sentiment factor, we propose Sentiment PLSA (S-PLSA), in which a review is considered as a document generated by a number of hidden sentiment factors, in order to capture the complex nature of sentiments. Training an S-PLSA model enables us to obtain a succinct summary of the sentiment information embedded in the reviews. Based on S-PLSFA, we propose ARSA, an Autoregressive Sentiment-Aware model for sales prediction. We then seek to further improve the accuracy of prediction by considering the quality factor, with a focus on predicting the quality of a review in the absence of user-supplied indicators, and present ARSQA, an Autoregressive Sentiment and Quality Aware model, to utilize sentiments and quality for predicting product sales performance. Extensive experiments conducted on a large movie data set confirm the effectiveness of the proposed approach.

#index 1755384
#* Practical Efficient String Mining
#@ Jasbir Dhaliwal;Simon J. Puglisi;Andrew Turpin
#t 2012
#c 7
#! In recent years, several algorithms for mining frequent and emerging substring patterns from databases of string data (such as proteins and natural language texts) have been discovered, all of which traverse an enhanced suffix array data structure. All of these algorithms lie at either extreme of the efficiency spectrum; they are either fast and use enormous amounts of space, or they are compact and orders of magnitude slower. In this paper, we present an algorithm that achieves the best of both these extremes, having runtime comparable to the fastest published algorithms while using less space than the most space efficient ones. This excellent practical performance is underpinned by theoretical guarantees. Our main mechanism for keeping memory usage low is to build the enhanced suffix array incrementally, in blocks. Once built, a block is traversed to output patterns with required support before its space is reclaimed to be used for the next block.

#index 1755385
#* Ranking Model Adaptation for Domain-Specific Search
#@ Bo Geng;Linjun Yang;Chao Xu;Xian-Sheng Hua
#t 2012
#c 7
#! With the explosive emergence of vertical search domains, applying the broad-based ranking model directly to different domains is no longer desirable due to domain differences, while building a unique ranking model for each domain is both laborious for labeling data and time consuming for training models. In this paper, we address these difficulties by proposing a regularization-based algorithm called ranking adaptation SVM (RA-SVM), through which we can adapt an existing ranking model to a new domain, so that the amount of labeled data and the training cost is reduced while the performance is still guaranteed. Our algorithm only requires the prediction from the existing ranking models, rather than their internal representations or the data from auxiliary domains. In addition, we assume that documents similar in the domain-specific feature space should have consistent rankings, and add some constraints to control the margin and slack variables of RA-SVM adaptively. Finally, ranking adaptability measurement is proposed to quantitatively estimate if an existing ranking model can be adapted to a new domain. Experiments performed over Letor and two large scale data sets crawled from a commercial search engine demonstrate the applicabilities of the proposed ranking adaptation algorithms and the ranking adaptability measurement.

#index 1755386
#* Tree-Based Mining for Discovering Patterns of Human Interaction in Meetings
#@ Zhiwen Yu;Zhiyong Yu;Xingshe Zhou;Christian Becker;Yuichi Nakamura
#t 2012
#c 7
#! Discovering semantic knowledge is significant for understanding and interpreting how people interact in a meeting discussion. In this paper, we propose a mining method to extract frequent patterns of human interaction based on the captured content of face-to-face meetings. Human interactions, such as proposing an idea, giving comments, and expressing a positive opinion, indicate user intention toward a topic or role in a discussion. Human interaction flow in a discussion session is represented as a tree. Tree-based interaction mining algorithms are designed to analyze the structures of the trees and to extract interaction flow patterns. The experimental results show that we can successfully extract several interesting patterns that are useful for the interpretation of human behavior in meeting discussions, such as determining frequent interactions, typical interaction flows, and relationships between different types of interactions.

#index 1847971
#* A Decision-Theoretic Framework for Numerical Attribute Value Reconciliation
#@ Zhengrui Jiang
#t 2012
#c 7
#! One of the major challenges of data integration is to resolve conflicting numerical attribute values caused by data heterogeneity. In addressing this problem, existing approaches proposed in prior literature often ignore such data inconsistencies or resolve them in an ad hoc manner. In this study, we propose a decision-theoretical framework that resolves numerical value conflicts in a systematic manner. The framework takes into consideration the consequences of incorrect numerical values and selects the value that minimizes the expected cost of errors for all data application problems under consideration. Experimental results show that significant savings can be achieved by adopting the proposed framework instead of ad hoc approaches.

#index 1847972
#* A Look-Ahead Approach to Secure Multiparty Protocols
#@ Mehmet Ercan Nergiz;Ercument Cicek;Thomas Pedersen;Yucel Saygin
#t 2012
#c 7
#! Secure multiparty protocols have been proposed to enable noncolluding parties to cooperate without a trusted server. Even though such protocols prevent information disclosure other than the objective function, they are quite costly in computation and communication. The high overhead motivates parties to estimate the utility that can be achieved as a result of the protocol beforehand. In this paper, we propose a look-ahead approach, specifically for secure multiparty protocols to achieve distributed k-anonymity, which helps parties to decide if the utility benefit from the protocol is within an acceptable range before initiating the protocol. The look-ahead operation is highly localized and its accuracy depends on the amount of information the parties are willing to share. Experimental results show the effectiveness of the proposed methods.

#index 1847973
#* Combining Tag and Value Similarity for Data Extraction and Alignment
#@ Weifeng Su;Jiying Wang;Frederick H. Lochovsky;Yi Liu
#t 2012
#c 7
#! Web databases generate query result pages based on a user's query. Automatically extracting the data from these query result pages is very important for many applications, such as data integration, which need to cooperate with multiple web databases. We present a novel data extraction and alignment method called CTVS that combines both tag and value similarity. CTVS automatically extracts data from query result pages by first identifying and segmenting the query result records (QRRs) in the query result pages and then aligning the segmented QRRs into a table, in which the data values from the same attribute are put into the same column. Specifically, we propose new techniques to handle the case when the QRRs are not contiguous, which may be due to the presence of auxiliary information, such as a comment, recommendation or advertisement, and for handling any nested structure that may exist in the QRRs. We also design a new record alignment algorithm that aligns the attributes in a record, first pairwise and then holistically, by combining the tag and data value similarity information. Experimental results show that CTVS achieves high precision and outperforms existing state-of-the-art data extraction methods.

#index 1847974
#* Continuous Detour Queries in Spatial Networks
#@ Sarana Nutanong;Egemen Tanin;Jie Shao;Rui Zhang;Ramamohanarao Kotagiri
#t 2012
#c 7
#! We study the problem of finding the shortest route between two locations that includes a stopover of a given type. An example scenario of this problem is given as follows: “On the way to Bob's place, Alice searches for a nearby take-away Italian restaurant to buy a pizza.” Assuming that Alice is interested in minimizing the total trip distance, this scenario can be modeled as a query where the current Alice's location (start) and Bob's place (destination) function as query points. Based on these two query points, we find the minimum detour object (MDO), i.e., a stopover that minimizes the sum of the distances: 1) from the start to the stopover, and 2) from the stopover to the destination. In a realistic location-based application environment, a user can be indecisive about committing to a particular detour option. The user may wish to browse multiple (k) MDOs before making a decision. Furthermore, when a user moves, the k{\rm MDO} results at one location may become obsolete. We propose a method for continuous detour query (CDQ) processing based on incremental construction of a shortest path tree. We conducted experimental studies to compare the performance of our proposed method against two methods derived from existing k-nearest neighbor querying techniques using real road-network data sets. Experimental results show that our proposed method significantly outperforms the two competitive techniques.

#index 1847975
#* Dense Subgraph Extraction with Application to Community Detection
#@ Jie Chen;Yousef Saad
#t 2012
#c 7
#! This paper presents a method for identifying a set of dense subgraphs of a given sparse graph. Within the main applications of this “dense subgraph problem,” the dense subgraphs are interpreted as communities, as in, e.g., social networks. The problem of identifying dense subgraphs helps analyze graph structures and complex networks and it is known to be challenging. It bears some similarities with the problem of reordering/blocking matrices in sparse matrix techniques. We exploit this link and adapt the idea of recognizing matrix column similarities, in order to compute a partial clustering of the vertices in a graph, where each cluster represents a dense subgraph. In contrast to existing subgraph extraction techniques which are based on a complete clustering of the graph nodes, the proposed algorithm takes into account the fact that not every participating node in the network needs to belong to a community. Another advantage is that the method does not require to specify the number of clusters; this number is usually not known in advance and is difficult to estimate. The computational process is very efficient, and the effectiveness of the proposed method is demonstrated in a few real-life examples.

#index 1847976
#* Dynamic In-Page Logging for B⁺-tree Index
#@ Gap-Joo Na;Sang-Won Lee;Bongki Moon
#t 2012
#c 7
#! Unlike database tables, {\rm B}^{+}-tree indexes are hierarchical and their structures change over time by node splitting operations, which may propagate changes from one node to another. The node splitting operation is difficult for the basic In-Page Logging (IPL) scheme to deal with, because it involves more than one node that may be stored separately in different flash blocks. In this paper, we propose DynamicIPLB^{+}\hbox{-}tree (d\hbox{-}IPLB^{+}\hbox{-}tree in short) as a variant of the IPL scheme tailored for flash-based {\rm B}^{+}-tree indexes. The d\hbox{-}IPLB^{+}\hbox{-}tree addresses the problem of frequent log overflow by allocating a log area in a flash block dynamically. It also avoids a page evaporation problem, imposed by the contemporary NAND flash chips, by introducing ghost nodes to d\hbox{-}IPLB^{+}\hbox{-}tree. This simple but elegant design of the d\hbox{-}IPLB^{+}\hbox{-}tree provides significant performance improvement over existing approaches. For a random insertion workload, the d\hbox{-}IPLB^{+}\hbox{-}tree outperformed a {\rm B}^{+}-tree with the plain IPL scheme by more than a factor of two in terms of page write and block erase operations.

#index 1847977
#* Efficient Computation of Range Aggregates against Uncertain Location-Based Queries
#@ Ying Zhang;Xuemin Lin;Yufei Tao;Wenjie Zhang;Haixun Wang
#t 2012
#c 7
#! In many applications, including location-based services, queries may not be precise. In this paper, we study the problem of efficiently computing range aggregates in a multidimensional space when the query location is uncertain. Specifically, for a query point Q whose location is uncertain and a set S of points in a multidimensional space, we want to calculate the aggregate (e.g., count, average and sum) over the subset S^{\prime } of S such that for each p \in S^{\prime }, Q has at least probability \theta within the distance \gamma to p. We propose novel, efficient techniques to solve the problem following the filtering-and-verification paradigm. In particular, two novel filtering techniques are proposed to effectively and efficiently remove data points from verification. Our comprehensive experiments based on both real and synthetic data demonstrate the efficiency and scalability of our techniques.

#index 1847978
#* Energy-Efficient Reverse Skyline Query Processing over Wireless Sensor Networks
#@ Guoren Wang;Junchang Xin;Lei Chen;Yunhao Liu
#t 2012
#c 7
#! Reverse skyline query plays an important role in many sensing applications, such as environmental monitoring, habitat monitoring, and battlefield monitoring. Due to the limited power supplies of wireless sensor nodes, the existing centralized approaches, which do not consider energy efficiency, cannot be directly applied to the distributed sensor environment. In this paper, we investigate how to process reverse skyline queries energy efficiently in wireless sensor networks. Initially, we theoretically analyzed the properties of reverse skyline query and proposed a skyband-based approach to tackle the problem of reverse skyline query answering over wireless sensor networks. Then, an energy-efficient approach is proposed to minimize the communication cost among sensor nodes of evaluating range reverse skyline query. Moreover, optimization mechanisms to improve the performance of multiple reverse skylines are also discussed. Extensive experiments on both real-world data and synthetic data have demonstrated the efficiency and effectiveness of our proposed approaches with various experimental settings.

#index 1847979
#* Evaluating Path Queries over Frequently Updated Route Collections
#@ Panagiotis Bouros;Dimitris Sacharidis;Theodore Dalamagas;Spiros Skiadopoulos;Timos Sellis
#t 2012
#c 7
#! The recent advances in the infrastructure of Geographic Information Systems (GIS), and the proliferation of GPS technology, have resulted in the abundance of geodata in the form of sequences of points of interest (POIs), waypoints, etc. We refer to sets of such sequences as route collections. In this work, we consider path queries on frequently updated route collections: given a route collection and two points n_s and n_t, a path query returns a path, i.e., a sequence of points, that connects n_s to n_t. We introduce two path query evaluation paradigms that enjoy the benefits of search algorithms (i.e., fast index maintenance) while utilizing transitivity information to terminate the search sooner. Efficient indexing schemes and appropriate updating procedures are introduced. An extensive experimental evaluation verifies the advantages of our methods compared to conventional graph-based search.

#index 1847980
#* Model-Based Method for Projective Clustering
#@ Lifei Chen;Qingshan Jiang;Shengrui Wang
#t 2012
#c 7
#! Clustering high-dimensional data is a major challenge due to the curse of dimensionality. To solve this problem, projective clustering has been defined as an extension to traditional clustering that attempts to find projected clusters in subsets of the dimensions of a data space. In this paper, a probability model is first proposed to describe projected clusters in high-dimensional data space. Then, we present a model-based algorithm for fuzzy projective clustering that discovers clusters with overlapping boundaries in various projected subspaces. The suitability of the proposal is demonstrated in an empirical study done with synthetic data set and some widely used real-world data set.

#index 1847981
#* Optimizing the Calculation of Conditional Probability Tables in Hybrid Bayesian Networks Using Binary Factorization
#@ Martin Neil;Xiaoli Chen;Norman Fenton
#t 2012
#c 7
#! Reducing the computational complexity of inference in Bayesian Networks (BNs) is a key challenge. Current algorithms for inference convert a BN to a junction tree structure made up of clusters of the BN nodes and the resulting complexity is time exponential in the size of a cluster. The need to reduce the complexity is especially acute where the BN contains continuous nodes. We propose a new method for optimizing the calculation of Conditional Probability Tables (CPTs) involving continuous nodes, approximated in Hybrid Bayesian Networks (HBNs), using an approximation algorithm called dynamic discretization. We present an optimized solution to this problem involving binary factorization of the arithmetical expressions declared to generate the CPTs for continuous nodes for deterministic functions and statistical distributions. The proposed algorithm is implemented and tested in a commercial Hybrid Bayesian Network software package and the results of the empirical evaluation show significant performance improvement over unfactorized models.

#index 1847982
#* Segmentation and Sampling of Moving Object Trajectories Based on Representativeness
#@ Costas Panagiotakis;Nikos Pelekis;Ioannis Kopanakis;Emmanuel Ramasso;Yannis Theodoridis
#t 2012
#c 7
#! Moving Object Databases (MOD), although ubiquitous, still call for methods that will be able to understand, search, analyze, and browse their spatiotemporal content. In this paper, we propose a method for trajectory segmentation and sampling based on the representativeness of the (sub)trajectories in the MOD. In order to find the most representative subtrajectories, the following methodology is proposed. First, a novel global voting algorithm is performed, based on local density and trajectory similarity information. This method is applied for each segment of the trajectory, forming a local trajectory descriptor that represents line segment representativeness. The sequence of this descriptor over a trajectory gives the voting signal of the trajectory, where high values correspond to the most representative parts. Then, a novel segmentation algorithm is applied on this signal that automatically estimates the number of partitions and the partition borders, identifying homogenous partitions concerning their representativeness. Finally, a sampling method over the resulting segments yields the most representative subtrajectories in the MOD. Our experimental results in synthetic and real MOD verify the effectiveness of the proposed scheme, also in comparison with other sampling techniques.

#index 1847983
#* Saturn: Range Queries, Load Balancing and Fault Tolerance in DHT Data Systems
#@ Theoni Pitoura;Nikos Ntarmos;Peter Triantafillou
#t 2012
#c 7
#! In this paper, we present Saturn, an overlay architecture for large-scale data networks maintained over Distributed Hash Tables (DHTs) that efficiently processes range queries and ensures access load balancing and fault-tolerance. Placing consecutive data values in neighboring peers is desirable in DHTs since it accelerates range query processing; however, such a placement is highly susceptible to load imbalances. At the same time, DHTs may be susceptible to node departures/failures and high data availability and fault tolerance are significant issues. Saturn deals effectively with these problems through the introduction of a novel multiple ring, order-preserving architecture. The use of a novel order-preserving hash function ensures fast range query processing. Replication across and within data rings (termed vertical and horizontal replication) forms the foundation over which our mechanisms are developed, ensuring query load balancing and fault tolerance, respectively. Our detailed experimentation study shows strong gains in range query processing efficiency, access load balancing, and fault tolerance, with low replication overheads. The significance of Saturn is not only that it effectively tackles all three issues together—i.e., supporting range queries, ensuring load balancing, and providing fault tolerance over DHTs—but also that it can be applied on top of any order-preserving DHT enabling it to dynamically handle replication and, thus, to trade off replication costs for fair load distribution and fault tolerance.

#index 1848054
#* A Performance Anomaly Detection and Analysis Framework for DBMS Development
#@ Donghun Lee;Sang K. Cha;Arthur H. Lee
#t 2012
#c 7
#! Detecting performance anomalies and finding their root causes are tedious tasks requiring much manual work. Functionality enhancements in DBMS development as in most software development often introduce performance problems in addition to bugs. To detect the problems as soon as they are introduced, which often happens during the early phases of a development cycle, we adopt performance regression testing early in the process. In this paper, we describe a framework that we developed to manage performance anomalies after establishing a set of conditions for a problem to be considered an anomaly. The framework uses Statistical Process Control (SPC) charts to detect performance anomalies and differential profiling to identify their root causes. By automating the tasks within the framework we were able to remove most of the manual overhead in detecting anomalies and reduce the analysis time for identifying the root causes by about 90 percent in most cases. The tools developed and deployed based on the framework allow us continuous, automated daily monitoring of performance in addition to the usual functionality monitoring in our DBMS development.

#index 1848055
#* Adding Temporal Constraints to XML Schema
#@ Faiz A. Currim;Sabah A. Currim;Curtis E. Dyreson;Richard T. Snodgrass;Stephen W. Thomas;Rui Zhang
#t 2012
#c 7
#! HASH(0x457fbbc)

#index 1848056
#* Coupled Behavior Analysis with Applications
#@ Longbing Cao;Yuming Ou;Philip S. Yu
#t 2012
#c 7
#! Coupled behaviors refer to the activities of one to many actors who are associated with each other in terms of certain relationships. With increasing network and community-based events and applications, such as group-based crime and social network interactions, behavior coupling contributes to the causes of eventual business problems. Effective approaches for analyzing coupled behaviors are not available, since existing methods mainly focus on individual behavior analysis. This paper discusses the problem of Coupled Behavior Analysis (CBA) and its challenges. A Coupled Hidden Markov Model (CHMM)-based approach is illustrated to model and detect abnormal group-based trading behaviors. The CHMM models cater for: 1) multiple behaviors from a group of people, 2) behavioral properties, 3) interactions among behaviors, customers, and behavioral properties, and 4) significant changes between coupled behaviors. We demonstrate and evaluate the models on order-book-level stock tick data from a major Asian exchange and demonstrate that the proposed CHMMs outperforms HMM-only for modeling a single sequence or combining multiple single sequences, without considering coupling relationships to detect anomalies. Finally, we discuss interaction relationships and modes between coupled behaviors, which are worthy of substantial study.

#index 1848057
#* Data Mining for XML Query-Answering Support
#@ Mirjana Mazuran;Elisa Quintarelli;Letizia Tanca
#t 2012
#c 7
#! Extracting information from semistructured documents is a very hard task, and is going to become more and more critical as the amount of digital information available on the Internet grows. Indeed, documents are often so large that the data set returned as answer to a query may be too big to convey interpretable knowledge. In this paper, we describe an approach based on Tree-Based Association Rules (TARs): mined rules, which provide approximate, intensional information on both the structure and the contents of Extensible Markup Language (XML) documents, and can be stored in XML format as well. This mined knowledge is later used to provide: 1) a concise idea—the gist—of both the structure and the content of the XML document and 2) quick, approximate answers to queries. In this paper, we focus on the second feature. A prototype system and experimental results demonstrate the effectiveness of the approach.

#index 1848058
#* Discovery of Delta Closed Patterns and Noninduced Patterns from Sequences
#@ Andrew K. C. Wong;Dennis Zhuang;Gary C. L. Li;En-Shiun Annie Lee
#t 2012
#c 7
#! Discovering patterns from sequence data has significant impact in many aspects of science and society, especially in genomics and proteomics. Here we consider multiple strings as input sequence data and substrings as patterns. In the real world, usually a large set of patterns could be discovered yet many of them are redundant, thus degrading the output quality. This paper improves the output quality by removing two types of redundant patterns. First, the notion of delta tolerance closed itemset is employed to remove redundant patterns that are not delta closed. Second, the concept of statistically induced patterns is proposed to capture redundant patterns which seem to be statistically significant yet their significance is induced by their strong significant subpatterns. It is computationally intense to mine these nonredundant patterns (delta closed patterns and noninduced patterns). To efficiently discover these patterns in very large sequence data, two efficient algorithms have been developed through innovative use of suffix tree. Three sets of experiments were conducted to evaluate their performance. They render excellent results when applying to genomics. The experiments confirm that the proposed algorithms are efficient and that they produce a relatively small set of patterns which reveal interesting information in the sequences.

#index 1848059
#* Discriminative Feature Selection by Nonparametric Bayes Error Minimization
#@ Shuang Hong Yang;Bao-Gang Hu
#t 2012
#c 7
#! Feature selection is fundamental to knowledge discovery from massive amount of high-dimensional data. In an effort to establish theoretical justification for feature selection algorithms, this paper presents a theoretically optimal criterion, namely, the discriminative optimal criterion (DoC) for feature selection. Compared with the existing representative optimal criterion (RoC, [CHECK END OF SENTENCE]) which retains maximum information for modeling the relationship between input and output variables, DoC is pragmatically advantageous because it attempts to directly maximize the classification accuracy and naturally reflects the Bayes error in the objective. To make DoC computationally tractable for practical tasks, we propose an algorithmic framework, which selects a subset of features by minimizing the Bayes error rate estimated by a nonparametric estimator. A set of existing algorithms as well as new ones can be derived naturally from this framework. As an example, we show that the Relief algorithm [CHECK END OF SENTENCE] greedily attempts to minimize the Bayes error estimated by the k-Nearest-Neighbor (kNN) method. This new interpretation insightfully reveals the secret behind the family of margin-based feature selection algorithms [CHECK END OF SENTENCE], [CHECK END OF SENTENCE] and also offers a principled way to establish new alternatives for performance improvement. In particular, by exploiting the proposed framework, we establish the Parzen-Relief (P-Relief) algorithm based on Parzen window estimator, and the MAP-Relief (M-Relief) which integrates label distribution into the max-margin objective to effectively handle imbalanced and multiclass data. Experiments on various benchmark data sets demonstrate the effectiveness of the proposed algorithms.

#index 1848060
#* EDISC: A Class-Tailored Discretization Technique for Rule-Based Classification
#@ Khurram Shehzad
#t 2012
#c 7
#! Abstract—Discretization is a critical component of data mining whereby continuous attributes of a data set are converted into discrete ones by creating intervals either before or during learning. There are many good reasons for preprocessing discretization, such as increased learning efficiency and classification accuracy, comprehensibility of data mining results, as well as the inherent limitation of a great majority of learning algorithms to handle only discrete data. Many preprocessing discretization techniques have been proposed to date, of which the Entropy-MDLP discretization has been accepted as by far the most effective in the context of both decision tree learning and rule induction algorithms. This paper presents a new discretization technique EDISC which utilizes the entropy-based principle but takes a class-tailored approach to discretization. The technique is applicable in general to any covering algorithm, including those that use the class-per-class rule induction methodology such as CN2 as well as those that use a seed example during the learning phase, such as the RULES family. Experimental evaluation has proved the efficiency and effectiveness of the technique as a preprocessing discretization procedure for CN2 as well as RULES-7, the latest algorithm among the RULES family of inductive learning algorithms.

#index 1848061
#* Efficient and Progressive Algorithms for Distributed Skyline Queries over Uncertain Data
#@ Xiaofeng Ding;Hai Jin
#t 2012
#c 7
#! The skyline operator has received considerable attention from the database community, due to its importance in many applications including multicriteria decision making, preference answering, and so forth. In many applications where uncertain data are inherently exist, i.e., data collected from different sources in distributed locations are usually with imprecise measurements, and thus exhibit kind of uncertainty. Taking into account the network delay and economic cost associated with sharing and communicating large amounts of distributed data over an internet, an important problem in this scenario is to retrieve the global skyline tuples from all the distributed local sites with minimum communication cost. Based on the well-known notation of the probabilistic skyline query over centralized uncertain data, in this paper, we propose the notation of distributed skyline queries over uncertain data. Furthermore, two communication- and computation-efficient algorithms are proposed to retrieve the qualified skylines from distributed local sites. Extensive experiments have been conducted to verify the efficiency, the effectiveness and the progressiveness of our algorithms with both the synthetic and real data sets.

#index 1848062
#* Energy Efficient Schemes for Accuracy-Guaranteed Sensor Data Aggregation Using Scalable Counting
#@ Yao-Chung Fan;Arbee L. P. Chen
#t 2012
#c 7
#! Sensor networks have received considerable attention in recent years, and are employed in many applications. In these applications, statistical aggregates such as Sum over the readings of a group of sensor nodes are often needed. One challenge for computing sensor data aggregates comes from the communication failures, which are common in sensor networks. To enhance the robustness of the aggregate computation, multipath-based aggregation is often used. However, the multipath-based aggregation suffers from the problem of overcounting sensor readings. The approaches using the multipath-based aggregation therefore need to incorporate techniques that avoid overcounting sensor readings. In this paper, we present a novel technique named scalable counting for efficiently avoiding the overcounting problem. We focus on having an (\varepsilon, \delta) accuracy guarantee for computing an aggregate, which ensures that the error in computing the aggregate is within a factor of \varepsilon with probability (1 - \delta). Our schemes using the scalable counting technique efficiently compute the aggregates under a given accuracy guarantee. We provide theoretical analyses that show the advantages of the scalable counting technique over previously proposed techniques. Furthermore, extensive experiments are made to validate the theoretical results and manifest the advantages of using the scalable counting technique for sensor data aggregation.

#index 1848063
#* Extending Recommender Systems for Disjoint User/Item Sets: The Conference Recommendation Problem
#@ Mark Hornick;Pablo Tamayo
#t 2012
#c 7
#! In this paper, we describe the problem of recommending conference sessions to attendees and show how novel extensions to traditional model-based recommender systems, as suggested in Adomavicius and Tuzhilin [CHECK END OF SENTENCE], can address this problem. We introduce Recommendation Engine by CONjoint Decomposition of ITems and USers (RECONDITUS)—a technique that is an extension of preference-based recommender systems to recommend items from a new disjoint set to users from a new disjoint set. The assumption being that preferences exhibited by users with known usage behavior (e.g., past conference session attendance), which can be abstracted by projections of user and item matrices, will be similar to ones of new (different) users where the basic environment and item domain are the same (e.g., new conference). RECONDITUS requires no item ratings, but operates on observed user behavior such as past conference session attendance. The RECONDITUS paradigm consists of projections of both user and item data, and the learning of relationships in projected space. Once established, the relationships enable predicting new relationships and provide associated recommendations. The approach can encompass several traditional data mining problems where both clustering and prediction are necessary. RECONDITUS has been evaluated using data from the Oracle OpenWorld conference.

#index 1848064
#* Maximum Ambiguity-Based Sample Selection in Fuzzy Decision Tree Induction
#@ Xi-Zhao Wang;Ling-Cai Dong;Jian-Hui Yan
#t 2012
#c 7
#! Sample selection is to select a number of representative samples from a large database such that a learning algorithm can have a reduced computational cost and an improved learning accuracy. This paper gives a new sample selection mechanism, i.e., the maximum ambiguity-based sample selection in fuzzy decision tree induction. Compared with the existing sample selection methods, this mechanism selects the samples based on the principle of maximal classification ambiguity. The major advantage of this mechanism is that the adjustment of the fuzzy decision tree is minimized when adding selected samples to the training set. This advantage is confirmed via the theoretical analysis of the leaf-nodes' frequency in the decision trees. The decision tree generated from the selected samples usually has a better performance than that from the original database. Furthermore, experimental results show that generalization ability of the tree based on our selection mechanism is far more superior to that based on random selection mechanism.

#index 1848065
#* Protecting Location Privacy against Location-Dependent Attacks in Mobile Services
#@ Xiao Pan;Jianliang Xu;Xiaofeng Meng
#t 2012
#c 7
#! Privacy protection has recently received considerable attention in location-based services. A large number of location cloaking algorithms have been proposed for protecting the location privacy of mobile users. In this paper, we consider the scenario where different location-based query requests are continuously issued by mobile users while they are moving. We show that most of the existing k-anonymity location cloaking algorithms are concerned with snapshot user locations only and cannot effectively prevent location-dependent attacks when users' locations are continuously updated. Therefore, adopting both the location k-anonymity and cloaking granularity as privacy metrics, we propose a new incremental clique-based cloaking algorithm, called ICliqueCloak, to defend against location-dependent attacks. The main idea is to incrementally maintain maximal cliques needed for location cloaking in an undirected graph that takes into consideration the effect of continuous location updates. Thus, a qualified clique can be quickly identified and used to generate the cloaked region when a new request arrives. The efficiency and effectiveness of the proposed ICliqueCloak algorithm are validated by a series of carefully designed experiments. The experimental results also show that the price paid for defending against location-dependent attacks is small.

#index 1848066
#* Shape Sensitive Geometric Monitoring
#@ Daniel Keren;Izchak Sharfman;Assaf Schuster;Avishay Livne
#t 2012
#c 7
#! An important problem in distributed, dynamic databases is to continuously monitor the value of a function defined on the nodes, and check that it satisfies some threshold constraint. We introduce a monitoring method, based on a geometric interpretation of the problem, which enables to define local constraints at the nodes. It is guaranteed that as long as none of these constraints is violated, the value of the function did not cross the threshold. We generalize previous work on geometric monitoring, and solve two problems which seriously hampered its performance: as opposed to the constraints used so far, which depend only on the current values of the local data, here we incorporate their temporal behavior. Also, the new constraints are tailored to the geometric properties of the specific monitored function. In addition, we extend the concept of safe zones for the monitoring problem, and show that previous work on geometric monitoring is a special case of the proposed extension. Experimental results on real data reveal that the new approach reduces communication by up to three orders of magnitude in comparison to existing approaches, and considerably narrows the gap between achievable results and a newly defined lower bound on communication complexity.

#index 1862868
#* 1996 Index IEEE Transactions on Knowledge and Data vol. 8
#@ 
#t 1996
#c 7
#! First Page of the Article

#index 1862869
#* An information retrieval approach for approximate queries
#@ P. P. Calado;B. Ribeiro-Neto
#t 2003
#c 7
#! First Page of the Article

#index 1890006
#* A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication
#@ Peter Christen
#t 2012
#c 7
#! Record linkage is the process of matching records from several databases that refer to the same entities. When applied on a single database, this process is known as deduplication. Increasingly, matched data are becoming important in many application areas, because they can contain information that is not available otherwise, or that is too costly to acquire. Removing duplicate records in a single database is a crucial step in the data cleaning process, because duplicates can severely influence the outcomes of any subsequent data processing or data mining. With the increasing size of today's databases, the complexity of the matching process becomes one of the major challenges for record linkage and deduplication. In recent years, various indexing techniques have been developed for record linkage and deduplication. They are aimed at reducing the number of record pairs to be compared in the matching process by removing obvious nonmatching pairs, while at the same time maintaining high matching quality. This paper presents a survey of 12 variations of 6 indexing techniques. Their complexity is analyzed, and their performance and scalability is evaluated within an experimental framework using both synthetic and real data sets. No such detailed survey has so far been published.

#index 1890007
#* Distributed Line Graphs: A Universal Technique for Designing DHTs Based on Arbitrary Regular Graphs
#@ Yiming Zhang;Ling Liu
#t 2012
#c 7
#! Most proposed DHTs engage certain topology maintenance mechanisms specific to the static graphs on which they are based. The designs of these mechanisms are complicated and repeated with graph-relevant concerns. In this paper, we propose the “distributed line graphs” (DLG), a universal technique for designing DHTs based on arbitrary regular graphs. Using DLG, the main features of the initial graphs are preserved, and thus people can design a new DHT by simply choosing the graph with desirable features and applying DLG to it. We demonstrate the power of DLG by illustrating four DLG-enabled DHTs based on different graphs, namely, Kautz, de Bruijn, butterfly, and hypertree graphs. The effectiveness of our proposals is demonstrated through analysis, simulation, and implementation.

#index 1890008
#* Efficient Iceberg Query Evaluation Using Compressed Bitmap Index
#@ Bin He;Hui-I Hsiao;Ziyang Liu;Yu Huang;Yi Chen
#t 2012
#c 7
#! Decision support and knowledge discovery systems often compute aggregate values of interesting attributes by processing a huge amount of data in very large databases and/or warehouses. In particular, iceberg query is a special type of aggregation query that computes aggregate values above a user-provided threshold. Usually, only a small number of results will satisfy the threshold constraint. Yet, the results often carry very important and valuable business insights. Because of the small result set, iceberg queries offer many opportunities for deep query optimization. However, most existing iceberg query processing algorithms do not take advantage of the small-result-set property and rely heavily on the tuple-scan-based approach. This incurs intensive disk accesses and computation, resulting in long processing time especially when data size is large. Bitmap index, which builds one bitmap vector for each attribute value, is gaining popularity in both column-oriented and row-oriented databases in recent years. It occupies less space than the raw data and gives opportunities for more efficient query processing. In this paper, we exploited the property of bitmap index and developed a very effective bitmap pruning strategy for processing iceberg queries. Our index-pruning-based approach eliminates the need of scanning and processing the entire data set (table) and thus speeds up the iceberg query processing significantly. Experiments show that our approach is much more efficient than existing algorithms commonly used in row-oriented and column-oriented databases.

#index 1890009
#* Efficient Multidimensional Fuzzy Search for Personal Information Management Systems
#@ Wei Wang;Christopher Peery;Amelie Marian;Thu D. Nguyen
#t 2012
#c 7
#! With the explosion in the amount of semistructured data users access and store in personal information management systems, there is a critical need for powerful search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools typically support some IR-style ranking on the textual part of the query, but only consider structure (e.g., file directory) and metadata (e.g., date, file type) as filtering conditions. We propose a novel multidimensional search approach that allows users to perform fuzzy searches for structure and metadata conditions in addition to keyword conditions. Our techniques individually score each dimension and integrate the three dimension scores into a meaningful unified score. We also design indexes and algorithms to efficiently identify the most relevant files that match multidimensional queries. We perform a thorough experimental evaluation of our approach and show that our relaxation and scoring framework for fuzzy query conditions in noncontent dimensions can significantly improve ranking accuracy. We also show that our query processing strategies perform and scale well, making our fuzzy search approach practical for every day usage.

#index 1890010
#* Enabling Multilevel Trust in Privacy Preserving Data Mining
#@ Yaping Li;Minghua Chen;Qiwei Li;Wei Zhang
#t 2012
#c 7
#! Privacy Preserving Data Mining (PPDM) addresses the problem of developing accurate models about aggregated data without access to precise information in individual data record. A widely studied perturbation-based PPDM approach introduces random perturbation to individual values to preserve privacy before data are published. Previous solutions of this approach are limited in their tacit assumption of single-level trust on data miners. In this work, we relax this assumption and expand the scope of perturbation-based PPDM to Multilevel Trust (MLT-PPDM). In our setting, the more trusted a data miner is, the less perturbed copy of the data it can access. Under this setting, a malicious data miner may have access to differently perturbed copies of the same data through various means, and may combine these diverse copies to jointly infer additional information about the original data that the data owner does not intend to release. Preventing such diversity attacks is the key challenge of providing MLT-PPDM services. We address this challenge by properly correlating perturbation across copies at different trust levels. We prove that our solution is robust against diversity attacks with respect to our privacy goal. That is, for data miners who have access to an arbitrary collection of the perturbed copies, our solution prevent them from jointly reconstructing the original data more accurately than the best effort using any individual copy in the collection. Our solution allows a data owner to generate perturbed copies of its data for arbitrary trust levels on-demand. This feature offers data owners maximum flexibility.

#index 1890011
#* Evaluating Tag-Based Preference Obfuscation Systems
#@ Andreas Pashalidis;Bart Preneel
#t 2012
#c 7
#! While personalization is key to increase the usability of online services, disclosing one's preferences is undesirable from a privacy perspective, because it enables profiling through the linkage of what may otherwise be unlinkable service invocations. This paper considers an easily implementable class of obfuscation strategies as a means to mitigate these risks, and examines its privacy/utility tradeoff. Our results are based on simulations that take place within a modular evaluation framework that can seamlessly accommodate real-world data. We conducted experiments with different simulated behaviors and using two preference populations, namely a population of maximally diverse preferences and one consisting of the movie preferences of some Netflix users. We measure utility in a way that is specific to the application of preference obfuscation. Privacy is measured in terms of unlinkability, with respect to two different adversaries. Our results show that reasonable privacy/utility tradeoffs require the disclosure of only small amounts of preference information.

#index 1890012
#* Flag Commit: Supporting Efficient Transaction Recovery in Flash-Based DBMSs
#@ Sai Tung On;Jianliang Xu;Byron Choi;Haibo Hu;Bingsheng He
#t 2012
#c 7
#! Owing to recent advances in semiconductor technologies, flash disks have been a competitive alternative to traditional magnetic disks as external storage media. In this paper, we study how transaction recovery can be efficiently supported in database management systems (dbmss) running on slc flash disks. Inspired by the classical shadow-paging approach, we propose a new commit scheme, called flagcommit, to exploit the unique characteristics of flash disks such as fast random read access, out-place updating, and partial page programming. To minimize the need of writing log records, we embed the transaction status into flash pages through a chain of commit flags. Based on flagcommit, we develop two recovery protocols, namely commit-based flag commit (cfc) and abort-based flag commit (afc), to meet different performance needs. They are flexible to support no-force buffer management and fine-grained concurrency control. Our performance evaluation based on the tpc-c benchmark shows that both cfc and afc outperform the state-of-the-art recovery protocols.

#index 1890013
#* Indexing Uncertain Data in General Metric Spaces
#@ Fabrizio Angiulli;Fabio Fassetti
#t 2012
#c 7
#! In this study, we deal with the problem of efficiently answering range queries over uncertain objects in a general metric space. In this study, an uncertain object is an object that always exists but its actual value is uncertain and modeled by a multivariate probability density function. As a major contribution, this is the first work providing an effective technique for indexing uncertain objects coming from general metric spaces. We generalize the reverse triangle inequality to the probabilistic setting in order to exploit it as a discard condition. Then, we introduce a novel pivot-based indexing technique, called UP-index, and show how it can be employed to speed up range query computation. Importantly, the candidate selection phase of our technique is able to noticeably reduce the set of candidates with little time requirements. Finally, we provide a criterion to measure the quality of a set of pivots and study the problem of selecting a good set of pivots according to the introduced criterion. We report some intractability results and then design an approximate algorithm with statistical guarantees for selecting pivots. Experimental results validate the effectiveness of the proposed approach and reveal that the introduced technique may be even preferable to indexing techniques specifically designed for the euclidean space.

#index 1890014
#* Mining Social Emotions from Affective Text
#@ Shenghua Bao;Shengliang Xu;Li Zhang;Rong Yan;Zhong Su;Dingyi Han;Yong Yu
#t 2012
#c 7
#! This paper is concerned with the problem of mining social emotions from text. Recently, with the fast development of web 2.0, more and more documents are assigned by social users with emotion labels such as happiness, sadness, and surprise. Such emotions can provide a new aspect for document categorization, and therefore help online users to select related documents based on their emotional preferences. Useful as it is, the ratio with manual emotion labels is still very tiny comparing to the huge amount of web/enterprise documents. In this paper, we aim to discover the connections between social emotions and affective terms and based on which predict the social emotion from text content automatically. More specifically, we propose a joint emotion-topic model by augmenting Latent Dirichlet Allocation with an additional layer for emotion modeling. It first generates a set of latent topics from emotions, followed by generating affective terms from each topic. Experimental results on an online news collection show that the proposed model can effectively identify meaningful latent topics for each emotion. Evaluation on emotion prediction further verifies the effectiveness of the proposed model.

#index 1890015
#* One Size Does Not Fit All: Toward User- and Query-Dependent Ranking for Web Databases
#@ Aditya Telang;Chengkai Li;Sharma Chakravarthy
#t 2012
#c 7
#! With the emergence of the deep web, searching web databases in domains such as vehicles, real estate, etc., has become a routine task. One of the problems in this context is ranking the results of a user query. Earlier approaches for addressing this problem have used frequencies of database values, query logs, and user profiles. A common thread in most of these approaches is that ranking is done in a user- and/or query-independent manner. This paper proposes a novel query- and user-dependent approach for ranking query results in web databases. We present a ranking model, based on two complementary notions of user and query similarity, to derive a ranking function for a given user query. This function is acquired from a sparse workload comprising of several such ranking functions derived for various user-query pairs. The model is based on the intuition that similar users display comparable ranking preferences over the results of similar queries. We define these similarities formally in alternative ways and discuss their effectiveness analytically and experimentally over two distinct web databases.

#index 1890016
#* Particle Competition and Cooperation in Networks for Semi-Supervised Learning
#@ Fabricio Breve;Liang Zhao;Marcos Quiles;Witold Pedrycz;Jiming Liu
#t 2012
#c 7
#! Semi-supervised learning is one of the important topics in machine learning, concerning with pattern classification where only a small subset of data is labeled. In this paper, a new network-based (or graph-based) semi-supervised classification model is proposed. It employs a combined random-greedy walk of particles, with competition and cooperation mechanisms, to propagate class labels to the whole network. Due to the competition mechanism, the proposed model has a local label spreading fashion, i.e., each particle only visits a portion of nodes potentially belonging to it, while it is not allowed to visit those nodes definitely occupied by particles of other classes. In this way, a “divide-and-conquer” effect is naturally embedded in the model. As a result, the proposed model can achieve a good classification rate while exhibiting low computational complexity order in comparison to other network-based semi-supervised algorithms. Computer simulations carried out for synthetic and real-world data sets provide a numeric quantification of the performance of the method.

#index 1890017
#* Toward Private Joins on Outsourced Data
#@ Bogdan Carbunar;Radu Sion
#t 2012
#c 7
#! In an outsourced database framework, clients place data management responsibilities with specialized service providers. Of essential concern in such frameworks is data privacy. Potential clients are reluctant to outsource sensitive data to a foreign party without strong privacy assurances beyond policy “fine prints.” In this paper, we introduce a mechanism for executing general binary JOIN operations (for predicates that satisfy certain properties) in an outsourced relational database framework with computational privacy and low overhead—the first, to the best of our knowledge. We illustrate via a set of relevant instances of JOIN predicates, including: range and equality (e.g., for geographical data), Hamming distance (e.g., for DNA matching), and semantics (i.e., in health-care scenarios—mapping antibiotics to bacteria). We experimentally evaluate the main overhead components and show they are reasonable. The initial client computation overhead for 100,000 data items is around 5 minutes and our privacy mechanisms can sustain theoretical throughputs of several million predicate evaluations per second, even for an unoptimized OpenSSL-based implementation.

#index 1890018
#* Using Graphics Processors for High Performance SimRank Computation
#@ Guoming He;Cuiping Li;Hong Chen;Xiaoyong Du;Haijun Feng
#t 2012
#c 7
#! Recently there has been a lot of interest in graph-based analysis. One of the most important aspects of graph-based analysis is to measure similarity between nodes in a graph. SimRank is a simple and influential measure of this kind, based on a solid graph theoretical model. However, existing methods on SimRank computation suffer from two limitations: 1) the computing cost can be very high in practice; and 2) they can only be applied on static graphs. In this paper, we exploit the inherent parallelism and high memory bandwidth of graphics processing units (GPU) to accelerate the computation of SimRank on large graphs. Furthermore, based on the observation that SimRank is essentially a first-order Markov Chain, we propose to utilize the iterative aggregation techniques for uncoupling Markov chains to compute SimRank scores in parallel for large graphs. The iterative aggregation method can be applied on dynamic graphs. Moreover, it can handle not only the link-updating problem but also the node-updating problem. We give the corresponding theoretical justification and analysis, propose three optimization strategies to further improve the computation efficiency, and extend the proposed algorithm to dynamic graphs. Extensive experiments on synthetic and real data sets verify that the proposed methods are efficient and effective.

#index 1903455
#* A Lightweight Algorithm for Message Type Extraction in System Application Logs
#@ Adetokunbo Makanju;A. Nur Zincir-Heywood;Evangelos E. Milios
#t 2012
#c 7
#! Message type or message cluster extraction is an important task in the analysis of system logs in computer networks. Defining these message types automatically facilitates the automatic analysis of system logs. When the message types that exist in a log file are represented explicitly, they can form the basis for carrying out other automatic application log analysis tasks. In this paper, we introduce a novel algorithm for carrying out message type extraction from event log files. IPLoM, which stands for Iterative Partitioning Log Mining, works through a 4-step process. The first three steps hierarchically partition the event log into groups of event log messages or event clusters. In its fourth and final stage, IPLoM produces a message type description or line format for each of the message clusters. IPLoM is able to find clusters in data irrespective of the frequency of its instances in the data, it scales gracefully in the case of long message type patterns and produces message type descriptions at a level of abstraction, which is preferred by a human observer. Evaluations show that IPLoM outperforms similar algorithms statistically significantly.

#index 1903456
#* A Lightweight Workbench for Database Benchmarking, Experimentation, and Implementation
#@ Xinyuan Zhao;Shashi K. Gadia
#t 2012
#c 7
#! We have developed a platform, called Cyclone Database Implementation Workbench (CyDIW), that can be used to implement new database prototypes, use existing command-based systems, and conduct experiments. The workbench allows seamless integration of multiple systems and provides useful services. To support database implementation page-based storage and buffer managers are built-in. A scripting language for batches of commands is included. Experiments are encapsulated as batches of commands on multiple systems. A simple and easy to use GUI is available that acts as an editor and a launchpad for execution of batches of commands. Emphasis in CyDIW is on simplifying the logistics surrounding setting up experiments that are comprehensive and self-contained. The benchmarking services in CyDIW can be used for lightweight benchmarking, where a benchmark consisting of a data set and a suite of commands is given. A benchmarking experiment collects performance statistics from multiple systems based on varying parameters and plots benchmarking results without leaving the GUI. Setup for the system is easy. All configuration settings are recorded in XML documents that are highly portable and readily visible. Once installed, batches representing experiments can be exchanged as text files and executed on CyDIW on any computer.

#index 1903457
#* An Information-Preserving Watermarking Scheme for Right Protection of EMR Systems
#@ Muhammad Kamran;Muddassar Farooq
#t 2012
#c 7
#! Recently, a significant amount of interest has been developed in motivating physicians to use e-health technology (especially Electronic Medical Records (EMR) systems). An important utility of such EMR systems is: a next generation of Clinical Decision Support Systems (CDSS) will extract knowledge from these electronic medical records to enable physicians to do accurate and effective diagnosis. It is anticipated that in future such medical records will be shared through cloud among different physicians to improve the quality of health care. Therefore, right protection of medical records is important to protect their ownership once they are shared with third parties. Watermarking is a proven well-known technique to achieve this objective. The challenges associated with watermarking of EMR systems are: 1) some fields in EMR are more relevant in the diagnosis process; as a result, small variations in them could change the diagnosis, and 2) a misdiagnosis might not only result in a life threatening scenario but also might lead to significant costs of the treatment for the patients. The major contribution of this paper is an information-preserving watermarking scheme to address the above-mentioned challenges. We model the watermarking process as a constrained optimization problem. We demonstrate, through experiments, that our scheme not only preserves the diagnosis accuracy but is also resilient to well-known attacks for corrupting the watermark. Last but not least, we also compare our scheme with a well-known threshold-based scheme to evaluate relative merits of a classifier. Our pilot studies reveal that—using proposed information-preserving scheme—the overall classification accuracy is never degraded by more than 1 percent. In comparison, the diagnosis accuracy, using the threshold-based technique, is degraded by more than 18 percent in a worst case scenario.

#index 1903458
#* An Unsupervised Approach for Person Name Bipolarization Using Principal Component Analysis
#@ Chien Chin Chen;Zhong-Yong Chen;Chen-Yuan Wu
#t 2012
#c 7
#! A topic is usually associated with a specific time, place, and person(s). Generally, topics that involve bipolar or competing viewpoints are attention getting and are thus reported in a large number of documents. Identifying the association between important persons mentioned in numerous topic documents would help readers comprehend topics more easily. In this paper, we propose an unsupervised approach for identifying bipolar person names in a set of topic documents. Specifically, we employ principal component analysis (PCA) to discover bipolar word usage patterns of person names in the documents, and show that the signs of the entries in the principal eigenvector of PCA partition the person names into bipolar groups spontaneously. To reduce the effect of data sparseness, we introduce two techniques, called the weighted correlation coefficient and off-topic block elimination. We also present a timeline system that shows the intensity and activeness development of the identified bipolar person groups. Empirical evaluations demonstrate the efficacy of the proposed approach in identifying bipolar person names in topic documents, while the generated timelines provide comprehensive storylines of topics.

#index 1903459
#* Cascading Spatio-Temporal Pattern Discovery
#@ Pradeep Mohan;Shashi Shekhar;James A. Shine;James P. Rogers
#t 2012
#c 7
#! Given a collection of Boolean spatiotemporal (ST) event-types, the cascading spatiotemporal pattern (CSTP) discovery process finds partially ordered subsets of these event-types whose instances are located together and occur serially. For example, analysis of crime data sets may reveal frequent occurrence of misdemeanors and drunk driving after and near bar closings on weekends, as well as after and near large gatherings such as football games. Discovering CSTPs from ST data sets is important for application domains such as public safety (e.g., identifying crime attractors and generators) and natural disaster planning, (e.g., preparing for hurricanes). However, CSTP discovery presents multiple challenges; three important ones are 1) the exponential cardinality of candidate patterns with respect to the number of event types, 2) computationally complex ST neighborhood enumeration required to evaluate the interest measure and 3) the difficulty of balancing computational complexity and statistical interpretation. Current approaches for ST data mining focus on mining totally ordered sequences or unordered subsets. In contrast, our recent work explores partially ordered patterns. Recently, we represented CSTPs as directed acyclic graphs (DAGs); proposed a new interest measure, the cascade participation index (CPI); outlined the general structure of a cascading spatiotemporal pattern miner (CSTPM); evaluated filtering strategies to enhance computational savings using a real-world crime data set and proposed a nested loop-based CSTPM to address the challenge posed by exponential cardinality of candidate patterns. This paper adds to our recent work by offering a new computational insight, namely, that the computational bottleneck for CSTP discovery lies in the interest measure evaluation. With this insight, we propose a new CSTPM based on spatiotemporal partitioning that significantly lowers the cost of interest measure evaluation. Analytical evaluation shows that our new CSTPM is correct and complete. Results from significant amount of new experimental evaluation with both synthetic and real data show that our new ST partitioning-based CSTPM outperforms the CSTPM from our previous work. We also present a case study that verifies the applicability of CSTP discovery process.

#index 1903460
#* Energy-Aware Set-Covering Approaches for Approximate Data Collection in Wireless Sensor Networks
#@ Chih-Chieh Hung;Wen-Chih Peng;Wang-Chien Lee
#t 2012
#c 7
#! To conserve energy, sensor nodes with similar readings can be grouped such that readings from only the representative nodes within the groups need to be reported. However, efficiently identifying sensor groups and their representative nodes is a very challenging task. In this paper, we propose a centralized algorithm to determine a set of representative nodes with high energy levels and wide data coverage ranges. Here, the data coverage range of a sensor node is considered to be the set of sensor nodes that have reading behaviors very close to the particular sensor node. To further reduce the extra cost incurred in messages for selection of representative nodes, a distributed algorithm is developed. Furthermore, maintenance mechanisms are proposed to dynamically select alternative representative nodes when the original representative nodes run low on energy, or cannot capture spatial correlation within their respective data coverage ranges. Using experimental studies on both synthesis and real data sets, our proposed algorithms are shown to effectively and efficiently provide approximate data collection while prolonging the network lifetime.

#index 1903461
#* Holistic Boolean-Twig Pattern Matching for Efficient XML Query Processing
#@ Dunren Che;Tok Wang Ling;Wen-Chi Hou
#t 2012
#c 7
#! Twig pattern matching is a critical operation for XML query processing, and the holistic computing approach has shown superior performance over other methods. Since Bruno et al. introduced the first holistic twig join algorithm, TwigStack, numerous so-called holistic twig join algorithms have been proposed. Yet practical XML queries often require support for more general twig patterns, such as the ones that allow arbitrary occurrences of an arbitrary number of logical connectives (AND, OR, and NOT); such types of twigs are referred to as B-twigs (i.e., Boolean-Twigs) or AND/OR/NOT-twigs. We have seen interesting work on generalizing the holistic twig join approach to AND/OR-twigs and AND/NOT-twigs, but have not seen any further effort addressing the problem of AND/OR/NOT-Twigs at the full scale, which therefore forms the main theme of this paper. In this paper, we investigate novel mechanisms for efficient B-twig pattern matching. In particular, we introduce “B-twig normalization” as an important first-step in our approach toward eventually conquering the complexity of B-twigs, and then present BTwigMerge—the first holistic twig join algorithm designed for B-twigs. Both analytical and experimental results show that BTwigMerge is optimal for B-twig patterns with AD (Ancestor-Descendant) edges and/or PC (Parent-Child) edges.

#index 1903462
#* Mining Distinction and Commonality across Multiple Domains Using Generative Model for Text Classification
#@ Fuzhen Zhuang;Ping Luo;Zhiyong Shen;Qing He;Yuhong Xiong;Zhongzhi Shi;Hui Xiong
#t 2012
#c 7
#! The distribution difference among multiple domains has been exploited for cross-domain text categorization in recent years. Along this line, we show two new observations in this study. First, the data distribution difference is often due to the fact that different domains use different index words to express the same concept. Second, the association between the conceptual feature and the document class can be stable across domains. These two observations actually indicate the distinction and commonality across domains. Inspired by the above observations, we propose a generative statistical model, named Collaborative Dual-PLSA (CD-PLSA), to simultaneously capture both the domain distinction and commonality among multiple domains. Different from Probabilistic Latent Semantic Analysis (PLSA) with only one latent variable, the proposed model has two latent factors y and z, corresponding to word concept and document class, respectively. The shared commonality intertwines with the distinctions over multiple domains, and is also used as the bridge for knowledge transformation. An Expectation Maximization (EM) algorithm is developed to solve the CD-PLSA model, and further its distributed version is exploited to avoid uploading all the raw data to a centralized location and help to mitigate privacy concerns. After the training phase with all the data from multiple domains we propose to refine the immediate outputs using only the corresponding local data. In summary, we propose a two-phase method for cross-domain text classification, the first phase for collaborative training with all the data, and the second step for local refinement. Finally, we conduct extensive experiments over hundreds of classification tasks with multiple source domains and multiple target domains to validate the superiority of the proposed method over existing state-of-the-art methods of supervised and transfer learning. It is noted to mention that as shown by the experimental results CD-PLSA for the collaborative training is more tolerant of distribution differences, and the local refinement also gains significant improvement in terms of classification accuracy.

#index 1903463
#* Multiview Semi-Supervised Learning with Consensus
#@ Guangxia Li;Kuiyu Chang;Steven C. H. Hoi
#t 2012
#c 7
#! Obtaining high-quality and up-to-date labeled data can be difficult in many real-world machine learning applications. Semi-supervised learning aims to improve the performance of a classifier trained with limited number of labeled data by utilizing the unlabeled ones. This paper demonstrates a way to improve the transductive SVM, which is an existing semi-supervised learning algorithm, by employing a multiview learning paradigm. Multiview learning is based on the fact that for some problems, there may exist multiple perspectives, so called views, of each data sample. For example, in text classification, the typical view contains a large number of raw content features such as term frequency, while a second view may contain a small but highly informative number of domain specific features. We propose a novel two-view transductive SVM that takes advantage of both the abundant amount of unlabeled data and their multiple representations to improve classification result. The idea is straightforward: train a classifier on each of the two views of both labeled and unlabeled data, and impose a global constraint requiring each classifier to assign the same class label to each labeled and unlabeled sample. We also incorporate manifold regularization, a kind of graph-based semi-supervised learning method into our framework. The proposed two-view transductive SVM was evaluated on both synthetic and real-life data sets. Experimental results show that our algorithm performs up to 10 percent better than a single-view learning approach, especially when the amount of labeled data is small. The other advantage of our two-view semi-supervised learning approach is its significantly improved stability, which is especially useful when dealing with noisy data in real-world applications.

#index 1903464
#* Rank Entropy-Based Decision Trees for Monotonic Classification
#@ Qinghua Hu;Xunjian Che;Lei Zhang;David Zhang;Maozu Guo;Daren Yu
#t 2012
#c 7
#! In many decision making tasks, values of features and decision are ordinal. Moreover, there is a monotonic constraint that the objects with better feature values should not be assigned to a worse decision class. Such problems are called ordinal classification with monotonicity constraint. Some learning algorithms have been developed to handle this kind of tasks in recent years. However, experiments show that these algorithms are sensitive to noisy samples and do not work well in real-world applications. In this work, we introduce a new measure of feature quality, called rank mutual information (RMI), which combines the advantage of robustness of Shannon's entropy with the ability of dominance rough sets in extracting ordinal structures from monotonic data sets. Then, we design a decision tree algorithm (REMT) based on rank mutual information. The theoretic and experimental analysis shows that the proposed algorithm can get monotonically consistent decision trees, if training samples are monotonically consistent. Its performance is still good when data are contaminated with noise.

#index 1903465
#* Redistricting Using Constrained Polygonal Clustering
#@ Deepti Joshi;Leen-Kiat Soh;Ashok Samal
#t 2012
#c 7
#! Redistricting is the process of dividing a geographic area consisting of spatial units—often represented as spatial polygons—into smaller districts that satisfy some properties. It can therefore be formulated as a set partitioning problem where the objective is to cluster the set of spatial polygons into groups such that a value function is maximized [1]. Widely used algorithms developed for point-based data sets are not readily applicable because polygons introduce the concepts of spatial contiguity and other topological properties that cannot be captured by representing polygons as points. Furthermore, when clustering polygons, constraints such as spatial contiguity and unit distributedness should be strategically addressed. Toward this, we have developed the Constrained Polygonal Spatial Clustering (CPSC) algorithm based on the {\rm A}^\ast search algorithm that integrates cluster-level and instance-level constraints as heuristic functions. Using these heuristics, CPSC identifies the initial seeds, determines the best cluster to grow, and selects the best polygon to be added to the best cluster. We have devised two extensions of CPSC—CPSC* and CPSC*-PS—for problems where constraints can be soft or relaxed. Finally, we compare our algorithm with graph partitioning, simulated annealing, and genetic algorithm-based approaches in two applications—congressional redistricting and school districting.

#index 1903466
#* Sample Pair Selection for Attribute Reduction with Rough Set
#@ D. G. Chen;S. Y. Zhao;L. Zhang;Y. P. Yang;X. Zhang
#t 2012
#c 7
#! Attribute reduction is the strongest and most characteristic result in rough set theory to distinguish itself to other theories. In the framework of rough set, an approach of discernibility matrix and function is the theoretical foundation of finding reducts. In this paper, sample pair selection with rough set is proposed in order to compress the discernibility function of a decision table so that only minimal elements in the discernibility matrix are employed to find reducts. First relative discernibility relation of condition attribute is defined, indispensable and dispensable condition attributes are characterized by their relative discernibility relations and key sample pair set is defined for every condition attribute. With the key sample pair sets, all the sample pair selections can be found. Algorithms of computing one sample pair selection and finding reducts are also developed; comparisons with other methods of finding reducts are performed with several experiments which imply sample pair selection is effective as preprocessing step to find reducts.

#index 1903467
#* Scalable Iterative Graph Duplicate Detection
#@ Melanie Herschel;Felix Naumann;Sascha Szott;Maik Taubert
#t 2012
#c 7
#! Duplicate detection determines different representations of real-world objects in a database. Recent research has considered the use of relationships among object representations to improve duplicate detection. In the general case where relationships form a graph, research has mainly focused on duplicate detection quality/effectiveness. Scalability has been neglected so far, even though it is crucial for large real-world duplicate detection tasks. We scale-up duplicate detection in graph data (ddg) to large amounts of data and pairwise comparisons, using the support of a relational database management system. To this end, we first present a framework that generalizes the ddg process. We then present algorithms to scale ddg in space (amount of data processed with bounded main memory) and in time. Finally, we extend our framework to allow batched and parallel ddg, thus further improving efficiency. Experiments on data of up to two orders of magnitude larger than data considered so far in ddg show that our methods achieve the goal of scaling ddg to large volumes of data.

#index 1931741
#* Erratum to "Mining Distinction and Commonality across Multiple Domains Using Generative Model for Text Classification"
#@ Fuzhen Zhuang;Zhiyong Shen;Qing He;Yuhong Xiong;Ping Luo;Zhongzhi Shi;Hui Xiong
#t 2012
#c 7

#index 1931742
#* Computing Exact Skyline Probabilities for Uncertain Databases
#@ Dongwon Kim;Hyeonseung Im;Sungwoo Park
#t 2012
#c 7
#! With the rapid increase in the amount of uncertain data available, probabilistic skyline computation on uncertain databases has become an important research topic. Previous work on probabilistic skyline computation, however, only identifies those objects whose skyline probabilities are higher than a given threshold, or is useful only for 2D data sets. In this paper, we develop a probabilistic skyline algorithm called PSkyline which computes exact skyline probabilities of all objects in a given uncertain data set. PSkyline aims to identify blocks of instances with skyline probability zero, and more importantly, to find incomparable groups of instances and dispense with unnecessary dominance tests altogether. To increase the chance of finding such blocks and groups of instances, PSkyline uses a new in-memory tree structure called Z-tree. We also develop an online probabilistic skyline algorithm called O-PSkyline for uncertain data streams and a top-k probabilistic skyline algorithm called K-PSkyline to find top-k objects with the highest skyline probabilities. Experimental results show that all the proposed algorithms scale well to large and high-dimensional uncertain databases.

#index 1931743
#* Constructing a New-Style Conceptual Model of Brain Data for Systematic Brain Informatics
#@ Ning Zhong;Jianhui Chen
#t 2012
#c 7
#! The development of brain science has led to a vast increase of brain data. To meet requirements of a systematic methodology of Brain Informatics (BI), this paper proposes a new conceptual model of brain data, namely Data-Brain, which explicitly represents various relationships among multiple human brain data sources, with respect to all major aspects and capabilities of human information processing systems (HIPS). A multidimension framework and a BI methodology-based ontological modeling approach have been developed to implement a Data-Brain. The Data-Brain, Data-Brain-based BI provenances, and heterogeneous brain data can be used to construct a Data-Brain-based brain data center which provides a global framework to integrate data, information, and knowledge coming from the whole research process for systematic BI study. Such a Data-Brain modeling approach represents a radically new way for domain-driven conceptual modeling of brain data, which models a whole process of systematically investigating human information processing mechanisms.

#index 1931744
#* Cost-Aware Rank Join with Random and Sorted Access
#@ Davide Martinenghi;Marco Tagliasacchi
#t 2012
#c 7
#! In this paper, we address the problem of joining ranked results produced by two or more services on the web. We consider services endowed with two kinds of access that are often available: 1) sorted access, which returns tuples sorted by score; 2) random access, which returns tuples matching a given join attribute value. Rank join operators combine objects of two or more relations and output the k combinations with the highest aggregate score. While the past literature has studied suitable bounding schemes for this setting, in this paper we focus on the definition of a pulling strategy, which determines the order of invocation of the joined services. We propose the Cost-Aware with Random and Sorted access (CARS) pulling strategy, which is derived at compile-time and is oblivious of the query-dependent score distributions. We cast CARS as the solution of an optimization problem based on a small set of parameters characterizing the joined services. We validate the proposed strategy with experiments on both real and synthetic data sets. We show that CARS outperforms prior proposals and that its overall access cost is always within a very short margin from that of an oracle-based optimal strategy. In addition, CARS is shown to be robust w.r.t. the uncertainty that may characterize the estimated parameters.

#index 1931745
#* Discovering the Most Influential Sites over Uncertain Data: A Rank-Based Approach
#@ Kai Zheng;Zi Huang;Aoying zhou;Xiaofang Zhou
#t 2012
#c 7
#! With the rapidly increasing availability of uncertain data in many important applications such as location-based services, sensor monitoring, and biological information management systems, uncertainty-aware query processing has received a significant amount of research effort from the database community in recent years. In this paper, we investigate a new type of query in the context of uncertain databases, namely uncertain top-k influential sites query ({\rm UT}k{\rm IS} query for short), which can be applied in a wide range of application areas such as marketing analysis and mobile services. Since it is not so straightforward to precisely define the semantics of {\rm top}k query with uncertain data, in this paper we introduce a novel and more intuitive formulation of the query on the basis of expected rank semantics. To address the efficiency issue caused by possible worlds exploration, we propose effective pruning rules and a divide-and-conquer paradigm such that the number of candidates as well as the number of possible worlds to be considered can be significantly reduced. Finally, we conduct extensive experiments on real data sets to verify the effectiveness and efficiency of the new methods proposed in this paper.

#index 1931746
#* Efficient Mining of Frequent Item Sets on Large Uncertain Databases
#@ Liang Wang;David W. Cheung;Reynold Cheng;Sau Dan Lee;Xuan Yang
#t 2012
#c 7
#! The data handled in emerging applications like location-based services, sensor monitoring systems, and data integration, are often inexact in nature. In this paper, we study the important problem of extracting frequent item sets from a large uncertain database, interpreted under the Possible World Semantics (PWS). This issue is technically challenging, since an uncertain database contains an exponential number of possible worlds. By observing that the mining process can be modeled as a Poisson binomial distribution, we develop an approximate algorithm, which can efficiently and accurately discover frequent item sets in a large uncertain database. We also study the important issue of maintaining the mining result for a database that is evolving (e.g., by inserting a tuple). Specifically, we propose incremental mining algorithms, which enable Probabilistic Frequent Item set (PFI) results to be refreshed. This reduces the need of re-executing the whole mining algorithm on the new database, which is often more expensive and unnecessary. We examine how an existing algorithm that extracts exact item sets, as well as our approximate algorithm, can support incremental mining. All our approaches support both tuple and attribute uncertainty, which are two common uncertain database models. We also perform extensive evaluation on real and synthetic data sets to validate our approaches.

#index 1931747
#* Hybrid Generative/Discriminative Approaches for Proportional Data Modeling and Classification
#@ Nizar Bouguila
#t 2012
#c 7
#! The work proposed in this paper is motivated by the need to develop powerful models and approaches to classify and learn proportional data. Indeed, an abundance of interesting data in several applications occur naturally in this form. Our goal is to discover and capture the intrinsic nature of the data by proposing some approaches that combine the major advantages of generative models namely finite mixtures and discriminative techniques namely support vector machines (SVMs). Indeed, SVMs often rely on classic kernels which are not generally meaningful for proportional data. One serious limitation of these kernels is that they do not take into account the nature of data to classify and choosing a suitable kernel continues to be a formidable challenge for data mining and machine learning researchers. Our approach builds on selecting accurate kernels generated from finite mixtures of Dirichlet, generalized Dirichlet and Beta-Liouville distributions which chief advantage is their flexibility and explanatory capabilities in the case of heterogenous proportional data. Using extensive simulations and a number of experiments involving scene modeling and classification, and automatic image orientation detection, we show the merits of the proposed mixture models and the accuracy of the generated kernels.

#index 1931748
#* Joint Optimization of Index Freshness and Coverage in Real-Time Search Engines
#@ Yongwook Shin;Junseok Lim;Jonghun Park
#t 2012
#c 7
#! Real-time search engines are increasingly indexing web content using data streams, since a number of web sources including news and social media sites are now delivering up-to-date information via streams. Accordingly, it is a crucial challenge for a real-time search engine using data streams to improve index freshness that primarily depends on the latencies involved during fetching and indexing processes. Retrieval latency is a time lag between document publication and fetching while indexing latency is a delay required for a fetched document to be indexed, which is caused by finiteness of indexing capacity. The problem of retrieval latency can be satisfactorily addressed by use of appropriate fetching scheduling or recent real-time content notification protocols. However, as the entire volume of real-time content rapidly grows, the indexing latency becomes a challenging problem. Furthermore, the need for maximizing index coverage makes it more difficult to reduce the indexing latency under the limited indexing capacity. We consider a problem of jointly optimizing the indexing latency as well as index coverage, in which their relative importance can be adjusted, and propose an optimization model based on inventory control theory. Extensive experiments have been conducted to validate the proposed model, and suggest that the proposed approach outperforms the other alternatives.

#index 1931749
#* Mining Bucket Order-Preserving SubMatrices in Gene Expression Data
#@ Qiong Fang;Wilfred Ng;Jianlin Feng;Yuliang Li
#t 2012
#c 7
#! The Order-Preserving SubMatrices (OPSMs) are employed to discover significant biological associations between genes and experiment conditions. Herein, we propose a new relaxed OPSM model by considering the linearity relaxation, which is called the Bucket OPSM (BOPSM) model. An efficient method called ApriBopsm is developed to exhaustively mine such BOPSM patterns. We further generalize the BOPSM model by incorporating the similarity relaxation strategy. We develop a generalized BOPSM model called GeBOPSM and adopt a pattern growing method called SeedGrowth to mine GeBOPSM patterns. Informally, the SeedGrowth algorithm adopts two different growing strategies on rows and columns in order to expand a seed BOPSM into a maximal GeBOPSM pattern. We conduct a series of experiments using both synthetic and biological datasets to study the effectiveness of our proposed relaxed models and the efficiency of the relevant mining methods. The BOPSM model is shown to be able to capture the characteristics of noisy OPSM patterns, and is superior to the strict counterparts. ApriBopsm is also significantly more efficient than OPC-Tree, which is the state-of-the-art OPSM mining method. Compared to all the current relaxed OPSM models, the GeBOPSM model achieves the best performance in terms of the number of mined quality patterns.

#index 1931750
#* Network Similarity Decomposition (NSD): A Fast and Scalable Approach to Network Alignment
#@ Giorgos Kollias;Shahin Mohammadi;Ananth Grama
#t 2012
#c 7
#! As graph-structured data sets become commonplace, there is increasing need for efficient ways of analyzing such data sets. These analyses include conservation, alignment, differentiation, and discrimination, among others. When defined on general graphs, these problems are considerably harder than their well-studied counterparts on sets and sequences. In this paper, we study the problem of global alignment of large sparse graphs. Specifically, we investigate efficient methods for computing approximations to the state-of-the-art IsoRank solution for finding pairwise topological similarity between nodes in two networks (or within the same network). Pairs of nodes with high similarity can be used to seed global alignments. We present a novel approach to this computationally expensive problem based on uncoupling and decomposing ranking calculations associated with the computation of similarity scores. Uncoupling refers to independent preprocessing of each input graph. Decomposition implies that pairwise similarity scores can be explicitly broken down into contributions from different link patterns traced back to a low-rank approximation of the initial conditions for the computation. These two concepts result in significant improvements, in terms of computational cost, interpretability of similarity scores, and nature of supported queries. We show over two orders of magnitude improvement in performance over IsoRank/Random Walk formulations, and over an order of magnitude improvement over constrained matrix-triple-product formulations, in the context of real data sets.

#index 1931751
#* Processing and Evaluating Partial Tree Pattern Queries on XML Data
#@ Xiaoying Wu;Stefanos Souldatos;Dimitri Theodoratos;Theodore Dalamagas;Yannis Vassiliou;Timos Sellis
#t 2012
#c 7
#! XML query languages typically allow the specification of structural patterns using XPath. Usually, these structural patterns are in the form of trees (Tree-Pattern Queries—TPQs). Finding the occurrences of such patterns in an XML tree is a key operation in XML query evaluation. The multiple previous algorithms presented for this operation focus mainly on the evaluation of tree-pattern queries. Recently, requirements for flexible querying of XML data have motivated the consideration of query classes that are more expressive and flexible than TPQs for which efficient nonmain-memory evaluation algorithms are not known. In this paper, we consider a class of queries, called Partial Tree-Pattern Queries (PTPQs), which generalize and strictly contain TPQs. PTPQs represent a broad fragment of XPath which is very useful in practice. In order to process PTPQs, we introduce a set of sound and complete inference rules to characterize structural relationship derivation. We provide necessary and sufficient conditions for detecting query unsatisfiability and node redundancy. We also show that PTPQs can be represented as directed acyclic graphs augmented with the “same-path” constraints. In order to leverage existing efficient evaluation algorithms for less expressive classes of queries, we design two approaches that evaluate a PTPQ by decomposing it into a set of simpler queries: algorithm IndexTPQGen, exploits a structural summary of the XML data and evaluates a PTPQ by generating an equivalent set of TPQs and unioning their answers. Algorithm PartialPathJoin decomposes the PTPQ into partial-path queries, and merge-joins their solutions. We also develop PartialTreeStack, an original polynomial time holistic algorithm for PTPQs. To the best of our knowledge, this is the first algorithm to support the evaluation of such a broad structural fragment of XPath in the inverted lists evaluation model. We provide a theoretical analysis of our algorithm and identify cases where it is asymptotically optimal. An extensive experimental evaluation shows that it is more efficient, robust, and stable than the other two and it outperforms a state-of-the art XQuery engine on PTPQs.

#index 1931752
#* Query Representation through Lexical Association for Information Retrieval
#@ Pawan Goyal;Laxmidhar Behera;Thomas M. McGinnity
#t 2012
#c 7
#! A user query for information retrieval (IR) applications may not contain the most appropriate terms (words) as actually intended by the user. This is usually referred to as the term mismatch problem and is a crucial research issue in IR. Using the notion of relevance, we provide a comprehensive theoretical analysis of a parametric query vector, which is assumed to represent the information needs of the user. A lexical association function has been derived analytically using the system relevance criteria. The derivation is further justified using an empirical evidence from the user relevance criteria. Such analytical derivation as presented in this paper provides a proper mathematical framework to the query expansion techniques, which have largely been heuristic in the existing literature. By using the generalized retrieval framework, the proposed query representation model is equally applicable to the vector space model (VSM), Okapi best matching 25 (Okapi BM25), and Language Model (LM). Experiments over various data sets from TREC show that the proposed query representation gives statistically significant improvements over the baseline Okapi BM25 and LM as well as other well-known global query expansion techniques. Empirical results along with the theoretical foundations of the query representation confirm that the proposed model extends the state of the art in global query expansion.

#index 1931753
#* Subontology Extraction Using Hyponym and Hypernym Closure on is-a Directed Acyclic Graphs
#@ Vincent Ranwez;Sylvie Ranwez;Stefan Janaqi
#t 2012
#c 7
#! Ontologies are successfully used as semantic guides when navigating through the huge and ever increasing quantity of digital documents. Nevertheless, the size of numerous domain ontologies tends to grow beyond the human capacity to grasp information. This growth is problematic for a lot of key applications that require user interactions such as document annotation or ontology modification/evolution. The problem could be partially overcome by providing users with a subontology focused on their current concepts of interest. A subontology restricted to this sole set of concepts is of limited interest since their relationships can generally not be explicit without adding some of their hyponyms and hypernyms. This paper proposes efficient algorithms to identify these additional key concepts based on the closure of two common graph operators: the least common-ancestor (lca) and greatest common descendant (gcd). The resulting method produces ontology excerpts focused on a set of concepts of interest and is fast enough to be used in interactive environments. As an example, we use the resulting program, called OntoFocus (http://www.ontotoolkit.mines-ales.fr/), to restrict, in few seconds, the large Gene Ontology (∼30,000 concepts) to a subontology focused on concepts annotating a gene related to breast cancer.

#index 1931754
#* Querying Uncertain Minimum in Wireless Sensor Networks
#@ Mao Ye;Ken Lee;Wang-Chien Lee;Xingjie Liu;Meng Chang Chen
#t 2012
#c 7
#! In this paper, we introduce two types of probabilistic aggregation queries, namely, Probabilistic Minimum Value Queries (PMVQ)s and Probabilistic Minimum Node Queries (PMNQ)s. A PMVQ determines possible minimum values among all imprecise sensed data, while a PMNQ identifies sensor nodes that possibly provide minimum values. However, centralized approaches incur a lot of energy from battery-powered sensor nodes and well-studied in-network aggregation techniques that presume precise sensed data are not practical to inherently imprecise sensed data. Thus, to answer PMVQs and PMNQs energy-efficiently, we devised suites of in-network algorithms. For PMVQs, our in-network minimum value screening algorithm (MVS) filters candidate minimum values; and our in-network minimum value aggregation algorithm (MVA) conducts in-network probability calculation. PMNQs requires possible minimum values to be determined a prior, inevitably consuming more energy to evaluate than PMVQs. Accordingly, our one-phase and two-phase in-network algorithms are devised. We also extend the algorithms to answer PMNQ variants. We evaluate all our proposed approaches through cost analysis and simulations.

#index 1938485
#* A Fast Clustering-Based Feature Subset Selection Algorithm for High-Dimensional Data
#@ Qinbao Song;Jinjie Ni;Guangtao Wang
#t 2013
#c 7
#! Feature selection involves identifying a subset of the most useful features that produces compatible results as the original entire set of features. A feature selection algorithm may be evaluated from both the efficiency and effectiveness points of view. While the efficiency concerns the time required to find a subset of features, the effectiveness is related to the quality of the subset of features. Based on these criteria, a fast clustering-based feature selection algorithm (FAST) is proposed and experimentally evaluated in this paper. The FAST algorithm works in two steps. In the first step, features are divided into clusters by using graph-theoretic clustering methods. In the second step, the most representative feature that is strongly related to target classes is selected from each cluster to form a subset of features. Features in different clusters are relatively independent, the clustering-based strategy of FAST has a high probability of producing a subset of useful and independent features. To ensure the efficiency of FAST, we adopt the efficient minimum-spanning tree (MST) clustering method. The efficiency and effectiveness of the FAST algorithm are evaluated through an empirical study. Extensive experiments are carried out to compare FAST and several representative feature selection algorithms, namely, FCBF, ReliefF, CFS, Consist, and FOCUS-SF, with respect to four types of well-known classifiers, namely, the probability-based Naive Bayes, the tree-based C4.5, the instance-based IB1, and the rule-based RIPPER before and after feature selection. The results, on 35 publicly available real-world high-dimensional image, microarray, and text data, demonstrate that the FAST not only produces smaller subsets of features but also improves the performances of the four types of classifiers.

#index 1938486
#* A Graph-Based Consensus Maximization Approach for Combining Multiple Supervised and Unsupervised Models
#@ Jing Gao;Feng Liang;Wei Fan;Yizhou Sun;Jiawei Han
#t 2013
#c 7
#! Ensemble learning has emerged as a powerful method for combining multiple models. Well-known methods, such as bagging, boosting, and model averaging, have been shown to improve accuracy and robustness over single models. However, due to the high costs of manual labeling, it is hard to obtain sufficient and reliable labeled data for effective training. Meanwhile, lots of unlabeled data exist in these sources, and we can readily obtain multiple unsupervised models. Although unsupervised models do not directly generate a class label prediction for each object, they provide useful constraints on the joint predictions for a set of related objects. Therefore, incorporating these unsupervised models into the ensemble of supervised models can lead to better prediction performance. In this paper, we study ensemble learning with outputs from multiple supervised and unsupervised models, a topic where little work has been done. We propose to consolidate a classification solution by maximizing the consensus among both supervised predictions and unsupervised constraints. We cast this ensemble task as an optimization problem on a bipartite graph, where the objective function favors the smoothness of the predictions over the graph, but penalizes the deviations from the initial labeling provided by the supervised models. We solve this problem through iterative propagation of probability estimates among neighboring nodes and prove the optimality of the solution. The proposed method can be interpreted as conducting a constrained embedding in a transformed space, or a ranking on the graph. Experimental results on different applications with heterogeneous data sources demonstrate the benefits of the proposed method over existing alternatives. (More information, data, and code are available at http://www.cse.buffalo.edu/~jing/integrate.htm.)

#index 1938487
#* A Survey of XML Tree Patterns
#@ Marouane Hachicha;Jerome Darmont
#t 2013
#c 7
#! With XML becoming a ubiquitous language for data interoperability purposes in various domains, efficiently querying XML data is a critical issue. This has lead to the design of algebraic frameworks based on tree-shaped patterns akin to the tree-structured data model of XML. Tree patterns are graphic representations of queries over data trees. They are actually matched against an input data tree to answer a query. Since the turn of the 21st century, an astounding research effort has been focusing on tree pattern models and matching optimization (a primordial issue). This paper is a comprehensive survey of these topics, in which we outline and compare the various features of tree patterns. We also review and discuss the two main families of approaches for optimizing tree pattern matching, namely pattern tree minimization and holistic matching. We finally present actual tree pattern-based developments, to provide a global overview of this significant research topic.

#index 1938488
#* Automatic Semantic Content Extraction in Videos Using a Fuzzy Ontology and Rule-Based Model
#@ Yakup Yildirim;Adnan Yazici;Turgay Yilmaz
#t 2013
#c 7
#! Recent increase in the use of video-based applications has revealed the need for extracting the content in videos. Raw data and low-level features alone are not sufficient to fulfill the user 's needs; that is, a deeper understanding of the content at the semantic level is required. Currently, manual techniques, which are inefficient, subjective and costly in time and limit the querying capabilities, are being used to bridge the gap between low-level representative features and high-level semantic content. Here, we propose a semantic content extraction system that allows the user to query and retrieve objects, events, and concepts that are extracted automatically. We introduce an ontology-based fuzzy video semantic content model that uses spatial/temporal relations in event and concept definitions. This metaontology definition provides a wide-domain applicable rule construction standard that allows the user to construct an ontology for a given domain. In addition to domain ontologies, we use additional rule definitions (without using ontology) to lower spatial relation computation cost and to be able to define some complex situations more effectively. The proposed framework has been fully implemented and tested on three different domains. We have obtained satisfactory precision and recall rates for object, event and concept extraction.

#index 1938489
#* Clustering Sentence-Level Text Using a Novel Fuzzy Relational Clustering Algorithm
#@ Andrew Skabar;Khaled Abdalgader
#t 2013
#c 7
#! In comparison with hard clustering methods, in which a pattern belongs to a single cluster, fuzzy clustering algorithms allow patterns to belong to all clusters with differing degrees of membership. This is important in domains such as sentence clustering, since a sentence is likely to be related to more than one theme or topic present within a document or set of documents. However, because most sentence similarity measures do not represent sentences in a common metric space, conventional fuzzy clustering approaches based on prototypes or mixtures of Gaussians are generally not applicable to sentence clustering. This paper presents a novel fuzzy clustering algorithm that operates on relational input data; i.e., data in the form of a square matrix of pairwise similarities between data objects. The algorithm uses a graph representation of the data, and operates in an Expectation-Maximization framework in which the graph centrality of an object in the graph is interpreted as a likelihood. Results of applying the algorithm to sentence clustering tasks demonstrate that the algorithm is capable of identifying overlapping clusters of semantically related sentences, and that it is therefore of potential use in a variety of text mining tasks. We also include results of applying the algorithm to benchmark data sets in several other domains.

#index 1938490
#* Distributed Processing of Probabilistic Top-k Queries in Wireless Sensor Networks
#@ Mao Ye;Wang-Chien Lee;Dik Lun Lee;Xingjie Liu
#t 2013
#c 7
#! In this paper, we introduce the notion of sufficient set and necessary set for distributed processing of probabilistic top-k queries in cluster-based wireless sensor networks. These two concepts have very nice properties that can facilitate localized data pruning in clusters. Accordingly, we develop a suite of algorithms, namely, sufficient set-based (SSB), necessary set-based (NSB), and boundary-based (BB), for intercluster query processing with bounded rounds of communications. Moreover, in responding to dynamic changes of data distribution in the network, we develop an adaptive algorithm that dynamically switches among the three proposed algorithms to minimize the transmission cost. We show the applicability of sufficient set and necessary set to wireless sensor networks with both two-tier hierarchical and tree-structured network topologies. Experimental results show that the proposed algorithms reduce data transmissions significantly and incur only small constant rounds of data communications. The experimental results also demonstrate the superiority of the adaptive algorithm, which achieves a near-optimal performance under various conditions.

#index 1938491
#* Evaluating Data Reliability: An Evidential Answer with Application to a Web-Enabled Data Warehouse
#@ Sebastien Destercke;Patrice Buche;Brigitte Charnomordic
#t 2013
#c 7
#! There are many available methods to integrate information source reliability in an uncertainty representation, but there are only a few works focusing on the problem of evaluating this reliability. However, data reliability and confidence are essential components of a data warehousing system, as they influence subsequent retrieval and analysis. In this paper, we propose a generic method to assess data reliability from a set of criteria using the theory of belief functions. Customizable criteria and insightful decisions are provided. The chosen illustrative example comes from real-world data issued from the Sym'Previus predictive microbiology oriented data warehouse.

#index 1938492
#* Large Graph Analysis in the GMine System
#@ Jose F. Rodrigues Jr.;Hanghang Tong;Jia-Yu Pan;Agma J. M. Traina;Caetano Traina Jr.;Christos Faloutsos
#t 2013
#c 7
#! Current applications have produced graphs on the order of hundreds of thousands of nodes and millions of edges. To take advantage of such graphs, one must be able to find patterns, outliers, and communities. These tasks are better performed in an interactive environment, where human expertise can guide the process. For large graphs, though, there are some challenges: the excessive processing requirements are prohibitive, and drawing hundred-thousand nodes results in cluttered images hard to comprehend. To cope with these problems, we propose an innovative framework suited for any kind of tree-like graph visual design. GMine integrates 1) a representation for graphs organized as hierarchies of partitions—the concepts of SuperGraph and Graph-Tree; and 2) a graph summarization methodology—CEPS. Our graph representation deals with the problem of tracing the connection aspects of a graph hierarchy with sub linear complexity, allowing one to grasp the neighborhood of a single node or of a group of nodes in a single click. As a proof of concept, the visual environment of GMine is instantiated as a system in which large graphs can be investigated globally and locally.

#index 1938493
#* Maximum Likelihood Estimation from Uncertain Data in the Belief Function Framework
#@ Thierry Denoeux
#t 2013
#c 7
#! We consider the problem of parameter estimation in statistical models in the case where data are uncertain and represented as belief functions. The proposed method is based on the maximization of a generalized likelihood criterion, which can be interpreted as a degree of agreement between the statistical model and the uncertain observations. We propose a variant of the EM algorithm that iteratively maximizes this criterion. As an illustration, the method is applied to uncertain data clustering using finite mixture models, in the cases of categorical and continuous attributes.

#index 1938494
#* Nonadaptive Mastermind Algorithms for String and Vector Databases, with Case Studies
#@ Arthur U. Asuncion;Michael T. Goodrich
#t 2013
#c 7
#! In this paper, we study sparsity-exploiting Mastermind algorithms for attacking the privacy of an entire database of character strings or vectors, such as DNA strings, movie ratings, or social network friendship data. Based on reductions to nonadaptive group testing, our methods are able to take advantage of minimal amounts of privacy leakage, such as contained in a single bit that indicates if two people in a medical database have any common genetic mutations, or if two people have any common friends in an online social network. We analyze our Mastermind attack algorithms using theoretical characterizations that provide sublinear bounds on the number of queries needed to clone the database, as well as experimental tests on genomic information, collaborative filtering data, and online social networks. By taking advantage of the generally sparse nature of these real-world databases and modulating a parameter that controls query sparsity, we demonstrate that relatively few nonadaptive queries are needed to recover a large majority of each database.

#index 1938495
#* On the Recovery of R-Trees
#@ Tuukka Haapasalo;Ibrahim Jaluta;Seppo Sippu;Eljas Soisalon-Soininen
#t 2013
#c 7
#! We consider the recoverability of traditional R-tree index structures under concurrent updating transactions, an important issue that is neglected or treated inadequately in many proposals of R-tree concurrency control. We present two solutions to ARIES-based recovery of transactions on R-trees. These assume a standard fine-grained single-version update model with physiological write-ahead logging and steal-and-no-force buffering where records with uncommitted updates by a transaction may migrate from their original page to another page due to structure modifications caused by other transactions. Both solutions guarantee that an R-tree will remain in a consistent and balanced state in the presence of any number of concurrent forward-rolling and (totally or partially) backward-rolling multiaction transactions and in the event of process failures and system crashes. One solution maintains the R-tree in a strictly consistent state in which the bounding rectangles of pages are as tight as possible, while in the other solution this requirement is relaxed. In both solutions only a small constant number of simultaneous exclusive latches (write latches) are needed, and in the solution that only maintains relaxed consistency also the number of simultaneous nonexclusive latches is similarly limited. In both solutions, deletions are handled uniformly with insertions, and a logarithmic insertion-path length is maintained under all circumstances.

#index 1938496
#* Ontology Matching: State of the Art and Future Challenges
#@ Shvaiko Pavel;Jerome Euzenat
#t 2013
#c 7
#! After years of research on ontology matching, it is reasonable to consider several questions: is the field of ontology matching still making progress? Is this progress significant enough to pursue further research? If so, what are the particularly promising directions? To answer these questions, we review the state of the art of ontology matching and analyze the results of recent ontology matching evaluations. These results show a measurable improvement in the field, the speed of which is albeit slowing down. We conjecture that significant improvements can be obtained only by addressing important challenges for ontology matching. We present such challenges with insights on how to approach them, thereby aiming to direct research into the most promising tracks and to facilitate the progress of the field.

#index 1938497
#* 2012 Reviewer's list
#@ 
#t 2013
#c 7
#! The publication offers a note of thanks and lists its reviewers.

#index 1938498
#* Ranking on Data Manifold with Sink Points
#@ Xue-Qi Cheng;Pan Du;Jiafeng Guo;Xiaofei Zhu;Yixin Chen
#t 2013
#c 7
#! Ranking is an important problem in various applications, such as Information Retrieval (IR), natural language processing, computational biology, and social sciences. Many ranking approaches have been proposed to rank objects according to their degrees of relevance or importance. Beyond these two goals, diversity has also been recognized as a crucial criterion in ranking. Top ranked results are expected to convey as little redundant information as possible, and cover as many aspects as possible. However, existing ranking approaches either take no account of diversity, or handle it separately with some heuristics. In this paper, we introduce a novel approach, Manifold Ranking with Sink Points (MRSPs), to address diversity as well as relevance and importance in ranking. Specifically, our approach uses a manifold ranking process over the data manifold, which can naturally find the most relevant and important data objects. Meanwhile, by turning ranked objects into sink points on data manifold, we can effectively prevent redundant objects from receiving a high rank. MRSP not only shows a nice convergence property, but also has an interesting and satisfying optimization explanation. We applied MRSP on two application tasks, update summarization and query recommendation, where diversity is of great concern in ranking. Experimental results on both tasks present a strong empirical performance of MRSP as compared to existing ranking approaches.

#index 1938499
#* Region-Based Foldings in Process Discovery
#@ Marc Sole;Josep Carmona
#t 2013
#c 7
#! A central problem in the area of Process Mining is to obtain a formal model that represents the processes that are conducted in a system. If realized, this simple motivation allows for powerful techniques that can be used to formally analyze and optimize a system, without the need to resort to its semiformal and sometimes inaccurate specification. The problem addressed in this paper is known as Process Discovery: to obtain a formal model from a set of system executions. The theory of regions is a valuable tool in process discovery: it aims at learning a formal model (Petri nets) from a set of traces. On its genuine form, the theory is applied on an automaton and therefore one should convert the traces into an acyclic automaton in order to apply these techniques. Given that the complexity of the region-based techniques depends on the size of the input automata, revealing the underlying cycles and folding the initial automaton can incur in a significant complexity alleviation of the region-based techniques. In this paper, we follow this idea by incorporating region information in the cycle detection algorithm, enabling the identification of complex cycles that cannot be obtained efficiently with state-of-the-art techniques. The experimental results obtained by the devised tool suggest that the techniques presented in this paper are a big step into widening the application of the theory of regions in Process Mining for industrial scenarios.

#index 1938500
#* Relationships between Diversity of Classification Ensembles and Single-Class Performance Measures
#@ Shuo Wang;Xin Yao
#t 2013
#c 7
#! In class imbalance learning problems, how to better recognize examples from the minority class is the key focus, since it is usually more important and expensive than the majority class. Quite a few ensemble solutions have been proposed in the literature with varying degrees of success. It is generally believed that diversity in an ensemble could help to improve the performance of class imbalance learning. However, no study has actually investigated diversity in depth in terms of its definitions and effects in the context of class imbalance learning. It is unclear whether diversity will have a similar or different impact on the performance of minority and majority classes. In this paper, we aim to gain a deeper understanding of if and when ensemble diversity has a positive impact on the classification of imbalanced data sets. First, we explain when and why diversity measured by Q-statistic can bring improved overall accuracy based on two classification patterns proposed by Kuncheva et al. We define and give insights into good and bad patterns in imbalanced scenarios. Then, the pattern analysis is extended to single-class performance measures, including recall, precision, and F-measure, which are widely used in class imbalance learning. Six different situations of diversity's impact on these measures are obtained through theoretical analysis. Finally, to further understand how diversity affects the single class performance and overall performance in class imbalance problems, we carry out extensive experimental studies on both artificial data sets and real-world benchmarks with highly skewed class distributions. We find strong correlations between diversity and discussed performance measures. Diversity shows a positive impact on the minority class in general. It is also beneficial to the overall performance in terms of AUC and G-mean.

#index 1938501
#* T-Drive: Enhancing Driving Directions with Taxi Drivers' Intelligence
#@ Jing Yuan;Yu Zheng;Xing Xie;Guangzhong Sun
#t 2013
#c 7
#! This paper presents a smart driving direction system leveraging the intelligence of experienced drivers. In this system, GPS-equipped taxis are employed as mobile sensors probing the traffic rhythm of a city and taxi drivers' intelligence in choosing driving directions in the physical world. We propose a time-dependent landmark graph to model the dynamic traffic pattern as well as the intelligence of experienced drivers so as to provide a user with the practically fastest route to a given destination at a given departure time. Then, a Variance-Entropy-Based Clustering approach is devised to estimate the distribution of travel time between two landmarks in different time slots. Based on this graph, we design a two-stage routing algorithm to compute the practically fastest and customized route for end users. We build our system based on a real-world trajectory data set generated by over 33,000 taxis in a period of three months, and evaluate the system by conducting both synthetic experiments and in-the-field evaluations. As a result, 60-70 percent of the routes suggested by our method are faster than the competing methods, and 20 percent of the routes share the same results. On average, 50 percent of our routes are at least 20 percent faster than the competing approaches.

#index 1984406
#* A Novel Profit Maximizing Metric for Measuring Classification Performance of Customer Churn Prediction Models
#@ Thomas Verbraken;Wouter Verbeke;Bart Baesens
#t 2013
#c 7
#! The interest for data mining techniques has increased tremendously during the past decades, and numerous classification techniques have been applied in a wide range of business applications. Hence, the need for adequate performance measures has become more important than ever. In this paper, a cost-benefit analysis framework is formalized in order to define performance measures which are aligned with the main objectives of the end users, i.e., profit maximization. A new performance measure is defined, the expected maximum profit criterion. This general framework is then applied to the customer churn problem with its particular cost-benefit structure. The advantage of this approach is that it assists companies with selecting the classifier which maximizes the profit. Moreover, it aids with the practical implementation in the sense that it provides guidance about the fraction of the customer base to be included in the retention campaign.

#index 1984407
#* A Predictive-Reactive Method for Improving the Robustness of Real-Time Data Services
#@ Jisu Oh;Kyoung-Don Kang
#t 2013
#c 7
#! Supporting timely data services using fresh data in data-intensive real-time applications, such as e-commerce and transportation management is desirable but challenging, since the workload may vary dynamically. To control the data service delay to be below the specified threshold, we develop a predictive as well as reactive method for database admission control. The predictive method derives the workload bound for admission control in a predictive manner, making no statistical or queuing-theoretic assumptions about workloads. Also, our reactive scheme based on formal feedback control theory continuously adjusts the database load bound to support the delay threshold. By adapting the load bound in a proactive fashion, we attempt to avoid severe overload conditions and excessive delays before they occur. Also, the feedback control scheme enhances the timeliness by compensating for potential prediction errors due to dynamic workloads. Hence, the predictive and reactive methods complement each other, enhancing the robustness of real-time data services as a whole. We implement the integrated approach and several baselines in an open-source database. Compared to the tested open-loop, feedback-only, and statistical prediction + feedback baselines representing the state of the art, our integrated method significantly improves the average/transient delay and real-time data service throughput.

#index 1984408
#* Achieving Data Privacy through Secrecy Views and Null-Based Virtual Updates
#@ Leopoldo Bertossi;Lechen Li
#t 2013
#c 7
#! We may want to keep sensitive information in a relational database hidden from a user or group thereof. We characterize sensitive data as the extensions of secrecy views. The database, before returning the answers to a query posed by a restricted user, is updated to make the secrecy views empty or a single tuple with null values. Then, a query about any of those views returns no meaningful information. Since the database is not supposed to be physically changed for this purpose, the updates are only virtual, and also minimal. Minimality makes sure that query answers, while being privacy preserving, are also maximally informative. The virtual updates are based on null values as used in the SQL standard. We provide the semantics of secrecy views, virtual updates, and secret answers (SAs) to queries. The different instances resulting from the virtually updates are specified as the models of a logic program with stable model semantics, which becomes the basis for computation of the SAs.

#index 1984409
#* Co-Occurrence-Based Diffusion for Expert Search on the Web
#@ Ziyu Guan;Gengxin Miao;Russell McLoughlin;Xifeng Yan;Deng Cai
#t 2013
#c 7
#! Expert search has been studied in different contexts, e.g., enterprises, academic communities. We examine a general expert search problem: searching experts on the web, where millions of webpages and thousands of names are considered. It has mainly two challenging issues: 1) webpages could be of varying quality and full of noises; 2) The expertise evidences scattered in webpages are usually vague and ambiguous. We propose to leverage the large amount of co-occurrence information to assess relevance and reputation of a person name for a query topic. The co-occurrence structure is modeled using a hypergraph, on which a heat diffusion based ranking algorithm is proposed. Query keywords are regarded as heat sources, and a person name which has strong connection with the query (i.e., frequently co-occur with query keywords and co-occur with other names related to query keywords) will receive most of the heat, thus being ranked high. Experiments on the ClueWeb09 web collection show that our algorithm is effective for retrieving experts and outperforms baseline algorithms significantly. This work would be regarded as one step toward addressing the more general entity search problem without sophisticated NLP techniques.

#index 1984410
#* Efficient All Top-$(k)$ Computation—A Unified Solution for All Top-$(k)$, Reverse Top-$(k)$ and Top-$(m)$ Influential Queries
#@ Shen Ge;Leong Hou U;Nikos Mamoulis;David W. Cheung
#t 2013
#c 7
#! Given a set of objects $(P)$ and a set of ranking functions $(F)$ over $(P)$, an interesting problem is to compute the top ranked objects for all functions. Evaluation of multiple top-$(k)$ queries finds application in systems, where there is a heavy workload of ranking queries (e.g., online search engines and product recommendation systems). The simple solution of evaluating the top-$(k)$ queries one-by-one does not scale well; instead, the system can make use of the fact that similar queries share common results to accelerate search. This paper is the first, to our knowledge, thorough study of this problem. We propose methods that compute all top-$(k)$ queries in batch. Our first solution applies the block indexed nested loops paradigm, while our second technique is a view-based algorithm. We propose appropriate optimization techniques for the two approaches and demonstrate experimentally that the second approach is consistently the best. Our approach facilitates evaluation of other complex queries that depend on the computation of multiple top-$(k)$ queries, such as reverse top-$(k)$ and top-$(m)$ influential queries. We show that our batch processing technique for these complex queries outperform the state-of-the-art by orders of magnitude.

#index 1984411
#* Efficient and Effective Duplicate Detection in Hierarchical Data
#@ Luis Leitao;Pavel Calado;Melanie Herschel
#t 2013
#c 7
#! Although there is a long line of work on identifying duplicates in relational data, only a few solutions focus on duplicate detection in more complex hierarchical structures, like XML data. In this paper, we present a novel method for XML duplicate detection, called XMLDup. XMLDup uses a Bayesian network to determine the probability of two XML elements being duplicates, considering not only the information within the elements, but also the way that information is structured. In addition, to improve the efficiency of the network evaluation, a novel pruning strategy, capable of significant gains over the unoptimized version of the algorithm, is presented. Through experiments, we show that our algorithm is able to achieve high precision and recall scores in several data sets. XMLDup is also able to outperform another state-of-the-art duplicate detection solution, both in terms of efficiency and of effectiveness.

#index 1984412
#* Failure-Aware Cascaded Suppression in Wireless Sensor Networks
#@ Yi Zhang;Kristian Lum;Jun Yang
#t 2013
#c 7
#! Wireless sensor networks are widely used to continuously collect data from the environment. Because of energy constraints on battery-powered nodes, it is critical to minimize communication. Suppression has been proposed as a way to reduce communication by using predictive models to suppress reporting of predictable data. However, in the presence of communication failures, missing data are difficult to interpret because these could have been either suppressed or lost in transmission. There is no existing solution for handling failures for general, spatiotemporal suppression that uses cascading. While cascading further reduces communication, it makes failure handling difficult, because nodes can act on incomplete or incorrect information and in turn affect other nodes. We propose a cascaded suppression framework that exploits both temporal and spatial data correlation to reduce communication, and applies coding theory and Bayesian inference to recover missing data resulted from suppression and communication failures. Experiment results show that cascaded suppression significantly reduces communication cost and improves missing data recovery compared to existing approaches.

#index 1984413
#* Multiview Partitioning via Tensor Methods
#@ Xinhai Liu;Shuiwang Ji;Wolfgang Glanzel;Bart De Moor
#t 2013
#c 7
#! Clustering by integrating multiview representations has become a crucial issue for knowledge discovery in heterogeneous environments. However, most prior approaches assume that the multiple representations share the same dimension, limiting their applicability to homogeneous environments. In this paper, we present a novel tensor-based framework for integrating heterogeneous multiview data in the context of spectral clustering. Our framework includes two novel formulations; that is multiview clustering based on the integration of the Frobenius-norm objective function (MC-FR-OI) and that based on matrix integration in the Frobenius-norm objective function (MC-FR-MI). We show that the solutions for both formulations can be computed by tensor decompositions. We evaluated our methods on synthetic data and two real-world data sets in comparison with baseline methods. Experimental results demonstrate that the proposed formulations are effective in integrating multiview data in heterogeneous environments.

#index 1984414
#* Novel Biobjective Clustering (BiGC) Based on Cooperative Game Theory
#@ Vikas K. Garg;Y. Narahari;M. Narasimha Murty
#t 2013
#c 7
#! We propose a new approach to clustering. Our idea is to map cluster formation to coalition formation in cooperative games, and to use the Shapley value of the patterns to identify clusters and cluster representatives. We show that the underlying game is convex and this leads to an efficient biobjective clustering algorithm that we call BiGC. The algorithm yields high-quality clustering with respect to average point-to-center distance (potential) as well as average intracluster point-to-point distance (scatter). We demonstrate the superiority of BiGC over state-of-the-art clustering algorithms (including the center based and the multiobjective techniques) through a detailed experimentation using standard cluster validity criteria on several benchmark data sets. We also show that BiGC satisfies key clustering properties such as order independence, scale invariance, and richness.

#index 1984415
#* Update Summarization via Graph-Based Sentence Ranking
#@ Xuan Li;Liang Du;Yi-Dong Shen
#t 2013
#c 7
#! Due to the fast evolution of the information on the Internet, update summarization has received much attention in recent years. It is to summarize an evolutionary document collection at current time supposing the users have read some related previous documents. In this paper, we propose a graph-ranking-based method. It performs constrained reinforcements on a sentence graph, which unifies previous and current documents, to determine the salience of the sentences. The constraints ensure that the most salient sentences in current documents are updates to previous documents. Since this method is NP-hard, we then propose its approximate method, which is polynomial time solvable. Experiments on the TAC 2008 and 2009 benchmark data sets show the effectiveness and efficiency of our method.

#index 1984416
#* On Generalizable Low False-Positive Learning Using Asymmetric Support Vector Machines
#@ Shan-Hung Wu;Keng-Pei Lin;Hao-Heng Chien;Chung-Ming Chen;Ming-Syan Chen
#t 2013
#c 7
#! The Support Vector Machines (SVMs) have been widely used for classification due to its ability to give low generalization error. In many practical applications of classification, however, the wrong prediction of a certain class is much severer than that of the other classes, making the original SVM unsatisfactory. In this paper, we propose the notion of Asymmetric Support Vector Machine (ASVM), an asymmetric extension of the SVM, for these applications. Different from the existing SVM extensions such as thresholding and parameter tuning, ASVM employs a new objective that models the imbalance between the costs of false predictions from different classes in a novel way such that user tolerance on false-positive rate can be explicitly specified. Such a new objective formulation allows us of obtaining a lower false-positive rate without much degradation of the prediction accuracy or increase in training time. Furthermore, we show that the generalization ability is preserved with the new objective. We also study the effects of the parameters in ASVM objective and address some implementation issues related to the Sequential Minimal Optimization (SMO) to cope with large-scale data. An extensive simulation is conducted and shows that ASVM is able to yield either noticeable improvement in performance or reduction in training time as compared to the previous arts.

#index 1984417
#* Optimal Route Queries with Arbitrary Order Constraints
#@ Jing Li;Yin Yang;Nikos Mamoulis
#t 2013
#c 7
#! Given a set of spatial points $(DS)$, each of which is associated with categorical information, e.g., restaurant, pub, etc., the optimal route query finds the shortest path that starts from the query point (e.g., a home or hotel), and covers a user-specified set of categories (e.g., {pub, restaurant, museum}). The user may also specify partial order constraints between different categories, e.g., a restaurant must be visited before a pub. Previous work has focused on a special case where the query contains the total order of all categories to be visited (e.g., museum $(\rightarrow)$ restaurant $(\rightarrow)$ pub). For the general scenario without such a total order, the only known solution reduces the problem to multiple, total-order optimal route queries. As we show in this paper, this na茂ve approach incurs a significant amount of repeated computations, and, thus, is not scalable to large data sets. Motivated by this, we propose novel solutions to the general optimal route query, based on two different methodologies, namely backward search and forward search. In addition, we discuss how the proposed methods can be adapted to answer a variant of the optimal route queries, in which the route only needs to cover a subset of the given categories. Extensive experiments, using both real and synthetic data sets, confirm that the proposed solutions are efficient and practical, and outperform existing methods by large margins.

#index 1984418
#* Pay-As-You-Go Entity Resolution
#@ Steven Euijong Whang;David Marmaros;Hector Garcia-Molina
#t 2013
#c 7
#! Entity resolution (ER) is the problem of identifying which records in a database refer to the same entity. In practice, many applications need to resolve large data sets efficiently, but do not require the ER result to be exact. For example, people data from the web may simply be too large to completely resolve with a reasonable amount of work. As another example, real-time applications may not be able to tolerate any ER processing that takes longer than a certain amount of time. This paper investigates how we can maximize the progress of ER with a limited amount of work using “hints,” which give information on records that are likely to refer to the same real-world entity. A hint can be represented in various formats (e.g., a grouping of records based on their likelihood of matching), and ER can use this information as a guideline for which records to compare first. We introduce a family of techniques for constructing hints efficiently and techniques for using the hints to maximize the number of matching records identified using a limited amount of work. Using real data sets, we illustrate the potential gains of our pay-as-you-go approach compared to running ER without using hints.

#index 1984419
#* Single-Database Private Information Retrieval from Fully Homomorphic Encryption
#@ Xun Yi;Md. Golam Kaosar;Russell Paulet;Elisa Bertino
#t 2013
#c 7
#! Private Information Retrieval (PIR) allows a user to retrieve the $(i)$th bit of an $(n)$-bit database without revealing to the database server the value of $(i)$. In this paper, we present a PIR protocol with the communication complexity of $(O(\gamma \log n))$ bits, where $(\gamma)$ is the ciphertext size. Furthermore, we extend the PIR protocol to a private block retrieval (PBR) protocol, a natural and more practical extension of PIR in which the user retrieves a block of bits, instead of retrieving single bit. Our protocols are built on the state-of-the-art fully homomorphic encryption (FHE) techniques and provide privacy for the user if the underlying FHE scheme is semantically secure. The total communication complexity of our PBR is $(O(\gamma \log m+\gamma n/m))$ bits, where $(m)$ is the number of blocks. The total computation complexity of our PBR is $(O(m\log m))$ modular multiplications plus $(O(n/2))$ modular additions. In terms of total protocol execution time, our PBR protocol is more efficient than existing PBR protocols which usually require to compute $(O(n/2))$ modular multiplications when the size of a block in the database is large and a high-speed network is available.

#index 1984420
#* Toward SWSs Discovery: Mapping from WSDL to OWL-S Based on Ontology Search and Standardization Engine
#@ Tamer A. Farrag;Ahmed I. Saleh;Hesham A. Ali
#t 2013
#c 7
#! Semantic Web Services (SWSs) represent the most recent and revolutionary technology developed for machine-to-machine interaction on the web 3.0. As for the conventional web services, the problem of discovering and selecting the most suitable web service represents a challenge for SWSs to be widely used. In this paper, we propose a mapping algorithm that facilitates the redefinition of the conventional web services annotations (i.e., WSDL) using semantic annotations (i.e., OWL-S). This algorithm will be a part of a new discovery mechanism that relies on the semantic annotations of the web services to perform its task. The “local ontology repository” and “ontology search and standardization engine” are the backbone of this algorithm. Both of them target to define any data type in the system using a standard ontology-based concept. The originality of the proposed mapping algorithm is its applicability and consideration of the standardization problem. The proposed algorithm is implemented and its components are validated using some test collections and real examples. An experimental test of the proposed techniques is reported, showing the impact of the proposed algorithm in decreasing the time and the effort of the mapping process. Moreover, the experimental results promises that the proposed algorithm will have a positive impact on the discovery process as a whole.

#index 1984421
#* Trace Ratio Optimization-Based Semi-Supervised Nonlinear Dimensionality Reduction for Marginal Manifold Visualization
#@ Zhao Zhang;Tommy W. S. Chow;Mingbo Zhao
#t 2013
#c 7
#! Visualizing similarity data of different objects by exhibiting more separate organizations with local and multimodal characteristics preserved is important in multivariate data analysis. Laplacian Eigenmaps (LAE) and Locally Linear Embedding (LLE) aim at preserving the embeddings of all similarity pairs in the close vicinity of the reduced output space, but they are unable to identify and separate interclass neighbors. This paper considers the semi-supervised manifold learning problems. We apply the pairwise Cannot-Link and Must-Link constraints induced by the neighborhood graph to specify the types of neighboring pairs. More flexible regulation on supervised information is provided. Two novel multimodal nonlinear techniques, which we call trace ratio (TR) criterion-based semi-supervised LAE ($({\rm S}^2{\rm LAE})$) and LLE ($({\rm S}^2{\rm LLE})$), are then proposed for marginal manifold visualization. We also present the kernelized $({\rm S}^2{\rm LAE})$ and $({\rm S}^2{\rm LLE})$. We verify the feasibility of $({\rm S}^2{\rm LAE})$ and $({\rm S}^2{\rm LLE})$ through extensive simulations over benchmark real-world MIT CBCL, CMU PIE, MNIST, and USPS data sets. Manifold visualizations show that $({\rm S}^2{\rm LAE})$ and $({\rm S}^2{\rm LLE})$ are able to deliver large margins between different clusters or classes with multimodal distributions preserved. Clustering evaluations show they can achieve comparable to or even better results than some widely used methods.

#index 1984422
#* Change Detection in Streaming Multivariate Data Using Likelihood Detectors
#@ Ludmila I. Kuncheva
#t 2013
#c 7
#! Change detection in streaming data relies on a fast estimation of the probability that the data in two consecutive windows come from different distributions. Choosing the criterion is one of the multitude of questions that need to be addressed when designing a change detection procedure. This paper gives a log-likelihood justification for two well-known criteria for detecting change in streaming multidimensional data: Kullback-Leibler (K-L) distance and Hotelling's T-square test for equal means (H). We propose a semiparametric log-likelihood criterion (SPLL) for change detection. Compared to the existing log-likelihood change detectors, SPLL trades some theoretical rigor for computation simplicity. We examine SPLL together with K-L and H on detecting induced change on 30 real data sets. The criteria were compared using the area under the respective Receiver Operating Characteristic (ROC) curve (AUC). SPLL was found to be on the par with H and better than K-L for the nonnormalized data, and better than both on the normalized data.

#index 1984423
#* Coping with Events in Temporal Relational Databases
#@ Paolo Terenziani
#t 2013
#c 7
#! Event relations are used in many temporal relational database approaches to represent facts occurring at time instants. However, to the best of our knowledge, none of such approaches fully copes with the definition of events as provided, e.g., by the “consensus” temporal database glossary. We propose a new approach which overcomes such a limitation, allowing one to cope with multiple events occurring in the same temporal granule. This move involves major extensions to current approaches, since indeterminacy about the time and number of occurrences of events need to be faced. Specifically, we have introduced a new data model, and new definitions of relational algebraic operators coping with the above issues, and we have studied their reducibility. Last, but not least, we have shown that our approach can be easily extended in order to cope with a general form of temporal indeterminacy. Such an extension further increases the applicability of our approach.

#index 1984424
#* Cutting Plane Training for Linear Support Vector Machines
#@ Nicholas A. Arnosti;Jugal K. Kalita
#t 2013
#c 7
#! Support Vector Machines (SVMs) have been shown to achieve high performance on classification tasks across many domains, and a great deal of work has been dedicated to developing computationally efficient training algorithms for linear SVMs. One approach [1] approximately minimizes risk through use of cutting planes, and is improved by [2], [3]. We build upon this work, presenting a modification to the algorithm developed by Franc and Sonnenburg [2]. We demonstrate empirically that our changes can reduce cutting plane training time by up to 40 percent, and discuss how changes in data sets and parameter settings affect the effectiveness of our method.

#index 1984425
#* Successive Group Selection for Microaggregation
#@ Costas Panagiotakis;Georgios Tziritas
#t 2013
#c 7
#! In this paper, we propose an efficient clustering algorithm that has been applied to the microaggregation problem. The goal is to partition $(N)$ given records into clusters, each of them grouping at least $(K)$ records, so that the sum of the within-partition squared error (SSE) is minimized. We propose a successive Group Selection algorithm that approximately solves the microaggregation problem in $(O(N^2 \log N))$ time, based on sequential Minimization of SSE. Experimental results and comparisons to existing methods with similar computation cost on real and synthetic data sets demonstrate the high performance and robustness of the proposed scheme.

#index 1984426
#* Errata Corrige on "Modeling and Computing Ternary Projective Relations between Regions"
#@ Eliseo Clementini;Roland Billen;Marco Santic
#t 2013
#c 7

#index 1984460
#* A Bound on Kappa-Error Diagrams for Analysis of Classifier Ensembles
#@ Ludmila I. Kuncheva
#t 2013
#c 7
#! Kappa-error diagrams are used to gain insights about why an ensemble method is better than another on a given data set. A point on the diagram corresponds to a pair of classifiers. The x-axis is the pairwise diversity (kappa), and the y-axis is the averaged individual error. In this study, kappa is calculated from the 2\times2 correct/wrong contingency matrix. We derive a lower bound on kappa which determines the feasible part of the kappa-error diagram. Simulations and experiments with real data show that there is unoccupied feasible space on the diagram corresponding to (hypothetical) better ensembles, and that individual accuracy is the leading factor in improving the ensemble accuracy.

#index 1984461
#* λ-Diverse Nearest Neighbors Browsing for Multidimensional Data
#@ Onur Kucuktunc;Hakan Ferhatosmanoglu
#t 2013
#c 7
#! Traditional search methods try to obtain the most relevant information and rank it according to the degree of similarity to the queries. Diversity in query results is also preferred by a variety of applications since results very similar to each other cannot capture all aspects of the queried topic. In this paper, we focus on the \lambda-diverse k-nearest neighbor search problem on spatial and multidimensional data. Unlike the approach of diversifying query results in a postprocessing step, we naturally obtain diverse results with the proposed geometric and index-based methods. We first make an analogy with the concept of Natural Neighbors (NatN) and propose a natural neighbor-based method for 2D and 3D data and an incremental browsing algorithm based on Gabriel graphs for higher dimensional spaces. We then introduce a diverse browsing method based on the distance browsing feature of spatial index structures, such as R-trees. The algorithm maintains a Priority Queue with mindivdist of the objects depending on both relevancy and angular diversity and efficiently prunes nondiverse items and nodes. We experiment with a number of spatial and high-dimensional data sets, including Factual's (http://www.factual.com/) US points-of-interest data set of 13M entries. On the experimental setup, the diverse browsing method is shown to be more efficient (regarding disk accesses) than k-NN search on R-trees, and more effective (regarding Maximal Marginal Relevance (MMR)) than the diverse nearest neighbor search techniques found in the literature.

#index 1984462
#* A New Algorithm for Inferring User Search Goals with Feedback Sessions
#@ Zheng Lu;Hongyuan Zha;Xiaokang Yang;Weiyao Lin;Zhaohui Zheng
#t 2013
#c 7
#! For a broad-topic and ambiguous query, different users may have different search goals when they submit it to a search engine. The inference and analysis of user search goals can be very useful in improving search engine relevance and user experience. In this paper, we propose a novel approach to infer user search goals by analyzing search engine query logs. First, we propose a framework to discover different user search goals for a query by clustering the proposed feedback sessions. Feedback sessions are constructed from user click-through logs and can efficiently reflect the information needs of users. Second, we propose a novel approach to generate pseudo-documents to better represent the feedback sessions for clustering. Finally, we propose a new criterion )“Classified Average Precision (CAP)” to evaluate the performance of inferring user search goals. Experimental results are presented using user click-through logs from a commercial search engine to validate the effectiveness of our proposed methods.

#index 1984463
#* Annotating Search Results from Web Databases
#@ Yiyao Lu;Hai He;Hongkun Zhao;Weiyi Meng;Clement Yu
#t 2013
#c 7
#! An increasing number of databases have become web accessible through HTML form-based search interfaces. The data units returned from the underlying database are usually encoded into the result pages dynamically for human browsing. For the encoded data units to be machine processable, which is essential for many applications such as deep web data collection and Internet comparison shopping, they need to be extracted out and assigned meaningful labels. In this paper, we present an automatic annotation approach that first aligns the data units on a result page into different groups such that the data in the same group have the same semantic. Then, for each group we annotate it from different aspects and aggregate the different annotations to predict a final annotation label for it. An annotation wrapper for the search site is automatically constructed and can be used to annotate new result pages from the same web database. Our experiments indicate that the proposed approach is highly effective.

#index 1984464
#* Building a Scalable Database-Driven Reverse Dictionary
#@ Ryan Shaw;Anindya Datta;Debra VanderMeer;Kaushik Dutta
#t 2013
#c 7
#! In this paper, we describe the design and implementation of a reverse dictionary. Unlike a traditional forward dictionary, which maps from words to their definitions, a reverse dictionary takes a user input phrase describing the desired concept, and returns a set of candidate words that satisfy the input phrase. This work has significant application not only for the general public, particularly those who work closely with words, but also in the general field of conceptual search. We present a set of algorithms and the results of a set of experiments showing the retrieval accuracy of our methods and the runtime response time performance of our implementation. Our experimental results show that our approach can provide significant improvements in performance scale without sacrificing the quality of the result. Our experiments comparing the quality of our approach to that of currently available reverse dictionaries show that of our approach can provide significantly higher quality over either of the other currently available implementations.

#index 1984465
#* Discovering Temporal Change Patterns in the Presence of Taxonomies
#@ Luca Cagliero
#t 2013
#c 7
#! Frequent itemset mining is a widely exploratory technique that focuses on discovering recurrent correlations among data. The steadfast evolution of markets and business environments prompts the need of data mining algorithms to discover significant correlation changes in order to reactively suit product and service provision to customer needs. Change mining, in the context of frequent itemsets, focuses on detecting and reporting significant changes in the set of mined itemsets from one time period to another. The discovery of frequent generalized itemsets, i.e., itemsets that 1) frequently occur in the source data, and 2) provide a high-level abstraction of the mined knowledge, issues new challenges in the analysis of itemsets that become rare, and thus are no longer extracted, from a certain point. This paper proposes a novel kind of dynamic pattern, namely the History Generalized Pattern (HiGen), that represents the evolution of an itemset in consecutive time periods, by reporting the information about its frequent generalizations characterized by minimal redundancy (i.e., minimum level of abstraction) in case it becomes infrequent in a certain time period. To address HiGen mining, it proposes HiGen Miner, an algorithm that focuses on avoiding itemset mining followed by postprocessing by exploiting a support-driven itemset generalization approach. To focus the attention on the minimally redundant frequent generalizations and thus reduce the amount of the generated patterns, the discovery of a smart subset of HiGens, namely the Non-redundant HiGens, is addressed as well. Experiments performed on both real and synthetic datasets show the efficiency and the effectiveness of the proposed approach as well as its usefulness in a real application context.

#index 1984466
#* Extending BCDM to Cope with Proposals and Evaluations of Updates
#@ Luca Anselma;Alessio Bottrighi;Stefania Montani;Paolo Terenziani
#t 2013
#c 7
#! The cooperative construction of data/knowledge bases has recently had a significant impulse (see, e.g., Wikipedia [1]). In cases in which data/knowledge quality and reliability are crucial, proposals of update/insertion/deletion need to be evaluated by experts. To the best of our knowledge, no theoretical framework has been devised to model the semantics of update proposal/evaluation in the relational context. Since time is an intrinsic part of most domains (as well as of the proposal/evaluation process itself), semantic approaches to temporal relational databases (specifically, Bitemporal Conceptual Data Model (henceforth, BCDM) [2]) are the starting point of our approach. In this paper, we propose {\rm BCDM^{PV}}, a semantic temporal relational model that extends BCDM to deal with multiple update/insertion/deletion proposals and with acceptances/rejections of proposals themselves. We propose a theoretical framework, defining the new data structures, manipulation operations and temporal relational algebra and proving some basic properties, namely that {\rm BCDM^{PV}} is a consistent extension of BCDM and that it is reducible to BCDM. These properties ensure consistency with most relational temporal database frameworks, facilitating implementations.

#index 1984467
#* Facilitating Effective User Navigation through Website Structure Improvement
#@ Min Chen;Young U. Ryu
#t 2013
#c 7
#! Designing well-structured websites to facilitate effective user navigation has long been a challenge. A primary reason is that the web developers' understanding of how a website should be structured can be considerably different from that of the users. While various methods have been proposed to relink webpages to improve navigability using user navigation data, the completely reorganized new structure can be highly unpredictable, and the cost of disorienting users after the changes remains unanalyzed. This paper addresses how to improve a website without introducing substantial changes. Specifically, we propose a mathematical programming model to improve the user navigation on a website while minimizing alterations to its current structure. Results from extensive tests conducted on a publicly available real data set indicate that our model not only significantly improves the user navigation with very few changes, but also can be effectively solved. We have also tested the model on large synthetic data sets to demonstrate that it scales up very well. In addition, we define two evaluation metrics and use them to assess the performance of the improved website using the real data set. Evaluation results confirm that the user navigation on the improved structure is indeed greatly enhanced. More interestingly, we find that heavily disoriented users are more likely to benefit from the improved structure than the less disoriented users.

#index 1984468
#* Information-Theoretic Outlier Detection for Large-Scale Categorical Data
#@ Shu Wu;Shengrui Wang
#t 2013
#c 7
#! Outlier detection can usually be considered as a pre-processing step for locating, in a data set, those objects that do not conform to well-defined notions of expected behavior. It is very important in data mining for discovering novel or rare events, anomalies, vicious actions, exceptional phenomena, etc. We are investigating outlier detection for categorical data sets. This problem is especially challenging because of the difficulty of defining a meaningful similarity measure for categorical data. In this paper, we propose a formal definition of outliers and an optimization model of outlier detection, via a new concept of holoentropy that takes both entropy and total correlation into consideration. Based on this model, we define a function for the outlier factor of an object which is solely determined by the object itself and can be updated efficiently. We propose two practical 1-parameter outlier detection methods, named ITB-SS and ITB-SP, which require no user-defined parameters for deciding whether an object is an outlier. Users need only provide the number of outliers they want to detect. Experimental results show that ITB-SS and ITB-SP are more effective and efficient than mainstream methods and can be used to deal with both large and high-dimensional data sets where existing algorithms fail.

#index 1984469
#* Modeling and Solving Distributed Configuration Problems: A CSP-Based Approach
#@ Dietmar Jannach;Markus Zanker
#t 2013
#c 7
#! Product configuration can be defined as the task of tailoring a product according to the specific needs of a customer. Due to the inherent complexity of this task, which for example includes the consideration of complex constraints or the automatic completion of partial configurations, various Artificial Intelligence techniques have been explored in the last decades to tackle such configuration problems. Most of the existing approaches adopt a single-site, centralized approach. In modern supply chain settings, however, the components of a customizable product may themselves be configurable, thus requiring a multisite, distributed approach. In this paper, we analyze the challenges of modeling and solving such distributed configuration problems and propose an approach based on Distributed Constraint Satisfaction. In particular, we advocate the use of Generative Constraint Satisfaction for knowledge modeling and show in an experimental evaluation that the use of generic constraints is particularly advantageous also in the distributed problem solving phase.

#index 1984470
#* On Similarity Preserving Feature Selection
#@ Zheng Zhao;Lei Wang;Huan Liu;Jieping Ye
#t 2013
#c 7
#! In the literature of feature selection, different criteria have been proposed to evaluate the goodness of features. In our investigation, we notice that a number of existing selection criteria implicitly select features that preserve sample similarity, and can be unified under a common framework. We further point out that any feature selection criteria covered by this framework cannot handle redundant features, a common drawback of these criteria. Motivated by these observations, we propose a new "Similarity Preserving Feature Selection” framework in an explicit and rigorous way. We show, through theoretical analysis, that the proposed framework not only encompasses many widely used feature selection criteria, but also naturally overcomes their common weakness in handling feature redundancy. In developing this new framework, we begin with a conventional combinatorial optimization formulation for similarity preserving feature selection, then extend it with a sparse multiple-output regression formulation to improve its efficiency and effectiveness. A set of three algorithms are devised to efficiently solve the proposed formulations, each of which has its own advantages in terms of computational complexity and selection performance. As exhibited by our extensive experimental study, the proposed framework achieves superior feature selection performance and attractive properties.

#index 1984471
#* Protecting Sensitive Labels in Social Network Data Anonymization
#@ Mingxuan Yuan;Lei Chen;Philip S. Yu;Ting Yu
#t 2013
#c 7
#! Privacy is one of the major concerns when publishing or sharing social network data for social science research and business analysis. Recently, researchers have developed privacy models similar to k-anonymity to prevent node reidentification through structure information. However, even when these privacy models are enforced, an attacker may still be able to infer one's private information if a group of nodes largely share the same sensitive labels (i.e., attributes). In other words, the label-node relationship is not well protected by pure structure anonymization methods. Furthermore, existing approaches, which rely on edge editing or node clustering, may significantly alter key graph properties. In this paper, we define a k-degree-l-diversity anonymity model that considers the protection of structural information as well as sensitive labels of individuals. We further propose a novel anonymization methodology based on adding noise nodes. We develop a new algorithm by adding noise nodes into the original graph with the consideration of introducing the least distortion to graph properties. Most importantly, we provide a rigorous analysis of the theoretical bounds on the number of noise nodes added and their impacts on an important graph property. We conduct extensive experiments to evaluate the effectiveness of the proposed technique.

#index 1984472
#* Robust Module-Based Data Management
#@ Francois Goasdoue;Marie-Christine Rousset
#t 2013
#c 7
#! The current trend for building an ontology-based data management system (DMS) is to capitalize on efforts made to design a preexisting well-established DMS (a reference system). The method amounts to extracting from the reference DMS a piece of schema relevant to the new application needs—a module—, possibly personalizing it with extra constraints w.r.t. the application under construction, and then managing a data set using the resulting schema. In this paper, we extend the existing definitions of modules and we introduce novel properties of robustness that provide means for checking easily that a robust module-based DMS evolves safely w.r.t. both the schema and the data of the reference DMS. We carry out our investigations in the setting of description logics which underlie modern ontology languages, like RDFS, OWL, and OWL2 from W3C. Notably, we focus on the {\rm DL\hbox{-}lite}_{\cal A} dialect of the DL-lite family, which encompasses the foundations of the QL profile of OWL2 (i.e., {\rm DL\hbox{-}lite}_{\cal R}): the W3C recommendation for efficiently managing large data sets.

#index 1984473
#* Sampling Online Social Networks
#@ Manos Papagelis;Gautam Das;Nick Koudas
#t 2013
#c 7
#! As online social networking emerges, there has been increased interest to utilize the underlying network structure as well as the available information on social peers to improve the information needs of a user. In this paper, we focus on improving the performance of information collection from the neighborhood of a user in a dynamic social network. We introduce sampling-based algorithms to efficiently explore a user's social network respecting its structure and to quickly approximate quantities of interest. We introduce and analyze variants of the basic sampling scheme exploring correlations across our samples. Models of centralized and distributed social networks are considered. We show that our algorithms can be utilized to rank items in the neighborhood of a user, assuming that information for each user in the network is available. Using real and synthetic data sets, we validate the results of our analysis and demonstrate the efficiency of our algorithms in approximating quantities of interest. The methods we describe are general and can probably be easily adopted in a variety of strategies aiming to efficiently collect information from a social graph.

#index 1984474
#* Supporting Flexible, Efficient, and User-Interpretable Retrieval of Similar Time Series
#@ Stefania Montani;Giorgio Leonardi;Alessio Bottrighi;Luigi Portinale;Paolo Terenziani
#t 2013
#c 7
#! Supporting decision making in domains in which the observed phenomenon dynamics have to be dealt with, can greatly benefit of retrieval of past cases, provided that proper representation and retrieval techniques are implemented. In particular, when the parameters of interest take the form of time series, dimensionality reduction and flexible retrieval have to be addresses to this end. Classical methodological solutions proposed to cope with these issues, typically based on mathematical transforms, are characterized by strong limitations, such as a difficult interpretation of retrieval results for end users, reduced flexibility and interactivity, or inefficiency. In this paper, we describe a novel framework, in which time-series features are summarized by means of Temporal Abstractions, and then retrieved resorting to abstraction similarity. Our approach grants for interpretability of the output results, and understandability of the (user-guided) retrieval process. In particular, multilevel abstraction mechanisms and proper indexing techniques are provided, for flexible query issuing, and efficient and interactive query answering. Experimental results have shown the efficiency of our approach in a scalability test, and its superiority with respect to the use of a classical mathematical technique in flexibility, user friendliness, and also quality of results.

#index 1984475
#* The Minimum Consistent Subset Cover Problem: A Minimization View of Data Mining
#@ Byron J. Gao;Martin Ester;Hui Xiong;Jin-Yi Cai;Oliver Schulte
#t 2013
#c 7
#! In this paper, we introduce and study the minimum consistent subset cover (MCSC) problem. Given a finite ground set X and a constraint t, find the minimum number of consistent subsets that cover X, where a subset of X is consistent if it satisfies t. The MCSC problem generalizes the traditional set covering problem and has minimum clique partition (MCP), a dual problem of graph coloring, as an instance. Many common data mining tasks in rule learning, clustering, and pattern mining can be formulated as MCSC instances. In particular, we discuss the minimum rule set (MRS) problem that minimizes model complexity of decision rules, the converse k-clustering problem that minimizes the number of clusters, and the pattern summarization problem that minimizes the number of patterns. For any of these MCSC instances, our proposed generic algorithm CAG can be directly applicable. CAG starts by constructing a maximal optimal partial solution, then performs an example-driven specific-to-general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set.

#index 1984476
#* Transductive Multilabel Learning via Label Set Propagation
#@ Xiangnan Kong;Michael K. Ng;Zhi-Hua Zhou
#t 2013
#c 7
#! The problem of multilabel classification has attracted great interest in the last decade, where each instance can be assigned with a set of multiple class labels simultaneously. It has a wide variety of real-world applications, e.g., automatic image annotations and gene function analysis. Current research on multilabel classification focuses on supervised settings which assume existence of large amounts of labeled training data. However, in many applications, the labeling of multilabeled data is extremely expensive and time consuming, while there are often abundant unlabeled data available. In this paper, we study the problem of transductive multilabel learning and propose a novel solution, called Trasductive Multilabel Classification (TraM), to effectively assign a set of multiple labels to each instance. Different from supervised multilabel learning methods, we estimate the label sets of the unlabeled instances effectively by utilizing the information from both labeled and unlabeled data. We first formulate the transductive multilabel learning as an optimization problem of estimating label concept compositions. Then, we derive a closed-form solution to this optimization problem and propose an effective algorithm to assign label sets to the unlabeled instances. Empirical studies on several real-world multilabel learning tasks demonstrate that our TraM method can effectively boost the performance of multilabel classification by using both labeled and unlabeled data.

#index 1984477
#* Minimally Supervised Novel Relation Extraction Using a Latent Relational Mapping
#@ Danushka Bollegala;Yutaka Matsuo;Mitsuru Ishizuka
#t 2013
#c 7
#! The World Wide Web includes semantic relations of numerous types that exist among different entities. Extracting the relations that exist between two entities is an important step in various Web-related tasks such as information retrieval (IR), information extraction, and social network extraction. A supervised relation extraction system that is trained to extract a particular relation type (source relation) might not accurately extract a new type of a relation (target relation) for which it has not been trained. However, it is costly to create training data manually for every new relation type that one might want to extract. We propose a method to adapt an existing relation extraction system to extract new relation types with minimum supervision. Our proposed method comprises two stages: learning a lower dimensional projection between different relations, and learning a relational classifier for the target relation type with instance sampling. First, to represent a semantic relation that exists between two entities, we extract lexical and syntactic patterns from contexts in which those two entities co-occur. Then, we construct a bipartite graph between relation-specific (RS) and relation-independent (RI) patterns. Spectral clustering is performed on the bipartite graph to compute a lower dimensional projection. Second, we train a classifier for the target relation type using a small number of labeled instances. To account for the lack of target relation training instances, we present a one-sided under sampling method. We evaluate the proposed method using a data set that contains 2,000 instances for 20 different relation types. Our experimental results show that the proposed method achieves a statistically significant macroaverage F-score of 62.77. Moreover, the proposed method outperforms numerous baselines and a previously proposed weakly supervised relation extraction method.

#index 1984478
#* Mining User Queries with Markov Chains: Application to Online Image Retrieval
#@ Konstantinos A. Raftopoulos;Klimis S. Ntalianis;Dionysios D. Sourlas;Stefanos D. Kollias
#t 2013
#c 7
#! We propose a novel method for automatic annotation, indexing and annotation-based retrieval of images. The new method, that we call Markovian Semantic Indexing (MSI), is presented in the context of an online image retrieval system. Assuming such a system, the users' queries are used to construct an Aggregate Markov Chain (AMC) through which the relevance between the keywords seen by the system is defined. The users' queries are also used to automatically annotate the images. A stochastic distance between images, based on their annotation and the keyword relevance captured in the AMC, is then introduced. Geometric interpretations of the proposed distance are provided and its relation to a clustering in the keyword space is investigated. By means of a new measure of Markovian state similarity, the mean first cross passage time (CPT), optimality properties of the proposed distance are proved. Images are modeled as points in a vector space and their similarity is measured with MSI. The new method is shown to possess certain theoretical advantages and also to achieve better Precision versus Recall results when compared to Latent Semantic Indexing (LSI) and probabilistic Latent Semantic Indexing (pLSI) methods in Annotation-Based Image Retrieval (ABIR) tasks.

#index 1984479
#* Reinforced Similarity Integration in Image-Rich Information Networks
#@ Xin Jin;Jiebo Luo;Jie Yu;Gang Wang;Dhiraj Joshi;Jiawei Han
#t 2013
#c 7
#! Social multimedia sharing and hosting websites, such as Flickr and Facebook, contain billions of user-submitted images. Popular Internet commerce websites such as Amazon.com are also furnished with tremendous amounts of product-related images. In addition, images in such social networks are also accompanied by annotations, comments, and other information, thus forming heterogeneous image-rich information networks. In this paper, we introduce the concept of (heterogeneous) image-rich information network and the problem of how to perform information retrieval and recommendation in such networks. We propose a fast algorithm heterogeneous minimum order k-SimRank (HMok-SimRank) to compute link-based similarity in weighted heterogeneous information networks. Then, we propose an algorithm Integrated Weighted Similarity Learning (IWSL) to account for both link-based and content-based similarities by considering the network structure and mutually reinforcing link similarity and feature weight learning. Both local and global feature learning methods are designed. Experimental results on Flickr and Amazon data sets show that our approach is significantly better than traditional methods in terms of both relevance and speed. A new product search and recommendation system for e-commerce has been implemented based on our algorithm.

#index 1984480
#* Supporting Search-As-You-Type Using SQL in Databases
#@ Guoliang Li;Jianhua Feng;Chen Li
#t 2013
#c 7
#! A search-as-you-type system computes answers on-the-fly as a user types in a keyword query character by character. We study how to support search-as-you-type on data residing in a relational DBMS. We focus on how to support this type of search using the native database language, SQL. A main challenge is how to leverage existing database functionalities to meet the high-performance requirement to achieve an interactive speed. We study how to use auxiliary indexes stored as tables to increase search performance. We present solutions for both single-keyword queries and multikeyword queries, and develop novel techniques for fuzzy search using SQL by allowing mismatches between query keywords and answers. We present techniques to answer first-N queries and discuss how to support updates efficiently. Experiments on large, real data sets show that our techniques enable DBMS systems on a commodity computer to support search-as-you-type on tables with millions of records.

#index 1984481
#* Simple Hybrid and Incremental Postpruning Techniques for Rule Induction
#@ Khurram Shehzad
#t 2013
#c 7
#! Pruning achieves the dual goal of reducing the complexity of the final hypothesis for improved comprehensibility, and improving its predictive accuracy by minimizing the overfitting due to noisy data. This paper presents a new hybrid pruning technique for rule induction, as well as an incremental postpruning technique based on a misclassification tolerance. Although both have been designed for RULES-7, the latter is also applicable to any rule induction algorithm in general. A thorough empirical evaluation reveals that the proposed techniques enable RULES-7 to outperform other state-of-the-art classification techniques. The improved classifier is also more accurate and up to two orders of magnitude faster than before.

#index 1984482
#* A Rough-Set-Based Incremental Approach for Updating Approximations under Dynamic Maintenance Environments
#@ Hongmei Chen;Tianrui Li;Da Ruan;Jianhui Lin;Chengxiang Hu
#t 2013
#c 7
#! Approximations of a concept by a variable precision rough-set model (VPRS) usually vary under a dynamic information system environment. It is thus effective to carry out incremental updating approximations by utilizing previous data structures. This paper focuses on a new incremental method for updating approximations of VPRS while objects in the information system dynamically alter. It discusses properties of information granulation and approximations under the dynamic environment while objects in the universe evolve over time. The variation of an attribute's domain is also considered to perform incremental updating for approximations under VPRS. Finally, an extensive experimental evaluation validates the efficiency of the proposed method for dynamic maintenance of VPRS approximations.

#index 1984483
#* Outgoing EIC Editorial
#@ Beng Chin Ooi
#t 2013
#c 7

#index 1984484
#* A Generalized Flow-Based Method for Analysis of Implicit Relationships on Wikipedia
#@ Xinpeng Zhang;Yasuhito Asano;Masatoshi Yoshikawa
#t 2013
#c 7
#! We focus on measuring relationships between pairs of objects in Wikipedia whose pages can be regarded as individual objects. Two kinds of relationships between two objects exist: in Wikipedia, an explicit relationship is represented by a single link between the two pages for the objects, and an implicit relationship is represented by a link structure containing the two pages. Some of the previously proposed methods for measuring relationships are cohesion-based methods, which underestimate objects having high degrees, although such objects could be important in constituting relationships in Wikipedia. The other methods are inadequate for measuring implicit relationships because they use only one or two of the following three important factors: distance, connectivity, and cocitation. We propose a new method using a generalized maximum flow which reflects all the three factors and does not underestimate objects having high degree. We confirm through experiments that our method can measure the strength of a relationship more appropriately than these previously proposed methods do. Another remarkable aspect of our method is mining elucidatory objects, that is, objects constituting a relationship. We explain that mining elucidatory objects would open a novel way to deeply understand a relationship.

#index 1984485
#* A Proxy-Based Approach to Continuous Location-Based Spatial Queries in Mobile Environments
#@ Jiun-Long Huang;Chen-Che Huang
#t 2013
#c 7
#! Caching valid regions of spatial queries at mobile clients is effective in reducing the number of queries submitted by mobile clients and query load on the server. However, mobile clients suffer from longer waiting time for the server to compute valid regions. We propose in this paper a proxy-based approach to continuous nearest-neighbor (NN) and window queries. The proxy creates estimated valid regions (EVRs) for mobile clients by exploiting spatial and temporal locality of spatial queries. For NN queries, we devise two new algorithms to accelerate EVR growth, leading the proxy to build effective EVRs even when the cache size is small. On the other hand, we propose to represent the EVRs of window queries in the form of vectors, called estimated window vectors (EWVs), to achieve larger estimated valid regions. This novel representation and the associated creation algorithm result in more effective EVRs of window queries. In addition, due to the distinct characteristics, we use separate index structures, namely EVR-tree and grid index, for NN queries and window queries, respectively. To further increase efficiency, we develop algorithms to exploit the results of NN queries to aid grid index growth, benefiting EWV creation of window queries. Similarly, the grid index is utilized to support NN query answering and EVR updating. We conduct several experiments for performance evaluation. The experimental results show that the proposed approach significantly outperforms the existing proxy-based approaches.

#index 1984486
#* A System to Filter Unwanted Messages from OSN User Walls
#@ Marco Vanetti;Elisabetta Binaghi;Elena Ferrari;Barbara Carminati;Moreno Carullo
#t 2013
#c 7
#! One fundamental issue in today's Online Social Networks (OSNs) is to give users the ability to control the messages posted on their own private space to avoid that unwanted content is displayed. Up to now, OSNs provide little support to this requirement. To fill the gap, in this paper, we propose a system allowing OSN users to have a direct control on the messages posted on their walls. This is achieved through a flexible rule-based system, that allows users to customize the filtering criteria to be applied to their walls, and a Machine Learning-based soft classifier automatically labeling messages in support of content-based filtering.

#index 1984487
#* Anonymization of Centralized and Distributed Social Networks by Sequential Clustering
#@ Tamir Tassa;Dror Cohen
#t 2013
#c 7
#! We study the problem of privacy-preservation in social networks. We consider the distributed setting in which the network data is split between several data holders. The goal is to arrive at an anonymized view of the unified network without revealing to any of the data holders information about links between nodes that are controlled by other data holders. To that end, we start with the centralized setting and offer two variants of an anonymization algorithm which is based on sequential clustering (Sq). Our algorithms significantly outperform the SaNGreeA algorithm due to Campan and Truta which is the leading algorithm for achieving anonymity in networks by means of clustering. We then devise secure distributed versions of our algorithms. To the best of our knowledge, this is the first study of privacy preservation in distributed social networks. We conclude by outlining future research proposals in that direction.

#index 1984488
#* AML: Efficient Approximate Membership Localization within a Web-Based Join Framework
#@ Zhixu Li;Laurianne Sitbon;Liwei Wang;Xiaofang Zhou;Xiaoyong Du
#t 2013
#c 7
#! In this paper, we propose a new type of Dictionary-based Entity Recognition Problem, named Approximate Membership Localization (AML). The popular Approximate Membership Extraction (AME) provides a full coverage to the true matched substrings from a given document, but many redundancies cause a low efficiency of the AME process and deteriorate the performance of real-world applications using the extracted substrings. The AML problem targets at locating nonoverlapped substrings which is a better approximation to the true matched substrings without generating overlapped redundancies. In order to perform AML efficiently, we propose the optimized algorithm P-Prune that prunes a large part of overlapped redundant matched substrings before generating them. Our study using several real-word data sets demonstrates the efficiency of P-Prune over a baseline method. We also study the AML in application to a proposed web-based join framework scenario which is a search-based approach joining two tables using dictionary-based entity recognition from web documents. The results not only prove the advantage of AML over AME, but also demonstrate the effectiveness of our search-based approach.

#index 1984489
#* Clustering Large Probabilistic Graphs
#@ George Kollios;Michalis Potamias;Evimaria Terzi
#t 2013
#c 7
#! We study the problem of clustering probabilistic graphs. Similar to the problem of clustering standard graphs, probabilistic graph clustering has numerous applications, such as finding complexes in probabilistic protein-protein interaction (PPI) networks and discovering groups of users in affiliation networks. We extend the edit-distance-based definition of graph clustering to probabilistic graphs. We establish a connection between our objective function and correlation clustering to propose practical approximation algorithms for our problem. A benefit of our approach is that our objective function is parameter-free. Therefore, the number of clusters is part of the output. We also develop methods for testing the statistical significance of the output clustering and study the case of noisy clusterings. Using a real protein-protein interaction network and ground-truth data, we show that our methods discover the correct number of clusters and identify established protein relationships. Finally, we show the practicality of our techniques using a large social network of Yahoo! users consisting of one billion edges.

#index 1984490
#* Detecting Intrinsic Loops Underlying Data Manifold
#@ Deyu Meng;Yee Leung;Zongben Xu
#t 2013
#c 7
#! Detecting intrinsic loop structures of a data manifold is the necessary prestep for the proper employment of the manifold learning techniques and of fundamental importance in the discovery of the essential representational features underlying the data lying on the loopy manifold. An effective strategy is proposed to solve this problem in this study. In line with our intuition, a formal definition of a loop residing on a manifold is first given. Based on this definition, theoretical properties of loopy manifolds are rigorously derived. In particular, a necessary and sufficient condition for detecting essential loops of a manifold is derived. An effective algorithm for loop detection is then constructed. The soundness of the proposed theory and algorithm is validated by a series of experiments performed on synthetic and real-life data sets. In each of the experiments, the essential loops underlying the data manifold can be properly detected, and the intrinsic representational features of the data manifold can be revealed along the loop structure so detected. Particularly, some of these features can hardly be discovered by the conventional manifold learning methods.

#index 1984491
#* Event Tracking for Real-Time Unaware Sensitivity Analysis (EventTracker)
#@ Siamak Tavakoli;Alireza Mousavi;Peter Broomhead
#t 2013
#c 7
#! This paper introduces a platform for online Sensitivity Analysis (SA) that is applicable in large scale real-time data acquisition (DAQ) systems. Here, we use the term real-time in the context of a system that has to respond to externally generated input stimuli within a finite and specified period. Complex industrial systems such as manufacturing, healthcare, transport, and finance require high-quality information on which to base timely responses to events occurring in their volatile environments. The motivation for the proposed EventTracker platform is the assumption that modern industrial systems are able to capture data in real-time and have the necessary technological flexibility to adjust to changing system requirements. The flexibility to adapt can only be assured if data is succinctly interpreted and translated into corrective actions in a timely manner. An important factor that facilitates data interpretation and information modeling is an appreciation of the affect system inputs have on each output at the time of occurrence. Many existing sensitivity analysis methods appear to hamper efficient and timely analysis due to a reliance on historical data, or sluggishness in providing a timely solution that would be of use in real-time applications. This inefficiency is further compounded by computational limitations and the complexity of some existing models. In dealing with real-time event driven systems, the underpinning logic of the proposed method is based on the assumption that in the vast majority of cases changes in input variables will trigger events. Every single or combination of events could subsequently result in a change to the system state. The proposed event tracking sensitivity analysis method describes variables and the system state as a collection of events. The higher the numeric occurrence of an input variable at the trigger level during an event monitoring interval, the greater is its impact on the final analysis of the system state. Experiments were designed to compare the proposed event tracking sensitivity analysis method with a comparable method (that of Entropy). An improvement of 10 percent in computational efficiency without loss in accuracy was observed. The comparison also showed that the time taken to perform the sensitivity analysis was 0.5 percent of that required when using the comparable Entropy-based method.

#index 1984492
#* Fast Activity Detection: Indexing for Temporal Stochastic Automaton-Based Activity Models
#@ Massimiliano Albanese;Andrea Pugliese;V. S. Subrahmanian
#t 2013
#c 7
#! Today, numerous applications require the ability to monitor a continuous stream of fine-grained data for the occurrence of certain high-level activities. A number of computerized systems—including ATM networks, web servers, and intrusion detection systems—systematically track every atomic action we perform, thus generating massive streams of timestamped observation data, possibly from multiple concurrent activities. In this paper, we address the problem of efficiently detecting occurrences of high-level activities from such interleaved data streams. A solution to this important problem would greatly benefit a broad range of applications, including fraud detection, video surveillance, and cyber security. There has been extensive work in the last few years on modeling activities using probabilistic models. In this paper, we propose a temporal probabilistic graph so that the elapsed time between observations also plays a role in defining whether a sequence of observations constitutes an activity. We first propose a data structure called “temporal multiactivity graph” to store multiple activities that need to be concurrently monitored. We then define an index called Temporal Multiactivity Graph Index Creation (tMAGIC) that, based on this data structure, examines and links observations as they occur. We define algorithms for insertion and bulk insertion into the tMAGIC index and show that this can be efficiently accomplished. We also define algorithms to solve two problems: the “evidence” problem that tries to find all occurrences of an activity (with probability over a threshold) within a given sequence of observations, and the “identification” problem that tries to find the activity that best matches a sequence of observations. We introduce complexity reducing restrictions and pruning strategies to make the problem—which is intrinsically exponential—linear to the number of observations. Our experiments confirm that tMAGIC has time and space complexity linear to the size of the input, and can efficiently retrieve instances of the monitored activities.

#index 1984493
#* Finding Rare Classes: Active Learning with Generative and Discriminative Models
#@ Timothy M. Hospedales;Shaogang Gong;Tao Xiang
#t 2013
#c 7
#! Discovering rare categories and classifying new instances of them are important data mining issues in many fields, but fully supervised learning of a rare class classifier is prohibitively costly in labeling effort. There has therefore been increasing interest both in active discovery: to identify new classes quickly, and active learning: to train classifiers with minimal supervision. These goals occur together in practice and are intrinsically related because examples of each class are required to train a classifier. Nevertheless, very few studies have tried to optimise them together, meaning that data mining for rare classes in new domains makes inefficient use of human supervision. Developing active learning algorithms to optimise both rare class discovery and classification simultaneously is challenging because discovery and classification have conflicting requirements in query criteria. In this paper, we address these issues with two contributions: a unified active learning model to jointly discover new categories and learn to classify them by adapting query criteria online; and a classifier combination algorithm that switches generative and discriminative classifiers as learning progresses. Extensive evaluation on a batch of standard UCI and vision data sets demonstrates the superiority of this approach over existing methods.

#index 1984494
#* Halite: Fast and Scalable Multiresolution Local-Correlation Clustering
#@ Robson L.  F. Cordeiro;Agma J.  M. Traina;Christos Faloutsos;Caetano Traina Jr.
#t 2013
#c 7
#! This paper proposes Halite, a novel, fast, and scalable clustering method that looks for clusters in subspaces of multidimensional data. Existing methods are typically superlinear in space or execution time. Halite's strengths are that it is fast and scalable, while still giving highly accurate results. Specifically the main contributions of Halite are: 1) Scalability: it is linear or quasi linear in time and space regarding the data size and dimensionality, and the dimensionality of the clusters' subspaces; 2) Usability: it is deterministic, robust to noise, doesn't take the number of clusters as an input parameter, and detects clusters in subspaces generated by original axes or by their linear combinations, including space rotation; 3) Effectiveness: it is accurate, providing results with equal or better quality compared to top related works; and 4) Generality: it includes a soft clustering approach. Experiments on synthetic data ranging from five to 30 axes and up to 1 \rm million points were performed. Halite was in average at least 12 times faster than seven representative works, and always presented highly accurate results. On real data, Halite was at least 11 times faster than others, increasing their accuracy in up to 35 percent. Finally, we report experiments in a real scenario where soft clustering is desirable.

#index 1984495
#* k-Pattern Set Mining under Constraints
#@ Tias Guns;Siegfried Nijssen;Luc de Raedt
#t 2013
#c 7
#! We introduce the problem of k-pattern set mining, concerned with finding a set of k related patterns under constraints. This contrasts to regular pattern mining, where one searches for many individual patterns. The k-pattern set mining problem is a very general problem that can be instantiated to a wide variety of well-known mining tasks including concept-learning, rule-learning, redescription mining, conceptual clustering and tiling. To this end, we formulate a large number of constraints for use in k-pattern set mining, both at the local level, that is, on individual patterns, and on the global level, that is, on the overall pattern set. Building general solvers for the pattern set mining problem remains a challenge. Here, we investigate to what extent constraint programming (CP) can be used as a general solution strategy. We present a mapping of pattern set constraints to constraints currently available in CP. This allows us to investigate a large number of settings within a unified framework and to gain insight in the possibilities and limitations of these solvers. This is important as it allows us to create guidelines in how to model new problems successfully and how to model existing problems more efficiently. It also opens up the way for other solver technologies.

#index 1984690
#* A Method for Mining Infrequent Causal Associations and Its Application in Finding Adverse Drug Reaction Signal Pairs
#@ Yanqing Ji;Hao Ying;John Tran;Peter Dews;Ayman Mansour;R. Michael Massanari
#t 2013
#c 7
#! In many real-world applications, it is important to mine causal relationships where an event or event pattern causes certain outcomes with low probability. Discovering this kind of causal relationships can help us prevent or correct negative outcomes caused by their antecedents. In this paper, we propose an innovative data mining framework and apply it to mine potential causal associations in electronic patient data sets where the drug-related events of interest occur infrequently. Specifically, we created a novel interestingness measure, exclusive causal-leverage, based on a computational, fuzzy recognition-primed decision (RPD) model that we previously developed. On the basis of this new measure, a data mining algorithm was developed to mine the causal relationship between drugs and their associated adverse drug reactions (ADRs). The algorithm was tested on real patient data retrieved from the Veterans Affairs Medical Center in Detroit, Michigan. The retrieved data included 16,206 patients (15,605 male, 601 female). The exclusive causal-leverage was employed to rank the potential causal associations between each of the three selected drugs (i.e., enalapril, pravastatin, and rosuvastatin) and 3,954 recorded symptoms, each of which corresponded to a potential ADR. The top 10 drug-symptom pairs for each drug were evaluated by the physicians on our project team. The numbers of symptoms considered as likely real ADRs for enalapril, pravastatin, and rosuvastatin were 8, 7, and 6, respectively. These preliminary results indicate the usefulness of our method in finding potential ADR signal pairs for further analysis (e.g., epidemiology study) and investigation (e.g., case review) by drug safety professionals.

#index 1984691
#* A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning
#@ Salvador Garcia;Julian Luengo;Jose A. Saez;Victoria Lopez;Francisco Herrera
#t 2013
#c 7
#! Discretization is an essential preprocessing technique used in many knowledge discovery and data mining tasks. Its main goal is to transform a set of continuous attributes into discrete ones, by associating categorical values to intervals and thus transforming quantitative data into qualitative data. In this manner, symbolic data mining algorithms can be applied over continuous data and the representation of information is simplified, making it more concise and specific. The literature provides numerous proposals of discretization and some attempts to categorize them into a taxonomy can be found. However, in previous papers, there is a lack of consensus in the definition of the properties and no formal categorization has been established yet, which may be confusing for practitioners. Furthermore, only a small set of discretizers have been widely considered, while many other methods have gone unnoticed. With the intention of alleviating these problems, this paper provides a survey of discretization methods proposed in the literature from a theoretical and empirical perspective. From the theoretical perspective, we develop a taxonomy based on the main properties pointed out in previous research, unifying the notation and including all the known methods up to date. Empirically, we conduct an experimental study in supervised classification involving the most representative and newest discretizers, different types of classifiers, and a large number of data sets. The results of their performances measured in terms of accuracy, number of intervals, and inconsistency have been verified by means of nonparametric statistical tests. Additionally, a set of discretizers are highlighted as the best performing ones.

#index 1984692
#* Clustering Uncertain Data Based on Probability Distribution Similarity
#@ Bin Jiang;Jian Pei;Yufei Tao;Xuemin Lin
#t 2013
#c 7
#! Clustering on uncertain data, one of the essential tasks in mining uncertain data, posts significant challenges on both modeling similarity between uncertain objects and developing efficient computational methods. The previous methods extend traditional partitioning clustering methods like $(k)$-means and density-based clustering methods like DBSCAN to uncertain data, thus rely on geometric distances between objects. Such methods cannot handle uncertain objects that are geometrically indistinguishable, such as products with the same mean but very different variances in customer ratings. Surprisingly, probability distributions, which are essential characteristics of uncertain objects, have not been considered in measuring similarity between uncertain objects. In this paper, we systematically model uncertain objects in both continuous and discrete domains, where an uncertain object is modeled as a continuous and discrete random variable, respectively. We use the well-known Kullback-Leibler divergence to measure similarity between uncertain objects in both the continuous and discrete cases, and integrate it into partitioning and density-based clustering methods to cluster uncertain objects. Nevertheless, a na茂ve implementation is very costly. Particularly, computing exact KL divergence in the continuous case is very costly or even infeasible. To tackle the problem, we estimate KL divergence in the continuous case by kernel density estimation and employ the fast Gauss transform technique to further speed up the computation. Our extensive experiment results verify the effectiveness, efficiency, and scalability of our approaches.

#index 1984693
#* Efficient Evaluation of SUM Queries over Probabilistic Data
#@ Reza Akbarinia;Patrick Valduriez;Guillaume Verger
#t 2013
#c 7
#! SUM queries are crucial for many applications that need to deal with uncertain data. In this paper, we are interested in the queries, called ALL_SUM, that return all possible sum values and their probabilities. In general, there is no efficient solution for the problem of evaluating ALL_SUM queries. But, for many practical applications, where aggregate values are small integers or real numbers with small precision, it is possible to develop efficient solutions. In this paper, based on a recursive approach, we propose a new solution for those applications. We implemented our solution and conducted an extensive experimental evaluation over synthetic and real-world data sets; the results show its effectiveness.

#index 1984694
#* Efficient Service Skyline Computation for Composite Service Selection
#@ Qi Yu;Athman Bouguettaya
#t 2013
#c 7
#! Service composition is emerging as an effective vehicle for integrating existing web services to create value-added and personalized composite services. As web services with similar functionality are expected to be provided by competing providers, a key challenge is to find the “best” web services to participate in the composition. When multiple quality aspects (e.g., response time, fee, etc.) are considered, a weighting mechanism is usually adopted by most existing approaches, which requires users to specify their preferences as numeric values. We propose to exploit the dominance relationship among service providers to find a set of “best” possible composite services, referred to as a composite service skyline. We develop efficient algorithms that allow us to find the composite service skyline from a significantly reduced searching space instead of considering all possible service compositions. We propose a novel bottom-up computation framework that enables the skyline algorithm to scale well with the number of services in a composition. We conduct a comprehensive analytical and experimental study to evaluate the effectiveness, efficiency, and scalability of the composite skyline computation approaches.

#index 1984695
#* Finding Probabilistic Prevalent Colocations in Spatially Uncertain Data Sets
#@ Lizhen Wang;Pingping Wu;Hongmei Chen
#t 2013
#c 7
#! A spatial colocation pattern is a group of spatial features whose instances are frequently located together in geographic space. Discovering colocations has many useful applications. For example, colocated plant species discovered from plant distribution data sets can contribute to the analysis of plant geography, phytosociology studies, and plant protection recommendations. In this paper, we study the colocation mining problem in the context of uncertain data, as the data generated from a wide range of data sources are inherently uncertain. One straightforward method to mine the prevalent colocations in a spatially uncertain data set is to simply compute the expected participation index of a candidate and decide if it exceeds a minimum prevalence threshold. Although this definition has been widely adopted, it misses important information about the confidence which can be associated with the participation index of a colocation. We propose another definition, probabilistic prevalent colocations, trying to find all the colocations that are likely to be prevalent in a randomly generated possible world. Finding probabilistic prevalent colocations (PPCs) turn out to be difficult. First, we propose pruning strategies for candidates to reduce the amount of computation of the probabilistic participation index values. Next, we design an improved dynamic programming algorithm for identifying candidates. This algorithm is suitable for parallel computation, and approximate computation. Finally, the effectiveness and efficiency of the methods proposed as well as the pruning strategies and the optimization techniques are verified by extensive experiments with “real $(+)$ synthetic” spatially uncertain data sets.

#index 1984696
#* Fuzzy Web Data Tables Integration Guided by an Ontological and Terminological Resource
#@ Patrice Buche;Juliette Dibie-Barthelemy;Liliana Ibanescu;Lydie Soler
#t 2013
#c 7
#! In this paper, we present the design of ONDINE system which allows the loading and the querying of a data warehouse opened on the Web, guided by an Ontological and Terminological Resource (OTR). The data warehouse, composed of data tables extracted from Web documents, has been built to supplement existing local data sources. First, we present the main steps of our semiautomatic method to annotate data tables driven by an OTR. The output of this method is an XML/RDF data warehouse composed of XML documents representing data tables with their fuzzy RDF annotations. We then present our flexible querying system which allows the local data sources and the data warehouse to be simultaneously and uniformly queried, using the OTR. This system relies on SPARQL and allows approximate answers to be retrieved by comparing preferences expressed as fuzzy sets with fuzzy RDF annotations.

#index 1984697
#* PMSE: A Personalized Mobile Search Engine
#@ Kenneth Wai-Ting Leung;Dik Lun Lee;Wang-Chien Lee
#t 2013
#c 7
#! We propose a personalized mobile search engine (PMSE) that captures the users' preferences in the form of concepts by mining their clickthrough data. Due to the importance of location information in mobile search, PMSE classifies these concepts into content concepts and location concepts. In addition, users' locations (positioned by GPS) are used to supplement the location concepts in PMSE. The user preferences are organized in an ontology-based, multifacet user profile, which are used to adapt a personalized ranking function for rank adaptation of future search results. To characterize the diversity of the concepts associated with a query and their relevances to the user's need, four entropies are introduced to balance the weights between the content and location facets. Based on the client-server model, we also present a detailed architecture and design for implementation of PMSE. In our design, the client collects and stores locally the clickthrough data to protect privacy, whereas heavy tasks such as concept extraction, training, and reranking are performed at the PMSE server. Moreover, we address the privacy issue by restricting the information in the user profile exposed to the PMSE server with two privacy parameters. We prototype PMSE on the Google Android platform. Experimental results show that PMSE significantly improves the precision comparing to the baseline.

#index 1984698
#* Range-Based Skyline Queries in Mobile Environments
#@ Xin Lin;Jianliang Xu;Haibo Hu
#t 2013
#c 7
#! Skyline query processing for location-based services, which considers both spatial and nonspatial attributes of the objects being queried, has recently received increasing attention. Existing solutions focus on solving point- or line-based skyline queries, in which the query location is an exact location point or a line segment. However, due to privacy concerns and limited precision of localization devices, the input of a user location is often a spatial range. This paper studies a new problem of how to process such range-based skyline queries. Two novel algorithms are proposed: one is index-based (I-SKY) and the other is not based on any index (N-SKY). To handle frequent movements of the objects being queried, we also propose incremental versions of I-SKY and N-SKY, which avoid recomputing the query index and results from scratch. Additionally, we develop efficient solutions for probabilistic and continuous range-based skyline queries. Experimental results show that our proposed algorithms well outperform the baseline algorithm that adopts the existing line-based skyline solution. Moreover, the incremental versions of I-SKY and N-SKY save substantial computation cost, especially when the objects move frequently.

#index 1984699
#* Skyline Processing on Distributed Vertical Decompositions
#@ George Trimponias;Ilaria Bartolini;Dimitris Papadias;Yin Yang
#t 2013
#c 7
#! We assume a data set that is vertically decomposed among several servers, and a client that wishes to compute the skyline by obtaining the minimum number of points. Existing solutions for this problem are restricted to the case where each server maintains exactly one dimension. This paper proposes a general solution for vertical decompositions of arbitrary dimensionality. We first investigate some interesting problem characteristics regarding the pruning power of points. Then, we introduce vertical partition skyline (VPS), an algorithmic framework that includes two steps. Phase 1 searches for an anchor point $(P_{anc})$ that dominates, and hence eliminates, a large number of records. Starting with $(P_{anc})$, Phase 2 constructs incrementally a pruning area using an interesting union-intersection property of dominance regions. Servers do not transmit points that fall within the pruning area in their local subspace. Our experiments confirm the effectiveness of the proposed methods under various settings.

#index 1984700
#* Spatial Query Integrity with Voronoi Neighbors
#@ Ling Hu;Wei-Shinn Ku;Spiridon Bakiras;Cyrus Shahabi
#t 2013
#c 7
#! With the popularity of location-based services and the abundant usage of smart phones and GPS-enabled devices, the necessity of outsourcing spatial data has grown rapidly over the past few years. Meanwhile, the fast arising trend of cloud storage and cloud computing services has provided a flexible and cost-effective platform for hosting data from businesses and individuals, further enabling many location-based applications. Nevertheless, in this database outsourcing paradigm, the authentication of the query results at the client remains a challenging problem. In this paper, we focus on the Outsourced Spatial Database (OSDB) model and propose an efficient scheme, called $({VN{\hbox{-}}Auth})$, which allows a client to verify the correctness and completeness of the result set. Our approach is based on neighborhood information derived from the Voronoi diagram of the underlying spatial data set and can handle fundamental spatial query types, such as $(k)$ nearest neighbor and range queries, as well as more advanced query types like reverse $(k)$ nearest neighbor, aggregate nearest neighbor, and spatial skyline. We evaluated VN-Auth based on real-world data sets using mobile devices (Google Droid smart phones with Android OS) as query clients. Compared to the current state-of-the-art approaches (i.e., methods based on Merkle Hash Trees), our experiments show that VN-Auth produces significantly smaller verification objects and is more computationally efficient, especially for queries with low selectivity.

#index 1984701
#* Supporting Pattern-Preserving Anonymization for Time-Series Data
#@ Lidan Shou;Xuan Shang;Ke Chen;Gang Chen;Chao Zhang
#t 2013
#c 7
#! Time series is an important form of data available in numerous applications and often contains vast amount of personal privacy. The need to protect privacy in time-series data while effectively supporting complex queries on them poses nontrivial challenges to the database community. We study the anonymization of time series while trying to support complex queries, such as range and pattern matching queries, on the published data. The conventional k-anonymity model cannot effectively address this problem as it may suffer severe pattern loss. We propose a novel anonymization model called (k, P)-anonymity for pattern-rich time series. This model publishes both the attribute values and the patterns of time series in separate data forms. We demonstrate that our model can prevent linkage attacks on the published data while effectively support a wide variety of queries on the anonymized data. We propose two algorithms to enforce (k, P)-anonymity on time-series data. Our anonymity model supports customized data publishing, which allows a certain part of the values but a different part of the pattern of the anonymized time series to be published simultaneously. We present estimation techniques to support query processing on such customized data. The proposed methods are evaluated in a comprehensive experimental study. Our results verify the effectiveness and efficiency of our approach.

#index 1984702
#* Synchronization-Inspired Partitioning and Hierarchical Clustering
#@ Junming Shao;Xiao He;Christian Bohm;Qinli Yang;Claudia Plant
#t 2013
#c 7
#! Synchronization is a powerful and inherently hierarchical concept regulating a large variety of complex processes ranging from the metabolism in a cell to opinion formation in a group of individuals. Synchronization phenomena in nature have been widely investigated and models concisely describing the dynamical synchronization process have been proposed, e.g., the well-known Extensive Kuramoto Model. We explore the potential of the Extensive Kuramoto Model for data clustering. We regard each data object as a phase oscillator and simulate the dynamical behavior of the objects over time. By interaction with similar objects, the phase of an object gradually aligns with its neighborhood, resulting in a nonlinear object movement naturally driven by the local cluster structure. We demonstrate that our framework has several attractive benefits: 1) It is suitable to detect clusters of arbitrary number, shape, and data distribution, even in difficult settings with noise points and outliers. 2) Combined with the Minimum Description Length (MDL) principle, it allows partitioning and hierarchical clustering without requiring any input parameters which are difficult to estimate. 3) Synchronization faithfully captures the natural hierarchical cluster structure of the data and MDL suggests meaningful levels of abstraction. Extensive experiments demonstrate the effectiveness and efficiency of our approach.

#index 1984703
#* Transfer across Completely Different Feature Spaces via Spectral Embedding
#@ Xiaoxiao Shi;Qi Liu;Wei Fan;Philip S. Yu
#t 2013
#c 7
#! In many applications, it is very expensive or time consuming to obtain a lot of labeled examples. One practically important problem is: can the labeled data from other related sources help predict the target task, even if they have 1) different feature spaces (e.g., image versus text data), 2) different data distributions, and 3) different output spaces? This paper proposes a solution and discusses the conditions where this is highly likely to produce better results. It first unifies the feature spaces of the target and source data sets by spectral embedding, even when they are with completely different feature spaces. The principle is to devise an optimization objective that preserves the original structure of the data, while at the same time, maximizes the similarity between the two. A linear projection model, as well as a nonlinear approach are derived on the basis of this principle with closed forms. Second, a judicious sample selection strategy is applied to select only those related source examples. At last, a Bayesian-based approach is applied to model the relationship between different output spaces. The three steps can bridge related heterogeneous sources in order to learn the target task. Among the 20 experiment data sets, for example, the images with wavelet-transformed-based features are used to predict another set of images whose features are constructed from color-histogram space; documents are used to help image classification, etc. By using these extracted examples from heterogeneous sources, the models can reduce the error rate by as much as 50 percent, compared with the methods using only the examples from the target task.

#index 1984704
#* Tweet Analysis for Real-Time Event Detection and Earthquake Reporting System Development
#@ Takeshi Sakaki;Makoto Okazaki;Yutaka Matsuo
#t 2013
#c 7
#! Twitter has received much attention recently. An important characteristic of Twitter is its real-time nature. We investigate the real-time interaction of events such as earthquakes in Twitter and propose an algorithm to monitor tweets and to detect a target event. To detect a target event, we devise a classifier of tweets based on features such as the keywords in a tweet, the number of words, and their context. Subsequently, we produce a probabilistic spatiotemporal model for the target event that can find the center of the event location. We regard each Twitter user as a sensor and apply particle filtering, which are widely used for location estimation. The particle filter works better than other comparable methods for estimating the locations of target events. As an application, we develop an earthquake reporting system for use in Japan. Because of the numerous earthquakes and the large number of Twitter users throughout the country, we can detect an earthquake with high probability (93 percent of earthquakes of Japan Meteorological Agency (JMA) seismic intensity scale 3 or more are detected) merely by monitoring tweets. Our system detects earthquakes promptly and notification is delivered much faster than JMA broadcast announcements.

#index 1984705
#* TW-$(k)$-Means: Automated Two-Level Variable Weighting Clustering Algorithm for Multiview Data
#@ Xiaojun Chen;Xiaofei Xu;Joshua Huang;Yunming Ye
#t 2013
#c 7
#! This paper proposes TW-$(k)$-means, an automated two-level variable weighting clustering algorithm for multiview data, which can simultaneously compute weights for views and individual variables. In this algorithm, a view weight is assigned to each view to identify the compactness of the view and a variable weight is also assigned to each variable in the view to identify the importance of the variable. Both view weights and variable weights are used in the distance function to determine the clusters of objects. In the new algorithm, two additional steps are added to the iterative $(k)$-means clustering process to automatically compute the view weights and the variable weights. We used two real-life data sets to investigate the properties of two types of weights in TW-$(k)$-means and investigated the difference between the weights of TW-$(k)$-means and the weights of the individual variable weighting method. The experiments have revealed the convergence property of the view weights in TW-$(k)$-means. We compared TW-$(k)$-means with five clustering algorithms on three real-life data sets and the results have shown that the TW-$(k)$-means algorithm significantly outperformed the other five clustering algorithms in four evaluation indices.

#index 1984706
#* U-Skyline: A New Skyline Query for Uncertain Databases
#@ Xingjie Liu;De-Nian Yang;Mao Ye;Wang-Chien Lee
#t 2013
#c 7
#! The skyline query, aiming at identifying a set of skyline tuples that are not dominated by any other tuple, is particularly useful for multicriteria data analysis and decision making. For uncertain databases, a probabilistic skyline query, called P-Skyline, has been developed to return skyline tuples by specifying a probability threshold. However, the answer obtained via a P-Skyline query usually includes skyline tuples undesirably dominating each other when a small threshold is specified; or it may contain much fewer skyline tuples if a larger threshold is employed. To address this concern, we propose a new uncertain skyline query, called U-Skyline query, in this paper. Instead of setting a probabilistic threshold to qualify each skyline tuple independently, the U-Skyline query searches for a set of tuples that has the highest probability (aggregated from all possible scenarios) as the skyline answer. In order to answer U-Skyline queries efficiently, we propose a number of optimization techniques for query processing, including 1) computational simplification of U-Skyline probability, 2) pruning of unqualified candidate skylines and early termination of query processing, 3) reduction of the input data set, and 4) partition and conquest of the reduced data set. We perform a comprehensive performance evaluation on our algorithm and an alternative approach that formulates the U-Skyline processing problem by integer programming. Experimental results demonstrate that our algorithm is 10-100 times faster than using CPLEX, a parallel integer programming solver, to answer the U-Skyline query.

#index 2000587
#* A Survival Modeling Approach to Biomedical Search Result Diversification Using Wikipedia
#@ Xiaoshi Yin;Jimmy Huang;Zhoujun Li;Xiaofeng Zhou
#t 2013
#c 7
#! In this paper, we propose a survival modeling approach to promoting ranking diversity for biomedical information retrieval. The proposed approach concerns with finding relevant documents that can deliver more different aspects of a query. First, two probabilistic models derived from the survival analysis theory are proposed for measuring aspect novelty. Second, a new method using Wikipedia to detect aspects covered by retrieved documents is presented. Third, an aspect filter based on a two-stage model is introduced. It ranks the detected aspects in decreasing order of the probability that an aspect is generated by the query. Finally, the relevance and the novelty of retrieved documents are combined at the aspect level for reranking. Experiments conducted on the TREC 2006 and 2007 Genomics collections demonstrate the effectiveness of the proposed approach in promoting ranking diversity for biomedical information retrieval. Moreover, we further evaluate our approach in the Web retrieval environment. The evaluation results on the ClueWeb09-T09B collection show that our approach can achieve promising performance improvements.

#index 2000588
#* Centroid-Based Actionable 3D Subspace Clustering
#@ Kelvin Sim;Ghim-Eng Yap;David R. Hardoon;Vivekanand Gopalkrishnan;Gao Cong;Suryani Lukman
#t 2013
#c 7
#! Actionable 3D subspace clustering from real-world continuous-valued 3D (i.e., object-attribute-context) data promises tangible benefits such as discovery of biologically significant protein residues and profitable stocks, but existing algorithms are inadequate in solving this clustering problem; most of them are not actionable (ability to suggest profitable or beneficial actions to users), do not allow incorporation of domain knowledge, and are parameter sensitive, i.e., the wrong threshold setting reduces the cluster quality. Moreover, its 3D structure complicates this clustering problem. We propose a centroid-based actionable 3D subspace clustering framework, named CATSeeker, which allows incorporation of domain knowledge, and achieves parameter insensitivity and excellent performance through a unique combination of singular value decomposition, numerical optimization, and 3D frequent itemset mining. Experimental results on synthetic, protein structural, and financial data show that CATSeeker significantly outperforms all the competing methods in terms of efficiency, parameter insensitivity, and cluster usefulness.

#index 2000589
#* Constrained Text Coclustering with Supervised and Unsupervised Constraints
#@ Yangqiu Song;Shimei Pan;Shixia Liu;Furu Wei;Michelle X. Zhou;Weihong Qian
#t 2013
#c 7
#! In this paper, we propose a novel constrained coclustering method to achieve two goals. First, we combine information-theoretic coclustering and constrained clustering to improve clustering performance. Second, we adopt both supervised and unsupervised constraints to demonstrate the effectiveness of our algorithm. The unsupervised constraints are automatically derived from existing knowledge sources, thus saving the effort and cost of using manually labeled constraints. To achieve our first goal, we develop a two-sided hidden Markov random field (HMRF) model to represent both document and word constraints. We then use an alternating expectation maximization (EM) algorithm to optimize the model. We also propose two novel methods to automatically construct and incorporate document and word constraints to support unsupervised constrained clustering: 1) automatically construct document constraints based on overlapping named entities (NE) extracted by an NE extractor; 2) automatically construct word constraints based on their semantic distance inferred from WordNet. The results of our evaluation over two benchmark data sets demonstrate the superiority of our approaches against a number of existing approaches.

#index 2000590
#* Crowdsourced Trace Similarity with Smartphones
#@ Demetrios Zeinalipour-Yazti;Christos Laoudias;Costantinos Costa;Michalis Vlachos;Maria I. Andreou;Dimitrios Gunopulos
#t 2013
#c 7
#! Smartphones are nowadays equipped with a number of sensors, such as WiFi, GPS, accelerometers, etc. This capability allows smartphone users to easily engage in crowdsourced computing services, which contribute to the solution of complex problems in a distributed manner. In this work, we leverage such a computing paradigm to solve efficiently the following problem: comparing a query trace $(Q)$ against a crowd of traces generated and stored on distributed smartphones. Our proposed framework, coined $({\rm SmartTrace}^+)$, provides an effective solution without disclosing any part of the crowd traces to the query processor. $({\rm SmartTrace}^+)$, relies on an in-situ data storage model and intelligent top-K query processing algorithms that exploit distributed trajectory similarity measures, resilient to spatial and temporal noise, in order to derive the most relevant answers to $(Q)$. We evaluate our algorithms on both synthetic and real workloads. We describe our prototype system developed on the Android OS. The solution is deployed over our own SmartLab testbed of 25 smartphones. Our study reveals that computations over $({\rm SmartTrace}^+)$ result in substantial energy conservation; in addition, results can be computed faster than competitive approaches.

#index 2000591
#* Customized Policies for Handling Partial Information in Relational Databases
#@ Maria Vanina Martinez;Cristian Molinaro;John Grant;V. S. Subrahmanian
#t 2013
#c 7
#! Most real-world databases have at least some missing data. Today, users of such databases are “on their own” in terms of how they manage this incompleteness. In this paper, we propose the general concept of partial information policy (PIP) operator to handle incompleteness in relational databases. PIP operators build upon preference frameworks for incomplete information, but accommodate different types of incomplete data (e.g., a value exists but is not known; a value does not exist; a value may or may not exist). Different users in the real world have different ways in which they want to handle incompleteness—PIP operators allow them to specify a policy that matches their attitude to risk and their knowledge of the application and how the data was collected. We propose index structures for efficiently evaluating PIP operators and experimentally assess their effectiveness on a real-world airline data set. We also study how relational algebra operators and PIP operators interact with one another.

#index 2000592
#* Decision Trees for Mining Data Streams Based on the McDiarmid's Bound
#@ Leszek Rutkowski;Lena Pietruczuk;Piotr Duda;Maciej Jaworski
#t 2013
#c 7
#! In mining data streams the most popular tool is the Hoeffding tree algorithm. It uses the Hoeffding's bound to determine the smallest number of examples needed at a node to select a splitting attribute. In the literature the same Hoeffding's bound was used for any evaluation function (heuristic measure), e.g., information gain or Gini index. In this paper, it is shown that the Hoeffding's inequality is not appropriate to solve the underlying problem. We prove two theorems presenting the McDiarmid's bound for both the information gain, used in ID3 algorithm, and for Gini index, used in Classification and Regression Trees (CART) algorithm. The results of the paper guarantee that a decision tree learning system, applied to data streams and based on the McDiarmid's bound, has the property that its output is nearly identical to that of a conventional learner. The results of the paper have a great impact on the state of the art of mining data streams and various developed so far methods and algorithms should be reconsidered.

#index 2000593
#* Discovering Characterizations of the Behavior of Anomalous Subpopulations
#@ Fabrizio Angiulli;Fabio Fassetti;Luigi Palopoli
#t 2013
#c 7
#! We consider the problem of discovering attributes, or properties, accounting for the a priori stated abnormality of a group of anomalous individuals (the outliers) with respect to an overall given population (the inliers). To this aim, we introduce the notion of exceptional property and define the concept of exceptionality score, which measures the significance of a property. In particular, in order to single out exceptional properties, we resort to a form of minimum distance estimation for evaluating the badness of fit of the values assumed by the outliers compared to the probability distribution associated with the values assumed by the inliers. Suitable exceptionality scores are introduced for both numeric and categorical attributes. These scores are, both from the analytical and the empirical point of view, designed to be effective for small samples, as it is the case for outliers. We present an algorithm, called $({\rm EXPREX})$, for efficiently discovering exceptional properties. The algorithm is able to reduce the needed computational effort by not exploring many irrelevant numerical intervals and by exploiting suitable pruning rules. The experimental results confirm that our technique is able to provide knowledge characterizing outliers in a natural manner.

#index 2000594
#* FoCUS: Learning to Crawl Web Forums
#@ Jingtian Jiang;Xinying Song;Nenghai Yu;Chin-Yew Lin
#t 2013
#c 7
#! In this paper, we present Forum Crawler Under Supervision (FoCUS), a supervised web-scale forum crawler. The goal of FoCUS is to crawl relevant forum content from the web with minimal overhead. Forum threads contain information content that is the target of forum crawlers. Although forums have different layouts or styles and are powered by different forum software packages, they always have similar implicit navigation paths connected by specific URL types to lead users from entry pages to thread pages. Based on this observation, we reduce the web forum crawling problem to a URL-type recognition problem. And we show how to learn accurate and effective regular expression patterns of implicit navigation paths from automatically created training sets using aggregated results from weak page type classifiers. Robust page type classifiers can be trained from as few as five annotated forums and applied to a large set of unseen forums. Our test results show that FoCUS achieved over 98 percent effectiveness and 97 percent coverage on a large set of test forums powered by over 150 different forum software packages. In addition, the results of applying FoCUS on more than 100 community Question and Answer sites and Blog sites demonstrated that the concept of implicit navigation path could apply to other social media sites.

#index 2000595
#* Improving Word Similarity by Augmenting PMI with Estimates of Word Polysemy
#@ Lushan Han;Tim Finin;Paul McNamee;Anupam Joshi;Yelena Yesha
#t 2013
#c 7
#! Pointwise mutual information (PMI) is a widely used word similarity measure, but it lacks a clear explanation of how it works. We explore how PMI differs from distributional similarity, and we introduce a novel metric, $({\rm PMI}_{max})$, that augments PMI with information about a word's number of senses. The coefficients of $({\rm PMI}_{max})$ are determined empirically by maximizing a utility function based on the performance of automatic thesaurus generation. We show that it outperforms traditional PMI in the application of automatic thesaurus generation and in two word similarity benchmark tasks: human similarity ratings and TOEFL synonym questions. $({\rm PMI}_{max})$ achieves a correlation coefficient comparable to the best knowledge-based approaches on the Miller-Charles similarity rating data set.

#index 2000596
#* Incentive Compatible Privacy-Preserving Data Analysis
#@ Murat Kantarcioglu;Wei Jiang
#t 2013
#c 7
#! In many cases, competing parties who have private data may collaboratively conduct privacy-preserving distributed data analysis (PPDA) tasks to learn beneficial data models or analysis results. Most often, the competing parties have different incentives. Although certain PPDA techniques guarantee that nothing other than the final analysis result is revealed, it is impossible to verify whether participating parties are truthful about their private input data. Unless proper incentives are set, current PPDA techniques cannot prevent participating parties from modifying their private inputs. This raises the question of how to design incentive compatible privacy-preserving data analysis techniques that motivate participating parties to provide truthful inputs. In this paper, we first develop key theorems, then base on these theorems, we analyze certain important privacy-preserving data analysis tasks that could be conducted in a way that telling the truth is the best choice for any participating party.

#index 2000597
#* Nonnegative Matrix Factorization: A Comprehensive Review
#@ Yu-Xiong Wang;Yu-Jin Zhang
#t 2013
#c 7
#! Nonnegative Matrix Factorization (NMF), a relatively novel paradigm for dimensionality reduction, has been in the ascendant since its inception. It incorporates the nonnegativity constraint and thus obtains the parts-based representation as well as enhancing the interpretability of the issue correspondingly. This survey paper mainly focuses on the theoretical research into NMF over the last 5 years, where the principles, basic models, properties, and algorithms of NMF along with its various modifications, extensions, and generalizations are summarized systematically. The existing NMF algorithms are divided into four categories: Basic NMF (BNMF), Constrained NMF (CNMF), Structured NMF (SNMF), and Generalized NMF (GNMF), upon which the design principles, characteristics, problems, relationships, and evolution of these algorithms are presented and analyzed comprehensively. Some related work not on NMF that NMF should learn from or has connections with is involved too. Moreover, some open issues remained to be solved are discussed. Several relevant application areas of NMF are also briefly described. This survey aims to construct an integrated, state-of-the-art framework for NMF concept, from which the follow-up research may benefit.

#index 2000598
#* On Identifying Critical Nuggets of Information during Classification Tasks
#@ David Sathiaraj;Evangelos Triantaphyllou
#t 2013
#c 7
#! In large databases, there may exist critical nuggets—small collections of records or instances that contain domain-specific important information. This information can be used for future decision making such as labeling of critical, unlabeled data records and improving classification results by reducing false positive and false negative errors. This work introduces the idea of critical nuggets, proposes an innovative domain-independent method to measure criticality, suggests a heuristic to reduce the search space for finding critical nuggets, and isolates and validates critical nuggets from some real-world data sets. It seems that only a few subsets may qualify to be critical nuggets, underlying the importance of finding them. The proposed methodology can detect them. This work also identifies certain properties of critical nuggets and provides experimental validation of the properties. Experimental results also helped validate that critical nuggets can assist in improving classification accuracies in real-world data sets.

#index 2000599
#* Radio Database Compression for Accurate Energy-Efficient Localization in Fingerprinting Systems
#@ Azin Arya;Philippe Godlewski;Marine Campedel;Ghislain du Che ne
#t 2013
#c 7
#! Location fingerprinting is a positioning method that exploits the already existing infrastructures such as cellular networks or WLANs. Regarding the recent demand for energy efficient networks and the emergence of issues like green networking, we propose a clustering technique to compress the radio database in the context of cellular fingerprinting systems. The aim of the proposed technique is to reduce the computation cost and transmission load in the mobile-based implementations. The presented method may be called Block-based Weighted Clustering (BWC) technique, which is applied in a concatenated location-radio signal space, and attributes different weight factors to the location and radio components. Computer simulations and real experiments have been conducted to evaluate the performance of our proposed technique in the context of a GSM network. The obtained results confirm the efficiency of the BWC technique, and show that it improves the performance of standard k-means and hierarchical clustering methods.

#index 2000600
#* Semi-Supervised Nonlinear Hashing Using Bootstrap Sequential Projection Learning
#@ Chenxia Wu;Jianke Zhu;Deng Cai;Chun Chen;Jiajun Bu
#t 2013
#c 7
#! In this paper, we study the effective semi-supervised hashing method under the framework of regularized learning-based hashing. A nonlinear hash function is introduced to capture the underlying relationship among data points. Thus, the dimensionality of the matrix for computation is not only independent from the dimensionality of the original data space but also much smaller than the one using linear hash function. To effectively deal with the error accumulated during converting the real-value embeddings into the binary code after relaxation, we propose a semi-supervised nonlinear hashing algorithm using bootstrap sequential projection learning which effectively corrects the errors by taking into account of all the previous learned bits holistically without incurring the extra computational overhead. Experimental results on the six benchmark data sets demonstrate that the presented method outperforms the state-of-the-art hashing algorithms at a large margin.

#index 2000601
#* Spatial Approximate String Search
#@ Feifei Li;Bin Yao;Mingwang Tang;Marios Hadjieleftheriou
#t 2013
#c 7
#! This work deals with the approximate string search in large spatial databases. Specifically, we investigate range queries augmented with a string similarity search predicate in both euclidean space and road networks. We dub this query the spatial approximate string (Sas) query. In euclidean space, we propose an approximate solution, the MhR-tree, which embeds min-wise signatures into an R-tree. The min-wise signature for an index node $(u)$ keeps a concise representation of the union of $(q)$-grams from strings under the subtree of $(u)$. We analyze the pruning functionality of such signatures based on the set resemblance between the query string and the $(q)$-grams from the subtrees of index nodes. We also discuss how to estimate the selectivity of a Sas query in euclidean space, for which we present a novel adaptive algorithm to find balanced partitions using both the spatial and string information stored in the tree. For queries on road networks, we propose a novel exact method, RsasSol, which significantly outperforms the baseline algorithm in practice. The RsasSol combines the $(q)$-gram-based inverted lists and the reference nodes based pruning. Extensive experiments on large real data sets demonstrate the efficiency and effectiveness of our approaches.

#index 2000602
#* SVStream: A Support Vector-Based Algorithm for Clustering Data Streams
#@ Chang-Dong Wang;Jian-Huang Lai;Dong Huang;Wei-Shi Zheng
#t 2013
#c 7
#! In this paper, we propose a novel data stream clustering algorithm, termed SVStream, which is based on support vector domain description and support vector clustering. In the proposed algorithm, the data elements of a stream are mapped into a kernel space, and the support vectors are used as the summary information of the historical elements to construct cluster boundaries of arbitrary shape. To adapt to both dramatic and gradual changes, multiple spheres are dynamically maintained, each describing the corresponding data domain presented in the data stream. By allowing for bounded support vectors (BSVs), the proposed SVStream algorithm is capable of identifying overlapping clusters. A BSV decaying mechanism is designed to automatically detect and remove outliers (noise). We perform experiments over synthetic and real data streams, with the overlapping, evolving, and noise situations taken into consideration. Comparison results with state-of-the-art data stream clustering methods demonstrate the effectiveness and efficiency of the proposed method.

#index 2000603
#* The Move-Split-Merge Metric for Time Series
#@ Alexandra Stefan;Vassilis Athitsos;Gautam Das
#t 2013
#c 7
#! A novel metric for time series, called Move-Split-Merge (MSM), is proposed. This metric uses as building blocks three fundamental operations: Move, Split, and Merge, which can be applied in sequence to transform any time series into any other time series. A Move operation changes the value of a single element, a Split operation converts a single element into two consecutive elements, and a Merge operation merges two consecutive elements into one. Each operation has an associated cost, and the MSM distance between two time series is defined to be the cost of the cheapest sequence of operations that transforms the first time series into the second one. An efficient, quadratic-time algorithm is provided for computing the MSM distance. MSM has the desirable properties of being metric, in contrast to the Dynamic Time Warping (DTW) distance, and invariant to the choice of origin, in contrast to the Edit Distance with Real Penalty (ERP) metric. At the same time, experiments with public time series data sets demonstrate that MSM is a meaningful distance measure, that oftentimes leads to lower nearest neighbor classification error rate compared to DTW and ERP.

#index 2000604
#* A User-Friendly Patent Search Paradigm
#@ Yang Cao;Ju Fan;Guoliang Li
#t 2013
#c 7
#! As an important operation for finding existing relevant patents and validating a new patent application, patent search has attracted considerable attention recently. However, many users have limited knowledge about the underlying patents, and they have to use a try-and-see approach to repeatedly issue different queries and check answers, which is a very tedious process. To address this problem, in this paper, we propose a new user-friendly patent search paradigm, which can help users find relevant patents more easily and improve user search experience. We propose three effective techniques, error correction, topic-based query suggestion, and query expansion, to improve the usability of patent search. We also study how to efficiently find relevant answers from a large collection of patents. We first partition patents into small partitions based to their topics and classes. Then, given a query, we find highly relevant partitions and answer the query in each of such highly relevant partitions. Finally, we combine the answers of each partition and generate top-$(k)$ answers of the patent-search query.

#index 2000612
#* A Context-Based Word Indexing Model for Document Summarization
#@ Pawan Goyal;Laxmidhar Behera;Thomas McGinnity
#t 2013
#c 7
#! Existing models for document summarization mostly use the similarity between sentences in the document to extract the most salient sentences. The documents as well as the sentences are indexed using traditional term indexing measures, which do not take the context into consideration. Therefore, the sentence similarity values remain independent of the context. In this paper, we propose a context sensitive document indexing model based on the Bernoulli model of randomness. The Bernoulli model of randomness has been used to find the probability of the cooccurrences of two terms in a large corpus. A new approach using the lexical association between terms to give a context sensitive weight to the document terms has been proposed. The resulting indexing weights are used to compute the sentence similarity matrix. The proposed sentence similarity measure has been used with the baseline graph-based ranking models for sentence extraction. Experiments have been conducted over the benchmark DUC data sets and it has been shown that the proposed Bernoulli-based sentence similarity model provides consistent improvements over the baseline IntraLink and UniformLink methods [1].

#index 2000613
#* A Segmentation and Graph-Based Video Sequence Matching Method for Video Copy Detection
#@ Liu Hong;Hong Lu;Xiangyang Xue
#t 2013
#c 7
#! We propose in this paper a segmentation and graph-based video sequence matching method for video copy detection. Specifically, due to the good stability and discriminative ability of local features, we use SIFT descriptor for video content description. However, matching based on SIFT descriptor is computationally expensive for large number of points and the high dimension. Thus, to reduce the computational complexity, we first use the dual-threshold method to segment the videos into segments with homogeneous content and extract keyframes from each segment. SIFT features are extracted from the keyframes of the segments. Then, we propose an SVD-based method to match two video frames with SIFT point set descriptors. To obtain the video sequence matching result, we propose a graph-based method. It can convert the video sequence matching into finding the longest path in the frame matching-result graph with time constraint. Experimental results demonstrate that the segmentation and graph-based video sequence matching method can detect video copies effectively. Also, the proposed method has advantages. Specifically, it can automatically find optimal sequence matching result from the disordered matching results based on spatial feature. It can also reduce the noise caused by spatial feature matching. And it is adaptive to video frame rate changes. Experimental results also demonstrate that the proposed method can obtain a better tradeoff between the effectiveness and the efficiency of video copy detection.

#index 2000614
#* Cross-Domain Sentiment Classification Using a Sentiment Sensitive Thesaurus
#@ Danushka Bollegala;David Weir;John Carroll
#t 2013
#c 7
#! Automatic classification of sentiment is important for numerous applications such as opinion mining, opinion summarization, contextual advertising, and market analysis. Typically, sentiment classification has been modeled as the problem of training a binary classifier using reviews annotated for positive or negative sentiment. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is costly. Applying a sentiment classifier trained using labeled data for a particular domain to classify sentiment of user reviews on a different domain often results in poor performance because words that occur in the train (source) domain might not appear in the test (target) domain. We propose a method to overcome this problem in cross-domain sentiment classification. First, we create a sentiment sensitive distributional thesaurus using labeled data for the source domains and unlabeled data for both source and target domains. Sentiment sensitivity is achieved in the thesaurus by incorporating document level sentiment labels in the context vectors used as the basis for measuring the distributional similarity between words. Next, we use the created thesaurus to expand feature vectors during train and test times in a binary classifier. The proposed method significantly outperforms numerous baselines and returns results that are comparable with previously proposed cross-domain sentiment classification methods on a benchmark data set containing Amazon user reviews for different types of products. We conduct an extensive empirical analysis of the proposed method on single- and multisource domain adaptation, unsupervised and supervised domain adaptation, and numerous similarity measures for creating the sentiment sensitive thesaurus. Moreover, our comparisons against the SentiWordNet, a lexical resource for word polarity, show that the created sentiment-sensitive thesaurus accurately captures words that express similar sentiments.

#index 2000615
#* Determining $(k)$-Most Demanding Products with Maximum Expected Number of Total Customers
#@ Chen-Yi Lin;Jia-Ling Koh;Arbee L. P. Chen
#t 2013
#c 7
#! In this paper, a problem of production plans, named k-most demanding products ($(k)$-MDP) discovering, is formulated. Given a set of customers demanding a certain type of products with multiple attributes, a set of existing products of the type, a set of candidate products that can be offered by a company, and a positive integer $(k)$, we want to help the company to select $(k)$ products from the candidate products such that the expected number of the total customers for the $(k)$ products is maximized. We show the problem is NP-hard when the number of attributes for a product is 3 or more. One greedy algorithm is proposed to find approximate solution for the problem. We also attempt to find the optimal solution of the problem by estimating the upper bound of the expected number of the total customers for a set of $(k)$ candidate products for reducing the search space of the optimal solution. An exact algorithm is then provided to find the optimal solution of the problem by using this pruning strategy. The experiment results demonstrate that both the efficiency and memory requirement of the exact algorithm are comparable to those for the greedy algorithm, and the greedy algorithm is well scalable with respect to $(k)$.

#index 2000616
#* Dirichlet Process Mixture Model for Document Clustering with Feature Partition
#@ Ruizhang Huang;Guan Yu;ZhaoJun Wang
#t 2013
#c 7
#! Finding the appropriate number of clusters to which documents should be partitioned is crucial in document clustering. In this paper, we propose a novel approach, namely DPMFP, to discover the latent cluster structure based on the DPM model without requiring the number of clusters as input. Document features are automatically partitioned into two groups, in particular, discriminative words and nondiscriminative words, and contribute differently to document clustering. A variational inference algorithm is investigated to infer the document collection structure as well as the partition of document words at the same time. Our experiments indicate that our proposed approach performs well on the synthetic data set as well as real data sets. The comparison between our approach and state-of-the-art document clustering approaches shows that our approach is robust and effective for document clustering.

#index 2000617
#* Discriminative Nonnegative Spectral Clustering with Out-of-Sample Extension
#@ Yang Yang;Yi Yang;Heng Tao Shen;Yanchun Zhang;Xiaoyong Du;Xiaofang Zhou
#t 2013
#c 7
#! Data clustering is one of the fundamental research problems in data mining and machine learning. Most of the existing clustering methods, for example, normalized cut and $(k)$-means, have been suffering from the fact that their optimization processes normally lead to an NP-hard problem due to the discretization of the elements in the cluster indicator matrix. A practical way to cope with this problem is to relax this constraint to allow the elements to be continuous values. The eigenvalue decomposition can be applied to generate a continuous solution, which has to be further discretized. However, the continuous solution is probably mixing-signed. This result may cause it deviate severely from the true solution, which should be naturally nonnegative. In this paper, we propose a novel clustering algorithm, i.e., discriminative nonnegative spectral clustering, to explicitly impose an additional nonnegative constraint on the cluster indicator matrix to seek for a more interpretable solution. Moreover, we show an effective regularization term which is able to not only provide more useful discriminative information but also learn a mapping function to predict cluster labels for the out-of-sample test data. Extensive experiments on various data sets illustrate the superiority of our proposal compared to the state-of-the-art clustering algorithms.

#index 2000618
#* Efficient Algorithms for Mining High Utility Itemsets from Transactional Databases
#@ Vincent S. Tseng;Bai-En Shie;Cheng-Wei Wu;Philip S. Yu
#t 2013
#c 7
#! Mining high utility itemsets from a transactional database refers to the discovery of itemsets with high utility like profits. Although a number of relevant algorithms have been proposed in recent years, they incur the problem of producing a large number of candidate itemsets for high utility itemsets. Such a large number of candidate itemsets degrades the mining performance in terms of execution time and space requirement. The situation may become worse when the database contains lots of long transactions or long high utility itemsets. In this paper, we propose two algorithms, namely utility pattern growth (UP-Growth) and UP-Growth+, for mining high utility itemsets with a set of effective strategies for pruning candidate itemsets. The information of high utility itemsets is maintained in a tree-based data structure named utility pattern tree (UP-Tree) such that candidate itemsets can be generated efficiently with only two scans of database. The performance of UP-Growth and UP-Growth+ is compared with the state-of-the-art algorithms on many types of both real and synthetic data sets. Experimental results show that the proposed algorithms, especially UP-Growth+, not only reduce the number of candidates effectively but also outperform other algorithms substantially in terms of runtime, especially when databases contain lots of long transactions.

#index 2000619
#* Entity Translation Mining from Comparable Corpora: Combining Graph Mapping with Corpus Latent Features
#@ Jinhan Kim;Seung-won Hwang;Long Jiang;Young-In Song;Ming Zhou
#t 2013
#c 7
#! This paper addresses the problem of mining named entity translations from comparable corpora, specifically, mining English and Chinese named entity translation. We first observe that existing approaches use one or more of the following named entity similarity metrics: entity, entity context, and relationship. Motivated by this observation, we propose a new holistic approach by 1) combining all similarity types used and 2) additionally considering relationship context similarity between pairs of named entities, a missing quadrant in the taxonomy of similarity metrics. We abstract the named entity translation problem as the matching of two named entity graphs extracted from the comparable corpora. Specifically, named entity graphs are first constructed from comparable corpora to extract relationship between named entities. Entity similarity and entity context similarity are then calculated from every pair of bilingual named entities. A reinforcing method is utilized to reflect relationship similarity and relationship context similarity between named entities. We also discover "latent" features lost in the graph extraction process and integrate this into our framework. According to our experimental results, our holistic graph-based approach and its enhancement using corpus latent features are highly effective and our framework significantly outperforms previous approaches.

#index 2000620
#* Harnessing Folksonomies to Produce a Social Classification of Resources
#@ Arkaitz Zubiaga;Victor Fresno;Raquel Martinez;Alberto P. Garcia-Plaza
#t 2013
#c 7
#! In our daily lives, organizing resources like books or webpages into a set of categories to ease future access is a common task. The usual largeness of these collections requires a vast endeavor and an outrageous expense to organize manually. As an approach to effectively produce an automated classification of resources, we consider the immense amounts of annotations provided by users on social tagging systems in the form of bookmarks. In this paper, we deal with the utilization of these user-provided tags to perform a social classification of resources. For this purpose, we have created three large-scale social tagging data sets including tagging data for different types of resources, webpages and books. Those resources are accompanied by categorization data from sound expert-driven taxonomies. We analyze the characteristics of the three social tagging systems and perform an analysis on the usefulness of social tags to perform a social classification of resources that resembles the classification by experts as much as possible. We analyze six different representations using tags and compare to other data sources by using three different settings of SVM classifiers. Finally, we explore combinations of different data sources with tags using classifier committees to best classify the resources.

#index 2000621
#* Preventing Private Information Inference Attacks on Social Networks
#@ Raymond Heatherly;Murat Kantarcioglu;Bhavani M. Thuraisingham
#t 2013
#c 7
#! Online social networks, such as Facebook, are increasingly utilized by many people. These networks allow users to publish details about themselves and to connect to their friends. Some of the information revealed inside these networks is meant to be private. Yet it is possible to use learning algorithms on released data to predict private information. In this paper, we explore how to launch inference attacks using released social networking data to predict private information. We then devise three possible sanitization techniques that could be used in various situations. Then, we explore the effectiveness of these techniques and attempt to use methods of collective inference to discover sensitive attributes of the data set. We show that we can decrease the effectiveness of both local and relational classification algorithms by using the sanitization methods we described.

#index 2000622
#* Optimizing Multi-Top-k Queries over Uncertain Data Streams
#@ Tao Chen;Lei Chen;M. Tamer Ozsu;Nong Xiao
#t 2013
#c 7
#! Query processing over uncertain data streams, in particular top-$(k)$ query processing, has become increasingly important due to its wide application in many fields such as sensor network monitoring and internet traffic control. In many real applications, multiple top-$(k)$ queries are registered in the system. Sharing the results of these queries is a key factor in saving the computation cost and providing real-time response. However, due to the complex semantics of uncertain top-$(k)$ query processing, it is nontrivial to implement sharing among different top-$(k)$ queries and few works have addressed the sharing issue. In this paper, we formulate various types of sharing among multiple top-$(k)$ queries over uncertain data streams based on the frequency upper bound of each top-$(k)$ query. We present an optimal dynamic programming solution as well as a more efficient (in terms of time and space complexity) greedy algorithm to compute the execution plan of executing queries for saving the computation cost between them. Experiments have demonstrated that the greedy algorithm can find the optimal solution in most cases, and it can almost achieve the same performance (in terms of latency and throughput) as the dynamic programming approach.

#index 2000623
#* Prequery Discovery of Domain-Specific Query Forms: A Survey
#@ Mauricio C. Moraes;Carlos A. Heuser;Viviane P. Moreira;Denilson Barbosa
#t 2013
#c 7
#! The discovery of HTML query forms is one of the main challenges in Deep web crawling. Automatic solutions for this problem perform two main tasks. The first is locating HTML forms on the web, which is done through the use of traditional/focused crawlers. The second is identifying which of these forms are indeed meant for querying, which also typically involves determining a domain for the underlying data source (and thus for the form as well). This problem has attracted a great deal of interest, resulting in a long list of algorithms and techniques. Some methods submit requests through the forms and then analyze the data retrieved in response, typically requiring a great deal of knowledge about the domain as well as semantic processing. Others do not employ form submission, to avoid such difficulties, although some techniques rely to some extent on semantics and domain knowledge. This survey gives an up-to-date review of methods for the discovery of domain-specific query forms that do not involve form submission. We detail these methods and discuss how form discovery has become increasingly more automated over time. We conclude with a forecast of what we believe are the immediate next steps in this trend.

#index 2000624
#* Principal Composite Kernel Feature Analysis: Data-Dependent Kernel Approach
#@ Yuichi Motai;Hiroyuki Yoshida
#t 2013
#c 7
#! Principal composite kernel feature analysis (PC-KFA) is presented to show kernel adaptations for nonlinear features of medical image data sets (MIDS) in computer-aided diagnosis (CAD). The proposed algorithm PC-KFA has extended the existing studies on kernel feature analysis (KFA), which extracts salient features from a sample of unclassified patterns by use of a kernel method. The principal composite process for PC-KFA herein has been applied to kernel principal component analysis [34] and to our previously developed accelerated kernel feature analysis [20]. Unlike other kernel-based feature selection algorithms, PC-KFA iteratively constructs a linear subspace of a high-dimensional feature space by maximizing a variance condition for the nonlinearly transformed samples, which we call data-dependent kernel approach. The resulting kernel subspace can be first chosen by principal component analysis, and then be processed for composite kernel subspace through the efficient combination representations used for further reconstruction and classification. Numerical experiments based on several MID feature spaces of cancer CAD data have shown that PC-KFA generates efficient and an effective feature representation, and has yielded a better classification performance for the proposed composite kernel subspace using a simple pattern classifier.

#index 2000625
#* Revealing Density-Based Clustering Structure from the Core-Connected Tree of a Network
#@ Jianbin Huang;Heli Sun;Qinbao Song;Hongbo Deng;Jiawei Han
#t 2013
#c 7
#! Clustering is an important technique for mining the intrinsic community structures in networks. The density-based network clustering method is able to not only detect communities of arbitrary size and shape, but also identify hubs and outliers. However, it requires manual parameter specification to define clusters, and is sensitive to the parameter of density threshold which is difficult to determine. Furthermore, many real-world networks exhibit a hierarchical structure with communities embedded within other communities. Therefore, the clustering result of a global parameter setting cannot always describe the intrinsic clustering structure accurately. In this paper, we introduce a novel density-based network clustering method, called graph-skeleton-based clustering (gSkeletonClu). By projecting an undirected network to its core-connected maximal spanning tree, the clustering problem can be converted to detect core connectivity components on the tree. The density-based clustering of a specific parameter setting and the hierarchical clustering structure both can be efficiently extracted from the tree. Moreover, it provides a convenient way to automatically select the parameter and to achieve the meaningful cluster tree in a network. Extensive experiments on both real-world and synthetic networks demonstrate the superior performance of gSkeletonClu for effective and efficient density-based clustering.

#index 2000626
#* The Adaptive Clustering Method for the Long Tail Problem of Recommender Systems
#@ Yoon-Joo Park
#t 2013
#c 7
#! This is a study of the long tail problem of recommender systems when many items in the long tail have only a few ratings, thus making it hard to use them in recommender systems. The approach presented in this paper clusters items according to their popularities, so that the recommendations for tail items are based on the ratings in more intensively clustered groups and for the head items are based on the ratings of individual items or groups, clustered to a lesser extent. We apply this method to two real-life data sets and compare the results with those of the nongrouping and fully grouped methods in terms of recommendation accuracy and scalability. The results show that if such adaptive clustering is done properly, this method reduces the recommendation error rates for the tail items, while maintaining reasonable computational performance.

#index 2000627
#* Secure Provenance Transmission for Streaming Data
#@ Salmin Sultana;Mohamed Shehab;Elisa Bertino
#t 2013
#c 7
#! Many application domains, such as real-time financial analysis, e-healthcare systems, sensor networks, are characterized by continuous data streaming from multiple sources and through intermediate processing by multiple aggregators. Keeping track of data provenance in such highly dynamic context is an important requirement, since data provenance is a key factor in assessing data trustworthiness which is crucial for many applications. Provenance management for streaming data requires addressing several challenges, including the assurance of high processing throughput, low bandwidth consumption, storage efficiency and secure transmission. In this paper, we propose a novel approach to securely transmit provenance for streaming data (focusing on sensor network) by embedding provenance into the interpacket timing domain while addressing the above mentioned issues. As provenance is hidden in another host-medium, our solution can be conceptualized as watermarking technique. However, unlike traditional watermarking approaches, we embed provenance over the interpacket delays (IPDs) rather than in the sensor data themselves, hence avoiding the problem of data degradation due to watermarking. Provenance is extracted by the data receiver utilizing an optimal threshold-based mechanism which minimizes the probability of provenance decoding errors. The resiliency of the scheme against outside and inside attackers is established through an extensive security analysis. Experiments show that our technique can recover provenance up to a certain level against perturbations to inter-packet timing characteristics.

#index 2000628
#* VChunkJoin: An Efficient Algorithm for Edit Similarity Joins
#@ Wei Wang;Jianbin Qin;Xiao Chuan;Xuemin Lin;Heng Tao Shen
#t 2013
#c 7
#! Similarity joins play an important role in many application areas, such as data integration and cleaning, record linkage, and pattern recognition. In this paper, we study efficient algorithms for similarity joins with an edit distance constraint. Currently, the most prevalent approach is based on extracting overlapping grams from strings and considering only strings that share a certain number of grams as candidates. Unlike these existing approaches, we propose a novel approach to edit similarity join based on extracting nonoverlapping substrings, or chunks, from strings. We propose a class of chunking schemes based on the notion of tail-restricted chunk boundary dictionary. A new algorithm, VChunkJoin, is designed by integrating existing filtering methods and several new filters unique to our chunk-based method. We also design a greedy algorithm to automatically select a good chunking scheme for a given data set. We demonstrate experimentally that the new algorithm is faster than alternative methods yet occupies less space.

#index 2000629
#* What's New in Transactions
#@ 
#t 2013
#c 7

#index 2000862
#* A Methodology for Direct and Indirect Discrimination Prevention in Data Mining
#@ Sara Hajian;Josep Domingo-Ferrer
#t 2013
#c 7
#! Data mining is an increasingly important technology for extracting useful knowledge hidden in large collections of data. There are, however, negative social perceptions about data mining, among which potential privacy invasion and potential discrimination. The latter consists of unfairly treating people on the basis of their belonging to a specific group. Automated data collection and data mining techniques such as classification rule mining have paved the way to making automated decisions, like loan granting/denial, insurance premium computation, etc. If the training data sets are biased in what regards discriminatory (sensitive) attributes like gender, race, religion, etc., discriminatory decisions may ensue. For this reason, antidiscrimination techniques including discrimination discovery and prevention have been introduced in data mining. Discrimination can be either direct or indirect. Direct discrimination occurs when decisions are made based on sensitive attributes. Indirect discrimination occurs when decisions are made based on nonsensitive attributes which are strongly correlated with biased sensitive ones. In this paper, we tackle discrimination prevention in data mining and propose new techniques applicable for direct or indirect discrimination prevention individually or both at the same time. We discuss how to clean training data sets and outsourced data sets in such a way that direct and/or indirect discriminatory decision rules are converted to legitimate (nondiscriminatory) classification rules. We also propose new metrics to evaluate the utility of the proposed approaches and we compare these approaches. The experimental evaluations demonstrate that the proposed techniques are effective at removing direct and/or indirect discrimination biases in the original data set while preserving data quality.

#index 2000863
#* Anomaly Detection via Online Oversampling Principal Component Analysis
#@ Yuh-Jye Lee;Yi-Ren Yeh;Yu-Chiang Frank Wang
#t 2013
#c 7
#! Anomaly detection has been an important research topic in data mining and machine learning. Many real-world applications such as intrusion or credit card fraud detection require an effective and efficient framework to identify deviated data instances. However, most anomaly detection methods are typically implemented in batch mode, and thus cannot be easily extended to large-scale problems without sacrificing computation and memory requirements. In this paper, we propose an online oversampling principal component analysis (osPCA) algorithm to address this problem, and we aim at detecting the presence of outliers from a large amount of data via an online updating technique. Unlike prior principal component analysis (PCA)-based approaches, we do not store the entire data matrix or covariance matrix, and thus our approach is especially of interest in online or large-scale problems. By oversampling the target instance and extracting the principal direction of the data, the proposed osPCA allows us to determine the anomaly of the target instance according to the variation of the resulting dominant eigenvector. Since our osPCA need not perform eigen analysis explicitly, the proposed framework is favored for online applications which have computation or memory limitations. Compared with the well-known power method for PCA and other popular anomaly detection algorithms, our experimental results verify the feasibility of our proposed method in terms of both accuracy and efficiency.

#index 2000864
#* CDAMA: Concealed Data Aggregation Scheme for Multiple Applications in Wireless Sensor Networks
#@ Yue-Hsun Lin;Shih-Ying Chang;Hung-Min Sun
#t 2013
#c 7
#! For wireless sensor networks, data aggregation scheme that reduces a large amount of transmission is the most practical technique. In previous studies, homomorphic encryptions have been applied to conceal communication during aggregation such that enciphered data can be aggregated algebraically without decryption. Since aggregators collect data without decryption, adversaries are not able to forge aggregated results by compromising them. However, these schemes are not satisfy multi-application environments. Second, these schemes become insecure in case some sensor nodes are compromised. Third, these schemes do not provide secure counting; thus, they may suffer unauthorized aggregation attacks. Therefore, we propose a new concealed data aggregation scheme extended from Boneh et al.'s homomorphic public encryption system. The proposed scheme has three contributions. First, it is designed for a multi-application environment. The base station extracts application-specific data from aggregated ciphertexts. Next, it mitigates the impact of compromising attacks in single application environments. Finally, it degrades the damage from unauthorized aggregations. To prove the proposed scheme's robustness and efficiency, we also conducted the comprehensive analyses and comparisons in the end.

#index 2000865
#* Classification and Adaptive Novel Class Detection of Feature-Evolving Data Streams
#@ Mohammad M. Masud;Qing Chen;Latifur Khan;Charu C. Aggarwal;Jing Gao;Jiawei Han;Ashok Srivastava;Nikunj C. Oza
#t 2013
#c 7
#! Data stream classification poses many challenges to the data mining community. In this paper, we address four such major challenges, namely, infinite length, concept-drift, concept-evolution, and feature-evolution. Since a data stream is theoretically infinite in length, it is impractical to store and use all the historical data for training. Concept-drift is a common phenomenon in data streams, which occurs as a result of changes in the underlying concepts. Concept-evolution occurs as a result of new classes evolving in the stream. Feature-evolution is a frequently occurring process in many streams, such as text streams, in which new features (i.e., words or phrases) appear as the stream progresses. Most existing data stream classification techniques address only the first two challenges, and ignore the latter two. In this paper, we propose an ensemble classification framework, where each classifier is equipped with a novel class detector, to address concept-drift and concept-evolution. To address feature-evolution, we propose a feature set homogenization technique. We also enhance the novel class detection module by making it more adaptive to the evolving stream, and enabling it to detect more than one novel class at a time. Comparison with state-of-the-art data stream classification techniques establishes the effectiveness of the proposed approach.

#index 2000866
#* Cross-Space Affinity Learning with Its Application to Movie Recommendation
#@ Jinhui Tang;Guo-Jun Qi;Liyan Zhang;Changsheng Xu
#t 2013
#c 7
#! In this paper, we propose a novel cross-space affinity learning algorithm over different spaces with heterogeneous structures. Unlike most of affinity learning algorithms on the homogeneous space, we construct a cross-space tensor model to learn the affinity measures on heterogeneous spaces subject to a set of order constraints from the training pool. We further enhance the model with a factorization form which greatly reduces the number of parameters of the model with a controlled complexity. Moreover, from the practical perspective, we show the proposed factorized cross-space tensor model can be efficiently optimized by a series of simple quadratic optimization problems in an iterative manner. The proposed cross-space affinity learning algorithm can be applied to many real-world problems, which involve multiple heterogeneous data objects defined over different spaces. In this paper, we apply it into the recommendation system to measure the affinity between users and the product items, where a higher affinity means a higher rating of the user on the product. For an empirical evaluation, a widely used benchmark movie recommendation data set—MovieLens—is used to compare the proposed algorithm with other state-of-the-art recommendation algorithms and we show that very competitive results can be obtained.

#index 2000867
#* Distributed Strategies for Mining Outliers in Large Data Sets
#@ Fabrizio Angiulli;Stefano Basta;Stefano Lodi;Claudio Sartori
#t 2013
#c 7
#! We introduce a distributed method for detecting distance-based outliers in very large data sets. Our approach is based on the concept of outlier detection solving set [2], which is a small subset of the data set that can be also employed for predicting novel outliers. The method exploits parallel computation in order to obtain vast time savings. Indeed, beyond preserving the correctness of the result, the proposed schema exhibits excellent performances. From the theoretical point of view, for common settings, the temporal cost of our algorithm is expected to be at least three orders of magnitude faster than the classical nested-loop like approach to detect outliers. Experimental results show that the algorithm is efficient and that its running time scales quite well for an increasing number of nodes. We discuss also a variant of the basic strategy which reduces the amount of data to be transferred in order to improve both the communication cost and the overall runtime. Importantly, the solving set computed by our approach in a distributed environment has the same quality as that produced by the corresponding centralized method.

#index 2000868
#* Inferring Statistically Significant Hidden Markov Models
#@ Lu Yu;Jason M. Schwier;Ryan M. Craven;Richard R. Brooks, Sr.;Christopher Griffin
#t 2013
#c 7
#! Hidden Markov models (HMMs) are used to analyze real-world problems. We consider an approach that constructs minimum entropy HMMs directly from a sequence of observations. If an insufficient amount of observation data is used to generate the HMM, the model will not represent the underlying process. Current methods assume that observations completely represent the underlying process. It is often the case that the training data size is not large enough to adequately capture all statistical dependencies in the system. It is, therefore, important to know the statistical significance level for that the constructed model representing the underlying process, not only the training set. In this paper, we present a method to determine if the observation data and constructed model fully express the underlying process with a given level of statistical significance. We use the statistics of the process to calculate an upper bound on the number of samples required to guarantee that the model has a given level significance. We provide theoretical and experimental results that confirm the utility of this approach. The experiment is conducted on a real private Tor network.

#index 2000869
#* Enhancing Access Privacy of Range Retrievals over $({\rm B}^+)$-Trees
#@ HweeHwa Pang;Jilian Zhang;Kyriakos Mouratidis
#t 2013
#c 7
#! Users of databases that are hosted on shared servers cannot take for granted that their queries will not be disclosed to unauthorized parties. Even if the database is encrypted, an adversary who is monitoring the I/O activity on the server may still be able to infer some information about a user query. For the particular case of a $({\rm B}^+)$-tree that has its nodes encrypted, we identify properties that enable the ordering among the leaf nodes to be deduced. These properties allow us to construct adversarial algorithms to recover the $({\rm B}^+)$-tree structure from the I/O traces generated by range queries. Combining this structure with knowledge of the key distribution (or the plaintext database itself), the adversary can infer the selection range of user queries. To counter the threat, we propose a privacy-enhancing $({\rm PB}^+)$-tree index which ensures that there is high uncertainty about what data the user has worked on, even to a knowledgeable adversary who has observed numerous query executions. The core idea in $({\rm PB}^+)$-tree is to conceal the order of the leaf nodes in an encrypted $({\rm B}^+)$-tree. In particular, it groups the nodes of the tree into buckets, and employs homomorphic encryption techniques to prevent the adversary from pinpointing the exact nodes retrieved by range queries. $({\rm PB}^+)$-tree can be tuned to balance its privacy strength with the computational and I/O overheads incurred. Moreover, it can be adapted to protect access privacy in cases where the attacker additionally knows a priori the access frequencies of key values. Experiments demonstrate that $({\rm PB}^+)$-tree effectively impairs the adversary's ability to recover the $({\rm B}^+)$-tree structure and deduce the query ranges in all considered scenarios.

#index 2000870
#* Lineage Encoding: An Efficient Wireless XML Streaming Supporting Twig Pattern Queries
#@ Jun Pyo Park;Chang-Sup Park;Yon Dohn Chung
#t 2013
#c 7
#! In this paper, we propose an energy and latency efficient XML dissemination scheme for the mobile computing. We define a novel unit structure called G-node for streaming XML data in the wireless environment. It exploits the benefits of the structure indexing and attribute summarization that can integrate relevant XML elements into a group. It provides a way for selective access of their attribute values and text content. We also propose a lightweight and effective encoding scheme, called Lineage Encoding, to support evaluation of predicates and twig pattern queries over the stream. The Lineage Encoding scheme represents the parent-child relationships among XML elements as a sequence of bit-strings, called Lineage Code(V, H), and provides basic operators and functions for effective twig pattern query processing at mobile clients. Extensive experiments using real and synthetic data sets demonstrate our scheme outperforms conventional wireless XML broadcasting methods for simple path queries as well as complex twig pattern queries with predicate conditions.

#index 2000871
#* MKBoost: A Framework of Multiple Kernel Boosting
#@ Hao Xia;Steven C. H. Hoi
#t 2013
#c 7
#! Multiple kernel learning (MKL) is a promising family of machine learning algorithms using multiple kernel functions for various challenging data mining tasks. Conventional MKL methods often formulate the problem as an optimization task of learning the optimal combinations of both kernels and classifiers, which usually results in some forms of challenging optimization tasks that are often difficult to be solved. Different from the existing MKL methods, in this paper, we investigate a boosting framework of MKL for classification tasks, i.e., we adopt boosting to solve a variant of MKL problem, which avoids solving the complicated optimization tasks. Specifically, we present a novel framework of Multiple kernel boosting (MKBoost), which applies the idea of boosting techniques to learn kernel-based classifiers with multiple kernels for classification problems. Based on the proposed framework, we propose several variants of MKBoost algorithms and extensively examine their empirical performance on a number of benchmark data sets in comparisons to various state-of-the-art MKL algorithms on classification tasks. Experimental results show that the proposed method is more effective and efficient than the existing MKL techniques.

#index 2000872
#* Mining Order-Preserving Submatrices from Data with Repeated Measurements
#@ Kevin Y Yip;Ben Kao;Xinjie Zhu;Chun Kit Chui;Sau Dan Lee;David W. Cheung
#t 2013
#c 7
#! Order-preserving submatrices (OPSM's) have been shown useful in capturing concurrent patterns in data when the relative magnitudes of data items are more important than their exact values. For instance, in analyzing gene expression profiles obtained from microarray experiments, the relative magnitudes are important both because they represent the change of gene activities across the experiments, and because there is typically a high level of noise in data that makes the exact values untrustable. To cope with data noise, repeated experiments are often conducted to collect multiple measurements. We propose and study a more robust version of OPSM, where each data item is represented by a set of values obtained from replicated experiments. We call the new problem OPSM-RM (OPSM with repeated measurements). We define OPSM-RM based on a number of practical requirements. We discuss the computational challenges of OPSM-RM and propose a generic mining algorithm. We further propose a series of techniques to speed up two time dominating components of the algorithm. We show the effectiveness and efficiency of our methods through a series of experiments conducted on real microarray data.

#index 2000873
#* Modeling Noisy Annotated Data with Application to Social Annotation
#@ Tomoharu Iwata;Takeshi Yamada;Naonori Ueda
#t 2013
#c 7
#! We propose a probabilistic topic model for analyzing and extracting content-related annotations from noisy annotated discrete data such as webpages stored using social bookmarking services. With these services, because users can attach annotations freely, some annotations do not describe the semantics of the content, thus they are noisy, i.e., not content related. The extraction of content-related annotations can be used as a prepossessing step in machine learning tasks such as text classification and image recognition, or can improve information retrieval performance. The proposed model is a generative model for content and annotations, in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content. We demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.

#index 2000874
#* Multiparty Access Control for Online Social Networks: Model and Mechanisms
#@ Hongxin Hu;Gail-Joon Ahn;Jan Jorgensen
#t 2013
#c 7
#! Online social networks (OSNs) have experienced tremendous growth in recent years and become a de facto portal for hundreds of millions of Internet users. These OSNs offer attractive means for digital social interactions and information sharing, but also raise a number of security and privacy issues. While OSNs allow users to restrict access to shared data, they currently do not provide any mechanism to enforce privacy concerns over data associated with multiple users. To this end, we propose an approach to enable the protection of shared data associated with multiple users in OSNs. We formulate an access control model to capture the essence of multiparty authorization requirements, along with a multiparty policy specification scheme and a policy enforcement mechanism. Besides, we present a logical representation of our access control model that allows us to leverage the features of existing logic solvers to perform various analysis tasks on our model. We also discuss a proof-of-concept prototype of our approach as part of an application in Facebook and provide usability study and system evaluation of our method.

#index 2000875
#* On the Analytical Properties of High-Dimensional Randomization
#@ Charu C. Aggarwal
#t 2013
#c 7
#! In this paper, we will provide the first comprehensive analysis of high-dimensional randomization. The goal is to examine the strengths and weaknesses of randomization and explore both the potential and the pitfalls of high-dimensional randomization. Our theoretical analysis results in a number of interesting and insightful conclusions. 1) The privacy effects of randomization reduce rapidly with increasing dimensionality. 2) The properties of the underlying data set can affect the anonymity level of the randomization method. For example, natural properties of real data sets such as clustering improve the effectiveness of randomization. On the other hand, variations in data density of nonempty data localities and outliers create privacy preservation challenges for the randomization method. 3) The use of a public information-sensitive attack method makes the choice of perturbing distribution more critical than previously thought. In particular, Gaussian perturbations are significantly more effective than uniformly distributed perturbations for the high dimensional case. These insights are very useful for future research and design of the randomization method. We use the insights gained from our analysis to discuss and suggest future research directions for improvements and extensions of the randomization method.

#index 2000876
#* TACI: Taxonomy-Aware Catalog Integration
#@ Panagiotis Papadimitriou;Panayiotis Tsaparas;Ariel Fuxman;Lise Getoor
#t 2013
#c 7
#! A fundamental data integration task faced by online commercial portals and commerce search engines is the integration of products coming from multiple providers to their product catalogs. In this scenario, the commercial portal has its own taxonomy (the “master taxonomy”), while each data provider organizes its products into a different taxonomy (the “provider taxonomy”). In this paper, we consider the problem of categorizing products from the data providers into the master taxonomy, while making use of the provider taxonomy information. Our approach is based on a taxonomy-aware processing step that adjusts the results of a text-based classifier to ensure that products that are close together in the provider taxonomy remain close in the master taxonomy. We formulate this intuition as a structured prediction optimization problem. To the best of our knowledge, this is the first approach that leverages the structure of taxonomies in order to enhance catalog integration. We propose algorithms that are scalable and thus applicable to the large data sets that are typical on the web. We evaluate our algorithms on real-world data and we show that taxonomy-aware classification provides a significant improvement over existing approaches.

#index 2000877
#* The Skyline of a Probabilistic Relation
#@ Ilaria Bartolini;Paolo Ciaccia;Marco Patella
#t 2013
#c 7
#! In a deterministic relation $(R)$, tuple $(u)$ dominates tuple $(v)$ if $(u)$ is no worse than $(v)$ on all the attributes of interest, and better than $(v)$ on at least one attribute. This concept is at the heart of skyline queries, that return the set of undominated tuples in $(R)$. In this paper, we extend the notion of skyline to probabilistic relations by generalizing to this context the definition of tuple domination. Our approach is parametric in the semantics for linearly ranking probabilistic tuples and, being it based on order-theoretic principles, preserves the three fundamental properties the skyline has in the deterministic case: 1) It equals the union of all top-1 results of monotone scoring functions; 2) it requires no additional parameter; and 3) it is insensitive to actual attribute scales. We then show how domination among probabilistic tuples (or P-domination for short) can be efficiently checked by means of a set of rules. We detail such rules for the cases in which tuples are ranked using either the “expected rank” or the “expected score” semantics, and explain how the approach can be applied to other semantics as well. Since computing the skyline of a probabilistic relation is a time-consuming task, we introduce a family of algorithms for checking P-domination rules in an optimized way. Experiments show that these algorithms can significantly reduce the actual execution times with respect to a naïve evaluation.

#index 2000878
#* Unsupervised Hybrid Feature Extraction Selection for High-Dimensional Non-Gaussian Data Clustering with Variational Inference
#@ Wentao Fan;Nizar Bouguila;Djemel Ziou
#t 2013
#c 7
#! Clustering has been a subject of extensive research in data mining, pattern recognition, and other areas for several decades. The main goal is to assign samples, which are typically non-Gaussian and expressed as points in high-dimensional feature spaces, to one of a number of clusters. It is well known that in such high-dimensional settings, the existence of irrelevant features generally compromises modeling capabilities. In this paper, we propose a variational inference framework for unsupervised non-Gaussian feature selection, in the context of finite generalized Dirichlet (GD) mixture-based clustering. Under the proposed principled variational framework, we simultaneously estimate, in a closed form, all the involved parameters and determine the complexity (i.e., both model an feature selection) of the GD mixture. Extensive simulations using synthetic data along with an analysis of real-world data and human action videos demonstrate that our variational approach achieves better results than comparable techniques.

#index 2000879
#* Comparable Entity Mining from Comparative Questions
#@ Shasha Li;Chin-Yew Lin;Young-In Song;Zhoujun Li
#t 2013
#c 7
#! Comparing one thing with another is a typical part of human decision making process. However, it is not always easy to know what to compare and what are the alternatives. In this paper, we present a novel way to automatically mine comparable entities from comparative questions that users posted online to address this difficulty. To ensure high precision and high recall, we develop a weakly supervised bootstrapping approach for comparative question identification and comparable entity extraction by leveraging a large collection of online question archive. The experimental results show our method achieves F1-measure of 82.5 percent in comparative question identification and 83.3 percent in comparable entity extraction. Both significantly outperform an existing state-of-the-art method. Additionally, our ranking results show highly relevance to user's comparison intents in web.

#index 2014661
#* Discrete Elastic Inner Vector Spaces with Application to Time Series and Sequence Mining
#@ Pierre-Francois Marteau;Nicolas Bonnel;Gildas Menier
#t 2013
#c 7
#! This paper proposes a framework dedicated to the construction of what we call discrete elastic inner product allowing one to embed sets of nonuniformly sampled multivariate time series or sequences of varying lengths into inner product space structures. This framework is based on a recursive definition that covers the case of multiple embedded time elastic dimensions. We prove that such inner products exist in our general framework and show how a simple instance of this inner product class operates on some prospective applications, while generalizing the euclidean inner product. Classification experimentations on time series and symbolic sequences data sets demonstrate the benefits that we can expect by embedding time series or sequences into elastic inner spaces rather than into classical euclidean spaces. These experiments show good accuracy when compared to the euclidean distance or even dynamic programming algorithms while maintaining a linear algorithmic complexity at exploitation stage, although a quadratic indexing phase beforehand is required.

#index 2014662
#* A Formalism and Method for Representing and Reasoning with Process Models Authored by Subject Matter Experts
#@ Jose Manuel Gomez-Perez;Michael Erdmann;Mark Greaves;Oscar Corcho
#t 2013
#c 7
#! Enabling Subject Matter Experts (SMEs) to formulate knowledge without the intervention of Knowledge Engineers (KEs) requires providing SMEs with methods and tools that abstract the underlying knowledge representation and allow them to focus on modeling activities. Bridging the gap between SME-authored models and their representation is challenging, especially in the case of complex knowledge types like processes, where aspects like frame management, data, and control flow need to be addressed. In this paper, we describe how SME-authored process models can be provided with an operational semantics and grounded in a knowledge representation language like F-logic to support process-related reasoning. The main results of this work include a formalism for process representation and a mechanism for automatically translating process diagrams into executable code following such formalism. From all the process models authored by SMEs during evaluation 82 percent were well formed, all of which executed correctly. Additionally, the two optimizations applied to the code generation mechanism produced a performance improvement at reasoning time of 25 and 30 percent with respect to the base case, respectively.

#index 2014663
#* A Similarity Measure for Comparing XACML Policies
#@ Dan Lin;Prathima Rao;Rodolfo Ferrini;Elisa Bertino;Jorge Lobo
#t 2013
#c 7
#! Assessing similarity of policies is crucial in a variety of scenarios, such as finding the cloud service providers which satisfy users' privacy concerns, or finding collaborators which have matching security and privacy settings. Existing approaches to policy similarity analysis are mainly based on logical reasoning and Boolean function comparison. Such approaches are computationally expensive and do not scale well for large heterogeneous distributed environments (like the cloud). In this paper, we propose a policy similarity measure as a lightweight ranking approach to help one party quickly locate parties with potentially similar policies. In particular, given a policy $(P)$, the similarity measure assigns a ranking (similarity score) to each policy compared with $(P)$. We formally define the measure by taking into account various factors and prove several important properties of the measure. Our extensive experimental study demonstrates the efficiency and practical value of our approach.

#index 2014664
#* A Survey on Region Extractors from Web Documents
#@ Hassan A. Sleiman;Rafael Corchuelo
#t 2013
#c 7
#! Extracting information from web documents has become a research area in which new proposals sprout out year after year. This has motivated several researchers to work on surveys that attempt to provide an overall picture of the many existing proposals. Unfortunately, none of these surveys provide a complete picture, because they do not take region extractors into account. These tools are kind of preprocessors, because they help information extractors focus on the regions of a web document that contain relevant information. With the increasing complexity of web documents, region extractors are becoming a must to extract information from many websites. Beyond information extraction, region extractors have also found their way into information retrieval, focused web crawling, topic distillation, adaptive content delivery, mashups, and metasearch engines. In this paper, we survey the existing proposals regarding region extractors and compare them side by side.

#index 2014665
#* Approximate Algorithms for Computing Spatial Distance Histograms with Accuracy Guarantees
#@ Vladimir Grupcev;Yongke Yuan;Yi-Cheng Tu;Jin Huang;Shaoping Chen;Sagar Pandit;Michael Weng
#t 2013
#c 7
#! Particle simulation has become an important research tool in many scientific and engineering fields. Data generated by such simulations impose great challenges to database storage and query processing. One of the queries against particle simulation data, the spatial distance histogram (SDH) query, is the building block of many high-level analytics, and requires quadratic time to compute using a straightforward algorithm. Previous work has developed efficient algorithms that compute exact SDHs. While beating the naive solution, such algorithms are still not practical in processing SDH queries against large-scale simulation data. In this paper, we take a different path to tackle this problem by focusing on approximate algorithms with provable error bounds. We first present a solution derived from the aforementioned exact SDH algorithm, and this solution has running time that is unrelated to the system size $(N)$. We also develop a mathematical model to analyze the mechanism that leads to errors in the basic approximate algorithm. Our model provides insights on how the algorithm can be improved to achieve higher accuracy and efficiency. Such insights give rise to a new approximate algorithm with improved time/accuracy tradeoff. Experimental results confirm our analysis.

#index 2014666
#* Benchmarking Data Exchange among Semantic-Web Ontologies
#@ Carlos R. Rivero;Inma Hernandez;David Ruiz;Rafael Corchuelo
#t 2013
#c 7
#! The increasing popularity of the Web of Data is motivating the need to integrate semantic-web ontologies. Data exchange is one integration approach that aims to populate a target ontology using data that come from one or more source ontologies. Currently, there exist a variety of systems that are suitable to perform data exchange among these ontologies; unfortunately, they have uneven performance, which makes it appealing assessing and ranking them from an empirical point of view. In the bibliography, there exist a number of benchmarks, but they cannot be applied to this context because they are not suitable for testing semantic-web ontologies or they do not focus on data exchange problems. In this paper, we present MostoBM, a benchmark for testing data exchange systems in the context of such ontologies. It provides a catalogue of three real-world and seven synthetic data exchange patterns, which can be instantiated into a variety of scenarios using some parameters. These scenarios help to analyze how the performance of data exchange systems evolves as the exchanging ontologies are scaled in structured and/or data. Finally, we provide an evaluation methodology to compare data exchange systems side by side and to make informed and statistically sound decisions regarding: 1) which data exchange system performs better; and 2) how the performance of a system is influenced by the parameters of our benchmark.

#index 2014667
#* COSAC: A Framework for Combinatorial Statistical Analysis on Cloud
#@ Zhengkui Wang;Divyakant Agrawal;Kian-Lee Tan
#t 2013
#c 7
#! In many scientific applications, it is critical to determine if there is a relationship between a combination of objects. The strength of such an association is typically computed using some statistical measures. In order not to miss any important associations, it is not uncommon to exhaustively enumerate all possible combinations of a certain size. However, discovering significant associations among hundreds of thousands or even millions of objects is a computationally intensive job that typically takes days, if not weeks, to complete. We are, therefore, motivated to provide efficient and practical techniques to speed up the processing exploiting parallelism. In this paper, we propose a framework, COSAC, for such combinatorial statistical analysis for large-scale data sets over a MapReduce-based cloud computing platform. COSAC operates in two key phases: 1) In the distribution phase, a novel load balancing scheme distributes the combination enumeration tasks across the processing units; 2) In the statistical analysis phase, each unit optimizes the processing of the allocated combinations by salvaging computations that can be reused. COSAC also supports a more practical scenario, where only a selected subset of objects need to be analyzed against all the objects. As a representative application, we developed COSAC to find combinations of Single Nucleotide Polymorphisms (SNPs) that may interact to cause diseases. We have evaluated our framework on a cluster of more than 40 nodes. The experimental results show that our framework is computationally practical, efficient, scalable, and flexible.

#index 2014668
#* KSQ: Top-$(k)$ Similarity Query on Uncertain Trajectories
#@ Chunyang Ma;Hua Lu;Lidan Shou;Gang Chen
#t 2013
#c 7
#! Similarity search on spatiotemporal trajectories has a wide range of applications. Most of existing research focuses on certain trajectories. However, trajectories often are uncertain due to various factors, for example, hardware limitations and privacy concerns. In this paper, we introduce p-distance, a novel and adaptive measure that is able to quantify the dissimilarity between two uncertain trajectories. Based on this measure of dissimilarity, we define top-$(k)$ similarity query (KSQ) on uncertain trajectories. A KSQ returns the $(k)$ trajectories that are most similar to a given trajectory in terms of p-distance. To process such queries efficiently, we design UTgrid for indexing uncertain trajectories and develop query processing algorithms that make use of UTgrid for effective pruning. We conduct an extensive experimental study on both synthetic and real data sets. The results indicate that UTgrid is an effective indexing method for similarity search on uncertain trajectories. Our query processing using UTgrid dramatically improves the query performance and scales well in terms of query time and I/O.

#index 2014669
#* Generative Models for Item Adoptions Using Social Correlation
#@ Freddy Chong Tat Chua;Hady W. Lauw;Ee-Peng Lim
#t 2013
#c 7
#! Users face many choices on the web when it comes to choosing which product to buy, which video to watch, and so on. In making adoption decisions, users rely not only on their own preferences, but also on friends. We call the latter social correlation, which may be caused by the homophily and social influence effects. In this paper, we focus on modeling social correlation on users item adoptions. Given a user-user social graph and an item-user adoption graph, our research seeks to answer the following questions: Whether the items adopted by a user correlate with items adopted by her friends, and how to model item adoptions using social correlation. We propose a social correlation framework that considers a social correlation matrix representing the degrees of correlation from every user to the users friends, in addition to a set of latent factors representing topics of interests of individual users. Based on the framework, we develop two generative models, namely sequential and unified, and the corresponding parameter estimation approaches. From each model, we devise the social correlation only and hybrid methods for predicting missing adoption links. Experiments on LiveJournal and Epinions data sets show that our proposed models outperform the approach based on latent factors only (LDA).

#index 2014670
#* Latent Structured Perceptrons for Large-Scale Learning with Hidden Information
#@ Xu Sun;Takuya Matsuzaki;Wenjie Li
#t 2013
#c 7
#! Many real-world data mining problems contain hidden information (e.g., unobservable latent dependencies). We propose a perceptron-style method, latent structured perceptron, for fast discriminative learning of structured classification with hidden information. We also give theoretical analysis and demonstrate good convergence properties of the proposed method. Our method extends the perceptron algorithm for the learning task with hidden information, which can be hardly captured by traditional models. It relies on Viterbi decoding over latent variables, combined with simple additive updates. We perform experiments on one synthetic data set and two real-world structured classification tasks. Compared to conventional nonlatent models (e.g., conditional random fields, structured perceptrons), our method is more accurate on real-world tasks. Compared to existing heavy probabilistic models of latent variables (e.g., latent conditional random fields), our method lowers the training cost significantly (almost one order magnitude faster) yet with comparable or even superior classification accuracy. In addition, experiments demonstrate that the proposed method has good scalability on large-scale problems.

#index 2014671
#* Managing Structured and Semistructured RDF Data Using Structure Indexes
#@ Thanh Tran;Gunter Ladwig;Sebastian Rudolph
#t 2013
#c 7
#! We propose the use of a structure index for RDF. It can be used for querying RDF data for which the schema is incomplete or not available. More importantly, we leverage it for a structure-oriented approach to RDF data partitioning and query processing. Based on information captured by the structure index, similarly structured data elements are physically grouped and stored contiguously on disk. At querying time, the index is used for "structure-level" processing to identify the groups of data that match the query structure. Structure-level processing is then combined with standard "data-level" operations that involve retrieval and join procedures executed against the data. In the experiment, our solution provides several times faster performance than a state-of-the-art technique for data partitioning and query processing, and compares favorably with full-fledged RDF stores.

#index 2014672
#* Mining Graph Topological Patterns: Finding Covariations among Vertex Descriptors
#@ Adriana Prado;Marc Plantevit;Celine Robardet;Jean-Francois Boulicaut
#t 2013
#c 7
#! We propose to mine the graph topology of a large attributed graph by finding regularities among vertex descriptors. Such descriptors are of two types: 1) the vertex attributes that convey the information of the vertices themselves and 2) some topological properties used to describe the connectivity of the vertices. These descriptors are mostly of numerical or ordinal types and their similarity can be captured by quantifying their covariation. Mining topological patterns relies on frequent pattern mining and graph topology analysis to reveal the links that exist between the relation encoded by the graph and the vertex attributes. We propose three interestingness measures of topological patterns that differ by the pairs of vertices considered while evaluating up and down co-variations between vertex descriptors. An efficient algorithm that combines search and pruning strategies to look for the most relevant topological patterns is presented. Besides a classical empirical study, we report case studies on four real-life networks showing that our approach provides valuable knowledge.

#index 2014673
#* Omnivariate Rule Induction Using a Novel Pairwise Statistical Test
#@ Olcay Taner Yildiz
#t 2013
#c 7
#! Rule learning algorithms, for example, Ripper, induces univariate rules, that is, a propositional condition in a rule uses only one feature. In this paper, we propose an omnivariate induction of rules where under each condition, both a univariate and a multivariate condition are trained, and the best is chosen according to a novel statistical test. This paper has three main contributions: First, we propose a novel statistical test, the combined $(5\times 2)$ cv $(t)$ test, to compare two classifiers, which is a variant of the $(5\times 2)$ cv $(t)$ test and give the connections to other tests as $(5\times 2)$ cv $(F)$ test and $(k)$-fold paired $(t)$ test. Second, we propose a multivariate version of Ripper, where support vector machine with linear kernel is used to find multivariate linear conditions. Third, we propose an omnivariate version of Ripper, where the model selection is done via the combined $(5\times 2)$ cv $(t)$ test. Our results indicate that 1) the combined $(5\times 2)$ cv $(t)$ test has higher power (lower type II error), lower type I error, and higher replicability compared to the $(5\times 2)$ cv $(t)$ test, 2) omnivariate rules are better in that they choose whichever condition is more accurate, selecting the right model automatically and separately for each condition in a rule.

#index 2014674
#* ReFinder: A Context-Based Information Refinding System
#@ Tangjian Deng;Liang Zhao;Hao Wang;Qingwei Liu;Ling Feng
#t 2013
#c 7
#! In this paper, we present a context-based information refinding system called ReFinder. It leverages human's natural recall characteristics and allows users to refind files and Web pages according to the previous access context. ReFinder refinds information based on a query-by-context model over a context memory snapshot, linking to the accessed information contents. Context instances in the memory snapshot are organized in a clustered and associated manner, and dynamically evolve in life cycles to mimic brain memory's decay and reinforcement phenomena. We evaluate the scalability of ReFinder on a large synthetic data set. The experimental results show that consistent degradation of context instances in the context memory and the ones in user's refinding requests can lead to the best refinding precision and recall. An 8-week user study is also conducted to examine the applicability of ReFinder. Initial findings show that time, place, and activity could serve as useful recall clues. On average, 15.53 seconds are needed to complete a refinding request with ReFinder and 84.42 seconds with other existing methods. Some further possible improvement of ReFinder is also discussed at the end of the paper.

#index 2014675
#* Scalable Diversified Ranking on Large Graphs
#@ Rong-Hua Li;Jeffrey Xu Yu
#t 2013
#c 7
#! Enhancing diversity in ranking on graphs has been identified as an important retrieval and mining task. Nevertheless, many existing diversified ranking algorithms either cannot be scalable to large graphs due to the time or memory requirements, or lack an intuitive and reasonable diversified ranking measure. In this paper, we propose a new diversified ranking measure on large graphs, which captures both relevance and diversity, and formulate the diversified ranking problem as a submodular set function maximization problem. Based on the submodularity of the proposed measure, we develop an efficient greedy algorithm with linear time and space complexity w.r.t. the size of the graph to achieve near-optimal diversified ranking. In addition, we present a generalized diversified ranking measure and give a near-optimal randomized greedy algorithm with linear time and space complexity for optimizing it. We evaluate the proposed methods through extensive experiments on five real data sets. The experimental results demonstrate the effectiveness and efficiency of the proposed algorithms.

#index 2014676
#* Static and Dynamic Structural Correlations in Graphs
#@ Jian Wu;Ziyu Guan;Qing Zhang;Ambuj Singh;Xifeng Yan
#t 2013
#c 7
#! Real-life graphs not only contain nodes and edges, but also have events taking place, e.g., product sales in social networks. Among different events, some exhibit strong correlations with the network structure, while others do not. Such structural correlations will shed light on viral influence existing in the corresponding network. Unfortunately, the traditional association mining concept is not applicable in graphs because it only works on homogeneous data sets like transactions and baskets. We propose a novel measure for assessing such structural correlations in heterogeneous graph data sets with events. The measure applies hitting time to aggregate the proximity among nodes that have the same event. To calculate the correlation scores for many events in a large network, we develop a scalable framework, called gScore, using sampling and approximation. By comparing to the situation where events are randomly distributed in the same network, our method is able to discover events that are highly correlated with the graph structure. We test gScore's effectiveness by synthetic events on the DBLP coauthor network and report interesting correlation results in a social network extracted from TaoBao.com, the largest online shopping network in China. Scalability of gScore is tested on the Twitter network. Since an event is essentially a temporal phenomenon, we also propose a dynamic measure, which reveals structural correlations at specific time steps and can be used for discovering detailed evolutionary patterns.

#index 2014677
#* User Action Interpretation for Online Content Optimization
#@ Jiang Bian;Anlei Dong;Xiaofeng He;Srihari Reddy;Yi Chang
#t 2013
#c 7
#! Web portal services have become an important medium to deliver digital content and service, such as news, advertisements, and so on, to Web users in a timely fashion. To attract more users to various content modules on the Web portal, it is necessary to design a recommender system that can effectively achieve online content optimization by automatically estimating content items' attractiveness and relevance to users' interests. User interaction plays a vital role in building effective content optimization, as both implicit user feedbacks and explicit user ratings on the recommended items form the basis for designing and learning recommendation models. However, user actions on real-world Web portal services are likely to represent many implicit signals about users' interests and content attractiveness, which need more accurate interpretation to be fully leveraged in the recommendation models. To address this challenge, we investigate a couple of critical aspects of the online learning framework for personalized content optimization on Web portal services, and, in this paper, we propose deeper user action interpretation to enhance those critical aspects. In particular, we first propose an approach to leverage historical user activity to build behavior-driven user segmentation; then, we introduce an approach for interpreting users' actions from the factors of both user engagement and position bias to achieve unbiased estimation of content attractiveness. Our experiments on the large-scale data from a commercial Web recommender system demonstrate that recommendation models with our user action interpretation can reach significant improvement in terms of online content optimization over the baseline method. The effectiveness of our user action interpretation is also proved by the online test results on real user traffic.

#index 2014678
#* Transactions Connection Newslater
#@ 
#t 2013
#c 7

#index 2044172
#* A Comparative Study of Implementation Techniques for Query Processing in Multicore Systems
#@ Stratis Viglas
#t 2014
#c 7
#! Multicore systems and multithreaded processing are now the de facto standards of enterprise and personal computing. If used in an uninformed way, however, multithreaded processing might actually degrade performance. We present the facets of the memory access bottleneck as they manifest in multithreaded processing and show their impact on query evaluation. We present a system design based on partition parallelism, memory pooling, and data structures conducive to multithreaded processing. Based on this design, we present alternative implementations of the most common query processing algorithms, which we experimentally evaluate using multiple scenarios and hardware platforms. Our results show that the design and algorithms are indeed scalable across platforms, but the choice of optimal algorithm largely depends on the problem parameters and underlying hardware. However, our proposals are a good first step toward generic multithreaded parallelism.

#index 2044173
#* A Rough Hypercuboid Approach for Feature Selection in Approximation Spaces
#@ Pradipta Maji
#t 2014
#c 7
#! The selection of relevant and significant features is an important problem particularly for data sets with large number of features. In this regard, a new feature selection algorithm is presented based on a rough hypercuboid approach. It selects a set of features from a data set by maximizing the relevance, dependency, and significance of the selected features. By introducing the concept of the hypercuboid equivalence partition matrix, a novel representation of degree of dependency of sample categories on features is proposed to measure the relevance, dependency, and significance of features in approximation spaces. The equivalence partition matrix also offers an efficient way to calculate many more quantitative measures to describe the inexactness of approximate classification. Several quantitative indices are introduced based on the rough hypercuboid approach for evaluating the performance of the proposed method. The superiority of the proposed method over other feature selection methods, in terms of computational complexity and classification accuracy, is established extensively on various real-life data sets of different sizes and dimensions.

#index 2044174
#* An Empirical Performance Evaluation of Relational Keyword Search Techniques
#@ Joel Coffman;Alfred Weaver
#t 2014
#c 7
#! Extending the keyword search paradigm to relational data has been an active area of research within the database and IR community during the past decade. Many approaches have been proposed, but despite numerous publications, there remains a severe lack of standardization for the evaluation of proposed search techniques. Lack of standardization has resulted in contradictory results from different evaluations, and the numerous discrepancies muddle what advantages are proffered by different approaches. In this paper, we present the most extensive empirical performance evaluation of relational keyword search techniques to appear to date in the literature. Our results indicate that many existing search techniques do not provide acceptable performance for realistic retrieval tasks. In particular, memory consumption precludes many search techniques from scaling beyond small data sets with tens of thousands of vertices. We also explore the relationship between execution time and factors varied in previous evaluations; our analysis indicates that most of these factors have relatively little impact on performance. In summary, our work confirms previous claims regarding the unacceptable performance of these search techniques and underscores the need for standardization in evaluations--standardization exemplified by the IR community.

#index 2044175
#* Data Mining with Big Data
#@ Xindong Wu;Xingquan Zhu;Gong-Qing Wu;Wei Ding
#t 2014
#c 7
#! Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.

#index 2044176
#* Active Learning of Constraints for Semi-Supervised Clustering
#@ Sicheng Xiong;Javad Azimi;Xiaoli Z. Fern
#t 2014
#c 7
#! Semi-supervised clustering aims to improve clustering performance by considering user supervision in the form of pairwise constraints. In this paper, we study the active learning problem of selecting pairwise must-link and cannot-link constraints for semi-supervised clustering. We consider active learning in an iterative manner where in each iteration queries are selected based on the current clustering solution and the existing constraint set. We apply a general framework that builds on the concept of neighborhood, where neighborhoods contain "labeled examples" of different clusters according to the pairwise constraints. Our active learning method expands the neighborhoods by selecting informative points and querying their relationship with the neighborhoods. Under this framework, we build on the classic uncertainty-based principle and present a novel approach for computing the uncertainty associated with each data point. We further introduce a selection criterion that trades off the amount of uncertainty of each data point with the expected number of queries (the cost) required to resolve this uncertainty. This allows us to select queries that have the highest information rate. We evaluate the proposed method on the benchmark data sets and the results demonstrate consistent and substantial improvements over the current state of the art.

#index 2044177
#* Approximate Shortest Distance Computing: A Query-Dependent Local Landmark Scheme
#@ Miao Qiao;Hong Cheng;Lijun Chang;Jeffrey Xu Yu
#t 2014
#c 7
#! Shortest distance query is a fundamental operation in large-scale networks. Many existing methods in the literature take a landmark embedding approach, which selects a set of graph nodes as landmarks and computes the shortest distances from each landmark to all nodes as an embedding. To answer a shortest distance query, the precomputed distances from the landmarks to the two query nodes are used to compute an approximate shortest distance based on the triangle inequality. In this paper, we analyze the factors that affect the accuracy of distance estimation in landmark embedding. In particular, we find that a globally selected, query-independent landmark set may introduce a large relative error, especially for nearby query nodes. To address this issue, we propose a query-dependent local landmark scheme, which identifies a local landmark close to both query nodes and provides more accurate distance estimation than the traditional global landmark approach. We propose efficient local landmark indexing and retrieval techniques, which achieve low offline indexing complexity and online query complexity. Two optimization techniques on graph compression and graph online search are also proposed, with the goal of further reducing index size and improving query accuracy. Furthermore, the challenge of immense graphs whose index may not fit in the memory leads us to store the embedding in relational database, so that a query of the local landmark scheme can be expressed with relational operators. Effective indexing and query optimization mechanisms are designed in this context. Our experimental results on large-scale social networks and road networks demonstrate that the local landmark scheme reduces the shortest distance estimation error significantly when compared with global landmark embedding and the state-of-the-art sketch-based embedding.

#index 2044178
#* Automatic Generation of the Domain Module from Electronic Textbooks: Method and Validation
#@ Mikel Larranaga;Angel Conde;Inaki Calvo;Jon A. Elorriaga;Ana Arruarte
#t 2014
#c 7
#! Technology-supported learning systems have proved to be helpful in many learning situations. These systems require an appropriate representation of the knowledge to be learned, the Domain Module. The authoring of the Domain Module is cost and labor intensive, but its development cost might be lightened by profiting from semiautomatic Domain Module authoring techniques and promoting knowledge reuse. DOM-Sortze is a system that uses natural language processing techniques, heuristic reasoning, and ontologies for the semiautomatic construction of the Domain Module from electronic textbooks. To determine how it might help in the Domain Module authoring process, it has been tested with an electronic textbook, and the gathered knowledge has been compared with the Domain Module that instructional designers developed manually. This paper presents DOM-Sortze and describes the experiment carried out.

#index 2044179
#* Consensus-Based Ranking of Multivalued Objects: A Generalized Borda Count Approach
#@ Ying Zhang;Wenjie Zhang;Jian Pei;Xuemin Lin;Qianlu Lin;Aiping Li
#t 2014
#c 7
#! In this paper, we tackle a novel problem of ranking multivalued objects, where an object has multiple instances in a multidimensional space, and the number of instances per object is not fixed. Given an ad hoc scoring function that assigns a score to a multidimensional instance, we want to rank a set of multivalued objects. Different from the existing models of ranking uncertain and probabilistic data, which model an object as a random variable and the instances of an object are assumed exclusive, we have to capture the coexistence of instances here. To tackle the problem, we advocate the semantics of favoring widely preferred objects instead of majority votes, which is widely used in many elections and competitions. Technically, we borrow the idea from Borda Count (BC), a well-recognized method in consensus-based voting systems. However, Borda Count cannot handle multivalued objects of inconsistent cardinality, and is costly to evaluate top $(k)$ queries on large multidimensional data sets. To address the challenges, we extend and generalize Borda Count to quantile-based Borda Count, and develop efficient computational methods with comprehensive cost analysis. We present case studies on real data sets to demonstrate the effectiveness of the generalized Borda Count ranking, and use synthetic and real data sets to verify the efficiency of our computational method.

#index 2044180
#* Decision Trees for Mining Data Streams Based on the Gaussian Approximation
#@ Leszek Rutkowski;Maciej Jaworski;Lena Pietruczuk;Piotr Duda
#t 2014
#c 7
#! Since the Hoeffding tree algorithm was proposed in the literature, decision trees became one of the most popular tools for mining data streams. The key point of constructing the decision tree is to determine the best attribute to split the considered node. Several methods to solve this problem were presented so far. However, they are either wrongly mathematically justified (e.g., in the Hoeffding tree algorithm) or time-consuming (e.g., in the McDiarmid tree algorithm). In this paper, we propose a new method which significantly outperforms the McDiarmid tree algorithm and has a solid mathematical basis. Our method ensures, with a high probability set by the user, that the best attribute chosen in the considered node using a finite data sample is the same as it would be in the case of the whole data stream.

#index 2044181
#* Discovering Emerging Topics in Social Streams via Link-Anomaly Detection
#@ Toshimitsu Takahashi;Ryota Tomioka;Kenji Yamanishi
#t 2014
#c 7
#! Detection of emerging topics is now receiving renewed interest motivated by the rapid growth of social networks. Conventional-term-frequency-based approaches may not be appropriate in this context, because the information exchanged in social-network posts include not only text but also images, URLs, and videos. We focus on emergence of topics signaled by social aspects of theses networks. Specifically, we focus on mentions of users--links between users that are generated dynamically (intentionally or unintentionally) through replies, mentions, and retweets. We propose a probability model of the mentioning behavior of a social network user, and propose to detect the emergence of a new topic from the anomalies measured through the model. Aggregating anomaly scores from hundreds of users, we show that we can detect emerging topics only based on the reply/mention relationships in social-network posts. We demonstrate our technique in several real data sets we gathered from Twitter. The experiments show that the proposed mention-anomaly-based approaches can detect new topics at least as early as text-anomaly-based approaches, and in some cases much earlier when the topic is poorly identified by the textual contents in posts.

#index 2044182
#* Ensembles of $({\alpha})$-Trees for Imbalanced Classification Problems
#@ Yubin Park;Joydeep Ghosh
#t 2014
#c 7
#! This paper introduces two kinds of decision tree ensembles for imbalanced classification problems, extensively utilizing properties of $(\alpha)$-divergence. First, a novel splitting criterion based on $(\alpha)$-divergence is shown to generalize several well-known splitting criteria such as those used in C4.5 and CART. When the $(\alpha)$-divergence splitting criterion is applied to imbalanced data, one can obtain decision trees that tend to be less correlated ($(\alpha)$-diversification) by varying the value of $(\alpha)$. This increased diversity in an ensemble of such trees improves AUROC values across a range of minority class priors. The second ensemble uses the same alpha trees as base classifiers, but uses a lift-aware stopping criterion during tree growth. The resultant ensemble produces a set of interpretable rules that provide higher lift values for a given coverage, a property that is much desirable in applications such as direct marketing. Experimental results across many class-imbalanced data sets, including BRFSS, and MIMIC data sets from the medical community and several sets from UCI and KEEL are provided to highlight the effectiveness of the proposed ensembles over a wide range of data distributions and of class imbalance.

#index 2044183
#* Event Characterization and Prediction Based on Temporal Patterns in Dynamic Data System
#@ Wenjing Zhang;Xin Feng
#t 2014
#c 7
#! The new method proposed in this paper applies a multivariate reconstructed phase space (MRPS) for identifying multivariate temporal patterns that are characteristic and predictive of anomalies or events in a dynamic data system. The new method extends the original univariate reconstructed phase space framework, which is based on fuzzy unsupervised clustering method, by incorporating a new mechanism of data categorization based on the definition of events. In addition to modeling temporal dynamics in a multivariate phase space, a Bayesian approach is applied to model the first-order Markov behavior in the multidimensional data sequences. The method utilizes an exponential loss objective function to optimize a hybrid classifier which consists of a radial basis kernel function and a log-odds ratio component. We performed experimental evaluation on three data sets to demonstrate the feasibility and effectiveness of the proposed approach.

#index 2044184
#* Linkable Ring Signature with Unconditional Anonymity
#@ Joseph K. Liu;Man Ho Au;Willy Susilo;Jianying Zhou
#t 2014
#c 7
#! In this paper, we construct a linkable ring signature scheme with unconditional anonymity. It has been regarded as an open problem in [22] since 2004 for the construction of an unconditional anonymous linkable ring signature scheme. We are the first to solve this open problem by giving a concrete instantiation, which is proven secure in the random oracle model. Our construction is even more efficient than other schemes that can only provide computational anonymity. Simultaneously, our scheme can act as an counterexample to show that [19, Theorem 1] is not always true, which stated that linkable ring signature scheme cannot provide strong anonymity. Yet we prove that our scheme can achieve strong anonymity (under one of the interpretations).

#index 2044185
#* Mining Weakly Labeled Web Facial Images for Search-Based Face Annotation
#@ Dayong Wang;Steven C. H. Hoi;Ying He;Jianke Zhu
#t 2014
#c 7
#! This paper investigates a framework of search-based face annotation (SBFA) by mining weakly labeled facial images that are freely available on the World Wide Web (WWW). One challenging problem for search-based face annotation scheme is how to effectively perform annotation by exploiting the list of most similar facial images and their weak labels that are often noisy and incomplete. To tackle this problem, we propose an effective unsupervised label refinement (ULR) approach for refining the labels of web facial images using machine learning techniques. We formulate the learning problem as a convex optimization and develop effective optimization algorithms to solve the large-scale learning task efficiently. To further speed up the proposed scheme, we also propose a clustering-based approximation algorithm which can improve the scalability considerably. We have conducted an extensive set of empirical studies on a large-scale web facial image testbed, in which encouraging results showed that the proposed ULR algorithms can significantly boost the performance of the promising SBFA scheme.

#index 2044186
#* Privacy-Preserving Enhanced Collaborative Tagging
#@ Javier Parra-Arnau;Andrea Perego;Elena Ferrari;Jordi Forne;David Rebollo-Monedero
#t 2014
#c 7
#! Collaborative tagging is one of the most popular services available online, and it allows end user to loosely classify either online or offline resources based on their feedback, expressed in the form of free-text labels (i.e., tags). Although tags may not be per se sensitive information, the wide use of collaborative tagging services increases the risk of cross referencing, thereby seriously compromising user privacy. In this paper, we make a first contribution toward the development of a privacy-preserving collaborative tagging service, by showing how a specific privacy-enhancing technology, namely tag suppression, can be used to protect end-user privacy. Moreover, we analyze how our approach can affect the effectiveness of a policy-based collaborative tagging system that supports enhanced web access functionalities, like content filtering and discovery, based on preferences specified by end users.

#index 2044187
#* Rough Sets, Kernel Set, and Spatiotemporal Outlier Detection
#@ Alessia Albanese;Sankar K. Pal;Alfredo Petrosino
#t 2014
#c 7
#! Nowadays, the high availability of data gathered from wireless sensor networks and telecommunication systems has drawn the attention of researchers on the problem of extracting knowledge from spatiotemporal data. Detecting outliers which are grossly different from or inconsistent with the remaining spatiotemporal data set is a major challenge in real-world knowledge discovery and data mining applications. In this paper, we deal with the outlier detection problem in spatiotemporal data and describe a rough set approach that finds the top outliers in an unlabeled spatiotemporal data set. The proposed method, called Rough Outlier Set Extraction (ROSE), relies on a rough set theoretic representation of the outlier set using the rough set approximations, i.e., lower and upper approximations. We have also introduced a new set, named Kernel Set, that is a subset of the original data set, which is able to describe the original data set both in terms of data structure and of obtained results. Experimental results on real-world data sets demonstrate the superiority of ROSE, both in terms of some quantitative indices and outliers detected, over those obtained by various rough fuzzy clustering algorithms and by the state-of-the-art outlier detection methods. It is also demonstrated that the kernel set is able to detect the same outliers set but with less computational time.

#index 2044188
#* Semisupervised Wrapper Choice and Generation for Print-Oriented Documents
#@ Alberto Bartoli;Giorgio Davanzo;Eric Medvet;Enrico Sorio
#t 2014
#c 7
#! Information extraction from printed documents is still a crucial problem in many interorganizational workflows. Solutions for other application domains, for example, the web, do not fit this peculiar scenario well, as printed documents do not carry any explicit structural or syntactical description. Moreover, printed documents usually lack any explicit indication about their source. We present a system, which we call PATO, for extracting predefined items from printed documents in a dynamic multisource scenario. PATO selects the source-specific wrapper required by each document, determines whether no suitable wrapper exists, and generates one when necessary. PATO assumes that the need for new source-specific wrappers is a part of normal system operation: new wrappers are generated online based on a few point-and-click operations performed by a human operator on a GUI. The role of operators is an integral part of the design and PATO may be configured to accommodate a broad range of automation levels. We show that PATO exhibits very good performance on a challenging data set composed of more than 600 printed documents drawn from three different application domains: invoices, datasheets of electronic components, and patents. We also perform an extensive analysis of the crucial tradeoff between accuracy and automation level.

#index 2044189
#* Spatially Aware Term Selection for Geotagging
#@ Olivier Van Laere;Jonathan Quinn;Steven Schockaert;Bart Dhoedt
#t 2014
#c 7
#! The task of assigning geographic coordinates to textual resources plays an increasingly central role in geographic information retrieval. The ability to select those terms from a given collection that are most indicative of geographic location is of key importance in successfully addressing this task. However, this process of selecting spatially relevant terms is at present not well understood, and the majority of current systems are based on standard term selection techniques, such as $(\chi^2)$ or information gain, and thus fail to exploit the spatial nature of the domain. In this paper, we propose two classes of term selection techniques based on standard geostatistical methods. First, to implement the idea of spatial smoothing of term occurrences, we investigate the use of kernel density estimation (KDE) to model each term as a two-dimensional probability distribution over the surface of the Earth. The second class of term selection methods we consider is based on Ripley's K statistic, which measures the deviation of a point set from spatial homogeneity. We provide experimental results which compare these classes of methods against existing baseline techniques on the tasks of assigning coordinates to Flickr photos and to Wikipedia articles, revealing marked improvements in cases where only a relatively small number of terms can be selected.

#index 2044190
#* Structural Diversity for Resisting Community Identification in Published Social Networks
#@ Chih-Hua Tai;Philip S. Yu;De-Nian Yang;Ming-Syan Chen
#t 2014
#c 7
#! As an increasing number of social networking data is published and shared for commercial and research purposes, privacy issues about the individuals in social networks have become serious concerns. Vertex identification, which identifies a particular user from a network based on background knowledge such as vertex degree, is one of the most important problems that have been addressed. In reality, however, each individual in a social network is inclined to be associated with not only a vertex identity but also a community identity, which can represent the personal privacy information sensitive to the public, such as political party affiliation. This paper first addresses the new privacy issue, referred to as community identification, by showing that the community identity of a victim can still be inferred even though the social network is protected by existing anonymity schemes. For this problem, we then propose the concept of structural diversity to provide the anonymity of the community identities. The $(k)$-Structural Diversity Anonymization ($(k)$-SDA) is to ensure sufficient vertices with the same vertex degree in at least $(k)$ communities in a social network. We propose an Integer Programming formulation to find optimal solutions to $(k)$-SDA and also devise scalable heuristics to solve large-scale instances of $(k)$-SDA from different perspectives. The performance studies on real data sets from various perspectives demonstrate the practical utility of the proposed privacy scheme and our anonymization approaches.

#index 2044191
#* A Bayesian Inference-Based Framework for RFID Data Cleansing
#@ Wei-Shinn Ku;Haiquan Chen;Haixun Wang;Min-Te Sun
#t 2013
#c 7
#! The past few years have witnessed the emergence of an increasing number of applications for tracking and tracing based on radio frequency identification (RFID) technologies. However, raw RFID readings are usually of low quality and may contain numerous anomalies. An ideal solution for RFID data cleansing should address the following issues. First, in many applications, duplicate readings of the same object are very common. The solution should take advantage of the resulting data redundancy for data cleaning. Second, prior knowledge about the environment may help improve data quality, and a desired solution must be able to take into account such knowledge. Third, the solution should take advantage of physical constraints in target applications to elevate the accuracy of data cleansing. There are several existing RFID data cleansing techniques. However, none of them support all the aforementioned features. In this paper, we propose a Bayesian inference-based framework for cleaning RFID raw data. We first design an $(n)$-state detection model and formally prove that the three-state model can maximize the system performance. Then, we extend the $(n)$-state model to support two-dimensional RFID reader arrays and compute the likelihood efficiently. In addition, we devise a Metropolis-Hastings sampler with constraints, which incorporates constraint management to clean RFID data with high efficiency and accuracy. Moreover, to support real-time object monitoring, we present the streaming Bayesian inference method to cope with real-time RFID data streams. Finally, we evaluate the performance of our solutions through extensive experiments.

#index 2044192
#* Binary- and Multi-class Group Sparse Canonical Correlation Analysis for Feature Extraction and Classification
#@ Zhao Zhang;Mingbo Zhao;Tommy W.  S. Chow
#t 2013
#c 7
#! This paper incorporates the group sparse representation into the well-known canonical correlation analysis (CCA) framework and proposes a novel discriminant feature extraction technique named group sparse canonical correlation analysis (GSCCA). GSCCA uses two sets of variables and aims at preserving the group sparse (GS) characteristics of data within each set in addition to maximize the global interset covariance. With GS weights computed prior to feature extraction, the locality, sparsity and discriminant information of data can be adaptively determined. The GS weights are obtained from an NP-hard group-sparsity promoting problem that considers all highly correlated data within a group. By defining one of the two variable sets as the class label matrix, GSCCA is effectively extended to multiclass scenarios. Then GSCCA is theoretically formulated as a least-squares problem as CCA does. Comparative analysis between this work and the related studies demonstrate that our algorithm is more general exhibiting attractive properties. The projection matrix of GSCCA is analytically solved by applying eigen-decomposition and trace ratio (TR) optimization. Extensive benchmark simulations are conducted to examine GSCCA. Results show that our approach delivers promising results, compared with other related algorithms.

#index 2044193
#* Clustering Based on Enhanced $(\alpha)$-Expansion Move
#@ Yun Zheng;Pei Chen
#t 2013
#c 7
#! The exemplar-based data clustering problem can be formulated as minimizing an energy function defined on a Markov random field (MRF). However, most algorithms for optimizing the MRF energy function cannot be directly applied to the task of clustering, as the problem has a high-order energy function. In this paper, we first show that the high-order energy function for the clustering problem can be simplified as a pairwise energy function with the metric property, and consequently it can be optimized by the $(\alpha)$-expansion move algorithm based on graph cut. Then, the original expansion move algorithm is improved in the following two aspects: 1) Instead of solving a minimal s-t graph cut problem, we show that there is an explicit and interpretable solution for minimizing the energy function in the clustering problem. Based on this interpretation, a fast $(\alpha)$-expansion move algorithm is proposed, which is much more efficient than the graph-cut-based algorithm. 2) The fast $(\alpha)$-expansion move algorithm is further improved by extending its move space so that a larger energy value reduction can be achieved in each iteration. Experiments on benchmark data sets show that the enhanced expansion move algorithm has a better performance, compared to other state-of-the-art exemplar-based clustering algorithms.

#index 2044194
#* Efficient Sentinel Mining Using Bitmaps on Modern Processors
#@ Morten Middelfart;Torben Bach Pedersen;Jan Krogsgaard
#t 2013
#c 7
#! This paper proposes a highly efficient bitmap-based approach for discovery of so-called sentinels. Sentinels represent schema level relationships between changes over time in certain measures in a multidimensional data cube. Sentinels are actionable and notify users based on previous observations, for example, that revenue might drop within two months if an increase in customer problems combined with a decrease in website traffic is observed. We significantly extend prior art by representing the sentinel mining problem by bitmap operations, using bitmapped encoding of so-called indication streams. We present a very efficient algorithm, SentBit, that is 2-3 orders of magnitude faster than the state of the art, and utilizes CPU specific instructions and the multicore architectures available on modern processors. The SentBit algorithm scales efficiently to very large data sets, which is verified by extensive experiments on both real and synthetic data.

#index 2044195
#* Efficient and Scalable Processing of String Similarity Join
#@ Chuitian Rong;Wei Lu;Xiaoli Wang;Xiaoyong Du;Yueguo Chen;Anthony K. H. Tung
#t 2013
#c 7
#! The string similarity join is a basic operation of many applications that need to find all string pairs from a collection given a similarity function and a user-specified threshold. Recently, there has been considerable interest in designing new algorithms with the assistant of an inverted index to support efficient string similarity joins. These algorithms typically adopt a two-step filter-and-refine approach in identifying similar string pairs: 1) generating candidate pairs by traversing the inverted index; and 2) verifying the candidate pairs by computing the similarity. However, these algorithms either suffer from poor filtering power (which results in high verification cost), or incur too much computational cost to guarantee the filtering power. In this paper, we propose a multiple prefix filtering method based on different global orderings such that the number of candidate pairs can be reduced significantly. We also propose a parallel extension of the algorithm that is efficient and scalable in a MapReduce framework. We conduct extensive experiments on both centralized and Hadoop systems using both real and synthetic data sets, and the results show that our proposed approach outperforms existing approaches in both efficiency and scalability.

#index 2044196
#* Identifying the Most Connected Vertices in Hidden Bipartite Graphs Using Group Testing
#@ Jianguo Wang;Eric Lo;Man Lung Yiu
#t 2013
#c 7
#! A graph is called hidden if the edges are not explicitly given and edge probe tests are required to detect the presence of edges. This paper studies the $(k)$ most connected vertices ($(k)$MCV) problem on hidden bipartite graphs, which has applications in spatial databases, graph databases, and bioinformatics. There is a prior work on the $(k)$MCV problem, which is based on the "2-vertex testing" model, i.e., an edge probe test can only reveal the existence of an edge between two individual vertices. We study the $(k)$MCV problem, in the context of a more general edge probe test model called "group testing." A group test can reveal whether there exists some edge between a vertex and a group of vertices. If group testing is used properly, a single invocation of a group test can reveal as much information as multiple invocations of 2-vertex tests. We discuss the cases and applications where group testing could be used, and present an algorithm, namely, GMCV, that adaptively leverages group testing to solve the $(k)$MCV problem.

#index 2044197
#* iLike: Bridging the Semantic Gap in Vertical Image Search by Integrating Text and Visual Features
#@ Yuxin Chen;Hariprasad Sampathkumar;Bo Luo;Xue-wen Chen
#t 2013
#c 7
#! With the development of Internet and Web 2.0, large-volume multimedia contents have been made available online. It is highly desired to provide easy accessibility to such contents, i.e., efficient and precise retrieval of images that satisfies users' needs. Toward this goal, content-based image retrieval (CBIR) has been intensively studied in the research community, while text-based search is better adopted in the industry. Both approaches have inherent disadvantages and limitations. Therefore, unlike the great success of text search, web image search engines are still premature. In this paper, we present iLike, a vertical image search engine that integrates both textual and visual features to improve retrieval performance. We bridge the semantic gap by capturing the meaning of each text term in the visual feature space, and reweight visual features according to their significance to the query terms. We also bridge the user intention gap because we are able to infer the "visual meanings" behind the textual queries. Last but not least, we provide a visual thesaurus, which is generated from the statistical similarity between the visual space representation of textual terms. Experimental results show that our approach improves both precision and recall, compared with content-based or text-based image retrieval techniques. More importantly, search results from iLike is more consistent with users' perception of the query terms.

#index 2044198
#* Improving Security and Efficiency in Attribute-Based Data Sharing
#@ Junbeom Hur
#t 2013
#c 7
#! With the recent adoption and diffusion of the data sharing paradigm in distributed systems such as online social networks or cloud computing, there have been increasing demands and concerns for distributed data security. One of the most challenging issues in data sharing systems is the enforcement of access policies and the support of policies updates. Ciphertext policy attribute-based encryption (CP-ABE) is becoming a promising cryptographic solution to this issue. It enables data owners to define their own access policies over user attributes and enforce the policies on the data to be distributed. However, the advantage comes with a major drawback which is known as a key escrow problem. The key generation center could decrypt any messages addressed to specific users by generating their private keys. This is not suitable for data sharing scenarios where the data owner would like to make their private data only accessible to designated users. In addition, applying CP-ABE in the data sharing system introduces another challenge with regard to the user revocation since the access policies are defined only over the attribute universe. Therefore, in this study, we propose a novel CP-ABE scheme for a data sharing system by exploiting the characteristic of the system architecture. The proposed scheme features the following achievements: 1) the key escrow problem could be solved by escrow-free key issuing protocol, which is constructed using the secure two-party computation between the key generation center and the data-storing center, and 2) fine-grained user revocation per each attribute could be done by proxy encryption which takes advantage of the selective attribute group key distribution on top of the ABE. The performance and security analyses indicate that the proposed scheme is efficient to securely manage the data distributed in the data sharing system.

#index 2044199
#* Incremental Learning of Concept Drift from Streaming Imbalanced Data
#@ Gregory Ditzler;Robi Polikar
#t 2013
#c 7
#! Learning in nonstationary environments, also known as learning concept drift, is concerned with learning from data whose statistical characteristics change over time. Concept drift is further complicated if the data set is class imbalanced. While these two issues have been independently addressed, their joint treatment has been mostly underexplored. We describe two ensemble-based approaches for learning concept drift from imbalanced data. Our first approach is a logical combination of our previously introduced Learn++.NSE algorithm for concept drift, with the well-established SMOTE for learning from imbalanced data. Our second approach makes two major modifications to Learn++.NSE-SMOTE integration by replacing SMOTE with a subensemble that makes strategic use of minority class data; and replacing Learn++.NSE and its class-independent error weighting mechanism with a penalty constraint that forces the algorithm to balance accuracy on all classes. The primary novelty of this approach is in determining the voting weights for combining ensemble members, based on each classifier's time and imbalance-adjusted accuracy on current and past environments. Favorable results in comparison to other approaches indicate that both approaches are able to address this challenging problem, each with its own specific areas of strength. We also release all experimental data as a resource and benchmark for future research.

#index 2044200
#* Inference-Based Naïve Bayes: Turning Naïve Bayes Cost-Sensitive
#@ Xiao Fang
#t 2013
#c 7
#! A fundamental challenge for developing a cost-sensitive Naïve Bayes method is how to effectively classify an instance based on the cost-sensitive threshold computed under the assumption of knowing the instance's true classification probabilities and the highly biased estimations of these probabilities by the Naïve Bayes method. To address this challenge, we develop a cost-sensitive Naïve Bayes method from a novel perspective of inferring the order relation (e.g., greater than or equal to, less than) between an instance's true classification probability of belonging to the class of interest and the cost-sensitive threshold. Our method learns and infers the order relation from the training data and classifies the instance based on the inferred order relation. We empirically show that our proposed method significantly outperforms major existing methods for turning Naïve Bayes cost-sensitive through experiments with UCI data sets and a real-world case study.

#index 2044201
#* On Co-Scheduling of Update and Control Transactions in Real-Time Sensing and Control Systems: Algorithms, Analysis, and Performance
#@ Song Han;Kam-Yiu Lam;Jiantao Wang;Krithi Ramamritham;Aloysius K. Mok
#t 2013
#c 7
#! Maintaining sensor data validity while exercising timely control is crucial in real-time sensing and control systems. The goal of scheduling algorithms deployed in such systems is to maintain the validity of real-time sensor data so as to maximize the schedulability of update transactions with minimum update workload so that control actions occur on time. In this paper, we first propose a dynamic scheduling algorithm, called Deferrable Scheduling with Least Actual Laxity First (DS-LALF). DS-LALF is designed by extending the deferrable scheduling algorithm, DS-FP which is designed for fixed priority systems. We develop a schedulability test algorithm for DS-LALF based on pattern analysis and a pattern search algorithm to find the shortest and earliest pattern in the schedule. Then, based on DS-LALF, a co-scheduling algorithm called Co-LALF--to schedule update transactions and control transactions in a real-time sensing and control system together--is developed 1) to meet the deadlines of all the control transactions and 2) to maximize the quality of data (QoD) utilized by the control transactions. Co-LALF schedules the jobs in the ascending order of their actual laxities and defers the release times of update jobs as long as the corresponding sensor data are maintained within the required quality. Experimental results show that DS-LALF incurs lower update workload compared with DS-FP and ML, and its schedulability is close to DS-FP but is much better than ML and DS-EDF. The experimental results also show that Co-LALF is effective in improving the overall performance by ensuring better QoD for the real-time data while meeting the deadline constraints of all the control transactions.

#index 2044202
#* Predictive Handling of Asynchronous Concept Drifts in Distributed Environments
#@ Hock Hee Ang;Vivekanand Gopalkrishnan;Indre Zliobaite;Mykola Pechenizkiy;Steven C. H. Hoi
#t 2013
#c 7
#! In a distributed computing environment, peers collaboratively learn to classify concepts of interest from each other. When external changes happen and their concepts drift, the peers should adapt to avoid increase in misclassification errors. The problem of adaptation becomes more difficult when the changes are asynchronous, i.e., when peers experience drifts at different times. We address this problem by developing an ensemble approach, PINE, that combines reactive adaptation via drift detection, and proactive handling of upcoming changes via early warning and adaptation across the peers. With empirical study on simulated and real-world data sets, we show that PINE handles asynchronous concept drifts better and faster than current state-of-the-art approaches, which have been designed to work in less challenging environments. In addition, PINE is parameter insensitive and incurs less communication cost while achieving better accuracy.

#index 2044203
#* Ranking Instances by Maximizing the Area under ROC Curve
#@ H. Altay Guvenir;Murat Kurtcephe
#t 2013
#c 7
#! In recent years, the problem of learning a real-valued function that induces a ranking over an instance space has gained importance in machine learning literature. Here, we propose a supervised algorithm that learns a ranking function, called ranking instances by maximizing the area under the ROC curve (RIMARC). Since the area under the ROC curve (AUC) is a widely accepted performance measure for evaluating the quality of ranking, the algorithm aims to maximize the AUC value directly. For a single categorical feature, we show the necessary and sufficient condition that any ranking function must satisfy to achieve the maximum AUC. We also sketch a method to discretize a continuous feature in a way to reach the maximum AUC as well. RIMARC uses a heuristic to extend this maximization to all features of a data set. The ranking function learned by the RIMARC algorithm is in a human-readable form; therefore, it provides valuable information to domain experts for decision making. Performance of RIMARC is evaluated on many real-life data sets by using different state-of-the-art algorithms. Evaluations of the AUC metric show that RIMARC achieves significantly better performance compared to other similar methods.

#index 2044204
#* Set Reconciliation via Counting Bloom Filters
#@ Deke Guo;Mo Li
#t 2013
#c 7
#! In this paper, we study the set reconciliation problem, in which each member of a node pair has a set of objects and seeks to deliver its unique objects to the other member. How could each node compute the set difference, however, is challenging in the set reconciliation problem. To address such an issue, we propose a lightweight but efficient method that only requires the pair of nodes to represent objects using a counting Bloom filter (CBF) of size $(O(d))$ and exchange with each other, where $(d)$ denotes the total size of the set differences. A receiving node then subtracts the received CBF from its local one via minus operation proposed in this paper. The resultant CBF can approximately represent the union of the set differences and thus the set difference to each node can be identified after querying the resultant CBF. In this paper, we propose a novel estimator through which each node can accurately estimate not only the value of $(d)$ but also the size of the set difference to each node. Such an estimation result can be used to optimize the parameter setting of the CBF to achieve less false positives and false negatives. Comprehensive analysis and evaluation demonstrates that our method is more efficient than prior BF-based methods in terms of achieving the same accuracy with less communication cost. Moreover, our reconciliating method needs no prior context logs and it is very useful in networking and distributed applications.

#index 2044205
#* Supervised Multiple Kernel Embedding for Learning Predictive Subspaces
#@ Mehmet Gonen
#t 2013
#c 7
#! For supervised learning problems, dimensionality reduction is generally applied as a preprocessing step. However, coupled training of dimensionality reduction and supervised learning steps may improve the prediction performance. In this paper, we propose a novel dimensionality reduction algorithm coupled with a supervised kernel-based learner, called supervised multiple kernel embedding, that integrates multiple kernel learning to dimensionality reduction and performs prediction on the projected subspace with a joint optimization framework. Combining multiple kernels allows us to combine different feature representations and/or similarity measures toward a unified subspace. We perform experiments on one digit recognition and two bioinformatics data sets. Our proposed method significantly outperforms multiple kernel Fisher discriminant analysis followed by a standard kernel-based learner, especially on low dimensions.

#index 2044206
#* T-Finder: A Recommender System for Finding Passengers and Vacant Taxis
#@ Nicholas Jing Yuan;Yu Zheng;Liuhang Zhang;Xing Xie
#t 2013
#c 7
#! This paper presents a recommender system for both taxi drivers and people expecting to take a taxi, using the knowledge of 1) passengers' mobility patterns and 2) taxi drivers' picking-up/dropping-off behaviors learned from the GPS trajectories of taxicabs. First, this recommender system provides taxi drivers with some locations and the routes to these locations, toward which they are more likely to pick up passengers quickly (during the routes or in these locations) and maximize the profit of the next trip. Second, it recommends people with some locations (within a walking distance) where they can easily find vacant taxis. In our method, we learn the above-mentioned knowledge (represented by probabilities) from GPS trajectories of taxis. Then, we feed the knowledge into a probabilistic model that estimates the profit of the candidate locations for a particular driver based on where and when the driver requests the recommendation. We build our system using historical trajectories generated by over 12,000 taxis during 110 days and validate the system with extensive evaluations including in-the-field user studies.

#index 2044207
#* Toward the Automatic Extraction of Policy Networks Using Web Links and Documents
#@ Theodosis Moschopoulos;Elias Iosif;Leeda Demetropoulou;Alexandros Potamianos;Shrikanth Narayanan
#t 2013
#c 7
#! Policy networks are widely used by political scientists and economists to explain various financial and social phenomena, such as the development of partnerships between political entities or institutions from different levels of governance. The analysis of policy networks demands a series of arduous and time-consuming manual steps including interviews and questionnaires. In this paper, we estimate the strength of relations between actors in policy networks using features extracted from data harvested from the web. Features include webpage counts, outlinks, and lexical information extracted from web documents or web snippets. The proposed approach is automatic and does not require any external knowledge source, other than the specification of the word forms that correspond to the political actors. The features are evaluated both in isolation and jointly for both positive and negative (antagonistic) actor relations. The proposed algorithms are evaluated on two EU policy networks from the political science literature. Performance is measured in terms of correlation and mean square error between the human rated and the automatically extracted relations. Correlation of up to 0.74 is achieved for positive relations. The extracted networks are validated by political scientists and useful conclusions about the evolution of the networks over time are drawn.

#index 2044208
#* NHOP: A Nested Associative Pattern for Analysis of Consensus Sequence Ensembles
#@ David K. Y. Chiu;Thomas W. H. Lui
#t 2013
#c 7
#! In this research, we introduce a novel, complex associative pattern that is found to be very useful because it identifies the core associative structure from the data. We refer to it as nested high-order pattern. The pattern is more specific than associative patterns represented as multiple variables. It also generalizes sequential patterns, as the outcomes need not be contiguous. This paper outlines two search algorithms, the $(r)$-Tree and Best-$(k)$ algorithm in its detection. It was then applied to an analysis of biomolecule using the aligned sequence family of the molecule. In the SH3 protein, a model for protein-protein interaction mediator, we identify functional groups (core and binding sites) in the three-dimensional structure as well as amino acid patterns dominating certain species.

#index 2044209
#* A Family of Joint Sparse PCA Algorithms for Anomaly Localization in Network Data Streams
#@ Ruoyi Jiang;Hongliang Fei;Jun Huan
#t 2013
#c 7
#! Determining anomalies in data streams that are collected and transformed from various types of networks has recently attracted significant research interest. Principal component analysis (PCA) has been extensively applied to detecting anomalies in network data streams. However, none of existing PCA-based approaches addresses the problem of identifying the sources that contribute most to the observed anomaly, or anomaly localization. In this paper, we propose novel sparse PCA methods to perform anomaly detection and localization for network data streams. Our key observation is that we can localize anomalies by identifying a sparse low-dimensional space that captures the abnormal events in data streams. To better capture the sources of anomalies, we incorporate the structure information of the network stream data in our anomaly localization framework. Furthermore, we extend our joint sparse PCA framework with multidimensional Karhunen Loève Expansion that considers both spatial and temporal domains of data streams to stabilize localization performance. We have performed comprehensive experimental studies of the proposed methods and have compared our methods with the state-of-the-art using three real-world data sets from different application domains. Our experimental studies demonstrate the utility of the proposed methods.

#index 2044210
#* An Evaluation of Model-Based Approaches to Sensor Data Compression
#@ Quoc Viet Hung Nguyen;Hoyoung Jeung;Karl Aberer
#t 2013
#c 7
#! As the volumes of sensor data being accumulated are likely to soar, data compression has become essential in a wide range of sensor-data applications. This has led to a plethora of data compression techniques for sensor data, in particular model-based approaches have been spotlighted due to their significant compression performance. These methods, however, have never been compared and analyzed under the same setting, rendering a "right" choice of compression technique for a particular application very difficult. Addressing this problem, this paper presents a benchmark that offers a comprehensive empirical study on the performance comparison of the model-based compression techniques. Specifically, we reimplemented several state-of-the-art methods in a comparable manner, and measured various performance factors with our benchmark, including compression ratio, computation time, model maintenance cost, approximation quality, and robustness to noisy data. We then provide in-depth analysis of the benchmark results, obtained by using 11 different real data sets consisting of 346 heterogeneous sensor data signals. We believe that the findings from the benchmark will be able to serve as a practical guideline for applications that need to compress sensor data.

#index 2044211
#* Category-Based Infidelity Bounded Queries over Unstructured Data Streams
#@ Manish Bhide;Krithi Ramamritham
#t 2013
#c 7
#! We present the Caicos system that supports continuous infidelity bounded queries over a data stream, where each data item (of the stream) belongs to multiple categories. Caicos is made up of four primitives: Keywords, Queries, Data items, and Categories. A Category is a virtual entity consisting of all those data items that belong to it. The membership of a data item to a category is decided by evaluating a Boolean predicate (associated with each category) over the data item. Each data item and query in turn are associated with multiple keywords. Given a keyword query, unlike conventional unstructured data querying techniques that return the top-$(K)$ documents, Caicos returns the top-$(K)$ categories with infidelity less than the user specified infidelity bound. Caicos is designed to continuously track the evolving information present in a highly dynamic data stream. It, hence, computes the relevance of a category to the continuous keyword query using the data items occurring in the stream in the recent past (i.e., within the current "window"). To efficiently provide up-to-date answers to the continuous queries, Caicos needs to maintain the required metadata accurately. This requires addressing two subproblems: 1) Identifying the "right" metadata that needs to be updated for providing accurate results and 2) updating the metadata in an efficient manner. We show that the problem of identifying the right metadata can be further broken down into two subparts. We model the first subpart as an inequality constrained minimization problem and propose an innovative iterative algorithm for the same. The second subpart requires us to build an efficient dynamic programming-based algorithm, which helps us to find the right metadata that needs to be updated. Updating the metadata on multiple processors is a scheduling problem whose complexity is exponential in the length of the input. An approximate multiprocessor scheduling algorithm is, hence, proposed. Experimental evaluation of Caicos using real-world dynamic data shows that Caicos is able to provide fidelity close to 100 percent using 45 percent less resources than the techniques proposed in the literature. This ability of Caicos to work accurately and efficiently even in scenarios with high data arrival rates makes it suitable for data intensive application domains.

#index 2044212
#* Dealing with Uncertainty: A Survey of Theories and Practices
#@ Yiping Li;Jianwen Chen;Ling Feng
#t 2013
#c 7
#! Uncertainty accompanies our life processes and covers almost all fields of scientific studies. Two general categories of uncertainty, namely, aleatory uncertainty and epistemic uncertainty, exist in the world. While aleatory uncertainty refers to the inherent randomness in nature, derived from natural variability of the physical world (e.g., random show of a flipped coin), epistemic uncertainty origins from human's lack of knowledge of the physical world, as well as ability of measuring and modeling the physical world (e.g., computation of the distance between two cities). Different kinds of uncertainty call for different handling methods. Aggarwal, Yu, Sarma, and Zhang et al. have made good surveys on uncertain database management based on the probability theory. This paper reviews multidisciplinary uncertainty processing activities in diverse fields. Beyond the dominant probability theory and fuzzy theory, we also review information-gap theory and recently derived uncertainty theory. Practices of these uncertainty handling theories in the domains of economics, engineering, ecology, and information sciences are also described. It is our hope that this study could provide insights to the database community on how uncertainty is managed in other disciplines, and further challenge and inspire database researchers to develop more advanced data management techniques and tools to cope with a variety of uncertainty issues in the real world.

#index 2044213
#* Distributed Autonomous Online Learning: Regrets and Intrinsic Privacy-Preserving Properties
#@ Feng Yan;Shreyas Sundaram;S. V.  N. Vishwanathan;Yaun Qi
#t 2013
#c 7
#! Online learning has become increasingly popular on handling massive data. The sequential nature of online learning, however, requires a centralized learner to store data and update parameters. In this paper, we consider online learning with distributed data sources. The autonomous learners update local parameters based on local data sources and periodically exchange information with a small subset of neighbors in a communication network. We derive the regret bound for strongly convex functions that generalizes the work by Ram et al. for convex functions. More importantly, we show that our algorithm has intrinsic privacy-preserving properties, and we prove the sufficient and necessary conditions for privacy preservation in the network. These conditions imply that for networks with greater-than-one connectivity, a malicious learner cannot reconstruct the subgradients (and sensitive raw data) of other learners, which makes our algorithm appealing in privacy-sensitive applications.

#index 2044214
#* Efficient Cluster Labeling for Support Vector Clustering
#@ Vincent D'Orangeville;Andre Mayers;Ernest Monga;Shengrui Wang
#t 2013
#c 7
#! We propose a new efficient algorithm for solving the cluster labeling problem in support vector clustering (SVC). The proposed algorithm analyzes the topology of the function describing the SVC cluster contours and explores interconnection paths between critical points separating distinct cluster contours. This process allows distinguishing disjoint clusters and associating each point to its respective one. The proposed algorithm implements a new fast method for detecting and classifying critical points while analyzing the interconnection patterns between them. Experiments indicate that the proposed algorithm significantly improves the accuracy of the SVC labeling process in the presence of clusters of complex shape, while reducing the processing time required by existing SVC labeling algorithms by orders of magnitude.

#index 2044215
#* Efficient Index-Based Approaches for Skyline Queries in Location-Based Applications
#@ Ken C. K. Lee;Baihua Zheng;Cindy Chen;Chi-Yin Chow
#t 2013
#c 7
#! Enriching many location-based applications, various new skyline queries are proposed and formulated based on the notion of locational dominance, which extends conventional one by taking objects' nearness to query positions into account additional to objects' nonspatial attributes. To answer a representative class of skyline queries for location-based applications efficiently, this paper presents two index-based approaches, namely, augmented R-tree and dominance diagram. Augmented R-tree extends R-tree by including aggregated nonspatial attributes in index nodes to enable dominance checks during index traversal. Dominance diagram is a solution-based approach, by which each object is associated with a precomputed nondominance scope wherein query points should have the corresponding object not locationally dominated by any other. Dominance diagram enables skyline queries to be evaluated via parallel and independent comparisons between nondominance scopes and query points, providing very high search efficiency. The performance of these two approaches is evaluated via empirical studies, in comparison with other possible approaches.

#index 2044216
#* Efficient Skyline Computation on Big Data
#@ Xixian Han;Jianzhong Li;Donghua Yang;Jinbao Wang
#t 2013
#c 7
#! Skyline is an important operation in many applications to return a set of interesting points from a potentially huge data space. Given a table, the operation finds all tuples that are not dominated by any other tuples. It is found that the existing algorithms cannot process skyline on big data efficiently. This paper presents a novel skyline algorithm SSPL on big data. SSPL utilizes sorted positional index lists which require low space overhead to reduce I/O cost significantly. The sorted positional index list $(L_j)$ is constructed for each attribute $(A_j)$ and is arranged in ascending order of $(A_j)$. SSPL consists of two phases. In phase 1, SSPL computes scan depth of the involved sorted positional index lists. During retrieving the lists in a round-robin fashion, SSPL performs pruning on any candidate positional index to discard the candidate whose corresponding tuple is not skyline result. Phase 1 ends when there is a candidate positional index seen in all of the involved lists. In phase 2, SSPL exploits the obtained candidate positional indexes to get skyline results by a selective and sequential scan on the table. The experimental results on synthetic and real data sets show that SSPL has a significant advantage over the existing skyline algorithms.

#index 2044217
#* Incremental Maintenance of the Minimum Bisimulation of Cyclic Graphs
#@ Byron Choi;Jintian Deng;Jianliang Xu;Haibo Hu;Sourav S. Bhowmick
#t 2013
#c 7
#! There have been numerous recent applications of graph databases (e.g., the Semantic Web, ontology representation, social networks, XML, chemical databases, and biological databases). A fundamental structural index for data graphs, namely minimum bisimulation, has been reported useful for efficient path query processing and optimization including selectivity estimation, among many others. Data graphs are subject to change and their indexes are updated accordingly. This paper studies the incremental maintenance problem of the minimum bisimulation of a possibly cyclic data graph. While cyclic graphs are ubiquitous among the data on the web, previous work on the maintenance problem has mostly focused on acyclic graphs. To study the problem with cyclic graphs, we first show that the two existing classes of minimization algorithms—merging algorithm and partition refinement—have their strengths and weaknesses. Second, we propose a novel hybrid algorithm and its analytical model. This algorithm supports an edge insertion or deletion and two forms of batch insertions or deletions. To the best of our knowledge, this is the first maintenance algorithm that guarantees minimum bisimulation of cyclic graphs. Third, we propose to partially reuse the minimum bisimulation before an update in order to optimize maintenance performance. We present an experimental study on both synthetic and real-data graphs that verified the efficiency and effectiveness of our algorithms.

#index 2044218
#* Large-Scale Personalized Human Activity Recognition Using Online Multitask Learning
#@ Xu Sun;Hisashi Kashima;Naonori Ueda
#t 2013
#c 7
#! Personalized activity recognition usually has the problem of highly biased activity patterns among different tasks/persons. Traditional methods face problems on dealing with those conflicted activity patterns. We try to effectively model the activity patterns among different persons via casting this personalized activity recognition problem as a multitask learning issue. We propose a novel online multitask learning method for large-scale personalized activity recognition. In contrast with existing work of multitask learning that assumes fixed task relationships, our method can automatically discover task relationships from real-world data. Convergence analysis shows reasonable convergence properties of the proposed method. Experiments on two different activity data sets demonstrate that the proposed method significantly outperforms existing methods in activity recognition.

#index 2044219
#* Multidimensional Sequence Classification Based on Fuzzy Distances and Discriminant Analysis
#@ Alexandros Iosifidis;Anastasios Tefas;Ioannis Pitas
#t 2013
#c 7
#! In this paper, we present a novel method aiming at multidimensional sequence classification. We propose a novel sequence representation, based on its fuzzy distances from optimal representative signal instances, called statemes. We also propose a novel modified clustering discriminant analysis algorithm minimizing the adopted criterion with respect to both the data projection matrix and the class representation, leading to the optimal discriminant sequence class representation in a low-dimensional space, respectively. Based on this representation, simple classification algorithms, such as the nearest subclass centroid, provide high classification accuracy. A three step iterative optimization procedure for choosing statemes, optimal discriminant subspace and optimal sequence class representation in the final decision space is proposed. The classification procedure is fast and accurate. The proposed method has been tested on a wide variety of multidimensional sequence classification problems, including handwritten character recognition, time series classification and human activity recognition, providing very satisfactory classification results.

#index 2044220
#* On Nonparametric Ordinal Classification with Monotonicity Constraints
#@ Wojciech Kotlowski;Roman Slowinski
#t 2013
#c 7
#! We consider the problem of ordinal classification with monotonicity constraints. It differs from usual classification by handling background knowledge about ordered classes, ordered domains of attributes, and about a monotonic relationship between an evaluation of an object on the attributes and its class assignment. In other words, the class label (output variable) should not decrease when attribute values (input variables) increase. Although this problem is of great practical importance, it has received relatively low attention in machine learning. Among existing approaches to learning with monotonicity constraints, the most general is the nonparametric approach, where no other assumption is made apart from the monotonicity constraints assumption. The main contribution of this paper is the analysis of the nonparametric approach from statistical point of view. To this end, we first provide a statistical framework for classification with monotonicity constraints. Then, we focus on learning in the nonparametric setting, and we consider two approaches: the "plug-in" method (classification by estimating first the class conditional distribution) and the direct method (classification by minimization of the empirical risk). We show that these two methods are very closely related. We also perform a thorough theoretical analysis of their statistical and computational properties, confirmed in a computational experiment.

#index 2044221
#* Practical Ensemble Classification Error Bounds for Different Operating Points
#@ Kush R. Varshney;Ryan J. Prenger;Tracy L. Marlatt;Barry Y. Chen;William G. Hanley
#t 2013
#c 7
#! Classification algorithms used to support the decisions of human analysts are often used in settings in which zero-one loss is not the appropriate indication of performance. The zero-one loss corresponds to the operating point with equal costs for false alarms and missed detections, and no option for the classifier to leave uncertain test samples unlabeled. A generalization bound for ensemble classification at the standard operating point has been developed based on two interpretable properties of the ensemble: strength and correlation, using the Chebyshev inequality. Such generalization bounds for other operating points have not been developed previously and are developed in this paper. Significantly, the bounds are empirically shown to have much practical utility in determining optimal parameters for classification with a reject option, classification for ultralow probability of false alarm, and classification for ultralow probability of missed detection. Counter to the usual guideline of large strength and small correlation in the ensemble, different guidelines are recommended by the derived bounds in the ultralow false alarm and missed detection probability regimes.

#index 2044222
#* Privacy Preserving Policy-Based Content Sharing in Public Clouds
#@ Mohamed Nabeel;Ning Shang;Elisa Bertino
#t 2013
#c 7
#! An important problem in public clouds is how to selectively share documents based on fine-grained attribute-based access control policies (acps). An approach is to encrypt documents satisfying different policies with different keys using a public key cryptosystem such as attribute-based encryption, and/or proxy re-encryption. However, such an approach has some weaknesses: it cannot efficiently handle adding/revoking users or identity attributes, and policy changes; it requires to keep multiple encrypted copies of the same documents; it incurs high computational costs. A direct application of a symmetric key cryptosystem, where users are grouped based on the policies they satisfy and unique keys are assigned to each group, also has similar weaknesses. We observe that, without utilizing public key cryptography and by allowing users to dynamically derive the symmetric keys at the time of decryption, one can address the above weaknesses. Based on this idea, we formalize a new key management scheme, called broadcast group key management (BGKM), and then give a secure construction of a BGKM scheme called ACV-BGKM. The idea is to give some secrets to users based on the identity attributes they have and later allow them to derive actual symmetric keys based on their secrets and some public information. A key advantage of the BGKM scheme is that adding users/revoking users or updating acps can be performed efficiently by updating only some public information. Using our BGKM construct, we propose an efficient approach for fine-grained encryption-based access control for documents stored in an untrusted cloud file storage.

#index 2044223
#* Profiling Moving Objects by Dividing and Clustering Trajectories Spatiotemporally
#@ Huey-ru Wu;Mi-Yen Yeh;Ming-Syan Chen
#t 2013
#c 7
#! An object can move with various speeds and arbitrarily changing directions. Given a bounded area where a set of objects moving around, there are some typical moving styles of the objects at different local regions due to the geography nature or other spatiotemporal conditions. Not only the paths that the objects move along, we also want to know how different groups of objects move with various speeds. Therefore, given a set of collected trajectories spreading in a bounded area, we are interested in discovering the typical moving styles in different regions of all the monitored moving objects. These regional typical moving styles are regarded as the profile of the monitored moving objects, which may help reflect the geoinformation of the observed area and the moving behaviors of the observed moving objects. In this paper, we present DivCluST, an approach to finding regional typical moving styles by dividing and clustering the trajectories in consideration of both the spatial and temporal constraints. Different from the existing works that consider only the spatial properties or just the interesting regions of trajectories, DivCluST focuses more on typical movements in local regions of a bounded area and takes the temporal information into account when designing the criteria for trajectory dividing and the distance measurement for adaptive $(k)$-means clustering. Extensive experiments on three types of real data sets with specially designed visualization are presented to show the effectiveness of DivCluST.

#index 2044224
#* The BoND-Tree: An Efficient Indexing Method for Box Queries in Nonordered Discrete Data Spaces
#@ Changqing Chen;Alok Watve;Sakti Pramanik;Qiang Zhu
#t 2013
#c 7
#! Box queries (or window queries) are a type of query which specifies a set of allowed values in each dimension. Indexing feature vectors in the multidimensional Nonordered Discrete Data Spaces (NDDS) for efficient box queries are becoming increasingly important in many application domains such as genome sequence databases. Most of the existing work in this field targets the similarity queries (range queries and k-NN queries). Box queries, however, are fundamentally different from similarity queries. Hence, the same indexing schemes designed for similarity queries may not be efficient for box queries. In this paper, we present a new indexing structure specifically designed for box queries in the NDDS. Unique characteristics of the NDDS are exploited to develop new node splitting heuristics. For the BoND-tree, we also provide theoretical analysis to show the optimality of the proposed heuristics. Extensive experiments with synthetic data demonstrate that the proposed scheme is significantly more efficient than the existing ones when applied to support box queries in NDDSs. We also show effectiveness of the proposed scheme in a real-world application of primer design for genome sequence databases.

#index 2044225
#* Toward Efficient Filter Privacy-Aware Content-Based Pub/Sub Systems
#@ Weixiong Rao;Lei Chen;Sasu Tarkoma
#t 2013
#c 7
#! In recent years, the content-based publish/subscribe , has become a popular paradigm to decouple information producers and consumers with the help of brokers. Unfortunately, when users register their personal interests to the brokers, the privacy pertaining to filters defined by honest subscribers could be easily exposed by untrusted brokers, and this situation is further aggravated by the collusion attack between untrusted brokers and compromised subscribers. To protect the filter privacy, we introduce an anonymizer engine to separate the roles of brokers into two parts, and adapt the $(k)$-anonymity and $(\ell)$-diversity models to the content-based pub/sub. When the anonymization model is applied to protect the filter privacy, there is an inherent tradeoff between the anonymization level and the publication redundancy. By leveraging partial-order-based generalization of filters to track filters satisfying $(k)$-anonymity and $(\ell)$-diversity, we design algorithms to minimize the publication redundancy. Our experiments show the proposed scheme, when compared with studied counterparts, has smaller forwarding cost while achieving comparable attack resilience.

#index 2044226
#* Bias Correction in a Small Sample from Big Data
#@ Jianguo Lu;Dingding Li
#t 2013
#c 7
#! This paper discusses the bias problem when estimating the population size of big data such as online social networks (OSN) using uniform random sampling and simple random walk. Unlike the traditional estimation problem where the sample size is not very small relative to the data size, in big data, a small sample relative to the data size is already very large and costly to obtain. We point out that when small samples are used, there is a bias that is no longer negligible. This paper shows analytically that the relative bias can be approximated by the reciprocal of the number of collisions; thereby, a bias correction estimator is introduced. The result is further supported by both simulation studies and the real Twitter network that contains 41.7 million nodes.

#index 2044227
#* A Blocking Framework for Entity Resolution in Highly Heterogeneous Information Spaces
#@ George Papadakis;Ekaterini Ioannou;Themis Palpanas;Claudia Niederee;Wolfgang Nejdl
#t 2013
#c 7
#! In the context of entity resolution (ER) in highly heterogeneous, noisy, user-generated entity collections, practically all block building methods employ redundancy to achieve high effectiveness. This practice, however, results in a high number of pairwise comparisons, with a negative impact on efficiency. Existing block processing strategies aim at discarding unnecessary comparisons at no cost in effectiveness. In this paper, we systemize blocking methods for clean-clean ER (an inherently quadratic task) over highly heterogeneous information spaces (HHIS) through a novel framework that consists of two orthogonal layers: the effectiveness layer encompasses methods for building overlapping blocks with small likelihood of missed matches; the efficiency layer comprises a rich variety of techniques that significantly restrict the required number of pairwise comparisons, having a controllable impact on the number of detected duplicates. We map to our framework all relevant existing methods for creating and processing blocks in the context of HHIS, and additionally propose two novel techniques: attribute clustering blocking and comparison scheduling. We evaluate the performance of each layer and method on two large-scale, real-world data sets and validate the excellent balance between efficiency and effectiveness that they achieve.

#index 2044228
#* Effective Online Group Discovery in Trajectory Databases
#@ Xiaohui Li;Vaida Ceikute;Christian S. Jensen;Kian-Lee Tan
#t 2013
#c 7
#! GPS-enabled devices are pervasive nowadays. Finding movement patterns in trajectory data stream is gaining in importance. We propose a group discovery framework that aims to efficiently support the online discovery of moving objects that travel together. The framework adopts a sampling-independent approach that makes no assumptions about when positions are sampled, gives no special importance to sampling points, and naturally supports the use of approximate trajectories. The framework's algorithms exploit state-of-the-art, density-based clustering (DBScan) to identify groups. The groups are scored based on their cardinality and duration, and the top-$(k)$ groups are returned. To avoid returning similar subgroups in a result, notions of domination and similarity are introduced that enable the pruning of low-interest groups. Empirical studies on real and synthetic data sets offer insight into the effectiveness and efficiency of the proposed framework.

#index 2044229
#* Efficient Keyword Search on Uncertain Graph Data
#@ Ye Yuan;Guoren Wang;Lei Chen;Haixum Wang
#t 2013
#c 7
#! As a popular search mechanism, keyword search has been applied to retrieve useful data in documents, texts, graphs, and even relational databases. However, so far, there is no work on keyword search over uncertain graph data even though the uncertain graphs have been widely used in many real applications, such as modeling road networks, influential detection in social networks, and data analysis on PPI networks. Therefore, in this paper, we study the problem of top-$(k)$ keyword search over uncertain graph data. Following the similar answer definition for keyword search over deterministic graphs, we consider a subtree in the uncertain graph as an answer to a keyword query if 1) it contains all the keywords; 2) it has a high score (defined by users or applications) based on keyword matching; and 3) it has low uncertainty. Keyword search over deterministic graphs is already a hard problem as stated in [1], [2], [3]. Due to the existence of uncertainty, keyword search over uncertain graphs is much harder. Therefore, to improve the search efficiency, we employ a filtering-and-verification strategy based on a probabilistic keyword index, PKIndex. For each keyword, we offline compute path-based top-$(k)$ probabilities, and attach these values to PKIndex in an optimal, compressed way. In the filtering phase, we perform existence, path-based and tree-based probabilistic pruning phases, which filter out most false subtrees. In the verification, we propose a sampling algorithm to verify the candidates. Extensive experimental results demonstrate the effectiveness of the proposed algorithms.

#index 2044230
#* EnBay: A Novel Pattern-Based Bayesian Classifier
#@ Elena Baralis;Luca Cagliero;Paolo Garza
#t 2013
#c 7
#! A promising approach to Bayesian classification is based on exploiting frequent patterns, i.e., patterns that frequently occur in the training data set, to estimate the Bayesian probability. Pattern-based Bayesian classification focuses on building and evaluating reliable probability approximations by exploiting a subset of frequent patterns tailored to a given test case. This paper proposes a novel and effective approach to estimate the Bayesian probability. Differently from previous approaches, the Entropy-based Bayesian classifier, namely EnBay, focuses on selecting the minimal set of long and not overlapped patterns that best complies with a conditional-independence model, based on an entropy-based evaluator. Furthermore, the probability approximation is separately tailored to each class. An extensive experimental evaluation, performed on both real and synthetic data sets, shows that EnBay is significantly more accurate than most state-of-the-art classifiers, Bayesian and not.

#index 2044231
#* Group Location Selection Queries over Uncertain Objects
#@ Chuanfei Xu;Yu Gu;Roger Zimmermann;Shukuan Lin;Ge Yu
#t 2013
#c 7
#! Given a set of spatial objects, facilities can influence the objects located within their influence regions that are represented by circular disks with the same radius $(r)$. Our task is to select the minimum number of locations such that establishing a temporary facility at each selected location would ensure that all the objects are influenced. Aiming to solve this location selection problem, we propose a novel kind of location selection query, called group location selection (GLS) queries. In many real-world applications, every object is usually located within an uncertainty region instead of at an exact point. Due to the uncertainty of the data, GLS processing needs to ensure that the probability of each uncertain object being influenced by one facility is not less than a given threshold $(\tau)$. An analysis of the time cost reveals that it is infeasible to exactly answer GLS queries over uncertain objects in polynomial time. Hence, this paper proposes an approximate query framework for answering queries efficiently while guaranteeing that the results of GLS queries are correct with a bounded probability. The performance of the proposed methods of the framework is demonstrated by theoretical analysis and extensive experiments with both real and synthetic data sets.

#index 2044232
#* Integrated Oversampling for Imbalanced Time Series Classification
#@ Hong Cao;Xiao-Li Li;Yew-Kwong Woon;See-Kiong Ng
#t 2013
#c 7
#! This paper proposes a novel Integrated Oversampling (INOS) method that can handle highly imbalanced time series classification. We introduce an enhanced structure preserving oversampling (ESPO) technique and synergistically combine it with interpolation-based oversampling. ESPO is used to generate a large percentage of the synthetic minority samples based on multivariate Gaussian distribution, by estimating the covariance structure of the minority-class samples and by regularizing the unreliable eigen spectrum. To protect the key original minority samples, we use an interpolation-based technique to oversample a small percentage of synthetic population. By preserving the main covariance structure and intelligently creating protective variances in the trivial eigen dimensions, ESPO effectively expands the synthetic samples into the void area in the data space without being too closely tied with existing minority-class samples. This also addresses a key challenge for applying oversampling for imbalanced time series classification, i.e., maintaining the correlation between consecutive values through preserving the main covariance structure. Extensive experiments based on seven public time series data sets demonstrate that our INOS approach, used with support vector machines (SVM), achieved better performance over existing oversampling methods as well as state-of-the-art methods in time series classification.

#index 2044233
#* Hierarchical Sampling for Multi-Instance Ensemble Learning
#@ Hanning Yuan;Meng Fang;Xingquan Zhu
#t 2013
#c 7
#! In this paper, we propose a Hierarchical Sampling-based Multi-Instance ensemble LEarning (HSMILE) method. Due to the unique multi-instance learning nature, a positive bag contains at least one positive instance whereas samples (instance and sample are interchangeable terms in this paper) in a negative bag are all negative, simply applying bootstrap sampling to individual bags may severely damage a positive bag because a sampled positive bag may not contain any positive sample at all. To solve the problem, we propose to calculate probable positive sample distributions in each positive bag and use the distributions to preserve at least one positive instance in a sampled bag. The hierarchical sampling involves inter- and intrabag sampling to adequately perturb bootstrap sample sets for multi-instance ensemble learning. Theoretical analysis and experiments confirm that HSMILE outperforms existing multi-instance ensemble learning methods.

#index 2044234
#* Transactions Connected Newsletter
#@ 
#t 2013
#c 7

#index 2044235
#* Stay Connected with the IEEE Computer Society
#@ 
#t 2013
#c 7

#index 2044236
#* A Learning Approach to SQL Query Results Ranking Using Skyline and Users' Current Navigational Behavior
#@ Zhiyuan Chen;Tao Li;Yanan Sun
#t 2013
#c 7
#! Users often find that their queries against a database return too many answers, many of them irrelevant. A common solution is to rank the query results. The effectiveness of a ranking function depends on how well it captures users' preferences. However, database systems often do not have the complete information about users' preferences and users' preferences are often heterogeneous (i.e., some preferences are static and common to all users while some are dynamic and diverse). Existing solutions do not address these two issues. In this paper, we propose a novel approach to address these shortcomings: 1) it addresses the heterogeneous issue by using skyline to capture users' static and common preferences and using users' current navigational behavior to capture users' dynamic and diverse preferences; 2) it addresses the incompleteness issue by using a machine learning technique to learn a ranking function based on training examples constructed from the above two types of information. Experimental results demonstrate the benefits of our approach.

#index 2044237
#* A Robust, Distortion Minimizing Technique for Watermarking Relational Databases Using Once-for-All Usability Constraints
#@ Muhammad Kamran;Sabah Suhail;Muddassar Farooq
#t 2013
#c 7
#! Ownership protection on relational databases--shared with collaborators (or intended recipients)--demands developing a watermarking scheme that must be able to meet four challenges: 1) it should be robust against different types of attacks that an intruder could launch to corrupt the embedded watermark; 2) it should be able to preserve the knowledge in the databases to make them an effective component of knowledge-aware decision support systems; 3) it should try to strike a balance between the conflicting requirements of database owners, who require soft usability constraints, and database recipients who want tight usability constraints that ensure minimum distortions in the data; and 4) last but not least, it should not require that a database owner defines usability constraints for each type of application and every recipient separately. The major contribution of this paper is a robust and efficient watermarking scheme for relational databases that is able to meet all above-mentioned four challenges. The results of our experiments prove that the proposed scheme achieves 100 percent decoding accuracy even if only one watermarked row is left in the database.

#index 2044238
#* Active Trace Clustering for Improved Process Discovery
#@ Xiangyu Tang;Jie Zhou;Jan J. Vanthienen;Bart Baesens
#t 2013
#c 7
#! Process discovery is the learning task that entails the construction of process models from event logs of information systems. Typically, these event logs are large data sets that contain the process executions by registering what activity has taken place at a certain moment in time. By far the most arduous challenge for process discovery algorithms consists of tackling the problem of accurate and comprehensible knowledge discovery from highly flexible environments. Event logs from such flexible systems often contain a large variety of process executions which makes the application of process mining most interesting. However, simply applying existing process discovery techniques will often yield highly incomprehensible process models because of their inaccuracy and complexity. With respect to resolving this problem, trace clustering is one very interesting approach since it allows to split up an existing event log so as to facilitate the knowledge discovery process. In this paper, we propose a novel trace clustering technique that significantly differs from previous approaches. Above all, it starts from the observation that currently available techniques suffer from a large divergence between the clustering bias and the evaluation bias. By employing an active learning inspired approach, this bias divergence is solved. In an assessment using four complex, real-life event logs, it is shown that our technique significantly outperforms currently available trace clustering techniques.

#index 2044239
#* Bridging Causal Relevance and Pattern Discriminability: Mining Emerging Patterns from High-Dimensional Data
#@ Kui Yu;Wei Ding;Hao Wang;Xindong Wu
#t 2013
#c 7
#! It is a nontrivial task to build an accurate emerging pattern (EP) classifier from high-dimensional data because we inevitably face two challenges 1) how to efficiently extract a minimal set of strongly predictive EPs from an explosive number of candidate patterns, and 2) how to handle the highly sensitive choice of the minimal support threshold. To address these two challenges, we bridge causal relevance and EP discriminability (the predictive ability of emerging patterns) to facilitate EP mining and propose a new framework of mining EPs from high-dimensional data. In this framework, we study the relationships between causal relevance in a causal Bayesian network and EP discriminability in EP mining, and then reduce the pattern space of EP mining to direct causes and direct effects, or the Markov blanket (MB) of the class attribute in a causal Bayesian network. The proposed framework is instantiated by two EPs-based classifiers, CE-EP and MB-EP, where CE stands for direct Causes and direct Effects, and MB for Markov Blanket. Extensive experiments on a broad range of data sets validate the effectiveness of the CE-EP and MB-EP classifiers against other well-established methods, in terms of predictive accuracy, pattern numbers, running time, and sensitivity analysis.

#index 2044240
#* Disputant Relation-Based Classification for Contrasting Opposing Views of Contentious News Issues
#@ Souneil Park;Jungil Kim;Kyung Soon Lee;Junehwa Song
#t 2013
#c 7
#! Contentious news issues, such as the health care reform debate, draw much interest from the public; however, it is not simple for an ordinary user to search and contrast the opposing arguments and have a comprehensive understanding of the issues. Providing a classified view of the opposing views of the issues can help readers easily understand the issue from multiple perspectives. We present a disputant relation-based method for classifying news articles on contentious issues. We observe that the disputants of a contention are an important feature for understanding the discourse. It performs unsupervised classification on news articles based on disputant relations, and helps readers intuitively view the articles through the opponent-based frame and attain balanced understanding, free from a specific biased viewpoint. The method is performed in three stages: disputant extraction, disputant partitioning, and article classification. We apply a modified version of HITS algorithm and an SVM classifier trained with pseudorelevant data for article analysis. We conduct an accuracy analysis and an upper-bound analysis for the evaluation of the method.

#index 2044241
#* Measuring Similarity Based on Link Information: A Comparative Study
#@ Hongyan Liu;Jun He;Dan Zhu;Charles X. Ling;Xiaoyong Du
#t 2013
#c 7
#! Measuring similarity between objects is a fundamental task in domains such as data mining, information retrieval, and so on. Link-based similarity measures have attracted the attention of many researchers and have been widely applied in recent years. However, most previous works mainly focus on introducing new link-based measures, and seldom provide theoretical as well as experimental comparisons with other measures. Thus, selecting the suitable measure in different situations and applications is difficult. In this paper, a comprehensive analysis and critical comparison of various link-based similarity measures and algorithms are presented. Their strengths and weaknesses are discussed. Their actual runtime performances are also compared via experiments on benchmark data sets. Some novel and useful guidelines for users to choose the appropriate link-based measure for their applications are discovered.

#index 2044242
#* On-Demand Snapshot: An Efficient Versioning File System for Phase-Change Memory
#@ Eunji Lee;Jee E. Jang;Taeseok Kim;Hyokyung Bahn
#t 2013
#c 7
#! Versioning file systems are widely used in modern computer systems as they provide system recovery and old data access functions by retaining previous file system snapshots. However, existing versioning file systems do not perform well with the emerging PCM (phase-change memory) storage, because they are optimized for hard disks. Specifically, a large amount of additional writes incurred by maintaining snapshot degrades the performance of PCM seriously as write operations are the performance bottleneck of PCM. This paper presents a novel versioning file system, designed for PCM, that reduces the writing overhead of a snapshot significantly. Unlike existing versioning file systems that incur cascade writes up to the file system root, our scheme breaks the recursive update chain at the immediate parent level. The proposed file system is implemented on Linux 2.6 as a prototype. Measurement studies with various I/O benchmarks show that the proposed file system improves the I/O throughput by 144 percent on average, compared to ZFS, a representative versioning file system.

#index 2044243
#* Online Seizure Prediction Using an Adaptive Learning Approach
#@ Shouyi Wang;Wanpracha Art Chaovalitwongse;Stephen Wong
#t 2013
#c 7
#! Epilepsy is one of the most common neurological disorders, characterized by recurrent seizures. Being able to predict impending seizures could greatly improve the lives of patients with epilepsy. In this study, we propose a new adaptive learning approach for online seizure prediction based on analysis of electroencephalogram (EEG) recordings. For each individual patient, we construct baseline patterns of normal and preseizure EEG samples, continuously monitor sliding windows of EEG recordings, and classify each window to normal or preseizure using a $(K)$-nearest-neighbor (KNN) method. A new reinforcement learning algorithm is proposed to continuously update both normal and preseizure baseline patterns based on the feedback from prediction result of each window. The proposed approach was evaluated on EEG data from 10 patients with epilepsy. For each one of the 10 patients, the adaptive approach was trained using the recordings containing the first half of seizure occurrences, and tested prospectively on the subsequent recordings. Using a 150-minute prediction horizon, our approach achieved 73 percent sensitivity and 67 percent specificity on average over 10 patients. This result is shown to be far better than those of a nonupdate prediction scheme and two native prediction schemes.

#index 2044244
#* Signature-Based Detection of Notable Transitions in Numeric Data Streams
#@ Andrii Cherniak;Vladimir Zadorozhny
#t 2013
#c 7
#! A major challenge in large-scale process monitoring is to recognize significant transitions in the process conditions and to distinguish them from random fluctuations that do not produce a notable change in the process dynamics. Such transitions should be recognized at the early stages of their development using a minimal "snapshot" of the observable process log. We developed a novel approach to detect notable transitions based on analysis of coherent behavior of frequency components in the process log (coherency portraits). We have found that notable transitions in the process dynamics are characterized by unique coherency portraits, which are also invariant with respect to random process fluctuations. Our experimental study demonstrates significant efficiency of our approach as compared to traditional change detection techniques.

#index 2044245
#* Valid-Time Indeterminacy in Temporal Relational Databases: Semantics and Representations
#@ Luca Anselma;Paolo Terenziani;Richard T. Snodgrass
#t 2013
#c 7
#! Valid-time indeterminacy is "don't know when" indeterminacy, coping with cases in which one does not exactly know when a fact holds in the modeled reality. In this paper, we first propose a reference representation (data model and algebra) in which all possible temporal scenarios induced by valid-time indeterminacy can be extensionally modeled. We then specify a family of 16 more compact representational data models. We demonstrate their correctness with respect to the reference representation and analyze several properties, including their data expressiveness. Then, we compare these compact models along several relevant dimensions. Finally, we also extend the reference representation and a representative of compact representations to cope with probabilities.

#index 2044246
#* Dynamic Personalized Recommendation on Sparse Data
#@ Xiangyu Tang;Jie Zhou
#t 2013
#c 7
#! Recommendation techniques are very important in the fields of E-commerce and other web-based services. One of the main difficulties is dynamically providing high-quality recommendation on sparse data. In this paper, a novel dynamic personalized recommendation algorithm is proposed, in which information contained in both ratings and profile contents are utilized by exploring latent relations between ratings, a set of dynamic features are designed to describe user preferences in multiple phases, and finally, a recommendation is made by adaptively weighting the features. Experimental results on public data sets show that the proposed algorithm has satisfying performance.

#index 2061129
#* A Group Incremental Approach to Feature Selection Applying Rough Set Technique
#@ Jiye Liang;Feng Wang;Chuangyin Dang;Yuhua Qian
#t 2014
#c 7
#! Many real data increase dynamically in size. This phenomenon occurs in several fields including economics, population studies, and medical research. As an effective and efficient mechanism to deal with such data, incremental technique has been proposed in the literature and attracted much attention, which stimulates the result in this paper. When a group of objects are added to a decision table, we first introduce incremental mechanisms for three representative information entropies and then develop a group incremental rough feature selection algorithm based on information entropy. When multiple objects are added to a decision table, the algorithm aims to find the new feature subset in a much shorter time. Experiments have been carried out on eight UCI data sets and the experimental results show that the algorithm is effective and efficient.

#index 2061130
#* Survey: Functional Module Detection from Protein-Protein Interaction Networks
#@ Junzhong Ji;Aidong Zhang;Chunnian Liu;Xiaomei Quan;Zhijun Liu
#t 2014
#c 7
#! A protein-protein interaction (PPI) network is a biomolecule relationship network that plays an important role in biological activities. Studies of functional modules in a PPI network contribute greatly to the understanding of biological mechanism. With the development of life science and computing science, a great amount of PPI data has been acquired by various experimental and computational approaches, which presents a significant challenge of detecting functional modules in a PPI network. To address this challenge, many functional module detecting methods have been developed. In this survey, we first analyze the existing problems in detecting functional modules and discuss the countermeasures in the data preprocess and postprocess. Second, we introduce some special metrics for distance or graph developed in clustering process of proteins. Third, we give a classification system of functional module detecting methods and describe some existing detection methods in each category. Fourth, we list databases in common use and conduct performance comparisons of several typical algorithms by popular measurements. Finally, we present the prospects and references for researchers engaged in analyzing PPI networks.

#index 2061131
#* A Cocktail Approach for Travel Package Recommendation
#@ Qi Liu;Enhong Chen;Hui Xiong;Yong Ge;Zhongmou Li;Xiang Wu
#t 2014
#c 7
#! Recent years have witnessed an increased interest in recommender systems. Despite significant progress in this field, there still remain numerous avenues to explore. Indeed, this paper provides a study of exploiting online travel information for personalized travel package recommendation. A critical challenge along this line is to address the unique characteristics of travel data, which distinguish travel packages from traditional items for recommendation. To that end, in this paper, we first analyze the characteristics of the existing travel packages and develop a tourist-area-season topic (TAST) model. This TAST model can represent travel packages and tourists by different topic distributions, where the topic extraction is conditioned on both the tourists and the intrinsic features (i.e., locations, travel seasons) of the landscapes. Then, based on this topic model representation, we propose a cocktail approach to generate the lists for personalized travel package recommendation. Furthermore, we extend the TAST model to the tourist-relation-area-season topic (TRAST) model for capturing the latent relationships among the tourists in each travel group. Finally, we evaluate the TAST model, the TRAST model, and the cocktail recommendation approach on the real-world travel package data. Experimental results show that the TAST model can effectively capture the unique characteristics of the travel data and the cocktail approach is, thus, much more effective than traditional recommendation techniques for travel package recommendation. Also, by considering tourist relationships, the TRAST model can be used as an effective assessment for travel group formation.

#index 2061132
#* Adaptive Preprocessing for Streaming Data
#@ Indre Zliobaite;Bogdan Gabrys
#t 2014
#c 7
#! Many supervised learning approaches that adapt to changes in data distribution over time (e.g., concept drift) have been developed. The majority of them assume that the data comes already preprocessed or that preprocessing is an integral part of a learning algorithm. In real-application tasks, data that comes from, e.g., sensor readings, is typically noisy, contain missing values, redundant features, and a very large part of model development efforts is devoted to data preprocessing. As data is evolving over time, learning models need to be able to adapt to changes automatically. From a practical perspective, automating a predictor makes little sense if preprocessing requires manual adjustment over time. Nevertheless, adaptation of preprocessing has been largely overlooked in research. In this paper, we introduce and address the problem of adaptive preprocessing. We analyze when and under what circumstances it is beneficial to handle adaptivity of preprocessing and adaptivity of the learning model separately. We present three scenarios where handling adaptive preprocessing separately benefits the final prediction accuracy and illustrate them using computational examples. As a result of our analysis, we construct a prototype approach for combining adaptive preprocessing with adaptive predictor online. Our case study with real sensory data from a production process demonstrates that decoupling the adaptivity of preprocessing and the predictor contributes to improving the prediction accuracy. The developed reference framework and our experimental findings are intended to serve as a starting point in systematic research of adaptive preprocessing mechanisms for adaptive learning with evolving data.

#index 2061133
#* Building Confidential and Efficient Query Services in the Cloud with RASP Data Perturbation
#@ Huiqi Xu;Shumin Guo;Keke Chen
#t 2014
#c 7
#! With the wide deployment of public cloud computing infrastructures, using clouds to host data query services has become an appealing solution for the advantages on scalability and cost-saving. However, some data might be sensitive that the data owner does not want to move to the cloud unless the data confidentiality and query privacy are guaranteed. On the other hand, a secured query service should still provide efficient query processing and significantly reduce the in-house workload to fully realize the benefits of cloud computing. We propose the random space perturbation (RASP) data perturbation method to provide secure and efficient range query and kNN query services for protected data in the cloud. The RASP data perturbation method combines order preserving encryption, dimensionality expansion, random noise injection, and random projection, to provide strong resilience to attacks on the perturbed data and queries. It also preserves multidimensional ranges, which allows existing indexing techniques to be applied to speedup range query processing. The kNN-R algorithm is designed to work with the RASP range query algorithm to process the kNN queries. We have carefully analyzed the attacks on data and queries under a precisely defined threat model and realistic security assumptions. Extensive experiments have been conducted to show the advantages of this approach on efficiency and security.

#index 2061134
#* Facilitating Document Annotation Using Content and Querying Value
#@ Eduardo J. Ruiz;Vagelis Hristidis;Panagiotis G. Ipeirotis
#t 2014
#c 7
#! A large number of organizations today generate and share textual descriptions of their products, services, and actions. Such collections of textual data contain significant amount of structured information, which remains buried in the unstructured text. While information extraction algorithms facilitate the extraction of structured relations, they are often expensive and inaccurate, especially when operating on top of text that does not contain any instances of the targeted structured information. We present a novel alternative approach that facilitates the generation of the structured metadata by identifying documents that are likely to contain information of interest and this information is going to be subsequently useful for querying the database. Our approach relies on the idea that humans are more likely to add the necessary metadata during creation time, if prompted by the interface; or that it is much easier for humans (and/or algorithms) to identify the metadata when such information actually exists in the document, instead of naively prompting users to fill in forms with information that is not available in the document. As a major contribution of this paper, we present algorithms that identify structured attributes that are likely to appear within the document, by jointly utilizing the content of the text and the query workload. Our experimental evaluation shows that our approach generates superior results compared to approaches that rely only on the textual content or only on the query workload, to identify attributes of interest.

#index 2061135
#* HEigen: Spectral Analysis for Billion-Scale Graphs
#@ U Kang;Brendan Meeder;Evangelos Papalexakis;Christos Faloutsos
#t 2014
#c 7
#! Given a graph with billions of nodes and edges, how can we find patterns and anomalies? Are there nodes that participate in too many or too few triangles? Are there close-knit near-cliques? These questions are expensive to answer unless we have the first several eigenvalues and eigenvectors of the graph adjacency matrix. However, eigensolvers suffer from subtle problems (e.g., convergence) for large sparse matrices, let alone for billion-scale ones. We address this problem with the proposed HEigen algorithm, which we carefully design to be accurate, efficient, and able to run on the highly scalable MapReduce (Hadoop) environment. This enables HEigen to handle matrices more than $(1{,}000 \times)$ larger than those which can be analyzed by existing algorithms. We implement HEigen and run it on the M45 cluster, one of the top 50 supercomputers in the world. We report important discoveries about near-cliques and triangles on several real-world graphs, including a snapshot of the Twitter social network (56 Gb, 2 billion edges) and the "YahooWeb" data set, one of the largest publicly available graphs (120 Gb, 1.4 billion nodes, 6.6 billion edges).

#index 2061136
#* Keyword Query Routing
#@ Thanh Tran;Lei Zhang
#t 2014
#c 7
#! Keyword search is an intuitive paradigm for searching linked data sources on the web. We propose to route keywords only to relevant sources to reduce the high cost of processing keyword search queries over all sources. We propose a novel method for computing top-k routing plans based on their potentials to contain results for a given keyword query. We employ a keyword-element relationship summary that compactly represents relationships between keywords and the data elements mentioning them. A multilevel scoring mechanism is proposed for computing the relevance of routing plans based on scores at the level of keywords, data elements, element sets, and subgraphs that connect these elements. Experiments carried out using 150 publicly available sources on the web showed that valid plans (precision@1 of 0.92) that are highly relevant (mean reciprocal rank of 0.89) can be computed in 1 second on average on a single PC. Further, we show routing greatly helps to improve the performance of keyword search, without compromising its result quality.

#index 2061137
#* Learning Conditional Preference Networks from Inconsistent Examples
#@ Juntao Liu;Yi Xiong;Caihua Wu;Zhijun Yao;Wenyu Liu
#t 2014
#c 7
#! The problem of learning conditional preference networks (CP-nets) from a set of examples has received great attention recently. However, because of the randomicity of the users' behaviors and the observation errors, there is always some noise making the examples inconsistent, namely, there exists at least one outcome preferred over itself (by transferring) in examples. Existing CP-nets learning methods cannot handle inconsistent examples. In this work, we introduce the model of learning consistent CP-nets from inconsistent examples and present a method to solve this model. We do not learn the CP-nets directly. Instead, we first learn a preference graph from the inconsistent examples, because dominance testing and consistency testing in preference graphs are easier than those in CP-nets. The problem of learning preference graphs is translated into a 0-1 programming and is solved by the branch-and-bound search. Then, the obtained preference graph is transformed into a CP-net equivalently, which can entail a subset of examples with maximal sum of weight. Examples are given to show that our method can obtain consistent CP-nets over both binary and multivalued variables from inconsistent examples. The proposed method is verified on both simulated data and real data, and it is also compared with existing methods.

#index 2061138
#* Learning the Gain Values and Discount Factors of Discounted Cumulative Gains
#@ Ke Zhou;Hongyuan Zha;Yi Chang;Gui-Rong Xue
#t 2014
#c 7
#! Evaluation metric is an essential and integral part of a ranking system. In the past, several evaluation metrics have been proposed in information retrieval and web search, among them Discounted Cumulative Gain (DCG) has emerged as one that is widely adopted for evaluating the performance of ranking functions used in web search. However, the two sets of parameters, the gain values and discount factors, used in DCG are usually determined in a rather ad-hoc way, and their impacts have not been carefully analyzed. In this paper, we first show that DCG is generally not coherent, i.e., comparing the performance of ranking functions using DCG very much depends on the particular gain values and discount factors used. We then propose a novel methodology that can learn the gain values and discount factors from user preferences over rankings, modeled as a special case of learning linear utility functions. We also discuss how to extend our methods to handle tied preference pairs and how to explore active learning to reduce preference labeling. Numerical simulations illustrate the effectiveness of our proposed methods. Moreover, experiments are also conducted over a side-by-side comparison data set from a commercial search engine to validate the proposed methods on real-world data.

#index 2061139
#* MWMOTE--Majority Weighted Minority Oversampling Technique for Imbalanced Data Set Learning
#@ Sukarna Barua;Md. Monirul Islam;Xin Yao;Kazuyuki Murase
#t 2014
#c 7
#! Imbalanced learning problems contain an unequal distribution of data samples among different classes and pose a challenge to any classifier as it becomes hard to learn the minority class samples. Synthetic oversampling methods address this problem by generating the synthetic minority class samples to balance the distribution between the samples of the majority and minority classes. This paper identifies that most of the existing oversampling methods may generate the wrong synthetic minority samples in some scenarios and make learning tasks harder. To this end, a new method, called Majority Weighted Minority Oversampling TEchnique (MWMOTE), is presented for efficiently handling imbalanced learning problems. MWMOTE first identifies the hard-to-learn informative minority class samples and assigns them weights according to their euclidean distance from the nearest majority class samples. It then generates the synthetic samples from the weighted informative minority class samples using a clustering approach. This is done in such a way that all the generated samples lie inside some minority class cluster. MWMOTE has been evaluated extensively on four artificial and 20 real-world data sets. The simulation results show that our method is better than or comparable with some other existing methods in terms of various assessment metrics, such as geometric mean (G-mean) and area under the receiver operating curve (ROC), usually known as area under curve (AUC).

#index 2061140
#* Probabilistic Sequence Translation-Alignment Model for Time-Series Classification
#@ Minyoung Kim
#t 2014
#c 7
#! We tackle the time-series classification problem using a novel probabilistic model that represents the conditional densities of the observed sequences being time-warped and transformed from an underlying base sequence. We call it probabilistic sequence translation-alignment model (PSTAM) since it aims to capture both feature alignment and mapping between sequences, analogous to translating one language into another in the field of machine translation. To deal with general time-series, we impose the time-monotonicity constraints on the hidden alignment variables in the model parameter space, where by marginalizing them out it allows effective learning of class-specific time-warping and feature transformation simultaneously. Our PSTAM, thus, naturally enjoys the advantages from two typical approaches widely used in sequence classification: 1) benefits from the alignment-based methods that aim to estimate distance measures between non-equal-length sequences via direct comparison of aligned features, and 2) merits of the model-based approaches that can effectively capture the class-specific patterns or trends. Furthermore, the low-dimensional modeling of the latent base sequence naturally provides a way to discover the intrinsic manifold structure possibly retained in the observed data, leading to an unsupervised manifold learning for sequence data. The benefits of the proposed approach are demonstrated on a comprehensive set of evaluations with both synthetic and real-world sequence data sets.

#index 2061141
#* Set Predicates in SQL: Enabling Set-Level Comparisons for Dynamically Formed Groups
#@ Chengkai Li;Bin He;Ning Yan;Muhammad Assad Safiullah
#t 2014
#c 7
#! In data warehousing and OLAP applications, scalar-level predicates in SQL become increasingly inadequate to support a class of operations that require set-level comparison semantics, i.e., comparing a group of tuples with multiple values. Currently, complex SQL queries composed by scalar-level operations are often formed to obtain even very simple set-level semantics. Such queries are not only difficult to write but also challenging for a database engine to optimize, thus can result in costly evaluation. This paper proposes to augment SQL with set predicate, to bring out otherwise obscured set-level semantics. We studied two approaches to processing set predicates--an aggregate function-based approach and a bitmap index-based approach. Moreover, we designed a histogram-based probabilistic method of set predicate selectivity estimation, for optimizing queries with multiple predicates. The experiments verified its accuracy and effectiveness in optimizing queries.

#index 2061142
#* Supporting Privacy Protection in Personalized Web Search
#@ Lidan Shou;He Bai;Ke Chen;Gang Chen
#t 2014
#c 7
#! Personalized web search (PWS) has demonstrated its effectiveness in improving the quality of various search services on the Internet. However, evidences show that users' reluctance to disclose their private information during search has become a major barrier for the wide proliferation of PWS. We study privacy protection in PWS applications that model user preferences as hierarchical user profiles. We propose a PWS framework called UPS that can adaptively generalize profiles by queries while respecting user-specified privacy requirements. Our runtime generalization aims at striking a balance between two predictive metrics that evaluate the utility of personalization and the privacy risk of exposing the generalized profile. We present two greedy algorithms, namely GreedyDP and GreedyIL, for runtime generalization. We also provide an online prediction mechanism for deciding whether personalizing a query is beneficial. Extensive experiments demonstrate the effectiveness of our framework. The experimental results also reveal that GreedyIL significantly outperforms GreedyDP in terms of efficiency.

#index 2061143
#* Uncertain One-Class Learning and Concept Summarization Learning on Uncertain Data Streams
#@ Bo Liu;Yanshan Xiao;Philip S. Yu;Longbing Cao;Yun Zhang;Zhifeng Hao
#t 2014
#c 7
#! This paper presents a novel framework to uncertain one-class learning and concept summarization learning on uncertain data streams. Our proposed framework consists of two parts. First, we put forward uncertain one-class learning to cope with data of uncertainty. We first propose a local kernel-density-based method to generate a bound score for each instance, which refines the location of the corresponding instance, and then construct an uncertain one-class classifier (UOCC) by incorporating the generated bound score into a one-class SVM-based learning phase. Second, we propose a support vectors (SVs)-based clustering technique to summarize the concept of the user from the history chunks by representing the chunk data using support vectors of the uncertain one-class classifier developed on each chunk, and then extend k-mean clustering method to cluster history chunks into clusters so that we can summarize concept from the history chunks. Our proposed framework explicitly addresses the problem of one-class learning and concept summarization learning on uncertain one-class data streams. Extensive experiments on uncertain data streams demonstrate that our proposed uncertain one-class learning method performs better than others, and our concept summarization method can summarize the evolving interests of the user from the history chunks.

#index 2061144
#* XSPath: Navigation on XML Schemas Made Easy
#@ Federico Cavalieri;Giovanna Guerrini;Marco Mesiti
#t 2014
#c 7
#! Schemas are often used to constrain the content and structure of XML documents. They can be quite big and complex and, thus, difficult to be accessed manually. The ability to query a single schema, a collection of schemas or to retrieve schema components that meet certain structural constraints significantly eases schema management and is, thus, useful in many contexts. In this paper, we propose a query language, named XSPath, specifically tailored for XML schema that works on logical graph-based representations of schemas, on which it enables the navigation, and allows the selection of nodes. We also propose XPath/XQuery-based translations that can be exploited for the evaluation of XSPath queries. An extensive evaluation of the usability and efficiency of the proposed approach is finally presented within the $({\rm E}\cal {X} {\rm up})$ system .

#index 2071591
#* A Meta-Top-Down Method for Large-Scale Hierarchical Classification
#@ Xiao-Lin Wang;Hai Zhao;Bao-liang Lu
#t 2014
#c 7
#! Recent large-scale hierarchical classification tasks typically have tens of thousands of classes on which the most widely used approach to multiclass classification--one-versus-rest--becomes intractable due to computational complexity. The top-down methods are usually adopted instead, but they are less accurate because of the so-called error-propagation problem in their classifying phase. To address this problem, this paper proposes a meta-top-down method that employs metaclassification to enhance the normal top-down classifying procedure. The proposed method is first analyzed theoretically on complexity and accuracy, and then applied to five real-world large-scale data sets. The experimental results indicate that the classification accuracy is largely improved, while the increased time costs are smaller than most of the existing approaches.

#index 2071592
#* Automatic Itinerary Planning for Traveling Services
#@ Gang Chen;Sai Wu;Jingbo Zhou;Anthony K. H. Tung
#t 2014
#c 7
#! Creating an efficient and economic trip plan is the most annoying job for a backpack traveler. Although travel agency can provide some predefined itineraries, they are not tailored for each specific customer. Previous efforts address the problem by providing an automatic itinerary planning service, which organizes the points-of-interests (POIs) into a customized itinerary. Because the search space of all possible itineraries is too costly to fully explore, to simplify the complexity, most work assume that user's trip is limited to some important POIs and will complete within one day. To address the above limitation, in this paper, we design a more general itinerary planning service, which generates multiday itineraries for the users. In our service, all POIs are considered and ranked based on the users' preference. The problem of searching the optimal itinerary is a team orienteering problem (TOP), a well-known NP-complete problem. To reduce the processing cost, a two-stage planning scheme is proposed. In its preprocessing stage, single-day itineraries are precomputed via the MapReduce jobs. In its online stage, an approximate search algorithm is used to combine the single day itineraries. In this way, we transfer the TOP problem with no polynomial approximation into another NP-complete problem (set-packing problem) with good approximate algorithms. Experiments on real data sets show that our approach can generate high-quality itineraries efficiently.

#index 2071593
#* Capturing Telic/Atelic Temporal Data Semantics: Generalizing Conventional Conceptual Models
#@ Vijay Khatri;Sudha Ram;Richard T. Snodgrass;Paolo Terenziani
#t 2014
#c 7
#! Time provides context for all our experiences, cognition, and coordinated collective action. Prior research in linguistics, artificial intelligence, and temporal databases suggests the need to differentiate between temporal facts with goal-related semantics (i.e., telic) from those are intrinsically devoid of culmination (i.e., atelic). To differentiate between telic and atelic data semantics in conceptual database design, we propose an annotation-based temporal conceptual model that generalizes the semantics of a conventional conceptual model. Our temporal conceptual design approach involves: 1) capturing "what" semantics using a conventional conceptual model; 2) employing annotations to differentiate between telic and atelic data semantics that help capture "when" semantics; 3) specifying temporal constraints, specifically nonsequenced semantics, in the temporal data dictionary as metadata. Our proposed approach provides a mechanism to represent telic/atelic temporal semantics using temporal annotations. We also show how these semantics can be formally defined using constructs of the conventional conceptual model and axioms in first-order logic. Via what we refer to as the "semantics of composition," i.e., semantics implied by the interaction of annotations, we illustrate the logical consequences of representing telic/atelic data semantics during temporal conceptual design.

#index 2071594
#* Learning Phenotype Structure Using Sequence Model
#@ Yuhai Zhao;Guoren Wang;Xiang Zhang;Jeffrey Xu Yu;Zhanghui Wang
#t 2014
#c 7
#! Advanced microarray technologies have enabled to simultaneously monitor the expression levels of all genes. An important problem in microarray data analysis is to discover phenotype structures. The goal is to 1) find groups of samples corresponding to different phenotypes (such as disease or normal), and 2) for each group of samples, find the representative expression pattern or signature that distinguishes this group from others. Some methods have been proposed for this issue, however, a common drawback is that the identified signatures often include a large number of genes but with low discriminative power. In this paper, we propose a $(g^\ast)$-sequence model to address this limitation, where the ordered expression values among genes are profitably utilized. Compared with the existing methods, the proposed sequence model is more robust to noise and allows to discover the signatures with more discriminative power using fewer genes. This is important for the subsequent analysis by the biologists. We prove that the problem of phenotype structure discovery is NP-complete. An efficient algorithm, FINDER, is developed, which includes three steps: 1) trivial $(g^\ast)$-sequences identifying, 2) phenotype structure discovery, and 3) refinement. Effective pruning strategies are developed to further improve the efficiency. We evaluate the performance of FINDER and the existing methods using both synthetic and real gene expression data sets. Extensive experimental results show that FINDER dramatically improves the accuracy of the phenotype structures discovered (in terms of both statistical and biological significance) and detects signatures with high discriminative power. Moreover, it is orders of magnitude faster than other alternatives.

#index 2071595
#* Classifier Ensembles with the Extended Space Forest
#@ M. Fatih Amasyali;Okan K. Ersoy
#t 2014
#c 7
#! The extended space forest is a new method for decision tree construction in which training is done with input vectors including all the original features and their random combinations. The combinations are generated with a difference operator applied to random pairs of original features. The experimental results show that extended space versions of ensemble algorithms have better performance than the original ensemble algorithms. To investigate the success dynamics of the extended space forest, the individual accuracy and diversity creation powers of ensemble algorithms are compared. The Extended Space Forest creates more diversity when it uses all the input features than Bagging and Rotation Forest. It also results in more individual accuracy when it uses random selection of the features than Random Subspace and Random Forest methods. It needs more training time because of using more features than the original algorithms. But its testing time is lower than the others because it generates less complex base learners.

#index 2071596
#* CoDe Modeling of Graph Composition for Data Warehouse Report Visualization
#@ Michele Risi;Maria Immacolata Sessa;Maurizio Tucci;Genoveffa Tortora
#t 2014
#c 7
#! The visualization of information contained in reports is an important aspect of human-computer interaction, for both the accuracy and the complexity of relationships between data must be preserved. A greater attention has been paid to individual report visualization through different types of standard graphs (Histograms, Pies, etc.). However, this kind of representation provides separate information items and gives no support to visualize their relationships which are extremely important for most decision processes. This paper presents a design methodology exploiting the visual language CoDebased on a logic paradigm. CoDe allows to organize the visualization through the CoDe model which graphically represents relationships between information items and can be considered a conceptual map of the view. The proposed design methodology is composed of four phases: the CoDe Modeling and OLAP Operation pattern definition phases define the CoDe model and underlying metadata information, the OLAP Operation phase physically extracts data from a data warehouse and the Report Visualization phase generates the final visualization. Moreover, a case study on real data is provided.

#index 2071597
#* Discovering the Top-k Unexplained Sequences in Time-Stamped Observation Data
#@ Massimiliano Albanese;Cristian Molinaro;Fabio Persia;Antonio Picariello;V. S. Subrahmanian
#t 2014
#c 7
#! There are numerous applications where we wish to discover unexpected activities in a sequence of time-stamped observation data--for instance, we may want to detect inexplicable events in transactions at a website or in video of an airport tarmac. In this paper, we start with a known set $({\cal A})$ of activities (both innocuous and dangerous) that we wish to monitor. However, in addition, we wish to identify "unexplained" subsequences in an observation sequence that are poorly explained (e.g., because they may contain occurrences of activities that have never been seen or anticipated before, i.e., they are not in $({\cal A})$). We formally define the probability that a sequence of observations is unexplained (totally or partially) w.r.t. $({\cal A})$. We develop efficient algorithms to identify the top-$(k)$ Totally and partially unexplained sequences w.r.t. $({\cal A})$. These algorithms leverage theorems that enable us to speed up the search for totally/partially unexplained sequences. We describe experiments using real-world video and cyber-security data sets showing that our approach works well in practice in terms of both running time and accuracy.

#index 2071598
#* Durable Queries over Historical Time Series
#@ Hao Wang;Yilun Cai;Yin Yang;Shiming Zhang;Nikos Mamoulis
#t 2014
#c 7
#! This paper studies the problem of finding objects with durable quality over time in historical time series databases. For example, a sociologist may be interested in the top 10 web search terms during the period of some historical events; the police may seek for vehicles that move close to a suspect 70 percent of the time during a certain time period and so on. Durable top-$(k)$ (DTop-$(k)$) and nearest neighbor ($({\rm D}k{\rm NN})$) queries can be viewed as natural extensions of the standard snapshot top-$(k)$ and NN queries to timestamped sequences of values or locations. Although their snapshot counterparts have been studied extensively, to our knowledge, there is little prior work that addresses this new class of durable queries. Existing methods for DTop-$(k)$ processing either apply trivial solutions, or rely on domain-specific properties. Motivated by this, we propose efficient and scalable algorithms for the DTop-$(k)$ and $({\rm D}k{\rm NN})$ queries, based on novel indexing and query evaluation techniques. Our experiments show that the proposed algorithms outperform previous and baseline solutions by a wide margin.

#index 2071599
#* Effectively Indexing the Multidimensional Uncertain Objects
#@ Ying Zhang;Wenjie Zhang;Qianlu Lin;Xuemin Lin;Heng Tao Shen
#t 2014
#c 7
#! As the uncertainty is inherent in a wide spectrum of applications such as radio frequency identification (RFID) networks and location-based services (LBS), it is highly demanded to address the uncertainty of the objects. In this paper, we propose a novel indexing structure, named $(U)$-Quadtree, to organize the uncertain objects in the multidimensional space such that the queries can be processed efficiently by taking advantage of $(U)$-Quadtree. Particularly, we focus on the range search on multidimensional uncertain objects since it is a fundamental query in a spatial database. We propose a cost model which carefully considers various factors that may impact the performance. Then, an effective and efficient index construction algorithm is proposed to build the optimal $(U)$-Quadtree regarding the cost model. We show that $(U)$-Quadtree can also efficiently support other types of queries such as uncertain range query and nearest neighbor query. Comprehensive experiments demonstrate that our techniques outperform the existing works on multidimensional uncertain objects.

#index 2071600
#* Identifying Features in Opinion Mining via Intrinsic and Extrinsic Domain Relevance
#@ Zhen Hai;Kuiyu Chang;Jung-Jae Kim;Christopher C. Yang
#t 2014
#c 7
#! The vast majority of existing approaches to opinion feature extraction rely on mining patterns only from a single review corpus, ignoring the nontrivial disparities in word distributional characteristics of opinion features across different corpora. In this paper, we propose a novel method to identify opinion features from online reviews by exploiting the difference in opinion feature statistics across two corpora, one domain-specific corpus (i.e., the given review corpus) and one domain-independent corpus (i.e., the contrasting corpus). We capture this disparity via a measure called domain relevance (DR), which characterizes the relevance of a term to a text collection. We first extract a list of candidate opinion features from the domain review corpus by defining a set of syntactic dependence rules. For each extracted candidate feature, we then estimate its intrinsic-domain relevance (IDR) and extrinsic-domain relevance (EDR) scores on the domain-dependent and domain-independent corpora, respectively. Candidate features that are less generic (EDR score less than a threshold) and more domain-specific (IDR score greater than another threshold) are then confirmed as opinion features. We call this interval thresholding approach the intrinsic and extrinsic domain relevance (IEDR) criterion. Experimental results on two real-world review domains show the proposed IEDR approach to outperform several other well-established methods in identifying opinion features.

#index 2071601
#* Identity Protection in Sequential Releases of Dynamic Networks
#@ Chih-Hua Tai;Peng-Jui Tseng;Philip S. Yu;Ming-Syan Chen
#t 2014
#c 7
#! Social networks model the social activities between individuals, which change as time goes by. In light of useful information from such dynamic networks, there is a continuous demand for privacy-preserving data sharing with analyzers, collaborators or customers. In this paper, we address the privacy risks of identity disclosures in sequential releases of a dynamic network. To prevent privacy breaches, we proposed novel $(k^w)$-structural diversity anonymity, where $(k)$ is an appreciated privacy level and $(w)$ is a time period that an adversary can monitor a victim to collect the attack knowledge. We also present a heuristic algorithm for generating releases satisfying $(k^w)$-structural diversity anonymity so that the adversary cannot utilize his knowledge to reidentify the victim and take advantages. The evaluations on both real and synthetic data sets show that the proposed algorithm can retain much of the characteristics of the networks while confirming the privacy protection.

#index 2071602
#* Knowledge Fusion for Probabilistic Generative Classifiers with Data Mining Applications
#@ Dominik Fisch;Edgar Kalkowski;Bernhard Sick
#t 2014
#c 7
#! If knowledge such as classification rules are extracted from sample data in a distributed way, it may be necessary to combine or fuse these rules. In a conventional approach this would typically be done either by combining the classifiers' outputs (e.g., in form of a classifier ensemble) or by combining the sets of classification rules (e.g., by weighting them individually). In this paper, we introduce a new way of fusing classifiers at the level of parameters of classification rules. This technique is based on the use of probabilistic generative classifiers using multinomial distributions for categorical input dimensions and multivariate normal distributions for the continuous ones. That means, we have distributions such as Dirichlet or normal-Wishart distributions over parameters of the classifier. We refer to these distributions as hyperdistributions or second-order distributions. We show that fusing two (or more) classifiers can be done by multiplying the hyperdistributions of the parameters and derive simple formulas for that task. Properties of this new approach are demonstrated with a few experiments. The main advantage of this fusion approach is that the hyperdistributions are retained throughout the fusion process. Thus, the fused components may, for example, be used in subsequent training steps (online training).

#index 2071603
#* TrustedDB: A Trusted Hardware-Based Database with Privacy and Data Confidentiality
#@ Sumeet Bajaj;Radu Sion
#t 2014
#c 7
#! Traditionally, as soon as confidentiality becomes a concern, data are encrypted before outsourcing to a service provider. Any software-based cryptographic constructs then deployed, for server-side query processing on the encrypted data, inherently limit query expressiveness. Here, we introduce TrustedDB, an outsourced database prototype that allows clients to execute SQL queries with privacy and under regulatory compliance constraints by leveraging server-hosted, tamper-proof trusted hardware in critical query processing stages, thereby removing any limitations on the type of supported queries. Despite the cost overhead and performance limitations of trusted hardware, we show that the costs per query are orders of magnitude lower than any (existing or) potential future software-only mechanisms. TrustedDB is built and runs on actual hardware, and its performance and costs are evaluated here.

#index 2071604
#* OCCT: A One-Class Clustering Tree for Implementing One-to-Many Data Linkage
#@ Ma'ayan Dror;Asaf Shabtai;Lior Rokach;Yuval Elovici
#t 2014
#c 7
#! One-to-many data linkage is an essential task in many domains, yet only a handful of prior publications have addressed this issue. Furthermore, while traditionally data linkage is performed among entities of the same type, it is extremely necessary to develop linkage techniques that link between matching entities of different types as well. In this paper, we propose a new one-to-many data linkage method that links between entities of different natures. The proposed method is based on a one-class clustering tree (OCCT) that characterizes the entities that should be linked together. The tree is built such that it is easy to understand and transform into association rules, i.e., the inner nodes consist only of features describing the first set of entities, while the leaves of the tree represent features of their matching entities from the second data set. We propose four splitting criteria and two different pruning methods which can be used for inducing the OCCT. The method was evaluated using data sets from three different domains. The results affirm the effectiveness of the proposed method and show that the OCCT yields better performance in terms of precision and recall (in most cases it is statistically significant) when compared to a C4.5 decision tree-based linkage method.

#index 2071605
#* Typicality-Based Collaborative Filtering Recommendation
#@ Yi Cai;Ho-fung Leung;Qing Li;Huaqing Min;Jie Tang;Juanzi Li
#t 2014
#c 7
#! Collaborative filtering (CF) is an important and popular technology for recommender systems. However, current CF methods suffer from such problems as data sparsity, recommendation inaccuracy, and big-error in predictions. In this paper, we borrow ideas of object typicality from cognitive psychology and propose a novel typicality-based collaborative filtering recommendation method named TyCo. A distinct feature of typicality-based CF is that it finds "neighbors" of users based on user typicality degrees in user groups (instead of the corated items of users, or common users of items, as in traditional CF). To the best of our knowledge, there has been no prior work on investigating CF recommendation by combining object typicality. TyCo outperforms many CF recommendation methods on recommendation accuracy (in terms of MAE) with an improvement of at least 6.35 percent in Movielens data set, especially with sparse training data (9.89 percent improvement on MAE) and has lower time cost than other CF methods. Further, it can obtain more accurate predictions with less number of big-error predictions.

#index 2071606
#* Online Feature Selection and Its Applications
#@ Jialei Wang;Peilin Zhao;Steven C. H. Hoi;rong jin
#t 2014
#c 7
#! Feature selection is an important technique for data mining. Despite its importance, most studies of feature selection are restricted to batch learning. Unlike traditional batch learning methods, online learning represents a promising family of efficient and scalable machine learning algorithms for large-scale applications. Most existing studies of online learning require accessing all the attributes/features of training instances. Such a classical setting is not always appropriate for real-world applications when data instances are of high dimensionality or it is expensive to acquire the full set of attributes/features. To address this limitation, we investigate the problem of online feature selection (OFS) in which an online learner is only allowed to maintain a classifier involved only a small and fixed number of features. The key challenge of online feature selection is how to make accurate prediction for an instance using a small number of active features. This is in contrast to the classical setup of online learning where all the features can be used for prediction. We attempt to tackle this challenge by studying sparsity regularization and truncation techniques. Specifically, this article addresses two different tasks of online feature selection: 1) learning with full input, where an learner is allowed to access all the features to decide the subset of active features, and 2) learning with partial input, where only a limited number of features is allowed to be accessed for each instance by the learner. We present novel algorithms to solve each of the two problems and give their performance analysis. We evaluate the performance of the proposed algorithms for online feature selection on several public data sets, and demonstrate their applications to real-world problems including image classification in computer vision and microarray gene expression analysis in bioinformatics. The encouraging results of our experiments validate the efficacy and efficiency of the proposed techniques.

#index 2071607
#* The Role of Hubness in Clustering High-Dimensional Data
#@ Nenad Tomasev;Milos Radovanovic;Dunja Mladenic;Mirjana Ivanovic
#t 2014
#c 7
#! High-dimensional data arise naturally in many domains, and have regularly presented a great challenge for traditional data mining techniques, both in terms of effectiveness and efficiency. Clustering becomes difficult due to the increasing sparsity of such data, as well as the increasing difficulty in distinguishing distances between data points. In this paper, we take a novel perspective on the problem of clustering high-dimensional data. Instead of attempting to avoid the curse of dimensionality by observing a lower dimensional feature subspace, we embrace dimensionality by taking advantage of inherently high-dimensional phenomena. More specifically, we show that hubness, i.e., the tendency of high-dimensional data to contain points (hubs) that frequently occur in $(k)$-nearest-neighbor lists of other points, can be successfully exploited in clustering. We validate our hypothesis by demonstrating that hubness is a good measure of point centrality within a high-dimensional data cluster, and by proposing several hubness-based clustering algorithms, showing that major hubs can be used effectively as cluster prototypes or as guides during the search for centroid-based cluster configurations. Experimental results demonstrate good performance of our algorithms in multiple settings, particularly in the presence of large quantities of noise. The proposed methods are tailored mostly for detecting approximately hyperspherical clusters and need to be extended to properly handle clusters of arbitrary shapes.

#index 2071608
#* Robust Perceptual Image Hashing Based on Ring Partition and NMF
#@ Zhenjun Tang;Xianquan Zhang;Shichao Zhang
#t 2014
#c 7
#! This paper designs an efficient image hashing with a ring partition and a nonnegative matrix factorization (NMF), which has both the rotation robustness and good discriminative capability. The key contribution is a novel construction of rotation-invariant secondary image, which is used for the first time in image hashing and helps to make image hash resistant to rotation. In addition, NMF coefficients are approximately linearly changed by content-preserving manipulations, so as to measure hash similarity with correlation coefficient. We conduct experiments for illustrating the efficiency with 346 images. Our experiments show that the proposed hashing is robust against content-preserving operations, such as image rotation, JPEG compression, watermark embedding, Gaussian low-pass filtering, gamma correction, brightness adjustment, contrast adjustment, and image scaling. Receiver operating characteristics (ROC) curve comparisons are also conducted with the state-of-the-art algorithms, and demonstrate that the proposed hashing is much better than all these algorithms in classification performances with respect to robustness and discrimination.

#index 2071609
#* Searching Dimension Incomplete Databases
#@ Wei Cheng;Xiaoming Jin;Jian-Tao Sun;Xuemin Lin;Xiang Zhang;Wei Wang
#t 2014
#c 7
#! Similarity query is a fundamental problem in database, data mining and information retrieval research. Recently, querying incomplete data has attracted extensive attention as it poses new challenges to traditional querying techniques. The existing work on querying incomplete data addresses the problem where the data values on certain dimensions are unknown. However, in many real-life applications, such as data collected by a sensor network in a noisy environment, not only the data values but also the dimension information may be missing. In this work, we propose to investigate the problem of similarity search on dimension incomplete data. A probabilistic framework is developed to model this problem so that the users can find objects in the database that are similar to the query with probability guarantee. Missing dimension information poses great computational challenge, since all possible combinations of missing dimensions need to be examined when evaluating the similarity between the query and the data objects. We develop the lower and upper bounds of the probability that a data object is similar to the query. These bounds enable efficient filtering of irrelevant data objects without explicitly examining all missing dimension combinations. A probability triangle inequality is also employed to further prune the search space and speed up the query process. The proposed probabilistic framework and techniques can be applied to both whole and subsequence queries. Extensive experimental results on real-life data sets demonstrate the effectiveness and efficiency of our approach.

