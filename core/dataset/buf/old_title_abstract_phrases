completeness;categorical attributes;mining association rules;relational tables;real-life;quantitative attributes;interesting rules;quantitative association rules
dimensional space;database;algorithms for computing;high probability;data mining;association rules;numeric attributes;data mining based;association rule
large amounts of;data analysis;business data;database;noisy data;data exploration
search space;optimizer;plans;plan quality;query optimizers;join-order
query optimization;sql queries;sql query;operator;lower cost;query optimizers
access plan;sql queries;business data;decision support applications;functional dependencies;optimization techniques
speech recognition;face recognition;parallel processing;massively parallel;content analysis;general-purpose;computationally intensive;multimedia objects;user-defined functions
large number of;continuous media;continuous media;fault-tolerant;rates;fault-tolerant;data placement;extensive simulations
multimedia objects;optimal plan;multiple types of;selection conditions;general problem;np-hard
clustering;clustering;multi-dimensional;data points;database;clustering method;large databases;large datasets;clustering algorithm;clustering quality
heuristic algorithm;minimum number of
parallel database;performance degradation;large number of;parallel databases;data movement;disk space
optimization technique;mediator systems;query result caching;query processing and optimization;data sources;cost-based query optimization;remote sources;cost-based;execution plans;semantic information;query results
query execution;database systems;query optimization;optimization strategy;performance tradeoffs;object-oriented;performance tradeoffs;wide range;query processing;operator
query processing;database technology;query processors;data stored in;data access;database functionality;schema information;data access
replication;update transactions
clustering;performance degradation;storage space;data blocks;storage management;extensive simulation
database systems;large volumes of data;database applications;access latency;random access;storage systems;databases;disk storage;disk-based;complex relationships
response times;complex queries;commercial systems;data cube;query optimization;decision support;large databases;raw data;algorithm performs;materialized views;optimization technique;data cubes;applications involve;greedy algorithms
complex queries;relational database systems;decision support applications;relational systems;query processing;decision support queries;query processor
database technology;general purpose;database;relational model;higher-level;scientific data;programming languages;query language;scientific computing;complex objects;databases;optimization techniques;data exchange
data locality;total cost
partition function;tree matching;dynamically generated;highly competitive;tree-based;pre-computed;spatial joins;join algorithms;wide range;data item;margin
computational geometry;join algorithm;partition based;database;intermediate results;complex query;join algorithms;real data sets;partition based;spatial join;clustering
high probability;data distribution;algorithm requires;sampling algorithm;size estimation
data structure;query patterns;database management systems;suffix tree;database;bgi;textual data;cost estimation;tree-based;query plan;dat;object-oriented;join order;input text;databases;query optimizers;asw, hasa, iop, lns, sac, wvt;selection predicate
access plan;selectivity estimation;sampling techniques;histogram construction;commercial database systems;query result
database;database applications;programming languages;active databases;database access;object-oriented databases
programming languages;object-oriented database;cache management;highly-reliable;object-oriented databases;persistent storage;distributed environment
database systems;database;semi-structured;databases;storage model;resource management;replication;query processing;storage-management;external data
object-oriented databases;access control;security requirements;access control
goal-oriented;goal-oriented;buffer management;multi-class;resource allocation;hit rate
execution environment;query optimization;multi-dimensional;query execution plans;scheduling algorithm;worst-case;resource sharing;intra-query;multi-dimensional;query plan;instance;heuristic algorithm;operator;parallel database systems
database;object databases;dynamic behavior;performance goals;database application;rates;semi-automatic;fundamental problem;adaptive control;semi-automatic
main memory;object-oriented database;database management systems;free space;simple heuristics;main-memory;space utilization;storage utilization;disk page;worst case;databases;storage management;excellent performance;clustering criteria;database systems
query optimization;rule-based;nested queries;optimizer;rule-based
path expressions;query languages;query optimizer;evaluating queries
query execution;programming languages;object-relational;database;duplicate elimination;instances;object-oriented;optimization problems;caching techniques
search space;optimization technique;optimizer;relational algebra;cost-based optimization;rule-based;database;decision-support queries;operator;cost-based;cost-based query optimizer;optimization process;sql queries
query optimization;integrity constraint;plans;subexpressions;multiple view;local optimization;optimization problem;global optimization;database updates;incremental maintenance;view maintenance;multiple relations;rule-based;optimal set of
constraints imposed by;local database;6;serializability;database
auxiliary;data warehouses;database;view maintenance;materialized views;base tables;update transactions
data sources;auxiliary data;views defined;source databases;incremental updates;data integration
detection problem;configuration management;structured documents;view maintenance;active databases;change detection;minimum-cost;data warehousing;structured information;algorithms for computing;general-purpose;structured data;relational data
great flexibility;relational algebra;database;path expressions;data representation;query language;data model;relational data;query languages;optimization techniques;unstructured data;data integration
materialized views;database applications;database research;view maintenance;complex objects;data centric;object-oriented;query dependent;database;concurrency control;higher order
world wide web;cross-language;database management systems;database;sql query;query language;query forms;end user;relational databases;relational dbms;markup language;general purpose
indexing schemes;indexing techniques;database research
resource sharing;parallel dbms;hardware architecture
clustering;clustering;clustering algorithms;clustering algorithm
query optimization;parallel database systems;query processing in
query processing in;estimation errors;control strategies;optimizer;control mechanism
database technology;database systems;classification;active functionality;database management system;active database
storage manager;database;transaction management
large scale;database systems
database management systems;object-relational;business processes;database management system;dynamic environment;cost-effective;relational dbmss

information filtering;resource usage;filtering systems
camps' italics;in today's sql dbmss;date's book an introduction to [database systems
declarative query language;data structure;efficient storage;database;aggregate data;raw data;data model;obtained by applying;statistical analysis;graphical user interface;user-friendly;database management system
users' preferences;database systems;3, 8, 11;database research;data warehousing;data mining;querying capabilities;information visualization;database instances;information sources;1, 9, 16;working group;instance;2;4;information visualization;data repositories;query languages;10;13;information systems;15;14;database;database applications;data sizes;numerical data;visualization techniques;12
information visualization
query formulation;data structures and algorithms;query interfaces;large databases;databases;database access
object oriented;graphical user interface;object model;data sources;information exploration
object oriented;graphical user interface;object model;data sources
database;query-independent;large databases;data items;wide range;query-dependent;visualization techniques
data set;query language;database
multiple views;world wide web;web browser;database
large networks;geographical information
information systems;database;workshop brought together;information systems;workflow management;distributed systems
performance gains;database;programming language
real customer;database

federated database;schema translation;information systems;database design;spatial information;decision support;problem statement;decision support system;database integration;water quality
context information;data sources;large number of;environmental data
world-wide web;web-based;databases;environmental data;database
information systems;information sources;multimedia documents;service-oriented;multimedia information;easy access
database technology;data management;data analysis;knowledge bases;data management
data stores;data mining;structured information;geographic information systems
instance;san diego;query language;object-oriented;information integration;complex structures;relational databases;semistructured data;working group
digital images;medical image;medical image;medical images;compression techniques;large sets of;medical imaging;information stored in;compression methods;similar images;databases;image compression
schema translation;database technology;data dependencies;object-oriented database;schema translation;object-oriented;relational schema;relational database;object-oriented databases
software development;schema mapping;binary data;database design;relational schemas;object-oriented;relational schema;relational databases
decision support;database technology;database;data warehouse;decision support;on-line analytical processing;commercial products;data analysis;data warehousing;database management system;data warehousing and olap;query processing;multidimensional data;database research;metadata management;transaction processing
information systems;data volume;data complexity;case study;user interface;information systems;environmental data;query refinement
data objects;nearest-neighbor search;high-dimensional feature space;multimedia databases;feature space;search techniques;special properties;large amounts of;high-dimensional spaces;data items;real data;nearest-neighbor algorithm;similarity search;high-dimensional feature spaces;high-dimensional
linear transformations;queries efficiently;time series;exact match;data set;similarity queries;moving average;tree index;similarity-based;time-series;query processing
detection problem;configuration management;detection algorithm;minimum-cost;active databases;change detection;update operations;bipartite graph;structured data;np-hard
multi-dimensional;concurrent updates;data warehousing;query performance
database systems;index pages
data structure;access methods;search tree;index structure;tree structures;tree-based;data records;data types;concurrency control;search trees;generally applicable
tree structures;auxiliary information;data cube;worst-case;range-sum queries;range query;data cubes;range queries;average-case;fast algorithms;range queries;ad hoc queries
data cube;on line analytical processing;data sets;data warehousing;operator;incremental updates
summary tables;large amounts of;large volumes of data;large number of;data warehouses;on-line analytical processing;data sources;data sets;aggregate queries over;data cubes;base tables;aggregate views;data warehousing;decision-support;transactional data
database systems;database;real-world;database size;trade-offs;database vendors;database;increasingly large;buffer size;databases;oltp) systems;hit rate;large database;transaction processing
end users;end users;database;application systems;real world;relational database system;database performance;database systems
object-relational;data types;user-defined;object-relational database;path expressions;relational systems;key features;relational database system;object-relational database systems
active rules;database;base data;derived data;real world;derived data
on-line analytical processing;compression techniques;gblp;olap) applications;operator;algorithm to compute
online aggregation;confidence intervals;database;traditional database systems;online aggregation;large volume;aggregation queries
pull-based;zdon, acha95b;scalability problems;push-based;data dissemination;wireless networks
world wide web;domain ontologies;agent technology;information sources;dynamic environments;web technologies;information management;semantic integration;search engines;data integration;databases;federated database;agent-based
query results;resource discovery;search engines;document collections;digital library
query optimization;query result;relational dbms;sql extension;query plans
logic programming;evaluation strategies;query evaluation;queries involving;relational algebra
memory management;operating systems;large-scale
synthetic data;real data;market basket data;candidate itemsets;market-basket data;large itemsets;low-level
census data;synthetic data;text documents;data mining;pruning strategies;market basket data;association rules;association rules
parallel algorithms;minimum support;database;data mining;mining association rules;candidate set;large space of;discovering association rules;partitioning scheme;user defined;databases;association rules;data distribution
compression ratio;space requirement;real world datasets;aggregate queries;reconstruction error;ad hoc querying;large datasets;very large datasets;ad hoc querying;compressed data;ad hoc queries
data exploration;visual presentation;remote sources;application domains;visualization tool;data records;wide range;large datasets;multimedia objects;user groups;integrate data from;visual exploration of
disk accesses
synthetic data sets;data distributions;query optimizer;spatial join;cost estimation;replication;data sets;relational systems;query processing techniques;spatial join;algorithm to compute
data set;relational database system;geo-spatial;database systems;spatial queries
multi-perspective;multi-user;data cube;multi-dimensional data;multiple views;data warehouses;data sources;decision support applications;materialized views;user interface;data warehousing;conceptual model;databases
high-dimensional;data objects;data points;distance based;distance function;high dimensional;image database;database applications;index structures;partitioning strategy;pre-computed;data items;index structure called;similarity queries;distance-based;distance based;metric spaces;approximate matches
high-dimensional;nearest neighbor;index structure called;large databases;content-based retrieval;index structure;index structures;similarity queries;nearest neighbor queries;queries efficiently;dimensional data;test results;similarity indexing;feature vectors;search efficiency
data mining;web service;queries efficiently;query response time;databases
multi-version;data warehouses;query rewrite;database;view maintenance;materialized views;algorithm called;base data;serializability;concurrency control;base tables;external sources;relational dbms
multiple views;materialized views;view maintenance;trade-offs
data warehouse;data warehouses;view maintenance;multiple data sources;data sources;concurrent updates;autonomous data sources
relational database systems;database systems;application programs
temporal aggregates;data analysis;temporal aggregation;database systems;knowledge discovery;time series;user-defined;management systems;logic-based;active database;complex patterns;temporal aggregation;formally defined;temporal reasoning;event-based;composite event
ordinal data;data points;takes into account;real-life datasets;interval data;mining association rules;association rules;association rules
database systems;simulation model;database;control mechanisms;database applications;representative set;data security;concurrency control;transaction processing
access control;language (called;access control mechanism;unified framework
transaction throughput;simulation model;representative set;distributed database systems;data processing;upper bound;distributed database;transaction processing
case studies;information systems
object-relational;object-relational database systems;object-relational database systems;commercial database;relational dbms;key features
database;decision support;on-line analytical processing;data warehousing;data warehousing and olap;databases;increasing attention;decision making
pre-defined;database;query language;visual query;user interface;constraint-based;object-oriented databases;query engine
very large datasets;data processing;relational queries;data visualization;data exploration;meta-data;main memory;application domains;data records;wide range;large datasets;multimedia objects;user groups;integrate data from;visual exploration of
heterogeneous systems;data access
information systems;data model;main components;database
heterogeneous information sources;query language;rule-based;post-processing;source-specific;template-based;1, 6;legacy systems;4,8;1;instance;template-based;5;7;access information;data transformation;source schema;language called;data structures;answering queries;user query;high level;query capabilities;data model
heterogeneous information sources;world wide web;data warehouse;user interfaces;collected data;multiple data sources;information integration;databases
world wide web;data source;database;cost model;data sources;distributed database;access information
clustering;user-friendly;databases;query language;data cube;spatial information;geo-spatial;on-line analytical processing;spatial knowledge;classification rules;3;data mining system;data mining;association rules;spatial data mining;high-level;spatial databases;spatial data mining
heterogeneous sources;current commercial;end users;data warehouse;major components;index structures;materialized views;data sources;main features;source data;wgl;end user
2, 5, 10;world wide web;13;labeled trees;database;edit operations;databases;9, 12, 14;matching techniques;7, 11;4;6;digital libraries;data warehousing;document databases
similarity query;database systems;database;index structures;similarity retrieval;object-oriented;similarity search;similarity queries;graphical user interface
span multiple;information systems;workflow management;enterprise-wide;workflow management;enterprise-wide;1, 2, 3;fall short;workflow management system
passage retrieval;document retrieval;fixed-length;passage retrieval;document ranking
approximate answers;storage cost;data analysis;data cube;multidimensional databases;approximation technique;aggregate function
public domain;spatial data;databases
conceptual modeling;entity-relationship;spatial relationships;entity-relationship
relational models;database;lower level;relational data;higher level
knowledge management;databases
designed specifically for;user interfaces;query processing and optimization;structural summaries;database management system;storage management;external sources;semistructured data
information systems;database technology;database systems;large networks;application systems;databases;data-intensive applications
indexing problem;automatic indexing
information systems;spatial data;multiple sets of;database;representative set;data sets;data sets;explicitly represented;spatial databases;geographic information systems
information science;information processing;information retrieval
traditional databases;object-oriented databases;semistructured data;abi, bun
web documents;underlying structure;wrapper generation;database;semi-structured;automatically generating;world wide web;wrapper generation
database;external information;query processing;semistructured data;external sources;external data
highly heterogeneous;information spaces;text search;data items;textual documents;information space
data elements;semistructured data;underlying structure;classification;large collections of
information systems;database systems;international workshop on
main memory;index pages
access control;state university;query evaluation;information systems;object-oriented;access controls;electronic commerce;unified framework;constraint programming;software systems;information systems;management systems;information flow;information systems;world wide web;data mining;software agents;constraint databases;information technology
information systems;database technology;database systems;database;information management;information processing;1;object-oriented;information systems;databases
workshop report;real-world;data management;data management;real-world;case-study
conference series;world wide web;object model;object-oriented database;domain-specific;resource description framework;technical program;information technology
database;relational database system;database research;code base
data warehouse;data warehouses;execution plan;relational views;materialized views;olap queries;source databases;relational views
database;database systems;database management systems;active functionality
fuzzy set;binary attributes;numerical values;databases;quantitative attributes;data mining;hidden knowledge;association rules;fuzzy association rules;fuzzy sets;discovered rules
spatiotemporal queries;access methods
parallel database;dimensional space;input data;real datasets;end users;data storage;25;multi-dimensional;21;22;28;multiple datasets;scientific datasets;aggregation functions;2, 7, 14, 19, 26,27, 29, 31;remote-sensing;multi-dimensional data;large volumes of data;storage capacity;long running;1, 5, 6, 10, 18, 23, 24, 28;medical images;17;computational power;database;32;4, 6, 20;user request;distinguishing features;memory management
query processing strategies;workflow systems;database;distributed architecture;data repositories;cost model;distributed environments;data management techniques;data mining;query processing techniques;management architecture
world wide web;object model;web applications;management architecture;depends crucially on;end-users;database;distributed applications;sol;computing environments;complex applications;enterprise applications;designed to support;enterprise systems;application development
data intensive applications;development process;object technology;database
database;object-oriented database;object oriented databases;object-oriented;object models;object-oriented databases;similar features;database schemas
clustering;text mining;database systems;classification;data warehouses;multimedia data mining;large databases;data mining methods;web mining;data mining system;data mining;visualization tools;relational databases;user-friendly;spatial data mining;data mining systems
association-rule mining;large databases;general-purpose;association-rule mining;query optimizers;data-mining problems;query-optimization
black-box;exploratory mining;human-centered;mining association rules;algorithm called;constraint-based;exploratory mining;user interaction
parallel algorithms;mining algorithms;frequently occurring;classification;frequent itemsets;association rule mining;generalized association rules;mining association rules;data items;memory space;candidate itemsets;interesting rules
query rewriting;optimizer;plans;intermediate result;decision support systems;queries efficiently;data flow;commercial database;optimization techniques
object-oriented databases;query optimization
rewrite rules;15;rule-based;1;query modification;optimizer;rule-based
clustering;data set;large databases;clustering quality;random sample;traditional clustering algorithms;data mining;clustering algorithm;clustering algorithm called;random sampling
database;mining algorithm;real data;long patterns;databases
end-user;dense clusters;clustering algorithms;data mining applications;high dimensional datasets;subspace clustering;data mining applications;clustering algorithm;dimensional data;data distribution;comprehensibility
query execution;query execution plans;query optimizer;database;object-relational;complex queries;databases;complex query;execution plan;key points;query execution plan;decision support queries;query optimizers;resource allocation
world wide web;query evaluation;database systems;ir) techniques;iterative process;information retrieval;evaluating queries;document collections;buffer management;access patterns;query processing strategy;natural language
query optimization;query execution plans;cost-based;plans;query optimizer;distributed query processing;simple heuristics;remote sites;disparate sources;remote sources;cost-based;query processing;original formulation;area network;data access
dimensional space;linear scan;partitioning strategy;index structures;real data;indexing method;query processing;dimensional data;range queries;data distribution
high-dimensional;multi-step;minimum number of;database applications;performance gain;increasing number of;similarity search;refinement step;korn et al. 1996;distance functions;query processing strategy;nearest neighbor search
multi-dimensional;data distribution;index structures;index structure;aggregate data;query performance;svd-based;query response times;dynamic databases;dimensionality reduction;databases;similarity searching;query results;dimensionality reduction;multi-media
world wide web;database;rule-based;heterogeneous data sources;language (called;graphical interface;query languages;pattern matching;semistructured data
information systems;query engines;semi-structured;data independence;higher order;data warehousing;query languages;query translation;higher order
world wide web;short queries;natural language text;exact matching;textual similarity;vector-space model;information retrieval;inference methods;heterogeneous databases;databases;real world
query optimization;spatial data;database systems;database;linear constraints;spatial objects;query language;data model;spatial database;kkr;constraint-based;optimizer;query languages;spatial queries
response times;access method;event-driven;disk accesses;query processing in;similarity queries;query processing;greedy algorithm;increasing importance;similarity query processing
incremental algorithms;incremental algorithms;attribute values;join operations;data structures;data types;semi-join;join algorithms;join result;distance metric;distance join;spatial databases
summary tables;incremental update;on-line analytical processing;decision support applications;query performance;olap queries;data warehousing;aggregate views;excellent performance;data generated from
clustering;response times;relational tables;level caching;multidimensional space;query results
query optimization;multi-dimensional;data analysis;plans;storage structures;query plans;query evaluation;global optimization;data sources;relational database systems;databases;multidimensional data;database researchers
mining algorithms;database systems;automatically extracting;text documents;html pages;document structure;query processing;semi-automatically;semistructured data;extracting data from
clustering;finding an optimal;general problem;raw data;directed graphs;general form;semistructured data;semistructured data;np-hard
accurate classifier;profile-based;classification;database;meta-data;link information;database;classifier;text classifier;databases;text classification;high-quality;statistical models;automatically extract;term-based
involving multiple;data source;huge amounts of data;answering queries;heterogeneous data sources;decision support applications;query plan;materialized views;data sources;cost-based;query processing cost;decision support queries;query languages;cost-based optimization;commercial database
approximate answers;highly-accurate;sampling-based;data warehouse;query answers;incremental maintenance;summary statistics;sample points;data distribution
database systems;data warehouses;association rule mining;object-relational;relational database systems;association rule mining;data mining;user-defined functions;mining algorithm;sql queries
bitmap index;decision support systems;bitmap index;database design;selection queries;bitmap-index;disk-space;design space
database administrator;existing indexes;taking into account;database;user interfaces;database design;impact analysis;databases;quantitative analysis;microsoft sql server
parallel processing;large data volumes;decision support queries;user-defined functions;object-relational dbms;parallel execution
security problems;java-based;database systems;database;object-relational;database server;database servers;user-defined functions;design issues
query rewriting;multimedia databases;text documents;index structures;feature extraction;relational operators;optimization methods;cost model;similarity based;search engine;complex queries;query plans;algorithms for computing;multimedia data;automatically extract
database management systems;semi-structured;web sites;data model;directed graphs;web-site;visual presentation;web site's
approximation algorithms;limited memory;memory requirements;approximation guarantees;large datasets;theoretical analysis;algorithms for computing;simulation results;random sampling
clustering;query optimization;error bounds;sampling algorithm;data distributions;commercial products;error metric;histogram construction;query optimizers;execution plans;adaptive algorithm;requires solving;microsoft sql server;random sampling
approximate answers;query optimization;data distributions;frequency distribution;selectivity estimation;query optimization;database;multiple attributes;relational database management systems;joint distribution;wavelet decomposition;olap queries;space usage;wavelet-based;databases;commercial database systems;fast algorithms;exact answers;random sampling
information systems;database systems;database;database applications;database server;application-server;fault tolerance
individual records;intra-query;memory size;wide range;traditional database;commercial database systems;memory management;memory utilization;memory management
10;simulation study;7;replication;graph-based;mutually exclusive;transaction throughput;replication;5;sufficiently high;serializability;wide range;atomicity;replicated data
database research;information systems
data warehouses;database systems;database management;knowledge management;data management;database;diverse sources;database application;semantic issues;query processing;database technologies;databases;information gathering;semantic information;application development
electronic commerce
database;database applications;database application;application systems;relational database;real world;relational database system
relational databases;stored procedures;data types;relational databases
relational data sources;data objects;data access;application-level;multi-tier;enterprise applications;data access;software components;model called;efficient access to
enterprise applications;data access
data sources;relational systems;data stored in;data access;external data
db2 universal database;large distributed;db2 udb
database systems;digital information;low-cost;data types;database;large databases;user-defined;databases;data server
commercial systems;processing power
data center;geographically distributed;retrieving information from;web servers;data centers;broadcast news;databases;multiple sites
resource manager;event-driven;large-scale;distributed applications;database transactions;database;stored procedures;user defined;databases;demand-driven;oracle database
data mining application;tree induction;association rules;data-mining;real-world
individual records;relational database systems;shared memory;million records;memory utilization;block) based
resource sharing;oracle8 database;database;large scale;parallel execution;buffer management;data server;high availability;highly scalable;operating systems
resource sharing;oracle8 database;large scale;parallel execution;digital media;buffer management;data server;high availability;highly scalable;operating systems
document content;business processes;document management;database;client applications;document search;document management;storage manager;relational database;ad hoc
web site;web sites
plans
web site;data integrity;high availability;data management;database
search engines;high availability
data management;transaction processing
data mining results;data mining applications;data mining;discovery algorithm;classification tree;computationally expensive;data mining;fault-tolerant;protein sequences
hybrid approach;3;web search engines;event-driven
database systems;database;visual query;visual representations;integrated environment;growing importance;main components
workflow management systems;business process;3;5;4;6;9;8;distributed systems;workflow management system
natural language text;database;web sites;textual similarity;vector-space model;information retrieval;heterogeneous databases;databases;database integration;real world;data integration
multiple sites;addressing this problem;integrate data from;web sources
data visualization;database;long-running;user feedback;batch-processing;data-intensive applications;query processing algorithms;aggregation queries
web documents;relevant documents;information retrieval;object-oriented database;database;query language;user-oriented;raw data;user query;filtering techniques;knowledge-based;6;user-oriented;relevant information;standard database;information retrieval;standard database
query evaluation;domain-independent;optimizer;database;query language;healthcare data
database applications;main-memory;high-level;mobile communication;concurrency control
clustering;classification;multimedia databases;visual content;data cube;multimedia data mining;multimedia data mining;data mining system;high-level;multimedia data;multimedia information
data manipulation;object oriented database;database;tree structured;relational database;query results
production process;world wide web;web-based
end users;database;free-form;semi-structured;user-defined;html pages;search engines;heterogeneous databases;databases;web browser
real-life problems;workflow management;database management;workflow management;mobile computing;business process;transaction management;workflow management
clustering;data visualization;classification;information systems;data mining methods;data mining techniques;database;data mining applications;data mining techniques;object-oriented;active databases;knowledge discovery in databases;broad applications;databases;data mining;pattern matching
classification;data mining techniques;data mining system;data mining;relational databases;user-friendly
speech recognition;face recognition;content analysis;massively parallel;user defined functions;general-purpose;computationally intensive;multimedia objects;ois
execution plans for;database systems;end users;database;data types;plans;database;query capabilities;set-valued;1;object-oriented;3;2;multimedia information;databases;query results;graphical interface;query language;data access
search terms;internet search;frequency distribution;information retrieval;relevance feedback;user queries;real life;queries posed
relevant documents;wide range;text databases;similarity measures;ranked queries
multimedia databases;nearest neighbor;huge amounts of data;indexing method;indexing structure;efficient retrieval of;nearest neighbour;search algorithm;pruning power
view maintenance
active databases;object-oriented database;control systems;huem
high-speed;38;database;low-cost;decision support;general-purpose;database servers;decision support systems;data warehousing;main memory;rapid rate;processing requirements
relevant documents;information retrieval;relevance assessment;large-scale
boolean satisfiability;search performance;large-scale;optimization problems;positive results;evaluation functions;global optimization;search algorithm;evaluation function
software architecture;probabilistic reasoning;everyday life;user interface;problem solving;high-level;low-level
filtering algorithms;numerical analysis
multi-agent systems
high degree of;upper bounds;artificial intelligence;upper bound
future events;social welfare
exhaustive search;tight bound;coalition structure generation;coalition structure;worst case;multiagent systems
learning process;social interactions;human-robot;facial expressions;learning task;social interaction
virtual world;learning mechanism;behavioral patterns;real world;virtual worlds;low-level
building blocks for;object-oriented;agent architecture;decision-making;making decisions
focused primarily on;parallel processing;computationally intensive;logic programming;case study;case studies;natural language processing;natural language
case study;database;software agents;cognitive science;software agents;real world;classifier;natural language
inference method;agent behaviors;multi-agent
single agent;intelligent agents;information sources;multi-agent;agent behaviors;dynamic environments
domain knowledge;bayesian network;intelligent tutoring system;plans;solution path
accurately reflect;spatial knowledge;knowledge-based;visual presentation;learning environment;natural language
preference relations;belief base;preference relation;unified framework
special case

design decisions;computing environment;large number of;evaluation criteria;filtering technique;promising candidates;high-level
solution space;construction algorithm;exhaustive search;computational power;multiple types of
explicitly model;dynamic bayesian networks;stochastic processes;dynamic bayesian networks;representation language;organizational structure;object-oriented;inference algorithm;compact representation;complex systems;real life;bayesian networks;9;inference mechanism;bayesian network
approximately optimal;multiple tasks;global solution;heuristic techniques;markov decision processes;resource constraints;resource allocation
large-scale;speech recognition;state representation;strongly correlated;dynamic bayesian networks;stochastic processes;dynamic bayesian networks;recognition task;state variables;real-world applications;speech recognition;hidden markov models;long-term;em algorithm;error rate;recognition problem;learned models;learn models;short-term
modeling tool;meta-level;numerical simulation;black-box;modeling task;parameter estimation;candidate models;real-world;simple linear;model construction;qualitative simulation;constraint reasoning;qualitative reasoning;abstraction levels
large scale;scientific discovery;theoretical foundations
learning rate;massively distributed;parameter estimation;large-scale
semi-structured data;database;description logics;logic programming;knowledge representation;data integration;digital libraries;biological databases;semistructured data;data management
specialized algorithms;multiple sites;web sources;knowledge representation;machine learning;information integration;addressing this problem;web sources;multiple sources;integrate data from
formal semantics;physical systems;dynamic systems;physical systems

spatial aggregation;qualitative analysis;spatial objects;physical systems;prohibitively expensive;control problem;computational methods
constraint satisfaction problem;constraint language;instance;qualitative simulation;qualitative simulation;clustering algorithm;qualitative model;constraint satisfaction problems
real numbers;simple temporal;partially ordered;temporal constraints;search nodes
total number of;plans;minimum number of;maximum number of;shortest-path;linear space;simple algorithm;plan execution
high computational complexity;polynomial space;davis-putnam;reasoning tasks;complexity classes;reasoning problems;wide range
incomplete knowledge;knowledge base;partial observability
approximation method;quality guarantees
target language;knowledge bases;lower bounds;lower bounds;algorithms for computing;computationally feasible
computational complexity
positive results;knowledge bases;negative results
search algorithms;branching factor;search trees;search spaces
search algorithms;branching factor;accurately predict;heuristic function;analytical model;heuristic search
hidden variable;constraint satisfaction problems;constraint satisfaction problem
partial order;polynomial space;search space
constraint satisfaction problem;constraint satisfaction problems;4
search techniques;arbitrarily large;problem solver;phase transition;theoretical properties
scheduling problems;graph coloring;search spaces;objective function;greedy algorithm
variable ordering;performance gains;csp instances
constraint satisfaction;problem instance;search tree;optimization problems;performance prediction;sampling method;instance;algorithm selection;performance prediction;instances
filtering algorithms;fast algorithm;general purpose;real life;arise naturally in;constraint based;constraint satisfaction problems;filtering algorithm;bound consistency
search space;domain specific
real-world;domain-specific;local search;tabu search;integer programming
constrained problems;real-life problems;constraint satisfaction problems;integer programming
search space;local search;random #sat;stochastic local search;statistical information;instances;local search
perform poorly;genetic algorithms
constraint satisfaction;instances;lower bounds
constrained problems;problem size;propositional satisfiability;lower-bound;search problem
state space;search algorithms;dynamic-programming;heuristic search;decision problems;heuristic search
state transitions;lower bound;single-agent search;search techniques;learning algorithm;application domains;search tree;single-agent
beam search;beam search;domain-independent;boolean satisfiability;real-world;state-space search;search algorithm
search methods;real-world;search algorithms;complete search
search problems;problem instance;information entropy;graph coloring;search algorithm;search behavior;np-hard
planning problem;heuristic search;utility theory;real-world;problem-solving;search algorithm;soft constraints
state-space;linear-space;search algorithms;algorithm called
automated reasoning;machine-learning techniques;feature-based;speed-ups;learning method;search problems
control mechanism;high cost;problem solver;control knowledge
probabilistic modeling;search algorithms;search techniques;combinatorial optimization;computational costs;incremental learning;probabilistic models;probabilistic models;population-based;search problems
simulated annealing;hill climbing;control algorithm;search tree;optimization methods;control method;genetic algorithms;computing resources;branching factor;expected utility;design process
data compression;genetic algorithm;high-precision;target image;hardware technology;compression method;image compression;field programmable gate;high precision
risk management;real world;decision-making;artificial intelligence
game trees;finding optimal;heuristic algorithms;test database;monte-carlo sampling;large database
world wide web;training data;knowledge-based;knowledge base;instances;problem solving;extract information from;machine learning algorithms;information extraction system
world wide web;extraction patterns;specific features;relational learning;learning systems;general-purpose;information extraction;knowledge discovery;document structure;extraction rules;learning problem;information extraction from;machine learning;feature set
knowledge engineering;real-world;text understanding;domain-specific;knowledge base;natural language
everyday life;world wide web;question-answering;information exchange;natural language
growing number of;digital libraries;ai research;vast number of;information sources;structured documents;large number of;human experts;extracted data;extracted information;information gathering;decision process;gather information;resource-bounded;text processing;plans
design decisions;design principles

planning domain;plans;ai research;diverse set of;knowledge-based;lessons learned
planning problems;problem-solving;problem solving;spoken-language;natural language
knowledge intensive;regression problems;data set;knowledge intensive;based reasoning;classification problems
handle complex;structured domains;knowledge bases;instance;probabilistic information;probability models;probability distribution over;organizational structure;inference algorithm;probabilistic representation;instances;knowledge representation;bayesian networks
counterpart;utility theory
common properties;fuzzy logic;instance;description logic;real world;constraint propagation
domain knowledge;knowledge bases;programming languages;knowledge base;knowledge representation;design issues;knowledge base;programming interface
knowledge representation;description logics
knowledge sharing;molecular biology;common properties;query interfaces;information retrieval;domain independent;additional features;inference rules
action language
plans;partial information
dynamic environments;specification language
propagation algorithm;np-complete;general problem;search algorithm for
representation language
temporal reasoning;reasoning tasks
domain knowledge;conventional methods;training examples;domain theory;knowledge-based;learning method
network model;classification models;classification problems;classification task;unsupervised fashion
line segment;real world;genetic algorithm;local search;genetic algorithm
total number of;reinforcement learning;management systems;traffic management;control strategies;artificial intelligence
training set;generalization error;generalization performance;linear programming;closely related;controlled experiments;ensemble classifier;margins;margin
similar patterns;data sets;training data;classifier
real-world;expected cost;classification systems;classifier;operating conditions
user preferences;recommendation systems;classification;nearest-neighbor;instance;inductive learning
learning task;problem domain;user models;machine learning
clustering;interface design;web pages;access logs;overlapping) clusters;index pages;clustering problem;web sites;web sites;web site;server logs;web site;data collected;traditional clustering algorithms;mining algorithm
sequence data;dna sequences;learning systems;feature generation;feature generation
class membership;sensor readings;time series
multi agent systems;learning process;multiagent systems;reinforcement learning;reinforcement learning
probability distributions over;convergence properties;uncertain information;model-free;bayesian approach
algorithm generates;state space;tree based;state spaces;multi-agent;reinforcement learning;regression tree;reinforcement learning;tree based
search space;search algorithms;tree search;sampling-based;state space;instances;genetic algorithms;local search;search algorithm;artificial intelligence
expression recognition;lower bound;character recognition
training set;unlabeled data;target function;text based;large quantities of;labeled documents;classifier;naive bayes classifier;unlabeled documents;training documents;text classifiers;class labels;text classification problems;classification error;expectation-maximization
automatically extracted from;feature sets;conditional distribution;word-sense disambiguation;gibbs sampling;em algorithm;corpus-based;unsupervised techniques
feature space;context-sensitive;statistics based;learning algorithm;large number of;data driven;natural language;machine learning algorithms;specific problem
fine-grained;natural language generation
text summarization;machine learning;training corpus;summarization task;machine learning
multi-agent;preference relations;inconsistent information;existing knowledge;decision making
select relevant;bayesian networks
operator
randomly generated;np(3)-complete;instances
logic programming;nonmonotonic reasoning;satisfiability problem;knowledge representation;query answering;knowledge base;knowledge bases
qualitative reasoning;domain-specific;mining algorithm;execution traces;plans
irrelevant information;planning systems;plans;computational power;data structure
planning problems;plans;large state spaces;binary decision diagrams;model checking;main features;automatic generation of;real world;deterministic domains;plans
action-based;user-intent;hierarchical task;plan-space
initial conditions;sensory information;plans
planning problems;complete information;sensing actions;gather information
initial conditions;domain-independent planning;automated methods;state constraints;operator;sat-based
completeness;knowledge base
factors affecting;dynamic environments;multiple tasks
task decomposition;hierarchical task
domain knowledge;recognition systems;action sequences;plan recognition;plans
large numbers of;pattern recognition;plan execution;large-scale;real-world;multiple agents;object tracking;plan recognition

planning problems;boolean satisfiability;dynamic programming;plans;planning problem;planning algorithms;performance degradation;memory intensive;stochastic domains
real world;cognitive science;social interaction;intelligent systems;complex tasks
face images;algorithm performs;face detection;static images;active vision;template-based;high-resolution
human robot;nearest;neural networks;hybrid approach;template matching;tracking algorithm;everyday life;template-based;lighting conditions
sensor readings;dynamic environments
fine-grained;mobile robotics;mapping problem;maximum likelihood
processing algorithms;knowledge-representation;increasing complexity;small sets of;control-strategy
scene analysis;hierarchical structure;knowledge representation;automatic speech recognition
individual agents;instance;agent architecture;problem solving;scheduling problem;agent-based;multiple objectives
case study;domain-specific;input data
data analysis;software tools;synthetic aperture;image processing;ai planning;data preparation;image processing;artificial intelligence
test results
business rules;databases
united states;instances;rule-based
pattern detection;pattern recognition;stock market;knowledge discovery;data mining;identify patterns
highly dynamic;decision support system
case study;knowledge-based systems;knowledge-based;artificial intelligence;knowledge base
planning systems;ai planning;hierarchical task
object technology;long distance;rule-based;network monitoring;object-oriented;programming language
web-based;high-level;span multiple;heterogeneous data sources;user-defined;user interests;data sources;allowing users to;data dissemination;ai technologies;obtain information
virtual camera;visual features;constraint-based
measurement data;fuzzy rules;genetic algorithms;expert systems;fuzzy-rule
network models;randomized algorithm;efficient search;broader range;network model;maximum likelihood
case-based reasoning;classification;rule-based;real data;knowledge based;operator;image classification;classifier
planning problems;planning domains;control strategy;control strategies;ai planning;partial-order;control strategies;total-order;planning systems;hierarchical task
fuzzy classification;classification techniques;rule-based;expert systems;fuzzy logic
control architecture;intelligent control;intelligent control;process control
neural networks;end users;knowledge representation;knowledge acquisition;end user;decision making
rule based;real world;constraint based;plans;constraint logic programming
error-prone;speech recognition;design choices;natural language processing;transaction processing;natural language
statistical information;transition probability;hidden state;conditional probability;statistical models;probability model
search methods;state-space;worst-case;minimum cost
classification learning;training data;hypothesis space;fold cross-validation;instances;labeled instances;expected error;e.g., 1;3;e.g., 2;worst-case;selection algorithm;algorithm requires;controlled experiments;model selection;learning problem;model selection
1;multi-source;multiple information sources;information integration;information sources
probabilistic modeling;probabilistic models;bayesian networks
dynamic programming algorithm;database;intermediate result;spatial datasets;spatial joins;spatial queries;join algorithms;operator;spatial join
range queries over;spatial data;selectivity estimation;selectivity estimation;relational database systems;spatial queries;real-life;selectivity estimates;spatial databases;geographic information systems
lock;access methods;database applications;index structures;index structure;high degree of;database management system;data types;concurrency control;search trees
clustering;benchmark data;search space;auxiliary;globally optimal;data warehouse;finding an optimal;query workload;probability distribution over;query classes;fact table;obtained by applying;hierarchy levels;dynamic programming algorithm
clustering;input parameters;density-based;interactive exploration;clustering structure;database;cluster analysis;large data sets;data sets;clustering result;data set;algorithms require;parameter settings;efficiently extract;data processing;clustering algorithm;real-data sets;cluster analysis;visualization technique
clustering problem;customer segmentation;synthetic data;classification;database;data mining applications;high dimensional;feature selection algorithms;high dimensional spaces;projected clustering;fast algorithms;numerous applications in;trend analysis
data values;database;cost-effective;cache management;8
low bandwidth;concurrency control;serializability;response times
data warehouses;existing protocols;database;replication;wide range;distributed systems;serializability;update propagation;data placement;serializable;database vendors;higher level
database;special case;view definitions;security level;databases
multiple databases;relational algebra;query language;specific properties;multimedia presentation;databases;query optimizers
external memory;semi-structured data;data model;data model for;real-life;queries posed;object-oriented databases;query languages;ad hoc;real world
association rule mining;support threshold;large itemsets;algorithm to compute;algorithm maintains
15;database management systems;query optimizer;ad-hoc;heuristic techniques;human-centered;support counting;effective pruning;frequent set;constraint-based;exploratory mining;optimization techniques;optimizer
decision trees;classification;classification;database;dynamic environments;class label;main-memory;decision tree construction;training dataset;data warehouses;tree construction;performance gain;data mining problem
query execution engine;low-cost;data distributions;multi-dimensional;prohibitively expensive;large data sets
approximate answers;high-dimensional;approximation techniques;sparse data;efficient representation;wavelet decomposition;data cube;data warehouse;storage space;efficient algorithms to;exact answer;real data;prohibitively expensive;high-dimensional data sets;data sets;query processing;desired accuracy;aggregation queries;olap applications;random sampling
multi-dimensional;multimedia databases;access plan;selectivity estimation;query optimizer;database;error rates;large number of;estimation technique;multiple attributes;multi-dimensional;storage overhead;data updates;discrete cosine transform;feature vectors;data distribution
complex queries;encoding schemes;selection queries;query classes;decision support systems;encoding scheme;commercial database systems;range queries
query optimization;database management systems;data warehouse;optimization strategies;selection criteria;optimal design
small sample;selectivity estimation;real data sets;database queries;data sets;temporal databases;estimation methods;range queries
order statistics;extreme values;sampling techniques;database applications;main memory;mrl;space requirements;input sequence;large datasets;simple algorithm;sampling scheme;random sampling
join tree;sampling techniques;sql server;theoretical results;random sampling;sampling algorithms;negative results
approximate answers;base relations;statistical summaries;database;approximate query answering;aggregate queries;incrementally maintaining;data warehousing;join-queries
online aggregation;statistical properties;query result;confidence intervals;exact answers;join algorithms;aspect-ratio;relational database management system;decision-support applications;aggregation queries;arise naturally in
query execution;designed to support;query operators;query processing in;data sources;data transfer;rates;autonomous data sources;data integration
query optimization;search space;plans;query optimization;optimization algorithm;search strategy;query plans;access patterns
plans;data model for;query processing;user-defined functions;query processing techniques;computational models
heterogeneous information sources;mapping rules;wide range;inter-dependencies
query execution;database;performs poorly;client-site;query optimization;optimize queries;user-defined functions;client site;distributed database;client-site
pruning strategy;minimum support threshold;minimum support;single attribute;memory requirements;apriori algorithm;association rules
complex structure;data warehouses;takes into account;pre-computed;data warehouses;aggregate functions;pre-computation;space constraints;materialized views;view selection;view maintenance
dynamic programming algorithm;similar objects;image based;real-life data sets;similarity retrieval;image databases;wavelet-based;similarity measure between;sliding windows;query result;similarity matching;wavelet-based
similarity function;excellent scalability;data mining methods;database size;index structure;similarity functions;market basket data;similarity search;wide range;structured information;association rule;search algorithm;similarity indexing
database;database applications;large databases;search process;index structure;similarity search;efficient similarity search;spatial databases
store data;document-type;xml data;data model;language called;relational database management systems;instance;data sources;generated automatically;semistructured data;data-mining techniques
data sources;query-processing capabilities;query-processing techniques;data-integration systems
query rewriting;rewriting queries;structural constraints;query rewriting;rewriting algorithm;semistructured data;relational data
tag based;web documents;heuristic approach;data-extraction
database contents;sampling approach;database;automatically selecting;language models;database selection;text databases;databases;language model;selection algorithm;random sampling
traditional databases;web-based;querying capabilities;database systems;data extraction;web content;web sources;semi-automatically;ad hoc;end user
database systems;database;plans;decision-makers;mobile computing;database vendors;small-scale
world wide web;database;wide range;huge amounts of data
database management systems;high-energy physics;databases
database
unstructured text;customer service;databases;information retrieval;database researchers;data mining;digital libraries;structured data;database researchers
object-relational;end user;db2 universal database;db2 udb;database
object-relational;object-relational;relational database system;virtual machine
relational dbmss
direct access to;database;information sharing;extended abstract
data management problems;data compression;data warehouse;query language;high-level;data management system
replication;sql server;replication;sql server
fine-grained;microsoft sql server
large scale;large distributed;poor performance
data warehouse;decision support;data quality problems;view update;view maintenance;data quality;supply chain management;replication;data warehousing;source databases;quality measures;data entry
data warehouse;data store;database;data warehousing;databases;enterprise data;data integration
moving-objects;database;1;2;moving objects;databases;current location;moving objects
pre-defined;ms;query evaluation;database operations;database systems;database;data providers;gmpq;query operators;data processing;distributed database;object-relational database systems;external data;query engine;plans
query rewriting;5, 6, 7;data warehouses;view maintenance;materialized views;views defined;view definition;human interaction
query optimization;interactive exploration;frequent set;exploratory mining;relational databases;optimization methods;constraint-based mining;mining association rules;frequent sets;ad-hoc;query-driven;5;data mining system;commercial systems;data mining;exploratory mining;query languages

database systems;user-defined;optimizer;pre-defined;current commercial;information sources;critical task;information management;bkkk, tec, obj;application systems;schema evolution;database technology;br;data-warehouses;query optimization techniques;cr;cjr98b;br, ler;database;ad-hoc;data model;object-oriented;data sources;tec, obj;cnr;tec, obj, bkkk
approximate answers;statistical summaries;database;approximate query answering;aggregate queries;incremental maintenance;original data;quality guarantees;relational dbms;olap applications
database;mobile devices;query processing;query processing techniques;database engine;small-scale
multi-dimensional;data warehouse;storage structure;multi-dimensional data;query performance;materialized-views
data store;database;meta-data;search engine;query language;optimize queries;search engines;databases;helps users
multimedia databases;user-centered;information retrieval;digital media;multimedia information;relevant information;query languages;query refinement
xml data;query evaluation;complex queries;query formulation;data exchange;san diego;xml queries;demand-driven;xml documents;instance;xml-based;graphical user interface;schema information;user-friendly;query results;xml query language;data exchange
world wide web;information retrieval;information retrieval
retrieval effectiveness;large-scale;databases;information retrieval;query processing;text retrieval;linguistic analysis;phrase-based;test collection
electronic commerce;electronic commerce;international workshop on
product information;database
electronic commerce
object model;electronic commerce;data exchange;extensible markup language;resource discovery;resource description framework;control flow;document type definitions;match making
business activities;business process management;business process;lower cost;agent technology;management systems;higher quality;resource constraints;agent-based
data-flow;software architecture;domain-specific
domain knowledge;pre-processing;web usage mining;data mining techniques;electronic commerce;specifically designed for;web log data;web usage;formally defined
electronic commerce;related data
scientific literature;customer base;high-quality
long-range;data stores;database research;database management
enterprise applications;great flexibility;web servers
information systems;distributed computing;information sources;large distributed;semantic interoperability;structural heterogeneity;increasingly complex
end-user;information systems;spatial information;decision-making;diverse sources;semantic integration;decision makers;quality control;making decisions;context-based

united states;semantic interoperability;attribute values;information systems;large-scale
domain knowledge;information systems;video data;domain specific;management systems;high-level;video indexing
document sets;document clusters;topic areas;digital library;digital libraries;information space
heterogeneous information sources;large-scale;related information;structured data sources;description logics;description logic;2, 6;working group;4, 5;information sources;mapping rules;3;5;7;8;semistructured data;multiple information sources;source schemas;data management;query optimization;query-processing;real-world;semantic integration;mapping rules;integration process;information systems;17;heterogeneous sources;1, 10, 14;query interfaces;data model;problems arise;object-oriented;information integration;inference techniques;semi-automatic;semantic heterogeneity
technical program;database systems;database systems
materialized views;data warehouse;answer sets;data warehouses
data point;search methods;spatial data
database architecture;data sources;databases
attribute-based;keyword-based search;large number of;user profiling;search engines;visual information;query processing;search engine;user profiles;user-friendly;automatically extracts
db2 universal database;database management systems;object-relational;object-oriented;object technology;additional features
data mining;data mining
information resources;business intelligence;information visualization;data warehousing;data mining
massive data sets
world-wide web;keyword queries;classification;data warehouses;data mining and machine learning;data mining;data mining research;search result;learning problems;semi-supervised
search space;social welfare;cost based;locally optimal
objective function;game theory;mechanism design
network model;decision-theoretic model;plans;sufficient conditions for
multi-agent systems;agent technology;automatically generating;potentially infinite
domain-specific knowledge
higher level;soft constraints;problem domain;genetic algorithm
private information
agent communication
web pages;background information;textual similarity;general-purpose;information retrieval;similarity queries;html documents;web page
linked data;query execution plans;data source;plans;user query;data sources;data integration systems;query answering;data integration
information integration;verification problem;information extraction
search results;hierarchical organization;knowledge-based;search tools;helping users;relevance-ranking;search engine;retrieved documents;helps users;clustering
electronic commerce;electronic commerce;sealed-bid;artificial intelligence
neural networks;real-world;information retrieval;rates;information retrieval techniques;neural network
classification techniques;classification;classification;decision tree;uci data sets;feature values;data sets;computational overhead;sufficient training data;semantic information;classifier;decision making
robot learning;planning algorithms
great promise for;knowledge integration;natural language;learning environments;knowledge-based
computational model;specific task
domain knowledge;intelligent tutoring system;high level
modeling framework;optimization problems;combinatorial optimization;linear programming;constraint propagation;traditional models;linear programming
search space;local search techniques;constraint satisfaction;main memory;data structures;constraint satisfaction problems;spatial databases
constraint satisfaction;ai techniques;information systems;constraint-based
constraint satisfaction;optimization problems;probability distribution over;constraint satisfaction problem;formal model
inverse consistency;hidden variable;constraint satisfaction problems;inverse consistency
local consistency;problem) solving;constraint satisfaction;inverse consistency;memory requirements;generic algorithm;local consistency;instance;generic framework;path consistency
domain size
genetic algorithms;plans
instance;basis functions;nearest-neighbor classifier;selection algorithm;training examples
neural network model
fuzzy set;fuzzy sets
source code;multi-agent;face recognition system;complex tasks;recognition performance;face detector;neural-network;human faces;high-level
fuzzy rules;knowledge representation;rule-based
knowledge bases;prior knowledge;domain-specific;knowledge bases;knowledge base;knowledge-based systems
knowledge based systems;problem-solving;problem solving
problem-solving;knowledge representation;knowledge acquisition;knowledge base;knowledge-based systems
knowledge acquisition;knowledge bases;knowledge-based systems
domain experts;learning agent;inference engine;knowledge-based;knowledge engineers;knowledge representation;knowledge acquisition;problem solving;knowledge base
target language;knowledge compilation;completeness;task-specific;general form
constraint logic programming;constraint logic programming;database updates;application domains;constraint-based
generalization hierarchies;formal framework;classification;description logics
high-level;obtain information
automated reasoning;social sciences
description language;active databases;action theories;communication network

constraint satisfaction;normal form;constraint satisfaction problem;constraint satisfaction problem
dynamic bayesian networks;information-theoretic;compact representation;tracking algorithm;dynamic systems;belief state;efficient approximate
fold cross-validation;confidence levels;training examples;generalization error
domain-specific knowledge;semantic classes;natural-language;learning algorithm;information extraction;machine learning;relational learning;inductive logic programming;information extraction;text processing
state space;monte carlo;sampling-based;coarse-grained;probability distributions;grid-based;large sample;high-resolution
real data sets;training set;active learner;nearest neighbor;classification;training examples;active learning;costly process;instance;sampling process;sampling algorithm;unlabeled data
target class;accuracies;learning algorithm;decision-tree;decision trees;feature subsets;databases;machine learning algorithms
approximate inference;bayesian networks;plans;belief networks
tree structure;statistical model;training set size;class label;decision tree
genetic algorithms;learning algorithm;feature selection algorithms;ensemble feature selection;feature subsets;feature selection;learning task;feature selection
learning strategies;learning approaches;reinforcement learning;prior knowledge;data-complexity;real-world;supervised learning
intelligent agents;natural language
large amounts of;control systems;optimization techniques;genetic algorithms;information processing;robotic systems;prohibitively expensive;optimization algorithms;data rich;neural nets;sensor data;real-valued;optimal control
based reasoning;parametric design;input parameters;sensitivity analysis;design space;optimization criteria
instance;linear programming;model-based diagnosis
data set;pruning strategy;phrase based;natural language processing
information extraction system;user specifies;information extraction
information overload;information filtering;individual agents;collaborative filtering;collaborative filtering
text collections;sample size;information retrieval
relevant documents;statistical techniques;information retrieval systems;input text;multi-document summarization;retrieved documents;automatically generate;machine learning;information retrieval;related documents;linguistic features
statistical methods;free text;large corpora;word senses;search engines;information gathered;high accuracy;major limitation;automatically generate;text processing;word sense disambiguation
document summaries;text summarization;summaries generated;feature set;sentence extraction;linguistic features
multi-level;web pages;high-quality;information extraction systems;information extraction;semantic categories;extraction patterns
filtering technique;mutual information;decision tree;information-theoretic;vector machine;input space;text categorization;feature selection;filtering methods;optimal number of;feature selection
natural languages;database;semantic representations;database queries;natural language
plan execution;concurrent execution;plans
search methods;search space;local search;completeness;local search techniques;search techniques;domain-independent planning;planning graphs;search problem;plan-generation;planning graph
detailed comparison;domain-independent;plans;search techniques;graph-based;domain specific;temporal logic;control knowledge;search strategies;control knowledge
probabilistic framework;multi-agent;object recognition;trajectory data;belief networks;automatically generated;temporal structure
planning problems;local search algorithms;plans;linear programs;objective functions;state-space
planning problems;control knowledge;planning algorithm;planning graphs;planning-graph
probabilistic planning;planning problem;partially observable;markov decision processes;decision problems;success probability
satisfiability problem;instance;plans;probabilistic planning;planning problem
state-space;propositional satisfiability;plan-space
resource-bounded;resource allocation;partial plans;plans;dynamic environment
plan space;automatically generating;plans;cost-effective
plans;probabilistic planning;refinement process;effective search;planning algorithms;control strategy;control mechanisms
scheduling problems;general purpose;question arises;constraint programming;constraint programming;constraint propagation;search algorithm;constraint satisfaction problems;combinatorial problems
state space;partial plans;representation language;graph analysis;total order;search algorithm
textual descriptions;intelligent agents;efficient search;data structures;computational model;plans
graph structure;planning graph
clustering;plans;machine learning
face recognition;answering queries;context-sensitive
feature extraction;feature vector;color information
sensor readings;web pages;time series;learning algorithm
decision problem;davis/putnam
phase transitions;state space;phase transition;search cost;np-complete;complexity classes;worst case;random 3-sat
small-world
local search algorithms;completeness;random walk;stochastic local search;satisfiability problem;search problems
computational complexity;random sat;probabilistic models;davis-putnam;complexity classes;satisfiability problem;randomly generated;deterministic domains
search methods
scheduling problems;plans;constraint-based
problem size;real-world;search space;real-world applications
perfect information;brute-force;search techniques;decision-making;hidden information;expected values;instance;missing data;decision making;expert knowledge;artificial intelligence
pattern database;automatically generating;large-scale
probabilistic representation;machine learning;database;information retrieval
search methods;artificial intelligence;search problems
belief change;belief base;makes sense;minimal change
underlying structure;point-based;point-based;query answering;general problem;temporal reasoning;test results
query evaluation;constraint networks;constraint databases
inference procedure;constraint propagation;classical logic
medical image;retrieve images;database;content-based retrieval;image content;machine learning;high resolution;computed tomography;similar images;databases;human input;human interaction;machine learning algorithms;medical databases;content-based image retrieval
speech recognition;position information;spatial information;source separation;visual information;recognition process;scene understanding;object tracking
real-life;decision making
customer service;constraint-programming;problem solving;constraint-based;software components;hong kong
background information;data structure;case-based reasoning;case retrieval;traditional database
constraint solving;end-users;real world;constraint solving;artificial intelligence
scheduling problems;end users;domain experts;user interfaces;domain specific knowledge;knowledge-based;human experts;semi-automatically;knowledge-driven
search techniques;temporal constraints;artificial intelligence
process planning;knowledge representation;manufacturing process;knowledge base
fully operational;control systems;high level;intelligent agents;physical systems;human operators;operator;cognitive processes
constraint programming;semi-automatic;constraint reasoning;scheduling problem;constraint logic programming;artificial intelligence
web-based;domain-independent;ai research;database;text search;query formulation;general-purpose;ad-hoc;information access;databases;preference-based
rule-based;information extraction;competing methods;rates;higher level;information extraction system;word sense disambiguation
artificial intelligence;plans;artificial intelligence;plan generation
high-quality;optimization problems;real-world;commercial products;highly dynamic;algorithm produces;problem-solving;real world;general problem;agent-based
plans;hierarchical task;general purpose;planning tasks
cost-effective;design process
multi-agent;data collected from;multi-agent system;fitness
multi-agent system;application area;motion control;decision-making;highly dynamic;role-based;main features
domain experts;wide range;knowledge-based;knowledge engineers;learning agent
decision-theoretic;intelligent systems;user interface;development environment;bayesian networks;user-friendly
retrieval engine;knowledge base;multi-media

recognition accuracy;natural-language;html pages
electronic commerce;knowledge representations;rule sets;knowledge representation;computationally tractable;computational overhead
confidence scores;constraint satisfaction;candidate generation;generation process;information retrieval
helps users
electronic commerce;game theoretic;optimizer;software agents
text classifier;helps users
knowledge bases;similar results;natural language interface;query interface;science applications;graphical user interface;question-answering;natural language understanding
unknown environment;unknown environments
state university;object identification
information from multiple sources;data instances;problems arise;instances;multi-source;los angeles;information integration
human players;multiple streams
search terms;short queries;web search;web search engine;individual queries;information retrieval techniques;information retrieval;search engines;highly correlated;query log;query sessions;search engine;web users;correlation analysis;user sessions
working group;ir systems;information processing;information retrieval systems;test collection
information systems;database systems;database;information sources;ceh;international workshop on
multi-dimensional;cost estimation;optimizer;database systems;database;cost estimation;user-defined;query execution plan;object-relational database systems
multi-layer;database systems;user-centered;multi-tier;databases;semi-automatically;application developers;data services
web document;information content;multi-resolution
large enterprises;database applications;database research;multi-tier;distributed data;database application;distributed environments;data consistency;transaction management;distributed transactions;distributed transactions
web-based;database;scientific data;limited bandwidth;user interface;xml document;external data
distributed data sources;global information;classification;information sources;classification framework;heterogeneous data sources;databases
world-wide web;web search;community discovery;classification;web graph;directed graph;graph models
xml documents;regular tree;xml query languages
query optimization;object databases;xml documents;data integration;semantic constraints;navigation paths
document content;xml data;context-free;view definition;selection conditions;data type;query language
information content;completeness;query language;materialized views;answering queries;data warehousing;aggregate views
regular-path queries;query rewriting;query processing in;database;regular path queries;view definitions;raw data;view-based;materialized views;finite-state;query processing;operator;query answering;semistructured data;view-based;computational complexity
query optimization;rewriting queries;database systems;conjunctive queries;data integration systems;query containment;query containment;data integration
graph theory;constraint-satisfaction problems;machine vision;constraint satisfaction;database theory;database;constraint satisfaction;constraint satisfaction-problem;np-complete;temporal reasoning
approximate algorithm;database;sensitive information;sum queries;continuous attributes;databases;special case;continuous data;aggregate function;range queries;np-hard
electronic commerce;3;active) database
query evaluation;reachability queries;connected components;large number of;data complexity;databases;query languages;constraint databases;model-checking
fixed-point;transitive closure;complexity classes;constraint databases;fixed-point;query languages;real numbers
transitive closure;transitive-closure;query language;constraint databases;spatial databases;geographic information systems
extended abstract;regular expressions;end users;web sites;data extraction;html pages;extraction techniques;computation model;generated dynamically;web page
extended abstract;fall short;regular expressions;tree-structured data;query language;query languages
relational algebra;query languages;finite structures;extended abstract;data complexity
high-dimensional;dimensional space;range query;data sets;margin;high dimensional spaces
indexing schemes;extended abstract;block size;indexing scheme;indexing structure;nearest-neighbor queries
update semantics;extended abstract;database;higher degree of;serializability;concurrency control;serializable
information systems
4;range queries
boolean queries;monte carlo;applications ranging from;exact answer;selectivity estimates;general problem;query terms
search space;statistical measures;upper bounds;correlation coefficient;apriori algorithm;upper bound;association rules
combined complexity;computational properties;data complexity;database relations;real-world applications
sampling techniques;extended abstract;efficient approximation;sufficient conditions for;constraint databases
adaptive sampling;estimation algorithms;monte-carlo;estimation algorithm;upper bound
real data sets;negative result;input data;synthetic data sets;worst-case;error guarantees;provably optimal;random sampling
frequent patterns;large number of;candidate set;transaction databases;candidate generation;tree) structure;search space;tree-based;fp-growth;apriori algorithm;large database;mining task;data structure;mining frequent patterns;pattern tree;complete set of;long patterns;data mining research;tree structure;database scans;databases;time-series;frequent pattern mining
data mining application;data mining tasks;data mining;data mining task;data mining;high-level;processing power
association-rule mining algorithms;database contents;database workloads;database;large databases;market-basket;algorithm called;mining algorithms;general-purpose;performance gains;database schemas;mining algorithm
database;database systems;high speed
data warehouses;large quantities of;user-defined;algorithm called;distributed sources;high-level
clustering;response times;database;object databases;application domains;algorithm called;schema evolution;storage manager;high availability
clustering;clustering algorithms;high dimensional;clustering technique;large databases;projected clustering;space requirements;dimensional data;projected clusters;feature vectors;high dimensional spaces
clustering;synthetic data sets;special case;biased sampling;data mining;biased sampling;uniform sampling;large data sets;original data;data mining;dense regions;random sampling
real-world datasets;density-based;outlier detection;local outliers;common patterns;instances;desirable properties;outlier factor
pair-wise;summary tables;sql queries;decision-support queries;response times;multiple dimensions;materialized views;base tables;user-defined) functions;user query;vast amounts of data;graphical representation of
data collected from;web sites;data source;database
materialized view;view maintenance
web portals;query evaluation;general purpose;type information;information sources;maintenance cost;query capabilities;xml query languages;query processing in;xml-based;integration systems;digital libraries;query languages;optimization techniques
xml data;data exchange;user defined;compression ratio;data types
minimum description length;data collection;semantically meaningful;regular expressions;efficient storage;document type;database;data representation;input sequences;xml documents;xml databases;real-life;finding patterns;xml document;xml data;xml queries;valuable information;document type;inference algorithms
spatial join;real datasets;general case;power law;relative error;selectivity estimates;spatial-joins;spatial join;feature vectors;point-sets
distributed data sources;data source;database;distributed query processing;data types;scientific data;user-defined;query operators;data sources;query plans
incremental algorithms;data sets;special case;wide range;spatial databases
data point;rnn) queries;nearest neighbor;rnn queries;database;decision support;large data sets;tree based;nearest neighbor queries;digital library;sequential scan;multimedia data;query point;nearest neighbors
end-users;parallel database systems;skewed;performance degradation;parallel database systems;access patterns;data placement
data structure;search performance;scalable distributed;storage scheme;data structure;high-availability;scalable distributed;data-intensive applications
evaluation cost;complex queries;heuristic algorithms;multi-query optimization;search strategy;search space;decision support systems;multi query optimization;complex queries;cost-based;multi-query optimization
query optimization;adaptive query processing;optimizer;learning algorithm;query plan;query operators;join algorithms;query processing;databases
materialized views;search space;semantic constraints;query optimization;query plans
traditional databases;query execution;low-overhead;web search;sql queries;database;database) queries;information stored in;web searches;search engines;database;web search results;databases;query processing;relational database;generated dynamically;web) queries;relational dbms;query optimization;query processor
search results;world wide web;user preferences;formal framework for;real-life;search queries;operator
database technology;database;large scale;meta-data;data warehouse;geo-spatial;general-purpose;image data;united states;web browsers
complete model;object-oriented;data types;spatio-temporal databases;spatial objects;query language;data model;data model for;data structures;databases;moving objects databases
tree based;dimensional space;data set;efficient querying;moving objects;objects moving;indexing technique
join algorithm;internet search;join operations;multi-stage;join processing;spatial distance;query processing;database applications;distance join;join queries
case studies;web documents;web pages;replication;textual data;million web pages;web collections;data set;web crawlers;real-life;search engines;document collections;search engine;ranking functions;efficiently identify
automatically created;web servers;cost model;web server;database-backed;scalable solution;base data;web page
selection predicates;data source;continuous query;incremental evaluation;web queries;optimization strategy;large number of;requires minimal;general-purpose;share similar;databases;continuous queries;query engine;continuous queries
hierarchical organization;attribute values;data record;linear models;indexing structure;linear scan;weighted sum of
multi-dimensional;index pages;business data;multiple attributes;index structures;numeric data;exact matching;fixed length;xml documents;wide range;databases;index structure;mapping function;string matching;string data
feature vector;video databases;video data;inter-related;fully automatic;cost-effective
synthetic data sets;nearest neighbor;partition-based;database;data set size;join algorithms;data set;large data sets;real-life;distance-based
individual records;accurate models;data values;decision-tree classifier;privacy concerns;data records;data mining research;aggregated data;training data;data mining;original data;privacy-preserving data mining
pair-wise;data analysis;dimensional space;database;efficient clustering;sloan digital sky;highly correlated;data access;numerical attributes;data management
approximate answers;range queries over;multi-dimensional;efficient approximation;kernel density;variable size;database;local density;data exploration;real valued;synthetic datasets;query optimization;real numbers;data distribution
decision support;update cost;index structures;main memory;static data;wide range;incremental updates;search trees;indexing technique
approximate answers;query rewriting;highly-accurate;database;random samples;decision support queries;data items;biased) samples;summary statistics;data warehousing;decision support queries;biased samples
search space;optimizer;plans;database;search strategies;large number of;execution plan;large sets of;uniform sampling;execution plans;cost-based query optimizer;common practice;commercial database;sql server;random sampling
summary tables;summary tables;database;db2 udb;materialized views;aggregation queries
state transition;representation scheme;business logic;data model;state transition;instance;business rules
resource manager;access plan;optimizer;plans;access control;data store;database server;db2 udb;database;black box;meta data;lock;data type;local database;cost based;existing database
clustering;concurrency control mechanism;space utilization;tree index
multi-level;search space;retrieve images;content-based retrieval;information retrieval;instances;allowing users to;content-based retrieval;filtering algorithm
real customer;database server;database;large databases;databases
network traffic;internet traffic;traffic data;traffic analysis
clustering;clustering;data mining applications;data mining;large databases;large data sets;data records;relational dbms;dimensional data;em algorithm;efficient sql;large number of
web servers;databases
taking into account;database management systems;server logs;user activity
access methods;database;database research;standard benchmarks;indexing problem;query processing;buffer management;relational dbmss;data management
web applications;database systems
database systems;databases
data source;database applications;data exchange;query processors;data sources;data access;databases;database researchers;data access
traditional databases;information stored in;relational databases;database;query language;general-purpose;database application;relational database;schema information;data repositories;access information
query optimization;query forms;theoretical framework;open source software;fully operational;object-relational;prototype systems;language called;query formulation;1;object-oriented;3;2;4;databases;ad-hoc;optimization techniques;database researchers
data objects;domain-independent;dimensional space;web search engine;database;index structures;search engine;keyword-based;xml documents;data structures;search engines;databases;graph isomorphism
information content;completeness;instances;information theory;instance;data mining
required information;web pages;classification;knowledge representations;key words;1;wrapper generation;input query;user profiles;query processing;search engine;2;prior knowledge
index structures;1;join algorithms;3;7;6;join-algorithms;relational databases;query processing algorithms;temporal data;generic algorithm;spatial databases
error-prone;data analysis tasks;data cube;database;real-life datasets;data cubes;data dimensionality;ad hoc
data source;database;data cleaning;intermediate results;relational tables;transitive closure;real world;high similarity
multi-dimensional;completeness;data warehouse;data warehouses;query rewriting;knowledge representation and reasoning;logic-based;data warehousing;concept based;optimization techniques;modeling techniques
data distributions;data sets;sequence databases;data mining;data mining methods;sequential patterns;fp-growth;large data sets;data mining systems;mining sequential patterns;data characteristics;mining associations;data mining algorithms;synthetic data;pattern mining;large sets of;mining algorithms;data warehouse systems;real world applications;frequent pattern mining;mining frequent patterns;sequential pattern mining;object-relational dbms;great potential;transaction databases;databases;time-series;relational dbms;frequent pattern mining
increasing demand for;classification approach;meaningful information;database
java-based;workflow management systems;1;workflow management system;workflow engine;formal semantics
end-users;temporal data;database;temporal information;relational database systems;client applications;applications requiring
processing cost;false positives;query routing;query refinement;communication overhead;information sources;query refinement;query response time;user query;information providers;source selection;query routing;data intensive
multi-modal;spatial-temporal;template matching;feature extraction;time series;unstructured data;image sequence;retrieval engine;retrieval engine
united states;4,6;database;spatio-temporal;objective functions;1;3;2;5;temporal data;data entry
test data;search engine;text retrieval systems;text documents;plans;text retrieval;text retrieval;test collections;web site;text retrieval;test collection
web documents;web search;plans;search engine logs;question & answer;retrieval methods;large data sets;trec ad hoc;link-based;retrieval task;trec ad hoc;search topics;trec-9 web track;cross-lingual;web data;web searching
lower-level;ranking methods;user behaviour;retrieval systems;probabilistic approach;logical structure;information retrieval;information retrieval systems;low-level
wide range;completeness;database systems;data management;data management
web pages;web servers;html pages;web servers;database-backed;dynamic content;web site
data analysis;data cube;classification;multidimensional databases
operator;query execution plan;temporal databases
database research;auxiliary;database;transitive closure;relational calculus;data warehouses;view maintenance;incremental maintenance;traditional database systems;conjunctive queries;definition language;maintenance algorithm
world wide web;common features;database;data representation;xml query languages;xml content;query languages;comparative analysis;xml query language
similarity-based;similarity measures;multimedia data;user behavior;document similarity
object-relational;databases;event-driven;database research;database
database;transaction processing;serializability;standard sql
query languages;database;constraint databases
relational model;comparative analysis;data cubes;olap applications;databases
end-users;information contained in
relevance feedback;trec collection;query-biased;relevance feedback;document clustering;search effectiveness;clustering algorithm;clustering approach
instance;weighting scheme;real users
ir systems;test data;query plans
evaluation measure;desired level of;evaluation measures;information retrieval;average precision;error rate
highly relevant documents;ir evaluation;query types;case study;text database;average precision;ir methods
large number of;selected features;statistical model;topic detection;automatic generation of;automatically generate;hypothesis testing
broadcast news;event tracking;event tracking
document streams;parameter optimization;nearest neighbor;data collections;error rates;text categorization;event tracking;training instances;training documents;text classification
music information retrieval;retrieval effectiveness;text-based;information retrieval;comprehensive evaluation;databases
test data;spoken document retrieval;video-based;bayesian framework;average precision;error rate
image retrieval;world wide web;object recognition;web search engine;face detection;automatically determines;search engines;multimedia information;search engine
traditional information retrieval;reference collection;extra information;belief networks;information extracted from;ranking algorithms;retrieval performance;retrieval model;information derived from;network model;link-based;average precision
ir systems;text categorization;term selection;information retrieval tasks;formal description;information theoretic
business intelligence;business information;tool called;user studies
clustering;document collection;short queries;cross-language;query term;average precision;translation model;retrieval task;long queries;retrieval applications;query translation;cross-language information retrieval;retrieval effectiveness
manual effort;domain-specific;information retrieval;classifier;machine learning
entire document;domain independent;high accuracy
concise representation;text summarization;web pages;probabilistic models;summarization techniques;web pages;automatically generating;web page
text summarization;feature vector;classification accuracy;machine learning;term frequencies;supervised learning algorithm
spam filtering;training-corpus;standard benchmarks;keyword-based;cost-sensitive;naive bayesian;classifier;naive bayesian
naive bayes;boosting methods;text data;text categorization;frequency information;boosting algorithms;text filtering;naive bayes classifiers;boosting algorithm;machine learning algorithms;learning method;text filtering
relevant documents;relevant information;document filtering;filtering systems
natural-language;question-answering;large document collections;parameter settings;question-answering;question answering
statistical approaches;learning process;customer service;statistical techniques;statistical model;document retrieval;question-answering;automatically learn
evaluation methodology;domain-independent;document relevance;large-scale;question answering;natural language processing;qa systems;question answering systems;document retrieval;question answering;test collection
clustering algorithms;mutual information;information bottleneck;clustering methods;joint distribution;document clusters;document clustering;document collections;word-clusters;document clustering;word clusters;clustering procedure
inter-document similarity;singular value decomposition;probabilistic model;rates;basis vectors;semantic space;latent semantic indexing;average precision
clustering algorithms;additional features;test data;input features;compare favorably with;news sources;optimal combination of;information from multiple sources;statistical model;clustering performance;text features;document clustering;clustering algorithm;linear regression;topic detection;hierarchical clustering methods;linguistic features
information resources;retrieval effectiveness;database;results merging;retrieval performance;database selection;query processing;local information;distributed systems;database) systems
hill climbing;increasing) amounts of;content-based retrieval;solution space;similarity queries;multimedia information;hill climbing
unstructured text;replication;ir) systems;distributed information retrieval;exact match;return results;information retrieval systems;hit rate
classification;hierarchical classification;decision rule;hierarchical approach;svm) classifiers;vector machine;web content;hierarchical classification;web content;hierarchical structure;hierarchical structures;scoring rules
class information;classifier;document categorization
world-wide web;web pages;anchor text;textual content;spatial locality;search engines
internet search;document relevance;internet search;query formulation;cognitive load;search process;query reformulation;query reformulation;cognitive load;search effectiveness;search engine;phrase-based
world wide web;quality metrics;search effectiveness;distributed search;search effectiveness;information fusion;ranking algorithms;distributed information retrieval;term frequency;similarity-based;information retrieval systems;information quality
web documents;high quality;high-quality;web documents;relevant documents;total number of;link-based;world wide web;ranking documents;link analysis
neural networks;document classification;tf-idf;poster session;neural network;positive examples
related words;information visualization;poster session;search engine
singular value decomposition;traditional information retrieval;query formulation;test collections;poster session;performance gains;latent semantic indexing;latent semantic indexing
long-term;information filtering;user profile;user profiles;poster session
feature space;classification;decision tree;text categorization;l;vector space model;classification problem;nearest neighbor;3;2;4;high dimensional feature space;training data;vector machine;statistical model;classification method;bayes classifier;pattern recognition;feature vector;classification methods;text categorization;machine learning;poster session;training and test data;support vector machines;classifier
document ranking;retrieval effectiveness;retrieval systems;frequency information;poster session;test collection;query words;retrieval method
poster session
search term;web search;search terms;real-world;web sites;search engines;poster session
information retrieval;direct access to;ir research;user models;internet users;1;5;4;7;6;9;8;world wide web;search strategies;user-centered;ir systems;communication networks;11;10;12;high-priority;poster session;information overload
high quality;specific information;production process;multimedia information retrieval;information retrieval;automatically creating;5;poster session;information retrieval techniques;data files;data streams
clustering;speech recognition;broadcast news;topic detection;poster session;speech-recognition;topic detection
trec ad hoc;document relevance;poster session;7;spoken document retrieval
user ratings;document content;collaborative filtering;collaborative filtering;information retrieval;poster session;vector space model
support vector machines;poster session;classification;web news
initial conditions;information filtering;parameter settings;poster session;adaptive filtering;databases;heuristic approach
accessing data;multi-dimensional;analytic processing;traditional information retrieval;trec collection;multidimensional databases;information retrieval;text collections;text collection;poster session;unstructured data
filtering systems;correlation-based;java-based;collaborative filtering algorithm;data set;clustering techniques;poster session;prediction algorithms;support vector machines
world wide web;information content;graph structure;web graph;related information;probabilistic models;highly complex;web crawlers;search engines;latent semantic analysis;poster session;agent systems;dimension reduction;web documents
speech recognition;parallel corpus;spoken document retrieval;retrieval systems;data set;retrieval performance;rates;poster session;document expansion
probabilistic model;poster session;retrieval systems
context information;information resources;context-aware;mobile computing;information retrieval;poster session;information access;information filtering;selection process
theoretical foundation;poster session;information retrieval
service provider;supply chain;information systems;business process management;1;management systems;business processes;program committee;workflow management systems
web-based;information systems;web-based;web-based;privacy concerns;business objects;international workshop on;business intelligence;information systems;wide range;data mining;sales data;international workshop on;program committee
electronic commerce
data uncertainty;query languages;spatial data
spatial data;efficient storage;user-defined;database management system;access structures;indexing methods
query execution engine;database systems;query optimizer;database;performance gains;compression techniques;update operations;databases;storage manager;compressed data;relational database system
data warehousing;data warehouse
comparative analysis;xml format;data structures;5;comparative analysis;xml schema
data residing;approximate query answering;data mining techniques;data mining algorithms;data warehouses;large collections of;knowledge discovery;knowledge extraction;specific characteristics;data mining;identify patterns
information systems
databases;decision-making;minimum support;rule generation;16;mining association rules;database;categorical data;4;data items;quantitative association rules;case study;2;4, 13;related algorithms;association rule;quantitative attributes;additional information
instance;object level;database
mobile clients;data item;computing systems;data items;simulation experiments
query optimization;data management problems;database;data independence;materialized views;answering queries;data warehousing;database relations;data integration
database systems;transaction processing;decision support;database systems;databases
complex structure;database systems;semantic network;data model;object-oriented;relational schema;web-site;high-level
 1 ;oltp systems;decision support;web site;transaction processing
information management
clustering;distance functions;distance based;high dimensional;distance-based;low dimensional;6;dimensional data;high dimensionality;high dimensional space;nearest neighbor search
indexing method;clustering;multi-dimensional data
digital libraries;electronic commerce;information flow;domain-specific;information filtering
digital libraries;management systems;continuous media
knowledge sharing;knowledge-based;keyword-based;high- order;digital library;digital libraries;high-level;information space;knowledge-based systems
database design;semistructured data
web-based;data sources;xml enabled;data source;web interface
semi-structured data;search techniques;text documents;search process;information retrieval;search engines;sigir workshop on;data exchange;extensible markup language;large collections of;xml documents;retrieval algorithms;indexing structures;keyword search;search systems;textual data;ir systems;data representation;large sets of;query languages;xml format;database;indexing techniques;ir techniques;document similarity;future directions for;xml search;xml structure;vast amounts of
relevant information;database management;international conference on;international conference on;database management;information sources;knowledge representation and reasoning;information retrieval;user requests;user queries;knowledge representation;query answering;query answering;information retrieval;distributed data;stored information;information technology;natural language
database systems;database;1;2;databases;external data;external data;standard sql
data transformation;case study;data management techniques;complex tasks
hierarchical data;commercial applications;relational tables;xml documents;inference mechanism;wrapper generation
query evaluation;database;database;query evaluation;query equivalence;evaluating queries;databases;query containment;semistructured data;formally defined
query optimization;database;trade-offs;optimization problem;distributed database systems;optimization algorithms;desired accuracy;query results
multi-query optimization;buffer space;plans;related queries;np-hard;multi-query optimization;share common;sufficient condition for;minimum cost;subexpressions;database systems
complex queries;database systems;database;pattern queries;algorithm exploits;inter-dependencies;optimization techniques;search queries;databases;search algorithm;sequential pattern
database queries;conjunctive queries;database
search engines;data structures;document databases;instance
naive algorithm;database;aggregation function;database size;random access;threshold algorithm;simple algorithm;high-probability;aggregation functions
np-complete;complexity bounds;implication problem;xml document;linear constraints
xml document;regular expressions;path expressions;xml documents;path expressions;path expression;query languages
xml documents;data values;xml queries;type checking
xml documents;partial information;data warehouse;xml data;query language;incomplete information;xml document;relational databases;additional features
query answers;probabilistic semantics;data sources;extracting data from;multiple data sources
online algorithms for;aggregation function;competitive analysis;actual values;aggregation functions;minimum cost
query evaluation;regular-expression;relational calculus;relational operations;static analysis;string-matching;query languages;data complexity;low complexity;pattern-matching
game theoretic;bounded treewidth;17;10;k;game-theory;conjunctive queries
lower bound;np-complete;instances;upper bound;set-containment;join predicates
aggregation functions;bag-set semantics;aggregate queries;query equivalence;databases
query optimization;fast algorithms;selectivity estimation;database;approximate query answering;aggregate queries;summary statistics;query processing;provably optimal;range queries;wavelet-based
large volume;evolving data;temporal aggregation;data warehouses;index structure called;efficient computation;key range;handle arbitrary;temporal aggregates;key-range;aggregation queries;temporal databases

em) algorithm;naturally leads to;privacy-preserving;hardware technology;expectation maximization;privacy preserving;user privacy;large amounts of data;perturbed data;em algorithm;maximum likelihood;data mining algorithms;algorithm converges;information loss
frequent patterns;closed sets;condensed representation;original data;data mining problem;frequent pattern
database applications;random projections;dimensional euclidean space;database
string data;index structure;xml documents;indexing problem;databases;index structures
data objects;high degree of;long-running;constraints imposed by;fault-tolerant;atomicity;higher level
high-dimensional;multi-dimensional;data cube;efficient computation;search space;efficient computation;pruning method;data mining;promising candidates
data arrives;synthetic data sets;sliding window;large number of;limited space;convergence properties;building blocks for;network management;data stream;exact computation;data streams
parallel algorithms;low cost;trade-offs;high-dimensional datasets;comprehensive evaluation;br;load balancing
intrusion detection;high dimensional;outlier detection;detection problem;data set;outlier detection;proximity-based;dimensional data;high dimensional space
total number of;database operations;term-matching;information retrieval;oo, db2sql;text retrieval;onq;relational table
data structure;data sequences;worst-case;space requirements;input sequence;space requirement;worst case
web “crawlers;hidden web;search interfaces;hierarchical classification;database;web sites;web-accessible;classification process;low overhead;databases;high classification accuracy
clustering;hierarchical clustering algorithms;compression methods;clustering structure;data bubbles;random sample;hierarchical clustering;post-processing;data items;data set;hierarchical clustering algorithm;step procedure;rates;large databases;data bubbles;data item;compressed data;clustering result;hierarchical clustering methods;random sampling
error-prone;false positives;target class;learning rules;network intrusion detection;high recall;real-life;learning models;rare classes;high precision;synthetic data
declarative query language;sql queries;xml tags;xml view;xml views;relational database;relational databases
filtering algorithms;publish/subscribe systems;operator;data structures;query processing;long-term
query optimization;query optimizer;database applications;real-world;query plan;temporal queries;cost-based;query processing;databases;query languages
memory space;index structures;main memory;block size;times faster than
singular value decomposition;dimensionality reduction technique;database;fourier transform;index structure;time series;reconstruction error;dimensionality reduction techniques;distance measures;similarity search;databases;high dimensionality;dimensionality reduction
tree structures;main-memory;index structures;databases;index nodes;main-memory;fixed-size
human-generated;automatic segmentation;training examples;real-life datasets;probabilistic model;rule-based;domain-specific;rule-learning;data sources;extraction methods;external data;databases;multiple sources;hidden markov models;automatically extract
databases;estimation method;large-scale;retrieving information from;database;user query;search engines;text databases;search engine;local database;query terms;retrieval effectiveness
query optimization;multi-dimensional;complete set of;correlation patterns;approximate query answering;data set;high-dimensional data sets;fundamental problem;real-life data sets;dimensional data;space partitioning;histogram-based;low-dimensional;data distribution
selectivity estimation;query workloads;histogram construction;workload-aware;data sets;selectivity estimates;real-world data sets;query results
data values;dynamic programming;global optimal;highly skewed;single attribute;global optimization;single-attribute;limited number of;greedy algorithm;data distributions;error reduction;global optimization
main-memory;main memory;data transfer;range scans;databases
data structure;systems support;collaborative filtering;constraint optimization problems;indexing techniques;real life datasets;query capabilities;hamming space;query processing in;data sets;indexing scheme;indexing technique;object relational
materialized view;database systems;synthetic datasets;optimization problems;ranked queries;everyday life;materialized views;space constraints;database management system;queries efficiently;preference queries;excellent performance
query optimization;database systems;main memory;heuristic algorithms;compression based;query execution;attribute-level;query optimizer;compression techniques;query plan;cost-based;rates;numerical attributes;plans;query optimization;compression method;query optimizers;provably optimal;compression methods;database;disk access;relational schemas;queries involving
approximate answers;search space;data collection;digital data;data samples;data-mining models;compression techniques;regression tree;accurate classification;optimization algorithms;data sets;data tables;compression schemes;real-life data sets;rates;construction cost;error bounds
taking into account;sampling-based;decision support;aggregate queries;optimization problem;microsoft sql server;database;answering queries;data mining tools;aggregation queries;data distribution
multi-query optimization;materialized views;view maintenance;materialized views;view selection;data warehouse systems;commercial databases;subexpressions
base relations;plans;minimum number of;search space;cost model;finding optimal;query optimizers;closed-world;open-world
optimizer;query optimizer;materialized views;heuristic rules;materialized views;scalable solution;aggregation queries over;query processing;cost based;microsoft sql server
user requests;memory requirement;memory requirements;user request;buffer size
data values;real-world;special case;algorithm performs;performance gains
data sharing;web documents;control policy;poor performance;data warehouses;ad-hoc;performance gain;data warehouses;on-line analytical processing;mobile users;central server;olap operations;enterprise data;network traffic;decision making
data analysis;data points;database;point sets;data sets;sort order;large data sets;similarity search;vector space;grid-based;data mining;main memory;dimensional data;scheduling strategy;similarity join
high-dimensional;prediction techniques;index pages;query execution;index structures;large number of;skewed data;dimensional data
data structure;multi-dimensional;aggregate queries;exact answer;multi-resolution;tree structure;multi-resolution;databases
xml data;data representation;heterogeneous systems;xml query language;xml “views;xml documents;queries over xml;data sources;update operations;relational database;markup language;xml view;data storage
query execution;query optimization;xml data;relational database systems;information retrieval;join algorithms;concurrency control;relational database management systems
xml documents;xml data;incoming documents;html pages;continuous queries
frequency distribution;database;approximation error;frequency domain;spatial objects;von neumann;histogram based;wavelet based;query plans;query optimizers;size estimation;higher dimensional;real datasets;range queries;sampling technique
join operations;probabilistic models;relational models;multiple attributes;selectivity estimates;multiple relations;selectivity estimation;approximate query answering;relational domains;complex queries;frequency distribution;real-world;unified framework;database query processing;probabilistic graphical models;queries involving;selectivity estimation;database;multi-attribute queries;cost-based query optimization;bayesian networks;databases;statistical models;join queries
mining algorithms;database;association rule mining;transactional databases;skewed;1;knowledge discovery;distributed environments;5;association rules;theoretical results;distributed data
data-driven;complex queries;data source;data transformation;declarative queries;data examples;data-intensive applications;schema mappings;database researchers
fast algorithm;database;tree patterns;tree pattern;tree-structured data;tree-structured;equivalent query;algorithm called;databases;tree pattern queries;pattern matching
data source;machine-learning techniques;high degree of;multiple types of;domain constraints;manual construction of;xml documents;semi-automatically;data sources;data-integration;machine-learning;machine learning techniques;semantic mappings;structural information;real-world domains;source schemas;matching accuracy
data values;management) systems;data warehouse;data stored;sql queries;post processing;data store;database;modeling techniques;data set;commercial applications;data warehousing;databases;microsoft sql server
business processes;database contents;web pages;end users;database;independent components;web sites;dynamically generated;application servers;web servers;database content;web content;dynamic content;databases;content providers;web traffic
data management system;sensor networks;distributed systems;data management;database
special case;application integration;data warehousing;data integration
data mining;lessons learned;data analysis;data mining methods;data mining;data mining techniques;tool called;related data;automated methods;data analysis;analysis task;database;generation process
legacy systems;data interchange;data integration;integrate data from
world wide web;data warehousing;data management;databases
sql server;optimization approach;execution strategies;execution plans;microsoft sql server;query processing
query optimization;integrity constraint;optimizer;database;optimize queries;cost estimation;query optimization techniques;long-term;cost-based;data mining tools;commercial systems;query plans;query optimizers;query rewrite;soft constraint;optimization strategies;soft constraints;key constraints;short-term
instance;fine-grained;database systems;databases
user interface;meta search engine
business processes;correlation based;event correlation;application development;impact analysis
image retrieval;human perception;labeled instances;sampling process;similarity measurements
response times;database systems;spatial queries;databases;database integration;data management
parallel database;statistical queries;random walk;query language;time series
xml data;data exchange;relational systems;xml data management
security issues;database researchers;data security
specific applications;speech recognition;dimensionality reduction technique;dimensional space;random projection;financial data;video frame;high dimensionality;finding similar;dimensionality reduction;pattern matching;data bases;similar behavior;dimensionality reduction techniques;management systems;ibm db;fourier transform;commercial databases;web data;query sequence;high-dimensional;signal processing;wavelet decomposition;lower dimensional space;time series;main idea;data stored in;indexing problem;discrete cosine transform;data type;temporal data;markov model;object based;data collection;distance function;stock market;medical data;time series;data mining research;environmental data;databases;real numbers;indexing techniques;longest common subsequence;similarity measures
semantic integration
multimedia content;multimedia content;schema based;multimedia databases;large number of
search results;document content;general purpose;domain specific;search engines;document collections;domain specific search engines;great promise
active databases;database systems;real-world;conflict resolution;active databases;concurrency control;data placement;multi-media
data objects;high-speed;data-access;database;resource management;information access;concurrency control;applications involving
security requirements;database systems;trade-offs;data consistency;database systems;transaction processing
main memory;unique features;database;derived data;databases
textual documents;database;database systems;special attention;text retrieval
software systems;storage model;personal data
object technology;software systems;database
world wide web;web applications;database;database applications;database content;data types
potential impact
information retrieval models;information retrieval;information retrieval
classification scheme;occurrence frequency;inter-relationships;classification;database;text documents;text retrieval;major components;takes into account;classification approach;classification algorithm
high-dimensional;dimensionality reduction techniques;high dimensional;lower dimensional space;semantic concepts;data set;similarity search;search quality;similarity indexing;implementation details;dimensionality reduction;positive effect
relevance feedback;document collection;query-expansion;document relevance;relevant documents;document summaries;query-expansion;term selection;information retrieval;relevance feedback;term-selection;summarization techniques;trec-8 ad hoc;retrieval task;document expansion;expansion terms;probabilistic retrieval model;query-biased

text summarization;summaries generated;weighting schemes;highly ranked;latent semantic analysis;summarization methods;ir methods
text summarization;test data;source documents;text categorization;evaluation scheme;document retrieval
retrieval effectiveness;vector-space;relevance feedback;evaluating queries;computational cost;information retrieval systems
relevance scores;pruning methods;index entries;retrieval results;index size;information retrieval systems;term-based
content analysis;image indexing
world-wide web;evaluation methodology;sampling techniques;relevance judgments;human judgments;search engines;text retrieval;retrieval systems;human relevance judgments;information retrieval systems;test collection
relevant pages;document sets;trec-9} web track;relevance judgments;highly relevant documents;discounted cumulative gain;search engine;retrieval techniques
precision-recall;relevance judgments;weighting schemes;document-term;highly correlated;scoring function;term weighting scheme;information retrieval;human relevance judgments
cross language information retrieval;query language;cross language retrieval
machine translation;high-quality;dictionary-based;statistical models;query translation;cross-language information retrieval
machine translation;translation probabilities;mixture model;probabilistic model;parallel corpus;retrieval results;cross-lingual information retrieval;cross-lingual;generative model;retrieval results;query translation
query language model;user preferences;markov chain;trec collections;language models;link analysis;web data;language model;tf-idf;ranking function;information retrieval;vector space;retrieval model;word senses;language modeling approach;risk minimization;short queries;markov chains;query expansion;decision theory;social networks
training data;language models;probabilistic models;relevance models;highly accurate;information retrieval;relevance model;language modeling approaches;language modeling
text-classification tasks;statistical properties;support vector machines;sufficient conditions for;generalization performance;unlike conventional;text-classification problems;text classifiers;text classification;feature spaces;support vector machines;statistical learning
local optimization;nearest neighbor;rank-based;text categorization;score-based;coarse-grained;classifier;decision making
distributional clustering;classification;vector machine;text categorization;data set;feature selection;svm) classifier
inter-document similarity;semantic indexing;semantic similarity;document representations
data values;intermediate data;structured documents;query language;textual similarity;general-purpose;information retrieval;xml documents;xml information retrieval;xml data;relational database;xml document;ranked queries;operator;keyword search;query languages;xml query language;relational data
query optimization;database;document-centric;query language;xml query languages;information retrieval;xml documents;logic-based;ir models
relevance feedback;query modification;query reformulation
pseudo-relevance feedback;highly relevant documents;term selection;compression ratio;information retrieval
question-answer;multi-party;relevance ranking;automatic generation of
graph structure;web graph;mutual reinforcement;html pages;increasingly complex;web search engines
end-users;mental models;web search engines;search engines;query results;user study;web search engines
real world;test collections;information retrieval systems;real users;relevant documents
image retrieval;content based image retrieval;user-centered;target image;search process;user centered;image data;content based image retrieval;search tools
search space;link analysis;dynamic nature of;limited bandwidth;web crawlers;computational resources;search engines;topic-specific;web page
document content;anchor text;search engines;web site;link-based;ranking methods;web search engines;document set
link analysis
score distribution;vector space;mixture model;relevance information;relevant documents;score distributions;search engines;normal distribution;search engine;trec-4 data;similar characteristics
upper bounds;search engines;bayesian inference
contingency table;estimation method;optimization problems;classification tasks;score distributions;topic detection;high accuracy;2;document scores
document collection;meta-learning;category based;learning phase;meta-learning;specific properties;text categorization;meta-model;category-specific;real-world;document categorization;document set
utility function;filtering systems;maximum likelihood estimation;relevance information;relevant documents;retrieval models;relevant document;explicitly models;maximum likelihood
topic tracking;topic tracking;topic detection;document clustering;supervised clustering
large volumes of;textual information;document filtering;case studies;comprehensibility;text classification;information-retrieval applications
semantic associations;high dimensional;cognitive model;inference mechanism;information flow;distance metric
speech recognition;language models;smoothing methods;rank documents;language model;information retrieval;test collections;retrieval performance;language modeling approaches;maximum likelihood;ad hoc information retrieval;model estimation;data sparseness
aspect model;topic segmentation;unstructured text;automatic speech recognition;topic segmentation;time series;sequence data;hidden markov model;7;5;latent topic
graph-theoretic;topic hierarchies;automatically generating;language model
large volumes of data;passage retrieval;multiple times;question-answering;question answering;web data;test collection
large text collections;passage retrieval;question/answering;machine learning techniques;unique characteristics;question/answering
task type;question answering
search terms;spatial proximity;concept-based;clustering technique;query interface;information retrieval;weighting scheme;natural language
search results
text mining;automatically extracting;web pages;query terms
source code;user interface;classification algorithms;information management;user feedback
training corpus;query words;parallel corpora;cross-language
latent semantic indexing;document space;latent semantic indexing;high computational cost;computational costs
document content;anchor text;web collection;probabilistic model;link information;unified framework
information-retrieval;real-world;high-level;training data;automatic speech recognition
summaries generated;text summarization;hidden markov model;hidden markov models
web-based;document relevance;helping users;query-biased;search result;web page;query-biased
query optimization;vector space;query execution plan;information retrieval
topic detection;hierarchical representation;topic segmentation;text-based;topic segmentation
relevance feedback;relevance feedback;test collections;negative feedback
1;data fusion;classification tasks;event detection;broadcast news;classification process;topic detection;document representations;evaluation methodology;semantic representation;document representation
digital library
information extraction from;matching techniques;web pages;extracting data from;web page
target language;web search;web-search queries;query generation;query-generation;automatically generating;pseudo-relevance feedback;retrieved documents;classifier
image similarity;relevance feedback;retrieval performance;retrieve images;similarity measures
image retrieval;personal preferences;object-based;preference learning;database
web document;web pages;classification;classification approaches;image-based;text-based;classifier;classification performance;hierarchical structure;hierarchical classifier;image classification;web document;classification approach;multimedia data;textual features
chi-square;document classification
clustering;similar queries;automatically discovering;content words;user feedback;search engine;question-answering

search services;digital library;digital library
database;z.50;cross-domain;information retrieval;resource discovery;retrieval algorithms;databases;structured data
deep web;search engine
web browser;web environment;user modeling;search queries;natural language processing
acm symposium on;world wide web;document processing
knowledge discovery;acm sigkdd international conference on;data mining;drug design;knowledge discovery;data mining;program committee;acm sigkdd international conference on;program committee
data collection;large data sets;large scale;data types;machine learning;knowledge discovery;information flow;high-throughput;gene expression analysis
extracting information from;machine learning methods;database;web sites;machine learning;learning theory;artificial intelligence
knowledge sharing;user interactions;large-scale;data mining;data mining;business intelligence;customer support;user experiences;massive amounts of data
risk management;data-rich;support vector machines;predictive modeling;customer relationship management
data mining;machine learning;years ago;data mining
clickstream data;electronic commerce;web servers;web server;emerging patterns;web site;server logs;data mining;log data
recommender systems
skewed data;clickstream data;maximum likelihood estimation;correlation coefficient;outlier detection;data mining tasks;skewed;probability distribution;sales data;special case;real world data sets
classification;classification trees;data mining applications;data mining;tree-based;data mining
probabilistic modeling;transaction data;predictive power;mixture model;large volumes of;transaction data;data mining applications;web sites;linear combination;modeling techniques;market basket data;outlier detection;web logs;data set;association rules;interactive visualization;automatically generate;histogram-based;mixture models;transaction-data
join algorithm;replication;spatial join;high-dimensional data sets;generally applicable;data replication;algorithm named;large data sets;data sets;cost-model;principal component;analytical model;dimensional data;high-dimensional feature spaces;similarity-joins;distance computations;similarity join;memory-requirements;memory requirements;high dimensional spaces
markov random field;viral marketing;collaborative filtering;database;databases;social network;data mining
mining algorithm;frequent item sets;item sets;database;item set;empirical bayes;empirical bayes;databases;market basket;interestingness" measure
feature space;test set;svm classifiers;classification;linear program;vector machine;large datasets;8, 9;simple algorithm;svm classifier;classifier
data point;feature space;regularization;classification problem;data points;49;linear combination;18;basis functions;data mining;28;high-dimensional feature space;data sets;rates;data mining;classifier
decision trees;large databases;random sample;stationary distribution;data streams;machine-learning algorithms;decision tree learner
multi-dimensional;business data;real datasets;decision-making;multi-dimensional data;query points;multiple dimensions;visualization technique;multiple factors;effective tools;knowledge discovery process
singular value decomposition;database;large databases;index structure;query response times;dimensionality reduction techniques;efficient indexing;similarity search;fourier transform;high dimensionality;fourier transform;dimensionality reduction
clustering;high-dimensional;outlier detection;frequent updates;distance-based;randomized algorithms;databases;applications involving;nearest neighbor search
data mining;version space;skewed;minimum support;database
user preferences;web sites;keyword-based search;web sites;information extraction
clickstream data;models learned;browsing behavior;models built;web site;data preprocessing;prediction tasks;data collected;user-centric;incomplete data
transaction data;data structure;selectivity estimation;real-world;compressed representation;original data;query-answering;data set;frequent itemsets;data sets;markov random fields;model-building;query answering;bayesian networks;compressed data;mixture models;memory footprint
world wide web;data obtained from;high-quality;future events
pair-wise;nearest neighbor;feature vector;classification;classification trees;real datasets;multidimensional datasets;finding patterns;data mining;feature vectors
sparse data;collaborative filtering;selectivity estimation;databases;database records;real-world applications;frequent itemsets;efficient discovery of;web browsing;clustering high-dimensional data;algorithm exploits;clustering algorithm;traditional clustering algorithms;transactional data
probability estimates;taking into account;training examples;random variables;decision tree;misclassification costs;making decisions;class membership;sample selection bias;data mining;test examples;bayesian learning
case study;database;similar results;data mining
clustering;dimensional data;high dimensionality;clustering algorithms;high dimensional
real data sets;image data sets;privacy concerns;original data;application domains;data set;data sets;data mining algorithms
text mining;human subjects;text-mining;knowledge-base;human judgments;data-mining methods;correlation coefficients
categorical datasets;categorical data
random projection;lower-dimensional;random projection;text documents;computational savings;text data;theoretical results;random projections;random projections;principal component analysis;dimensionality reduction methods;high dimensionality;information retrieval;dimensionality reduction
induction algorithms;classification;high dimensional;training examples;vector machine;real-valued;projection-based;high-dimensional spaces;dimensional data;svm classifiers;support vector machines;margin;classification tasks
user's interests;user profile
clustering;data mining applications;summary statistics;traditional clustering algorithms;data mining;likelihood function;clustering;clustering method;noisy data;8;data mining problems;large database;data mining tool;cluster membership;dense regions;data records;discover patterns;very large datasets;clustering algorithm;distance measure;categorical attributes;hierarchical clustering algorithm;real life;probabilistic model
graph partitioning;clustering;document collection;clustering problem;np-complete;singular vectors;clustering documents;document clustering;document matrix;bipartite graph;clustering algorithm
graph partitioning;clustering;graph structures;breadth-first search;web graph;spectral methods;web graph;laplacian matrix;connected components;depth-first search;objective function
clustering;clustering;spatial data;weighted graph;significant patterns;random walks;cluster analysis;spatial databases
clustering;regression trees;classification problem;regression problems;rule-based;ensemble classifiers;decision-rule;learning method
data objects;cluster-based;outlier detection;large number of;large databases;local neighborhood;local outliers;outlier detection;computationally expensive;video surveillance;data mining;credit card fraud;mining algorithm;numerous applications;nearest neighbors;outlier factor
clustering;clustering algorithms;clustering algorithms;supervised methods;instance;iterative optimization;clustering methods;analyzing data;supervised learning;unsupervised techniques
transaction data;approximate answers;database;data mining;customer behavior;high dimensional
large-scale;classification technique;test set;communication overhead;database;classification accuracy;weighted voting;boosting algorithm;data set;large data sets;databases
semantic classes;high-dimensional space;natural language text;classification;textual information;large number of;tedious task;low frequency
information retrieval;natural language processing;inference rules;dependency trees;artificial intelligence;inference rules
predictive models;pruning techniques;real-life datasets;significant rules;data mining;association rules;data mining algorithms;discovered rules
real-life datasets;association rules;total number of;application domain;huge number of
expected number of;large numbers of;dynamic programming;monte carlo;web access;sequence data;data mining applications;simple heuristics;computationally intensive;bayesian approach;maximum likelihood;markov chain
data-driven;entropy-based;classification;genetic algorithm;instance;computationally expensive;greedy search;finding similar;search technique
computing infrastructure;database;large number of;mining frequent;spatial database;instance;service providers;euclidean distances;location-based services;location-based;location information;spatial databases
ensemble learning;training data;generalization performance;real datasets;classification accuracy;boosting algorithms;online algorithms
data sets;prefix tree
outlier detection;graph-based;cost model;data sets;spatial outliers;statistical tests;distance metric;graph structured data;fast algorithms
training data;classification;large-scale;ensemble methods;streaming data;machine learning;fast algorithm;decision tree;data mining;fixed-size
association rules;3;data sets;interesting rules;numeric data
unlabeled data;unsupervised learning;network intrusion detection;detection process;detection algorithm;detecting outliers;labeled examples;supervised learning
occurrence frequency;computational biology;large margin;surprising patterns;periodic patterns;information gain
support levels;real-world datasets;real datasets;artificial data;association rule;real world
ad-hoc;business intelligence;statistical models;predictive accuracy
web site;noise reduction;web site
statistical methods;nearest neighbor;united states;information services;real world;ad hoc
training data;predictive accuracy;learning algorithm;bayesian classifiers;data mining;statistical significance;supervised learning;classifier
process model;data cleaning;detection approach;machine learning;preprocessing phase;data mining models;data mining problems
predictive models;total number of;data mining techniques;data mining techniques;classification trees;regression models;information stored in;forecasting accuracy
text mining;customer relationship management;business decisions;valuable information;survey data;statistical learning
clustering;text mining;web usage;classification;tree-based;sequential patterns;data mining research;data mining projects;web usage mining
confidence intervals;prediction-model;selection criteria;evaluation-criteria;case study;prediction models
knowledge bases;cost effective;customer service;text clustering;knowledge base;distance metric;knowledge base
web-based;document collection;navigation patterns;user session;database;related information;customer service;user navigation;finding optimal;user interaction;users interact with;desired information;knowledge base
web documents;data mining;web log mining;web caching;web logs;web-document;higher accuracy;access patterns;prediction models;web-access
decision tree construction;high-quality;kdd conference;acm sigkdd international conference on;data mining;frequent-pattern mining;knowledge discovery;distributed environments;web mining;data mining;specific topics
xml documents;relational database system;xml storage;xquery queries
user profile;user click;extensible markup language;user profiles;stream data;mobile devices;user interests;web site;user profiles;click stream;resource description framework;privacy preferences
xml-based;data processing;multi-tier
semantic interpretation;business applications;ontology-based;business rules;active functionality

conceptual modeling;conceptual modeling;business process;tool called;meta-model;business process;business processes
integrating information from;ontology-based;information resources
electronic commerce;information discovery;data repository;data management;business data
dynamically changing;multi-agent system
process model;supply chain;information sharing;knowledge sharing;supply chain;software components
data interchange;application integration;generic framework;web services
typically performed;msv, hp, hvp, amn+01a, amn+01b;xml documents;ffm;xml document;generated dynamically;working group;xml schema
bern;application servers;multi-tier
conference series;database;remote sensing;database research;large-scale;poster session;databases;high-energy physics;information technology
acm sigmod;conference series;program committee;information systems;databases

database;change detection;generic framework;data mining;data mining models;maintenance algorithm;mining data streams;generic algorithm
clustering;mining results;intermediate data;computational power;objective functions;large amounts of data;data mining;association rules;data mining problem;data mining algorithms;automated techniques
data arrives;clustering data streams;stock-market;data streams
11;10;data base;mining association rules;mining process;mining results;7;8;association rules;incremental-mining
hand-held;quadratic programming;decision trees;data mining techniques;financial data;stock market;limited bandwidth;user-interfaces;data mining system;data mining;wireless network;ad hoc;wireless networks;multi-media
biological databases;drug design
multimedia data mining;international workshop on;multimedia data mining
knowledge discovery;acm sigkdd international conference on;data mining;workshop on data mining;workshop on data mining
databases;databases
ecml/pkdd;knowledge discovery in databases;visual data mining;machine learning;international workshop on
data rich;temporal dimension;threshold values;discovered rules
data mining;world wide web;databases;web applications;web sites;semantic web;knowledge discovery;semantic web;resource description framework;easy access;web resources;resource description framework;web data;information source
search space;frequent patterns;level wise;long patterns;databases;level-wise
statistical methods;predictive models;neural networks;classification;real-world data mining applications;categorical attributes;database;large number of;logistic regression;statistical method;categorical data;hierarchical structure;prediction problems;high-cardinality;relevant information;empirical bayes;categorical data
sequence data;model-based clustering;cluster analysis
data mining and knowledge discovery;data mining and knowledge discovery


automatic extraction of;digital library
search results;cross-language information retrieval;test collection;cross-language;text retrieval
international conference on;end users;database applications;international workshop on;knowledge management;web information and data management

description language;world wide web;increasing importance;web server
data stream systems;query languages;query processing;data processing;data streams
tree structures;web pages;web information extraction;web information extraction;theoretical foundations;information extraction from
dimensional data;data blocks;multidimensional data;range queries;maximum number of
domain-independent;database systems;index structures;keyword-based;search engines;keyword search
xml document;memory constraints;finite-state;xml documents;memory requirement;additional information
xpath queries;xml data;query language;xml document;key constraints;negative results
regular tree;complexity classes;tree-structured data;xml query language;computational power
redundant information;xml documents;functional dependencies;relational databases;normal form;functional dependency
distributed computation;computational power;web queries;web queries are;computation model;web queries
optimization techniques;physical characteristics;obtained by applying;optimization algorithm;main memory;cost model;case study;query plans;selection conditions;disk-based;data-intensive
multi-dimensional;spatial datasets;spatio-temporal;main memory;sum queries;query performance;efficiently computing;index nodes;disk-based
decision support system;query results;range-sum queries;wavelet based
data collection;data warehouses;database applications;dynamic environments;data sources;schema information;missing information;relational views
meta-data;objective functions;view update;source database;data sets;data provenance;databases;relational databases;objective function;np-hard
query optimization;selection problem;lower bound;database schema;query workloads;data warehouse;database contents;cost model;join algorithms;materialized views;information integration;databases;query workload;conjunctive queries;relational databases;selection algorithm
analytic processing;directed graph;implication problem;instances;views defined;multidimensional space;multidimensional data
real data sets;query evaluation;dynamic programming;cost-based optimization;histogram construction;olap queries;data warehousing;fast algorithms;olap applications
past queries;computational geometry;extended abstract;distance based;database;database applications;spatio-temporal databases;future queries;query language;moving object;constraint databases;moving objects;nearest-neighbor queries;evaluation techniques;databases;continuous functions;focus primarily on;moving object databases;future" queries;distance" function
data structure;query rewriting;database;knowledge compilation;complexity classes;knowledge compilation;databases;relational databases;view synthesis
maximally-contained rewritings;answering queries;answering queries;conjunctive queries
continuous data streams;data streams;memory requirements;instances;bounded memory;conjunctive queries
data residing;special attention;data sources;data integration systems;data integration;real world applications;data integration;processing queries
query optimization;computational complexity;question arises;database;constraint satisfaction;regular path queries;mobile computing;view-based;data warehousing;query answering;data integration
xml data;regular expressions;np-complete;upper bounds;entire document;multi-attribute;relative constraints
xml databases;xml trees;support queries;xml tree
query optimization;join order;optimization problem;np-hard
query optimization;cost functions;plans;database;expected cost;chs
world wide web;web sites;world wide web
intelligent systems;artificial intelligence;information exchange;artificial intelligence

description language;world wide web;increasing importance;web server
web sites
years ago;information retrieval;annual international acm sigir conference on;technical program;program committee;information retrieval
ir research;statistical models;automatic indexing;meta-data;text data;information retrieval;search engines;retrieval models;retrieval performance;web retrieval;user-oriented
years ago;document classification;input data;effectively identify;natural language;natural language
term weights;short queries;query execution;highly competitive;vector-space;large document collections;rates;long queries;web data;web retrieval
14;large-scale;relevant documents;term-frequency;data set;document frequency;original document;search engine;promising candidates;related documents;web page
sampled data;results merging;training data;search engines;distributed information retrieval;search engine results;databases;distributed ir
web pages;language model;content features;prior probability;ad hoc;ad hoc search;world-wide-web
relevant documents;query term;statistical language models;information retrieval;explicitly models;language modeling approach;information retrieval systems;query terms
sparse data;language model;smoothing methods;translation model;information retrieval;document collections;document pairs;smoothing method;conditional probability;vector space model
document collection;reference model;dirichlet prior;language model;smoothing method;language models;information retrieval;retrieval performance;databases;special case;mixture models;estimation methods;test data
statistical significance;information seeking;retrieval results;web search;relevant documents
access pattern;prediction quality;path-based;user's preferences;information space;markov model
unstructured text;document collections;information resources;statistical measures
information filtering;similarity metric;language models;adaptive filtering;relevant documents
information filtering;topic tracking;topic tracking
text classification;text documents;classification systems;text filtering
text summarization;classification models;unlabeled data;costly process;learning paradigm;increasing demand for;automatic text summarization;data sets;labeled data;machine learning techniques;desirable properties;semi-supervised;supervised learning
clustering;clustering algorithms;text summarization;web documents;text documents;spectral graph;mutual reinforcement;clustering quality
filtering systems;document understanding;document summarization;classification;document sets
higher precision;information bottleneck;document classification;local maximum;naive bayes classifier;clustering methods;clustering algorithm;margin;space complexity
factor analysis;document sets;text categorization;generalized eigenvalue problem;distinctive features;classifier;document set
training data;feature set;classification;selected features;genre classification;document frequency;natural language processing
online algorithms for;reuters corpus;learning algorithms;labeled documents;ranking algorithms;perceptron algorithm;reuters-21578 corpus
cross-language;query expansion;relevance feedback;instances;parallel corpora;performance gains;query translation;cross-language information retrieval
target language;translation model;statistical model;source language;statistical models;cross-language information retrieval
target language;average precision;query expansion;cross-language retrieval;parallel corpus;relevance models;topic models;topic model;formal framework;cross-lingual;formal model;source language;query translation;cross-language information retrieval;language modeling
mutual information;translation model;query translation;cross-language information retrieval
em) algorithm;document corpus;gaussian mixture model;feature set;similar results;accuracies;refinement process;clustering method;clustering accuracy;clustering process;document clusters;document clustering;high accuracy;document clustering;model selection;expectation-maximization
clustering;clustering;clustering algorithms;evaluation measure;data elements;retrieval results;document clustering;cluster quality;higher quality;clustering tasks;evaluation measures;clustering algorithm called;information retrieval tasks;evaluation methodology
context-sensitive;classifier;text classifiers
auxiliary;space overhead;inverted files;search engines;queries posed;search engines
query evaluation;space overhead;inverted lists;large collections of;compression scheme;compression schemes
term weights;document collection;data mining technique;retrieval effectiveness;index terms;answer set;information retrieval;ranking mechanism;association rules;trec-3 collection;average precision;vector space model
factor analysis;collaborative filtering;collaborative filtering;ubiquitous computing;great potential;missing data;privacy risks
search algorithms;disk based;collaborative filtering;times faster than
performance metric;probabilistic framework;recommender systems;naïve bayes classifier;real-world applications;data set;recommending items;cold-start
automatic indexing;optical character recognition;document images;term selection;text collection;retrieval effectiveness
parallel corpus;cross-lingual;relevance judgments;search strategies;retrieval results
clustering;trec-2001 data;retrieval effectiveness;information retrieval
search results;trec data;relevant documents;irrelevant documents;information gain;scoring function;fully automatic;search engine;query refinement;query terms;related words
query language model;trec data;relevance information;language model;relative entropy;query performance;average precision;test sets
language modeling
error rates;rates;highly variable
relevant documents;highly relevant documents;relevance criteria;information retrieval;lessons learned;relevance criteria;test collections;relevance assessment
probabilistic approach;probabilistic model;large number of;massive datasets;generative model;music retrieval
network model;context information;video retrieval;video retrieval;contextual information
trec data;word-segmentation;dictionary based;information retrieval;language independent;information retrieval systems;word segmentation
classification;document classification;user interface;key features;vector space model;classification algorithm
3;1;ranking algorithms;highly correlated;link-based;unified framework;link analysis
high-dimensional;decision trees;text categorization;sample size;predictive performance
text document;customer service;classification;independent component analysis
web search results
question answering;data collections
selection problem;retrieval effectiveness;database;global search;retrieval performance;databases;user-centric;structured data
retrieval effectiveness;document sets;data set;web search engine;retrieval task;retrieval task;retrieved documents;test collection
search algorithms;text retrieval systems;text documents;query sets;trec ad hoc;data structures;retrieval performance;term frequencies;term frequency;term frequencies;2, 3
traditional models;information retrieval
document collection;length normalization;average precision
interface design;user-centered;user-centered;user requirements;cross-language information retrieval;data collected;cross-language information retrieval
tracking algorithms;topic tracking;topic detection
estimation method;parameter estimation;training documents;naive bayes;naive bayes;naive bayes classifiers;text classifiers;text classification
majority voting;classifier;concept-based
higher precision;higher precision;specific properties;retrieval results;low recall;pseudo-relevance feedback;retrieved documents
distributed search;regression algorithm;probabilistic approach;test collections;distributed information retrieval;poster session;logistic regression;search evaluation;distributed ir
manually assigned;web access;machine learning techniques;wide range;automatically generated;natural language processing;digital libraries;user study
cost function;topic detection
language called
text summarization;unsupervised learning;decision tree;human judgments;clustering framework;supervised learning
clustering;content-based music;text-based
fusion strategies;data fusion;retrieval systems;text fragments;search engines;meta-search;document similarity
iterative process;data cleaning;text categorization;machine learning;text corpora;natural language texts;automatic generation of;information retrieval
classifier;topic structure;topic structure
language model;meta-data;language model;clustering method;information retrieval;meta data;category information;retrieval accuracy
world-wide web;world wide web;web pages;traditional information retrieval;automatic evaluation;query log analysis;search results;search engines;ranking methods;search services;web search engines
power law;connected components;web search;test collection
ir systems;dictionary-based;4;language modeling
retrieval effectiveness;test collection
keyword queries;boolean queries;construction cost;intermediate results;inverted files;keyword-based;information retrieval;1;information retrieval;extensive simulations
dictionary-based;retrieval performance;cross-language information retrieval
textual descriptions;language models;probabilistic models;retrieval tasks;information retrieval;multimedia retrieval;gaussian mixture models
statistical information;stochastic process;keyword extraction
text mining;machine translation;bipartite graph;multilingual documents
web documents;highly relevant;web search;implicit feedback;main features;search engine;web document;query-biased;retrieval results;search interface
multimedia documents;search engine
web-based;individual queries;ir research;information retrieval;learning environments;2,3;test collection;web-based;search topic;query performance;ir methods;1;information retrieval systems;test results;multi-lingual;user-generated;ir systems;query performance;test collections;4,5;trec collection;query performance
4, 5;web pages;extracted information;6;user-defined;information extraction;1;semantic web;5;user activity;7;error prone;information extraction;learning strategy;training corpus
digital media;web page;random-access;digital library
initial query;dynamically generated;hierarchical approach;search engine;expansion terms;retrieved documents
clustering;clustering;regular expressions;database;financial data;extraction process;iterative process;text-based;extracted data;1;3;2;data items;clustering algorithm;query languages;extracting data from
automatically learns;attribute values;classification;electronic commerce;conflict resolution;search engines;final step;text classifiers;service provider;information content;extraction algorithm;web sources;mining algorithm;ontology-based;web search engine;united states;web pages;business opportunities;database;web sites;keyword-based;classifier;service providers;web page;keyword based
data structure;hierarchical data;database;space overhead;xml data;scientific data;large number of;compression techniques;data structures
large amounts of;historical data;data warehouses;historical information;data set;data cubes;data structure;data streams
large distributed;core components;database systems;digital information;on-line analytical processing;query cost;complex networks;distributed caching;highly scalable;increasingly popular
query optimization;query evaluation;optimization framework;information sources;database;rates;query plans;query optimizers;query engine;plans
selection predicates;continuous queries over;relational algebra;continuous query;query processing;continuous queries
approximate answers;approximate) answers;ip network;limited memory;continuous data streams;approximation error;sql queries;aggregate queries;streaming data;statistical information;real-life;aggregate queries over;base data;data streams
data objects;large numbers of;communication overhead;real-world;data sources;rates;resource constraints
optimal) plan;query evaluation;optimizer;plans;query optimizer;large space of;cost-based;low cost
reusability;world wide web;web sites;dynamically generated;query result caching;increasingly complex;pre-defined;web sites;response times;fine-grained;dynamic content;web site;web site;real-world;level caching;internet traffic;web applications
xpath queries;database;path expressions;indexing structures;xquery expressions;index structure;designed to support;xml document
existing indexes;partial matching;xml data;real-life data sets;xml query languages;query performance;performance degradation;query processing;query workloads;structured data;path queries;query processing cost
summary tables;graph-structured xml;path expression;path expressions;index structures;evaluating queries;path queries
basic operations;multimedia applications;database;data elements;database operations;query processing algorithms;database engine;sequential scans
cache performance;main memory;db2 universal database;tree nodes;range scans
hash-based;performance degradation;realistic data;commercial dbms;join algorithms;data skew;excellent performance
query optimization;xml storage;statistical summaries;regular expressions;providing users;xml documents;cost-based;xml document;xml data;xml schema
web-based;query forms;end-user;xml data;database;large numbers of;query aspects;semistructured data;formal model
sql queries;data exchange;relational database systems;xml queries;data model;xml documents;relational database system;encoding methods;data management
encrypted data;data stored;database;database;service providers;data privacy;query processing;privacy issues;client site;sql queries;service-provider
workflow management systems;workflow management;mathematical models;application servers;replication;business processes;quality guarantees;distributed systems
data-driven;schema mapping;database;data mining;data sets;data mining research;database content;data sources;data mining tools;databases;data quality;structure mining
problem instances;join algorithm;parallel dbms;exact answer;online aggregation
optimizer;plans;query optimizer;base-table;query plan;statistical information;execution plans;query plans;base tables;microsoft sql server;estimation errors
query execution;distributed data sources;initial query;user interests;query processors;user interface;query plan;exploratory queries;query processing;operator;query results;result tuples
data sets;join operations;tree edit distance;sampling based;upper bounds;document structure;data elements;large collections of;data interchange;approximate match;xml data sources;multiple sources;reference set
acm sigmod;minimization of [tree pattern queries;tree pattern queries
selection predicates;synthetic data;twig pattern;twig patterns;xml query;xml queries;data model;tree-structured;holistic twig join;tree structure;xml database;intermediate result;join algorithms;pattern matching;xml query processing;intermediate results
synthetic data sets;storage scheme;data collections;multimedia retrieval;database applications;index structures;relational database systems;candidate set;similarity metrics;nn search;similarity search;nn queries;nearest neighbor search;database design;high dimensional space;high dimensional spaces
indexing method;window queries;distance functions;spatio-temporal databases;spatial joins;spatial queries;mobile objects;dynamic nature of;query type;database objects;spatio-temporal;nearest neighbors;nearest neighbor search
boolean queries;synthetic datasets;real-estate;database;user-defined;ranked queries;user-defined functions;multiple relations;provably optimal
forward selection;graph-structured data;graph-structured xml;statistical summaries;selectivity estimation;depends crucially on;large volumes of;path expressions;xml data;selectivity estimates;xml query languages;data elements;limited space;distribution information;general setting;synopsis structures;heuristic algorithm;databases;real-life data sets;path-expression;specification language
nearest;fast response;prediction methods;database;pattern queries;performance gain;similarity-based;time series;predict future;prediction errors;prediction error;fourier transform;batch processing
theoretical basis for;window size;data sequences;matching method;matching methods;stock data;query sequence;databases;time-series;false alarms;sliding windows;dual match;query length;false dismissal
clustering;synthetic data sets;similar objects;collaborative filtering;instance;large data sets;microarray analysis;expression levels;gene regulatory networks;clustering model
accurately reflect;computational biology;applications including;sequential patterns;long patterns;high confidence;amino acid;behavior analysis;sequence database;protein sequence;pattern discovery;support measure;market basket;long sequences;pruning technique
monte carlo;mathematical formulation;projective clustering;high probability;human faces;classifier
query optimization;approximate query answering;data visualization;continuous data streams;database research;large data sets;real data sets;data structure;data distribution
selectivity estimation;access plan;query optimizer;selectivity estimation;temporal information;spatio-temporal;average error;real-life;moving objects;spatial data;temporal data;spatio-temporal queries
data point;data sets;data collection;sampling approach;selectivity estimation;high dimensional;data mining applications;data collections;reduction techniques;data reduction;data set size;compression techniques;database size;database;desirable property;wide range;unified framework;processing requirements;dimensional data;sampling technique;nearest neighbor search
clustering;data structure;synthetic datasets;high dimensional;storage space;highly compressed;construction algorithm;query response time;query performance;fact table;data cubes
approximate answers;data values;highly-accurate;wavelet decomposition;synthetic data sets;real-world;data reduction;approximate query-processing;error guarantees;optimization algorithms;wavelet-based;large amounts of data;error metrics;random sampling
relational database;approximate query processing;task specific;index selection
database systems;sql query;database;meta-data;data tables;efficient search;1;management systems;ibm db;file systems;relational database;data stored in;data consistency;data management system;data updates
electronic commerce;business process;essential information;ad hoc;business processes
relational data sources;query execution engine;db2 universal database;database management systems;execution plan;support queries;source-specific;wide range;query processing;commercial systems;query planning;stored information
tree-based
data warehouse;database;data warehouses
index structures;database research;spatial indexing;database
parallel database;optimizer;query optimizer;data partitioning;database design;optimal) performance;heuristic rules;query performance;base data;query optimizers;tightly integrated with;rank-based
database;sloan digital sky;sloan digital sky
database;skewed;decision support;multiple types of;transaction processing
relational database;disk-based;high throughput;database
database applications;web caching;dynamically generated;web cache
cost-based optimization;database server;db2 udb;multi-tier;database servers;instance;data freshness;usage patterns;database caching;application server
sql queries;web pages;3;distributed query processing;xml data;dynamic nature of;database;db2 udb;application servers;multi-tier;high scalability;business applications;database caching;web services;databases;database functionality;level caching;local database;database schema;user queries
cardinality estimates;query optimization;query execution plan;optimizer;query optimizer;database schema;learned knowledge;database;query processing
query optimization;query rewriting;plans;query optimizer;programming languages;rewriting rules;query rewriting;plan generation;1;2;query optimization;programming language
semantic caching;view-based;regular expression;input query;wide range;query containment;free space;query pattern;xquery processing;1;3;2;query answering;tree-automata;source-specific;xml query processing;explicitly represented;xml processing;large volumes of data;xml queries;traditional database;cache management;desired information;mapping problem;tree automata;web applications;regular-expression;xquery engine;semantic caching;xml data sources;query containment;type inference
ontology-based;database;query interface;web information extraction;information extraction;information integration;xml-based;view-based;query answering;wrapper generation;large database
network processing;computational power;database;large number of;1;sensor networks;sensor network;sensor nodes;resource usage;computing platform;data management;small-scale
pre-defined;ad-hoc;query interface;network monitoring;large networks;huge volumes of data;long term;network traffic;similar characteristics;high speed
1, 2;web services;workflow management;web service;programming languages;language called;web services;programming language
data cube;data cube;feature extraction;large databases;integrated environment;1;2;data cubes;data repositories
gene expression;gene expression data;gene expression analysis
clustering;clustering;clustering results;data bases;clustering algorithms;clustering process;data base;visualization techniques;dimensional data
keyword based search;world wide web;access methods;semi-structured data;text documents;large number of;text-based;tree structure;file systems;xml-based;query languages
source code;database systems;database;database research;databases;open-source
electronic commerce;data sources;transaction processing;application servers;application servers
service providers
query optimization;transformation rules;relational algebra;database
clustering;multi-dimensional;data-management;scientific applications;space-partitioning;retrieval performance;partitioning scheme;efficient retrieval of;dimensional data;indexing technique;high-energy physics
database;query language;language (called;multidimensional databases;object-oriented;databases
xml data;query languages;high level;xml query languages;database
multimedia content;database systems;vice versa;database systems
clustering;clustering algorithms;cluster validity;cluster validity;data set;clustering scheme;clustering algorithm;clustering validity
database schema;relational tables
knowledge engineering;program committee;multi-dimensional;data warehousing and olap;international conference on;data warehouse;knowledge management;object-based;query optimization;international workshop on;data warehousing;acm international workshop on;theoretical foundations;data warehousing;online analytical processing;online analytical processing
web pages;web data extraction;direct comparison;qualitative analysis;databases;data extraction;machine learning;instances;traditional database;natural language processing;web data extraction;information retrieval;web data
fault-tolerant;databases
sql/xml;data types;1;2;xml document;data type;xml schema
aggregation function;obtained by combining;database
clustering;clustering algorithms;optimization problem
decision trees;monotonicity constraints;class attribute;classification trees;decision tree algorithms;classification problems
approximate answers;pruning strategies;search space;data collection;digital data;data-mining models;compression techniques;regression tree;accurate classification;data sets;bayesian network structure;data tables;compression schemes;real-life data sets;rates;small samples
lower bound;linear algebra;range query;raw data;upper bound;real-world data sets;constraint programming;missing values
mining frequent itemsets;plays an essential role in;pattern-growth;constraint-based mining;data mining tasks;large number of;sequential patterns;interesting patterns;large databases;mining process;sequential pattern mining;frequent pattern mining
search space;tree based;frequent-set;mining algorithms;frequent itemsets;algorithm called;effective pruning;data mining
response times;mining algorithms;iterative process;association rule mining;human centered;rule generation;data mining;result set;hypothesis testing;highly interactive
domain knowledge;domain knowledge;user preferences;mining frequent itemsets;multiple attributes;user-defined;mining process;instance;multi-attribute;functional dependencies;mining framework;relational data
world wide web;web pages;high-quality;image-based;dynamic content;information needed
information retrieval techniques;information retrieval techniques
acm symposium on;acm symposium on;emerging area;knowledge management;document processing;technical program;program committee
physical characteristics;query optimizer;declarative queries;large number of;user query;query processing in;sensor networks;query processing;sensor network;sensor nodes;query plan;resource usage;computing platform;data management system;small-scale
clustering results;clustering validity;pattern recognition;clustering validity
simulation model;database applications;mobile computing;mobile devices;information access;transaction management;retrieve information
db2 universal database;database applications;growing rapidly;databases;database administrators;db2 universal database;total cost
recent database
xml query;query language;working group;formal semantics
query-processor;databases;querying xml documents;database design;query workload;relational database systems;xml documents;queries over xml;relational schema;query processor;xml data;relational database system;sql queries;relational data
storage schemes;database;xml data;query language;standard benchmarks;xml databases;management systems;databases;relational databases
world wide web;web pages;semantically meaningful;data elements;individual objects;html documents;automatically generate;real world;web data
xml applications;database theory;problems arising;database
clustering;web documents;fall short;database;applications ranging from;xml query languages;reference-based;xml documents;simple queries;complex queries;xml document;databases;description language;database researchers
data formats;knowledge representations;knowledge-based;long-term;digital data;high-level
bu, ll;gl;gg, lo;internal node;internal nodes
information systems;database;web technologies;large number of;lessons learned;data sources;distributed architecture;relevant information
life sciences;data analysis;database;data mining technologies;data analysis tasks;data analysis;data mining;measurement data;vast amounts of
continuous queries over data streams;related concepts;database;continuous data streams;continuous queries over data streams;query processing in;data sets;semantic issues;materialized views;query processing;data streams;continuous queries;focus primarily on;data management;flexible architecture
database
intrusion detection;data analysis;data mining techniques;intrusion detection systems;intrusion detection systems;data mining
classification;classification;anomaly detection;classification accuracy;detecting anomalies;detection rates
intrusion detection;mining framework;audit data;intrusion detection;data mining approaches;feature extraction;raw data;data mining;semantic information;application area
association rules;database;data mining technology;large data sets
features including;linguistic patterns;vector machine;structural characteristics;learning algorithm
semistructured data;query languages
2;user-defined;high level;l;database
data-intensive;software systems;operating systems;database
entire process;limited space;complementary information
context-specific
knowledge management;ontology based;case study
world wide web;supply chain;web services;business logic;semantic web;semantic web;web services
web service;service selection;conceptual model;web services;service descriptions;conceptual model
domain-independent;semantic associations;search techniques;search process;data model for;semantic web;semantic relationships;operator;semantic associations;large amounts of;search space;web search engine;relevant documents;structural properties;business intelligence;semantic annotation;semantic analysis;knowledge representation;complex relationships;national security;semantic web;current web
semantic interoperability;large numbers of;social network;logical level;information sharing
semantic web;multimedia information;multimedia documents
web documents;information sources;specific queries;data extraction;data sources;semantic web;data management;web services;string matching;computing systems;semantic web
resource sharing;web services;large-scale;scientific data;semantic web;semantic web;problem solving;databases;semantic web technologies
user study;high-level;graph structure;high sensitivity;web site
related information;world wide web;end-user
resource discovery;software tools
web-based;user preferences;design choices;provide feedback;user feedback
web-based;designed to support;usage patterns
hand-held;knowledge sharing;visual interface;real-world
c.f. ferrara, brunner & whittemore (1991) and and several others;web-based;spoken language;social activities;web browser
performance gains;join operations;database
xpath processing;processing queries
information systems;ad-hoc queries;query language;mobile environment;computing environment;language called;location information;theoretical framework
query rewriting;data independence;data model;data model for;database management system;key features;multimedia database
end-users;database systems;database;video databases;formal specification;user-friendly;constraint satisfaction problem
hierarchical data;hierarchical structures;xml data;data integration;native xml;query processing;memory utilization;working set;standard sql
database;database tuning;databases;logical level;logical database;data mining;database applications;functional dependencies;discovered knowledge
conference series;international conference on;databases
indexing schemes;tree structures;spatial indexing;brute-force approach;access methods;trade-offs;database applications;index structures;resource-constrained;mobile devices;computing devices;spatial databases;data storage
memory bandwidth;queries involving;cache performance;relational database systems;storage model;relational databases;data placement;model called
selection problem;optimizer;query optimizer;depends crucially on;database;ubiquitous computing;join queries;data management;data warehouse;view selection;query processing;decision support queries;data placement;database schema;area network;data integration
web scale;highly heterogeneous;large-scale;linear) complexity;view definitions;definition language;automatic generation of;query translation
data structure;dynamic programming;suffix tree;approximate matching;string data;database;string similarity;database;index size;database technologies;70;protein sequence;string matching;error bounds;biological data
query optimization;classification;query optimizer;input data;frequent itemset;optimal algorithms;set containment;business intelligence applications;data mining;relational databases;complete classification
high correlation;query optimization;query result;database;computational cost;structural patterns;intermediate results;query pattern;xml queries;data sets;limited space;query plans;xml elements;native xml;query processing;xml database;pattern queries;schema information;query results;join) algorithm;query refinement
indexing schemes;evolving data;temporal aggregation;data warehouses;storage space;spatio-temporal;temporal aggregates;data streams;temporal databases
query optimization;computational complexity;data analysis tasks;information sources;data distributions;optimal combination of;search space;data mining tasks;data sources;data synopses;information quality
local sites;data collection;plans;data warehouses;data warehouse;aggregate queries;service providers;query processing in;olap queries;data cubes;distributed nature of;internet applications;network data;optimization techniques
data center;scientific data;scientific applications;multi-tier;large data volumes;error analysis;scientific databases;databases;building block
clustering;instance-based;classification;time series;knowledge discovery in databases;databases;association rules;data mining and knowledge discovery
clustering;data cube;classification;data warehouse;conference on knowledge discovery and data;data mining;missing data;knowledge discovery;web mining;data mining;association rules;sequence mining
domain knowledge;domain knowledge;classification;unsupervised learning;unsupervised learning;classification rules;conceptual clustering;domain theory;knowledge discovery;learning process
world wide web;induction algorithm;required information;effective search;edit distance;search engines;retrieve information;wrapper generation;user interaction;result pages
data points;decision tree;sample data;decision tree;neural network;data sets;linear classifiers;input variables;neural network
intrusion detection;greedy heuristics;genetic algorithms;information exploration;genetic algorithm;search techniques;network intrusion detection;data mining systems;genetic algorithm;network traffic;network data;network data
information extraction systems;machine learning;text corpora;common task;natural language processing;machine learning
decomposition approach;classifier;learning problems
expected cost;learning algorithm;cost-sensitive;obtained by applying;boosting algorithms;classifier
classification accuracy;large amounts of;training data;unlabeled documents;text classification tasks;partially supervised;real-world;learning algorithm;labeled documents;supervised learning algorithms;unlabeled documents;accurate classifiers;training documents;similarity-based;text classification;classification framework;partially supervised;classifier;expectation-maximization
feature space;multi-class classification;multi-class;pattern recognition;binary classification problems;voting scheme;vector machine;classification problem;learning machines;multi-class;fault-tolerant;support vector machines
linear programming;neural nets;game theory;evaluation criteria;temporal difference learning
tree induction;decision trees;computational costs;tree induction;continuous attributes;variance reduction;interpretability
decision trees;predictive accuracy;decision tree;learning algorithm;pruning method;7;2, 10, 12;margin-based
feature space;exemplar-based;naive bayes;boosting algorithm;word sense disambiguation
inductive learning;training data is;poor performance;hypothesis space;search process;generalization error;occam's razor;occam's razor;machine learning and data mining
multi-strategy;training data;cluster membership;distinguishing feature;regression methods;clustering method;regression models;linear regression;data sets;regression model;learning phase;test case;linear regression;learning task
discriminant analysis;nearest neighbor;classifier;images acquired;decision tree
customer satisfaction;classification techniques;classification;classification;class distributions;highly skewed;real world;complete classification
domain-independent;training examples;learning algorithms;learning paradigm;task decomposition;general form
data mining
temporal dependencies;large-scale;pruning methods;event sequences;event-sequence;selection methods
case-based reasoning;model offers;case-based reasoning;competence;case-base;additional cost;problem solving;competence
predictive accuracy;biological sequence;case study;inductive logic programming;cost function;predictive accuracy;learning framework;predictive accuracy
machine learning techniques;learning algorithms;workflow management;machine learning
prediction accuracy;decision trees;classification;unsupervised discretization;data mining applications;context-sensitive;machine learning;discretization methods;supervised methods;wide range;continuous attributes;data mining problems;comprehensibility
beam search;context-free;context-free
regularization;decision trees;generalization error;decision tree;generalization performance;learning problems;pruning) algorithm;regularization;decision trees;learning problem;regularization parameter
probability distributions;hidden markov models;hidden markov model
minimum support threshold;interesting itemsets;probabilistic models;user-defined;naïve bayes;interesting patterns;classification accuracy;chi-square;context-specific;bayes classifier;conditional probability;classifier
conditional entropy;prediction problem;loss function;classification;classification;ensemble members;feature subset-selection;entropy measure;feature selection
distance measure;inductive learning;hypothesis space;learning tasks;theoretical results;language independent
database;learning algorithm;learning tasks;machine learning applications;case studies;raw data;knowledge discovery
nearest-neighbor classification;nearest-neighbor classification;similarity measure;rule-based;similarity measure;noisy datasets
machine learning;time series;time series
intrusion detection;intrusion detection systems;intrusion detection;user-defined;cost-sensitive;machine learning;cost-sensitive
learning process;hierarchical clustering;machine learning;clustering task;clustering tasks;feature selection;8;incremental clustering;comprehensibility;feature selection;feature selection
distance measure;classification problem;success rate;classification;correlation coefficient;ranking methods;algorithm selection;ranking methods;training datasets
context information;detection results;neural networks;neural networks;neural network
data structure;frequent items;large databases;data clustering;knowledge discovery;conceptual model;data mining;association rules
case-based reasoning;product recommendation;electronic commerce;customer service;case-based reasoning;refinement step;short-term;short-term
linear regression;real-valued;real-valued
predictive model;inductive logic programming;learning strategies;historical data;positive examples
taking into account;training examples;behavior patterns
individual agents;learning algorithm;optimal strategy;game-theoretic;fitness
boosting framework;predictive accuracy;learning process;feature subsets;multi-relational classification;margins;margin-based
base-classifier;optimization technique;linear combination;vector machine;learning algorithm;quadratic programming;base-classifiers;binary classification problems
dynamic range;training data is;vector machine;quadratic programming problem;kernel approach;kernel methods
domain knowledge;mathematical models;machine learning methods;hybrid approach
pattern recognition;predictive performance;pattern recognition;nearest
factor analysis;signal processing;latent variables;text document;case study;observed data;statistical model;independent component analysis;time series;multivariate data;principal component analysis;blind source separation;independent components;large database
databases;markov chains;detection task;markov chains;markov chain
phase transitions;phase transitions;negative examples;problem instances;stochastic local search;machine learning;inductive logic programming;stochastic local search;artificial intelligence
probabilistic latent semantic analysis;latent dirichlet allocation;principle component analysis;matrix factorization;instance;expectation-maximization algorithm
decision tree learning;candidate models;decision-tree learning;inductive inference
state space;relevant features;highly complex;high complexity;action space;state-action;neural networks
12;target values;classification;quadratic programming;vector machine;reinforcement learning;1;obtained by applying;support vector machines
10, 11;14;learning algorithm;learning algorithms;irrelevant attributes;perceptron algorithm;gradient-descent
classification;classification;rule sets;inductive logic programming;machine learning techniques;business decisions
large data sets;confidence values;regression problems;inductive inference
probability distribution;sequential nature;natural language;domain specific;machine learning
probability estimates;decision trees;classification decisions;loss function;probability estimation;cost-sensitive learning;cost-sensitive;instances;decision making;decision boundary;learning models;cost-sensitive;machine learning algorithms;class probabilities;classification algorithms
state/action
learning process;refinement operator;search-space;automatically created;search space;inductive logic programming;search-space;operator
hill climbing;genetic algorithm;encoding schemes;causal discovery;encoding schemes;causal models;data sets;causal models;search strategies;markov chain;encoding methods
feature space;support vector regression;kernel-based;regression problems;nonlinear regression
cut algorithm;graph theoretic;learning agent;learning phase;learning algorithm;reinforcement learning;dynamic environment;space partitioning
ranking approaches;instance-based;learning algorithms;makes sense;clustering trees;target variables;machine learning algorithms;clustering trees
markov decision processes;optimization problems;reinforcement learning algorithms;function approximation;learning systems
nonlinear regression;loss function;active learning;conditional distributions;active learning;gaussian noise;learning scheme;special case
takes into account;local classifiers;dynamic programming;natural language
collaborative recommendation;collaborative recommendation;collaborative recommendation;instance-based;real-world;machine learning;information access;theoretical analysis
classification problem;classification;classification;vector machine;support vector machines;classification problems;collaborative filtering
meta-level;level features;linear regression;meta-level;probability distributions;cross validation;classifier
base classifiers;classification problem;base classifiers;boosting algorithm;upper bound;base classifier;classifier
base classifiers;instance-based;instance based;test instance;weighting scheme;base classifier;classifier;weighting scheme
classification technique;decision trees;predictive accuracy;decision tree;binary classification problems;classification rules;decision trees;original formulation;induction algorithm
multi-class problems;multi-class classification;pairwise classification;class values;classification;performance gain;classification performance;pairwise classification;ensemble technique;ensemble technique;decision tree learners
clustering;high-dimensional;vector space;market basket" data;mixture model;large-scale;mixture models;data exploration;basic concepts;web site;vector data;prediction problems;real-world;real-world applications;machine learning and data mining;mixture models
labeled and unlabeled data;spam detection;text summarization;high-accuracy;unlabeled data;classification;classification;semi-supervised learning;costly process;large number of;machine learning;labeled data;information access;text classification;classification models;labeled and unlabeled data;labeled examples;classification algorithms
collaborative learning;management systems;ad-hoc;vector space model;automatic query expansion;retrieval systems;pseudo relevance feedback;concept learning;term-based;digital libraries;information retrieval;information retrieval systems;term-based;retrieval effectiveness
individual agents;multiagent systems;agent performance;case-based reasoning;multiagent learning
clustering;clustering;evaluation criterion;cluster membership;attribute domains;minimum description length;instance;clustering algorithm;cluster boundaries
domain experts;classification accuracy;learning algorithm;machine learning;classification problems;machine learning algorithms
convergence properties;policy gradient;rates;learning rate
discriminative clustering;auxiliary data;mutual information;discriminative clustering;discriminative clustering
training data;class probabilities;real-world;specifically designed for;main idea;class labels;classification problems;classification algorithm;class probabilities;classification algorithms
boosting algorithms;boosting algorithm;boosting algorithm
instance-based learning;classification accuracy;learning phase;rule-based;test case;classifier
data corruption;database management system;database
web-based;world wide web;extensible markup language;xml documents;xml documents;access control model;access control;markup language
db2 universal database;design requirements;database;db2 udb;view definitions;object-oriented;relational data
increasing amounts of;specific context;upper bound;lower bound;network applications
large data sets;olap) applications;decision-support;moving average;database
world wide web;query languages;data model for;semistructured data;graph-based;tree-based;data model;data model for;data sources;inconsistent information;semistructured data;inconsistent information;real world
query routing;physical design;replication;olap queries;databases;data placement;query routing;data organization;database cluster
join algorithms;index structure;cost model;spatial joins;index construction
discovery algorithms;induction algorithms;decision tree;large number of;data structures;database;large number of;pruning strategies;classification rules;large datasets;pruning techniques
query evaluation;semi-structured data;semi-structured data;path expressions;clustering method;directed graphs;query processing;semi-structured data
clustering;topic discovery;accurate models;clustering methods;classification;highly accurate;user feedback;naive bayes classifier;user effort;text database;naive bayes classifiers;clustering algorithm;textual documents;clustering process;clustering accuracy;classification algorithm
high-dimensional;high-dimensional;database;dynamically changing;index structures;cost model;data set;query processing;schema information;query processing techniques;sequential scan;index structures;clustering
web-based;simulation result;database management;scientific data;database;limited bandwidth;post-processing;xml document;user interface;data reduction;managing data;large datasets;external data;external data;simulation results;database security
incremental algorithms;generalized queries;query templates;declarative queries;real data;user queries;low overhead
database;real-world;databases;large databases;functional dependencies;algorithm called;efficient discovery of;theoretical foundations;database administrators;functional dependencies
heterogeneous information sources;information sources;database;queries posed;automatic generation of;source schemas
multi-dimensional;query result;high degree of;count based;answering queries;cost-based;count based
data model for;database;relational algebra;databases
systems require;software tools;database;large scale;data types;query operators;data sources;code base
taking into account;data warehouses;ad-hoc;data model;view maintenance;materialized views;auxiliary data;base data;source data;data warehousing;relational model
special case;query response time;aggregate data;fact table;view selection;cost models;real world applications;recent database;olap applications
db2 universal database;database;db2 udb;communication protocol;relational database management system;transaction processing
search performance;index structures;storage utilization;query performance;performance metric;real-world data sets;fixed size;range queries
systems support;database;database applications;database server;client applications;database;operator;persistent database
nearest-neighbor search;feature points;query evaluation;search performance;filtering step;nn search;nn-search;instance;result quality;evaluation techniques;query result;nn-search;nearest neighbor search
recursive queries;regular expressions;database;databases;path queries;partial order;partially ordered;graph data;object-oriented databases;relational databases;graph databases
workflow management systems;markov-chain;enterprise-wide;application servers;replication;business processes;software components;distributed systems;workflow management systems
database applications;huge amounts of data;database technology;database
database;database server;originally designed;database;design decisions;information exchange
step forward;database;web sites;data representation;xml query languages;data interchange;document management;xml-based;database researchers
query evaluation;mediator systems;communication overhead;lower level;xml documents;xml views;evaluation scheme;query answer;real world;query results;demand-driven;plans
data set;workflow execution;software systems;workflow execution
sparse data;data cube;data cube;range sum queries;data cubes;range sum queries
user-friendly;information systems;domain experts;user queries;real-life;operator;change frequently;knowledge base;semi-automatic
query optimization;classification;input data;optimal algorithms;query optimizer;business intelligence applications;relational databases;optimal performance
gene expression data;related data;gene expression;clinical) data;case study;data repositories;gene expression;data management
database;database systems;database
response times;performance goals;problem setting;class-specific
mining results;multi-dimensional;extraction algorithm;relational algebra;association rule mining;mining process;incremental mining;association rule mining;obtained by combining;incremental mining;association rules;association rules;takes place
algorithm finds;pruning strategy;query patterns;total number of;database;schema-based;query language;tree-pattern queries;cost-based;approximate results;total cost
span multiple;similarity search;querying xml data;relevance ranking;data collections;xml data;xml documents;ranked retrieval;search engine;query languages;query result;relevance ranking;exact-match;path queries;search efficiency;xml collections;search paradigm;index structures;xml elements;search engine;web search engines;additional information
business processes;management systems;semantic analysis;business process;business analysts;business operations;log data
query execution;search performance;application area;data stored;electronic commerce;large number of;search queries;database systems;query processing;cost-based query optimizer;database engine;data storage
base relations;conjunctive queries;compare favorably with;data integration systems;conjunctive queries;query containment
data mining applications;data sets;wide range;decision making;pattern extraction
data warehouse;computation cost;representative set;data warehouse;trade-offs;cost-model;user requirements;source database;olap queries;data warehousing;aggregate views;source databases
query execution;multiple queries;continuous query;query-processing;intermediate results;multiple continuous queries;query response time;large number of;long running;data streams;data characteristics;query performance;multiple continuous queries;continuous queries
query plans;data sets;native xml;schema information;query results;query optimization;22;query pattern;intermediate results;query result;high correlation;structural patterns;xml database;computational cost;join) algorithm;query refinement;database;pattern queries;xml queries;limited space;xml elements;query processing
scientific data;scientific applications;multi-tier;large data volumes;scientific databases;data processing;databases;building block
database applications;data sets;algorithm called;set containment;wide range;set containment;set containment;operator;set-valued attributes
indexing schemes;access methods;information dissemination;selection criteria;mobile clients;data access;wireless networks
object-relational;designed to support;database technology;related data
web pages;web sites;user experience;major components;rule-based;web sites;domain ontology;web site;data-intensive
indexing schemes;temporal aggregation;evolving data;temporal aggregation;data warehouses;storage space;data streams;temporal aggregates;data streams;temporal aggregation;temporal databases
clustering;data analysis;decision-making;classification process;data mining system;data mining system
domain knowledge;graph structure;auxiliary;information sources;tool called;information sources;application domains;data sources;information integration;source data;expert knowledge;source specific;physical objects
matching algorithms;database;online shopping;large sets of;xml documents;query processing in;supply-chain management
distributed query processing;xml query;heterogeneous data sources;data integration;xml schema;main components
computational efficiency;database;moving objects databases;spatio-temporal;efficient querying;inherent uncertainty;moving objects databases;range queries
xml databases;database theory;databases;relational tables;functional dependencies;data replication;data-centric;functional dependencies;xml data;functional dependencies
data independence;complete information;access structures;shared information;stored information;information space;higher-order;traditional database systems;database functionality;relevant information;data storage;information space;higher level
poor performance;real-world;web sites;user behavior;predict future;log mining;web log mining;web logs;user transactions;higher accuracy;access patterns;web document;prediction models;web-access
mobile objects;high-quality;temporal information;mobile objects;rates;database management system;databases;query processing techniques
database systems;spatial queries;database management system;performance gains;optimization criteria;query sets;database systems
federated database;core components;database;application systems;commercial products;data sources;application systems;workflow management system;databases;data integration;user-defined
clustering;total number of;access method;large volumes of;efficient indexing;historical queries;upper bound;temporal databases
window queries;query cost;worst-case;extra information;indexing structure;cost models;query processing;processing cost
schema evolution;configuration management;application level;content management;schema evolution;semi-automatic;database systems
search systems;database systems;object-relational;similarity retrieval;relevance feedback;query refinement;result tuples;similarity) searches;databases;query refinement;query refinement
efficient algorithms to;xml query;tree patterns;tree pattern;tree pattern;tree-structured data;intermediate results;pruning strategies;information retrieval;query answers;tree patterns;xml data;query results;exact answers;query evaluation
information systems;current commercial;data warehouse;spatio-temporal;data collections;data warehouse systems;temporal evolution;spatio-temporal
1;database systems;selectivity estimation;probabilistic models;selectivity estimates;spatial joins;estimation process;spatial joins;query plans;queries involving;spatial join;join operations
mutually exclusive;data items;energy-efficiency
search space;approximate query processing;large databases;multimedia information;spatial joins;prohibitively expensive;fast retrieval;spatial joins;increasing amounts of;approximate matches;query processing
specifically designed for;ontology language;semantic web;web resources;ontology language;object oriented;semantic interoperability;description logic
relational databases;data mining problem;mining algorithm;inference problem;databases
context information;sensor technology;relevant content;mobile devices;knowledge representation and reasoning;user context;mobile access;digital library;digital libraries;relevant information;ontology based;mobile access
data warehouses;local sites;ip network;plans;data warehouses;data warehouse;aggregate queries;distributed nature of;query processing in;olap queries;data collection;data cubes;internet applications;network data;optimization techniques
user preferences;information filtering;information systems;user profiles;databases;query languages;decision making
data analysis tasks;information sources;data distributions;optimal combination of;physical design;data synopses
range-sum queries;approximate query answering;aggregate queries;compression techniques;wavelet-based;data structures;wavelet-based;approximate results
ubiquitous computing;user interfaces;relies heavily on;instance;databases

object technology;database
search space;database;data sequences;application domains;approximate queries;real-valued;approximate queries
query language;object model;database;definition language;database management
distributed databases;data items;distributed database systems;data item;serializability;data placement
database;user interfaces;database applications;instances;caching scheme;database applications;existing database
parameter space;node failures;redundant information;large scale;high availability;high availability
object-oriented database;logic-based;object-oriented databases;active rules
database;automatically generated;database;human-machine;databases;cost-effective
log-structured;log-structured;random access
large-scale;semantic heterogeneity;information sources;data quality;large-scale;semantic heterogeneity
order preserving;relational database system;order-preserving;database systems
parallel database;complex queries;database;rewritten query;complex query;decision support applications;algorithm called
multi-dimensional;tree based;storage utilization;multi-dimensional;access structures;query processing;access structures;range queries
transaction models;workflow systems;theoretical foundation;database applications;commercial products;databases;transaction models;transaction processing
span multiple;workflow management;enterprise-wide;workflow management;enterprise-wide;enterprise-wide;fault tolerance
data source;security constraints;databases;databases;external sources;formal model
rule-based;conflict resolution;replication;data replication;query processing;control mechanism;distributed data
large amounts of;data collection;valuable knowledge;frequently occurring;knowledge discovery;databases;dynamically generated;iterative process;knowledge discovery;user interface;sequential data;databases;pattern discovery;discovered knowledge
data formats;data integration;data integration
parallel databases;queries submitted to;parallel database systems;database systems;database
data mining;database researchers;data mining
clustering;clustering;databases;large databases;association rule;minimum description length;association rules;association rule;association rules
high-speed;end-user;classification model;content based search;classification;database;asset management;complex data;asset management;complex data;data characteristics
data sharing;cost-effective;resource management;cost effective
database systems;end users;database;legacy systems;heterogeneous databases;databases
heterogeneous information sources;conjunctive queries;frequently occurring;information sources;decision support;query language;query-translation;domain-specific;domain--independent;databases
digital media;real-world;knowledge discovery;knowledge representation;complex data;semantic information;knowledge base;vast amounts of
distributed computing;digital media;asset management;decision-making;digital media;complex networks;database architecture;large datasets;asset management
query optimization;selectivity estimation;selectivity estimation;database;storage structures;efficient algorithms to;query optimization;error analysis;relational database management systems;general case;space requirements;general problem;special case
relational operations;computing environments;simulation model;computing environment;computing environments;case studies;view maintenance
object-oriented databases;data model;complex applications;data model
distinguishing feature;database queries;data model;multidimensional databases;multidimensional databases;relational database system;database engine;programming interface
response times;large volumes of;remote sensing;post processing;database;database;remote sensing data;temporal data;remote sensing
database systems;spatio-temporal;query language;data model;temporal logic;data model for;moving objects;moving objects
optimizer;relational algebra;query optimizer;database;sql query;nested queries;relational algebra
multiple classes;multi-dimensional;taking into account;single class;class-hierarchy;instances;class hierarchy;access patterns;object-oriented databases;object-instances
business processes;transaction models;transaction execution
caching scheme;video sequences;video sequence;caching scheme
data objects;simulation study;low-overhead;cost-model;cost model;network nodes;variable-size;data-intensive applications;distributed caching
oltp systems;years ago;database;indexing techniques;database;query performance;data warehousing;table scans;performance gains;data warehousing;commercial database;storage management;low level
high level;database;web sites;query language;web servers;data model;data model;traditional database;pose queries;query languages;relational views;relational views;web data;information source
database architecture;query plan;data flow;data warehouse
object-oriented;database schema;database management system;event detection;event-condition-action
similarity-based;databases;database;concept hierarchies;database
path expressions;object-oriented database;databases;query language;complex objects;class hierarchy;databases;object oriented
schema translation;data exchange;molecular biology;database;scientific data;databases;data model;sequence database;object-oriented;database;scientific databases;databases;relational dbmss;data management;data management;scientific databases
database;relational databases;database;language called;data structures;heterogeneous databases;schema evolution;databases;object-oriented databases;data management;standard database
worst-case;concurrency control;database systems
schema integration;database systems;database;databases;application domains;object-oriented;schema evolution;structured objects
event-condition-action;business rules;database systems;user's perspective;object-oriented;business rules;business rules
rule sets;management systems;object oriented;event/condition/action;active database
domain specific;database;object databases;object databases;database;definition language;database management system;salient features;scientific databases
query processing strategies;spatial clustering;optimization strategies;spatial joins;path queries;query processing strategies;spatial constraints;spatial relation;path queries
rewrite rules;query optimization;complex queries;query rewrite;decision support;ibm db;data driven;query optimizations;relational dbmss;relational dbms;control scheme
highly skewed;disk accesses;skewed;algorithm requires;application domains;uniformly distributed;query performance
case based reasoning;case based reasoning;customer support;problem solving;data mining;rule based;expert systems
concise representations;semi-structured data;world wide web;semi-structured;data sources;data stored in;schema information;object-oriented databases
source code
federated database;databases;relational schema;object-oriented;federated database
heterogeneous information sources;world wide web;complex queries;highly heterogeneous;schema integration;provide answers;information retrieval;complex query;constraint-based;relevant information
semantic query optimization;semantic query optimization;description logics;inference techniques
user interfaces;specific context;user interface;main components;active database;geographic information systems
database;active rules;data model;heterogeneous databases;databases;global constraints;semantic constraints
multiple dimensions;long term;data warehousing;data warehousing;data warehouse
high-dimensional;high-dimensional;storage cost;real-life datasets;data mining applications;index structure;dimensional data;similarity joins;similarity join;similarity joins;internal nodes
data lineage;fine-grained;object-relational;large data sets;database;data set;fine-grained;data sets;data lineage;base data;database management system
high probability;distributed environments;high reliability
distributed data sources;data warehouse;multiple views;multiple view;materialized views;base data;data warehousing;source data;data consistency;multiple view;distributed environment
growing number of;response times;storage devices;scheduling problems;video data;rates;data stored in
search engine;fault tolerance;text retrieval
retrieval strategies;mobile computing;mobile computing;database servers;mobile users;data management
indexing schemes;index structures;index selection;class hierarchy;multi-attribute;range queries
query response time;database;database design;data warehouses;data warehouses;materialized views;remote sources;data warehouses;views defined;index selection
trade-offs;database;data instances;simulation experiments;active databases;operating conditions;active database
update transactions;individual nodes;distributed databases;management systems;rates;user transactions;serializability;concurrency control;data item;distributed transactions;monitoring systems
naive implementation;transaction models;general-purpose;high level language
materialized views;data warehouse;source data;heterogeneous sources;data warehouse
software development;database management systems;database applications;rule-based;storage manager;application domains;database application;serializability;concurrency control;concurrency control;semantic information;workflow management systems
optimal performance;np-hard;computing environment;performance gain;mobile computing;analytical model;data items;bandwidth consumption;communication bandwidth;heuristic algorithm;data access;performance gains;selection process;distributed caching;wireless network;caching techniques;distributed caching
memory capacity;web sites;web servers;data replication;buffer management;network traffic;memory management;web data;memory management
summary tables;database systems;data cube;on-line analytical processing;raw data;index selection;olap queries
large amounts of;multimedia applications;database;object-oriented database;online shopping;data model;object-oriented;document management;database systems;databases;digital libraries;relational database;multimedia database
. achas.;cost-based;data item;actual values;probability estimation;demand-driven;estimation errors
concurrency-control;index pages;simulation studies;database;concurrency control;quantitative analysis
indexing schemes;multi-class;indexing scheme;complex queries;indexing scheme;object-oriented databases;object-oriented databases;single-class;range queries;exact match
multimedia presentation;multimedia databases;multimedia data;data model;user specifies
human interactions;process management;configuration management;configuration management;computing environment;network management;heterogeneous systems;business processes;execution engine
user interfaces;related information;concurrency control
distributed query processing;application domain;query language;query language;data model;complex objects;object-oriented;scientific databases;query processing techniques
database;information systems;data items;information systems
case study;control systems;business model;business process
learning process;computational complexity;discovery algorithms;rough set;database;knowledge discovery;machine learning techniques;large databases;learning procedure;data reduction;knowledge discovery in databases;rough set theory;databases;classification rules;rough set approach;learning task;rough set approach
algorithm parameters;cost functions;cost model;performance gain
aggregate functions
information systems;high dimensional;similarity indexing;search tree;large databases;approximate queries;efficient indexing;similarity queries;dimensional data;similarity indexing;feature vectors
hierarchical algorithm;arbitrary length;long sequences;search algorithm for;databases
database;text retrieval
schema-based;provide feedback;global schema
database;relational schemas;relational schema;relational databases;functional dependencies;normal form;entity-relationship;join queries;application programs
user-defined functions;relational database;file systems;user-defined
join algorithms;data structures;analytical model;higher-order;join algorithms
clustering;clustering;knowledge bases;storage scheme;knowledge-based;object-oriented;object-oriented databases;knowledge-based systems;knowledge base;knowledge-based systems
query optimization;query processing in;databases;distributed databases;materialized views;conjunctive queries;query answering
transaction data;discovered association rules;database;large databases;large databases;transaction database;efficient discovery of;discovered association rules;association rules;discovered rules
parallel processing;spatial joins;spatial locality;join algorithms;sufficiently large;spatial joins;load balancing
service provider;information providers;service providers;client applications
directed graph;graph-theoretic;indexing problem;update operations;object-oriented databases;graph-theoretic
propagation algorithm;evaluation techniques;condition monitoring
object model;declarative queries;database technology;query processors;data stored in;data access;database functionality;primary goal;query processing
transaction models;workflow systems;process management;hierarchical approach;atomicity;business process management
complex data;object-relational;extensible framework;database management system;database
temporal queries;temporal databases;temporal databases;databases
mobile computing;energy-efficient;computing environment;communication bandwidth;query processing;energy-efficient
user preferences;database systems;database;specific properties;database systems;multimedia database
source code;transaction models;database transactions
transaction management;disk space;log records
query-processing;graphical representation of;information systems;petri nets;information systems
composite events;database;implementation details;active database;active databases;event based;composite event;credit card;finite state
heterogeneous information sources;main features;semi-structured data;specification language
algorithm performs;navigation systems;navigation systems;graph model;graph model;efficiently computing;current location;minimum cost
user experience;visualization tool;database
object-oriented;data management;relational database management system;workflow management system;data management
recursive queries;database;rule-based;query language;data model;object-oriented
inverted files;input query;efficient algorithms to;document collections;simulation results
information systems;temporal locality;performance gains;spatial locality;information systems;data dissemination;access patterns;data dissemination;network traffic;network traffic
world wide web;relevance scores;keyword-based search;retrieval effectiveness;ranking algorithms;user queries;ranking algorithms;retrieval model;spreading activation;information retrieval techniques;document ranking;vector space model;spreading activation
database management systems;structured documents;query language;structural characteristics;regular expression;management systems;document collections;object oriented;information retrieval systems
distributed queries;data transfer;query processing strategy;execution plan
transaction management;concurrency control mechanism;communication cost;buffer management;transaction management;multi-threaded
materialized views;optimization strategies;database;view updates;object-oriented;object-oriented;update operations;object-oriented databases;update propagation;relational databases;relational views
transaction models;formal semantics;computing environments;databases
database operations;induction algorithm;classification;data mining applications;classification rules;distinctive features;rough sets;database operations;classifier;rough sets

threshold values;classification
pattern recognition;classification;classification;correct classification;association rules;association rules;training algorithm
multiple data streams;scalability issues;bayesian network;bayesian networks;multiple data streams;bayesian networks;theoretical justification;data streams;web-log;web mining
data mining tool;predictive models;information services;data mining
costly process;fp-tree;frequent patterns;frequent patterns;efficient algorithms to;candidate set;apriori algorithm;large datasets;data mining
clustering;classification accuracy;training classifiers;learn models;obtained by applying;large datasets
search space;tree learning;classification;linear models;decision tree;decision tree learning;generalization ability;benchmark datasets;classification problems
data analysis;data mining;data set;data sets;data mining;mining algorithm
medical research;rule generation;patient data;patient data;rule generation;data mining;basis function
text summarization;tree based;decision tree;direct comparison;text summarization;minimum description length;clustering algorithm
equivalence classes;learning algorithms;directed graphs;bayesian networks;network structures;equivalence classes
data values;attribute values;tree based;classification;data mining tasks;human experts;interestingness measure;real-life;knowledge representation;interesting rules;databases;data mining algorithms;discovered rules
classification models;classification techniques;ensemble classifiers;4,9,10 ;margin;classification error
production systems;item-sets;synthetic data;frequent patterns;stock market;highly correlated;pattern discovery;occur frequently
rules generated;fixed-length;fuzzy-rule;randomly generated;rule base;fuzzy rules;fuzzy rules
classification;learning algorithm;information processing;neural network;adaptive algorithm;network model;neural network
decision trees;subspace clusters;disk-resident;decision trees;data mining models;association rules;class distribution;classifier
decision trees;data mining;naive bayes;multiple classifiers;linear regression;meta-level;decision trees;classifier ensembles
clustering;data set;clustering validity;clustering algorithms;clustering criteria;data sets;data set;clustering scheme;real data sets;clustering algorithm;clustering validity
large amounts of;interpretability;3 ;incremental learning;machine learning;incremental learning;dimensional data;support vector machines;support vectors
high-confidence;fp-tree;classification;classification;database;databases;classification methods;fp-growth;tree structure;machine learning;association rules;classification method;association rules;associative classification;frequent pattern mining;class-association rules;unstructured data;large database;high classification accuracy
bayesian network;mutual information;learning algorithm;probability distribution;evolutionary algorithm;network structures
rule set;rule set;database;large number of;transaction databases;association rules;transaction database
local structure;data analysis;large number of;covariance matrix;dimensional data
candidate patterns;candidate patterns;question arises;upper bound;level wise;upper bound;frequent patterns
classification performance;knowledge representation;continuous attributes;data mining;association rules;simulation results
text mining;web search;automatically extracting;web queries;link structures;cross-language information retrieval;query terms
data structure;mining algorithm;frequent patterns;frequent patterns;database;space overhead;large databases;large databases;mining process;highly scalable;data set;databases;data characteristics;disk-based;structure mining;data mining methods
concise representation;frequent patterns;frequent patterns;frequent itemsets;problems require;concise representations;sequential patterns;frequent itemsets;data mining;association rules
database size;database;sequential patterns;sequential patterns;vice-versa;databases;support threshold;mining algorithm;large database
continuous data;data mining;data set;large data sets;statistical tests;continuous data
clustering;clustering algorithm;efficient clustering
6 ;minimum set of;rules generated;closed itemsets;significant rules;event sequences;risk management;event sequences;frequent itemsets;discovering association rules;association rules;decision support system;frequent closed;time-series;association rules;data mining problem;frequent closed
database;association rule mining;item set;sql3 queries;object relational database;join query;data mining;association rules;ad hoc
learning process;storage cost;learning bayesian networks;hidden variables;incremental learning;bayesian networks;incremental learning;bayesian networks;high accuracy;em algorithm;evolutionary algorithm;missing values;learning method
web-based;network models;modeling assumptions;data mining;causal modeling;bayesian approach;data mining;user-friendly;analysis tool
similarity metrics;database;data sequences;data mining;data representation;problem domains;sequential patterns;edit distance;automatically extracting;computational efficiency;databases;sequential patterns;multimedia data
hierarchical classification;hierarchical classification;hierarchical text classification;training documents;classification performance;text collection;distance-based;classification method;similarity measures
knowledge discovery;intelligent agent;data mining
data set;data mining
variable selection;real-world;mutual information;time series;real data
discovery algorithm;discover patterns;dimensional euclidean space
clustering;classification;association rule mining;data mining;time series;time series;databases;piecewise linear
clustering;clustering;efficient indexing;time-series;real data;distance measure;time-series;distance measures;moving average;time-series;similarity measures
mining sequential patterns;data mining technique;sequential patterns;data mining technique;sequential patterns
prediction accuracy;low quality;high quality;low quality;test cases
learning algorithms;vector machine;large data sets;incremental learning;stream data;memory requirement;data streams
great significance;large corpora;general-purpose;bayesian inference;support vector machines;bayesian belief;natural language
clustering method;clustering method;data sets;large data sets;data sets;incomplete data;metric spaces
clustering;clustering;fast algorithm;data points;database;high dimensional;data set size;data sets;fast algorithm;data mining problem;high dimensionality
boundary points;real-world;training set;evaluation functions
textual descriptions;concept space;user-queries;similarity queries;indexing techniques;large number of;similarity search;text data;user-defined;similarity search;search engines;similarity search;latent semantic indexing;search efficiency
rule sets;rule set;classification;classification accuracy;rule learning;class distributions;machine learning;rule sets;instance;perform poorly;skewed
clustering;similarity computation;agglomerative hierarchical clustering;similarity computation;agglomerative hierarchical clustering;approximation method;traditional clustering algorithms;clustering algorithm;dimensional data;brute-force algorithm;large database
11;parallel algorithm;frequent patterns;database;association rule mining;pattern tree;mining process;fp-growth
long-term;learning paradigm;time series;great potential;water resources;data mining;long-term;neural network
clustering;original data;multidimensional data;data analysis;model trained
frequent itemset;synthetic datasets;frequent patterns;data mining techniques;frequent subgraph;databases;large graph;computationally hard;frequent itemsets;data sets;subgraph isomorphism;frequent subgraphs;occur frequently
clustering;graph partitioning;web resources;graph analysis;laplacian matrix;web resources
interesting patterns;data-preprocessing;user-interaction;databases
attribute values;data analysis;ordered information;ordered information;data mining algorithms;machine learning;real world;machine learning and data mining
clustering;clustering problem;data analysis;spatial data;data points;database;data sets;data set;hierarchical clustering algorithm;data sets;data mining;clustering algorithm;clustering algorithm
attribute values;multiple attributes;multiple attributes;typically involves;data collected from;frequent itemsets;multi-attribute;real world;execution times
database;temporal association rules;database;association rule mining;temporal association rules;databases;support counting
term-frequency;xml mining;vector-space model;information retrieval;xml documents;document management;xml document;data mining
search accuracy;image features;image features;similarity measurement;irrelevant features;large number of;multi-resolution;user query;concept learning;query processing;image similarity;mining algorithm;learning task
fp-growth;frequent itemsets;algorithm called;transaction databases;frequent itemsets
specially designed;synthetic datasets;classification;data mining applications;classification performance;rare classes;rare class;boosting algorithms;qualitative analysis;rare classes;boosting algorithms;classifier
predictive models;predictive models
decision trees;mobile environment;vice versa;decision tree;mobile environment;computing devices;decision trees;continuous data streams;data streams
statistical methods;temporal dimension;temporal dimension;interesting rules;data mining;association rules;rule discovery;discovered rules
mining results;mining results;frequent itemsets;frequently occurring;item set;link structure;document sets;hierarchical clustering;association mining;directed graphs;item sets;link mining;applications involving;information spaces
association rules;discovering association rules;medical data;mining association rules;discovered rules
classification;equivalence relation;equivalence class;artificial data;clustering procedure;equivalence relations;rough sets;clustering procedure
prediction accuracy;decision tree;decision trees;decision tree
classification technique;classification;classification;highly structured;naïve bayes;expectation maximization;nearest neighbors;retrieval method
frequent itemsets;search space;dataset characteristics;frequent itemsets
association rules;tabular data;association rules;tabular data
association rules;rules generated;interesting association rules
synthetic data sets;color images;large datasets
clustering;intrusion detection;computational complexity;sequential nature;web-logs;scientific data;data sets;share similar;sequential data;clustering algorithm;protein sequences;clustering
high-dimensional;association mining;computational complexity;market-basket analysis;np-hard problem;data mining task;support measures
graph structure;database;takes into account;generalized association rules;set valued;global features;sequential data;association rules
clustering;dimensional data;tree structure
case study;data mining techniques;named entities;textual content;xml documents;knowledge discovery;domain-specific;textual documents;automatically determined;xml tags
frequent itemset;database systems;frequent itemset;heuristic techniques;query processing in;heuristic approach
intrusion detection systems;model trained;anomaly detection
graph partitioning;graph partitioning;cut algorithm;cut algorithm;weighted graph;linkage-based;graph model;spectral graph;theoretical analyses;data clustering;data clustering;adjacency matrix;clustering methods;data objects;objective function;clustering
data collection;data warehouse;multiple views;data mining;web server;knowledge discovery;event streams;data mining;pre-processing;data mining algorithms;application server;transaction processing
feature space;data analysis;data points;classification;sampling techniques;quadratic programming;upper bound;support vector machines;margin
data structure;closed itemsets;complete set of;mining frequent closed;mining frequent;mining frequent;frequent itemsets;candidate generation;frequent pattern;huge number of
1,3 ;classification algorithm;classification;classification;3 
clustering;graph partitioning;image segmentation;textual information;clustering method;effectively identify
clustering;automatically extracted from;document cluster;statistics-based;clustering documents;document clustering;model-based clustering
decomposition approach;decomposition approach;decomposition approach
association rules;manually assigned;dimensionality reduction;selecting features
data mining techniques;time series;time series;similar patterns;similar patterns;discovered knowledge
mining algorithms;statistical measures;human-centered;mining association rules;mining algorithm;discovering association rules;rule extraction;interesting items;discovered rules
data objects;clustering;classification;database;data mining processes;data mining;real data;distance measures;algorithm requires;iterative algorithm;databases;similarity measures;context-based;distance) measures
efficient parallel;clustering algorithms;computational complexity;data collection;estimation algorithms;database;data mining algorithms;data clustering;area network;large data sets;data sets;data mining;estimation algorithms;programming language;sufficient statistics;clustering algorithms;clustering
prediction accuracy;induction algorithms;classification;decision tree;irrelevant features;real-world;real-world;learning algorithm;data set;data set;automatic feature selection;preprocessing phase;feature subset;learning phase;feature subset selection;classifier;classification algorithm
minimum support;molecular biology;1;instance;b c;data mining
genetic algorithm;databases;mining association rules;discovery algorithm;frequent itemsets;discovering association rules;databases
prior distribution;instances;rule discovery;rule discovery
learning algorithm;equivalence classes;learning algorithms;predictive power;score-based;knowledge discovery;bayesian networks;equivalence classes;bayesian networks;score-based;instances;tabu search
inductive logic programming;data mining algorithms;data mining
simple linear;nearest-neighbors;document classification;similarity measure;centroid-based;wide range;naive bayesian;centroid-based;classifier;classification algorithm
clustering algorithms;data points;hierarchical clustering algorithms;clustering method;high quality;hierarchical clustering;compression technique;clustering result;relies heavily on;hierarchical clustering algorithm;data records;compressed data;sufficient statistics;graphical representation of
transaction data;database;mining association rules;mining association rules;knowledge discovery;association rules;graph-based;association rules;large itemsets;large database
statistical methods;neural networks;data mining based;basic concepts;data mining;artificial neural networks
knowledge discovery;tree structure
high efficiency;training data;instance-based;classification;learning approaches;emerging patterns;data reduction;test instance;instances;emerging patterns;data-representation;training instances;high accuracy;classifier;instance-based
generalized association rules;generalized association rules;mining association rules;mining process;computationally expensive;simulation results
association rules;large database;database
clustering;clustering;real datasets;synthetic datasets;similarity measure
association rule mining;databases
tree induction;decision tree;decision tree;real-life datasets;very large datasets;continuous attributes;databases
medical data;information systems;medical research;knowledge discovery;hospital information;rule induction methods;data mining;information systems;medical datasets;knowledge discovery;case studies;stored data;databases;data mining process;human beings;interesting patterns
action-rules
clustering;supervised learning;unsupervised learning;prediction problems
decision rule;nearest-neighbors;classification;neural networks;data-mining;knowledge discovery;rule-based;design parameters;data engineering;dependency analysis;instance-based;real world;data-mining and knowledge discovery
decision trees;knowledge discovery in databases;database;reduction techniques;knowledge discovery;learning algorithms;reduction techniques;knowledge discovery;overfitting problem;comprehensibility;instances;feature selection
multi-valued;feature selection method;correlation coefficient;conditional independence;database;main idea;databases;supervised learning;feature selection
large amounts of;training data;naïve bayes classifier;unlabeled documents;text classification tasks;semi-supervised learning;real-world;similarity-based;vector machine;labeled documents;supervised learning algorithms;unlabeled documents;accurate classifiers;training documents;text classifiers;text classification;semi-supervised;semi-supervised
visual feature;clustering algorithm;similar features;numerical features;data mining
dataset characteristics;takes into account;classification algorithms;ranking method;classification algorithms
clustering;clustering problem;data analysis;constrained clustering;partial order;3;mathematical framework;expert knowledge;artificial intelligence
decision trees;database;real life;sampling strategies;learning strategies;small samples;sampling strategies
large databases;sequential patterns;sequential patterns;mining sequential patterns;database scans;frequent sequences;mining sequential patterns;large database
minimum support;association rule mining;mining association rules;minimum confidence;rule sets;association rules
simulation data;high efficiency;frequent patterns;real-world;mining frequent;apriori-based;data set;adjacency matrix;graph data;association rules
closed-loop;monte carlo;large scale;brute force approach;reinforcement learning;reinforcement learning;control law
common patterns;time series;classification trees;time series;time series
clustering;clustering algorithms;real data sets;input parameters;clustering process;data set;clustering scheme;algorithm produces;clustering algorithm;clustering process;optimal number of;clustering
learning method;machine learning
rule sets;induction algorithm;classification accuracy;takes into account;predictive performance;predictive performance;relative accuracy;relative accuracy;4
web pages
clustering;clustering;categorical data;categorical data
discovered patterns;generalized association rules;knowledge discovery;chi-square;data mining systems;data cubes;patterns discovered;large database;interestingness measures
text mining;human experts;feature maps;text document;hierarchical structures;semi-structured;document collection;data mining;category structure;text categorization;knowledge discovery;hierarchical structure;text documents;automatically generate;text documents
data mining;materialized view;mining views;database;simple queries;decision support;mining views;query rewriting;data mining;patterns discovered;data mining algorithms
domain knowledge;rule sets;predictive accuracy;rule set;learning algorithms;rule sets;obtained by applying;decision model;classifier;decision making
frequent itemsets;numerical values;frequent itemsets;heuristic algorithms;support measure;processing cost
multi-valued;spatial data;multiple layers;database;association rules;association rules;geographic information systems
multi-relational data mining;language called;multi-relational data mining;wide range;inductive logic programming;relational database;specification language
clustering;multi-level;multi-level;data structures;higher-level;data reduction;statistical model;large data sets;data sets;clustering methods;model-based clustering;clustering algorithms;clustering
decision function;real-life datasets;decision trees;search strategy;optimization procedure;decision trees;feature selection
base classifiers;training set;learning approaches;classification tasks;weighted voting;dynamic integration;machine learning;instances;dynamic integration;machine learning techniques;classification error
valuable knowledge;database;cascade model;cascade model;mining process;basic assumption;instance;databases
textual data;user interfaces;search engines;web mining;data mining;web mining;pre-processing;learning task;web search engines
learning rules;classification;database;association rule mining;naïve-bayes classifier;accurate classification;machine learning;exhaustive search;accurate classifiers;classification;rule based;benchmark datasets;heuristic search;classifier;classification algorithms
decision support;interesting patterns;partially ordered;data mining;complex structures;association rule;semistructured data;semistructured data;large database
communication overhead;data mining applications;principal component analysis;heterogeneous data;computing environments;data set;data sets;web mining;distributed clustering;statistical technique;principal component analysis;link analysis;dependency structure
real-world data sets;common sense;theoretical analysis
minimum support;stock market;user-defined;application domains;mining associations;higher order
model complexity;classification;irrelevant features;vector machine;vector machine;input space;knowledge discovery;feature selection
search engines;databases;information retrieval;classification;databases
mining task;time series;time series;database application;meta learning;database objects
10;itemset support;frequent itemset;pruning strategies;frequent itemsets;data sets;frequent item-set;data mining problem
unsupervised discretization;association rule mining;real-world;context-sensitive;data set;algorithm combines;supervised methods;functional dependencies;numerical data;association rules
large volumes of data;world wide web;web usage mining;input data;web servers
predictive models;gene ontology;expression patterns;gene expression data;unsupervised learning;knowledge discovery;biological processes;auc;knowledge discovery;classification quality;medical knowledge;gene expressions;microarray data;gene expression data;microarray experiments
error-prone;genetic algorithms;predictive accuracy;classification;decision tree;genetic algorithm;large number of;decision-tree;genetic algorithm
clustering;database;customer service;similarity scores;clustering method;clustering performance;document clustering;similar documents;similarity measures
attribute values;rule generation;interesting rules;data mining technique;rough set approach
monotonicity constraints;classification trees;prior knowledge;classification performance;classification tree;data mining;prior knowledge
pattern recognition;feature sets
frequent itemset;fuzzy set;mining association rules;data mining;large databases;quantitative association rules;knowledge discovery in databases;real-life;quantitative attributes;association rules;association rules;discovered rules
classification models;neural networks;classification;predictive distribution;classification models;real world;classification model;simple models
nearest neighbor algorithm;decision trees;learning algorithm;learning algorithms;naive bayes;data sets;decision trees;multiple models;multiple models
heuristic algorithm
meta-learning;learning algorithms;data mining technology;model selection;machine learning
highly competitive;interval based;time series;time series;inductive logic programming;distance based
decision trees;decision tree;decision tree algorithms;decision tree;decision tree algorithms;design choices
clustering;clustering algorithms;rough set;rough set;text mining;knowledge discovery;text collections;document clustering;clustering algorithm;document clustering;clustering algorithm based on;document representation
database;knowledge discovery;main features;numerical attributes;fuzzy rules;knowledge discovery
belief networks;special attention;belief networks
relational databases;database;aggregation operator;classification;relational database
domain knowledge;domain knowledge;databases;data exploration;structural properties;database;machine learning;knowledge discovery;knowledge representation;domain model;mathematical framework;knowledge base;domain models
taking into account;molecular biology;dna sequences;data mining;biological sequence;real-world applications;kdd community;multiple factors;fundamental problem;complex structures;mining algorithm;gene expression;biological data
high-dimensional;approximation algorithms;high-dimensional;euclidean space;time-series;concept called
clustering;clustering algorithms;data item;clustering methods;clustering
association rules;apriori algorithm;database;database records;association rules
interestingness measure;data records;interesting rules;databases;association rules;fuzzy association rules;customer data
cosine measure;cosine measure;relevant documents;fourier transform;frequency domain;document filtering;search engines;fourier domain;latent semantic indexing
worst case
world-wide web;domain-specific knowledge;web pages;single view;information sources;learning systems;visualization tool;automatically generating;real estate;information extraction from;ad hoc
spatial relations;large text collections;semantically meaningful;text categorization;text categorization;euclidean space;information space;euclidean spaces;text databases
clustering;clustering algorithms;real data sets;clustering criteria;data set;data sets;clustering scheme;data set;algorithm selection;data mining;clustering validity;diverse domains;clustering
learning rules;association rule mining;event sequences;temporal patterns;time series;time series;instance;temporal patterns
remote sensing images;clustering;high-level;time-series;feature extraction;object-relational database;image data;time-series;knowledge discovery;user interface;moving objects;data mining;time-series;association rules;rule discovery;satellite images
clustering;multi-valued;class hierarchy;multi-valued
data mining tool;fuzzy association rules;fuzzy association rules;rule mining;semantic model;attribute values;association rules;quality measures;fuzzy association rules;theoretical basis;natural language
instances;knowledge discovery;classifier;natural language
interpretability;classification;classification;time series;machine learning;classification rules;time-series;classification problems;time series;pattern extraction
long-term;knowledge discovery;machine learning;data mining methods;machine learning
information systems;database;large databases;time-series;moving average;short term;databases;time-series;moving average;long-term;temporal databases;short-term
large data sets;data mining research;database;data mining
synthetic data;real-world;model construction;hidden markov models;learned model;databases;real world applications
phase space reconstruction;accurately identify;probability distributions;phase space reconstruction;machine learning techniques;phase space
data-driven;algorithm parameters;learning algorithms;replication;generalization performance;meta-data;meta-learning;meta-model;machine learning;error rates;case base;model selection;evaluation strategy;dataset characteristics;model selection;similarity measures
multi-objective;multi-dimensional;tree induction;classification;multi-objective;decision tree;accuracies;decision tree;decision-tree learning;single-dimensional;readability;data set;attribute values;data analysis;decision tree;decision-tree;tree construction;data sets;classifier;multiple objectives
domain experts;search algorithms;rough set;medical databases;interesting patterns;databases;rule induction methods;decision processes
evolutionary algorithms;genetic programming;encoding scheme;rule set;classification rules;evolutionary process;rule sets;classification rules;extracting knowledge from;genetic programming;data mining;evolutionary algorithm;normal form
data analysis;missing values;large number of;knowledge discovery;multi-label;class labels;complete classification
frequent patterns;temporal information;event sequences;multi-relational;data set;case study;event sequences;association rules;association rule mining
probability distribution;chi-square
density estimation;document summarization;classification;semi-supervised learning;automatic text summarization;instance;semi-supervised learning;em algorithm
aggregate functions;data sets;data mining;mining algorithm;data mining algorithms;relational data
quality guarantee;active learning;sampling techniques;distinguishing feature;database;very large datasets
storing data;web pages;collaborative filtering;collaborative filtering
fast algorithm;hypothesis space;knowledge discovery;apriori algorithm;association rules;association rule;association rules;bayesian framework
standard svm;function approximation;classification;policy gradient;optimization problems;linear programming;mathematical programming;reinforcement learning;machine learning;temporal difference;weight vector;kernel methods;reinforcement learning;decision boundaries;support vector;support vector machines;margin;excellent performance;support vectors
discovered patterns;fuzzy association rules;information-theoretic;attribute domains;information theory;surprising patterns;quantitative association rules;data set;strong evidence;interestingness measures;data mining;fuzzy association rules;interestingness measures
search methods;genetic algorithms;exhaustive search;genetic algorithm;evaluation metric;genetic algorithms;search technique
simulated annealing;support vector machines;kullback-leibler;text categorization;code length;text categorization;text corpora;binary classifiers;multi-class
classification problem;naive bayes;classification methods;information extraction;preprocessing phase;computationally expensive;relevant information;feature selection
data reduction;progressive sampling;data samples;pruning technique;sampling based;weighted voting;data reduction;boosting algorithm;spatial correlation;data sets;multiple models;computational cost;data examples;spatial data;random sampling;induction algorithms
domain knowledge;mining algorithms;frequent patterns;special attention;case study;user-defined;user-defined;case study;apriori algorithm;data mining algorithms
association rules;pruning methods;closely related;effective pruning;discovered association rules;association rules;association rules
hierarchical clustering algorithms;vector space;hierarchical data;clustering algorithms;hierarchical data;space complexity;clustering algorithm;clustering algorithm based on;clustering algorithm based on;high order;clustering quality
large data sets;visual data mining;data sets
evaluation methodology;machine learning;classification performance;text classification;information gain;feature selection;data mining;text classification;support vector machines;text classification problems;feature selection
dynamic range;training data is;vector machine;quadratic programming problem;kernel approach;kernel methods
text mining;text mining;vector space;document ranking;storage space;text documents;discrete cosine transform;fourier domain;discrete cosine transform
spatial data;data base;database;object-relational;query language;spatial database;object-relational;search strategies;data mining algorithms
clustering;web portals;ontology-based;input data;hierarchical clustering algorithm;knowledge management;current web;multimedia retrieval;semantic web;pre-defined;machine learning techniques;similarity measures;ontology-based;clustering
gene expression
factor analysis;signal processing;latent variables;text document;component analysis;observed data;statistical model;independent component analysis;case study;time series;multivariate data;principal component analysis;blind source separation;independent components;large database
ensemble members;global model;neural network;rule-set;neural networks
data mining methods;binary attributes;regression analysis;originally designed;quantitative attributes;association rules;association rules
distance measure;classification models;compared with conventional;training data set;highly accurate;data set;classification model;artificial data sets;combining multiple;learning method
database;decision rules;databases;rough sets;databases;bayesian networks;rule induction methods;decision processes
high-dimensional;high-dimensional;data mining method;database;real-world;databases;multiple hypotheses;data set;instances;feature subsets;selection methods;feature subset;databases;noise levels;predictive accuracy
computational complexity;algorithm performs well;semi-structured data;semi-structured data;classification accuracy;information entropy;theoretical analyses;interesting patterns;ordered trees;pattern discovery;real datasets
probability distribution;sequential nature;natural language;domain specific;machine learning
search space;outlier detection;data set;distance-based;nearest neighbors;high dimensional spaces
mining task;association mining;accurate models;high accuracy;data updates;databases;databases;data mining;machine learning and data mining
mining algorithms;data mining;time series;time series;detection algorithm;databases;real world
commercial search engines;web servers;web search engines;retrieval performance;long-term;incremental learning;test collections;document transformation;long-term;web search engines;document representations
rule learning;classification;actionable knowledge;actionable knowledge;actionable knowledge
compares favorably with;specific information;structured documents;structured documents;information extraction;tree structure;tree automata;information extraction;benchmark data sets;tree automata
data owner;data privacy;data mining
utility function;main memory;sampling algorithm;sampling techniques;hypothesis space;large databases;data mining tasks;large databases;instance;quality guarantees;greedy algorithms;pattern discovery;sampling-based;large database
data streams;observed data;estimation process;stock-market;stock data;data mining system;covariance matrix
false positives;induction algorithm;real-world;class distributions;cost-sensitive;skewed;cost-sensitive;rare classes;benchmark datasets;boosting algorithm;classifier

time-series;time-series;medical databases;similar sequences;clustering
search space;large numbers of;frequent sets;instances;frequently-occurring;databases;association rules;special case;generic algorithm
multi-relational data mining;multi-relational;aggregate functions;aggregate functions;data mining;mining algorithm;relational data
clustering;high-dimensional;vector space;market basket" data;mixture model;large-scale;mixture models;data exploration;basic concepts;web site;vector data;prediction problems;real-world;real-world applications;machine learning and data mining;mixture models
sequence mining;domain-specific;xml documents;domain-specific;information integration;frequent sequences;probabilistic xml;association rule discovery;document type;xml tags
prediction model;separability;edge weight;neighborhood graph;supervised learning;separability
singular value decomposition;statistical properties;classification problem;svm) classification;temporal sequences;vector machine;real data;accurate predictions;svm classification;classification approach;event data
clustering;high-dimensional datasets;large scale;algorithms for computing;matrix decomposition;error bounded;data gathering
data objects;clustering;unsupervised learning;similarity matrices;data clustering;principal component;principal component analysis;dimensionality reduction
long term;data mining;data mining
frequent itemset;mining algorithms;frequent itemsets;real-life datasets;concise representations;candidate itemsets;concise representation;frequent itemsets;highly correlated;frequent set;support threshold
classification accuracy;speech recognition;svm classification;document classification;classification
classification learning;large data sets;learning algorithm;data set size;large data sets;data sets;machine learning algorithms;classification error
prediction accuracy;rule induction methods;high level;expression levels;classification;gene expression;information gain;data sets;finding patterns;feature selection;small sets of;expression patterns;interpretability;gene expression;prediction models;array data
clustering;variable size;common properties;clustering approaches;categorical data;transactional data
discovered patterns;association rule mining;closed itemsets;association rule mining;data mining task;data mining;highly correlated
operator;perfect information;game theory
randomized algorithms;data mining applications;expectation maximization;data set;clustering methods;algorithms for computing;robust estimation
clustering;clustering algorithms;spatial proximity;high-density;high-quality;data points;optimization problems;prior knowledge;data sets;m+r;line-segments
data point;clustering;clustering results;expectation maximization;randomly generated;clustering algorithm;clustering algorithm;data distribution
clustering;data values;ground-based;large volumes of data;clustering technique;data sets;range queries;remote sensing data;range queries
data structure;pruning techniques;candidate) patterns;candidate patterns;temporal patterns;discovery algorithms
data analysis;spatial data;data mining and knowledge discovery;spatio-temporal;data mining research;temporal data
probability distributions;data mining;probability distribution;partial order;data mining;data mining problem;temporal data
classification rules;classification rules;tree based;mining process;databases
rough sets;spatio-temporal;rough sets;spatio-temporal;data mining
information systems;taking into account;data mining methods;index structure;spatial data mining;data stored in;spatial databases;spatial data mining
temporal patterns;data mining;time series;time series;data mining;temporal patterns
fine-grained;digital information;web application;case study;meta data;distributed systems
petri nets;multimedia applications;graphical model
high-dimensional;high-dimensional;nearest neighbor;clustering approaches;compression techniques;compression techniques;problem domain;nearest neighbor
web service;detailed comparison;data availability;node failures
search systems;query optimization;query processing strategies;response times;distributed query processing;query language;user profiles;search strategies;operator;user profiles
xml applications;rewriting algorithm;rewriting rules;data processing;xml processing
access pattern;cache management;cache management;performance degradation;interval-based;cache management
xml schemas;data warehouses;extensible markup language;html pages;xml documents;olap applications;databases;automatically generate;xml schema
query optimization;database;multi query optimization;multi query optimization;data mining;relational database;database instance
data warehouse;software architecture;xml-based;data warehouse systems;software architecture;data warehousing;software components;metadata management
document retrieval;disk scheduling;simulation result;multimedia data;data blocks
xml documents;highly relevant;classification;structural patterns;xml documents;document structure;data mining;association rules
management systems;knowledge management;relevant documents;instance;structured information;text retrieval;document type;programming language
ct images;database;object-oriented;multimedia objects;textual documents;multi-media
native xml;database
sql query;xml documents;xml document;semistructured data;relational database system;xml query language;relational data
image clustering;clustering method;clustering methods;specific domains
xml tree;takes into account;xml applications;xml data;xml queries;xml documents;query processing;xml data;structural information;arbitrarily large
video retrieval;retrieval systems;databases;complex objects
information systems;database technology;database;xml data sources;query language;data model;collect data;xml documents;object-oriented;large repositories of;relational databases;relational dbmss;entity-relationship;relational dbms;data management system;xml technologies
query optimization;data streams;large number of;stream data;data streams;continuous queries
xml documents;xml data;xml publishing;document representation;instance
xml document collections;xml document collections
xml documents;xml document;database schema;object-relational;xml documents;object-relational;modeling approach;databases;object oriented;relational dbms
web-based;semantic web
growing number of;object model;xml schemas;xml documents;xml-based;xml schema;document type definitions;data bases;xml schema
mobile environment;rates;data management;transaction execution
clustering;video segmentation;clustering technique;clustering technique;feature extraction;video databases;camera motions;wide range;database management system;information needed;high-level;video segments
query execution plan;optimizer;heterogeneous sources;object-oriented;query processing techniques;query processing in;query processing;databases;autonomous data sources;distributed data;data integration;query processor
high-dimensional;high-dimensional;multimedia databases;compression techniques;compression technique;similarity search;retrieval performance;similarity search
ad-hoc;scientific data;retrieval tasks;data quality;data sets;semantic information;primary goal
geographically distributed;html pages;feature selection technique;common interests;html documents;compares favorably with
xpath queries;xml instances;query capabilities;xml query languages;xml documents;xml schemas;high-level;xml trees;xml schema
distributed data sources;context-aware;distributed databases;mobile communication;mobile access;information services;local information;access information
query operators;xml databases;document content;xml query language;query operators
high-dimensional;image retrieval;multimedia databases;similar objects;database;similarity queries;index structures;similarity queries;similarity queries;user defined;access structures
querying xml data;database;relational dbms;xml data;data type;extensible markup language;xml documents;storage space;algorithm requires;xml document;large repositories of;querying xml data;relational dbmss;object-relational dbms;data management systems
web-based;database;query interface;database;visual features;video object;content-based video;low-level;operator;relational dbms;semi-automatic
web portal;web applications;profile-based;increasing number of;large number of;bandwidth consumption;data access;profile-based;data delivery;web browsing
ibm db;data warehousing;databases
real-life;streaming applications;continuous media
database systems;application requirements;wide range;databases;operator;database systems
fall short;data services;database;meta-data;high quality;client applications;resource management;data management;data access;resource usage;data management systems;data management system;data services
search space;database;query optimizer;data structures;main-memory;data independence;plan generation;resource bounded;query processing in;low level;query plans;database technology;optimizer
data model;management architecture;low-level;past experience
huge amounts of data;sql queries;on-line analytical processing;commercial dbms;olap queries;4;operator;query engine
response times;completeness;replication;durability;high availability;main-memory;main memory;replication;database;high availability;data storage

traditional databases;13;20;network applications;database;network nodes;highly distributed;database technologies;databases
domain specific;input data;data warehouses;data warehouse;case study;data warehouse;application developers;network traffic
information systems;highly heterogeneous;query capabilities;scientific applications;data representation;data sources;integration systems;query languages;complex applications;data integration
semantic indexing;concept-based;meta-data;scientific data;heterogeneous data sources;text-based;application domains;web-accessible;instances;collect data;data describing;database integration;distributed data;data integration;global schema
15;14;semi-structured data;document management;18;xml content;xml documents;document management
relational databases;xml data;xml repositories;hierarchical model;relational model;data interchange;native xml;databases;xml-enabled;xml query language;data management
xml databases
data integration;source selection;web services;data integration;business data
multi-user;xml data;xml data management;1;management systems;xml database systems;user evaluation
pre-defined;genetic algorithm;xml data;relational database systems;query performance;relational schema;xml data
similarity query;relevance ranking;xml data;similarity queries;relevant data;xml data;query conditions;data organization
regular tree;formal framework for;relational models;regular tree;xml-schema;constraints imposed by;normal form;semantic constraints;xml schema
xml element;instance;path expressions;xml data;xml documents;instances;xml database systems;query processing;path expression;databases;query languages;optimization techniques
xml documents;xml documents;decision support
web-based;management architecture;information sources;web-based;database;digital information;allowing users to;semantic information
query rewriting;xpath queries over;formal framework for;formal methods;xpath queries;xml documents;conjunctive queries;xpath queries;semistructured data;relational database system;distributed caching
graph data;knowledge discovery;representation language;inference algorithm;logic programming;knowledge discovery;graph data
attribute selection;pre-processing;database;data mining applications;complete set of;knowledge discovery;data sets;knowledge extraction;irrelevant attributes;data mining;relevant attributes;real world;pre-processing
clustering;clustering;fast algorithm;meaningful patterns;clustering algorithms;density-based clustering;database;real data;density-based;density-based clustering algorithm;data set;density-based clustering;equivalence relationship;1;large database;data mining;clustering algorithm called;input parameters;large database
world wide web;data sources;valuable information;financial services
clustering;high-quality;high quality;data sets;geo-referenced;geo-referenced;objective function
semi-structured data;user specifies;association rule mining;digital libraries;association rules;textual documents;large itemsets;higher order
association rules;numeric attributes;mining association rules;data mining approaches;numeric attributes
statistical methods;classification;data mining and knowledge discovery;medical diagnosis;classification methods;decision-tree;knowledge discovery;knowledge based;rough sets;classification;relational databases;neural networks
high-dimensional;input data;neural network model;data set;cluster assignment;prior information;cluster analysis
data mining processes;data mining;attribute values;data mining
entropy-based;discretization methods;classification rules;information entropy;minimum description length;classification rules;3;predictive performance;instance;continuous attributes
causal model;causal model;induction algorithms;theoretical result
base classifiers;distributed data sources;learning process;distributed data mining;meta-learning;data set;learning phase;data mining applications;distributed data mining;data mining algorithms
simulated annealing;classification problem;mutual information;genetic algorithm;real-world;information theory;highly accurate;excellent performance;distance metric;classification problem;distance metric;high risk;nearest neighbour;real world;classification task;nearest neighbour
business intelligence;data warehouses;decision support;database servers
mutual information;test data;error rates;selected features;raw data;knowledge discovery;feature selection;feature selection;real world;real world domains;feature based;selecting features
large itemsets;discovering association rules;data analysis;mining association rules;data mining
database theory;data mining;real life applications;data mining
data structure;13;15;14;semistructured data;incremental mining;huge amounts of;incremental mining;raw data;9;8;semistructured data;semistructured data;web data
network model;path planning;neural network;model called;neural network
inter-transaction;inter-transaction;database research;data mining;association rule;association rule;association rule
world wide web;web pages;extraction techniques;knowledge discovery;structured information;natural language processing;perform poorly
association rules;database;relational databases;databases;large databases;discovering association rules;transaction database;missing data;relational database;databases;association rules;missing values;association rule
domain knowledge;attribute reduction;database;rough sets;database
transaction data;graph structure;frequent itemsets;graph patterns;association rules;graph structured data;confidence levels
association rule mining;association rules;observed data;mining process;association rules
clustering;clustering;algorithm produces;space-partitioning;clustering method
medical databases;bayesian networks;databases;rough sets;rule induction methods;decision processes
attribute values;databases;theoretical foundation;databases
scientific discovery;extended abstract;scientific discovery
database;databases;data mining;concept called;databases
unified framework
event processing;pattern language;area network;event processing
rough set;conditional probabilities;missing values;medical databases;databases;rule discovery;rule induction methods;missing values
incremental algorithm;rough set theory;database;1;incremental learning;rough sets;rule extraction;rough sets
minimum description length;human experts;existing knowledge
summaries generated;information theory;probability distribution;highly correlated;discovered knowledge;databases;interestingness measures
database;databases;large databases;real data;knowledge discovery;complex patterns;databases;complex patterns
information-theoretic;database;information-theoretic;data mining
10;12;classification accuracy;6;large databases;intelligent systems;data mining;9;8;2,1;feature based
clustering;clustering;classification;classification;data mining techniques;large volumes of data;database applications;data mining techniques;rule generation;dimensional data
time-series;time-series;data mining and knowledge discovery
database;document retrieval;document structure;information retrieval;knowledge discovery;knowledge embedded in;knowledge representation;retrieval results;document databases
tree based;genetic algorithm;decision tree;information theory;decision trees;information gain
databases;mining algorithms;database;graph-based;large databases;mining association rules;discovered association rules;graph-based;association rules;large itemsets;large database
discovered patterns;linguistic patterns;data mining techniques;stock market;data set;linguistic patterns;numerical data;hong kong
problem solving;data owners;data mining;data mining;large collections of;data mining projects;data mining;evolutionary approach
data mining;database systems;sql queries;association rule mining;large scale;data mining;large scale;processing nodes;sql query;databases;relational database;association rule
data-driven;mining algorithms;special attention;data sets;discovery algorithm;rule discovery;rule discovery
improving accuracy;decision trees;decision tree;missing values;information contained in;learning algorithm;rates;computational cost;missing information;missing values
classification learning
domain knowledge;domain knowledge;natural language interface;databases;natural language interface;databases;semantic information
instance-based;classification;classification tasks;database queries;algorithm's performance;continuous attributes
database;interesting association rules;association rules;interesting rules;databases;association rules;existing knowledge;discovered rules
real-life data sets;data mining techniques;databases;efficient search;data sets;finding patterns;data mining
supervised learning;data-mining;data mining
application domain;clustering algorithm;clustering process
generation algorithm;candidate itemsets;association rule mining;candidate set;database;apriori algorithm;apriori-based;association rule mining;candidate set;2;database scans;generation algorithm;large itemsets
tree structures;classification;classification;average error;bayesian classifiers;naive bayesian;classifier;naive bayesian
classification problems;large number of;neural network;classification;neural networks
10;genetic algorithms;evaluation metric;causal discovery;kullback-leibler;causal models;sample sizes;3;predictive performance;causal models;prior probability
prediction accuracy;training data;test data;average error;missing values;missing attribute values;classifier learning;unseen) data;missing values
prediction accuracy;training set;decision trees;great success;classifier learning;distributed processing;tree induction;large datasets;attribute selection;classifier
logic programming;inductive learning;induction algorithm;classification;logic programming;inductive logic programming;numeric attributes
rule extraction from;decision trees;neural networks;rule extraction from;prediction models;knowledge discovery in databases;prediction models;rule extraction from;accurate predictions;association rules;prediction models
decision tree;decision tree;data pre-processing;logic programming;data pre-processing;association rules;selection methods;data mining;association rules;data driven
database;distributed memory;principal component analysis;data allocation;storage capacity;association rule discovery;association rule discovery;data distribution
classification;spatial-temporal;database applications;algorithm selection;selection techniques;large amounts of data;data mining;clustering;large number of;data mining systems;data mining algorithms;database researchers;data analysis;database technology;association rule mining;data mining;data mining;large volume;database management systems;data mining techniques;machine learning;database application;databases;mining algorithm;semi-automatic
stored data;inductive learning;database;decision tree;human experts;minimum description length;data sets;knowledge acquisition;knowledge acquisition;knowledge base
segmentation methods;database;probabilistic segmentation;multiple topics;information access;keyword based;segmentation algorithms
knowledge management;extracting knowledge from;automatic speech recognition
speech recognition;maximum entropy model;hand-crafted;named entities;spoken document retrieval;extraction methods;named entity;information extraction from
speech recognition;language model;text retrieval;speech recognition;retrieval accuracy;statistical language models;retrieval methods;speech recognition;test collections;text retrieval;real world applications
clustering;multimedia documents;clustering technique;similarity measure;similarity measure;chi-square;automatic generation of;similarity based
text documents;information retrieval
real-world datasets;training data;classification;classification;direct comparison;poor performance;skewed;test instance;emerging patterns;decision tree classifier;distinguishing features
hierarchical classification;hierarchical classification;classification;classification;document classification;internal node;large number of;high level;single classifier;classifier;fault tolerance
amino-acid;amino-acid;knowledge discovery;data set;knowledge discovery;peculiar data;association rules
training set;function approximation;variable selection;time series;input-output;machine learning;pattern classification;data sets;decision trees;input/output;feature selection;databases;nearest neighbour
concept space;related terms;computationally intensive;concept space;information retrieval;information retrieval systems
domain knowledge into;decision trees;user's preference;decision tree construction
clustering problem;medical image;classification;database;semi-supervised learning;learning problem;medical image;database;semi-supervised clustering;semi-supervised learning;graph-based;semi-supervised
attribute values;generated automatically;concept hierarchies;concept hierarchies;semantically related;relational model;data model;attribute domains;relational structures;data mining;real world;relational data
domain knowledge;data mining methods;rough set;data mining techniques;similarity metrics;classification rules;knowledge extraction;data mining approach;rough set theory;knowledge discovery process;intelligent tutoring systems
matching algorithms;spatial objects;access methods;query pattern;spatial objects;index structure;time series;indexing method;time-series
statistical approaches;neural networks;classification;database;feature space;text documents;training examples;high dimensional;text categorization;text categorization;predictive accuracy;knowledge based;relevant documents;supervised learning;user-defined
wallace and korb, 1999;predictive accuracy;rule-based;bayesian networks;data mining;bayesian networks;spirtes et al., 1993
database technology;data mining techniques;association rule mining;data mining;interesting patterns;data set;large data sets;share similar;data mining technology;database;data mining;completeness;knowledge discovery
linear classifier;decision tree;sample data;decision tree;multilayer perceptron;multilayer perceptron;data sets;linear classifiers;input variables;numerical attributes;neural network
document collection;document categorization;learning phase;real-world;meta-learning;linear combination;meta-model;learning models;document categorization;document set
agent based;visual data mining;web-mining;web-based;development environment;web-based;agent based;agent-based
induction algorithm;classification;training data;induction algorithms;classification;decision tree;categorical attributes;training data is;distance measure;supervised classification;test instance;data sets;data set;continuous attributes;nearest neighbour;algorithm maintains;neural network;instance-based
algorithm generates;rule set;class association rule set;mining process;databases;association rules;avoid redundant;class association rule set
3, 4
association rules;discovered association rules;minimum confidence;minimum support
association rules;incremental computation;rule set;concise representation
clustering;feature vector;data mining techniques;interesting patterns;feature selection;similar patterns
association mining;mining framework;association mining;ad-hoc;user-defined;user requirements;instances;data mining task;mining tasks
clustering;hybrid approach;arbitrary shape;large databases;cluster quality;data set;clustering methods;clustering algorithm
clustering problem;neural networks;relevant documents;text documents;topic detection;clustering algorithm;trend analysis;neural networks
synthetic data;partial information;database;similarity queries;time series;time series;similarity queries;real data;finding similar;similarity based
13;exploratory analysis;spatial clustering;6;data mining;clustering algorithm;cluster boundaries;nearest neighbors;spatial data mining
user preferences;recommender systems;recommender systems;linear models;linear classifiers;computational requirements;decision trees;historical data;filtering methods;web-site;linear classifiers;recommending items
clustering;hierarchical clustering algorithms;multi-dimensional;similar objects;data exploration;agglomerative hierarchical clustering;single-dimensional;finding clusters;memory requirement;clustering algorithm
evolutionary algorithms;learning algorithms;learning bayesian networks;hidden variables;hidden variables;network structure;bayesian networks;operator;em algorithm;network structures;incomplete data;evolutionary algorithm
highly skewed;summaries generated;synthetic data;diversity measures;information theory;theoretical results;discovered knowledge;databases;patterns discovered;large database;interestingness measures
text mining;text mining;text documents;incoming documents;training documents;training corpus
clustering;high quality;clustering methods;data points;data sequences;agglomerative hierarchical clustering;data mining;clustering method;traditional clustering algorithms;databases;frequently occurring
indexing method;case-based reasoning;data warehousing;data warehouse;storage space;similarity indexing;data warehouse;on-line analytical processing;indexing method;data warehousing;analytical processing;source databases;decision-support
scaling factors
data structure;concept hierarchies;concept hierarchies;data structure;transitive closure;data types;main memory;object-relational;database management system;databases
mining frequent sequences;frequently occurring;mining frequent sequences;database;transactional databases;1;iterative algorithm;frequent sequences;cpu cost
decision trees;predictive accuracy;rule set;decision tree;rules generated;decision tree learning;hierarchical structure;pruning algorithm;rule sets;tree structure;decision tree learning;decision trees;large datasets;data mining;optimization strategy;high accuracy;existing knowledge;discovered rules
data mining tasks;data collection;data mining;database queries;data mining;relational database
incremental algorithm
web documents;tree structured;web documents;tree pattern;discovering frequent;tree patterns;xml documents;labeled tree;tree structured;semistructured data;data mining problem;semistructured documents
decision trees;database;random walk;decision tree;knowledge discovery;linear complexity;comprehensibility;average case
world wide web;speech recognition;automatically extracting;multiple hypotheses;error rates;wide range
error rates;speech recognition;related words;spoken document retrieval;information retrieval
retrieval systems
discovered patterns;distance measure;mining framework;time series;rates;temporal data mining
rough set theory
visual field;machine learning methods;classification;data mining techniques;data mining;data mining problem
data sequences;index structure;content-based retrieval;data mining;index structure;sequential patterns;post-processing;traditional database;databases;source databases;optimization techniques;exact-match;sequential pattern
amino acid;classification;rough set;clustering method;artificial data;clustering method;equivalence relations;clustering result;equivalence relations;rough set theory;numerical data;cluster validity;weighted sum of
domain knowledge;domain knowledge;data mining tasks;induction algorithm;classification;knowledge discovery in databases;stock market;prediction performance;knowledge integration;knowledge discovery;real-life;data preparation;induction algorithms;data mining;interesting patterns;knowledge-intensive;induction algorithm
human behavior;stock market;knowledge extraction;rates;language independent;human beings;hong kong;knowledge extraction;cross lingual
learning process;data compression;classification;data reduction;real data;classification process;data compression;nearest neighbor rule;classification task

dependency relations;learning algorithm;predictive accuracy;data mining applications;learning algorithms;naive bayes;naive bayes classifier;comprehensibility;large datasets;data mining applications;large number of
high level;data mining results;electronic commerce;real data;web logs;data transformations;data mining;pre-processing;web logs
web documents;human effort;training examples;data structure;semi-structured;pattern mining;web information extraction;extraction rules;information extraction;instances;search engines;integration systems;pattern mining;automatically generate;machine learning;human intervention
sequence patterns;decision tree;categorical datasets;pattern mining;input-output;similar sequences;data generated from;sequence patterns
instance-based learning;feature selection method;dataset characteristics;meta-learning;feature selection;learning problems;effective classification
simple linear;mining frequent patterns;frequent patterns;frequent patterns;data structures;frequent pattern;apriori-based;frequent itemsets;candidate set;data mining;candidate generation;frequent pattern
nearest neighbor classification;classification techniques;classification quality;classification;multi-modality;large number of;text categorization;text categorization;data set;data sets;training samples;naive-bayesian;nearest neighbor;hill climbing;feature weights;classification algorithms
distance measure;regularization;neural networks;classification;metric space;radial basis function;high dimensional spaces
real valued;rough set;rough sets;interesting rules;rough sets;rule discovery;pre-processing
base classifiers;na茂ve;na茂ve bayesian;na茂ve;bayesian classifiers;boosting algorithm;bayesian classifiers;training samples;decision trees;naïve;optimal performance;bayesian learning
computational complexity;encoding scheme;19;causal discovery;data mining and machine learning;large number of;causal models;discovery algorithm;causal models;information-theoretic;causal structure
user access;optimal algorithms;web log mining;data structures;data preparation;web logs;user access;higher level;web logs
intrusion detection;training data;network security;intrusion detection;detection approach;user profiling;hidden markov models;user profiles;model parameters;modeling approach;storage requirements;instance-based learning;maximum likelihood;hypothesis testing;usage data;occurrence frequency
high quality;predictive accuracy;classification;emerging patterns;6;classification systems;emerging patterns;huge number of;emerging patterns;2;accurate classifiers;7;multi-attribute;classifier
human perception;domain knowledge;visualization technique;entire process;user's preference;classification rules;data sets;classifier
scale poorly
domain knowledge;attribute values;genetic algorithm;continuous data;na茂ve;evolutionary approach;bayesian classifiers;probabilistic models;evolutionary approach;classification task;classifier
clustering algorithm;data mining;database;high dimensional space;data mining
frequent itemset;database;databases;evaluation function;frequent itemsets;association rules;association rules;association rules;evolutionary algorithm;evolutionary algorithm;numeric attributes
outlier detection;credit card fraud;data sets;outlier factor
incremental update;concept space;related terms;pruning technique;incremental update;computationally intensive;information retrieval;information retrieval systems
document collection;incoming documents;document classification;classification;large-scale;large number of;tree-based;classification performance;multilingual documents;multilingual documents
clustering;clustering;information management;user-defined
accurate models;application domains;privacy preserving data mining;data records;data mining;privacy preserving data mining
world wide web;web pages;web documents;semi-structured;web page;information extraction;web pages;structured data from;pattern discovery;labeled examples;wrapper generation
accuracy compared to;nearest neighbor;classification;large volumes of;sequence data;svm-based;markov model;biological information;databases;protein sequences;support vector machines;machine learning algorithms;biological sequences;classification algorithms
incremental algorithm;synthetic data;data mining techniques;temporal sequences;real data;efficient discovery of;rule-sets
data mining techniques;data mining;mining association rules;user requirements;interesting association rules;transaction database;data mining;association rules;large database
association-rule mining algorithms;mining algorithms;association rule mining algorithms;synthetic datasets;database;mining process;frequent itemsets;mining algorithm
predictive modeling;data mining and knowledge discovery;database;predictive modeling;conventional techniques;case study;hidden markov models;hidden markov model;data mining;network data;network data;real world applications;traffic data
data warehouse;data warehouse;query efficiency;selection process;cost model;materialized views;materialized views;lower cost;data warehousing;selection algorithm;total cost
discovered patterns;web pages;web access;data mining techniques;internet users;web log data;data mining;sequential pattern mining;web log
incremental algorithms;incremental update;frequent sequences;mining frequent sequences;incremental update;database;real life;frequent sequences
clustering;clustering;clustering validation;clustering approaches;clustering techniques;clustering analysis
response times;relational database system;mining framework;highly interactive;association rule mining;knowledge discovery in databases;rule generation;data mining;association rules;query languages;mining algorithm;execution times
naive algorithm;data cube;exploratory mining;optimization methods;constraint-based;mining process;user-defined;5;data cubes;constraint-based;data cubes;exploratory mining;automatically detects;multidimensional space
statistical approaches;nearest neighbor;vector machine;data mining techniques;assignment problem;decision-making;naive bayes;naive bayes classifier;classification results;bayes classifier;gibbs sampling;multi-layer;bayesian framework;assignment problem
genetic programming;classification problem;classification;classification;genetic programming;fitness function;knowledge discovery;training data;high accuracy;data mining;learning scheme;classification learning;learning strategy;classifier
databases;pattern-growth;data mining;pattern-growth;sequential patterns;sequential patterns;efficient discovery of;sequence database
learning mechanism;preprocessing phase;knowledge base
low support;interesting patterns;frequent itemsets;high confidence;association rules mining;real world applications
attribute values;random variables;numerical values;data mining;data model;association rules;association rules
class label;learning algorithm;classification methods;lazy learning;naïve bayes;classification method;na茂ve bayes;classifier
maximum entropy;closed form solutions;algorithms produce;small sets of;data mining;association rules;interesting association rules
computational efficiency;database;web application;web mining;large datasets;data mining;web mining;web data
nearest neighbor classification;nearest neighbor;classification;classification;class label;data-mining;data streams;higher classification accuracy;training dataset;data streams;spatial data;classifier;nearest neighbors
data structure;summaries generated;directed graph;data mining tasks;data set;partial order;databases;total order;rank order
customer relationship management;low cost;application domain;customer segmentation;customer relationship management;data warehousing;data analytics;data mining;data management
text mining;association patterns;electronic documents;electronic documents;structural characteristics;semistructured documents;mining algorithm;semistructured documents
fault-management;large amounts of;data-mining models;communication network;accurate classification;regression tree;data sets;network-management;data-compression;capacity planning;data mining;data tables;data warehousing;network data;data-mining techniques;event-correlation;communication networks
clustering;mixture model;classification;probabilistic approach;probabilistic model;categorical data;instance;categorical data;clustering methods;classification;em algorithm;categorical attributes
web documents;web documents;tree patterns;tree pattern;discovering frequent;tree patterns;labeled tree;tree structured;semistructured data
concise representation;frequent patterns;frequent patterns;sequential patterns;frequent itemsets;association rules;data mining problems
clustering;data mining techniques;stock market;time series;weighting scheme;data mining;text mining techniques;segmentation algorithm
human behavior;stock market;knowledge extraction;domain independent;natural language texts;hong kong;natural language texts
cluster-based;missing values;linear complexity;large datasets;cluster-based;processing cost;missing values
data sizes;mining algorithm;pruning techniques;tree) structure;association rule mining;fp-growth;spatial data;mining process;association rule mining;interesting patterns;market basket data;tree structure;compressed representation;association rules;image data;association rule;resource discovery
clustering;real data sets;gene expression data;microarray data;high quality;high efficiency;clustering methods;data mining;clustering quality;clustering approach;gene expression data
search space;fp-tree;frequent patterns;association rule mining;pruning strategies;fp-growth;minimum confidence;association rules
parameter values;algorithm requires;minimum support;level set
vector space;random fields;vector representation;optimization problem;map) estimation;noisy data;model parameters;real images
distance measure;point-set;point correspondences;distance measure;real-world;generative models;distance measures;generative model;image matching;graph matching;weighted graphs;point-sets
sample size;observed data;minimum description length;computational cost;mixture models;expectation-maximization
auxiliary;large number of;markov random fields;image processing;higher order;auxiliary
computational efficiency;markov random field;tree-based;image analysis;image analysis;grid-based
feature detection;high quality;simple heuristics;facial features;rates;eye movements
prior distribution;cost functions;image segmentation;segmentation results;posterior distribution;synthetic data
1,2,3,4,5,6;magnetic resonance imaging;mri data;entropy measure;high resolution
constraint satisfaction;simulation results;labeling schemes;dynamical systems
regularization;markov chain;auxiliary;sampling algorithm;takes into account;markov chain;optimal parameters;operator;maximum likelihood;satellite images;image restoration;monte carlo
energy functions;output quality;local minimum;graph cuts;energy minimization;energy minimization;6;approximation algorithm;matching problem;np-hard
hill-climbing;hill climbing;genetic algorithm;convergence rate;graph matching;search procedure;theoretical justification;graph matching
multi-valued;vision systems;data fusion;data fusion;variational framework;boundary detection
parameter tuning;14
data-driven;markov random field;image segmentation;16;tree structure;noise model;image segmentation
numerical simulation;image processing;level set
free-form;real data;energy minimization;shape information;free-form;sampling strategy
human brain;markov random field;special attention;parameter estimation;markov random field;em-algorithm;fmri data;magnetic resonance;machine learning;theoretical justification;unlabelled data
problem instances;information theory;tree search;real-world;tree search;probability distribution;vision problems;convergence rates;convergence rates;bayesian formulation;rates;probability theory;reward function
hand-drawn
motion parameters;energy function;image sequences;intensity values;image sequence;distinguishing features;maximum likelihood;maximum likelihood;object shape
clustering;feature space;image patches;path based;graph theoretic;data clustering;low-dimensional;data sources;multi-scale;path based;cost function;dimensional data;texture segmentation
conditional likelihood;conditional likelihood;long-range;takes into account;test images;texture segmentation;training sample
surface reconstruction
takes place;desired-properties;human visual system
matching techniques;genetic algorithms;pattern recognition;matching problem;graph matching;evolutionary computation;optimization algorithms;instance;evolutionary computation;graph matching;continuous domains;combinatorial optimization problem;image recognition
contour extraction;extraction algorithm;spatial locations;random noise;contour extraction;natural images
body parts;motion analysis;object tracking;genetic algorithm;genetic algorithm
fast marching;key points;complete set of;connected components
tree matching;local optima;relational structures;vision problems;globally optimal solution;free trees;game theory;free trees
search algorithms;global optimality;energy minimization;numerical experiments;energy function;learning problems;neural network
directed graphs;image segmentation;globally optimal
prior distribution;information-theoretic;variational approach;gradient descent;image denoising;maximum entropy;variational formulation
decomposition method;image representation;metropolis hastings;faster convergence
11;markov random field;local features;shape descriptor;kullback-leibler;9;small size
markov random field model;spatio-temporal;markov random field model;moving objects;tracking problem;mrf model;spatio-temporal
11;10;13;12;22;25;optimization problems;special case;free energy;free energy;3;5;4;7;24;8;belief propagation;iterative algorithm
tuning parameters;optimization technique;vision systems;numerical results;energy minimization;image labeling;instances;dimensional space;image labeling;convex optimization problem
object reconstruction;large number of;object reconstruction;reconstruction algorithm
genetic algorithms;stereo vision;shape reconstruction;stereo vision;optimum solution;high accuracy;object reconstruction
markov random field;markov random field model;model generalizes;decision rules;computational framework;optimization technique;test images;training samples;long range
markov random field;estimation problem;image processing;bayesian approach;discrete optimization
transitive closure;maximum weight;tree edit distance;tree edit distance;efficiently computing
sufficient conditions;image registration;sufficient conditions for;motion field;local minimum;multi-scale;motion estimation;optical flow;image registration;optical flow estimation
simulated annealing;image segmentation;sensitivity analysis;monte-carlo;markov random field;image segmentation
operator;process model;markov process;partial differential equations
markov random field;simulated annealing;regularization;stereo matching;energy functional;markov random fields;stereo algorithm;mrf) model
level-set;auxiliary;vector field;parameter free;data set;energy functional;variational formulation
feature space;6, 12;feature space
regularization term;ground truth;taking into account;stochastic model;classification methods;real data;level set;image classification;observation data;supervised classification
graph matching;pattern recognition;graph matching;energy minimization;optimization problem;state-space;graph matching;matching problem
face images;distance measure;training set;natural scenes;optimization framework;classification;basis functions;natural image;basis vectors;image representations;natural images
expected values;line-segment;probability distribution;relational clustering;maximum likelihood;maximum likelihood
feature space;large databases;similarity functions;fast retrieval;shape retrieval;kernel approach;similarity measures
linear transformation;covariance functions;complete set of;recognition accuracy;covariance functions;spatial structure;correlation based
clustering;information extracted from;video sequence;object tracking;classification;image features;learning algorithms;object boundary;shape information;1;object boundaries;competitive learning;learning problems;unified framework;informative features;centroid-based;high dimensional feature spaces
classification;data sequences;hidden markov models;hidden markov model;specific problem;classification performances;model selection;rates;dna sequence
running times;combinatorial optimization;interactive segmentation;energy minimization;maximum flow;energy minimization;10, 15, 12, 2, 4;low-level vision;image restoration;times faster than
bayesian methods;probability density function;heuristic approaches;maximum likelihood;medical data
small scale;template matching;template matching
segmentation methods;estimation problem;moving object;taking into account;moving objects;moving object;maximum likelihood estimation;maximum likelihood;image sequence;segmentation algorithm
protocol called;data mining applications;large amounts of data;data server;distributed data mining;high speed
learning rules;rule set;learning algorithms;aggregate data;rule sets;data sets;distributed data
error rates;wide range;training dataset;classification;data mining techniques
virtual environments;parallel algorithms;data analysis;large-scale;data mining;large collections of;large-scale data mining;data mining tools;data access;data mining projects;data mining;data management
data mining application;data collection;large-scale;entire process;data mining;large-scale data mining;mining algorithms;large datasets;data mining;distributed data mining;distributed data
parallel algorithms;parallel algorithms;network monitoring;large amounts of data;analyzing data;mining associations
text mining;parallel implementation;data points;distributed memory;data set;data sets;algorithm exploits;clustering algorithm;clustering algorithm based on;massive data sets
search space;parallel algorithm;join operations;search techniques;large databases;sequence mining;shared-memory;shared memory;intra-class;frequent sequences;main-memory;inter-class;load balancing
parallel algorithms;partition based;parallel algorithm;candidate itemsets;association rule mining;generalized association rules;association rule mining;large scale;data mining;association rules;frequently occurring;large database;load balancing
search space;large-scale;shared-memory;optimization problem;association rules;association rule;association rules;key features;search algorithm
user interactions;database;sequence mining;database updates;data mining;data mining algorithms;user interaction;mining tasks
efficient parallel;training set;tree based;classification;classification;classification trees;efficient computation;distributed memory;decision tree;data model;query performance;worst case;online analytical processing;coarse-grained;classification algorithms
search results;user profile;web access;text documents;user profiles;access patterns;user profiles;web page
world wide web;data miners;content providers;data mining results;data mining
clustering;clustering;web usage;clustering method;web server;data set;access patterns;web mining;web usage;access patterns;web users
navigation patterns;web site;web site;web usage mining;web page
web usage analysis;visual analysis of;online stores;rates
world wide web;taking into account;web server;existing protocols;web servers;user oriented;web servers;web browser;access patterns;association rules
domain knowledge;navigation patterns;concept hierarchies;web site;electronic commerce;specific features;algorithm called;web logs;wide range;access patterns;pattern discovery;network structures;multidimensional data;data mining algorithms
domain knowledge;discovered patterns;web usage mining;data mining techniques;application domains;frequent itemsets;usage patterns;usage patterns;web data;data mining;web site;data repositories;web data
user navigation;navigation patterns;theoretical foundation;algorithm runs in;data mining;statistical properties;real data;data mining
search terms;web pages;usage information;demographic information;database;conjugate gradient;internet users;high quality;model trained;information retrieval;1;vector space;latent semantic analysis;usage information;internet users;usage data;page content
description language;mining algorithms;access logs;web usage mining;frequent patterns;web graph;data cleaning;xml applications;web usage mining;web logs;web site;web-log;structural information;increasingly complex;markup language;web log
data mining technique;association patterns;data mining techniques;browsing behavior;interesting patterns;stream data;web data;web data;web users
clustering;data structure;web access;web access;data mining algorithms;data mining;data representation;clustering method;server logs;navigational patterns;clustering algorithm;log data;cluster analysis;mining tasks;clustering categorical data
database;web cache;caching scheme;web log mining;web log mining;buffer management;cache performance;operating systems
data collected from;web-based;large number of;web-based
web-sites;specifically designed for;navigation patterns;web usage mining;application requirements;data mining techniques;similarity measure;user behavior;accurate classification;usage data;data collected from;real-life;access patterns;real data;web personalization;user interactions
web pages;web usage mining;concept hierarchies;web sites;case study;interval-based;web usage;association rules;usage data;web usage mining
dimensionality reduction;nearest neighbor;classification accuracy;learning algorithms;implicit assumption;supervised learning algorithms;learning task;dimensionality reduction
data updates;vice versa;heterogeneous data sources;incremental maintenance;view maintenance;view update;incremental maintenance;views defined;data integration;databases;maintenance algorithm;sql extension
fault-tolerant;fault-tolerance;fault-tolerant;database systems;closely related
programming model;databases;database transactions;query processing;databases
approximately optimal;dynamic programming;mobile devices;noise level;mobile devices;time series;user interface;time series;sensor data;mobile phone;dynamic programming
precision-recall;unlabeled data;binary classification problems;classification tasks;large number of;text classification;multiclass problems;text classification;labeled and unlabeled data;labeled and unlabeled data;large number of
clustering;unsupervised clustering;quality measures;objective functions
emerging patterns;fast algorithms;accurate classifiers;emerging patterns;times faster than
multi-level;complex queries;database systems;database;relevant objects;similarity functions;retrieval systems;multi-level;similarity queries;traditional database systems;formal specification;fuzzy logic;information retrieval systems;scoring rules
data cleaning;mining algorithm;valuable information;database
functional dependency;relational database;functional dependency;high efficiency;relational database
base classifiers;active learning;vector machine;sample set;training samples;svm classifier;base classifier;support vector machines;support vectors
measure called;binary attributes;categorical attributes;significant rules;association rules;association rules;objective rule
word recognition;input data;error analysis;automatic speech recognition;automatic speech recognition;linguistic analysis
computational biology;instance;classification task;classification errors;theoretical properties;17, 18;density function;4;natural language processing;density function;estimation errors;margin;classifier;classification algorithms
query response time;data items;storage systems;range queries;simulation results
join algorithm;partition based;database systems;index structure;spatial joins;generic algorithm;spatial join;generic algorithm
indexing schemes;xml documents;complex queries;path expressions;xml data;complex query;tree nodes;xml documents
type information;formal specification
1;15;synthetic data;access methods;database;database applications;databases;data model;indexing method;main idea;sample points;databases;cost function;query processing
basic concepts
world-wide web;data analysis;high level language;image analysis;graph nodes;search services
query optimization;storage space;instance;lower-cost;storage requirements;disk space
object-oriented database;active rules;object-oriented database;data model;object-oriented;data types
clustering;object-oriented;high level;database systems;database size
candidate generation;high level;efficient computation;time series;lower level;interesting patterns;mining algorithm;periodic patterns;periodic patterns;pruning power;higher level
data mining application;directed graph;web sites;graph structure;web sites
relevant features;feature selection method;text categorization;text document;text classification;learning task
data mining;bayesian belief;markov model;belief networks
data mining tool;learning algorithms;classification;dataset characteristics;learning algorithm;training data;case studies;controlled experiments;classification algorithms
clustering;computational complexity;nearest neighbor classification;numeric data;data mining methods;similarity measure;real-world;databases;knowledge discovery;distance-based;large datasets;data mining;similarity measures;distance-based
clustering;clustering;spatial data;17;data structures;data mining applications;spatio-temporal;clustering method;prohibitively expensive;data structures;data sets;dimensional data;linear programming;geographical information;np-hard problem
clustering algorithms;data analysis;similar objects;conceptual clustering;objective functions;clustering process
association rules;database;mining association rules;conventional techniques;association rules
information sources;feature extraction;query capabilities;database;video data;content-based retrieval;dynamic bayesian networks;detection method;knowledge-based;information retrieval;case study;low-level features;content-based video;database management system;learning algorithms;high-level;artificial intelligence;audio-visual
structured databases;xml instances;instances;xml data;data instances;primary key;instance;data sources;view definition;xml data;identification problem;real world;data integration;query processing
clustering;clustering;classification;classification;similarity measure;supervised classification
database;association rule mining;rule mining;data association;heterogeneous databases;association rule;association rules;multimedia database
association rule mining;association rules;large itemsets;large database;database
data structures;time series;time-series;databases;time-series;periodic patterns;data mining problem;search algorithm;long-term;short-term
clustering;clustering algorithms;optimal values;hierarchical data;database;highly skewed;data clustering;data instances;data set;hierarchical clustering algorithm;data sets;optimal parameters;clustering algorithm;clustering algorithm based on;clustering quality
probability estimates;auc;predictive accuracy;classification accuracy;data mining applications;learning algorithms;accurate ranking;classification accuracies;naive bayes;bayesian classifiers;traditional classifiers;probability estimation;error rate;high classification accuracy
automatic speech recognition;text processing
bayesian methods;detection method;synthetic and real images
hidden-layer;numerical experiments;classification;regularizer;global optimality;iterative algorithm;neural network;recognition problem
clustering;total number of;distance matrix;algorithm runs in;cluster model;heterogeneous data;hierarchical clustering;heterogeneous data;data set;space requirement;communication costs;clustering algorithm;cluster models
relational query processing;optimizer;access methods;database;cost estimation;query optimization techniques;cost-based;native xml;query processing;xml database;design decisions
traditional databases;xml document collections;multi-user;database systems;database;xml data;management systems;relational systems;file systems;specifically tailored;transaction management;query processing;native xml;database management system
fast algorithm;database;tree patterns;tree pattern;tree-structured data;tree-structured;equivalent query;algorithm called;pattern matching;tree pattern
clustering;web documents;clustering technique;space overhead;reference-based;xml documents;simple queries;original document;xml document
xpath expression;attribute values;matching algorithm;specific information;publish/subscribe systems;index structure;large-scale;large number of;xml documents;designed to support;wide range;xml document;xml data;margins;avoid redundant;information exchange
data sharing;xml data;adaptive query processing;query results;xml schemas;data providers;integrating data from;data exchange;xml data sources;excellent scalability;general-purpose;query performance;disparate sources;path expressions;data sources;data integration;query processor;data formats;ad hoc;query engine
multi-valued;database systems;object-relational;order-preserving;join processing;object-oriented;query processing;decision support queries;relational databases
multi-level;database systems;simulation model;database;database applications;financial services;buffer management;control policy
systems require;detailed comparison;access method;database;stock market;history data;query performance;incoming data;log-structured;concurrency control;rates;temporal data;numerous applications
clustering;similarity measure;dynamical systems;categorical data;linear dynamical systems;clustering categorical data
video surveillance;real-life applications;knowledge discovery;electronic commerce;multidimensional datasets;outlier detection;distance-based;large datasets;credit card fraud;temporal data;disk-resident;distance-based
data set;association rule mining algorithms;real data sets;association rule mining;memory requirements;ais93b, sa;data matrix;data mining;detecting outliers;association rules;large itemsets;biological data;discovered rules
database applications;database;user transactions;temporal data;databases
clustering;feature space;spatial data;input data;arbitrary shape;very large datasets;large databases;effectively identify;information retrieval;knowledge discovery;wavelet-based;clustering methods;data mining;clustering approach;spatial databases
update propagation;data availability;database applications;query response time;replication;data freshness;databases;update propagation;replicated data
query optimization;evaluation cost;data analysis;hash-based;plans;zdns;storage structures;upper bounds;multidimensional databases;general case;heuristic algorithms;performance guarantees;approximation algorithm;algorithms for computing;multidimensional data;join queries;olap applications
web-based;semantic caching;web queries;semantic caching;keyword-based;query answers;query processing;databases
query optimization;web accessible;training data;optimizer;plans;response times;learning algorithms;collected data;cost model;query feedback;neural network
declarative query language;complex structure;web sites;multiple data sources;visual representation;web site's;data-intensive
multiple databases;linked data;information systems;navigation patterns;structural constraints;data collections;query interfaces;web sites;dynamically generated;huge amounts of;web site;integrating multiple;query capabilities;web usage;databases
optimization techniques;formally defined;relational algebra;transitive closure;optimization method;data complexity;query language;data model;xml trees;mediator systems;relational calculus;semistructured data;pattern matching;query languages;relational data;relational queries
user interaction;constraint networks;multimedia documents
multimedia databases;user interactions;probabilistic model;multimedia presentation;user profiles;interactive visualization;query results
pair-wise;data objects;multimedia applications;search algorithm for;content-based retrieval;indexing method;distance-based;multidimensional space;information loss;distance computation;feature vectors;nearest neighbors;nearest neighbor search
relational database;database queries;query result;context-based;database
gbc;base relations;hash-based;plans;order-preserving;intermediate results;decision support;early stage;query evaluation;sort order;decision support queries;query processing;decision support queries;input data;result tuples
query optimization;multi-dimensional;string matching;selectivity estimation;information sources;constraint-based;multiple attributes;text-based;multiple dimensions;data structure;selectivity estimates;estimation algorithms;queries involving;increasing importance
access pattern;database systems;data structures;cache performance;main-memory;database;analytical model;cost model;database architecture;data access;data structures and algorithms;random-access
online aggregation;large-scale;data distributions;data items;data- intensive;query processing;data streams;operator;long running;data feeds;performance goals
approximate answers;online aggregation;approximate) answers;nested queries;confidence intervals;providing users;execution model;decision-making;aggregate queries;multi-threaded;decision makers;evaluation strategy;aggregate query
high-dimensional;multimedia databases;image features;distance function;access methods;image database;principle component analysis;neural network;hybrid method;efficient indexing;distance-based;similarity measurements;access method;visual perception;feature vectors;visual features
transliteration;constraint networks;database;temporal information;data model;simple temporal;incomplete knowledge;explicitly represented;temporal databases;natural language
database;query specification;query relaxation;content distribution;query reformulation;statistical information;multimedia retrieval;provide feedback;query processing;databases;multimedia database
query optimization;graph model;cost model;execution plans;queries involving;graph-theoretic
high-dimensional;similarity searches;storage space;indexing techniques;complex objects;similarity queries;feature vectors
distributed database systems;detection algorithm;automatically creating;connected component;simulation model
complex queries;database;query language;decision support;design choices;data model;database application;data mining;object oriented
information sharing;database;semantic heterogeneity;meta)data;data model;instances;databases;database integration;local database;data-intensive;semantic heterogeneity
high-speed;network bandwidth;buffer space;large number of;multi-resolution;space requirements;load-balancing;cost-effective;resource allocation
join algorithm;vertically partitioned;join results;join algorithms;main memory;cost model;data skew;algorithms require;selection conditions
structural heterogeneity;database operations;database systems;heterogeneous sources;database;mapping language;integrating data from;heterogeneous data sources;data model;user-defined;object-oriented;update operations;data sources
continuous media;continuous-media;simulation results
query optimization;search space;optimizer;database;search strategy;object-oriented;search strategies;optimization techniques
query optimization;search space;query execution plans;relational queries;nested queries;simple heuristics;optimization strategy;database systems
software architecture;client applications;application programs;database
algorithms for computing;object-oriented databases;mgpf, mgpf, mgpf;database
taking into account;data partitioning;real-life;performance tuning;access patterns;main components;load balancing
database
multi-user;database systems;database;semantic features;query language;knowledge-based;application domains;data model;query-processing;data processing;data management
database technology;negative results;database;positive results;data stored in;answer queries;optimization techniques;standard database
image retrieval;retrieval effectiveness;image database;spatial information;image space;multi-tier;spatial locality;efficient retrieval;multi-attribute
database systems;database;object-relational;data types;emerging applications;relational systems;databases;multimedia content;data type;implementation issues;object-relational database systems;query optimizations
expected number of;data migration;markov-chain;simulation experiments;web-server;markov chain
small sample;classification;database;textual information;context-sensitive;large number of;relational operations;classifier;feature selection;statistical pattern recognition;databases;digital libraries;web document;high speed;topic hierarchies;relational data;text databases
auxiliary;programming languages;counting algorithm;schema information;long-term;object-oriented databases;transaction processing
database;memory capacity;main memory;databases;persistent database;address space
web-based;temporal relations;semantic associations;video data;databases;query language;data model;data model;spatial constraints;video database;content-based video;database management system;video data;model called
optimization criteria;buffer space;bipartite graph
image retrieval;plans;classification;feature vector;storage space;retrieval accuracy;contextual constraints;image content;image databases;relational information;database management system;spatial constraints;classification approach;multimedia database
approximation techniques;high-quality;similarity search;similarity retrieval;real-life;similarity queries;tree index;lower cost
buffer management
high-dimensional;performance guarantees;exact-match;multi-attribute;storage utilization;general-purpose;index size;data types
data set;compression methods;compression rate;database;order-preserving;data sets;general case;dictionary-based;dictionary-based;real-life data sets
database systems;database;commercial applications;lock;relational database systems;ibm db;concurrency control;concurrency control;database workloads
simulation study;parallel database systems;database systems;database administrator;hardware technology;performance tradeoffs;data placement;data placement
design requirements;database;instance;object identity;object-oriented databases;complex applications
information systems;query operations;graph model;object-based;visual interface;data model;heterogeneous databases;databases;query languages;formally defined;main components;user-oriented
distributed query processing;join queries
parameter values;large numbers of;database systems;query optimization;optimize queries;output quality;parametric query optimization;execution plans;randomized algorithms;buffer size
management architecture;transaction management;concurrency control;data stored in;databases;concurrency control
execution environment;object technology;specifically designed for;business process;business applications;business processes;database vendors;object technology;transaction processing;application development;transaction processing
dynamic programming;database technology;database systems;high quality;genetic algorithms;fitness;working principle;cost-effective;optimization techniques
database management systems;log records;object-oriented;data item;multi-granularity;storage manager;concurrency control
database transactions;multi-attribute
join algorithm;cost model;query processing;query optimizers;relational database;ad hoc
database
data residing;initial query;relational structures;database;optimize queries;query plans;object-oriented;query processing;object identity;relational database;database access;relational databases;relational data
specific applications;event-condition-action;special attention;management systems;execution model;large number of;object -oriented;object-oriented;easily extensible;object-oriented databases;active database
document collection;database technology;database;query language;document structure;document type
query execution;space overhead;query capabilities;attribute domains;data structures;indexing scheme
distance measure;high-dimensional feature space;high-dimensional space;database;feature space;efficient storage;feature points;similarity retrieval;feature-based;similarity search;databases;multidimensional space;feature objects;search problems
plans;query optimizer;storage structures;data independence;conventional techniques;object-oriented;logical structure;query processor;relational algebra;database systems
data structures;data type;programming languages;higher level;database
object-oriented databases;schema evolution;database
data objects;databases;database systems;database schema;object-based;database;pose queries;typically rely on;local database;database management systems
graph traversal
active databases;database;dynamically changing;main memory;active databases;database technologies;databases;semantic information;disk-resident;transaction processing
completeness;query-response times;database;caching scheme;performance tradeoffs;central server;dynamic behavior;relational databases;query result
query optimization;optimizer;cost-based optimization;large number of;distributed database systems;cost-based;data type;distributed database;area network
minimal cost;join algorithm;buffer space;join algorithm;main memory;join algorithms;memory size;cpu-based;disk page;buffer size
database applications;databases;transaction models;serializability;database
update semantics;relational algebra;relational model;query language;aggregate functions;relational operations;temporal behavior
database systems;knowledge-base;language called;knowledge-intensive;object-oriented;database management system;high-level;knowledge base;programming language
relational model;relational dbms;access path;access path;search cost
indexing structure;database systems;object-oriented;single class;efficient search;instance;class hierarchy;object-oriented databases;instances
query processing and optimization;relational database management system;query processing and optimization;database applications
query optimization;database technology;knowledge bases;query processing algorithms;object-oriented;data structures and algorithms;cost estimation;representation language;general-purpose;update-intensive;management systems;cost model;highly structured;process control;digital libraries;concurrency control;storage management;control policy;temporal reasoning;knowledge base;knowledge-based systems
storage structures;data items;data set;worst case;access structures;range queries;exact match
lower bound;semantic level;semantic similarity;description logics;database content;domain-specific;database objects;context-based
database applications;spatio-temporal;support queries;real-world entities;databases;temporal data;special properties
fast retrieval;indexing technique;hand-written;efficient retrieval of;performance gain
data structure;metric space;metric spaces;distance function;data structures
world wide web;sharing data;data types;scientific data;data providers;data sources;application domain;data repositories
database;access control;data protection;1;digital libraries;multimedia information;digital library;digital libraries;unlike conventional
digital images;query result;optimizer;database systems;database;medical images;application domains;language called;databases;query processing techniques;query results;optimization techniques;execution plans
data objects;real data sets;spatial indexing;large number of;index structure;similarity searches;cost model;tree nodes;dimensional data;obtain information
spatial data;main memory;spatial joins;cpu cost;spatial constraints;queries involving;disk-resident;spatial databases
recommendation systems;market basket data;search queries;similarity queries;databases;efficient similarity search;market basket
association rule mining;association rules;optimization techniques;computationally tractable;real-life data sets
building blocks for;legacy systems;heterogeneous systems;application development;abstraction levels
focused primarily on;data formats;electronic commerce;data integration;web services;business opportunities;specification language;web data
pattern-based;active rules;active rules;active xml;query language;xml documents;information exchange
query operators;running times;security requirements;optimizer;distributed query processing;large number of;web servers;query processing capabilities;data sources;query processing;data processing;query processor;additional cost
web-based;supply chain;cost-benefit analysis;constraint satisfaction;quantitative analysis;aggregation methods;web servers;human intervention
java-based;xml-enabled;web servers;client applications;workflow management
historical data;navigation patterns;database;data store;fine-grained;web site
query execution;database technology;database management;database;durability;data structures;personal data;atomicity;applications involving
execution plans for;world wide web;business data;relational database systems;aggregate functions;xml documents;xml document;takes place;relational data
heterogeneous information sources;rating.8;complex queries;case study;grade=a;algorithm generates;instance;score;query translation
query optimization;data management problems;database;conjunctive query;data independence;large number of;materialized views;answering queries;database relations;conjunctive views;data integration
high-dimensional;approximate answers;multi-dimensional;complex queries;response times;wavelet decomposition;query processing in;query execution;approximate query processing;aggregate queries;query execution engine;general-purpose;relational tables;decision support systems;data sets;query processing;real-life data sets;cost-effective;query processing algorithms;wavelet-based;data volumes
information content;data analysis tasks;database systems;context-sensitive;expected values;actual values;maximum entropy
erroneous data;database management systems;database;database design;real-world;decision-making;object-)relational;language constructs;current commercial;business rules;databases;standard sql;database systems
query optimization;data management problems;database;query execution plan;answering queries;theoretical results;materialized views;data sources;answering queries;data integration systems;database relations
materialized views;databases;formal framework;view maintenance;data warehouses
graph theory;workflow management systems;real-world;formal specification;workflow management system;transaction management
classification;database;typically performed;schema matching;application domains;schema matching;solution space;data warehousing;query processing;instance-level;constraint-based;data integration
databases;data mining;national security;inference problem;data mining
inference problem;sensitive information;database security;access control;sensitive data;low-bandwidth;data security
efficient representation;distributed computation;cryptographic techniques;privacy preserving;data mining algorithms;privacy-preserving data mining
database privacy;database privacy;personal data
data mining problems;distributed data mining;distributed data;privacy preserving;privacy-preserving;data mining applications;numerous applications;privacy preserving
intrusion detection;audit data;intrusion detection;prototype systems;data mining techniques;data mining;mining patterns
protect privacy;categorical data;personal information;privacy preserving data mining
data mining research;data mining processes;high-level;mining framework;data mining
inductive databases;query optimization;databases;query language;inductive databases;data mining;implementation issues
database;simulated data;data mining;supervised learning algorithms;data mining approach;objective function
information extraction from;knowledge discovery;acm sigkdd international conference on;data mining
rule-based;post-processing;information extraction;specific task;development environment;natural language processing;pattern matching;semantic constraints
naïve bayes classifier;classification;extracted features;data cleaning;feature extraction;machine learning;text classification;pre-processing
frequently occurring;feature selection method;keyword-based;local patterns;classification results;text classification;traditional classifiers
missing information;data sources;data mining problems
target class;high dimensional space;vector machine;training examples;linear classifiers
modeling techniques
automatically extracted from;case study;relational information;text mining techniques;information sources
1;cross-validation;instance;prediction task;induction algorithm
multi-valued;classification;genetic algorithm;fitness function;large number of;tree-based;vector representation;classifier;relational data
workshop report;acm sigkdd;data mining
conference series;data mining and knowledge discovery;multimedia data mining;multimedia data mining;international workshop on;acm sigkdd
workshop report;multi-relational data mining;acm sigkdd international conference on;knowledge discovery;multi-relational data mining;data mining
acm sigkdd international conference on;knowledge discovery;usage patterns;data mining;usage patterns;web mining
fundamental properties;distributed computation;data manipulation;composite services;process models;data management
xml data;information content;database;database design;information-theoretic;information theory;normal forms;databases;complex data;normal form
approximation algorithms;storage devices;data migration;web servers;data items;managing data;worst case;np-hard
data values;databases;information systems;digital data;database systems;database;data records;original data;decision-tree classifier;data mining;large databases;aggregated data;information flow;accurate models;data mining;association rules;preserving privacy;perturbed data;personal information
search space;database;conjunctive views;distributed databases;dynamic-programming algorithm;data warehousing;conjunctive queries;answer queries;upper bound;data integration
schema design;structure information;view updates;database relations;databases
complexity bounds;conjunctive queries;regular path queries;databases;mobile computing;query rewriting;view-based;data management;query containment;query containment;view-based;data integration
data values;selection problem;relational queries;algorithm performs;optimization problem;xml content;xml documents;direct access to;view selection;xml document;approximation algorithm;stream processing;np-hard
query evaluation;database;relational model;general case;semistructured data;special case
computational complexity;graph theory;data exchange problem;data exchange;decision problem;source schema;instance;query processing;source data;conjunctive queries;algorithms for computing;target instances;data exchange;np-hard
operator;query evaluation;databases
query containment;databases;query containment;regular path queries
set cover;result sets;database;xml queries;minimum description length;databases;semistructured data;data organization;multi) query optimization
data mining;hash-based;join operations;join results;databases
tree-edit distance;document streams;vector space;streaming model;xml data;data streams;sketching techniques;upper bound;similarity joins;streaming xml data
data complexity;answer questions;database
xml documents;complexity bounds;xml schemas;querying xml documents;query evaluation;complexity bound;ordered trees;tree automata
query evaluation;query complexity;complexity classes;data complexity;combined complexity;xpath query;query processing;xpath processing
xml documents;learning theory;databases;relational databases;query languages;numerical attributes
n;preserving privacy;reconstruction algorithm;statistical databases;database
individual records;unlike earlier;randomized data;mining association rules;aggregate data;privacy preserving data mining;9;original data;high confidence;data mining models;storage cost;data distribution
lower bounds;data stream;data item;data items
clustering;data points;sliding window;data stream applications;data elements;data stream;answering queries;relative error;approximation algorithm
selection problem;query cost;query workload;query response time;probability distribution;future queries;query distribution;high probability;optimal performance
mobile objects;indexing schemes;indexing method;nearest neighbor;index structure
query answering over;completeness;database schema;database;databases;query answering;multiple information sources;data integration
itemset mining;data mining
frequent itemset;synthetic datasets;frequent patterns;database;frequent itemsets;lower bounds;data mining;frequent item-sets
data structure;database management systems;selectivity estimation;anomaly detection;frequent items;performance guarantees;database;algorithm maintains;data mining;algorithm relies on;synthetic data
pattern recognition;acm sigkdd international conference on;large databases;hardware technology;machine learning;knowledge discovery;relational data;data mining projects;data mining;database technology;kdd 2002 conference
importance sampling;data accesses;monte carlo;data mining techniques;particle filters;massive datasets;data access;dynamic systems;data mining;posterior distribution;computationally feasible;markov chain;web traffic
14;outlier detection;principal components;detect outliers;linear complexity;worst case;data mining
domain knowledge;computational efficiency;regularization;kernel methods;potentially infinite;parameter selection;kernel matrices;kernel matrix;kernel functions;boosting methods;boosting algorithm;column generation;gradient boosting;benchmark datasets;support vector machines
data set;conflicting information;association patterns;association rule mining;application domains;interestingness measure;application domain;feature selection;machine learning and data mining
search space;constraint-based mining;frequent itemsets;pruning algorithm;theoretical analysis;algorithm (called
multiple sets of;efficient query evaluation;query language;rule-based;theoretical foundations;large number of;rule mining;real-world applications;rule mining;data mining task;interesting rules;data mining;ad hoc querying;discovered rules
viral marketing;knowledge-sharing;knowledge-sharing;probabilistic models;computational cost;viral marketing;data mining
mining algorithm;tree mining;mining frequent;mining frequent;web logs;web mining;frequent subtrees;data structure called;ordered trees;semistructured data;pattern matching;usage patterns
data mining tool;highly-accurate;data source;database;massive graphs;data sources;disk resident;exact computation;social networks;data mining;allowing users to;ad hoc;answer questions;mining large graphs;times faster than
document streams;text mining;state transitions;fundamental problem;hierarchical structure;data mining;network traffic
clustering;classification;real world datasets;time series;implementation details;data mining;real world
exploratory analysis;data warehouses;hierarchical structures;database;scientific computing;multidimensional databases;hierarchical structure;user interface;data cubes;databases;interactive visual;structured data
high-dimensional;tree structures;high-dimensional datasets;distance function;graph-based data;unstructured text;multi-dimensional;multi-dimensional;integrating multiple;distance-based;7;dimensional data;interactive visualization;real world;high-level;projection based;movie database
query-log;meta-search engine;retrieval function;low cost;training examples;relevant documents;clickthrough data;vector machine;retrieval functions;relevance judgments;large sets of;information retrieval;search engines;risk minimization;retrieval quality;search engine;theoretical results;data generated from
sparse data;state spaces;web navigation;markov models;web sites;web site
error rate;discovery algorithms;computational biology;dna sequences;real-world;lower bound;pattern discovery;theoretical analysis;pattern discovery;general problem;data mining problem
relevant features;classification;database;multi-resolution;local patterns;wavelet decomposition;data sets;wide range;effective classification;classification task;biological data
bayesian model;regression method;gaussian process;minimization problem;algorithm called;process model;support vector machines
clustering;large numbers of;small groups;large datasets;hierarchical clustering;clustering methods;computational cost;model-based clustering
hierarchical classification;information-theoretic;class models;support vector machines;objective function;information theoretic;clustering;local minimum;clustering techniques;text classification;word clusters;2, 28;high computational cost;hierarchical text classification;jensen-shannon divergence;naive bayes;html documents;distributional clustering;classification accuracy;higher classification accuracy;data set;feature selection
em) algorithm;parallel algorithm;large document collections;learning algorithm;labeled documents;naive bayes classifier;text classification;unlabeled documents;supervised learning algorithms;text classification;em algorithm;classification task;expectation-maximization
pre-defined;classification technique;training data;naïve;naïve bayesian;classification;large text collections;classification accuracy;text documents;document corpora;text categorization;information retrieval;classification performance;text classifier;bayesian classifier;prediction performance;text classifiers;machine learning and data mining;classifier;text categorization
mining algorithms;real datasets;mining association rules;privacy preserving;association rules;discovered rules
frequent item sets;frequent item set;real world datasets;large databases;support threshold;complete set of;tree-based;mining frequent;depth first search;item sets;artificial datasets;databases;breadth first search;highly scalable
algorithm performs;web pages;unlabeled data;web page classification;training examples;classification accuracy;negative examples;instance;algorithm called;web mining;pre-processing;classifier;positive examples
world wide web;extracting information from;web pages;classification;feature space;classification accuracy;web sites;pruning method;web site;web site;web page;html-documents;tree model;classification algorithms
application domain;cost-sensitive learning;reinforcement learning;cost-sensitive;data set;decision rules;data mining;learning framework;decision making
active learning;real-life datasets;integrating data from;instances;fast convergence;high accuracy;multiple sources;classifier;design issues
social security;classification;genetic algorithm;data mining applications;regression models;solution space;data sets;data dissemination;preserving privacy;personal information
multi-class problems;decision trees;unlabeled data;neural networks;classification;hypothesis space;ensemble methods;unlabeled data;cost-sensitive;benchmark datasets;ensemble method;base classifier;semi-supervised;margin;labeled and unlabeled data;learning method;classification algorithm
classification accuracy;learning algorithm;cost-sensitive;high recall;rare class;boosting algorithm;base learners;rare classes;high precision
feature space;large numbers of;training data;feature space;ensemble classifier;accurate classifier;massive datasets;feature selection;classifier ensembles;high-dimensional feature spaces;high dimensionality;classification task;classifier
pattern mining;software systems;web-accessible;case study;discovery algorithm;usage scenarios;user interaction
historical data;data mining;accurate models;customer behavior;limited resources;customer data
real-world scenarios
text mining;web pages;database;multiple target;survey data;customer relationship management;meaningful information;search engine;text mining techniques
data objects;error-prone;domain-independent;application domain;18;object identification;domain-specific;manual construction of;integrating information from;string transformations;application domains;high accuracy;higher accuracy;user input
finding patterns;rule induction methods;text analysis;competitive analysis;text categorization
intrusion detection;intrusion detection systems;clustering technique;real-world;large number of;actionable knowledge
intrusion detection systems;learning algorithm;data set;9;false alarms;network traffic;normal behavior
intrusion detection systems;false positive;intrusion detection system;anomaly-based;data model;training data;user profiles;data mining;data collection;detection rate
large numbers of;discovered association rules;microarray data;data mining techniques;large number of;expression levels;association rules;data mining process;association rule discovery
clustering;bayesian network;bayesian network learning;major source of;text descriptions;text-based;joint distribution;knowledge discovery;learning problems;statistical models;measurement data;building block
expression patterns;microarray experiments;nearest neighbor;vector machine;time series;recurrent neural networks;gene expressions;classification algorithms;recurrent neural network;gene expression data
public domain;world wide web;web pages;linkage based;user-centered;user behavior;resource discovery;topic specific;user experiences;keyword query
benchmark data;search space;frequent itemsets;database;sequential pattern mining;search strategy;sequential patterns;support counting;effective pruning;mining sequential patterns;depth-first search
web documents;cluster based;clustering algorithms;association rule mining;text documents;hierarchical clustering;large sets of;frequent sets;text clustering;clustering methods;databases;high dimensionality;term-based
theoretical framework;classification;related data;learning algorithms;learning theory;manual construction of;machine learning;data sources;information gathered;semantic mappings;mathematical analysis;learning task
nonnegative matrix factorization;topic models;theoretical results;large 0--1 datasets;independent component analysis;real-world data sets;market basket analysis;information retrieval
decision trees;neural networks;pruning technique;high dimensional;extraction methods;computationally expensive;hidden knowledge;real world;decision trees;neural networks
association-rule;require expensive;database;sampling based;sampling-based;large databases;random sample;discovering association rules;sampling technique
clustering;corpus based;retrieval effectiveness;smoothing techniques;smoothing method;data sparsity;information retrieval;strongly correlated;information retrieval systems;statistical significance;retrieve information;clustering process;query expansion;enterprise systems
clustering;high-dimensional data sets;real-world entities;databases;data integration;extraction methods
regression trees;classification problem;classification;linear models;construction algorithm;high quality;regression models;regression problem;regression tree;large datasets;very large datasets;em algorithm;data mining problem;tree construction;artificial data
simulation data;large-scale;ad-hoc;weighted average;modeling technique;data set;ad-hoc queries;data sets;statistical modeling;storage requirements;statistical models;modeling techniques;range queries
base classifiers;increasing demand for;extracted features;image processing;large repositories of;real-life;medical images;mining patterns;quantitative analysis;classifier;meaningful information
instance selection;redundant features;mutual information;classification;instance selection;instance selection;classification methods;naïve bayes;instances;feature selection;text classification;support vector machines;feature selection
data point;real data sets;data set;large data sets;clustering algorithm;automatically determined;clustering approach
multi-class problems;multi-class;classification;support vector machines;generative models;naïve bayes;standard benchmarks;learning problems;support vector machines;classifier;inter-class;times faster than
mining algorithms;case-studies;visualization tools;user-centered;entire process;medical datasets;knowledge discovery;data preprocessing;discovered knowledge;knowledge discovery process
learning bayesian networks;active-learning;database;large databases;web log;predictive performance;databases;arbitrarily large;induction algorithm
algorithm generates;probabilistic model;dynamic nature of;customer behavior
similar objects;similarity measure;efficient computation;application domains;domain-specific;item-sets;graph-theoretic;similarity measures
data set;classification;partial information;similarity measure;stock market;similarity measure;time series;raw data;data collected from;similarity measurements
database;time series;surprising patterns;interesting patterns;massive datasets;search technique
clustering;meaningful clusters;clustering results;distance function;data points;hierarchical clustering;information contained in;clustering methods;clustering method;pair-wise;scale-invariant;chi-square;traditional clustering algorithms;simulated data;special case;rank order;estimation problem;real world data sets
training data;test data;classification;database;association rule mining;missing attribute values;rule set;rule sets;traditional classification;accurate predictions;relational databases;traditional classifiers;class association rule set
decision tree;rules generated;tree structure;classification rules;training samples;confidence level;decision making;classification algorithms
inter-transaction;matching algorithms;index terms;database;distributed data mining;rule mining;knowledge discovery;distributed database;distributed data mining;huge number of
clustering;clustering results;partitional clustering;index terms;data points;similarity measurement;data mining;hierarchical clustering;data clustering;data clustering;data set;data sets;data mining;clustering algorithm;clustering algorithm based on;hierarchical clustering methods
web documents;web pages;answer set;web sites;information retrieval;web site;rates;web page
data mining technique;linear model;information sharing;log linear;united states;risk management;chi-square;million records;data mining;high level;vast amounts of data
higher-precision;theoretical analyses;incrementally maintaining;document classification;database
minimal cost;linear programming;optimization problem;objective function;solution space;class distribution;classifier
clustering;evaluation methodology;feature vector;word senses;domain-specific;clustering algorithm called
clustering;feature space;support vector machines;classification;unlike standard;supervised learning algorithms;training methods;text classification;classification accuracy;unlabelled data
parameter space;world wide web;learning algorithms;generative models;mutually exclusive;multiple categories;binary classifier;classification approach;mixture models;multiple topics
source code;machine learning methods;classification;programming languages;svm) classifiers;vector machine;world wide web;programming language
transaction data;support levels;association rule mining;frequent itemsets;privacy preserving;data mining projects;association rules;vertically partitioned data
classification techniques;classification;data visualization;classification accuracy;dimensionality reduction techniques;dimensional data;low-dimensional
ranking problem;maximum number of;mutual reinforcement;web pages;fundamental problem
large distributed;computing environments;knowledge discovery;grid-based;computing resources;distributed data;mining tasks
information-theoretic;intrinsic dimensionality;real data;probability distributions
em algorithm;em) algorithm;unlabeled data;classification;real data sets;classification accuracy;expectation maximization;unlabeled examples;joint probability distribution;labeled data;data mining;image classification;labeled and unlabeled data;classifier
data analysis;data source;learned model;outlier detection;probabilistic model;stock market;learning algorithm;time series;regression models;change detection;data stream;data mining;detecting outliers
clustering;large volume;real world datasets;categorical data;heuristic method;clustering algorithm;high dimensionality;transactional data
pre-defined;detection problem;performance gains;feature-based;named-entities;broadcast news;novelty detection;supervised learning algorithm
domain knowledge;probability estimates;classification;misclassification costs;vector machine;decision-making;naive bayes;text categorization;class membership;multiclass problems;data mining;classifier
conference series;world wide web;program committee;international conference on
daily activities;document similarity;broadcast news;document frequency;news search;query length;additional information
web information retrieval;semantic level;semantic content;visual cues;page segmentation;document retrieval;multiple topics;page segmentation;segmentation method;pseudo-relevance feedback;expansion terms;vision-based;web page;irrelevant information
search results;probabilistic model;queries submitted to;search engine users;search engines;search engine;query results;web search engines;result pages
model-theoretic;model-theoretic;semantic web;web users
semantic web;semantic web
description logic;vice versa;large-scale;closely related;semantic web;knowledge representation;description logic;description logic
model offers;web access
web applications;web browsers;design choices;web applications;rates;web browsing;web browser;wireless network;increasingly popular;wireless networks;web browsing
sensor devices;service selection;sensor data
web-based;web sites;textual data;similarity metric;web sources;sampling-based;exact answer;information retrieval;incomplete information;query processing;web services;databases;real-life data sets;relational database management system;sql queries;data integration
high frequency;world wide web;document collection;update operations;search engine
spatial indexing;location-based services;query loads;main-memory;spatial data;query performance;moving objects;capacity planning;location-based services;spatial index;indexing methods
diverse set of;general problem;extensive simulations;np-hard
service provider;service providers;short-term
web server;classification;web servers;web server;web site;content delivery;unified framework
world wide web;web applications;black-box;open-source;dynamic analysis;real-world situations;web application;web application
digital information;web-service;confidential information;sensitive information
content extraction;content extraction;xml content
semantic annotation;text analytics;large corpora;semantic tags;large-scale;large scale;semantic web;million web pages
data objects;web databases;web pages;hidden web;database;data extraction;databases;data extraction;label assignment;regular expression;automatically generating;generated dynamically;integrate data from;result pages
pair-wise;information sharing;takes into account;case study;data sources;large amounts of data;databases;semantic interoperability;formal framework;semi-automatic
text summarization;web pages;content extraction;easily extensible;content extraction;web pages;relevant" content;html documents
network bandwidth;document summarization;memory capacity;mobile devices;mobile devices;computing power;information space
web page;mobile devices;web pages
world-wide web;case study;semantic web;business process;web-based
graph model;world wide web;semantic structure
search techniques;emerging topics;web pages;topic-specific;search engine
pagerank algorithm;web pages;link graph;web graph;linear algebra;fast computation;minimal overhead;million nodes
pagerank algorithm;dynamic programming;web graph;search techniques;theoretical results;incremental computation;personalized web search;algorithms for computing
synthetic data;scheduling strategies;page importance;page rank;page importance;web/graph
automatically created;web technologies;browsing behavior;web interface;tightly integrated with
simulation experiments
simulation study;content distribution;existing protocols;requires minimal;current web;web servers;content delivery
desired properties
agent based;electronic commerce;service descriptions;semantic web;semantic web;description logic;web services;ontology based
application domain;rule-based;semantic web services;semantic web;knowledge representation;web services;software agents
world wide web;web sites;web page
clustering;web pages;clustering technique;linear constraints;quadratic programming;data set;clustering techniques
rank aggregation;world wide web;search results;lessons learned
music retrieval;content-based music;computationally intensive;content-based search;multimedia documents
data-driven;document transformation;text-based
video data
context sensitive;context free;operator;finite state
process model;multiple criteria;linear programming;web services composition;business logic;optimization problem;budget constraints;business applications;web services;composite service;global constraints;service selection;composite web services
web pages;semi-automated;web users
semantic annotation;web pages;data description;database;mapping rules;semantic web;web services;databases
problems arise;semantic web;taking into account
geographically distributed;object-based;database;shared information;data replication;web services
dynamic content;content delivery
focused primarily on;simple models;content distribution;accurately predict;selection criteria;server selection;distance metric
web applications;web services
web pages;large-scale
xml documents;originally designed;large-scale
clustering;document cluster;graph partitioning;density based;distinctive features;similarity measure between;clustering algorithm
search results;web-based;classification;feature extraction;machine learning;web searches;information retrieval techniques;opinion mining;classifier
graph structure;low accuracy;social behavior;information retrieval;link-based;statistical analysis;link graph
clustering;schema-based;super-peer;complex query;keyword-based;semantic web;frequency counting;super-peer;transformation rules
nearest;query evaluation;transitive closure;interval based;labeling schemes;bit vector;semantic web;1;6;answer queries;relational dbms
rdf data;world wide web;complex queries;document structure;xml data;data model;semantic web applications;rich semantics;semantic web;relevant data;huge amounts of;small sets of;query answering;data management;data sources
11;directed graph;connected component;community structure
search results;web communities;distributed search
dynamic content;web applications;response times;database replication;web site;data centers;web application;data center;web applications;content delivery;user request;database replicas;network latency
web objects;web cache;web sites;current web;web cache;content providers
data structure;memory-intensive;memory requirements;memory space
web caching;caching techniques
memory footprint;private information;language called;privacy preferences;error prone;privacy preferences;web users

instance selection;collaborative filtering algorithm;nearest neighbor;collaborative filtering;collaborative filtering;similarity measure;information filtering;similarity measure;selection method;instance;nearest neighbors
query results;relevant pages;traffic flow;probabilistic model;performance metric;instance;search queries;web pages;continuous queries;resource allocation;decision making;continuous queries
web pages;feature vector;large-scale;html pages;search engines;data transformations
world wide web;basic algorithm;main memory;distributed architecture;web crawl;log data;web crawling;caching techniques;hit rate
semantic associations;graph model;data model;semantic web;national security;complex relationships;business intelligence;semantic associations
search systems;search results;search terms;application called;semantic search;semantic web;relevant data;web services;distributed sources;semantic search;web searching
multi-agent systems;web services;ad-hoc;service descriptions;domain-specific;semantic web;semantic web services;agent-based;web services;service discovery;distributed systems
multi-modal;web applications;web access;user interfaces;diverse range of;user interface;multiple modalities;web resources;user experiences
web pages;web browser;link structure;content-based retrieval;web pages;web browser;web browser;related documents;web page
world wide web;web pages;web browsers;visualization techniques;user study;readability
semantic content;geographic information;complex relationships;heterogeneous data;web-accessible;knowledge discovery;domain-specific;information systems;rich information;digital library;ade;complex relationships
database;decision rules;record pairs;decision rule;data sources;cost-based;decision model;missing values;record matching
data lineage;data warehouse;data warehouses;data items;data cleansing;data sources;data warehousing;source data;integration process
data interchange
traditional databases;pre-defined;query evaluation;streaming applications;application requirements;data stream management;emerging applications;streaming data;data streams;query languages;personal information;data stream management systems
database;related queries;aggregate queries;user-defined;databases;query languages
xpath processing;processing queries
emerging area;mobile ad-hoc networks;database systems;mobile ad-hoc network
information systems;databases;databases;database
bitmap index;auxiliary;case study;tree) index;tree structure;tree index;microsoft sql server
wide range
web searches;search engines;web search;information retrieval
high-level;web search engines;information retrieval
power law;connected components;web search;web collections;test collection
web search engine;united states;data sets;web searching;search engine users;web searching
end-users;database;information retrieval" workshop;structured data from;data exchange;information retrieval;business applications;search engines;knowledge management;xml search;information exchange
sigir workshop on;years ago;information retrieval;wide range;information retrieval;information retrieval;image processing;multiple image;small size
sigir workshop on;theoretical framework;fuzzy logic;world wide web;formal methods;similarity functions;information retrieval;linear space;semantic web;graph theory;rough sets;information filtering;digital libraries;user modeling;information retrieval;metric spaces;fuzzy sets;retrieval applications
camera parameters
image segmentation;long distance;image noise;image data;produce accurate;diverse range of;synthetic and real images
object boundaries;image processing
visual field;image-based;linear combination;autonomous navigation;optical flow;feature tracking
range images;range images
contour based;contour based
scale-space;low-level
shape recovery
face images;handwritten digits;object classes;linear models
estimation technique;active vision;moving objects;high resolution;optical flow;moving targets;segmentation algorithm
point correspondences
singular value decomposition;object recognition;multi-dimensional;image patches;frequency domain
human body;learning procedure;genetic algorithm;lighting conditions;human body;images acquired
image points;selection methods;model selection
total number of;input data;image processing;instance;binary decision diagrams;image processing
line segments;matching algorithm;closed-form solution;distance measure
temporal information;estimation techniques
image sequences;line correspondences
color information;tracking objects;mobile objects;rates;video frame;object tracking;tree model;video frame
scene reconstruction;scene reconstruction;input images
image information;reconstruction algorithms
shape representation;light source
visual tracking;image sequence
hand-held;video sequence;visual field;imaging conditions;single image;video sequences
image features;error model
image regions
edge image;facial expression recognition;facial expression recognition;facial expressions;low dimensional;input images;facial image
feature space;object parts;robust tracking;moving objects;clustering algorithm;vision-based
multiple views;scene structure;minimum number of
transformation parameters;basis functions;computational approach;minimum number of
structured light;transformation matrix;calibration method;structured light
tracking algorithms;tracking algorithm;statistical models
tracking algorithms;feature points;tracking results;highly accurate;perspective camera;image sequence;single image;image sequence;image points;feature tracking
long-range;real images;contour extraction
variational approach;synthetic and real images;image restoration
real images;singular value decomposition;point features;stereo matching;stereo correspondence
pattern recognition;large number of;small sample sizes;minimum description length;code-length;sample sizes
image-based;image-based;image-based
temporal data;real data
partial order;logic-based;partially ordered
human perception;feature set;database;increasing number of;image content;shape features;salient features;visually similar
image retrieval;image retrieval;closely related;texture segmentation;multi-scale;statistical tests;similarity measures;texture segmentation;similarity measures
aerial images;multiple images
appearance-based;object recognition;appearance-based;large numbers of;appearance-based
operator;stereo vision;computational resources;matching methods;stereo matching
sensitivity analysis;object recognition;spatial information;color images;partial occlusion
structural information;information content;dimensional space;worst case;sufficient conditions for
search space;recognition task;object representation;articulated objects
real images
stereo algorithm;closely related
illumination conditions
real images;image understanding;information source
object recognition;classification;classification;similarity measure;multi-scale;multi-scale
recognition accuracy;object recognition;constraint based
data set;structure information;sampled data
learning process;training set;probability models;markov process;false-alarm;learning procedure;face detection;correct-answer;negative examples;neural network;detection process;face detection;computationally expensive;motion information;probability model;visual learning;likelihood ratio
tracking results;multi-body
window based;user interface;hand tracking
information sources;smoothing methods;range images;reconstruction algorithms;reconstruction algorithm;surface reconstruction;image data
object recognition;surface shape;object models;image based
perspective projection
control points;minimum description length;model parameters;adaptive approach;optimal number of;medical) images

optical flow;image sequence;local minimum;block-based;motion tracking
texture analysis;classification;classification accuracy;great potential;spatial distributions;image domain;classification;feature set
maximum likelihood;matching algorithm;image noise;data-base
sensor data;linear features;high-resolution;multiple images;information sources
vision-based;vision problems
real-time tracking;contextual information;closed-world;image features
tracking algorithm;object tracking;image sequence;kalman filter;point correspondences
prediction intervals;selection criteria;segmentation techniques
image sequences;physical systems;information derived from
corner detection;local minima
low accuracy;real world;vision algorithms;image sequences;view-based
initial conditions;process model;hidden markov models;action recognition;training algorithm
deformable model;deformable models;deformable models;object reconstruction
image sequences;objective function;multi-objective
operator;tensor space;view synthesis;virtual camera
camera motion;feature points;image noise;perspective projection
segmentation results;natural scenes;segmentation method;edge detection;image data
motion estimation;image region;magnetic resonance images;intensity values;image sequence;motion estimation
clustering;feature space;nearest neighbor;markov process;gaussian distributions;em) algorithm;texture features;local shape;prior distribution;map) estimation;bayesian formulation;object segmentation;expectation-maximization
low bandwidth;video coding;classification;video stream;static images;classification
view synthesis;image based;visual representations;user interface;prior models;image sequences;information needed;view synthesis;model estimation
partition function;energy function;data representation;shape models;deformable models;bayesian framework
high-dimensional;feature space;approximate algorithm;higher-dimensional;low-dimensional;nearest-neighbour;high-dimensional spaces;object models;complex objects;databases;nearest neighbour;search algorithm;query point;indexing methods
low-resolution;natural scenes;qualitative spatial;machine vision;image indexing;low resolution;scene classification;scene classification;large database
competing methods;key features;vision problems;wide range;computational cost
expectation-maximization;observed data;data analysis;motion segmentation
auxiliary;stereo correspondence
real-world;clustering techniques;scene analysis;image sequences;labeling process;contextual information;estimation techniques
algorithm's performance;low dimensional;flow fields;spatiotemporal data;prior distribution;parametric model;image sequences;flow field;em algorithm;computationally feasible;motion segmentation;model estimation;motion segmentation
lower-level;kalman filter;appearance-based;em) algorithm;state transition;lower level;prediction errors;minimum description length;learning scheme;appearance-based;special case;expectation-maximization
global illumination;correlation-based;large database;similarity measure
object recognition;approximate algorithm;region-based;computationally hard
background clutter;sample data;model construction;highly correlated;uniformly distributed;multiple images
data structure;face recognition;matching algorithm;frequency-domain;frequency domain;complex objects;pose estimation;spatial structure
data-point;real-world;control points;data-sets;maximum-likelihood;data-points
optimization technique;quadratic programming;intrinsic parameters;scene structure;vision problems;euclidean structure
genetic algorithms;data acquisition
multiple views;real images;feature detection;point correspondences
multi-modal;cross correlation;face detection;covariance matrix;robust tracking;color histogram;face tracking;kalman filter
statistical learning;excellent performance;plans;test sets;image data
curve evolution;computational savings;medical images;energy minimization;term based
matching score;line segment;multiple view;cross-correlation;image pairs;partial occlusion;long range;line segments;line correspondences;correlation based
feature extraction;object representation;point correspondences;complex objects
video sequences;video sequence;estimation technique;video indexing;web page
feature points;image sequences;extended kalman filter
range images;efficient approximation;range images;optimization techniques
increasing complexity;level features;intensity images;feature-based;line segments;aerial images;confidence measure;search space
probability distribution;kalman filter
image sequences;hidden markov models;scale-space;feature vector
independent motion;linear model;parameter estimation;flow fields;flow fields;specific problem;optical flow;visual perception;robust estimation
operator;markov random field;objective function;genetic algorithm
perspective images;spatial resolution;vision applications
partial occlusion;feature-based;machine vision
gradient-based;document processing;learning algorithms;learning paradigm;neural network;objective function
classification;classification accuracies;facial expressions;hidden markov model;human face;low-level
closed loop;image patch;extended kalman filter;classification;feature trajectories;search space;correlation-based;video sequence;feature tracking
evaluation methodology;face recognition;facial images;database;large databases;face-recognition systems;face-recognition;face-recognition algorithms;large database
detection problem;data points;face detection;global optimality;basis functions;memory requirements;quadratic programming problem;optimization problem;data set;large data sets;main idea;support vector machines;neural network
point correspondences
face image;image understanding;content information;natural language processing;information extraction from;automatically extracts
multiple images;image alignment;instance;linear transformations;multiple images;reference image;image alignment
image sequence;vision applications
camera motions;stereo matching;stereo images
medical imaging;instances;shape recovery;deformable model;medical images
euclidean reconstruction;image sequences;image sequences;simulated data;special case;aspect ratio
ground truth;template matching;feature extraction;image sequences;quality control;image matching;matching method;high precision
multiple views;spatio-temporal;depth map;point correspondences;optical flow;real images
shape recovery
motion estimation;multiple cameras;flow fields;flow fields;moving object;multi-camera;motion parameters;motion estimation
large amounts of;range images;model selection;range data
ground truth;template matching;feature extraction;image sequences;quality control;image matching;matching method;high precision
general case;human interaction
specular reflections;principal components analysis;linear models;basis vectors
illumination conditions;direct computation;surface normals
image sequences;simulated data;image sequences;shape analysis
scene geometry;contextual constraints;originally designed;visual correspondence;parameter settings;vision problems;connected components;visual correspondence;image restoration
human movement;instances;image sequence;spatial location;view-based
image sequences;human actions;wide range;multi-scale;spatio-temporal
synthetic and real images;flow field;global analysis;expectation-maximization algorithm;expectation-maximization
camera parameters;model fitting
correlation-based;stereo algorithm;error analysis
classification;euclidean reconstruction;euclidean reconstruction;camera motions;motion sequences;motion sequences
constraints imposed by;geometrical constraints
clustering;gait recognition;high-level;dynamical systems;training data;temporal context;hidden markov models;learned model;low level;video sequences;abstraction levels;statistical models;low-level;higher level
training set;linear combination;flow fields;multi-resolution;human motion;image sequences;image motion;principal component analysis;optical flow;optical flow estimation;motion models
face recognition;pattern analysis;vision problems;pose estimation;surface shape;character recognition
singular value decomposition;image sequences;motion parameters;fundamental problem
image database;illumination conditions;color images;classification;semantically meaningful
moving objects;confidence measure;image motion;image sequence;image sequences
surface normals;intensity images;low rank;point features;intensity images;data matrix;scene points;vision problems;missing data;light source
detection algorithm;corner detection;test statistic;corner detection;covariance matrix
intensity images;dimensional space;scale space;medical images;image enhancement;image processing
graph partitioning;local features;image segmentation;static images;generalized eigenvalue problem;image data;image segmentation
clustering;feature space;image segmentation;image features;high quality;color images;edge image;feature spaces;color image
boundary detection;boundary detection;user defined;image segmentation
dimensional space;auxiliary;scale-space;feature analysis;scale-space;vector-field
background clutter;false alarm;real-valued;object detection;hash table;operating parameters;theoretical analysis;binary features;binary features
static images;object detection;high degree of
image based;feature detection;template matching;matching methods;feature-based;detection algorithm;object detection;map estimation;object detection;map estimation
theoretical framework;light sources;free space;light source
multiple images;iterative algorithm;light sources
visual appearance;database;real-world;databases;image analysis;imaging conditions;real-world;distribution function;vision algorithms
illumination conditions;surface normal;condensed representation;object recognition;scale invariant;intensity values;covariance matrix
image retrieval;spatial proximity;storage overhead;database;image database;color features;efficient indexing;user interface;color histogram;large variations;test results;spatial relationships
image feature;image indexing;image indexing;color histogram;spatial correlation;content-based image retrieval
object-based;object-based;video object;shape information;direct access to;object tracking;video indexing
image understanding;multimedia applications
maximum likelihood;real images;camera parameters;camera parameters
image features;prior models;scale invariant;learned model;prior models;natural images;image restoration
markov random field;optimization method;estimation problem;optimization problem;computational costs;constrained optimization;image restoration
sparse representations;compact representation;function approximation
analysis reveals
image segmentation;scale space;segmentation results;fourier domain;mr images;image segmentation
algorithm produces;edge detection;contour-based;adaptive algorithm
image sequence;light source
image segmentation;processing algorithms;vector-valued;partial-differential-equations;object segmentation;vector-valued;level-sets
regularization;energy function
random fields;markov random field;multi-channel;texture analysis
minimal cost;regularization term;local minimum;detection approach;shortest path;medical images
instance;edge detection
image data
run-length
similar behavior;autonomous navigation;optical flow;real images;vision-based;pre-processing;feature tracking
pattern classification;input data;classification;feature extraction;large number of;bayesian framework;character recognition;pattern classification;optimization process;computationally expensive;matching process;deformable model;deformable models;deformable models;recognition accuracy
data set;motion analysis;point correspondences;point correspondences;instance;data sets;motion-analysis
statistical method;low-resolution;statistical analysis
optical character recognition
face recognition;training data;recognition rate;database;probability distributions;facial image;image matching;bayesian framework
facial features;facial image
similarity assessment;traditional databases;similarity measure;image database;image space;databases;complex query;image databases;similarity queries;similarity-based
mr images;robust estimation;left ventricle;motion models;motion model
operator;depth estimation;depth maps;produce accurate;fundamental problem
clustering;feature space;similarity learning;efficient search;image database;texture features;feature extraction;compare favorably with;image data;database;similarity search;feature representation;neural network
illumination conditions;linear model;color image;linear transformation;functions defined;classification results;color images;spatial correlation
lighting conditions;local constraints;synthetic and real images;data structure
image points;feature points;point set
parameter values;object model;estimation method;data fusion;pose estimation;matching techniques;edge information;pose estimation;edge image;human beings
human faces;image patches;large variations;combined method
local search;synthetic data;matching methods;real data;gaussian noise;statistical method
features extracted from;partial occlusion;hash table;candidate models
image database;automatically detected;object recognition;local constraints;image databases
medical diagnosis;real data;applications including
data structure;object recognition;search algorithms;nearest neighbor;high dimensional;machine vision;high-dimensional spaces;search technique;search problems
face images;feature points;feature detection;image representation;feature-based;reference image
parallel structure;object recognition;voting scheme;indexing techniques;image features
hybrid model;regularization;edge detection;scale space;image data;specific task;goal-directed;continuous functions;stereo images
physics-based;image regions;physics-based;segmentation algorithms

motion estimation;point correspondences;linear subspace;linear subspace
physical characteristics;physics-based;search strategy;united states;geometric structure;content- based retrieval;spatial structure;satellite images
surface normals;flow fields;geometric information;geometric structure;flow field;high-level;low-level
linear model;color image;image region;linear transformations;spatial structure;local image;reflectance model
light sources;structure information;light source
real world applications;training images;object recognition;background clutter
verification problem;false alarm;camera parameters;error model;perspective projection;rates;statistical test
estimation technique;theoretical results;pose estimation;pose estimation;real images;pose parameters
distance measures;bayesian framework;matching process
state transitions;formal model
scale-space;object detection;image denoising;edge detection;level-sets;euclidean space
shape similarity;shape similarity;uniform distribution;similarity metric;similarity measurements
human subjects;multiple images;image understanding;semi-automated;image understanding;human operators;human intervention;semi-automated;geometric constraints
regularization;geometric constraints;global consistency
data set;distance measures;linear programming;geometric constraints;range data
gradient-based;feature extraction;context-based
feature space;accurate classifier;algorithm performs;classification accuracy;test sample;image analysis;multiple classifiers;individual classifiers
image sequences;flow field
scene points;video camera;image acquisition;image acquisition
tracking algorithms;correlation-based;vector-space;image regions;real-time tracking;tracking problem;video sequences;vector space
finite difference;stochastic processes;random walk;parallel computation;monte carlo simulation;probability distribution;local image
ground truth;vision algorithms;output quality;optimal parameters;detection results;low level;operator;real images;low-level
large number of;optimization criteria;entropy measure
object boundary;image features;shape recovery;object boundaries;image domain;curve evolution;curve evolution;shape analysis
hierarchical approach;detection algorithms;edge detection;high resolution
parameter space;high dimensional;human movement;multi-view;similarity measure between;search problem;multiple views;image sequences;database;decomposition approach;tracking results;pose parameters
closed loop;face tracking;pose estimation;pose estimation;spatial location;face tracking

deformable objects;intensity images;large number of
body parts;physics-based;tracking results;sensory data;human motion
context information;operator;domain constraints;sensitivity analysis;decision theoretic
object recognition;object recognition;classification;classification
target recognition;matching techniques;template matching;synthetic-aperture;multi-step
background clutter;sensory data;high complexity;target detection;detection performance;contextual information;false alarms;target detection
convergence properties;algorithms for computing;simulation results
camera motion;feature points;dense set of
video sequence;segmentation problem;mixture model;algorithm's performance;image sequences;spatial constraints;em algorithm;motion segmentation
point features;motion sequences
regularization;multi-layer;regularization;optical flow;image motion;optical flow;image patches
depth map;video-rate;video-rate;video rate;virtual worlds;high precision
feature-tracking;active vision;robot vision;active vision
motion estimation;visual cues;human face;deformable model;image sequences;optical flow;face model;facial expressions;motion estimation;optical flow;deformable models;human face;deformable models
control mechanisms;operating conditions;provide answers;control strategy;active vision
completeness;camera motion;active vision;camera motions;scene reconstruction;spatial structure;robust estimation;visual features
statistical properties;local features;image features;human activity;change detection;imaging conditions;individual features;aerial images;visual features
edge detection;local maximum;automatic selection;feature detectors;image data;addressing this problem;scale-space;extracting features from
high level;parameter estimation;feature detection;dimension reduction;feature detectors;low dimensional;heuristic search
instance
visual field;vision applications;template matching;visual representations;uniform sampling;special case;image processing
objects moving;image patch;image motion;ground truth
optical flow;matching algorithms;image processing;matching method
distance measure;computational complexity;energy function;matching algorithm;constraint satisfaction;graph matching;subgraph isomorphism;randomly generated;graph matching;0(lm, where l and m are the number of links in the two graphs;compares favorably with;neural network
euclidean reconstruction;euclidean space
matching algorithm;point sets;motion parameters;closed-form solution;point correspondences;real images;missing data;simulation results
motion estimation;visual cues;human face;deformable model;image sequences;optical flow;face model;facial expressions;motion estimation;optical flow;deformable models;human face;deformable models
order statistics;face recognition;training data;pattern recognition;recognition rate;feature-based;database;training examples;selection problem;face recognition;feature-based;distance function;statistical model;nearest neighbor search
object classes;success rate;probabilistic model;feature detectors;instances;object classes;partial occlusion;false alarms
regularization;probability density function;pattern recognition;neural networks;kullback-leibler;lower bounds;radial basis function
distance metric;distance metrics
object recognition;recognition algorithm;vision systems;image segmentation;object recognition;reinforcement learning;reinforcement learning;color images;input images;automatic generation of;closed-loop;confidence level;real-world applications;evaluation function
clustering algorithm;clustering algorithm based on;partitional clustering;objective function;distance measures
feature detection
sparse data;range images;fast algorithm;connected components;range data;connected components
image registration;correlation method
discriminant analysis;recognition algorithm;pattern recognition;feature detection;large number of;object recognition;simple algorithm;classifier
document images;main idea
scene structure;image sequences

image sequences;estimation techniques;large number of;dynamic model;perspective projection
image points;similar results;svd-based;image data;perspective images;factorization method
parameter space;feature point;perspective images
explicitly model;view synthesis;scene geometry;stereo vision;stereo vision;real images;view synthesis
simulated data;motion tracking
image points
line correspondences;multi-step;point features;factorization method;line correspondences;factorization method
image points;multiple classes;appearance-based;matching methods;minimum description length;robust estimation
search space;negative examples;relevant features;interactive learning;multiple features;selection process;learning algorithm;large number of;context-dependent;digital library;semi-automated;similarity measures
recognition accuracy;computational complexity;conventional methods;segmentation results;test set;neural network;word segmentation
template matching;data acquisition;highly optimized;target detection;context- sensitive;high resolution;target recognition;target detection
data collection;shape models;object models;simulation results;classification
speech recognition;explicitly represented;human-machine;higher level
robot vision;robot vision;low level;moving object;filtering technique
intrinsic parameters
parallel algorithm;machine vision;resource allocation;machine vision;computing platform;development environment;image processing;computing platform
surface normals;question arises;light sources;object recognition;illumination conditions;lighting conditions
real images;intensity images;image based;complex objects;inference rules
synthetic data;image region;small scale;vision problems;range data;operator;objective function
global model;left ventricle;shape model;instance;global models;mr images
geometric constraints;skewed
image registration;estimation technique;stereo matching;image pairs;image regions;window sizes;stereo matching
depth maps
image registration;correlation coefficients;visual correspondence;vision problems;motion estimation;intensity values;image matching;distance metric;simple algorithm
geometric information;depth map;feature tracking;depth maps;computationally intensive
feature points;multiple images;multiple views;image matching;image domain;image matching
model building
multi-modal;pattern detection;classification;face detection;intermediate results;body parts;face tracking;pattern matching
order statistics;vision systems;analysis reveals;natural image;desired level of;vision systems;natural images
pose estimation;pose estimation
approximation techniques
13;statistical analysis;scene geometry;linear models

global optimal solution;threshold values;minimum set of;maximum-flow;connected component;energy function
theoretical analysis;decision rule;shape representation;distance transform
probabilistic model;maximum likelihood estimation;image matching;vision applications;prior knowledge
image registration;error bounds
3;5;4, 3
linear combination;large number of;gradient descent;obtained by applying;energy functional;likelihood function
markov random field;image segmentation;18;prior knowledge;markov random fields;7;diffusion process;inference problem
face recognition;matching systems;linear discriminant analysis;real data;principal components;simulated data;principal component analysis;classification task;feature based
image segmentation;image segmentation;graph-theoretic
edge detection;recognition problem;edge detection;scene points
object recognition;similarity measure;large-scale;object recognition;gaussian kernel;recognition accuracy;nearest neighbour
image segmentation;numerous applications;vector field;local maximum;image gradient;image data;image denoising;image processing;synthetic and real images
synthetic data;30;partial differential equations;deformable model;deformable models;deformable models
sparse data;image segmentation;image features;general-purpose;uniformly distributed;deformable models;deformable models
training set;edge information;input image;prior information;object boundary;prior knowledge;statistical information;bayesian formulation;shape models;medical images;boundary points;image quality
stereo correspondence;neighborhood information;line correspondences;surface reconstruction;stereo images
nearest;face recognition;nearest neighbor;classification;nearest;classification methods;linear combination;face recognition;classification approach
extended kalman filter;tracking framework;image features;estimation process;image sequences;computational requirements
maximum likelihood;image registration;images acquired;map estimation
face images;face recognition;object classes;applications including;object class;image analysis;12;partial occlusion;6), [5
constraint network;human action;temporal constraints;human actions;constraints imposed by;temporal structure
face recognition;facial images;image sets;recognition performance;face space;statistical information;conventional methods
vision systems;image segmentation;image-based;target object;moving object;human motion;image sequence
face images;classification;feature extraction;hidden markov models;facial expression recognition;facial expressions;principal component analysis;weight vector;feature point;motion information;low-dimensional
classification;color information;learning algorithm;motion patterns;image sequence;graph matching;motion patterns;neural network
face recognition algorithms;face recognition;database size;real world;face recognition algorithms
handle arbitrary;expectation-maximization;hidden markov model;14;recognition process
face images;recognition rate;facial expressions;hidden markov models;facial expressions;motion information
digital images;data objects;generated automatically;camera parameters;point correspondences;operator
error rate;database;graph matching
classification rule;face recognition;facial images;database;principal component analysis;face recognition;probabilistic reasoning;probabilistic reasoning;bayes classifier;density function;conditional probability;recognition problem
virtual camera;image sequence;camera parameters;image sequence
tracking algorithm;human interaction
hierarchical organization;object recognition;appearance-based;background clutter;large databases;2;object representation;appearance-based;appearance-based
hierarchical approach;4, 5;gaussian noise;theoretical results
probability distributions;object model;object classes;perspective camera
multiple objects;object recognition;shape descriptor;image representation;compression scheme
ground truth;real data;visual correspondence;vision problems;markov random fields;algorithms for computing
local image;similarity functions;local similarity
dynamic programming;mutual information;face detection;search strategy;search process;object detection
local features;synthetic aperture;statistical model;multi-scale;databases;multi-scale;discrimination power;feature vectors;target detection;generative model
simulated annealing;random fields;region-based;data quality;probability distributions;energy functional;markov random fields
scene structure;convergence properties;reconstruction algorithm;surface reconstruction;light source
multiple target;data sources
perspective images;camera parameters;maximum likelihood
image data
real images;13;large number of;classification;computational cost
spatially varying;illumination conditions;noise levels;dense set of;reliable detection
range data;range images;classification;classification;range data
discrimination power;0,1;database;generally applicable;uniformly distributed;load balancing
euclidean reconstruction;euclidean reconstruction;unlike standard;scaling factors;multiple images
long-range;image motion;real images;bayesian framework;feature matching;statistical framework;similarity measures
clustering;clustering;similarity measure;unsupervised clustering;euclidean space;large database
parameter space;local features;global information;complex objects;global models;specific problem;clustering analysis
input image;linear constraints;geometrical constraints;transformation matrix
pose estimation;estimation method
takes into account;high accuracy;depth map;color information;motion estimation
shape information;surface shape;perspective projection;density estimation;filtering technique
clustering;real-time tracking;real time tracking;spatio-temporal;real time tracking;color features;instance;local features
single image;motion analysis;image gradient;single image;flow fields;motion model;adaptive approach;motion models
video-rate;rates;background modeling;background modeling
speech recognition;inference problem;context-free;lower level;hidden markov models;temporal constraints;low level;temporal features;action recognition
local minima;multiple frames;multiple frames
local search;color histogram
image feature;estimation method;spatio-temporal;depth map;motion model;scene structure;instances;image sequences;motion estimation;reference image
vast number of;image sequences;moving objects;em algorithm;optical flow;image sequence;motion segmentation
18;image sequences;multiple-view;7;unified framework
singular value decomposition;surface normal;point correspondences
feature tracking
detection algorithms;ground truth;evaluation methodology;data sets;image sequences;image sequence
low-resolution;mental models;physics-based;autonomous agents;eye movements;virtual worlds;high-resolution
search space;special case;stereo algorithm;high accuracy
object reconstruction
object tracking;object tracking
provably optimal;cost-function;optimization problem
regression method;feature detection;flow fields;flow fields;image motion;image sequence;optical flow;motion models
motivating application;success rate
computational efficiency;segmentation problem;image gradient;takes into account;image analysis;brain images;level set;magnetic resonance;automatic segmentation;image information;left ventricle
object localization;classifier;multiple classifiers;higher order
database
dynamic programming algorithm;dynamic programming;pruning techniques;background clutter;efficient optimization
real images;vector space;hierarchical classification;hierarchical classification;database;image databases;hierarchical structure;clustering algorithm;image similarity;image classification
geometric constraints;connected components
modal analysis;matching algorithm;motion tracking;modal analysis;additional constraints;topological structure;image sequences;motion sequences;filtering technique;low-frequency
decision tree classifier;object recognition;brute-force;template matching;range images;internal node;object recognition;nodes represent;hypothesis testing;spatial location
image retrieval;image retrieval;discriminative features;classification;similarity metric;content based image retrieval;semantic categories;ct images;feature selection problem;similarity metric;decision theory

finite element;motion analysis;motion tracking;motion analysis;finite element;missing information
graph partitioning;learning process;training images;image features;real images
hierarchical approach;optimization method;high dimensionality;image registration
image sequence;6
stereo vision;breast cancer
human behavior;feature extraction;single-object;image features;selective attention;selective attention;image sequences;appearance based;event driven;moving objects;real world
target recognition;image understanding;years ago;image understanding;limited number of
lighting conditions;illumination conditions;reflectance model;database
real-world scenes;analytical model;database
long term;pattern detection;classification;face detection;unknown environments;pattern detection;body parts;person tracking;long-term;person tracking;short-term;short-term
low-cost;video camera;image features;field programmable gate arrays;feature detection
image alignment;control strategies;image alignment
real world scenes;depth map;image sensor;3;optimization process;pattern matching

false alarm;texture features;detection rate;input images;segmentation algorithm
domain knowledge;image registration;registration algorithm;anomaly detection
connected components;local regions
extraction process;image sequences;multi-resolution;image sequences;single image;optical flow
high accuracy;estimation technique;high degree-of;image sequences;vision based;motion estimation;video sequences;human body;linear systems
tracking algorithms;sharing information;visual cues;image-based;data association;state estimation
body parts;multiple views;multiple views;image sequences;human actions;human body
object classes;moving objects;motion tracking;common patterns
object recognition;training images;appearance-based;light sources;complete set of;database;3
test set;face detection;sensitivity analysis;neural network;face detection;training methods;neural network
linear subspaces;training set;training data;mixture model;linear subspace;face recognition;nearest-neighbor classifier;face recognition;recognition performance;expectation maximization algorithm;principal component;linear subspaces;gaussian distribution;noise model
probabilistic modeling;local appearance;object recognition;training images;test set;local appearance;spatial relationships;object recognition;explicitly models;false alarms;human faces;detection rate;test sets;posterior probability
face recognition;facial expressions;image registration;low quality
classification;detection task
video retrieval;low level features;high level;video based;database
multiple views;computational framework;regularization;shape reconstruction
matching scheme;object recognition;range images;data set;target recognition;shape matching;physics based;target recognition;projection based
semantic content;image descriptors;content filtering;bayesian inference;semantic content;bayesian networks;databases;bayesian framework;user interaction;low-level
video databases;video indexing;segmentation algorithms
indexing schemes;relevance feedback;human behavior;database;probabilistic model;relevance judgments;relevance feedback;algorithm takes;2, 1
image sequence;motion models;real data
instances;probability density;shape models;false positives;level features
image retrieval;image feature;image features;negative examples;large databases;retrieval systems;relevance feedback;query refinement;image representations;content-based image retrieval
image information;body parts;human body;structured light;visual cues
clustering;clustering;image retrieval;large scale;indexing scheme;user requirements;image indexing;color images;image databases;retrieval method
user friendly;classification;database;databases;range images;retrieval algorithms;databases;content-based search
moving objects;moving objects
motion model;video sequence;single image;motion model;video summarization
1;user interface;2;selection method;vision algorithms
segmentation results;hierarchical structure;short term;video sequences;clustering algorithm;high-level;local minima
theoretical framework;fusion strategies
distance measure;feature vector;traditional classification;identification problem;feature vectors;likelihood ratio
training data;test data;training data is;language models;sufficiently large;probability values;recognition rates;word recognition
web documents;graph model;semi-structured;data model;phrase-based;data set;clustering techniques;document clustering;similarity measures;similarity based;phrase-based;clustering quality
pruning strategy;association mining;association patterns;data mining;constraint-based;association rules
brute-force search;instance;tree based;chemical compounds;computationally expensive
large scale;text corpora;diverse set of;text corpora;large-scale
clustering results;data clustering;data mining
frequent patterns;frequent pattern;databases;frequent pattern mining;frequent pattern;frequent pattern mining
clustering;data mining tasks;frequently occurring;classification;database;databases;time series;time series;discovery algorithm;databases;association rules
classification problem;neural network;classification accuracy;large database;classifier;nearest neighbor classifier;feature selection;error rate;error rate;selection algorithm
competing methods;controlled experiments;reinforcement learning;learning strategies
human efforts;web pages;training examples;web page classification;classification accuracy;learning framework;negative examples;training phase;training data;high accuracy;classification;machine learning algorithms;web page classification
frequent sets;specific features;main memory;large databases;transactional databases;multi-threaded;frequent sets;long patterns;computing platform
data structure;mining frequent patterns;frequent patterns;transactional databases;fp-growth;discovery algorithm;path tree;disk-based;path tree
attribute values;execution times;association rule mining;relational tables;management systems;structural information;mining associations;relational table
pruning techniques;database;generalized association rules;frequent itemsets;taxonomy tree;association rules;pruning techniques
database;association rule mining;mining association rules;mining association rules;real life;join result;data mining problem;mining algorithm;relational data
uci datasets;active learning;data acquisition;data acquisition;active learning
online algorithms for;semi-structured data;semi-structured;data mining;incremental maintenance;data stream;xml data;frequent patterns
databases;classification;pruning technique;database;high quality;classification rules;associative classification;associative classification;classifier;classification
sample size;data sets;taking into account;feature selection algorithms;feature selection algorithms
transductive learning;test set;class distributions;vector machine;learning theory;training sets;support vector;margins;working set
classification model;textual information;storage space;real-life;text classification;model training
algorithm called;ensemble methods;regression problems
frequent sequential patterns;frequent patterns
clustering;similarity measure;unsupervised clustering;string data;weighted average;user input
learning algorithm;inductive learning;computing resources;modeling framework
clustering;clustering algorithms;excellent scalability;high dimensional data sets;high quality;input space;large data sets;data sets;scalability problems;large amounts of data;high dimensionality;clustering;sampling technique
tree structure;web pages;classification;web page classification;related algorithms
clustering;clustering;dimensionality reduction technique;entropy measure;clustering criteria;large number of;kdd community;feature subsets;feature selection;clustering algorithm;pre-processing;feature selection
unsupervised algorithm;time series;time series;discover patterns;sensor data;unsupervised algorithm;learning problem;discovery problem
frequent patterns;data mining techniques;discovering frequent;low support;frequent itemsets;data sets;database objects;occur frequently;translation invariant
time series;unsupervised algorithm;time series
operator;data set;data analysis;rough set;formal concept analysis
segmentation problem;time series;optimization problem;time series;stock data;data mining;pattern matching
computational efficiency;large numbers of;prediction accuracy;naive bayes;naive bayes classifier;bayesian learning
supervised learning;neighborhood graph;database
human effort;training examples;web sites;text categorization;extraction rules;information extraction;web site
search space;noise handling;efficient discovery of;large datasets;protein structures;discover meaningful
clustering;clustering;internal) node;market-basket data;nearest;category-based;market-basket;category-based;market-basket data;taxonomy tree;prior works;clustering algorithm;high dimensionality;clustering quality
data analysis;14, 6;5;principal component analysis;continuous functions;image processing
prediction problem;discrimination power;rule-based;finding patterns;time-series;temporal data mining
dependency analysis;hybrid approach;18;databases;data mining approach;data sets;bayesian networks;learning problem;bayesian networks;network structures;search efficiency
term association;data-mining;classification;database;text documents;term association;text categorization;application domains;text classifier;market basket analysis;document categorization;classifier
query answering;query evaluation;version space;data structure;version spaces
mining task;fp-tree;minimum support;closed patterns;tree mining;hash-based;pattern mining algorithms;association rules;closed patterns;support threshold;frequent pattern mining;frequent pattern;minimum support
classification rule;uci data sets;instances;prediction tasks;rule learning
discovered patterns;typical patterns;graph isomorphism;semi-structured;compact representation;graph patterns;graph data;data mining;semistructured data;structured data
data mining technique;rule sets;data mining;association rules;log data;log data
high dimensional;cluster membership;clustering high dimensional data;local minimum;data clustering;clustering analysis;dimension reduction
nearest-neighbor algorithm;plans;marketing strategies;case base;data mining;cost-effective
clustering;database;temporal patterns;time-series;data mining;time-series;medical databases;similar patterns
text mining;human effort;recommender systems;data mining techniques;semantic features;feature construction;data mining systems;provide evidence;data mining;automatically infer;knowledge base
progressive sampling;progressive sampling;learning curve;sample size;equivalence classes;association rule mining;data mining;association rules
clustering;hierarchical clustering algorithms;hierarchical clustering;selection methods;objective function
domain knowledge;user preferences;mining frequent itemsets;multiple attributes;multiple attributes;mining process;mining framework;functional dependencies;relational data
mining results;mining algorithms;fp-tree;iterative process;mining process;frequent itemset mining;frequent itemsets;data mining
tree-structured;completeness;data model;data mining
clustering;clustering;unsupervised classification;arbitrary shape;large data sets;supervision;clustering process;algorithms require;data sets;large datasets;clustering algorithm;class labels
theoretical properties;high quality;rule mining;frequent itemsets;rule generation;statistical significance;association rule
data mining
incremental algorithms;incremental update;incremental algorithm;mining frequent sequences;frequent sequences;incremental update;database;frequent sequences;sequence database
classification;information retrieval;naive bayes classifier;search engines;visual information;hierarchical representation;visual information;web page;classifier
candidate 2-itemsets;database;temporal association rules;large databases;candidate2-itemsets;association rules
spatial data;ranking methods;spectral methods;text retrieval;retrieval systems;discrete cosine transform;high precision
feature space;capacity control;hypothesis space;learning algorithm;boolean functions;classifier;randomly generated;unseen data;support vector machines;boolean functions;naive bayes classifiers;feature spaces;capacity control;classification task;classifier
clustering;nearest neighbor classification;nearest neighbor;database;data mining;sampling-based;complex objects;data cleansing;similarity search;point set;databases;data mining;similar objects;distance join;nearest neighbor;similarity join;algorithm to compute;nearest neighbors
clustering;high-dimensional;local search;data points;high dimensional;local maximum;iterative clustering;text data;search procedure;document collections;text data;cluster sizes;cosine similarity;objective function
recommendation systems;formal model for;user preference;formal model;user preference;data sparseness
cross-validation;instance;monte carlo;time series;model complexity
domain experts;vice versa;minimum description length;knowledge acquisition;knowledge based systems;class distribution
causal structure;15;data sets;causal model;data mining and knowledge discovery
frequent itemsets;search tree;frequent itemsets;algorithm called;information gathered;data mining;support counting
clustering;similar queries;web documents;real-world;domain-specific;search engines;automatic generation of;clustering algorithm;clustering approach;query terms
data set;data sets;secondary structure
instance
data analysis;data summarization;multiple attributes;event data;categorical data;data model;typically involves;large data volumes;large data sets;event data;mining tasks
em) algorithm;dynamic programming algorithm;specially designed;cross-validation;dna sequence;expectation-maximization
significant patterns;information gain;periodic patterns;periodic patterns;mining algorithm;information gain
significant rules;emerging patterns;decision trees;emerging patterns;decision tree
state space;data mining;functions defined;data sets;data mining;classifier
music recommendation;user preference;classification;user's preference;classification approach;classifier
data structure;mining frequent patterns;fp-tree;database;pattern tree;databases;support threshold;association analysis
machine translation;parameter estimation;adaptive approach
dimensionality reduction method;classification;textual information;term-based;text classification;latent semantic indexing;vector space model;positive effect
database;databases;sequential patterns;sequential patterns
outlier detection;outlier detection;8;neural networks;data mining
real-world datasets;databases;classification;database;accuracies;learning algorithms;classification methods;image data;classification performance;decision tree;6;noisy data;biological data;classification algorithm
ensemble learning;parameter space;state space;learning algorithms;meta-data;learning algorithm;parameter settings;data mining
domain-independent;evaluation metric;application domain;rare classes;rare class;rare classes;classifier
similarity function;classification accuracy;similarity-based;learning systems;data set;data sets;rough sets;selection algorithm;rough set theory;similarity-based;selection algorithm
clustering;data set;interactive exploration;clustering
inductive databases;database;mined patterns;inductive databases;knowledge discovery;xml-based;source data;data mining;model called
clustering;tree based;similarity matrices;conceptual clustering;machine learning;clustering methods
clustering;clustering;hidden patterns;spatial data;arbitrary shape;clustering problem;large databases;search space;clustering process;taking into account;spatial data;geographic information systems
point sets;database;point sets;index structure;real data;discovering patterns;pattern discovery;pattern discovery;indexing technique
sequential pattern;recommender systems;sequential patterns;sequential patterns;frequent itemsets;web usage;usage data;mining tasks
patient data;classification;patient data
functional dependencies;databases;uci datasets;database
detection problem;independent motion;database;independent motion;real data;surveillance video;surveillance video;moving targets
neural networks;artificial data;real data;statistical technique;principal component analysis;dimensional data
theoretical analysis;learning parameters;bayesian network;bayesian network;distributed data
web pages;database;electronic commerce;web sources;extraction techniques;service providers;web sources;web page
item set;item sets;association rule mining;object-relational;aggregate functions;optimization problem;ad hoc querying;operator;databases;operator;relational databases
intrusion detection;extraction algorithm;data sets;problem domain;8;normal behavior
association rules;apriori algorithm;mining association rules;real data sets
clustering;data-driven;mined patterns
correlated features;5;4;6;9;8;classification problems
network structure
graph-based;pattern mining;pattern mining;graph datasets;graph-based;depth-first search
multi-dimensional;classification;large sets of;wavelet based;classification problems;wavelet transformation
item sets;artificial data;learning task;realistic data;training examples
clustering;em) algorithm;real datasets;data mining tasks;time series;time series;clustering problems;clustering problems
association rules;databases;mining process;7;mining process
naïve bayes;feature selection;modeling approach;probability estimation;class probabilities;probability model
web usage;rule learning;web log data;post-processing;interesting association rules;interesting rules;log data
predictive models;database;database
data mining algorithms;data mining;data mining tools;data mining
fourier transform;historical data;neural network;time series
web-based;mining algorithm;data warehouses;data warehouse;web sites;web log data;web logs;web site;click streams;mining algorithm
business intelligence;web logs;access-control;business intelligence;database
visualization techniques;real-life;visual representation;case study
scientific literature;case study
adaptive sampling;estimation algorithms;monte-carlo;estimation algorithm;upper bound
query optimization;rewriting queries;database systems;conjunctive queries;data integration systems;data-integration;query containment;query containment
electronic commerce
xml documents;regular tree;extensible markup language;xml query languages
web documents;web applications;selectivity estimation;selectivity estimation;real data sets;conjunctive queries;large number of;ckkm;data set;boolean queries;selectivity estimates;relational databases;monte carlo
information content;rewriting queries;query language;materialized views;data warehousing;aggregate views;completeness
query evaluation;reachability queries;connected components;large number of;data complexity;databases;query languages;constraint databases;model-checking
indexing schemes;indexing structure;block size;nearest-neighbor queries;indexing scheme
approximate algorithm;sensitive information;continuous data;sum queries;continuous attributes;databases;special case;range queries;aggregate function;statistical databases;np-hard
database management;xml applications;xml documents;navigation paths;databases;object-oriented databases;semantic constraints;information exchange;data integration
data mining
classification scheme;software tools;data mining and knowledge discovery;knowledge discovery;database;knowledge discovery in databases;data mining
spatio-temporal;data mining;spatial data;data mining research
network models;information resources;neural networks;neural network model;modeling technique;sales data;forecasting accuracy
data mining;data mining;data mining


data mining;data mining
web-based;biological databases;data analysis;pattern recognition;data mining techniques;databases;data mining research;interestingness measure;database;business intelligence;data mining;biological databases;mining algorithm;dna sequence
1;association rules;contingency table;generalized) association rules
world-wide web;keyword queries;data mining;data warehouses;data mining and machine learning;learning problems;data mining research;search result;data mining;semi-supervised
web-based;pattern analysis;web usage mining;data mining techniques;usage patterns;pattern discovery;web data
database;knowledge base;data-mining;data mining
human learning;artificial neural networks;learning algorithms;intelligent systems;stored information;artificial neural networks;neural network
knowledge discovery;acm sigkdd international conference on;high quality;technical program;data mining
data mining
data providers;databases;data mining;industrial applications;data mining
knowledge discovery in databases
classification;classification
classifier learning;classifier learning;data-mining tool
decision trees;classifier learning
knowledge discovery;exploratory data analysis;prediction model;decision boundary;decision tree
bayesian network;database;causal models;knowledge discovery;bayesian networks;incomplete data
data miners;valuable information;data mining
knowledge discovery;data-rich;data rich;geographic information;workshop brought together
web usage analysis;international conference on;web usage analysis;user profiling;knowledge discovery;user profiling;data mining
query evaluation;data source;database schema;conjunctive query;query rewriting;constraint databases;data sources;data integration;np-complete;query planning;source databases;constraint databases;data integration;global schema
accessing data;years ago;data stored;database;db2 udb;data model;query optimization techniques;data stores;data sources;query processing;databases;application developers;relational database system;data access
information discovery;heterogeneous data sources;scalability issues;agent-based;match-making
linear transformations;time series;time series;multiple times;similarity queries;moving average;similarity-based;real data;processing queries
point-based;based reasoning;database;temporal reasoning;temporal queries;interval-based;query languages;temporal reasoning
temporal aggregation;database management systems;database size;temporal dimension;data reduction;temporal aggregates;processing nodes;algorithms for computing
singular value decomposition;multi-dimensional;fourier transform;range query;time series;time series;fast retrieval;nearest neighbor query;databases;false dismissal;time series;feature vectors
mining algorithms;spatial data;access methods;database;data types;static data;user-defined;statistical information;hierarchical structure;data mining approach;query processing;spatial data mining;spatial data mining
frequent patterns;database;databases;time series;time series;database;periodic patterns;time-series;periodic patterns;data mining problem
multi-level;query routing;information sources;digital information;indexing techniques;large-scale;pruning strategies;user query;query execution;search engines;keyword-based;digital library;digital libraries;user-friendly;query routing;data management systems
query formulation;semistructured data;query processors;view definition;document type definitions
large corpora;data mining;databases;machine learning;knowledge discovery in databases;databases;data mining;artificial intelligence
clustering;information sharing;data warehouses;xml data;data warehouse;multimedia documents;keyword-based;rule-based classification;decision makers;content-based retrieval;structured data;unstructured data;multimedia database
database applications;database applications;xml documents;relational data;relational database
database;databases;mobile devices;data items;replication;computing devices;fundamental problem;databases;concurrency control
applications involve;database;application domain;query language;query language;data model;multimedia presentation;object-oriented;query processing;graph data;operator;query processing techniques
response times;access method;table scan;relational algebra;relational queries;sort order;interactive applications;cost model;algorithm relies on;result set;space partitioning;disk page;efficient access to
fast approximate;fall short;data analysis;database;large volumes of data;olap) applications;complex queries;databases;query answering;query results;summary statistics;olap applications
aggregation function;optimization techniques;minimum number of;memory requirement;database
join algorithm;similarity join;index structures;tree-based;index structure;join algorithms;algorithm called;spatial join
data source;database systems;data sources;data access;queries involving;data access
information sources;rewritten query;rewriting queries;trade-offs;data warehouse;query processing and optimization;query rewriting;view maintenance;view definitions;equivalent} query;query cost;data warehouses;analytical model;cost-model;query answers;query refinement
simulation study;multimedia databases;similarity searches;efficient retrieval of;database;indexing structures;index structures;efficient retrieval of;common features;indexing techniques;query type;index structures;multimedia applications
data warehouse;cost model;trade-offs;query results;information space;query rewriting;optimization technique;data warehouse;large number of;1;3;2;4;query result;query rewriting;information resources;rewritten query;maintenance cost;query interface;2, 3, 4;large-scale;essential information;view maintenance;computational cost;result (quality;information systems;trade-offs;equivalent query;long-term
object-oriented;query processor;object identification;object-oriented;definition language;object identity;database integration;object-oriented databases;database schemas
fast approximate;fast approximate;tree based;nearest neighbor;search algorithm for;nn search;index structure called;index structures;nn) search;nearest neighbor queries;search algorithm
data mining;query optimization;information systems;query optimizer;data types;data lineage;query execution plan;user-defined;query performance;fine-grained;data lineage;query processing;data streams
selectivity estimation;selectivity estimation;database;search tree;relational database systems;user-defined;query optimizer;estimation methods
data analysis;systems support;data points;data warehouse;database;data model;materialized views;hierarchical structure;data cubes;commercial systems;aggregate views;formal model
data cube;range sum queries;application domains;data cubes;range sum queries;data cubes
data structure;optimizer;database;data types;user-defined;complex objects;data structures;optimizer;object-relational database systems
complex queries;individual queries;maintenance cost;decision support systems;database administrators;microsoft sql server
data blocks;simulation results
distributed memory;network nodes;cost based;distributed memory;goal oriented;hit rate
algorithm performs;update transactions;database;databases;web servers;databases;greedy algorithm
multiple users;scheduling policies;main memory;storage systems;multimedia data;user-interaction
data partitioning;multimedia information;databases;dimensional data;multidimensional data;range queries
access patterns;access method;memory size;spatial data;range queries
web- based;web-based;java-based;database systems;mobile agents;mobile agents;database access;current commercial
enterprise applications;web accessible;databases;databases
systems support;workflow management systems;workflow management;workflow management;workflow management systems;business processes;workflow management system;integration process;typically requires;control flow
database;space overhead;main-memory;database;low-cost;direct access to;hardware architecture;storage manager;application code;data entry
large numbers of;database;index structure;main-memory;main memory;large number of;tuple)-level;selection predicate
search performance;database systems;index structure;similarity search;high dimensionality;high dimensional feature spaces;high dimensional;support queries;space partitioning;high dimensional feature space;large database;data structure;high dimensional feature spaces;index structure;data items;data structures;linear scan;feature based;data partitioning;search paradigm;range search;index structures;similarity search;distance based;databases
23;index structures;12;access control
index nodes;update operations;parallel database systems;fast retrieval;high speed
decision trees;classification;database;data-intensive;databases;data organization;microsoft sql server
storage structure;disk access;post-processing;discrete data;access patterns;storage management;range queries
data warehouses;data warehouses;view maintenance;materialized views;databases;key constraints
parallel algorithms;classification;shared-memory;decision-tree classifier;shared-memory;decision-tree;data mining;load balancing
minimum support;databases;rule mining;minimum confidence;data-set;frequent itemsets;algorithm maintains;constraint-based;relational data;constraint-based
dynamic programming algorithm;memory requirements;domain size;association rules;association rule;numeric attributes;numeric attributes;execution times
composite events;temporal relations;event detection;composite events;distributed environments;distributed environments;composite event;operator;formal semantics
event-condition-action;database systems;sql server;relational database systems;composite events;database;implementation details;agent-based;database functionality;active database;database management system
management systems;language constructs;meta-model;complex events;complex events;complex events;active database;management systems;active database
instance;transaction models;data resources;object-oriented;business processes;concept called;implementation issues
distributed database systems;distributed transactions;database systems
web documents;application-level;web servers;web server;web content;web server;data management
user requests;workflow engine;workflow management system
database systems;selection criteria;resource management;efficient access to;resource manager;reference model;workflow systems;query optimizer;interval-based;1;management systems;3;2;4;resource usage;workflow engine;resource allocation;query rewriting;main memory;initial query;1, 2;3, 4;efficient retrieval of;allowing users to;oracle database;resource manager
object- oriented;web pages;database;web browsers;database;learning systems;web pages;web page
object-oriented;query evaluation;semantic relationships
web service;replication;web browsers;current web;replication
clustering;data objects;vector space;similar objects;distance function;database;real-life;large datasets;very large datasets;clustering algorithm;clustering quality;metric spaces;clustering
clustering;clustering algorithms;data points;synthetic data sets;categorical attributes;hierarchical clustering algorithm;real-life;traditional clustering algorithms;categorical attributes;clustering algorithm;similarity measures
mining algorithms;minimum support;database;disk accesses;apriori algorithm;data access;data mining;association rules;data organization
computing environment;expected cost;image data;content based image retrieval;complex queries;image processing;image database;large collections of;query processing;multimedia) databases;multimedia databases;multi-step;retrieve images;color histogram;high level;database;texture features;query language;data model;algorithm requires;query processing;upper bound
data structure called;content-based retrieval;data analysis;databases
video content;database technology;database systems;video sequence;database;video data;rule-based;database research;query language;data model;database;data types;databases;video data;multimedia data;video frames
approximation techniques;constraint databases;indexing techniques;spatial objects;constraint databases;data structures;5;query processing;temporal data
spatial objects;optimization techniques;database technology;constraint databases;linear constraints;efficient query evaluation;index structure;spatial joins;constraint databases;worst case;upper bound;constraint databases;constraint solving;spatial join
join algorithm;scheduling strategy;storage devices;large-scale;scheduling strategies;scientific applications;long-running;cost model;disk-resident;database;data warehousing;analytical model;data mining;online analytical processing;aggregation queries
multi-pass;scheduling algorithm;greedy algorithms;multi-pass
parameter space;simulation study;replication;large-scale;database applications;increasing number of;scheduling algorithm;data replication;storage systems;massive amounts of data
search engines;statistical method;search engines;estimation method;search engine
software development;world wide web;information systems;designed to support;cost-based;business opportunities;computational models;computing platform;operating systems
protocol called;communication network;database;large scale;large number of;large-scale;control method;large-scale;data replication;communication costs;databases;concurrency control;distributed database;efficient access to
xml data;database technology;database;data types;management systems;modern database;object-relational database systems;data integration
query evaluation;database;query response time;user queries;decision makers;query processing strategy
query processing;plans;query plans;query processing;query-processing capabilities
data-intensive applications;world wide web;object based;multimedia databases;caching techniques
business objects;application integration;application integration;business objects;event-based;application integration;information exchange;business objects;relational databases;diverse applications
data transformation;database;application integration;application integration;database
enterprise applications;application integration
ad hoc;data mining;real-life;olap queries;data analysis;decision support queries;ad hoc;sql extension
similarity query;distance function;database;data mining applications;database objects;similarity queries;similarity queries;nearest neighbor queries;query processing;databases;databases;sequential scan;data mining algorithms;cpu cost;range queries
image database;amdahl's ratio laws for;multiple-instance learning;storage systems
high-dimensional;metric spaces;nearest neighbor queries;storage systems;amdahl's ratio laws for
process model;service-oriented;large number of
partial orders;pattern language;database;spatio-temporal;database;query language;data model;temporal order;activity recognition;activity recognition
amdahl's ratio laws for;workflow management system;storage systems
amdahl's ratio laws for;moving objects;storage systems
automatically generates;instance-level;multi-dimensional;data analysis;systems support;fine-grained;data warehouse;data items;relational views;materialized views;view definition;data lineage;data warehousing;data item;source data;arbitrarily complex;data warehousing;distributed sources;auxiliary;2, 3;4
databases
data sources;information integration;web site;query planning;retrieve information;distributed sources;information integration

image database;object-oriented;storage systems;amdahl's ratio laws for
constraint satisfaction;dynamic nature of;cost-benefit analysis;supply chain;main components
amdahl's ratio laws for;storage systems;semi-structured data
amdahl's ratio laws for;multimedia information;storage systems
cost model;features including;optimizer;web environment;query optimizer
mobile user;multi-level;mobile environment;multi-channel;wide range;mobile users
low bandwidth;query results;plans;intermediate results;optimal plan;optimization algorithms;compression methods;semantic information;query results;decision-support applications
clustering;indexing schemes;indexing techniques;multi-attribute queries;power consumption;cost models;multi-attribute queries
algorithms for computing;efficiently computing
amdahl's ratio laws for;storage systems
amdahl's ratio laws for;storage systems;data management
amdahl's ratio laws for;database engine;storage systems
network bandwidth;database;amdahl's ratio laws for;data engineering;web caching;storage systems;web data
data distributions;high-dimensional space;data points;database;index structures;compressed representation;index structure;compression technique;cost model;compression techniques;query cost;high-dimensional spaces;wide range;databases;dimensional data;indexing techniques;nearest neighbors;optimization algorithm
amdahl's ratio laws for;evolving data;storage systems
completeness;multimedia data mining;spatial relationships;content-based retrieval;data mining;multi-resolution;frequent item-sets;data repositories;multimedia data
decision tree classifier;decision tree;classification accuracy;categorical attributes;single attribute;classification errors;traditional classifiers;building classifiers
regular path queries;query optimization;regular expressions;database;regular path queries;data independence;mobile computing;combined complexity;information integration;data warehousing;query answering;query answering;semistructured data;regular path queries
amdahl's ratio laws for;storage systems;data warehouses
amdahl's ratio laws for;storage systems;database
data sequences;amdahl's ratio laws for;similarity search;storage systems
real data sets;multimedia databases;implicit assumption;query optimization;worst-case;skewed;data set;nearest neighbor queries;similar images;uniformly distributed;spatial databases;nearest neighbor search
amdahl's ratio laws for;access method;storage systems
cost model;heuristic algorithms;data dissemination;clustering algorithm;search algorithm;np-hard
amdahl's ratio laws for;index structure;storage systems;data warehouses
optimizer;plans;query optimizer;database;databases;query plans;query optimizers;microsoft sql server
scheduling algorithm;amdahl's ratio laws for;storage systems;multimedia objects
incremental algorithm;large-scale;information filtering;large-scale;users' interests;data delivery;push-based;user interests;user profiles;user access;push-based;user profiles;user feedback;data delivery
software development;building blocks for;information content;web pages;information sources;rules generated;content filtering;extraction rules;web source;xml documents;xml tags;xml-enabled;source-specific;automatic generation of;code generation;xml-enabled;user-friendly;wrapper generation;information extraction;web source
large amounts of;java-based;response times;distributed query processing;database;remote sites;current web;client-site
diverse range of;replication;data replication;data consistency;data access;constraints imposed by;data consistency;data management
amdahl's ratio laws for;multi-layer;database systems;storage systems
database applications;relational database systems
data sets
database technology;database systems;database;declarative queries;computing environment;mobile devices;large collections of;database objects;small-scale
xml data;amdahl's ratio laws for;storage systems;efficient storage
relational databases;storage systems;semi-structured data;amdahl's ratio laws for
similarity queries;queries efficiently;multimedia databases;query points;query refinement
amdahl's ratio laws for;similarity search;image collections;storage systems
selectivity estimation;selectivity estimation;synthetic datasets;real datasets;power law;relative error;matching systems;special case;uniform distribution;range queries
clustering;amdahl's ratio laws for;storage systems;image databases
data integrity;sql queries;business transactions;extensible markup language;db2 udb;user-defined;xml documents;entire document;xml elements;document type definitions;databases;sql query;information exchange;data management
data warehouse;related information;keeping track of;instance;control flow;data flow;local information;higher order;web interface;workflow management system
xml enabled;database technology;semi-structured data;database;xml data;data model for;xml enabled;structured data;information exchange;data management system
mobile user;pruning strategy;search space;access latency;mobile computing;data allocation;data allocation;data access
amdahl's ratio laws for;optimization techniques;data-intensive;storage systems
amdahl's ratio laws for;storage systems
amdahl's ratio laws for;relational systems;storage systems;user defined
temporal aggregation;temporal aggregation;large-scale;worst-case;database applications;database;query processing and optimization;temporal aggregates;data warehousing;unique characteristics;average-case;small-scale
amdahl's ratio laws for;storage systems
graph partitioning;clustering categorical data;clustering categorical data
missing data;missing data;databases
historical data;systems support;data warehouses;data warehouse;on-line analytical processing;pre-computed;decision-makers;materialized views
temporal association rules;amdahl's ratio laws for;storage systems
closed sets;item set;discovered association rules;association rules;association rules;frequent closed
frequent sets;amdahl's ratio laws for;approximate query answering;storage systems;maximum entropy
data cleaning;amdahl's ratio laws for;storage systems;extensible framework
data warehouses;data warehouse;incremental maintenance;extraction methods;data warehouse;common assumption
base relations;database;np-complete;data warehouse;view maintenance;great potential;incremental maintenance;views defined;view maintenance;update streams;conjunctive queries
data replication;implementation details;integrate data from
data-warehouse;data analysis;data warehousing;multidimensional databases;parallel computation;data cleansing;data-warehouse;incremental computation;rates;data volumes;business intelligence applications;data mining;data flow;traffic analysis;traffic analysis
years ago
query optimization;variable selection;sampling method;cost model;data clustering;regression analysis;cost models;dynamic environment;cost models
integration systems;query capabilities;optimization problem;query planning;query planning;query result
poor performance;data integration systems;execution plans;performance gains;query optimizers;data delivery;data integration
lower-bound;distance functions;suffix tree;similarity measure;index structure;sequence databases;fast retrieval;query processing;rates;disk-based;indexing technique
similarity-based;databases;time series;time series;similarity-based;similarity measures
amdahl's ratio laws for;storage systems;data mining
amdahl's ratio laws for;isolation level;storage systems
isolation level;serializability;serializable;serializable;transaction processing
data flow
data mining;decision trees;classification;access methods;database;real datasets;data mining;tree-based;traditional models;real-life;selection methods;tree construction;databases;main memory
databases;user preferences;access control;configuration management;large number of;database;application developers;network applications;managing data;security applications;object-oriented databases;highly distributed;key features
large amounts of;database research;special properties;database;optimization problems;database applications;indexing techniques;database;large number of;high-dimensional spaces;high-dimensional spaces;data warehousing;visualization techniques;query processing techniques;dimensional data;content-based search;feature vectors;processing queries
web information retrieval;web ir;data structure;classification methods;information retrieval;search engine;problems arising
database access;amdahl's ratio laws for;storage systems
indexing schemes;indexing schemes;user-supplied;database;index-scan;domain-specific;indexing scheme;information retrieval;case study;domain-specific;definition language;index scan;index-maintenance
selection problem;optimizer;user interface
amdahl's ratio laws for;high availability;storage systems
amdahl's ratio laws for;storage systems;databases
spatial data;auxiliary;multidimensional) data;range query;skewed;wide range;range queries;data structure called;index nodes;range queries
join processing;amdahl's ratio laws for;storage systems;duplicate detection;data redundancy
key range;databases;index scans;sex,age;index scan
world wide web;fast response;growing rapidly;control systems;web servers;easy access;web users
database systems;database;acm sigmod;database;database management system;databases
information retrieval research;information retrieval applications;annual international acm sigir conference on;information retrieval;annual international acm sigir conference on;los angeles
web pages;web ir;web graph;web graph;information retrieval;ir methods
database systems;large-scale;semi-structured;information access;information retrieval;information retrieval;ir research;information access;data mining;database functionality;human judgments;search interfaces;xml documents;ir researchers;web search engine;user feedback;1;3;information science;data types;user context;unstructured data;natural language;simple queries;ir systems;scientific data;data structures;2;information systems;jim gray;distributed search;structured data;database;knowledge-based;machine learning;retrieval models;search engines;text databases;databases;long-term;ir) research;question answering;semantic heterogeneity
language model;ad-hoc;language models;predictive distribution;ad hoc information retrieval;ir models;language model
subtopic retrieval;mixture model;query topic;relevance ranking;statistical language models;retrieval methods;data set;query likelihood;subtopic retrieval
naïve;probabilistic model;text data;information retrieval;learned model;retrieval algorithms;text retrieval
syntactic structures;question classification;tree kernel;support vector machines;question answering;kernel function;decision tree;naive bayes;text features;support vector machines;nearest neighbors;question classification;machine learning algorithms;dynamic programming;machine learning approaches
external knowledge;question answering;event-based;event-based
density-based;retrieval algorithms;document retrieval;passage retrieval;passage retrieval;future directions for;question answering systems;question answering;query terms
automatic query expansion;link structure;high quality;information retrieval;domain-specific;structure information;automatically constructing;natural language processing;link analysis;question answering
pagerank algorithm;web search;link structures;current web;web-pages;search engines;link-based;access patterns;link analysis;web-page
classification scheme;web documents;mutual information;query type;classification;document retrieval;link information;finding task;classification method;user query;user queries;content information;retrieval performance;query type;topic relevance
information discovery;search tools;web page;search interface;information retrieval
task type;search tool;query formulation;search tools;search strategies;content-based image retrieval;content-based image retrieval
search results;web search;query reformulation;search engine
computational complexity;nearest neighbor;classification method;text categorization;real-world applications;power law;logistic regression;svm classifiers;support vector machines
data structure;reuters corpus;text categorization;text collections;text collection;multi-class;compression-based
probability estimates;high quality;text classifier;score distribution;text classifiers;cost function;text classifiers
clustering;machine translation;training set;retrieve images;image features;probabilistic models;relevance models;cross-media;image collections;information retrieval models;image annotation;relevance model
latent variable model;latent dirichlet allocation;database;text-based image retrieval;conditional distribution;joint distribution;instance;annotated data;mixture models
image retrieval;topical relevance;generative probabilistic;retrieval results;visual information;retrieval model;evaluation measures;retrieval systems;average precision
search algorithms;language model;data fusion;document representations;meta-search
information discovery;ranking results;xml fragments;database;xml query;information retrieval;xml documents;business applications;high precision;search engine;xml search;vector space model;xml collections
search systems;tf*idf;vector space;retrieval effectiveness;information retrieval;retrieval model;word sense disambiguation;information retrieval systems;word sense disambiguation
retrieved documents;natural language;linguistic knowledge
feature extraction;conventional techniques;text categorization;optimization problem;classifier;performance metric;highly nonlinear;learning framework;objective function;application scenarios
automatically extracted from;automatically extracted;concept-based;semantic features;text categorization;standard benchmarks;information retrieval;document representations;probabilistic latent semantic analysis;phrase-based;term-based
classification;optimization problems;score distributions;classification methods;text categorization;labeled documents;real-world applications;rare class;controlled experiments;logistic regression;loss functions;error bounds;reuters-21578 corpus;positive examples
user profile;information retrieval research;takes into account;concept hierarchy;information filtering;evaluation function
ir systems;query length;information retrieval;ir) systems;search effectiveness;interactive information retrieval;query length
query expansion;automatic query expansion;interactive query expansion
related terms;latent semantic indexing;latent semantic analysis;optimal number of;database
retrieval models;document frequency;parameter estimation
multi-dimensional;graphical models;table extraction;table extraction;information retrieval tasks;hidden markov models;modeling techniques;data mining;extract information from;question answering;conditional random fields
search systems;filtering systems;document collection;search algorithms;search topics;reuters corpus;relevance judgments;test collections;test collection
patent retrieval;patent retrieval;information retrieval;retrieval models;test collections;document genre;test collection
probabilistic latent semantic analysis;predictive models;user ratings;collaborative filtering;database;filtering techniques;data set;user preferences;gaussian distribution;compares favorably with;user communities
document cluster;document corpus;cluster membership;clustering method;clustering results;matrix factorization;document clustering;document matrix;clustering methods;accuracies;semantic space;document clustering;latent semantic indexing
clustering;data objects;clustering results;web pages;clustering algorithms;feature space;link structure;multi-type;high dimensional;cluster quality;clustering process;multi-type;clustering accuracy;clustering;clustering approach;data sparseness;web users
content-based music;feature extraction method;classification algorithms;content-based music;linear discriminant analysis;genre classification;classification accuracies;support vector machines;machine learning;information retrieval systems;global information
digital library;resource selection;normal distribution;retrieval quality;decision-theoretic framework;resource selection;document scores
ranking algorithm;database contents;estimation method;database;database size;large databases;relevant document;resource selection;large" databases;document rankings;selection algorithm
efficient search;topic segmentation;social network;machine learning;query processing;network traffic;design issues;long-distance
sentence level;novelty detection
domain-independent;distance matrix;text documents;text streams;domain-specific;segmentation method;dynamic programming;domain-independent;semantic information;text segmentation;image representation;segmentation problem
test data;tf-idf;event detection;source-specific;multiple streams;detection task
retrieval effectiveness;query term;optical character recognition;cross-language retrieval;document frequency;term frequency
target language;transformation rules;generated automatically;cross-lingual;source data
transliteration;cross language information retrieval;mapping rules;automatically creating;test collections;automatically generating;text retrieval;pseudo-relevance feedback;average precision
highly correlated;relevance judgments;retrieval systems
link analysis;text analysis;information retrieval
precision-recall;precision recall;information retrieval performance;language model
world wide web;graph structure;relevant documents;query topic;content analysis;1
web directories;web pages;automatic evaluation;relevance judgments;information retrieval;web queries
context information;trec data;document ranking;user query;retrieval performance;interactive information retrieval;average precision
web search engine;relevance judgments;information retrieval;ir) systems;retrieval systems;human relevance judgments;test collection
term weights;term frequencies;information retrieval performance;average precision;information retrieval
syntactic information;question answering;information retrieval;syntactic features;question answering;syntactic information
information-seeking;web pages
query expansion;query terms;information retrieval
salient features;hierarchical structure;source text;user evaluation
trec data;loss function;relevant documents;error rates;closely related;linear combination;retrieval systems;online learning;unified framework;average precision;excellent performance;document set
visual feature;retrieval tasks;video retrieval;average number of;video content
test collection;statistics-based;transliteration;cross-language information retrieval;cross-language information retrieval
classification;text categorization;training examples
building classifiers;visual concepts;semantic concepts;video annotation;user-defined
trec-8 ad hoc;error analysis;information retrieval
xml elements;comparative analysis;xml element;xml retrieval;information retrieval
information flow;semi-automated;data processing
search engines;general-purpose;3;search engines;search engine;machine learning algorithms
language independent;retrieval accuracy
information-seeking;specifically designed to;query terms;information retrieval
source code;classification;classification;world wide web;classification accuracy;high degree of;vector machine
semantic content;neural networks;extracted features;texture features;similar images;image classification;support vector machines;image processing;classifier;retrieval effectiveness
lda model;latent semantic indexing;language modelling;latent dirichlet allocation;dirichlet prior
users' search;high-quality;web search;web search;query logs
retrieval effectiveness
search engine;general purpose;databases
broadcast news;segmentation algorithm
automatic evaluation;source text;text categorization;neural networks
clustering;classification;rule-based;clustering method;context-dependent;databases;text classification;rule-based;dimensionality reduction;class-specific
reusability;description language;index structure;high speed
information-theoretic measure;information theory;similarity measures;document similarity
svm-based;term-selection;training data;classification;positive examples
ir systems;comprehensive evaluation;digital library;information retrieval
web searches;search engine;automatically generating;documents retrieved
query processing engine;database;textual content;keyword-based;native xml;ir-style;processing queries;structured queries
textual information;document retrieval;information-seeking;web sites;knowledge-intensive;vector space model
clustering
naive algorithm;database;worst-case;aggregation function;database size;obtained by combining;worst case;simple algorithm;high probability;high-probability;aggregation functions;random access;threshold algorithm
search engines;data structures;document databases;instance
random projections;projection matrix;database;dimensional euclidean space;database
xml documents;data values;xml queries
multi-query optimization;buffer space;plans;related queries;np-hard;multi-query optimization;share common;sufficient condition for;minimum cost;subexpressions;database systems
string data;index structure;xml documents;indexing problem;databases;index structures
game theoretic;bounded treewidth;game-theory;k;conjunctive queries
hierarchical data;commercial applications;relational tables;xml documents;inference mechanism;wrapper generation
'orgchart'
world-wide web;random walk;signal processing;user models
load balancing;scheduling policies;web servers;web server
world wide web;extracted information;image collection;image content;relevance feedback;user interface;web site;similarity metrics;relevance feedback;search engine;relevance feedback
mining algorithm;server logs;document clusters;access patterns;main memory;spatial locality;document clusters;server logs;data mining;access patterns;spatial locality;simulation results;address space
computational geometry;world wide web;collaborative learning;extensible framework;web browser;learning environment
database;information retrieval;business process;object oriented;cost-effective;data storage
distributed memory;address space


distributed memory;address space
database systems;database
query evaluation;standard "database;natural language text;database;evaluation strategies;information retrieval techniques;information retrieval;xml databases;xml database;queries efficiently;access method;stack-based
xml element;search engine;xml keyword search;xml elements;data model;xml documents;search queries;keyword search over;html documents
web usage analysis;real-world;large number of;error guarantees;continuous queries over data streams;communication cost;monitoring queries;applications involve;data streams;extensive simulation;distributed data streams
online algorithms for;resource constraints;sliding window;real data;join processing;data stream processing;set-valued;data stream;result tuples;data streams;query result;limited resources;quality measure
11;sql-query;optimizer;access methods;16;relational model;efficient computation;window functions;scalability problems;18;access structures
base table;answering queries;specific algorithms;data cube;data structure called
digital content;lower bound;web data;large sets of
sharing information;database;information integration;information sharing;databases;information sharing
data mining application;data loss;linear transformation;database content;real life;sales data;subset selection;relational data
tree structures;querying xml data;join operations;data structures;xml queries;data exchange;index structure;xml documents;growing importance
network bandwidth;data values;real life data sets;xml data;query performance;xml documents;file systems;compression ratio;compressed data;disk space;encoding methods
graph-structured data;semi-structured data;structured documents;path expressions;semi-structured;structural summaries;structural summaries;update operations;xml data;query load
query optimization;xml storage;plans;data interchange;query processing techniques;xml query processing;estimation methods;size estimation
set valued attributes;join algorithms;selection queries;inverted files;query operators;object-oriented;set containment;object-relational dbms;join predicates;set-valued attributes
incomplete information;query evaluation;database technology
error-prone;retrieval precision;time series;high scalability;similarity queries;databases;dimensionality reduction methods
generic model;high-level;xml schemas;relational schemas
problem instances;data values;pair-wise;mutual information;matching techniques;schema matching;instances;data stored in;semi) automatic;matching algorithm;matching problem
heterogeneous information sources;large numbers of;web query interfaces;deep web;schema matching;schema matching;data sources;structured information;web sources;statistical framework;small size
storage utilization;large quantities of;synthetic datasets
data set;data structure;ad-hoc queries;streaming data;data-set
multiple queries;resource consumption;continuous data streams;memory usage;scheduling strategies;sliding-window;query operators;multiple streams;operator;data stream systems;scheduling strategy;queries involving;applications involving
data-item;high-confidence;ip network;hash-based;update streams;data structure;continuous data streams;query classes;application domains;lower bounds;data stream;space usage;general form;estimation algorithms;memory resources;special case;arise naturally in
sql queries;integrating data from;dependency relations;multiple data sources;xml documents;context-dependent;cost-based;xml document;optimization techniques;semantic constraints;specification language;data integration
accessing data;3, 1;xml data;data exchange;active xml;xml documents;real life;web services;xml schema
pre-defined;classification problems;aggregation algorithm;classification;database;efficient similarity search;similarity search;evaluation criteria;high-quality;high probability;euclidean space;random accesses;dimensional data;rank aggregation;efficient similarity search;nearest neighbor;nearest neighbors
data warehouses;data cleaning;real datasets;similarity functions;data quality;similarity function;incoming data;external sources
learning examples;web pages;database;automatically extracting;structured data from;web sites;large number of;large sets of;web pages;human input
wide range;data warehouses;processing algorithms;design choices;scientific data;domain specific;moving target;computing environments;data center;management systems;usage patterns;meta data;data repositories;database schemas;processing power
query optimizers;decision-support applications;complex query;trade-offs
search space;optimizer;plans;query optimizer;execution plan;plan generation;commercial database systems;commercial database
query optimization;optimizer;storage devices;query optimizer;database;query plan;data structures;vector-based;wide range;query plans;query optimizers
concurrency control
database systems;programming languages;query language;database objects;query optimizations;query languages;high-level;formal semantics;programming language
xml documents;large numbers of;total number of;xpath queries;stream processing;optimization techniques
streaming xml data;xpath queries;high throughput;streaming data
window queries;nearest neighbor;query types;analytical models;spatial queries;mobile clients;queries issued;location-based;query processing algorithms;query result
computational geometry;database operations;filtering step;real world datasets;typically performed;require expensive;index structures;refinement step;computational cost;pre-processing;cpu cost
problem instances;data distributions;skyline queries;database;space overhead;skyline computation;duplicate elimination;tree nodes;skyline points;online) algorithms;nearest neighbor search;nearest neighbors;high speed
high-dimensional;multi-level;search space;nearest neighbor;main memory;index structure;desirable property;distance computation;principal component analysis;dimensional data;knn) queries
query optimization;data collection;data acquisition;sensor devices;power consumption;sensor networks;query processing;query processor
correct answers;xpath queries;sensor data is;query response times;distributed queries;xml) data;query-driven;sensor databases;xml document;databases;local database;query result
database;xml publishing;query language;xml documents;xml view;relational databases;additional features;xml query language
required information;query evaluation;service calls;distributed computing;web services;user queries;xml data;active xml;1, 3, 22;data model;cost model;xml documents;replication;web services;higher level;query language;distributed computation
sample selection;approximate answers;approximate query answering;approximate query processing;decision support applications;pre-processing;aggregation queries
network bandwidth;query evaluation;wind speed;classification;database;uncertain data;database queries;moving object;query answers;imprecise data;moving objects;query processing;algorithms for computing;result set;limited resources
continuous queries over;distributed data sources;stream processor;synthetic data;communication overhead;query workload;provide answers;network monitoring;data sources;rates;update streams;distributed data;distributed data streams;continuous queries
density estimation;data points;databases;hardware technology;instances;data stream;rapid rate;data streams;evolving data streams;batch processing;temporal data;transactional data
distance measure;database systems;molecular biology;efficient computation;data sets;application domains;medical imaging;similarity search;similarity queries;meaningful results;efficient similarity search;real world;feature vectors
feature space;complex queries;classification;distance function;image databases;adaptive clustering;relevance feedback;linear transformations;retrieval quality;retrieval method;query expansion;query point;content-based image retrieval
query engines;xml publishing;data model;relational model;operator;query results
query operators;query evaluation;xquery language;xquery expressions;sql query;intermediate results;xml documents;query plans
clustering;clustering;multi-dimensional;db2 universal database;database systems;database;range query;multi-dimensional;multi-dimensional;primary key;data layout;minimal overhead;data warehousing;database operations
multi-channel;plans;database
intrusion detection;database research;applications including;network applications;database;query language;network monitoring;traffic analysis
sql queries;database queries;large databases;db2® universal database;database;domain-specific;temporal databases;programming language
relational query processing;optimizer;xml data;relational tables;cost-based;native xml;query processing;7;databases;relational database
fully operational;ip network;data feed;network management;network management;ad-hoc querying;network traffic;user-friendly;traffic data
data-driven;data analysis;data cleaning;data mining;data quality problems;data quality;case study;quality metrics;databases;data quality;database research;typically requires;metadata management
world wide web;query language;xml query language
data grid;management systems;data grid;storage space;management systems;data grid
1;autonomous systems;large databases
multiple databases;association rules;peculiar data;interesting patterns;database
statistical methods;training data;pruning techniques;classification;activity relationships;highly skewed;classification methods;drug design;data set;predictive performance;chemical compounds;data mining;local learning;class distribution
hash-based;data reduction;data reduction;mathematical model;memory requirement;operator;real data;microsoft sql server;aggregation queries
cost-based optimization;query optimization;optimizer;database systems;query optimizer;query optimization;federated database;relational database systems;trade-offs;query optimization techniques;optimization scheme;design space;data sources;databases;optimization process;optimization techniques;database design
join algorithm;adaptive query processing;object-relational dbms;duplicate elimination;join algorithm
theoretical analysis;query result;range query;tree index
real-world datasets;attribute values;sampling-based;maintenance cost;database applications;sampling-based;range query;high recall;dimensional data;relational dbmss;histogram-based;arise naturally in
range queries over;query patterns;database systems;query optimizer;dynamically changing;data reduction;estimation error;adaptive sampling;user queries;databases;estimation techniques;data distribution
small sample;synthetic data;specific information;xml data;memory space;xml documents;np-hard
storage engine;xml storage;optimizer;heuristic techniques;xml data;xml-schema;relational tables;tree structure;cost-based;cost-based;web applications;relational databases;xml storage;xml schema
web documents;web based;xml documents;topic specific;html documents;keyword based;web data;document transformation
multi-version;multi-version;spatio-temporal;tree-based;query optimization;tree structure;cost models;databases;cost models;indexing methods
high-dimensional;dimensionality reduction technique;similarity search;time-series;similarity search;time-series;efficient similarity search;image compression
database systems;access methods;probabilistic data;object-relational;index structures;monitoring applications;probability distribution;sensor networks;applications involve;data type
minimum number of;uniform distribution;disk access;continuous media;storage capacity;continuous media;general form;low complexity
tree based;search performance;location-based services;database;highly dynamic;tree structure;moving objects;location-based services;indexing technique
metadata management;large number of;replication;relevant information;relational database system;metadata management
business processes;workflow management systems;workflow management systems;business process management
web services;xml documents;web services;dynamic environment;composite services;software components;composite service
model fitting;data points;web server;network management;disk scheduling;data mining;fast algorithms;estimation algorithm
clustering;decision trees;relational database systems;naive bayes;traditional database;internal structure;microsoft sql server;relational data;ad hoc queries
clustering;clustering;attribute values;synthetic data sets;collaborative filtering;applications including;algorithm performs;cluster model;clustering results;data set;special case;cluster models
plans;structural properties;query plans;user query;data sources;data integration;query plans;data integration;data-integration
web object
active rules;xquery update;xml repositories;data repository;semantic constraints;execution model;data management
dimensional space;synthetic data;features extracted from;sign language;similarity functions;indexing structure;nearest neighbor queries;mobile phone;distance functions;efficient approximate;longest common subsequence;similarity measures
clustering;web documents;data analysis;high-quality;large data streams;applications including;algorithm's performance;streaming-data;data streams
high dimensional;real datasets;approximate query answering;reduction techniques;data reduction;dimensional data;dimensional data
distributed queries;operator;main memory;temporal databases;semi-join
indexing schemes;clustering;join processing;large data volumes;join algorithms;link-based;temporal data;times faster than
approximation algorithms;spatial datasets;lower bound;spatial relations;spatial datasets;highly accurate;prior knowledge;wide range;answer queries
frequency counting;data mining algorithms;optimal number of;candidate patterns;frequency counting
high-dimensional;high dimensional;visual representations;closely related;data representation;nearest neighbor search;application domains;theoretical results;data sets;dimensional data;nearest neighbor search;query point;nearest neighbors;nearest neighbor search
geographic regions;distance computation;tabular data;distance function;tabular data;data stores;highly accurate;tabular data;data sets;relational databases;distance computations;distance computations
keyword-based search;web server;keyword-based search;management systems;search engines;relational database;traditional database;relational databases;relational databases;query languages;keyword-based search
duplicate records;public domain;record linkage;knowledge discovery in databases;record linkage;real-world;real data;machine learning;record pairs;data cleaning;data stored in;databases;data cleaning
service provider;interface design;database;database;data privacy;databases;data encryption;data management
web-accessible;web applications;web search engine;target values;search engine;user's location;queries efficiently;databases;web-accessible;relational data
performance bottlenecks;web crawlers;network resources;software architecture;large collections of;search tools;web search engines
xml documents;matching algorithm;publish/subscribe systems;xml data;large-scale;index structure;xml documents;designed to support;wide range;xml document;margins;avoid redundant;xpath expression
selection predicates;query optimizer;optimization strategies;large number of;cost-based;cost models;continuous queries;continuous queries
database;tool called;xml documents;user queries;designed to support;original document;compressed domain;query processing
web-based;object model;information systems;information discovery;processing algorithms;client applications;evaluation model;xml views;source data
selection predicates;structural joins;xml database;worst-case;xml query;xml queries;join processing;counterpart;join algorithms;native xml;tree structured;pattern matching;xml query processing;query engine
graph-structured data;complex queries;semi-structured data;path expressions;semi-structured;structural summaries;exact answers;simple queries;local structure;schema information;query languages;regular path queries;path queries;local similarity
matching algorithm;schema matching;data instances;data structures;data warehousing;algorithm takes;matching algorithm;high-level;user study
web-based;database;multi-tier;case study;multi-tier;user input
data set;data streams;application requirements;applications including;database;database theory;data distributions;data stream;application domains;histogram construction;histogram based;incremental maintenance;data stream;real data sets;data set size;data mining;similarity searching;database management;approximate queries;data arrive;data distribution
collecting data;query throughput;multiple queries;historical information;communication costs;power consumption;traditional database systems;sensor data;traditional database;wireless networks
data cube;data cube;large number of;computation costs;on-line analytical processing;pre-computed;pre-computed;answering queries;real-world;algorithms for computing;memory management
data cube;data warehouses;ad-hoc;olap operations;spatio-temporal;data warehouse;mobile communication;individual objects;data structures;supervision;databases;average number of;spatiotemporal data;spatio-temporal databases
ranking queries;window queries;application area;data warehouse;data warehouse;aggregation functions;processing queries
regular expression;lessons learned;regular expression;text data;matching systems
image retrieval;total number of;auxiliary;approximate matching;search techniques;similarity retrieval;point set;data structures;query execution plans;object boundaries;dimensionality reduction;processing queries
complex queries;web search;keyword-based search;databases;heuristic algorithm;online databases;relational databases;query languages;query results
xml documents;graph-based;semantic similarity;xml documents;information providers;model (called;semantic relationships;information exchange
base table;systems support;database;primary key;replication;commercial database

clustering;clustering;similar objects;distance function;clustering algorithms;spatial clustering;clustering algorithm;real world;spatial data mining
query execution plans;spatial data;synthetic datasets;selectivity estimation;data structures;sampling techniques;spatial joins;spatial database;histogram based;management systems;data sets;spatial joins;estimation techniques;spatial join;range queries
data-warehouses;database;data mining;databases;data representation;traditional database systems;data mining;microsoft sql server
data warehousing;business information;data warehouses
query optimization;optimization strategy;related" queries;summary tables;db2 udb;data warehouse;aggregate queries;query performance;fact table;materialized views;theoretical framework;base data;database;base tables;huge number of
quality-aware;resource consumption;individual queries;similarity queries;plans;query-evaluation;scoring function;similarity queries;image collections;query planning;result quality;approximate results
fine-grained;join algorithm;vector space;database;analysis reveals;data mining algorithms;index structures;point sets;cost model;join algorithms;cost model;similarity join;similarity join
skyline queries;database operations;database systems;data points;skyline operator
mining frequent itemsets;constraint-based mining;large databases;sequential patterns;interesting patterns;frequent itemset mining;frequent itemsets;fp-growth
frequent itemset;database;search strategy;real data;transactional databases;frequent itemsets;effective pruning;transactional database
clustering;synthetic data sets;efficient approximation;detection algorithms;outlier detection;data mining tasks;biased sampling;user requirements;multidimensional datasets;local density;great flexibility;data mining tasks;random sampling
relational database;sql server;database
web-based;application integration;special attention;computing environments;application integration;application integration
web-based;web applications;web service;web services;application developers;application development
multiple queries;nn) queries;nearest neighbor;rnn queries;database;index structure;queries efficiently;index structure;resource management;data set;nearest neighbor queries;nn queries;databases;nearest neighbor;real world;margin;query point
nearest-neighbor search;search algorithm;nearest neighbor;database;search cost;high degree of;retrieval systems;similarity retrieval;similarity retrieval;nn) search;multimedia information;high dimensional space;high dimensional feature space;multi-media
clustering;real data sets;approximate nearest neighbor;multimedia databases;nearest neighbor;cluster-based;approximate nearest neighbor;data set;nearest neighbor queries;query processing;progressive processing;approximate results
dimension hierarchies;rewritten query;data warehouses;rewriting algorithm;materialized view;data warehouses;normal forms;olap queries;dimension hierarchies;materialized views;olap queries;materialized views;queries efficiently;aggregate views;semantic information
data analysis;relational algebra;data cube;relational operators;decision support applications;olap queries;great flexibility;operator;join operator
uniform sampling;sql server;performs poorly;skewed;sampling based;aggregation queries;approximation error;uniform sampling;aggregation queries
concurrency control
query optimization;database;databases
web pages;semi-structured data;business data;database;traditional database systems;higher degree of;database;business applications;business operations;file systems;data stored in
database operations;database operations;database systems;database;database queries;db2 universal database
running times;dynamic programming;plans;query optimizer;communication costs;query processors;semi-join;distributed database systems;semi-join;query plans;query optimizers;distributed systems;semi-join
operator;database server;optimizer;database systems
data structure;hierarchical data;cost-based query optimization;twig queries;limited space;decomposition techniques;query feedback;frequency information;data sets;labeled tree;twig query;estimation algorithms
multi-dimensional;search performance;feature vector;distance function;real-world;sequence databases;similarity search;similarity search;sequence databases;stock data;similar patterns;false dismissal;primary goal;synthetic data
nearest;nearest neighbor;data points;high dimensional;branching factor;high dimensional;similarity search;nearest neighbor queries;nearest neighbors;real data sets
access method;access methods;access methods;synthetic datasets;database size;disk accesses;commercial dbms;similarity search;nearest-neighbor queries;sequential scan;query processing
queries posed;database
query routing;database;query response time;databases;databases;aware query;query routing
access pattern;database management systems;object-relational;access pattern;access patterns;object-level;relational dbmss;context-based;database management systems
web-based
mobile clients;replication;mobile devices
multi-threaded
end-users;systems support;database;persistent database;data access;operator;high availability;persistent database
formally defined;relational models;query language;data model;information systems;conceptual model;time-series;user-friendly;temporal data;model called
point-based;database;databases;operator;wide range;databases;operator
database;index structure called;temporal aggregates;incremental computation;tree index;aggregate views;aggregation queries;temporal databases
data formats;database systems;database;database applications;data types;complex data;database systems
supply chain management;supply chain;business applications;supply chain management;business applications;supply chain management;data repositories
select relevant;query processing;query capabilities;mediator systems;view-based
query execution;mediator systems;parallel processing;distributed architecture;issue queries;processing queries;mediator systems
response times;user interactions;sql queries;data management systems;related data;response times;distributed environments;relational dbms;area network;local context
information systems;information sources;information management;disparate sources;database management system;data entry
response times;database systems;resource consumption;database transactions;serializability;high-level;atomicity;database cluster;data distribution;transaction processing
primary key;relational databases;brute force;temporal databases;primary key
programming model;xml data;object databases;text-based;xml documents;object-oriented;database management system;xml data;programming language
database;database design;data model;object-oriented;designed to support;database management system
data cleaning;data integration;data interchange;integrate data from;data integration
lock;data structures;lock;data structure;programming language
log records;database;shared-memory;main-memory;main memory;disk access;problem domains;disk accesses;database;disk-resident
relational databases;large amounts of data;database
frequent itemset mining;instance;simple algorithm;queries involving;database
support levels;takes into account;real-world applications;data collected from;distributed environments;noisy data;mining temporal;level-wise;structure mining
databases;sequential pattern mining;pattern growth;complete set of;sequence databases;sequential patterns;apriori-based;broad applications;sequence database;mining sequential patterns;data mining problem;sequential pattern mining
database servers;computing devices;data access;data management;area network;mobile users;data access
wide range;application requirements
ibm db;relational database system;increasingly popular

workflow management;application domain;real-world;application scenarios;graph-based;formal semantics
management architecture;process management;multi-agent;business process;agent architecture;workflow systems;workflow engine;business process management
window size;storage overhead;data sequences;database applications;databases;false alarms;storage space;query sequence;matching method;time-series;false dismissal;sliding windows;dual match;query length;times faster than
index structures;time series;linear scan;similar patterns;information loss;indexing technique;total cost
synthetic data sets;search space;temporal association rules;mining temporal;data mining;rule sets;association rule mining;association rules;association rule;numerical attributes;mining patterns;numerical attributes;discovery problem
increasing number of;mobile devices;united states;service providers;databases;database performance
supply chain;web applications;data management
xml element;indexing structure;xml data;index structures;indexing scheme;tree-structured;xml documents;indexing structure;main idea;source data;frequently updated
nearest;nearest;querying xml documents;query formulation;xml query languages;xml documents;tree structure;hierarchical structure;databases;operator;database instance
hierarchical data;source documents;data exchange;web sites;xml documents;detection algorithm;heuristic algorithm;html documents;internal structure;html documents
massive data streams;approximate) answers;data-analysis;continuous data streams;processing algorithms;data-streaming;streaming data;pseudo-random;service providers;usage information;query processor;similarity joins;xml trees
clustering;streaming algorithms;data mining tasks;symbolic representation;classification;anomaly detection;space overhead;data structures and algorithms;real valued;streaming data;time series;distance measures;original data;scale poorly;data mining;lower bound;data mining algorithms;text processing
clustering;sparse matrices;higher quality;incremental learning;data streams;higher quality;clustering data streams;sufficient statistics;data mining problem
frequent itemset;very large datasets;query execution plans;query execution;main-memory;database;join operators;commercial dbms;frequent itemsets;set containment;discovery problem;data mining;commercial database systems;data mining algorithms;database vendors
data accesses;spatial data;data warehouses;data warehouse;survey data;olap operations;online analytical processing;traditional olap;geographical information
clustering;clustering problem;data residing;distance functions;gene expression data;similarity measure;relational dbms;input features;input space;dimensionality reduction techniques;weight vector;efficient sql;high dimensional spaces
ranking measures;graph-based;ranking algorithms;real data;graph--based
tag tree;web information retrieval;web pages;tree structure
clustering;clustering;mining algorithms;classification;anomaly detection;sliding window;time series;streaming data;time series;data mining;complex data;clustering algorithm;rule discovery;mining algorithm;online algorithms
query optimization;mining algorithm;extraction algorithm;database;ad-hoc;database research;time series;feature values;case study;historical data;low overhead;similarity-based;operator;continuous queries;data distribution;size estimation
closed sets;pattern discovery;data sets;microarray data;frequent itemset
context information;training set;amino acid;sliding window;classification;secondary structure;prediction methods;cross validation;vector machine;data sets;protein sequence;protein sequences
classification rules;data mining techniques;homeland security;privacy-preserving;learning models;data collected
inference problem;access control;database;large number of;query response times;query processing;information access;easy access;user query;security level
agent based;service oriented;world wide web;case study;web pages;agent technology;agent-based
world wide web;service oriented;agent technology;case study;web pages;agent technology;agent-based

user-centered;user-centered;information service;user-centered;resource discovery;search engine;user-friendly;internet traffic
increasing number of;database
relational databases;distributed architecture;high probability
reference model;mobile computing;communication networks
database applications;information systems;java-based;relevant objects;database
xml documents;case study;java-based;instance;knowledge engineers

information gathering;response times;retrieve information;regression models;response times

information service;network bandwidth;scheduling strategies;client-site;performance bottlenecks
service provider;world wide web;simulation studies;rates;capacity planning;hit rate
reference collection;machine learning algorithms;classification
distance measure
content analysis;information retrieval
instance;small world;web site
integrating data from;database systems;data integration;database
open-source;database
query processing techniques;data independence;database research;storage systems;database
databases;database;schema-based;databases;design decisions;structured data;data management;design issues
access control;sharing data;peer data management;heterogeneous data;query reformulation;query answering;data management system
multiple databases;databases;database management systems;data integration;global schema
data sharing;data sharing;keyword-based;fine-grained;data management systems;data management system
information dissemination;computational complexity;super-peer;ad-hoc querying
service calls;hierarchical data;massively distributed;query language;service-oriented;xml documents;context-dependent;xml-based;location-dependent;path queries;data management system;metadata management
database systems;query workload;storage model;reconstruction algorithms;storage manager;high cost;ad hoc
approximation algorithms;search space;multidimensional data;synthetic data sets;regular expressions;database;index structure;sampling-based;large databases;tree nodes;spatial locality;query string;databases;tree-based;application scenarios
monitoring applications;application area;data stream management;data streams;monitoring applications;data processing;human operators
query results;emerging applications;continuous queries;streaming data;query-processing techniques;data streams;ad hoc;ad hoc queries
real-world applications;original data;database relations;desirable properties;high probability;relational data
web directories;web pages;naive bayes classifiers;decision tree;text classification tasks;cache performance;test instances;naive bayes;instances;data sets;training instances;statistical pattern recognition;memory requirement;sequential scans;text classification;support vector machines;low-dimensional;classification algorithm
hill-climbing;total number of;heuristic algorithm;disk accesses;object oriented databases;instance;partitioning scheme;cost model;object-oriented databases
decision tree;instance;information integration;sufficient condition for;conjunctive queries;access patterns
public domain;erroneous data;attribute values;instance-based;classification;instances;heuristic rules;data instances;instance;data sets;schema information;information derived from;identification method;database integration
database size;index structures;inverted files;data items;fast retrieval;efficient retrieval of;domain size;set-valued attributes;data distribution
window size;database;index structure called;temporal aggregates;incremental computation;tree index;aggregate views;aggregation queries;temporal databases
web usage mining;statistical analysis
production line;sensitivity analysis;neural network;production data;data mining;neural network
common patterns;case study;distance based;similar patterns;analyzing data;interesting patterns
text mining;text mining
data set;data sets;neural networks
noise handling;classification accuracy;noise handling;data set;case study;genomic data
dna sequences;tree-structured;structural relationships;data sets;expression levels;discovery problem
accuracies;confidence level;database
decision makers;vector machine;fold cross-validation;logistic regression
semantic networks;text mining;mining algorithms;knowledge discovery;text documents;knowledge discovery;network structure;text content;knowledge base;mining tasks
association rule mining;actionable knowledge;transactional databases;data mining
text mining;occurrence frequency;web pages;takes into account;word senses;mutually exclusive;underlying assumption;information retrieval
text filtering;correlation coefficient;chi-square;feature selection;text filtering
interaction data;unlabeled data;semi-supervised learning;multi-relational;information extraction;data sources;problem setting
total number of;training data;learning systems;continuous attributes
loss function;svm-based;svm based;kernel functions;kernel function;vector machine;rates;prediction error;rates;regularization parameter;neural network
association rules;case study
data mining technique;significant patterns;data stream;usage patterns;prediction algorithms;discovery problem
nearest neighbor classifier;nn search;decision tree;nearest neighbor;decision tree
unstructured text;semantic structure;classification problem;labeled training;huge amounts of
association rules;association rules;interesting rules;decision rules
discriminant analysis;discriminant analysis;multi-class classification;benchmark datasets
misclassification costs;class distributions;learning algorithm;model trained;algorithm produces;class distribution;classifier
tree structures;pattern recognition;frequently occurring;synthetic data;database;chemical compounds;frequent subtrees;free trees;indexing technique;free trees
multiple classes;feature space;unlabeled data;classification;accurate classification;single-class;positive class
clustering;clustering;training set;predictive accuracy;classification;input space;real-world domains;linear classifiers;clustering algorithm;pre-processing;low-variance
instance;feature values;knowledge discovery;feature selection;heuristic algorithm;feature selection;active sampling;standard database
clustering;clustering;genetic algorithms;multi-objective;multi-objective;large number of;interesting association rules;genetic algorithms;association rules mining;large itemsets
high-dimensional;training data;classification;uci data sets;medical data;classification performance;decision trees;gene expression
bayesian classifiers;data characteristics;predictive models;relational data
fast algorithm;close connection;emerging patterns;emerging patterns;large datasets;data mining
classification trees;real data;tree-structured;large data sets;nodes represent;curve evolution;clustering algorithm;margin
data-mining approach
clustering;clustering;relative entropy;mutual information;information theory;naive bayes;optimization problem;local search;dimensional data;local minima;information theoretic
transaction data;aspect model;collaborative filtering;real-world;large margin;graphical model;association rules;association rule
vector machine;accurate classifiers;classifier;linear classifiers
markov chains;closely related;statistical technique;markov chain
kernel machines;gaussian kernels;generalization performance;higher dimensional;kernel matrices;vector machine;exact computation;higher dimensional;algorithm to compute
high-dimensional datasets;data record;data visualization;high dimensional;high dimensional;data set;case study;dimensional data
density-based clustering;spatial datasets;prohibitively expensive;data structures;data sets;data mining approach
frequent subgraph mining;vertical search;frequent subgraphs;real datasets;data mining;subgraph mining;databases;mining patterns;frequent subgraph
mining frequent itemsets;communication overhead;database;skewed;frequent itemset mining;frequent itemsets;computational resources;dynamic databases
detection algorithms;attribute values;outlier detection;real-world;data set;spatial outliers;accurately detect;spatial patterns
world-wide web;information sources;naive bayes;web sites;naive bayes;topic hierarchy;data set;data sources;topic hierarchy
clustering;clustering algorithms;clustering results;clustering validation;domain experts;skewed;rates;main idea
bayesian network structure;large-scale datasets;large networks;data sets;bayesian networks;network structure
underlying structure;detection algorithm;probabilistic model;prior knowledge;large data sets;data sets
customer relationship management;data mining algorithms;decision trees;resource constraints;objective function;actionable knowledge
2;nearest neighbor algorithm;nearest neighbor;fuzzy set;remote sensing;multi-channel;pattern classification;evidence theory;3;1;4;contextual information;classifier;nearest neighbors;training sample
statistical significance;base classifier;classifier;decision-tree
semi structured;web content;web site;web usage;web log
dimensional data;data sets;noise levels;dimensional data
pattern discovery;databases;concept hierarchy;decision processes
nearest-neighbor rule;discriminant analysis;classification;classification performance;competing methods;classification problems;dimensionality reduction
real-world datasets;auc;decision trees;predictive accuracy;classification;data mining applications;learning algorithms;naive bayes;evaluation criterion;data mining;data mining algorithms
data cube;united states;knowledge discovery;association rules mining;association rules mining;fuzzy association rules
clustering;clustering;text document;text documents;text document;clustering techniques;clustering methods
classification;irrelevant documents;instance;search engines;association rule;class labels
indexing method;nn) search;nearest neighbors;nearest neighbors;large data sets
clustering;nearest neighbor;clustering algorithm;nearest neighbor;clustering algorithms
human-centered;decision-making;mining association rules;rule sets;enormous amounts of;association rule;rule discovery;data mining algorithms
high quality;collaborative filtering;collaborative filtering;privacy-preserving
auc;genetic algorithm;misclassification costs;sensitivity analysis;noisy data;data mining;sensitivity analysis
clustering algorithms;mining frequent itemsets;high dimensional;pattern based;projected clustering;real data;frequent itemsets;clustering techniques;irrelevant attributes;projected clusters
association rule mining;pattern-growth;correlated patterns
databases;time series;time series;arbitrary length;databases
intrusion detection;learning rules;anomaly detection;algorithm called;long range;time-series;network traffic
clustering;clustering;efficient clustering;data clustering;real dataset;data sets;prior works;clustering algorithm;clustering quality
association rule mining;data structure;input data
association rule mining;rough set approach;rough set theory;data mining;association rules;rough set approach;transaction processing
high-dimensional;singular value decomposition;discriminant analysis;sample size;linear discriminant analysis;data dimension;text data;computational complexity;optimization criterion;special case;structure-preserving
level-wise;apriori algorithm;frequent pattern mining
natural language processing techniques;text documents;digital camera;product review;pattern database;topic specific;natural language processing
clustering;real-world;objective function;variable selection
statistical methods;clustering;synthetic data sets;data visualization;database;large databases;data sets;databases;visualization techniques;link analysis;image processing
base learners;ensemble method
maximum entropy;markov models;real-world data sets;maximum entropy;long-term;em algorithm;higher order
clustering;similar objects;data sets;expression levels;high dimensional space;similarity measures
unlabeled examples;classifier;unlabeled examples;text classifiers
log analysis;search engine;user query;log analysis;behavior model;semantic relations;search logs
real world data mining applications;incomplete data;mutual information;learning bayesian networks;learning algorithm;bayesian networks;joint probability;missing data;incomplete data
transaction data;pattern-based;pattern-based;behavioral patterns;clustering approaches;user-centric;web usage data;mixture models;clustering approach;clustering algorithms
predictive modeling;model training;prediction models
inductive learning;loss function;decision tree;meta-learning;multiple models;memory requirement;np-hard
time-series;visualization methods;wide range;test data;mixture model
high utility;plans;plans;algorithm produces;databases;high frequency;data mining algorithms;ai planning;markov decision process;high-utility
distributed algorithm for;association rule mining;mining association rules;database
low-level features;feature vectors;raw data
graph structures;significant rules;real world datasets;frequent itemsets;labeled graphs;knowledge representation;complex patterns;upper bound;class labels;transactional data
high utility;high utility;association rule mining algorithms;closed patterns;large number of;pruning strategy;data mining;level-wise

model construction;discovery algorithms;chemical compounds;classifier;classification
association-rule mining algorithms;pruning strategy;association patterns;data sets;skewed;data sets;support levels;skewed;high dimensional space;clustering
pattern sets;query evaluation;version spaces;special attention;query plans
clustering;clustering;training data;data samples;labeled data is;semi-supervised learning methods;labeled data;labeled dataset;text classification
clustering;clustering;mining algorithms;anomaly detection;sliding window;time series;streaming data;time series;time series;data mining;complex data;mining algorithm;online algorithms
data cleaning;probabilistic model;real world;generative model;classifier
expected number of;event sequences;reliable detection;event sequences;false alarm;fundamental problem;false alarms
clustering;clustering problem;mutual information;clustering algorithms;intra-class;multiple clusterings;data set;clustering algorithm;combining multiple
data values;additive noise;data perturbation;randomized data;data mining techniques;data mining applications;sensitive data;original data;privacy preserving;data privacy;privacy-preserving
data objects;clustering algorithms;posterior probabilities;minimum number of;information-theoretic;text data;clustering framework;model-based clustering;clustering
association rule mining;data mining;association rule;association rules;heuristic approach;np-hard
tree induction;rule set;markov blanket;specialized algorithms;data sets;decision rule;target variable;feature selection
micro-array data;pattern-based;fast algorithm;pattern-based clustering;synthetic data sets;pattern-based clustering;clustering methods;large databases;pattern-based clustering;databases;mining algorithm;real datasets
synthetic data sets;labeled trees;tree mining;tree structure;frequent subtrees;data mining;frequent induced
spatial relationships;complex relationships;spatial data
clustering;em) algorithm;model-based clustering;data summarization;large databases;data set;computational resources;data mining;em algorithm;model-based clustering;expectation-maximization
search space;mining algorithms;logistic regression;relational structures;real-world;classification;statistical relational learning;statistical modeling;relational data;classifier;model selection
minimum support threshold;closed patterns;database;sequential patterns;closed sequential patterns;data mining;closed sequential patterns;frequent patterns;mining algorithm;sequential pattern mining
text corpus;multi-dimensional;data collection;hierarchical) data;data collections;collection size;data mining;interactive visualization;interactive visual
data point;domain experts;data mining and knowledge discovery;rules generated;data sets;large datasets;high similarity
computational efficiency;multi-attribute;data extraction;learning algorithms;data extraction;precision/recall;machine learning techniques
global model;digital library;mixture model;user behavior;user behavior;predictive models;behavior patterns;maximum entropy;behavior model;web users
learning process;improving accuracy;density estimation;unlabeled data;classification;database;data mining;cost-effective;real-life;prediction problems;data mining;labeled data from;unlabeled data
computational complexity;event sequences;sequence database;application domains;interesting patterns;pattern discovery;pattern discovery;event sequences
classification;input data;kernel functions;kernel function;kernel functions;dimension reduction;computational savings
genetic algorithms;pattern classification;vehicle detection;incremental clustering;vehicle detection;support vector machines;similar characteristics
generation algorithm;rule set;generation algorithm;rule generation;rule generation;interesting rules;generation process;association rule
database;relational model;data-mining problem;databases;data-mining problems;discovery problem
high-dimensional;classification model;hierarchical approach;target variable;hierarchical clustering;data records;data sets;target variables;clustering algorithm;target variables;clustering approach;prediction models
classification;cost-sensitive learning;learning algorithms;cost-sensitive;predictive performance;classification algorithm
database systems;data updates;database;association rule mining;computing environments;network topology;grid computing;association rule mining;information gathered;communication overhead;data mining problem;computing systems
frequent patterns
real-world;random sample;performance degradation;association rules;theoretical bounds;real world data sets
clustering;clustering;global model;monte carlo;vice versa;communication costs;high quality;generative models;data types;privacy-preserving distributed;perturbed data;higher quality;semi-supervised;privacy requirements;information theoretic
clickstream data;data points;unsupervised learning;data streams;computational costs;data sets;user profiles;data mining;mining data streams;clustering approach
clustering;hybrid model;hybrid model;large number of;hierarchical clustering;text clustering;data sets;clustering
instances;link discovery;link discovery;knowledge discovery;finding patterns;real-world;relational data
semantic web;ontology-based;software systems
error-prone;web scale;matching process;domain constraints;matching accuracy;information processing;data instances;multiple types of;semantic web;ontology mapping;learning strategies;machine learning techniques;semantic mappings;real-world domains;highly accurate;similarity measures
search results;web documents;taking into account;classification;semantic level;effective search;clustering process;document collections;similarity measure between;clustering algorithm;clustering
error-prone;ontology-based;semantic features;web service composition;high-level;semantic web;web services;composite services;ad hoc;low-level
activity models;domain-specific;higher degree of;web services
classification;selection process;data mining;citation networks;semi-structured data;clustering;international conference on;knowledge discovery;acm sigkdd international conference on;large number of;kdd community;technical program;vice-versa;data streams;knowledge discovery;learning theory;data mining;temporal data;data cleaning;machine learning;frequent sets;knowledge discovery;program committee;acm sigkdd international conference on
end-user;distance functions;distance function;distance function;data mining applications;collect data;user requirements;data type;data mining algorithms
high-dimensional;von mises;cosine similarity;gene-expression data;high dimensional;special case;expectation maximization;information retrieval;high-dimensional spaces;gaussian distribution;text data;clustering tasks;model-based clustering;clustering algorithms
average case;data set;high-dimensional data sets;distance-based;fast algorithms;worst case
duplicate records;distance functions;duplicate detection;distance metrics;database;data cleaning;vector-space;detection accuracy;textual similarity;edit distance;vector machine;databases;data integration;similarity measures
real data sets;pattern discovery;statistical framework;large data sets;discover patterns;data mining;pattern discovery;hypothesis-testing;hypothesis testing;data storage
contingency table;small sample;fast algorithm;approximation methods;database;association rule mining;problems require;fixed-length;data reduction;computationally intensive;random sample;contingency-table;association-rule discovery;data-reduction;random sampling
database;condensed representation;version spaces;supervised classification;visualization tool;16;data cubes;operator
clustering;em) algorithm;gene expression data;predictive power;small sample sizes;trajectory data;data sets;translation-invariant;real-world data sets;mixture models;estimation methods;expectation-maximization
clustering;contingency table;data analysis;mutual information;random variables;clustering problem;information-theoretic;information theory;market-basket;mutual information between;joint probability distribution;document clustering;optimization problem;high-dimensionality;web-log;clustering algorithm
user profile;mining process;usage data;web content;web site;web usage;web site's;web personalization
data structure;fp-tree;association rule mining algorithms;frequent patterns;database;frequent items;large number of;main memory;memory requirements;mining process;transactional databases;association rule mining;data structures;algorithm called;efficient discovery of;large datasets;random access;memory resources;disk-based;knowledge discovery process;transactional data
world wide web;data mining techniques;hidden variables;multi-strategy;predictive model;mining algorithm
high-dimensional;real data sets;data collections;database;partial order;observed data;data set;data sets;noise levels
greedy strategy;approximation algorithms;marketing strategies;social network analysis;influential nodes;social network;optimization problem;approximation guarantees;social networks;game-theoretic;performance guarantees;np-hard
high-dimensional;data analysis;high dimensional;association rule mining;computationally expensive;large datasets;error-bounded;desirable properties;compressed data;correlated patterns
cluster structure;feature extraction;brute force;feature selection;data mining projects;dimensional data;visualization techniques
increasing complexity;generalization performance;related entities;aggregation methods;vector-based;relational schema;concept classes;relational learning;classification problems;relational data;prediction tasks
text mining;classification;vice versa;semi-supervised learning;accurate classification;label sets;training documents;classifier
human users;domain experts;sensor readings;data analysis;time series;time-series
clustering;feature space;mixture components;bayesian information criterion;mixture model;clustering method;common sense;model based clustering;clustering algorithm;model-based clustering
clustering;security concerns;privacy-preserving;knowledge discovery;data mining projects;vertically partitioned data
multi-dimensional;video surveillance;distance function;similarity measure;index structure;data mining research;distance measures;noisy data;efficient retrieval;time-series;precision/recall;real world applications;similarity measures;longest common subsequence;increasingly popular
prediction accuracy;classification;classification models;test data;applications including;network intrusion detection;classification accuracy;ensemble classifiers;streaming data;knowledge discovery;data stream;wide range;data streams;credit card fraud;ensemble framework;ensemble approach;single-classifier;mining data streams
real data sets;tree-structure;itemset mining;pattern analysis;database;memory usage;closed itemsets;mining frequent;mining frequent;data structures;mining algorithms;search strategies;frequent closed;depth-first search
interesting rules;prior knowledge
data analysis;contrast-set mining;data mining technique;rule-discovery;special case;contrast-set-mining
graph theory;biological networks;markov models;web graphs;graph analysis;social networks;data analysis;real-world networks
synthetic data sets;association patterns;theoretical results;data set;large data sets;log-linear;data mining;association rules;higher order
data structure;mining frequent itemsets;database;pruning methods;closed frequent;graph patterns;frequent subgraphs;pattern discovery;mining algorithm
clustering;mining results;web pages;classification;information contained in;data mining tasks;tree structure;web site;user access;data mining;web page
cost sensitive classification;classification;classification;rule based classification;xml data;classification methods;xml documents;data mining;classifier;semi-structured data
association mining;mining algorithms;frequent patterns;intermediate results;data representation;frequency counting
burst detection;data structure;large-scale;burst detection;window sizes;data streams;sliding windows;direct computation
web pages
real-world;web environment;data mining;discovering patterns;databases;event data
long-term;data analysis;short-term;machine learning methods;database
sufficient conditions for;data mining systems;simulation model;national security
randomly generated;considerable effort;general-purpose;partial differential equations;algorithm called;data mining approach;input-output;data mining;data mining models;data mining algorithms
expected number of;specific information;probabilistic models;predictive modeling;naive bayes;decision-tree;linear regression;data sets;rates;model parameters;response variable;probability model
clustering;classification;data analysis;gene expression data;microarray data;microarray datasets;multi-class classification;case studies;microarray data;data mining;pre-processing;gene expression;gene selection
free text;clinical data;clinical information;automatically created;medical applications;knowledge discovery;high-quality;bayesian framework
network models;event logs;classification techniques;large-scale;high degree of;event data;filtering technique;rule-based;historical data;rule-based;time-series;prediction algorithms;computing systems;classification algorithms
support vector machines;frequent patterns;data mining research;association rule;drug design;classifier;classification algorithms
clustering;principal components analysis;cluster based;singular value decomposition;predictive power;time series
data stored;database;decision rules;knowledge-base;knowledge-based;machine learning;data mining;knowledge base
end-users;classification methods;core components;classifier;classification algorithm
learn models;classification accuracy;knowledge-management;vector representation;information retrieval
clustering algorithms;gene expression data;data points;high dimensional;gene-expression;high dimensional data sets;statistical techniques;observed data;data set;statistical inference;dimensional data;gene expression;classifier
mining algorithms;data mining method;memory usage;valuable information;data elements;frequent itemsets;knowledge embedded in;data stream;rapid rate;data streams;optimization techniques
pattern discovery;discovery algorithm;data mining problems;time series;high probability
single class;automated methods;extraction methods;classification task
data mining and knowledge discovery;decision tree;randomized response;high accuracy;data mining;privacy-preserving data mining
data set;data sets;continuous variables;random sample;weighting schemes;statistical model;model-free;principal components;4;statistical modeling;massive data sets;model free
random projection;machine learning methods;random projections;machine learning;random projections;supervised learning;dimensionality reduction
decision trees;classification techniques;decision tree;continuous data;learning algorithms;data sets;data streams;data streams;high-speed data streams;mining data streams;tree models
singular value decomposition;multiple data streams;data mining problems;continuous data streams;data mining algorithms;multiple streams;data reduction;data items;approximate query answering;data sets;data stream;data stream mining;data streams;synthetic data sets;streaming environments;dimensionality reduction
pair-wise;web-based;cluster based;web pages;prediction model;user session;markov models;click-stream;sequence mining;similarity measure;user behavior;sequential pattern;data set;user sessions;frequent sequences;web page;sequential pattern mining;higher order;large number of
input data;database;large-scale;clustering techniques;clustering algorithm;citation graph;community structure
clustering;feature extraction;large data sets;similarity search;text databases;confidence measure;massive data sets;efficient approximate
high-dimensional;database;correlation analysis;search strategies;human interaction;result quality
real data sets;information needed;expression patterns;gene expression data;interactive exploration;large data sets;data set;tree structure;gene expression data sets;time-series;interactive exploration;biomedical applications;bioinformatics research
sample size;decision tree construction;decision tree construction;streaming data;data instances;data mining;numerical attributes;execution times
web documents;partial information;efficient storage;data set;query optimization;tree model;xml documents;relevant data;computationally expensive;document collections;structural information;structural information;similarity measures;tree models;structural similarity
users' preferences;collaborative filtering;recommender systems;user preference;ranking method;individual preferences
hierarchical clustering;low-dimensional;single image
classification;models learned;generalization error;ad-hoc;large-scale data mining;individual classifiers
web pages;matching algorithm;accuracies;extract information from;data records;essential information;structured objects
data structure;mining frequent patterns;minimum support;frequent patterns;pattern tree;related algorithms;transactional databases;building blocks for;tree construction;very large datasets;patterns discovered;disk-based;frequent pattern;mining tasks;frequent pattern mining
temporal sequence;temporal sequences;regression algorithm;detection algorithm;novelty detection;support vector;real world
general form;risk management;predicting future;sufficient condition for;raw data;probability distribution;normal distribution;information gain;probability distributions;information loss
training data;classification trees;machine learning and data mining;learning algorithms;data instances;instances;learning tasks;aggregation functions;probability estimation;relational data
network intrusion;graph-based data;anomaly detection;graph-based;real-world;network intrusion detection;graph-based;anomaly detection
specially designed;discovery algorithms;closed patterns;gene expression;large number of;biological datasets;algorithm called;bioinformatics datasets;pattern mining algorithms;frequent pattern
clustering;cluster based;numerical experiments;large volumes of;initial conditions;rule-based;algorithm requires;large datasets;very large datasets;high density;clustering algorithm
data mining algorithms;data mining;temporal locality;working set;spatial locality
clustering;mining results;complete set of;gene expression data;synthetic data sets;statistics-based;real-world applications;broad applications;real-world data sets;heuristic search
iterative process;association rule mining;weighted association rule mining;algorithm called;weighted association rule mining;large itemsets
decision trees;classification;decision tree;classification methods;valuable information;knowledge discovery;decision trees;additional knowledge
bayesian network learning;classification;markov blanket;variable selection;target variable;training instances;data mining;causal structure;efficient discovery of;graph connectivity;local region
redundant features;correlation-based;irrelevant features;real-world;feature selection;data mining;dimensional data;pre-processing;feature selection
nearest-neighbor search;feature space;large parts;data set;nearest neighbors;nearest neighbor search
simulation data;data mining approaches;decision support;input parameters;knowledge discovery;bayesian networks;data base;bayesian networks;rule discovery
knowledge engineering;domain knowledge;data mining;data quality;case study;real-life;business operations;production rules;business rules;databases;change frequently;data quality
text mining;similarity analysis;semi-structured;similarity analysis;knowledge engineers;text documents;multiple sources
data set;statistical model;statistical models;data mining techniques
intrusion detection;anomaly detection;intrusion detection system;network security;intrusion detection
data-driven;domain experts;event logs;data-driven;high quality;design process
visualization technique;original data;power demand;feature values
text mining;single item;multiple categories;database;association rule mining;word pairs;data set;essential information;sequential data;text database;association rules;selection criterion;association rule;dependency structure;meaningful information;multiple aspects
link discovery;link information;model generation;correlation analysis;data mining
user queries;query expansion;text retrieval;ranked retrieval;ir research;highly ranked;search effectiveness;expansion terms;initial retrieval;query expansion;web search engines;web retrieval
parameter tuning;training data;ad-hoc;information retrieval;document frequency;document collections;term frequency;precision/recall;query terms
relevant documents;automatic evaluation;relevance judgments;ir systems;web queries;web search engines
high-dimensional;query execution;multi-dimensional;similarity searches;answer set;index structures;query points;nearest neighbors;real datasets;databases;sequential scan;feature vectors;result quality;times faster than
retrieval strategies;replication;data availability;takes into account;data blocks;scheduling algorithm;replication;data placement;multidimensional data;simulation results;range queries
simulation data;user requirements;physical characteristics;wavelet decomposition;large scale;scientific data;data compression;modeling technique;multi-resolution;data-sets;change detection;ad-hoc querying;compressed representation;clustering problems
image descriptors;database systems;user interfaces;retrieved images;information visualization;image similarity;visualization techniques;query results;image processing;database researchers;content-based image retrieval
directed acyclic graphs;network structures;communication patterns;software tools;communication patterns
music information retrieval;databases
image retrieval;multi-step;distance function;database;image database;region-based;image collections;retrieval performance;region-based;databases;distance functions;content-based image retrieval
result sets;user interfaces;information retrieval;user interface;short term;document databases
multi-dimensional;optimization problems;estimation technique;wide range;databases;distance metric;computational overhead;data distribution
rnn) queries;nearest neighbor;rnn queries;high dimensional;high dimensional;profile based;range query;data streaming;decision support systems;nearest neighbor queries;range queries;dimensional data;wide range;query point;strong correlation;document databases
approximate answers;shape information;dimensionality reduction technique;vector space;lower dimensional;high dimensional;database applications;high dimensional data sets;compression technique;data set;dimensionality reduction
semantic concept;web pages;classification;web sites;classification performance;instance;semantic concepts;web site;web page;classification task
frequently occurring;real-world;massive amounts of;natural language processing;semi\-structured;question answering;user queries;providing users;web sources;question answering;web data;natural language
structural relationships;information retrieval applications;document structure;multi-document;binary classifier;related documents;multi-class
training set;hidden markov models;test collections;hidden markov models;linguistic knowledge
transliteration;cross language information retrieval;hand-crafted;named entities;translation model;cross language;source language;statistical technique;test set;linguistic knowledge
information retrieval research;retrieval effectiveness;cross language;cross-language retrieval;language pairs;test collections
indexing schemes;multi-channel;wide range;data access;data dissemination;access structures
mobile ad-hoc networks;ad-hoc;wireless communication;mobile environment;data allocation;data items;theoretical results;mobile ad-hoc network;network traffic
lessons learned;database;1,2;machine learning techniques
tool called;query language;data model;business intelligence;complex patterns;data processing
response times;database systems;poor performance;database applications;rates;xml data;transactional database;database performance
web-based;business objects
user activity;user profile;information retrieval systems;information retrieval
classification model;information filtering;sliding window;benchmark data sets;information retrieval;relevance feedback;margin;margin-based;adaptive filtering;retrieved documents;margin-based;classifier;positive examples
large-scale;document retrieval;content-based retrieval;resource selection;digital library;digital libraries;content-based retrieval
node failures;document collection;query performance;storage management;information retrieval systems;load balancing
information sources;knowledge management;data warehouse;information retrieval;user interface;document management;data stored in;semantic web technologies;providing users;relevant information;related documents;user context;unstructured) information;efficient access to
web pages;missing) data;data extraction;large number of;web pages;data stored in;np-complete;databases;web page;quality measure;giving rise to
feature space;training data;training data is;17;unlabeled examples;generalization performance;vector machine;large number of;information retrieval;text classification;unlabeled documents;binary classifier;text classification;labeled examples;text classification;high dimensional feature space;support vector;positive examples
classification problem;classification;competing methods;real-world;classification methods;instance;numerical features;classification method;text classification;classification problems
parameter free;nearest;parameter-free;information retrieval;generalization error;algorithm parameters;misclassification costs;cross validation;learning algorithms;vector machine;margin based;upper bound;text classification;support vector machines;margin
world wide web;high accuracy;multi-resolution;natural language
human-generated;overlapping clusters;hierarchical clustering;hierarchical structure;detection" task;broadcast news;clustering task;topic detection;evaluation techniques;false alarms
continuous data streams;relational operators;streaming data;stream processing;probability distribution;query processing;data processing;data streams
xml streams;execution plan;token-based;query plans;xml stream;abstraction levels
frequent items;bounded memory;data stream;memory space;dynamic environment
hierarchical organization;unsupervised classification;document classification;knowledge management;machine learning;unlabeled documents;learning models;labeled examples;supervised learning;classifier;semi-automatic
semantic features;unstructured text;support vector machines;local features;entity recognition
data sets;user's preferences;collaborative filtering;collaborative filtering
binary classification;singular value decomposition;discriminant analysis;classification problem;classification tasks;text categorization;text categorization;support vector machines;multi-class classification;text classification;classification problems;multi-class;large-margin;classification problems
search engines;web pages;web search engine;web queries;machine learning;houses for sale;wildflowers;real estate;large sample;web page;search engine users;result quality;search engine queries
training phase;category-based;main-memory;learning algorithm;index construction;support vector machines
xpath expression;traffic control;high throughput;memory usage;xml streams;large number of;stream processing;xpath processing;xml stream;deterministic finite automata
xml data;query languages;query optimization
hidden web;path expressions;xml data;xpath queries;xml queries;xml view;queries issued;data sources;xml views;xml document;databases;data source
semantic interoperability;knowledge bases;information technology;databases
highly heterogeneous;spatial data;query language;database integration;data representations;query languages;data integration;geographic information systems
latent variable model;probabilistic latent semantic analysis;document collection;vector space;text documents;user's interests;information access;search engine;clustering algorithm
search systems;search results;web search;user's perspective;reinforcement learning;user queries;search engines;topic-specific;web search;search services
workflow management system;biological data
information extraction from;knowledge discovery;approximate matching;data mining
high-dimensional;gene expression data;high-dimensional data sets;data sets;real-world data sets;biomedical research;gene expression
query evaluation;total number of;taking into account;partial information;efficient query evaluation;query term;efficient query evaluation;query independent;promising candidates;pruning techniques;trec web track
search systems;theoretical framework;6, 7;1, 2, 3;replication;3;5;4;query processing
search results;domain experts;completeness;duplicate detection;document detection;million documents;document collections;feature selection;collection statistics;multiple domains;test collection
vertex set;query response times;large graphs;graph indexing;tree index;data distribution;web data;traffic analysis
incremental algorithm;database;large databases;visual exploration of
language model;ad-hoc queries;heuristic techniques;language models;relevance models;query-likelihood
trec test collections;index terms;language modeling approach;query likelihood;ir) methods;statistical language models;information retrieval;user queries;retrieval performance;language models;language models;natural language processing;information seeking;user query;syntactic structure
trec data;high-quality;relevant documents;relevance judgments;retrieval systems;document collections
search results;information sources;knowledge management;clustering techniques;databases;information overload;multiple information sources;web search engines
database;unstructured text;programming languages;data sources;information retrieval;data set;relational model;ranking algorithms;relational database;application code;unstructured data;document databases
highly-accurate;highly accurate;information retrieval;data sets;text retrieval;retrieval engine
provide evidence;7;user evaluation;adaptive approach
takes into account;rates;multi-document summarization
web documents;web page;web sites;unstructured data;retrieval precision
web pages;information sharing;dynamically generated;web sites;web site;error prone
storage structure;dynamic nature of;xml documents;xml indexing;multi-axis;storage structure
web portal;end users;knowledge management;management systems;text classifier;similar documents;fully automatic;text classification
ranking algorithm;algorithm performs;communication patterns;graph-based;information retrieval;social networks
text based;data cube;multi-dimensional data;intermediate result;visual interface;dimension hierarchies;helping users;visual interfaces;web log;query refinement
approximation algorithms;mining algorithms;valuable information;frequent itemsets;knowledge embedded in;data stream;window-based;data streams
structured document retrieval;relevant document;small-scale;user study;starting-points
similar queries;specific domains;real-world;image data;topic hierarchy;search engine logs;visual information;web images;users' search;web-image;topic hierarchies;web images;automatically generating
information filtering;data clustering;relevance judgments;user interests;pseudo-relevance feedback;keeping track of;reuters-21578 corpus
context-dependent
link prediction;social network;link prediction problem;network topology;social networks
test set;relevant documents;anchor text;query feedback;relevance feedback;relevant document;retrieval task;query-likelihood;statistical models;language modeling
ontology-based;ontology-driven;semantic web;semantic search;knowledge discovery;semantic web
search results;classification technique;web pages;concept hierarchies;concept hierarchy;search engines;web search results;automatic generation of;user study
retrieval engine;document collection;semantic network;retrieval framework
information space;fully connected;information processing
database;low-cost;database research;low-power;sensor networks;sensor network
data loss;data stored;database;sensor data is;data mining applications;streaming data;problems arise;stored data;sensor data
sensor networks;data management;sensor network;higher level
network bandwidth;sensor readings;evaluating queries;database;data uncertainty;computation costs;probabilistic threshold;moving objects;actual values;query answer;answer queries;limited resources;sensor data
data mining;sensor network;databases;united states;knowledge discovery;sensor networks;mobile phone;water quality;finding patterns;temporal data;decision making
database;energy-efficient;database;space constraints;sensor networks;query processing;data dissemination;high-level
sensor nodes;traffic load;sensor node;existing protocols;sensor networks;rates;sensor networks;simulation results
minimum spanning tree;aggregation techniques;data fusion;low-cost;simulation results;data gathering;user queries;power consumption;sensor networks;sensor network;sensor nodes;network lifetime;data gathering
large-scale;large scale;sensor node;high quality;sensor networks;limited number of;sensor network;network structure;sensor nodes
sensor networks;sensor networks;sensor network;streaming data;distributed environment
raw data;view-based;materialized views;conjunctive queries;operator;relational model;regular path queries;complex data;query answering;semistructured data;regular-path queries;complex queries;view based;nodes represent;relational data;information systems;database;query language;edges represent;query processing;databases;query containment;processing queries
software development;software systems;agent-based
knowledge discovery;clinical data;knowledge base;data mining and knowledge discovery;data mining techniques
multi-relational data mining;association rule discovery;data mining approaches;multi-relational;multi-relational data mining;decision trees;data mining;relational database;data mining algorithms;distance-based
data mining approaches;multi-relational data mining;theoretical results;multi-relational;data mining
probabilistic logic;probabilistic logic;probabilistic reasoning;machine learning
pre-processing;support vector machines;kernel methods;learning tasks;instances;vector space;positive definite;kernel methods;structured data
data structures;graph based data mining;structured data;graph-based data mining;theoretical basis
multi-relational data mining;data mining techniques;data types;multi-relational;relational structure;biological databases;biological data
multi-relational data mining
data mining tasks;data mining;social networks;statistical models;link mining
logic-based;pattern language;data mining techniques;graph-based;graph-theoretic;data preparation;inductive logic programming;graph-based;relational learning;graph-based data mining;frequent subgraph
user browsing;extraction algorithm;indexing techniques;personal information;information retrieval;competitive learning;information retrieval techniques;relevant information;local information;personal information;retrieval task
complex queries;data warehouses;query language;data model;multidimensional databases;dimension hierarchies;multidimensional data;content delivery;location-based services;multidimensional data;location-dependent
update processing;maintenance algorithm;update transactions;data updates;data warehouse;information sources;view maintenance;materialized views;data warehousing;information source;integrate data from;distributed environment
database;ranked queries;query performance;materialized views;space constraints;ranked queries;optimal set of
retrieval effectiveness;user requests;query generation;relevant information;keyword-based search;textual information;higher degree of;digital media;high precision;keyword search;natural language;ontology-based;application domain;database queries;index structure;high recall;query expansion;semantic information;domain-dependent;irrelevant information;concept-based;automatic query expansion;huge amounts of;documents retrieved;user-friendly
web-based;spatiotemporal queries;query execution;video databases;rule-based;query language;query processing;rule-based;database management system
hierarchical clustering;energy-efficient;large number of;hierarchical model;shortest path;sensor networks;sensor network;sensor networks;ad hoc;key management
sensor nodes;sensor networks
simulation results
large numbers of;quality-aware;data generation;large number of;distributed database systems;sensor devices;quality aware;sensor data;data streams;rates;unlike conventional;data management;query processing
clustering;memory space;minimum support;data elements;mutually exclusive;data stream;grid-based;rapid rate;data streams;multidimensional data
ad hoc networks;data items;data item;data access;sensor networks;ad hoc;network topology
adaptive query processing;open source;database;dynamic environments;1;2;query processing;data streams;operator
classification;privacy preserving data mining
xml document collections;lock;higher degree of;xml documents;native xml;document type definitions
data structure;data bases;data base;context-aware;power consumption;data base
database design;2;entity-relationship;database systems;databases
multi-dimensional;database;data warehouse;data warehouse
simulation data;simulation data;enormous amounts of;ad-hoc queries;data streams
web pages;information sources;information filtering;large number of;instances;search engines;assists users
workshop report;data sharing;information systems;web-services;information systems;multiple sources;integrate data from
visualization tools;database;visualization tool;data management;data-intensive;data management
general-purpose;user queries;free text;storage overhead;indexing scheme
database engines;database server;systems support;database
aggregation techniques;emerging area;sensor network;sensor databases;produce accurate;database systems
search space;lower bound;query optimizer;plan generation;cost-based query optimization;space requirement;upper bound;operator;functional dependencies;interesting orders
software components
naive implementation;lower bound;external memory;main memory;hierarchical structure;native xml;xml document;external memory
search space;sequential pattern mining;sequential patterns
association rules;data sets;association rules
large amounts of;multi-dimensional;historical data;spatio-temporal databases;approximate query processing;moving objects;road networks;data streams;dynamic nature of
data structure;designed to support;tree structure;real-life;common practice;indexing methods
clustering;search results;search strategy;real data;relevance feedback;test results;average precision;feature vectors
database;xml documents;twig pattern;query processing;xml database;false alarms
semi-automatic;machine learning
structural joins;synopsis structures;selectivity estimates;aggregate information;selectivity estimation;twig queries;path expressions;xml data;twig queries;tree-structured data;space budget;distribution information;queries over xml;space constraints;twig query;fundamental problem;optimizer;path-based
data point;nearest;nearest neighbor;query retrieves the;instance;nearest neighbor queries;query points
query patterns;suffix tree;indexing technique;index construction;linear chain;disk-resident
high-dimensional;clustering;window queries;dimensional space;data points;query types;knn queries
xml documents;linear space;auxiliary;xml schema
semi-structured data;equivalence classes;path expressions;directed graph;query performance;workload-aware;index size;path expression;index nodes;local similarity
information dissemination;network management;matching problem;fuzzy set;publish/subscribe systems
multi-dimensional;total order;scheduling algorithm;linear order;disk scheduling;scheduling problem;disk scheduling
web applications;scheduling policies;database;online shopping;ibm db;web applications;high-priority;traditional database systems;resource usage;limited resources;execution times
web databases;search interfaces;database;probabilistic model;valuable information;probabilistic approach;database selection;databases;hidden-web
legacy systems;data exchange;xml elements;web-services;business applications;data stored in;data volumes;web services;data exchange;distributed processing
data sets;data integration;database
stream query processing;traffic data;query language;data stream management system;sensor data
specific task;reference model;schema matching;meta-model
xml processing;storage scheme;path expressions;path expression;pattern matching;path queries
xml schemas;recursive queries;schema-based;xml queries;xml queries;xml documents;sql query;arbitrarily complex;query translation;generic algorithm;xml schema
labeling scheme;labeling scheme;xml queries;labeling schemes;theoretical analysis;node labels;xml trees
temporal aggregation;worst-case;aggregate queries;query cost;space requirements;processing cost
temporal aggregation;spatio-temporal;multiple times;moving objects;association rules;space requirements;mobile users
database;sequence data;real datasets;query performance;update operations;databases
data arrives;join algorithm;hash-based;join results;join results;join algorithm;data items;join algorithms;remote sources;disk storage;network traffic;join operator
selectivity estimation
query optimization;data integration;data integration;query classes
medical research;database relations;classification;database;classification approaches;high scalability;essential information;multi-relational classification;databases;multi-relational classification;structured data;decision making
mining frequent sequences;large databases;frequent sequences;mining sequential patterns;processing cost;support counting
clustering;clustering;dimensional space;clustering methods
cost-effective;real datasets;data compression
algorithm performs;search tree;index structures;space-partitioning;tree nodes;data items;data structures;space-partitioning;clustering algorithm;disk-based;disk page
dimensional data;dimensional space;database;indexing techniques;efficient querying;real life;indexing methods;databases;distance functions;knn) queries;nearest neighbors
concurrency control;real world
data sharing;range queries;range queries
communication cost;web caching;web site;large collections;problem called;distributed environment

join algorithms;cache performance;databases;data dependencies;join algorithm
search space;frequent patterns;database;mining process;counterpart;data mining;frequent pattern mining
selection queries;aggregate queries;time series;imprecise data;general setting;additional cost;selection predicate
xml data;xml data sources;query language;data model;xml data;short-term
perform poorly;databases;database
algorithm generates;data structure;attribute values;data cube;data cube;data sparsity;real dataset;computational cost;input data;synthetic data
knowledge workers;knowledge workers;information spaces;knowledge sharing
data-driven;cluster based;database technology;cost effective;data warehouses;supply chain;databases;commercial database;high speed
low-overhead;tree construction;suffix tree;internal node;tree nodes;tree construction;buffer management;protein sequences
algorithm finds;labeled trees;synthetic data;xml processing;tree mining;log analysis;structure mining;free trees
online algorithms for;large volumes of;high quality;mobile devices;time series;time series
discovered patterns;semi-structured;mining frequent;partially labeled;compact representation;graph patterns;partially labeled;graph data;data mining;structured data
frequent patterns;database size;mining frequent closed;pruning method;space usage;pattern mining algorithms;real-life data sets;frequent closed;mining algorithm;frequent pattern
simulation study;database systems;database replication;replication;serializability;concurrency control
data objects;data structures;data stored in;scalable distributed;signature scheme
xml data;database schema;native xml;existing database
user query;database systems;database systems;complex query;user-centered
sql queries;relational database systems;expression data;database;data type;relational database system
interactive exploration;data mining methods;microarray data;interactive exploration;gene interactions;hierarchical clustering;microarray data;gene interactions;expression patterns;gene expression;association rule
nested queries;nested queries;order-preserving;xquery language
data accesses;ubiquitous computing;analyzing data;real world applications;simulation results;traffic analysis
data loss;categorical data;massive data;categorical data;subset selection;sales data;encoding methods;relational data;theoretical bounds
query results;database size;relational operations;result tuples;application logic;query result
pre-defined;intrusion detection;stream mining;user click;observed data;network monitoring;data stream;web log mining;data stream;worst case;space requirement;data streams;applications involving
data streams;query plan;query answers;aggregation queries over;data streams;data characteristics;aggregation queries
web site;deep web
replication;database;database server;materialized views;multi-tier;distributed queries;replication;application)servers;database caching;database caching;sql server
data grid;trade-offs;query capabilities;data model;view maintenance;materialized views;view maintenance;dynamic environment;detection algorithms;high availability;data integration
specific queries;web content
xml documents;xml storage;xml database;xml database;database servers
database research;relational queries
xquery language;integrating data from;information integration;xml-based;query processing;enterprise data
electronic commerce
false positives;expression patterns;gene expression data;database;biological processes;large number of;microarray analysis;principal component analysis;gene expression
ranking algorithm;microarray experiments;microarray datasets;artificial data;test statistic;statistical tests;small-sample
statistical methods;improving accuracy;classification;microarray data;clustering method;large number of;generalization error;selection methods;feature selection;feature selection methods;gene expressions;gene selection;learning task;classification algorithms
cross-validation;informative features;accurate classifiers;machine learning
clustering;microarray experiments;classification;prediction methods;gene expression;replication;microarray analysis;statistical analysis;hypothesis testing;data management;class prediction
nearest;machine learning methods;microarray data;large number of;machine learning;dna microarray data
high-dimensional;parameter space;data analysis;loss function;parameter estimation;microarray data analysis;cross-validation;censored data;cross-validation;density estimation;correlation patterns;statistical inference;gene expression
gene expression data sets;gene expression data;classification;classification accuracy;impact factors;classification performance;training samples;microarray analysis;classification;gene expression;classifier;nearest neighbors;microarray experiments
domain knowledge;mining results;expression patterns;gene expression data sets;clustering approaches;pattern mining;data set;user interface;clustering methods;databases;interactive exploration;biomedical applications;bioinformatics research
clustering;expression patterns;data mining methods;microarray data;gene interactions;interactive exploration;gene expression
meaningful clusters;life sciences;gene expression data;large-scale;free-text;large number of;meta-clustering;expression data;extracted information;data sets;high-throughput;relevant information;biological databases;existing knowledge
probabilistic framework;graph structure;frequent patterns;microarray data;markov models;long-range;em algorithm;supervised learning;markov model
statistical methods;false positive;database;cluster analysis;cross- validation;predictive model for;logistic regression;data mining;gene expression;dna sequence
clustering;data mining methods;classification;gene expression;heterogeneous data;machine learning;incremental learning;machine learning;microarray analysis;low-level;semi-supervised learning;pre-processing;machine learning and data mining;low-level
real-world datasets;data mining;classification problem;spam filtering
data cleaning;acm sigkdd international conference on;social network;knowledge discovery;real-life;data mining
database;data mining techniques;time series;social network;data processing;citation graph
data cleaning;citation graph;information extraction
data cleaning;regular expression;automatically extracting;relevant information
feature construction;learning algorithm;training data;citation graph;linear svm
text mining;prediction errors;linear regression;modeling approach;credit card fraud;predictive analytics
relational learning;relational structure;high-energy physics
link discovery;real world;training examples;world-wide-web
matching techniques;review process
classification scheme;classification;equivalence relation;hierarchical clustering;high energy physics;citation graph
kdd-2003 workshop;data mining;data mining standards
workshop report;multi-relational data mining;acm sigkdd international conference on;data mining;international workshop on;knowledge discovery;multi-relational;finding patterns;data mining;structured data
dimensional data;high-dimensional;multi-dimensional;data analysis;interactive exploration;distance function;graph-based data;projection-based;unstructured text;real-world;high-dimensional data sets;distance-based;major limitation;interactive visualization;low-dimensional
domain experts;conflicting information;social science;association patterns;data mining;machine learning;data mining;association analysis
clustering;large numbers of;small groups;large datasets;hierarchical clustering;clustering methods;computational cost;model-based clustering
historical data;data mining;accurate models;customer behavior;limited resources;customer data
association rules;high-dimensional data sets;privacy preserving;multi-dimensional;projection-based
text summarization;text summarization;plans
indexing method;single-dimensional;dimensional space;indexing structures;data sets;high-dimensional space;window queries;high-dimensional spaces;dimensional data;nearest neighbor;margin;range queries
semantic web;knowledge representations;step forward;knowledge representation;natural language
very large datasets;memory space;olap queries;data warehousing;data structure called;selection algorithm
query optimization;cost models;database systems;cost model
event-condition-action;execution model
world wide web;electronic commerce;program-committee;electronic commerce;databases;artificial intelligence;mechanism design
access pattern;database systems;object-relational;historical data;data mining;transaction processing;clustering;spatial data;programming languages;optimization strategy;relational algebra;probabilistic reasoning;web service;data structure;decision trees;analytic processing;database;publish-subscribe systems;relational operators;database systems;workflow systems;access methods;database;disk access;time series;incoming data;data cubes;external data;machine learning algorithms;stream-processing
extended abstract;information systems;database;database applications;database security;instance;databases;statistical inference;obtain information
network bandwidth;kalman filter;communication overhead;trade-offs;real-world;data streams;static data;resource management;queries efficiently;high-volume;adaptive filtering;query plans;ad hoc;resource constraints;stream queries;resource usage
high sensitivity;massive data streams;distance function;event-driven;data movement;data streams;data representation;time series;piecewise linear;data stream;specific applications;similarity search;pruning algorithm;statistical information;query sequence;automatically generated;databases;stock data;similarity matching;indexing methods
high-speed;data stream applications;performance bottlenecks;sketch-based;data stream management;data streams;user-defined;aggregate functions;data stream;query processing;query processor;heavy hitters
xpath queries over;xpath queries;query engines;xml data;xpath processing;queries involving
twig queries;processing algorithms;xml data;tree-structured;labeled tree;twig query;xml document;decomposition approach;selection predicate;twig query
tree matching;plans;tree nodes;entire document;xml elements;operator;8;xml query processing
querying xml data;approximate matching;querying xml documents;text search;database;textual content;ranking schemes;exact match;query answering;keyword search;query languages
clustering;interactive learning;query interfaces;increasing number of;deep web;effectively identify;data sources;parameter tuning;ordered trees;querying capabilities
web databases;query forms;automatic extraction of;web query interfaces;query interfaces;query interface;query capabilities;common patterns;databases;query conditions
automatic segmentation;constraint satisfaction problem;automatic methods;web sites;html pages;information extraction;web site;domain-independent;additional information;probabilistic inference;long term;web source;user input
similar queries;database;query logs;index structure;time series;information extracted from;search queries;search engine;time-series;primary goal;similarity matching;short-term
microarray datasets;specially designed;association rule mining algorithms;minimum support;microarray datasets;large number of;algorithm called;mining algorithms;association rules;interesting rules;rules generated;association rules;chi-square;bioinformatics datasets;unlike conventional
multiple dimensions;multi-dimensional;heavy hitters;synthetic data;data warehouses;multi-dimensional data;data stream applications;data items;view based;data warehousing;data streams;heavy hitters;online algorithms
query optimizer;linear programming;data mining;market-basket;labeling problem;cost-based;cost-based;query plans;data mining problems
query evaluation;optimizer;relational queries;query pattern;client applications;predict future;database servers;cost-based;training phase;context-based;total cost
query execution;main memory;database operations;database systems;database;cache performance;database research;query plan;query processing;operator;query performance
join results;rank aggregation;operator;aware query;rank-aware;query optimization;optimization framework;query optimizer;query engines;query operators;rank-aware;query processing;plans;rank-join;real-world;join operators;query plans;dynamic programming algorithm;open-source;probabilistic model;query processing and optimization;database management system
database operations;programming model;database;graphics processing units;data mining applications;fast computation;data warehousing;million records;databases;fast computation;cpu-based
query execution;query evaluation;service calls;active xml;data elements;xml documents;web services;performance gains;irrelevant information
xml document;data stream management;xml data;historical queries;sliding windows;temporal data
databases;semi-structured data;xquery expressions;query specification;xml data;twig) patterns;xml queries;data model;tree-structured;join algorithms;native xml;database;twig patterns;semantic structure;information exchange
approximate answers;twig queries;data sets;selectivity estimates;data representation;approximate query answering;query answers;data collected;unlike earlier;complex queries;xml tree;providing users;query interface;tree-structured data;error metric;data-stores;decision support systems;real-life;relational systems;xml database;synthetic data sets;selectivity estimation;tree-structured;xml databases;tree structure;xml queries
data values;synthetic data sets;sampling methods;database;summary statistics;bi-level;optimization problem;skewed;parameter settings;heuristic method;wide range;commercial systems;commercial database systems;rates;sampling scheme;processing speed
block-level;sample size;database;random samples;histogram construction;block-level;desired accuracy;large database;random sampling
data structure;online algorithms for;random samples;random sample;streaming data;data set;implicit assumption;high accuracy;large sample;disk-based;data management;random sampling
query optimization;dynamic programming algorithm;plans;selectivity estimation;cardinality estimation
materialized view;database;update transactions;materialized views;key range;decision support;materialized views;concurrent updates;multi-granularity;serializability;databases;concurrency control;query results;multi-version
query processing;graph mining;index size;database;graph-based;index structure;xml documents;indexing problem;chemical compounds;graph indexing;data mining;frequent pattern mining;path-based;large database;database updates
real-life;real-life and synthetic data;block size;distributed data;worst-case optimal
database design;physical design;materialized views;scalable solution;database administrators;relational database system;microsoft sql server
algorithm generates;query rewrite;query rewriting;xml schemas;integrating data from;information from multiple sources;multiple data sources;real-life;answering queries;constraint-based;query answering;source schemas;data integration
data sharing;error-prone;high accuracy;domain knowledge;schema matching;real-world domains;relational tables;data sources;matching accuracy;matching process;semi-automatically;database schemas
adaptive query processing;plans;query optimizer;data partitioning;query plan;query processing;source data;query processing techniques;data integration
data stream management system;processing cost;quality guarantee;theoretical guarantees;stream joins
optimizer;plans;conjunctive queries over;sliding window;execution plan;query plan;instances;computational resources;conjunctive queries;sliding windows;resource constraints;resource usage
reusability;long-running queries;plans;query plan;continuous queries over data streams;cost models;query engine;7;query plans;rates;data characteristics;window-based;monitoring systems
clustering;clustering;density-based;road networks;correctly identify;shortest path;analysis tasks;spatial network;spatial databases
data objects;principal components analysis;molecular biology;competing methods;electronic commerce;medical diagnosis;density-based clustering;data mining task;arbitrarily complex;feature vectors
high-quality;clustering structure;data points;data summarization;data bubbles;dynamically changing;hierarchical clustering;prohibitively expensive;databases;clustering algorithm;real world applications;static databases;cluster analysis;quality measure;data distribution
matching algorithm;relational database systems;large number of;xml documents;relational database;publish/subscribe system;xpath queries;finite state
graph-structured data;path expressions;incremental maintenance;algorithm maintains;incremental maintenance;query processing;index nodes;query answering;local similarity;index size
incremental update;tree-structure;data warehouse;incremental evaluation;database;xml publishing;high-end;commercial dbms;xml documents;incremental evaluation;xml views;4;sql 99 queries;optimization techniques;simple queries;xml trees
aggregate queries;query semantics;highly dynamic;real-life;6, 13, 21, 34, 35, 37;sensor networks;network topologies;dynamic networks
data values;approximation techniques;historical data;wireless sensor network;historical information;high degree of;real datasets;discrete cosine transform;data reduction;sensor networks;data processing;multiple streams;error metrics;limited resources
data sharing;small sets) of;real-life data sets;xml queries;query reformulation;peer data management systems;relevant data;reformulated queries;semantic relationships between;peer data management systems;flexible architecture
query rewriting;fine-grained access control;query optimizer;access control;database applications;information contained in;database;large numbers of;inference rules;fine-grained;user queries;access control model;control mechanisms;database relations;application code;fine-grained access control
database systems;numeric data;order-preserving;sensitive data;encrypted data;order preserving;query results;encrypted database
prior knowledge;database;information-theoretic;data exchange;theoretical results;probability theory;information disclosure;multi-party
query evaluation;execution engine;xml view;user groups;schema information;access-control mechanism;query nodes;security policies;xpath queries;structural properties;xml query;xml documents;access-control model;original document;security views;query rewriting;security constraints;sensitive information;view definitions;sensitive data;real-life;security views;query formulation;equivalent query
nearest;euclidean distance between;16;spatio-temporal;data sets;pruning power
motion patterns;spatio-temporal databases;moving objects;indexing scheme;individual objects
join algorithm;hash-based;incremental evaluation;spatio-temporal databases;moving objects;shared execution;continuous spatio-temporal queries;spatial join;spatio-temporal;spatio-temporal queries;continuous queries
dimensional space;query operations;moving object databases;large number of;support queries;higher-dimensional;indexing method;indexing structure;search queries;moving objects;databases;indexing methods
conjunctive predicates;query optimization;data mining tool;optimizer;low overhead;focus primarily on;real-world;candidate pairs;dependency relations;large numbers of;selectivity estimates;query execution times;databases;commercial systems;functional dependencies;query optimizers;relational databases;query feedback;dependency structure;poor performance
cardinality estimates;query optimization;optimizer;query optimizer;query execution;sensitivity analysis;real-world;optimal plan;cost model;query plan;relies heavily on;database;olap queries;query processing;cardinality estimation;commercial dbms;actual values;case-study;estimation errors
search space;optimizer;optimize queries;plan generation;join query;generate plans;join predicates
spatial data;access methods;data types;interval data;join algorithms;relational systems;relational databases;relational database system;excellent performance
quality guarantees;query optimization;approximation techniques;information systems;spatial data;selectivity estimation;selectivity estimation;database;spatial objects;spatial joins;spatial database;management systems;relational database systems;query processing;query plans;high-quality;range queries;approximate results
numerous applications;data items;databases;sensor nodes;heavy hitters;aggregation functions;hyperlink structure;random sampling
relational algebra;answering queries;databases;xml databases;similarity queries;answer quality;relational dbmss
duplicate records;real data sets;information content;instance;information-theoretic;database;information content;data redundancy;structural summaries;physical design;large data sets;data sets;information-theoretic;functional dependencies;missing values
partitioning strategy;main memory;edit-distance;set containment;similarity predicates;cosine similarity;similarity measures
min-cost;sql queries;ad-hoc queries;analytical models;optimization problem;category structure;heuristic algorithms;information overload;query results
web databases;database contents;short queries;statistical summaries;document classification;content summaries;database;classification;selection process;large numbers of;relevance judgments;data sets;database selection;text databases;databases;database selection;trec data;low-frequency;large database;text database
information-retrieval;optimal algorithms;query evaluation;auxiliary;relevance ranking;instance;path expressions;inverted lists;join algorithms;native xml;path expression;threshold algorithm;wide range;graph traversal
long-running queries;user-friendly;database queries;long-running;software systems
query execution;microsoft sql server;long running queries;database systems;sql queries
consistency constraints;plans;cost-based query optimizer;database
continuous query;fault-tolerance;long-running;network monitoring;stream processing;operator;high-throughput;8;fault-tolerant;query processing techniques;high availability;result quality;code base
random sample;db2 universal database;exploratory analysis;large databases;prohibitively expensive;db2® universal database;operator;exact answers;ad hoc queries
update processing;query execution;data manipulation;query processing;relational algebra;trade-offs;update operations;query processing;query processor;query processing techniques;microsoft sql server;plans
execution engine;optimization techniques;computing environments;database
1;join operations;data warehouse;time series;window functions;fact table;2;relational database system;join operator
database;data types;business logic;sql server;user-defined;stored procedures;programming languages;microsoft sql server
optimization strategies;efficient sql;aggregate functions;standard sql
database operations;data services;web services;information integration;web services;databases;information sharing
operating systems;diverse set of;typically involves;1;enterprise application;3;2;rates;4;web services;multiple domains;application server;xml schema
special attention;web services
multi-dimensional;nearest neighbor;similarity retrieval;time-series;valuable knowledge;feature vectors
user experience;web browsers;business logic;user interface;business applications;generation process
database instances;database;database applications;database server;data centers;grid computing;database;major components;high availability;area network
enterprise applications;xml query language;xml schema;xml technologies
labeling scheme;general case;node labels;xml tree;microsoft® sql server
web portal;web applications;web service;service providers;web application;real-life;business processes;web services;automatically generated;high-level;data-intensive
data fusion;data transformation;data migration;domain specific;development environment;main features;data transformations;relevant information;assists users
modeling approach;integrating data from;heterogeneous data sources;distributed processing;information integration;xml-based;web services;data repositories;enterprise data;relational databases;programming model;xml schema;xml technologies
dense clusters;spatial clustering;parameter selection;data engineering;clustering methods;spatial data;faÇade;clustering approach
data-driven;autonomous data sources;query processing;data stream management system;1, 2, 3
1;stream processing engine;high availability;distributed stream processing system;3, 6
adaptive query processing;query processing;query processing engine;query loads;memory requirements;long-running;network monitoring;1, 2, 3;stream query processing;stream data;stream joins;data stream management system;intermediate results;continuous queries;quality guarantees;4
query interfaces;query/result;network monitoring;network topology;internet scale;monitoring queries;keyword-based;hash table;query processor;diverse applications;load balancing
tree structures;data representation;join operations;xml data;tree patterns;data model;xml documents;join operations;query equivalence;xml indexing;xml queries;tree-pattern;tree pattern queries;xml query processing;indexing methods;structured queries
text search;query results;vice versa;search engine
data security
scientific computing;database;sliding window;time series;time series;case studies;sensor data;data mining;fast algorithms
clustering;subspace clustering algorithms;search strategy;subspace clustering;dense regions;noisy data;subspace clustering;low dimensional;traditional clustering;dimensional data;subspace clustering algorithms;feature selection
rare classes;real-world;imbalanced data;increasingly complex;data mining
auc;training data;sampling methods;data cleaning;data describing;uci data sets;learning systems;sampling method;machine learning;data sets;decision trees;provide evidence;real world;original data;sampled data;positive examples
base classifiers;training set;decision trees;data generation;synthetic data;class distribution;learning algorithm;predictive power;boosting algorithm;machine learning;data set;base classifier;imbalanced data sets;machine learning algorithms;predictive accuracy
classifier performance
skewed data;base classifiers;meta-learning;sampling approach;classification;class distributions;data distributions;detection method;skewed;data set;data partitions;training data set;data mining problem;classifier;naive bayesian
support vector machines;synthetic data;single class;case study;feature selection;supervised learning;dimensionality reduction
classification model;classification;text categorization;feature selection;document categorization;classifier;flexible architecture
great potential;correlation coefficient;imbalanced data;optimal combination of;text categorization;naïve bayes;chi-square;feature selection;logistic regression;information gain;feature selection
multimedia data mining;acm sigkdd international conference on;international workshop on;knowledge discovery;data mining;kdd2003 workshop
data mining and knowledge discovery;data mining and knowledge discovery;acm sigmod;information integration;data mining;program committee
search space;web query interfaces;semantically meaningful;web sources;query interfaces;correlation mining;semantic relationships;deep web;schema matching;semantic correspondences;complex matchings;information integration;databases;data mining problem
attribute values;similarity measure;record linkage;integrating data from;similarity scores;record linkage;multiple sources;data cleaning
data integration;database;integrating data from;data integration;privacy-preserving;data mining;multiple sources;privacy-preserving data mining
privacy concerns;data mining results;association rule mining;privacy concerns;mining association rules;survey data;privacy preserving;data mining;association rule mining algorithms
optimization strategies;sql queries;categorical attributes;large data sets;relational operators;aggregate functions;data set;instance;data sets;data preparation;data mining;relational database;data mining algorithms;answer sets;standard sql
real images;data points;hierarchical clustering;domain-specific;data set;clustering techniques;pattern discovery;spatial patterns;cluster analysis;benchmark data sets;spatial databases
search space;frequent patterns;frequent subgraphs;pattern mining;apriori algorithm;candidate generation
specific information;large-scale;association rule mining;pattern growth;data mining techniques;tree structure;database;databases;data mining
attribute values;negative association rules;associative classifiers;classification rules;associative classifier;correlation coefficient;class labels;association rules;negative association rules;uci datasets
data structure;mining frequent itemsets;synthetic data;frequent itemsets;performs poorly;real datasets;transactional databases;fp-growth;instance;large datasets
data analysis;traffic information;statistics-based;bandwidth consumption;web usage;search engine;web crawling
geographic regions;growing number of;manually assigned;web ir;annual international acm sigir conference on;large number of;information retrieval;machine learning;ir research;topic areas;text classification;reviewing process;information retrieval;cross-language
web pages;high level;database;meta-data;computing devices;1;3;2;5;4;6;document type;cluster analysis;personal information;higher level
information retrieval;relevance feedback;similar documents;structural similarity;clustering;1, 2;ir researchers;3;5;4;7;6;application domain;document clustering;information retrieval techniques;similarity searching;data fusion;database;software systems;application domains;biological information;exact match;databases
information retrieval research;ad-hoc;document retrieval;ranked list;trec data;retrieval task;high accuracy;question answering;retrieval techniques;high precision
data sources;highly correlated;relevant pages;relevance judgments
document collection;meta-data;information retrieval systems;relevance feedback;documents retrieved;temporal features;average precision;temporal profiles
evaluation methodology;completeness;relevance information;relevant documents;relevance judgments;retrieval evaluation;incomplete information;test collections;highly correlated;evaluation measures;test collection
test collections;retrieval systems;relevance judgments;test collection
relevance assessment;information retrieval;user requests;ranked retrieval;test collections;automatic speech recognition;information retrieval systems;test collection
parameter values;information retrieval methods;closely related;tf-idf;information retrieval;retrieval functions;retrieval performance;retrieval function
personalized search;retrieval precision;mobile search;probabilistic models;case study;retrieval models;query log;retrieval model;probabilistic model;retrieval applications
ad-hoc retrieval;machine learning problems;discriminative models;discriminative models;theoretical properties;finding task;language models;maximum entropy model;generative model for;generative models;information retrieval;maximum entropy;support vector machines;language modeling
content-oriented xml retrieval;xml retrieval;skewed
length normalization;xml element;xml retrieval;document retrieval;comparative analysis;xml elements;document collections;language modeling framework;length normalization
false positives;xml element;evaluation metric;xml tags;low recall;term frequency;xml information retrieval;average precision;high precision
clustering;discriminative features;classification;databases;global structure;document representation;algorithm called;document space;semantic structure;local structure;latent semantic indexing;document representation
user feedback;matrix decomposition;dimensional space;information retrieval;document matrix;computational cost;semantic structure;latent semantic indexing;filtering technique;storage requirements;latent semantic indexing
singular value decomposition;32, 33;search cost;fault-tolerance;document selection;overlay network;term selection;low-dimensional;document clustering;retrieval quality;memory consumption;latent semantic indexing;latent semantic indexing
low-dimensional space;document corpus;probabilistic model;document-term;retrieval model;discrete data;random variable
text mining;computational efficiency;real-life applications;theoretical foundation;ir model;information retrieval;user queries;information flow;logic-based
factors affecting;ranking scheme;search performance;ad-hoc retrieval;search engine's;ad-hoc;large scale;high quality;ranking function;user queries;retrieval performance;ranking functions
web search;cross-language;cross-language information retrieval;web search engines;query terms;search-result pages
machine translation;high-quality;general-purpose;training corpus;performance degradation;domain-specific;cross-lingual;resource selection;cosine similarity;cross-language information retrieval
statistical analysis;cross-language information retrieval;corpus-based;cross-language information retrieval
hidden variable;taking into account;related terms;parameter estimation;language model;smoothing method;language models;information retrieval;term dependencies;language modeling approach;probabilistic retrieval model;trec collections
build models;information retrieval;language models
trec collections;retrieval effectiveness;relevance information;language models;cluster-based retrieval;fully automatic;language modeling approach;retrieval framework;cluster-based retrieval
language models;information retrieval;similar documents;specific characteristics;ad hoc information retrieval;language-modeling approach
data point;clustering;clustering results;data points;clustering method;linear combination;reconstruction error;matrix factorization;clustering task;document clustering;kernel space;reuters-21578 corpus
search results;clustering problem;web search results;ranked list;labeled training data;ranking problem;clustering techniques;regression model;web search results;web search engine
real data sets;clustering approaches;data reduction;explicitly modeling;optimization procedure;document clustering;clustering algorithm;information retrieval
clustering;web directories;classification techniques;data collections;real-world;document collections;purity;clustering methods;traditional clustering
classification models;linear models;feature weighting;learning algorithms;linear svm;naïve bayes;linear svms;classification performance;linear classifier;support vector machines;feature selection;feature selection;learning models;learning algorithm;comparative analysis;information gain
text summarization;web pages;summaries generated;text based;web-page classification;text-based;information embedded in;ensemble classifier;text classification;classification algorithms;web-page;classification algorithm
ir research;world wide web;desired properties;text categorization;test collections;automatically generated
retrieval effectiveness;large-scale;trec collection;word senses;information retrieval;pseudo relevance feedback;text collection;retrieval function;information retrieval;coarse-grained;query terms
decision tree;query term;document retrieval;content words;word senses;window sizes;web data;query terms
location-based;web content;web pages;large collections of;data mining system
classification problem;document summarization;named entities;classification algorithms;machine learning;search result;entity recognition;natural language processing;statistical model;linguistic features
similarity information;mining framework;training examples;learning algorithms;optimization problem;named entity;web news;entity matching
vector space;classification techniques;multi-stage;named entities;event detection;event detection;text classification;document representation
clustering;compression methods;large-scale;document retrieval;encoding schemes;web search engines;memory utilization;encoding methods
filtering algorithms;database;large databases;data structures;information retrieval models;continuous queries
web search;retrieval effectiveness;query traffic;general-purpose;query log;query logs
collaborative filtering;evaluation metric;user experience;collaborative filtering algorithm;recommendation algorithms;error metric;nearest-neighbor;information overload;user modeling
collaborative filtering;collaborative filtering;correlation coefficient;optimization algorithm;user-similarity;weighting scheme;similar users
training data;logistic regression;low variance;information filtering;learning algorithms;early stage;logistic regression;compare favorably with;user profiles;adaptive filtering;long-term;maximum likelihood;classifier;classification algorithms
content-based filtering;collaborative filtering;information filtering;dirichlet process;prior distribution;user profiles;filtering methods;user models;information filtering;bayesian framework;bayesian framework
image retrieval;multi-level;classification technique;natural scenes;multi-level;image-based;svm) classifiers;vector machine;image content;region-based;image regions;semantic concepts;model parameters;large-scale;em algorithm;image annotation;mixture models;class distributions;salient objects
unlabeled images;training set;test set;error rates;require expensive;large collections of;text queries;joint probability distribution;search engine;statistical models
user-centered;information-seeking;implicit feedback;potential impact;specific task;web browser
search results
feature set;predictive features;web communities;ranking function;linear regression;document collections;large repositories of;ranking functions
topic tracking;link detection;topic tracking;topic models
web data;real-world;semantic web;classifier;classification
redundant features;component analysis;qa systems;information extraction;user interests;scoring function;linguistic features
event types;event extraction
cluster-based;document collection;retrieval model;problem setting
link analysis;web search;block-level;web graph;link structure;great potential;block-level;semantic graph;web page;semantic structure;vision-based;link analysis;block level;web data;segmentation algorithm
weighting schemes;computational overhead;hyperlink structure;retrieved documents;query-biased;information theoretic
web pages;web search;block-level;page segmentation;fixed-length;block-based;combined method;search engines;segmentation method;retrieval performance;great potential;search performance;special characteristics;vision-based;query expansion;current web;segmentation algorithms
automatic evaluation;evaluation metric;machine learning
1;search term;image based;search engine
text classification tasks;support vector machines;vector machine;web page;web pages
clustering;1;2;access logs
interface design;total number of;users interact with;user behavior;machine learning;search engine;implicit feedback
search performance;highly-relevant;user-defined;information retrieval;topic-specific;search engine results;simple algorithm
relevance feedback;information retrieval;document length;information access;information retrieval
relevant documents;cross language;query reformulation;pseudo relevance feedback;retrieval performance;retrieval task;average precision;test collection
language modeling framework;retrieval accuracy;information retrieval performance;mixture model;feedback documents
named entities;information visualization;named entities;search space;natural language processing
cross-language retrieval;cross-language;retrieval quality
web pages;clickthrough data;session-based;user query;features including;query terms;search engine;search session
search results;document filtering;relevant documents;user experience;web queries;retrieving relevant;higher-level;filtering techniques;cost-based;result set;high precision
markov process;document level;language models;probability distribution over;statistical language models;joint distribution;theoretical result;markov chain;query expansion;vector space model
knowledge workers;community detection;diverse sources
web pages;answer questions;probabilistic approach;question answering;keyword based;natural language
pair-wise;semantically-related;database;semantic similarity between;high precision and recall;ontology matching;hash table;databases;search performance
retrieval performance;document retrieval;highly correlated;relevance information;ir systems
real world;question-answering;qa systems;context-based
user interfaces;user interface;narrative structure;information retrieval techniques
web data;search tasks;anchor text
clustering;tf*idf;low-level;visual features;text retrieval;video retrieval;vector space model
correct answers;passage retrieval;qa systems;question answering;language modeling;models trained on
document retrieval;high quality;social networks
search systems;search topic;similarity measures
clustering;web-based;web page classification;link structures;complete set of;qa systems;question answering;natural language
document collection;cross-language information retrieval;query terms;web search engine
user study;automatic query expansion;interactive query expansion;advanced search
web search;topical relevance;relevant documents;web sites;web searches;relevant document;web page
clustering technique;recommendation service;audio features;recommender systems;collaborative filtering
information extraction;structural features;data items;information extraction;pattern discovery;pattern matching
digital libraries;search tool;large quantities of;search engine
context sensitive;expression patterns;secondary structure;prediction methods;protein structures;classification methods;context sensitive;information retrieval techniques;protein sequence;protein sequences;discover meaningful
language modelling;document collection;language modelling;ad-hoc retrieval
pagerank algorithm;web pages;web search;web graph;probabilistic model;random walk on;textual content;page rank;ranking algorithms;web resources;search engine;link analysis;hyperlink structure
content-based filtering;item pairs;content filtering;user preference;similarity function;learning problems
term weights;language model;weighting schemes;information retrieval;term dependencies;3;1;vector space model;2;term-weighting scheme;term dependency
test set;retrieval effectiveness;xml retrieval;structural constraints;document-centric;query language;xml tags;multiple sources;xml collections
1;text categorization;test collection
web documents;web pages;hidden web;dynamically generated;related data;data extraction;general-purpose;user query;search engines;databases;high recall;web page;document databases
search task;multi-dimensional;test collections;retrieval task;retrieval task
evaluation methodology;evaluation measures;margin;retrieval evaluation
trec data;hierarchical dirichlet process;document length;information retrieval;1;3;dirichlet processes;retrieval method
5, 6;information retrieval systems;information retrieval
user interests;user ratings;collaborative filtering;collaborative filtering algorithms
information access;relevant documents;relevance feedback;information access;query expansion;information retrieval systems
world wide web;information sharing;community based;inter-relationships;common interests;similarity matching;information sharing
compression-based;higher order;context-based;context-based;text categorization
text corpus;search results;document collections;duplicate detection;test collection
retrieved documents;ir) research;ir research;information access;information retrieval
cross-validation;automatically extracted from;training data is;machine learning methodology;search engine queries
semantic features;collection size;video retrieval;classification;news video
semantic video;parameter estimation;classification;large-scale;classifier training;video databases;large number of;unlabeled samples;knowledge discovery;labeled samples;limited number of;em algorithm;model selection;semantic video;video retrieval;mixture models;semi-supervised;class distributions;salient objects
automatically generates;related information;context-sensitive;automatically generated
relevant information;retrieval strategies;query terms
geographic information;multilingual documents
relevance feedback;measure called;retrieval systems;relevance feedback;large document collections;document representations;data mining;higher quality;document representation;retrieval effectiveness
world wide web;information systems;information sharing;community based;web communities;inter-relationships;common interests;similarity matching;information sharing
information sources;named entities;world knowledge;temporal dimension;event tracking;manually annotated;supervised machine learning;named entity;question answering
knowledge sharing;web searches;personalized ranking;retrieval techniques;web search
clustering;world wide web;web pages;natural language text;relevant documents;document retrieval;ir techniques;document clustering;query processing;link structure;user interaction
genetic programming;genetic algorithms;ranking function;information retrieval;relevance feedback;document structure;vector space;training documents;artificial intelligence
information retrieval research;xml retrieval;structured documents;probabilistic models;statistical language models;information retrieval;rich information;language modeling framework;special case;question answering;theoretical basis
user interfaces;topic tracking;user interface;detection techniques;narrative structure
classification scheme;matching algorithms;database;information-seeking;ir techniques;information retrieval;object-oriented;theoretical foundations;visualization techniques
classification;selection process;data mining;dimensionality reduction;clustering;international conference on;frequent itemsets;acm sigkdd international conference on;knowledge discovery;technical program;text mining;bayesian methods;data mining;privacy preserving data mining;knowledge discovery;data mining;temporal data;reviewing process;network analysis;pattern recognition;acm sigmod;machine learning;knowledge discovery;databases;program committee;classification tasks
end-users;user-centered;user-centered
variational methods;graphical models;applications ranging from;structure learning;graphical models;data mining;traffic analysis
computational efficiency;benchmark data sets;classification;cost-sensitive learning;binary classifier;cost-sensitive;cost-sensitive learning;predictive performance;gradient boosting;learning problems;multi-class;theoretical guarantees;classification algorithm
approximation algorithms;frequent item sets;frequent-set;approximation methods;transactional databases;frequent sets;data mining;frequent item-set
automatic segmentation;data warehouses;unstructured text;information contained in;data warehouse;manual effort;text collections;training data;segmentation accuracy;real datasets;relational databases;text segmentation;manually labeled
hidden variables;particle filters;real data;time-series;network traffic;statistical models;area network;estimation errors
11;feature space;large numbers of;nearest neighbor;nonlinear regression;classification;low-dimensional space;massive data sets;database;target image;nearest-neighbor search;sloan digital sky;regression problem;parametric model;data sets;missing data;model parameters;linear regression;search problem;local minima;speed-ups
clustering;clustering;euclidean distance between;clustering technique;time series;clustering accuracy;model parameters;distance metric;real world data sets;moving average
clustering;distance measure;probabilistic framework;cosine similarity;model generalizes;unsupervised clustering;probabilistic model;pairwise constraints;supervision;instances;semi-supervised clustering;markov random fields;text data sets;similarity measures;principled framework;objective function;clustering quality
precision/recall;squared error;nearest neighbor;average precision;average precision;excellent performance;classification performance;margin;neural nets;low dimensional;data mining;perform poorly;squared error;metric space;supervised learning;learning performance;maximum margin;correlation analysis;general purpose
clustering;graph partitioning;parameter-free;high-quality;collaborative filtering;real-life datasets;data mining applications;binary matrix;information retrieval;frequent itemset mining;principal components;web graphs;social networks;fully automatic;information-theoretic;problem size;user intervention;sparse matrix
classification;level features;extraction process;semi-markov;similarity functions;named entity;entity recognition;recognition systems;multiple domains;entity extraction;data integration;similarity measures
spam detection;intrusion detection;classification;formal framework;optimal strategy;ad hoc;data mining algorithms;classifier
regularization;kernel function;real data;instance;predictive performance;multi--task learning;support vector machines
world wide web;high-quality;large social networks;social network;large graph;disk resident;knowledge discovery;million nodes;million edges
cross-validation;ensemble method;ad hoc;decision tree;data streams
search space;application requirements;pruning methods;closed itemsets;efficient algorithms to;pattern mining;real-valued;algorithm called;frequent pattern mining
search space;dcm;heterogeneous sources;web query interfaces;web query interfaces;query interfaces;correlation mining;author;deep web;schema matching;semantic correspondences;complex matchings;data preparation;information integration;first name, last name;databases;mining algorithms;semantic relationships
world-wide web;graph mining;frequent patterns;computational cost;tree patterns;kernel function;structured objects;kernel functions;graph kernels;classification performance;instance;predictive performance;data mining;support vector machines;graph databases
text summarization;product features;summarization task;customer reviews
frequent itemsets;bayesian network;bayesian networks;attribute sets
theoretical framework;graph properties;specific properties;link-based;data mining algorithms;structural similarity
clustering;probabilistic latent semantic analysis;web usage mining;classification;web objects;semantic associations;web users;semantic relationships;real-world data sets;discovering association rules;probabilistic inference;probabilistic latent semantic analysis;analysis tasks;navigational patterns;unified framework;primary goal;automatically discover;user sessions
clustering;parameter-free;classification;data-mining;great promise for;time series;input parameters;algorithms require;data mining;data mining process;data mining algorithms
search results;fast algorithm;local search;matching problem;graph-theoretic;graph representation
data structure;data cube;data cube;equivalence classes;storage space;large databases;sliding window;aggregate functions;pre-computation;incremental maintenance;online analytical processing;aggregation functions
discovered patterns;spatiotemporal queries;pattern mining;index structure;spatiotemporal data;periodic patterns;mining algorithm;data management
data-driven;database;query relaxation;application domains;machine learning;nearest-neighbor;query-driven;continuous attributes;decision rules;normal form;processing queries
density function;tree data structure;dense regions
redescription mining;design decisions;classification tree;classification trees;implementation details;data mining task;high-level
training data;false-alarm;intrusion-detection systems;intrusion-detection;large number of;rates;machine-learning;normal behavior
learning process;bayesian network;learning algorithms;conditional independence;real-world situations;training set;learned model;training sets;bayesian networks
exploratory analysis;association patterns;low support;data set;support threshold;algorithm to compute
information discovery;extracting information from;interactive exploration;monte carlo;unsupervised learning;large text collections;query interface;probability distribution over;topic models;stochastic process;topic distributions;digital library;markov chain
structural patterns;large databases;main memory;index structure;mining frequent;graph patterns;existing graph;broad applications;pattern mining algorithms;databases;pattern mining;disk-based;graph databases
data-driven;human knowledge;machine learning methods;margin;svm) classifiers;vector machine;training dataset;prior knowledge;minimal optimization;support vector machines;margin
real data sets;brute-force;database;correlation coefficient;upper bounds;data sets;cost model;item pairs;computation cost;strongly correlated;upper bound;market basket
frequent itemset;frequent patterns;database;data structures;decision problem;frequent itemset mining;frequent itemsets;partial order;data mining;support threshold;data mining problems;np-hard
face images;lower-dimensional;vector space;high-dimensional space;computational cost;large volumes of;dimension reduction;similarity metric;database;image databases;image data;medical imaging;vector spaces;principal component analysis;databases;data collected;tensor product;relevant information;high dimensionality;spatial locality;image compression
singular value decomposition;main memory;dimensional space;efficient storage;classification accuracy;data mining applications;database;linear discriminant analysis;data items;data matrix;large data sets;real-world data sets;eigenvalue problem;computational cost;dimension reduction;dimensional data;dimension reduction
significant rules;large number of;decision making;learning procedure
join algorithm;spatial neighborhood;spatial features;mining process;instances;transactional data
collaborative recommendation;highly scalable;collaborative filtering
clustering;high levels of;classification problem;data source;learning problem;fine-grained;data mining;accurate predictions;transactional data
real data sets;microarray data;prediction accuracy;vector machine;data sets;normal distribution;ranking methods;bioinformatics research
decision tree;great promise for;knowledge discovery;specific problem;logistic regression;data mining
synthetic data sets;microarray data;gene clusters;complete set of;time series;data set;data sets;expression levels;test results;biomedical applications;bioinformatics research
web-based;probability distribution;weighted graph;anomaly detection;detection method;multi-tier;web application;management systems;adjacency matrix;time-series;automatically detected;feature vector
large-scale;clustering;data mining technique;data record;large-scale;specific set of;data mining techniques;large number of;regression models;data records;training data;regression model;complex structures;clustering approach;prediction models
symbolic representation;synthetic datasets;suffix tree;time series;visualization tool;time-series;false positive
machine learning and data mining;decision trees;naive bayes;support vector machines
gradient-based;neural networks;censored data;support vector machines;machine learning algorithms;objective function;high-risk
density-based;detection method;unsupervised learning;document space
interesting rules;test data;case study;industrial applications;common patterns;automated methods;decision makers;product design;data mining tools;distance based;data mining;similar patterns;analyzing data
classification problem;classification;classification process;data stream classification;training data;large data sets;data sets;data stream;real life;data streams;classification task;classifier;high classification accuracy
clustering;clustering problem;data mining technique;clustering framework;recommender systems;clustering algorithms;information-theoretic;text clustering;objective function;probability distributions;microarray analysis;maximum entropy;clustering approach
clustering;clustering algorithms;evaluation criterion;unlabeled data;test set;3;selection method;clustering performance;clustering algorithm;text analysis;classification algorithm
benchmark data;regularization;classification;linear programs;kernel parameters;automatically determines;boosting methods;cross-validation;column generation;large datasets;kernel methods;column-generation
incremental algorithms;incremental algorithm;database;margin;sequential patterns;incremental mining;sequence databases;real life;database updates;sequential pattern mining;large database
parallel algorithms;high computational cost;high dimensional;data mining applications;parallel computation;data set;high degree of;gene expression;giving rise to
surveillance systems;surveillance systems;pomdp model;time series;scan statistics;belief state;simple algorithm;temporal data;increasingly popular
binary classification;outlier detection;color images;image restoration
clustering;spectral clustering;local search;gene-expression;input space;data set;data sets;kernel k-means;similarity matrix;positive definite;kernel k-means;special case;objective function
segmentation problem;7;data mining;real data;randomized algorithms;greedy algorithms;data mining problem;segmentation algorithms
association-rule;mining algorithm;large-scale;real-world;large number of;theoretical results;2;distributed environments;large-scale distributed systems
density estimation;classification;test statistic;classification methods;uniform distribution;tree-based;input space;machine learning;highly correlated;statistical test;data distribution
high-dimensional;algorithm runs in;black box;graphical model;low dimensional;takes place;interpretability
semi-structured;total number of;graph database;frequent subgraphs;mined patterns;computational resources;large graph;data sets;mining algorithms;databases;graph databases
data set;randomized algorithm;hypothesis testing;heuristic search;monte carlo
sufficient conditions for;classification;related data;instances;simulated data;classification error;collective inference;error reduction;statistical models;relational data;inference techniques
input data;data mining results;privacy-preserving data mining
inter-document similarity;detection techniques;cosine measure;document content;information filtering;detection accuracy;computational requirements;data mining;collection statistics;web-page
clustering;feature space;data points;similarity measure;vector representation;data sets;similarity measure between;spatially varying;algorithms typically
clustering;clustering;high quality;20;14;data points;spatial-temporal;database research;clustering method;clustering result;instance;moving objects;large amounts of data
domain knowledge;clustering algorithms;search space;gene ontology;ontology-driven;pruning technique;gene clusters;domain knowledge into;algorithm generates;cluster quality;clustering process;clustering result;clustering performance;subspace clustering;gene expression data;traditional clustering;subspace clusters;subspace clustering algorithms;clustering
classification technique;nearest neighbor classification;nearest neighbor;kernel-based;classification;high dimensional datasets;tree search;highly optimized;linear scan;dimensional data;nearest neighbors
microarray experiments;algorithm performs well;monte-carlo;specific properties;relative error;expression levels;synthetic data
semantic content;classification;training examples;semantic representation;data mining;semantic concepts;performance gains;digital media;low-level features;concept detection;databases;multimedia content;semantic retrieval;semantic representation;retrieval effectiveness
increasing complexity;mining algorithms;graph mining;database;minimum confidence;structure mining;free trees
clustering;database size;image database;graph-based;domain specific;similarity function;multimedia objects;general problem
classification;monte carlo;long distance;low variance;finite mixture;model parameters;data mining;markov chain;bayesian approach
clustering;selection criteria;predictive accuracy;predictive features;cluster-based;database schema;large number of;statistical relational learning;automatic generation of;link prediction;feature spaces
approximation algorithm;directed graph;semidefinite programming;marketing strategies
data record;distributed computation;data owners;regression analysis;databases;privacy preserving;data mining;vertically partitioned data
set cover;mining algorithms;database;sufficiently large;data mining research;frequent itemset mining;frequent itemsets;data sets;large datasets;association rules
frequent itemsets;continuous data;association patterns;association analysis;binary data
singular value decomposition;aggregation techniques;unsupervised learning;weighted average;disparate sources;search engines;information gathered;data mining;multiple sources;data mining algorithms;labeled data
density-based;probabilistic model;data sets;probabilistic approach;hidden markov models;visualization methods;data types;web-log data
distance measure;motion-capture data;video-tracking;time-series;distance measures;computationally tractable;motion patterns;similar patterns;real world;motion-capture
bayesian network;bayesian network structure;privacy-preserving;data stored;privacy concerns;heterogeneous data;mining algorithms;data mining task;privacy-preserving;databases;distributed data;privacy-preserving data mining
clustering;shortest paths;graph structures;knowledge embedded in;real-world;scale-free;data reduction;large graph;interaction graphs;data structures;power law;social networks;data structure called;real-world graphs;path queries;graph clustering
face recognition;large-scale;real world datasets;learning approaches;classification tasks;learning algorithm;streaming data;subspace learning;principal component analysis;maximum margin;document categorization;maximum margin
data structures;query evaluation;query patterns;algorithm called;mining process
microarray data;discriminative power;high degree of;large number of;accurate classification;microarray data analysis;classification accuracy;data sets;selection methods;feature selection;feature relevance;gene expression;small sample size
text mining;text mining;mixture model;em) algorithm;generative probabilistic;clustering method;business intelligence;data set;text collections;text data sets;general problem;expectation-maximization;clustering
image retrieval;feature space;statistical properties;semantic categories;image feature;fuzzy set;classification accuracy;image collection;classification tree;data mining approach;semantic relationships;modeling method
classification problems;multiple classifiers
density estimation;individual privacy;interval-based;privacy preserving data mining;performance degradation;privacy preserving;data mining;mixture models;simulation results;information loss
clustering;feature space;data points;similarity measure;vector representation;data sets;similarity measure between;spatially varying;algorithms typically
classification learning;classification technique;active learning;target domain;active learning;ranking functions;supervised learning
prediction problems;classification;prediction methods;large scale;stochastic gradient descent;text data;machine learning;logistic regression;support vector machines;online algorithms
classification framework;margin-based;categorical data;linear classifiers;text classification;margin
hill-climbing;exhaustive search;beam-search;template-based;search strategy;hill-climbing;running times;search algorithm;search space
logistic regression;naive bayes;text categorization;cost-sensitive;classifier;logistic regression;piecewise linear
theoretical properties;takes into account;update rule;weighting scheme;noisy data;additional information
graph-structured data;graphical models;synthetic data;secondary structure;semi-supervised learning algorithms;graph kernels;sparse representations;labeled graphs;risk minimization;selection methods;feature spaces;structured data;conditional random fields
data set;linear combination;sampling scheme;classifier learning;machine learning
conditional likelihood;probability estimates;classification;naive bayes;maximum likelihood;bayesian network classifiers;probabilistic representation;bayesian networks;computational cost;perform poorly;objective function;benchmark datasets
pattern recognition;classification;conditional independence;learning algorithms;online learning;classification error
structured output;cutting plane algorithm;text classification;kernel-based;support vector;features extracted;sequence alignment;optimization problem;machine learning;output variables;entity recognition;support vector;functional dependencies;machine learning
expectation-maximization;combining multiple;optimization problem;logistic regression model
relevance vector machine;motion capture;regression method;human motion;shape descriptors;video sequence;shape model;combination methods;motion capture data;video sequences;human body
em) algorithm;synthetic data;loss function;label noise;margin-based;boosting algorithms;expectation maximization;accurate classifiers;margin;loss functions
feature space;manifold learning;semidefinite programming;classification;locally linear embedding;kernel matrix;local constraints;large margin;instance;gaussian kernels;nearest neighbors;low dimensional;dimensional data;learning algorithms;kernel methods;dimensionality reduction
baum-welch;action selection;partially observable;partially observable markov decision processes;problem domains;hidden markov models;hidden markov model;hidden markov models;arbitrary length
low rank;singular value decomposition;image data;closed form solution
image retrieval;data dimension;feature extraction;extracted features;linear discriminant analysis;feature extraction;linear discriminant analysis;image data;optimization criterion;high dimensionality
clustering;collecting data;taking into account;classification;active learning;active learning;data representation;image databases;classifier;learning process;noise model;formal framework;data distribution
probabilistic framework;unlabeled data;web page classification;multi-view learning;multi-view;vector machine;labeled data;linear classifiers;support vector;semi-supervised;text classification problems;class probabilities;naive bayesian
learning algorithms;lower bound;learning algorithm;machine learning;lower bounds;communication complexity;communication complexity;game theory
training data;margin based;margin based;accurate classifiers;feature selection;multi-class classification problems;selection criterion;margins;classification problems;margin;feature selection
automatically creating;state space;reinforcement learning
secondary structure;markov models;generalization performance;numerical results;benchmark data sets;multiple sequence alignment;long range;graphical model;likelihood function
clustering;state transitions;graph theoretic;learning agent;learning phase;basic algorithm;learning algorithm;reinforcement learning;topological structure;dynamic environment;clustering algorithm;state space
high-dimensional;classification problem;predictive features;feature selection;text classification;scoring methods;multi-class;classification task;information gain
shortest-path;real-world;spatio-temporal;human motion;data collected from;dimension reduction
relevance vector machine;classification;real-world;expectation maximization;computational resources;regression methods;statistical learning;model selection;bayesian learning
state representation;dynamical systems;learning algorithm;operator;discovery algorithm;model parameters;requires solving
clustering algorithms;distance-function;unsupervised learning;similarity metric;semi-supervised clustering;semi-supervised clustering;labeled data;metric learning;constraint-based;clustering algorithm;semi-supervised;principled framework
binary classification;score function;training examples;test sample;learning algorithm;text categorization;training samples;decision rules;multi-label;classification approach
classification performance;linear subspace;image data sets;classification tasks;basic assumption;pattern classification
multi-class problems;probability estimates;multi-class;statistical technique;error-correcting output codes;learning algorithms;domain knowledge;general-purpose;pairwise classification;accurate classifiers;logistic regression;ensemble classifier;binary classifiers;classification problems;classification task
linear program;real data sets;variable selection;convergence rate;quadratic program;gradient descent;feature selection;operator
synthetic data sets;large margin classifiers;vector machine;large margin;benchmark data sets;real world;special case;margin;classifier
auc;unlike standard;gradient descent;binary classifiers;binary classifier;classification problems;objective function
parameter space;prohibitively expensive;gaussian process;input space;input parameters;gaussian processes
em) algorithm;transductive learning;classification;bayesian model;kernel matrices;kernel matrix;closely related;hierarchical model;bayesian inference;missing data;gibbs sampling;input space;kernel methods;expectation-maximization
missing information;decision trees;test cases;large number of;test strategies;decision trees;cost-sensitive learning;desirable properties;attribute selection;missing values;total cost
feature space;training data;training data is;input features;classification accuracy;kernel function;vector machine;classification performance;high dimension;support vector machines
high-dimensional;algorithm learns;state representation;state spaces;optimization methods;human motion;generative models;prior knowledge;visual tracking;training sets;generative model;motion estimation;automatically extracted;human interaction;inference algorithm;visual perception;low-dimensional;dimensionality reduction
training set;linear model;induction algorithms;decision tree;dynamic model;prior knowledge;incremental learning;model trees
algorithms require;decision trees;locally optimal;globally optimal
classification learning;bayesian methods;hierarchical classification;label set;margin;large margin;optimization problem;large margin;counterpart;hierarchical structure;combines ideas from;speech data;kernel methods;learning task;worst case
sparse data;mutual information;information bottleneck;generative process;text documents;large data sets;objective function
constraints imposed by;building block for;document filtering;instances;update rule;worst case;batch learning;supervised learning;large-margin
joint probability;feature relevance;classification models;graphical model;reference model
binary classification;small groups;independent variables;feature subset selection;vector machine;case study;regression methods;feature subset selection;real world;model selection
propagation algorithm;nonnegative matrix factorization;image data;higher-level;lower-level
evolutionary algorithm;learning algorithm;multi-objective optimization;instance-based;optimal set of
small sample;simulated annealing;training data set;monte carlo;training examples;machine learning;solution space;single target;policy search;markov chain
multiple tasks;multi-task;gaussian process;recognition task;vector machine;data-set;multi-task learning
learning algorithm;kernel-based;hidden variables;learning algorithms;kernel functions;conditional random fields;maximum entropy;structured data;markov models
domain knowledge;search space;continuous variables;reinforcement learning;application domains;reinforcement learning;optimal policies
training data;training examples;active learning
monte carlo;expected error;ensemble classification;ensemble classifier;theoretical analysis;ensemble approach;classifier
rule sets;set covering;predictive accuracy;rule learning;learning algorithm;rule-learning;learning approaches;learning scheme;error rate
redundant features;classification;text categorization;text categorization;feature selection;support vector machines;margin
feature space;clustering methods;gaussian mixture model;graph based;em algorithm;margins;semi-supervised;clustering;clustering algorithms;product space;clustering performance;labeled data;metric learning methods;distance functions;binary classifiers;decision trees;distance function;database;margin based;data set;color images;databases;margin;learning method
function approximation;reinforcement learning;algorithm converges
clustering;real data sets;metric learning methods;label information;linear transformation;classification tasks;learning algorithms;locally linear;optimization problem;learning method;locally linear;data sets;semi-supervised clustering;clustering tasks;metric learning;learning problem;distance metric;pairwise similarity;problem called
induction algorithms;high dimensional;decision tree;boolean functions;randomly generated;real world;decision tree learner;information gain
clustering;clustering algorithms;classification;multivariate gaussian;linear projection;hierarchical clustering;local linear;automated methods;principal component;principal component analysis;dimensional data;dimensionality reduction
content-based filtering;item pairs;user-item;recommender systems;data set;user preference;similarity function
classification;ensemble methods;multi-classifier;comprehensibility;single classifier;classifier
markov chain;error bounds;random variables;conditional distributions;probabilistic models;belief propagation;product space;approximate inference;markov chains;inference algorithms
multi-class problems;data set;redundant features;data sets;multiple classes
phase transition;phase transition;constraint satisfaction;learning algorithm;competence;instance;learning problem;competence
information-bottleneck;document corpus;set covering;data points;loss function;optimization problem;wide range;learning task;gene expression data;positive examples
auc;evaluation criterion;test set;classification;approximation error;estimation error;statistical analysis;error rate;model selection
operator;markov decision processes;constraint logic programming;relational domains;relational reinforcement learning
transactional database;markov chains
clustering;data point;classification;neighborhood structure;high-order;probability distribution;inference methods;class labels;clustering methods;class label;local structure;semi-supervised;probability model
entropy-based;clustering;monte-carlo;entropy-based;clustering categorical data;formal framework
content-based image-retrieval;vector machine;multiple-instance learning;svm-based
sparse data;question arises;probabilistic model;sufficient statistics;frequent sets;massive datasets;bayesian network structure;sparse datasets;social networks;data mining;local structure;probabilistic models;web logs
clustering;spectral clustering;cluster structure;data points;spectral clustering;cluster assignment;similarity matrix;clustering problems
clustering;singular value decomposition;optimal values;closely related;unsupervised learning;covariance matrix;gene expression;principal component analysis;cluster membership;data clustering;learning tasks;low-dimensional;noise-reduction;principal components;statistical technique;principal component analysis;dimension reduction;objective function;lower bounds;pca-based
generalization performance;cross validation;kernel function;linear combination;optimization problem;real-life;iterative algorithm;benchmark datasets;kernel parameters;classification algorithm
compact representation;multiagent systems;action space;sparse representation
systems support;application domain;reinforcement learning;daily activities;constraint-based;long-term;constraint reasoning;temporal reasoning
regularization;modeling tool;training examples;large number of;maximum-entropy;maximum entropy;observation data;interpretability
machine learning methods;motion model;learned model;sensor data;particle filter;motion models
graph partitioning;clustering;multiple clusterings;clustering result;instances;cluster ensemble;graph models;bipartite graph
regularization;multi-layer;learning rate;stochastic gradient descent;hidden layer;computationally expensive;cost function;support vector machines;margin;classification algorithms
high-dimensional;binary classification;support vector machines;classification;classification;classification accuracy;require expensive;input space;unseen data;low dimensional;error rate;validation measures
kernel pca;locally linear embedding;graph laplacian;neighborhood information;kernel methods;dimensionality reduction
sparse learning;bayesian methods;classification problems;test data;data points;input features;irrelevant features;real-world;optimization algorithm;large number of;predictive performance;feature selection;bayesian framework;expectation propagation;classifier
clustering;clustering algorithms;clustering results;similarity measure;machine learning;share common;fundamental problem;similar features;clustering algorithm called;cluster analysis
supervised learning algorithm;relational structure;inference algorithm;relational domains;hidden states
relevant features;classification;svm-based;markov-blanket;vector machine;linear svm;feature selection algorithms;target variable;feature selection;causal discovery;margin
state representation;training data;dynamic bayesian networks;sequence data;probabilistic models;linear-chain;tree-based;natural-language;state variables;long-range;exact inference;approximate inference;belief propagation;conditional random fields
mixture components;mixture model;dirichlet process;dirichlet process;bayesian approach;type inference
base classifiers;regularization;minimal optimization;kernel-based;data points;large-scale;kernel matrices;convex optimization;vector machine;general-purpose;quadratic program;multiple kernels;cost function;multiple kernel learning;interior point methods;convex optimization problem
training data;classifier learning;classifier learning;machine learning;sample selection bias;learned model;test examples;classifier
multi-task;multi-task;kernel selection;classification tasks;optimal combination of;input space;global solution;inter-related;maximum entropy;svm classification;support vector machines;relevant features;feature selection
multi-class problems;support vector machines;decision tree;cross-validation;tree structure;binary classifiers;nodes represent;decision boundaries;multi-class
regression trees;regression trees;large numbers of;web pages;parameter space;input features;conditional likelihood;learning algorithms;crf model;potential functions;conditional random fields;gradient descent;information extraction from;search algorithm;conditional random fields;dna sequence;markov model
graph partitioning;markov random field;sample complexity;unlabeled data;classification problem;data points;semi-supervised learning;labeled training data;label assignment;application domains;graph-construction;labeled data;accurate classifiers;theoretical justification;graph structure;markov random field
apprenticeship learning;inverse reinforcement learning;reward function;markov decision process;linear combination
information bottleneck;unsupervised learning;theoretical results;maximum likelihood;special case;information theoretic
test problems;learning algorithms;ensemble selection;ensemble selection;performance metric;parameter settings
high-dimensional;probabilistic semantics;gaussian process;gaussian process;classification;real-world;classification tasks;multiple labels;prediction tasks;entity recognition;class labels;conditional random fields;arise naturally in
efficient algorithms to;support vector machines;classification;classification accuracy;classification tasks;additional information;kernel methods;kernel framework;higher order;similarity measures
regularization;sample complexity;neural networks;training examples;irrelevant features;worst case;feature selection;logistic regression;lower-bound;supervised learning;feature selection
data point;learning process;training data set;auxiliary;data points;training (and test) data;data sources;auxiliary data;training and test data;support vector;supervised learning;classifier
taking into account;collaborative filtering;latent space;distinguishing feature;missing data;expression levels;latent variable models
real data sets;statistical properties;classification;joint distribution;risk minimization;kernel methods
variational methods;graphical models;sampling methods;variational methods;probabilistic models;high dimensional;dirichlet process;variational approach;inference methods;loopy belief propagation;probabilistic inference;approximate inference;conditional probabilities;mixture models
markov decision processes;parallel processing;high scalability;special attention;large-scale
baum-welch;learning algorithm;hidden markov models;partially observable markov decision processes;observational data;low dimensional;algorithm produces;real world;principal-components
probability estimates;training data;similar objects;link analysis;random walk;stationary distribution;transition probabilities;multiple sources;markov chain;higher order;automatically learn;random walks
regularization;kernel methods;machine learning
classification;markov network;image processing;linear programming;markov networks;quadratic program;margin;natural language
state spaces;approximation methods;basic algorithm;stationary distribution;approximation method;operator;fixed point
support vector machines;classification;random walk model;graph kernels;labeled graphs;chemical compounds;positive definite;kernel methods
real-world datasets;search space;inductive learning;learning approaches;instance;inductive logic programming;perform poorly;search heuristics;fold cross validation
learning process;classification problem;verification problem;high accuracy;learned models
interaction data;customer relationship management;automatically generated;reinforcement learning;markov decision processes;unified framework;real world
attribute values;decision trees;interactive learning;classification;remote sensing;land cover;image processing;content-based retrieval;decision tree;content extraction;image analysis;data sources;decision trees;missing data;remote sensing image;databases;expert knowledge;powerful tools for
clustering;classification;small-world;information stored in;clustering methods;community structure
redundant features;classification model;induction algorithms;remote sensing;scientific datasets;scientific data;scientific applications;feature selection methods;lessons learned;exploratory data analysis;data sets;feature selection;data mining;numerous applications;diverse applications;feature selection
predictive accuracy;database;classification task;manufacturing process;general-purpose;data quality;data cleansing;data mining;ensemble approach;data quality;data mining algorithms
multi-stage;industrial applications;case study;data driven;data mining;operating conditions
error-prone;domain knowledge;classification models;manufacturing process;automatically learn
topic structure;mixture model;knowledge management;learning algorithm;topic structure;wide range;main components;data streams;data collected;model selection;trend analysis
state space;data collection;time series;collect data;minimum description length;statistical analysis;data collected;traffic data
clustering;nearest;clustering results;synthetic data sets;data mining algorithms;sufficient statistics;machine learning;large data sets;relational dbms;data mining;clustering algorithm;efficient sql
clustering;multiple classes;classification;naive bayes;naive bayes;text categorization;naive bayes classifier;user queries;low computational cost;clustering problems;memory consumption
classification;outlier detection;classification methods;great potential;variable selection;missing data;support vector machines
simple linear;data set;improving accuracy;high level;linear models;regression models;building blocks for;case study;squared error;regression model;massive data sets;response variable
multi-dimensional;programming model;application requirements;business data;application level;decision support;data sources;feature-set;rich semantics;business intelligence;data warehousing;higher degree of;object oriented;database schema
web pages;spam pages;web sites;web spam;web spam;highly correlated;search engines;statistical analysis;increasing importance;page content
semi-structured;bi-level;information management;html pages;query interface;bi-level;data integration
web graphs;access logs;visualization tools;web graph;visualization tool;decision makers;web site;web site;navigational patterns;ad hoc
multi-dimensional;multi-player;systems support;spatial-database;large number of;database technologies;load-balancing;range queries
fault-tolerant;index structure called;range queries
database applications;data management problems;web site;classification problem
large-scale;application domain;monitoring applications;fine-grained;data streams;multiple sources;highly-distributed
random graph;graph structures;graph-structured xml;evaluation techniques;semi-structured data;graph-structured data;real world situations;directed acyclic graphs;structural summaries;tree structured data;directed graphs;query processing;evaluating queries;underlying structure;xml query processing;structural summaries
path expression;sql queries;xml queries;xml query languages;duplicate elimination;path expression;duplicate-elimination;query translation;relational data
free text;structural constraints;document-centric;exact-match;query language;xml structure;xml collections
life sciences;utility functions;navigational queries;query evaluation
classification scheme;large corpora;analysis reveals;text documents;index structures;ranking models;xml documents;large document collections;efficient retrieval;query result;ranking model;index structures
web databases;end-user;distance metrics;database;categorical attributes;database research;answering queries;query processing;functional dependencies
sheds light on;xml schemas;regular expressions;structural properties;xml schema;document type definitions;real-world;xml schema
xml documents;prohibitively expensive;lower bounds;xml streams;finite state
xml documents;document-centric;textual content;xml document
search results;semi-structured;information retrieval;entity extraction;xml applications
human interactions;web accessible;web sites;web interfaces;search interfaces;search engines;database;real-world;multiple sites;increasing number of;extract information from;user queries;search engines;matching accuracy;databases;produce high-quality;allowing users to;desired information
ranking queries;database engine;join results;web databases;information retrieval applications;query processors;join operators;emerging applications;query operators;multimedia retrieval;join order;queries efficiently;scoring function;join query;data mining;relational databases;join queries;join algorithm;execution plans
10,6,27;synthetic datasets;communication bandwidth;algorithms require;modeling method;user intervention;monitoring applications;estimation methods;fall short;handle arbitrary;sensor devices;limited resources;data gathering;wavelet analysis;pattern detection;meaningful patterns;resource consumption;query processing engine;long-range;stream mining;interesting patterns;long-term
selection problem;database;web content;web servers;web caching;data freshness;sheer volume of;adaptive algorithm;multiple users
explicitly models;data sources;dynamic environments;query capabilities
life sciences;molecular biology;database;emerging area;databases;sequence database;life science;databases;life science;image data;data management
application domain;life sciences;data management;data management;database management
tree structures;graph-based;efficient algorithms to;tree-based;labeled tree;np-complete;biological databases
gene ontology;high-quality;instance;gene ontology;databases;biological data
data engineering;matching algorithm;tree model;ordered trees
specially designed;hash-based;database;main memory;biological sequence;high sensitivity;similarity search;search efficiency;hash table;databases;biological sequences;dna sequence;query sequence
increasing complexity;access methods;semi-structured;data model;data stored in;genomic data
information systems;extensible framework;data cleaning;data quality;databases;biological data
applications requiring;structured databases;semi-structured;web site;biological data;html documents;automatically generate;wrapper generation;biological data
fully operational;large-scale;image data;complex systems;information gathered;digital library;databases;distributed database;high-resolution;information technology
life sciences;data sources;keyword search on
database;data confidentiality;databases
world wide web;information systems;temporal databases
high-quality;35, 25, 22, 29, 3, 2, 26;web information extraction;instance;logic-based;34, 27, 24, 30, 23;structured information;web services;html documents;information overload
acm sigmod;database management systems;database systems;database

image data;large volumes of
database tuning;performance tuning;information systems;large volumes of

database systems;theoretical foundation;spatio-temporal databases;commercial systems;7;temporal databases
database technologies;database technologies
state university;design process;data management;legacy systems
data streams;xquery processing;xml query language;xquery language;xquery engine
continuous queries over data streams;continuous queries over;abw, cc, cc, wzl;bbd, geh, go;continuous queries
multi-version;control algorithm;bbgmoo;serializability;serializable;update transactions
digital libraries;database
domain knowledge;information systems;web services;web service;legacy systems;data sources;information services;ontology languages;web services
structural joins;structural join;relational queries;path expressions;xml data;xml queries;structural relationships;xml indexes;xml query processing
intermediate results;logical level;data stream processing;stream processing;rates;execution plans;data processing;data streams;optimization techniques;data items;data stream management systems
database research;metadata management
data integration;integration systems
query forms;structured databases;sampling approach;query interfaces;databases;deep web;search engines;data access;specific characteristics;web sources
object-oriented;conceptual model;data streams;data flow;optimization techniques;higher level
1;sql/xml;2
acm symposium on;access control;xml data;access control;international workshop on;security analysis;distributed environments;wide range;databases;grid-based;program committee;computing systems;distributed systems
web-based;ubiquitous computing;highly dynamic;web service;semantic web;sensor networks
applications requiring;inter-related;metadata management;information management;metadata management
contour extraction;point cloud
estimation technique;theoretical framework;takes into account;object-based;articulated objects;visual tracking;multi-body;object-based
spatio-temporal;fmri data;motion estimation
object boundary;motion model;image sequences;intensity values;motion estimation;object tracking;dense set of
object model;generated automatically;automatically generated
tracking algorithms;appearance-based;linear subspace;tracking performance;video-rate;pca-based
excellent scalability;step size;stochastic sampling;optimization approach;visual tracking;gradient descent;accurate tracking;optimization process;local minima;likelihood function;video frames
optimization method;tracking algorithm;linear combination;image sequence
key features;labeled training data;automatically learn
dimensionality reduction techniques;motion analysis;image sets;image set;distance metrics;low-dimensional space;linear combination;data set;distance measures;data sets;image domain;low dimensional;intrinsic structure;low-dimensional
appearance models;shape model;probability distributions;medical images;algorithm relies on;deformable models;object segmentation
dynamic bayesian network;human behavior;bayesian network;high level;applications including;decision tree;body parts;video data;semantic-level;human actions;natural-language;human interactions;human action;bayesian networks;low level;temporal constraints;human actions;user-friendly;video annotation
illumination conditions;viewing conditions;dynamic environments;multiple hypotheses;multiple hypotheses;object boundary;image sequences;moving objects;final step;object segmentation
clustering;appearance model;video sequence;algorithm requires;multiple views;prior knowledge;pose estimation;hidden markov model;expectation-maximization
operator;tracking methods;video sequence;human intervention
clustering;distance measure;estimation algorithm;shape matching;point correspondences;em algorithm;point-sets;clustering
body parts;multi-view;detection methods;multi-view;single-view;human body
feature trajectories;gait recognition;recognition rate;feature extraction;gait recognition;data-set;model-free;hidden markov models;image sequences
high-dimensional;parameter space;tracking framework;vision algorithms;deformable model;large number of;deformable model;face tracking;deformable model;optical flow;ground truth data;video sequences;low-level
low-resolution;body parts;shape priors;human activity;optical flow estimation;motion segmentation
physics-based;sensitivity analysis;numerical experiments;high precision;sensitivity analysis
user-input;structured light;face recognition;face model;high quality;shape representation;multi-resolution;free form;optimization process;tracking algorithm;facial feature;high resolution;high accuracy;constraints imposed by;high resolution
shape information;motion field;point correspondences
cross correlation;motion analysis;correlation method;remote sensing;multi-scale;satellite images;estimation algorithm;synthetic aperture
regularization;statistical estimation;tensor based;edge information;image sequences;motion estimation;constrained optimization
object recognition;'orgchart'
stereo vision;scene understanding;structured light
high-speed;high-resolution;high-resolution;moving objects;structured light
dynamic programming;high quality;stereo-matching
em) algorithm;training data;classification;classification;united states;classification results;model parameters;posterior probabilities;expectation-maximization;classification algorithm
multiple cameras;computing power;synthetic data
structured light;object model;high quality;time series
data collected by;ground truth;evaluation methodology;output quality;unified framework;data sets;evaluation techniques;unified framework;uniformly distributed;data fusion
image information;tensor product
high-speed;depth maps
correlation-based;graphics processing;stereo algorithm;cpu-based;high-level;times faster than
visual tracking;robust tracking;hand tracking;face tracking
laser range;real world;sensor measurements;vision algorithms
hand-held;extended kalman filter;range data
closely related;surface normal;multiple images;simulation results
low power;general purpose;application domains;stereo vision;autonomous navigation;rates;person tracking;high speed;stereo camera;high speed
reviewing process
illumination conditions;false positive;motion models;activity recognition;appearance based;databases;algorithms rely on;low-level
closed loop;bayesian network;image segmentation;bayesian network;image understanding;object boundaries;spanning tree;boundary points;boundary detection
training samples;spatial patterns;texture features;unsupervised fashion;statistical model
baum-welch;search algorithm for;efficient computation;local constraints;hidden markov model;bayesian formulation
input image;image-based;image regions;object boundaries;object representation;cost function
image information;image representation;image domain
object recognition;limited number of
image pixels;image patches;similarity measure;probabilistic segmentation;image patch;feature values;similarity function;data sets;approximation technique;image set;multiple types of;informative features;likelihood function;segmentation algorithms
arbitrarily complex;static images;object segmentation
quality assessment;segmentation algorithms;automatic segmentation;evaluation metric
single view;monte carlo;spatial relation;generative models;single image;markov chain;statistical framework;projection matrix
graph partitioning;graph partitioning;training set;geo-spatial;geo-spatial;domain knowledge;knowledge-based;statistical model;segmentation method;curve evolution;spatial objects;segmentation quality
single view;classification;graph-based;voting scheme;minimum spanning tree;real images
human perception;classification;computational framework;fully automatic;human visual system;decision making
human perception;computational model
image patches;texture analysis;classification;texture analysis;explicitly modeling;multiple models;texture segmentation;classification algorithms
threshold selection;threshold selection;local minima;scale-space
local features;feature detectors;segmentation results;agglomerative clustering;information-theory;abstraction level;low-level;similarity measures
vision-based;human interaction;video coding;multi-media
face detection;brute-force-search;starting points;face detection;low-level
pose variation;dynamic programming;real time tracking;face detection;processing speed;face detector;facial features;motion estimation;video camera;tracking" problem;human face;detection rate
video sequence;discriminative features;face detection;human face;candidate pairs;training data;color images;high accuracy;single image;learning framework;kalman filter;kullback leibler;conditional probabilities;weak classifiers
detection algorithms;face-tracking;redundant information;video sequence;target object;object model;detection approach;obtained by applying;detection results;face tracking;kalman filter
classification problem;classification;classification;high dimensional;face detection;image space;complete set of;real data;human face;classifier;weak classifiers;4, 5, 6, 7, 2, 3, 8, 9, 10
illumination conditions;human face;color image;database
change detection;vision-based
cognitive states;cognitive states;classification
feature points;major components;face model;face tracking;inference algorithm;facial features;motion parameters;face tracking;feature tracking
1, 3;feature space;state space;particle filter;estimation problem;face tracking;specific set of;dynamic model;particle filtering;particle filtering;tracking problem;fitness;video sequences
feature points;pose variation;training images;user interfaces;expression recognition;image sequences;pose estimation;face tracking;active appearance models
face image;face detection;face model;optimization problem;pose estimation;image sequence;real life
evaluation methodology;ground truth;tracking performance;face tracking;video sequences;face tracking;theoretical analysis;vision-based;vision-based
human faces;human face;input images
image matching;np-complete;sequence matching;elastic [image matching;image matching;np-complete;an efficient two-dimensional warping algorithm
mobile user;video based;mobile applications
face recognition;small sets of;homeland security;static images;database;training process;probabilistic approach;recognition tasks;video sequences
associative memory;associative memory;vision systems;computational power;human brain;vision systems;public domain
motion capture;support vector machines;image patches;machine learning methods;generalization performance;linear discriminant analysis;facial features;face detector;video-frame;facial expressions;fully automatic;support vector machines;classifier;feature selection techniques
low-dimensional space;probabilistic framework;mixture model;classification;expression recognition;shape model;facial features;facial expressions;low dimensional
face images;face image;database;face detection;expression recognition;feature extraction;data extraction;facial expression recognition;low resolution;facial expressions;high resolution;data collected
global illumination;kalman filter;dimensional space;input image;machine learning;temporal information;low resolution;image sequence;appearance based;radial basis function
face images;clustering;face recognition;viewing conditions;probability density function;face recognition;face space;intra-class;cluster boundaries;face space;separability;inter-class;classifier
illumination conditions;recognition rate;database;face recognition;face recognition;face recognition system;face detector;input images;real images
face recognition;low power;recognition rate;machine vision;face recognition;high-precision;multi-core;visconti: multi-vliw [image recognition;face recognition system;image processing;face recognition system;image processing;robot vision
face recognition;multi-class;robust real time [object detection;classification;database;face detection;quadrature-phase simple-cell pairs are ap-propriately described in complex analytic from;negative examples;face recognition;the feret [evaluation methodology;feature based;face-recognition algorithms;weak classifiers;beyond euclidean eigenspaces:bayesian matching for visian recognition;feature based
face images;face recognition;kernel-based;normal distributions;information-theoretic;face recognition;recognition performance;databases;recognition rates
image patch;face verification;database;face model;gaussian mixture models;real world;spatial) information
face recognition;face databases;recognition rate;recognition performance;image-based;databases;detection method;face recognition;face detector;purity;databases;human supervision;user interaction;detection rate
face recognition;learned models;video databases;learning algorithms;face recognition;supervision;video indexing;automatic speech recognition;news video;databases;multiple-instance;supervised learning algorithm;supervised learning;learning framework;model training;retrieval applications
nearest-neighbor classification;parameter-free;classification;classification;database;retrieval accuracy;class membership;shape retrieval;euclidean distances;higher accuracy than;shape retrieval;learning method
multiple users;mixture model;database;concept-based;efficient search;concept learning;image databases;indexing structure;concept learning;retrieval performance;image databases;image set;information derived from;model fitting
kernel machines;feature space;training data;euclidean space;geometrical structure;separability
11;detection techniques;selecting features;training examples;training examples;feature selection method;automatically selecting;target object;detection results;object categories;detection performance;building classifiers;11, 14, 9;feature set;object categories;positive examples
principal components analysis;discriminant analysis;training data;object categorization;linear discriminant analysis;face recognition;covariance matrix;optimal set of;optimal number of;automatically discover;unsupervised techniques
kernel-based;classification;data sets;sparse representation;reproducing kernel hilbert space;risk minimization;classification;support vector machines
feature space;computational complexity;component analysis;optimization algorithm;linear representations;feature mapping;reproducing kernel hilbert space;component analysis
erroneous data;synthetic data;learning framework;geo-spatial;motion model;real data;mobile objects;motion patterns;learning framework
mrf model;background clutter;belief propagation;potential function;higher-order;belief propagation;higher-order;approximation method;scene text;low-level;scene text
random projection;object recognition;point features;object recognition;real data;lower-dimensional;takes place;mixture models;expectation-maximization
state space;markov chain;state spaces;scene geometry;scene geometry;data fusion;prior knowledge;probabilistic graphical model;image data;monte carlo sampling;expectation maximization algorithm;sampling scheme;statistical learning
similar results;belief propagation;images captured;temporal context;context models;scene classification;image collections;single image;transition probabilities;scene classification;temporal context;applications involving
input data;decision tree;computation cost;decision tree;classifier;tree structure;conditional probability;classifier;weak classifiers
face recognition;classification;linear discriminant analysis;principal component analysis;visual representations;learning problem;learning method
[unsupervised learning;mixture components;supervised learning;lower computational cost;appearance models;object class recognition;appearance models;labeled data;selection of [scale-invariant;semi-supervised;supervised learning;class-specific;object categories;positive examples
illumination conditions;physics-based;algorithm maintains;detection rates;statistical information;physics-based;sensor measurements;moving object detection;moving object detection;sensor fusion;sensor fusion
multi-modal;ct images;image segmentation;linear combination;optimization techniques;mixture models;expectation-maximization
high levels of;video segmentation;classification techniques;detection algorithm;detection algorithm;semantic content;hidden markov model;decision process;model selection
detection algorithm;high levels of;human actions
surveillance video;surveillance video;petri nets;petri nets;users' queries
event recognition;surveillance systems;user interfaces;tracking results;activity recognition;recognition systems;communication protocol;fundamental problem;operator;user study
pair-wise;clustering;similarity scores;real data;event detection;aspect ratio;features including;event detection;detection methods;accurately detect;clustering algorithm;feature selection process;optimal number of;high dimensionality
inter-dependencies;classification results;data driven;object shape
scene understanding;high levels of;bayesian networks;ontology-driven;detection algorithm
simpler models;hidden markov model;low-level;human interaction;low-dimensional;visual features
detection algorithm;high levels of;human motion
user community;content based video;design choices;representation language;language called;instances;formal language;knowledge representation;video surveillance;markup language
human detection;moving objects;classifier;geometric relationship between;moving platform
moving objects;robust face recognition;geometric relationship between;moving platform
illumination conditions;taking into account;video surveillance;similarity measure;camera motion;geometric relationship between;motion model;data association;spatial distribution;moving objects;tracking problem;unified framework;joint probability;moving objects;kalman filter;moving platform;integrating multiple;probability model
moving platform;moving objects;geometric relationship between
search region;time series
moving platform;moving objects;image segmentation;classification;geometric relationship between
feature space;kernel-based;kernel functions;input space;simulation results;learning theory;target detection;high dimensionality;higher order;target detection
wide range;contour-based;local regions
recognition performance;automatically detected;face recognition;wide range;face recognition algorithms
human detection;specular reflections;human detection;foreground detection;hidden markov model;bayesian framework;high order
visual tracking;moving platform;moving objects;geometric relationship between
speech recognition;motion pattern;classification algorithm;ground-based;classification algorithm
moving platform;face recognition;moving objects;geometric relationship between
training set;estimation error
motion patterns;dimensionality reduction technique;vector representation
multi-modal;moving platform;moving objects;geometric relationship between;sensor fusion
image retrieval;image retrieval;textual description;image database;retrieval accuracy;natural language processing;content-based image retrieval systems;word senses;image databases;selection algorithm;visual information;visual features;high potential;natural language processing;semantic information;low-level
image retrieval;visual information;content-based image retrieval systems;natural language processing
image retrieval;visual information;content-based image retrieval systems;natural language processing
noise estimation;additive noise;vision algorithms;noise estimation;gaussian noise;error metric;image indexing;error metric;image databases;error metrics;maximum likelihood
object recognition;vision systems;probabilistic approach;detection rates;probabilistic approach;spatial distributions;low-level;detection methods;low-level vision;bayesian framework;natural images;low-level
high-dimensional;multiple queries;indexing structures;query performance;multimedia retrieval;similarity queries;nearest neighbor queries;nn queries;parallel processing;optimal performance
multimedia data;traditional database systems;database systems;classification;database
image retrieval;digital media;multimedia retrieval;retrieval performance;compressed domain;compression method;retrieval techniques
discriminant analysis;object categorization;feature extraction;image categorization;object categorization;discriminant analysis;optimization problem;optimization procedure;discriminative features;distance-based;visual feature;dimensionality reduction technique;benchmark data sets
image retrieval;graph-based;visual information;content-based image retrieval systems;natural language processing
shape retrieval;image retrieval;visual information;content-based image retrieval systems;natural language processing
image retrieval;natural language processing;visual information;content-based image retrieval systems;retrieval results
image retrieval;image retrieval;content-based image retrieval systems;region-based;visual information;natural language processing;support vector machines
scene reconstruction;active vision
user interfaces;general problem;sign language;classification algorithm
appearance model;particle filters;motion model;appearance models;particle-filter;accurate tracking;observation model;video sequences
multi-level;facial features;facial expressions;facial expressions;dynamic bayesian network;classifier
scale invariant;vision-based;edge detection;shape descriptor;segmentation algorithms
tracking method;articulated objects;hand tracking;hand tracking
multi-view;user interaction;vision based
change detection;video camera;image sequence;segmentation techniques
eye movements;video camera;multi-scale
cluster structure;neural networks;region-based;recognition accuracy;image sequence;image pair;learning scheme;matching algorithm;motion cues
clustering;matching score;image feature;feature vector;database;features extracted from;real-world;feature distribution;local orientation;computational cost;locality sensitive hashing;local orientation
pattern recognition
large scale;data sources;accurate models;ground truth;optimization problem
video sequences;perspective images;multi-perspective
camera motion;aerial images;image based;image alignment
multiple layers;mixture model;video summarization;probability models;flow fields;temporal coherence;appearance models;scene structure;robust tracking;image sequences;image motion;optical flow;bayesian framework;estimation techniques;mixture models;multiple image
clustering;clustering;computational efficiency;mathematical framework;image sequences;statistical technique;noise model;motion segmentation;higher order;motion segmentation
partial matching;image registration;region features;image noise;medical images;expectation-maximization;feature-based;global consistency;hybrid method;aerial images;scale-invariant;region features;individual features;transformation parameters;automatically extracted;matching algorithm;scale-invariant;similarity measures
domain knowledge;high quality;low-cost;high resolution images;digital camera;high-resolution images;matching method;video camera;video camera;camera views
view synthesis;high-quality;input images;multiple views;object boundaries;fully automatic;view synthesis;objective function
calibration method;aspect ratio;synthetic and real images
detection problem;detection algorithms;descriptions of image surfaces;image analysis;salient points;fundamental problem;good features to track;mathematical framework;error estimation;a combined corner and edge detector;a [condition number
matching scheme;object recognition;object parts;viewing conditions;object parts;object classes;dense set of
convergence properties;object model;image registration;kernel density;optimization algorithm;target image;feature values;data-sets;background clutter;mutual information;density functions;kernel density
parameter space;synthetic and real images;highly accurate;similarity measures
regression trees;appearance model;training set;segmentation methods;case study;appearance model;statistical region-based;regression tree;appearance models;low resolution;segmentation accuracy;cross validation;computational requirements;human faces
learning process;incremental algorithm;training images;training examples;probabilistic model;bayesian methods;maximum-likelihood;classification performance;incremental learning;small training sets;bayesian approach;prior information;maximum likelihood;object categories;object categories
appearance model;training set;segmentation methods;multiple objects;statistical region-based;low resolution
appearance model;training set;segmentation methods;statistical region-based;belief propagation;low resolution;shape matching
operator;matching problem;statistical properties;dimensional space
geometric structure;linear combination;generative models;input space;facial expressions;dimensionality reduction
energy minimization;potential functions;probability distributions;markov random fields;generative model;object segmentation
statistical region-based;training set;segmentation methods;low resolution;appearance model
appearance model;training set;segmentation methods;statistical region-based;human motion;low resolution;motion sequences
dimensional space;linear subspace;linear subspace;variational bayes;linear subspaces;real data;expectation maximization;bayesian inference;probabilistic approach;vision problems;vision problems
local features;background clutter;object class;statistical model;object class recognition;scale invariant;probabilistic model;classification approaches;local features;real world;maximum-likelihood;learning method
statistical region-based;training set;segmentation methods;low resolution;appearance model
prior model;markov random field;graphical structure;posterior distribution;computational efficiency;belief propagation;particle filters;belief propagation;visual tracking;image sequences;tree based;hand tracking;local structure;graphical model
statistical region-based;training set;segmentation methods;low resolution;appearance model
appearance model;training set;segmentation methods;statistical region-based;probabilistic approach;low resolution;optical flow
monte carlo;sampling techniques;hyper-parameters;stereo matching;image pairs;loopy belief propagation;image pair;bayesian framework;markov chain;approximate inference
natural scenes;state space;natural scenes;posterior distribution
clustering;feature space;nearest neighbor;clustering high-dimensional data;high dimensional;real-world data sets;data sets;data mining task;feature selection technique;traditional clustering algorithms;high-dimensional feature spaces;clustering
special case;event sequences;database;event sequences;reliable detection;[reliable detection;reference model;probabilistic model;theoretical analysis;market basket
clustering;clustering algorithms;multi-view clustering;web pages;classification;multi-view learning;multi-view;multi-view clustering;text data;single-view;clustering problems;negative results
clustering;clustering algorithms;feature space;density-based clustering;database;projected clustering;subspace clustering;clustering high-dimensional data;high-dimensional feature spaces;dimensional data
frequent patterns;frequent pattern mining
clustering algorithms;density-based;distance functions;multimedia applications;clustering algorithm;density-based clustering;real-world;large databases;application domains;complex objects;clustering result;distance measures;data sets;query processing;data mining;multi-step;distance computations;complex objects;data mining algorithms;range queries
test strategies;inductive learning;classification;classification;cost sensitive;classification accuracy;naive bayes;decision tree;decision tree algorithms;naive bayes classifier;cost sensitive;naive bayes;classification errors;test case;compares favorably with;missing values;counterpart
mining closed;algorithm performs;data structure;closed frequent itemsets;sliding window;memory constraints;sliding window;memory space;frequent itemsets;data stream;compact data structure;sliding-window;closed frequent itemsets
decision trees;decision tree;distributed data;communication cost;communication complexity;distributed data;efficient construction
text mining;mutual information;information bottleneck;relevance information;data clustering;data clustering;numeric attributes;optimization scheme;existing knowledge
clustering;clustering;synthetic datasets;clustering algorithms;disk-resident;data mining;theoretical analysis;typically requires
data mining;mining frequent itemsets from;mining frequent itemsets;database;main memory;mining association rules;disk accesses;data structures;databases;mining frequent itemsets
public domain;regularization;parameter estimation;numerical results;penalty term;vector machine;pattern classification;data sets;bayesian framework;margin;classifier;estimation algorithm
regression algorithm;dynamic-programming approach
matrix factorization;continuous data;frequently occurring;probabilistic latent semantic analysis;frequent sets;gene expression;data structures;database;observed data;frequent sets;independent component analysis;data sets;gene expression;dna sequence
analyzing data;environmental monitoring;taking into account
data-driven;human-generated;attribute values;uci data sets;naive bayes;application domains;data set;classification accuracies;hierarchical agglomerative clustering
unlabeled data;mixture model;classification;semi-supervised learning;semi-supervised classification;semi-supervised;modeling techniques
estimation methods;confidence values;data analysis;quality assessment;kernel density estimation;learning algorithms;problem domains;classifier;confidence levels;machine learning algorithms;confidence level;machine learning and data mining;individual classifiers
relational tables;mining associations;relational table
training set;classification;information retrieval;classification performance;text classification;text classification;latent semantic indexing;local region
real-world datasets;graphical models;machine learning methods;classification;scientific literature;synthetic data;graphical model;instance;relational data;performance gains;instances;relational data;models learned
vector space;related terms;query term;document retrieval;retrieval methods;vector space;latent semantic analysis;latent semantic;latent semantic;latent semantic;semantic information;semantic retrieval;query times;document set
high-dimensional;feature space;real datasets;traditional clustering algorithms;dimensional data;depth-first search
data-driven;association rules;item-pairs;market-basket;association rules
clustering;data analysis;expression levels;data visualization;ad-hoc;data mining applications;data-rich;data sets;human activities;data mining;real world;case-study
data point;real data sets;large data sets;takes into account;spatial neighborhood;spatial outliers;spatial outliers
classification technique;multi-class;classification techniques;multiple labels;classification;large-scale;association rule mining;classification approach;wide range;data mining;multi-class;accurate classifiers;associative classification;databases;classification approaches;multi-label;classification approach;classification problems;highly competitive;multi-label
convergence properties;cluster ensemble;clustering solution;clustering ensembles;cluster ensemble;real-world data sets;classifier
large distributed;security concerns;privacy-preserving;outlier detection;outlier detection;detect outliers;electronic commerce;databases;credit card fraud;national security
clustering;document) classification;frequent-itemset;minimum support;clustering algorithms;association rule mining;frequent itemset;database size;high quality;categorical data;search space;frequent itemset mining;frequent itemsets;algorithm called;transaction database;pruning methods;transaction databases;mining algorithm;clustering
scalable solution;classification models;classification;randomized data;data mining techniques;private information;data mining;hierarchical structure;data mining task;data mining technology;data mining;data mining based;sensitive information
prediction accuracy;target class;real-world;kernel matrix;training dataset;learning tasks;training instances;training-data;prior information;theoretical analysis;data distribution;kernel space
probabilistic framework;web page;training examples;real-world;web sites;generative model for;information extraction;text fragments;probabilistic approach;web site;learning method;learning models;information extraction;discovery problem;bayesian learning
data objects;web objects;web objects;classification results;classification method;search engine;additional features
data structure;sample size;feature extraction method;linear models;data types;feature extraction;data sets;dimensional data
effectively exploit;attribute values;training data;training examples;learning algorithms;application domains;data sets;accurate classifiers;naïve bayes
real-world datasets;class noise;classification;noise handling;classification accuracy;cost-sensitive learning;real-world;noise handling;cost-sensitive;machine learning;training sets;classification;data mining;classifier
decision trees;classification;classification;rare-class;classification methods;emerging patterns;instances;emerging patterns;decision trees;rare-class;traditional classifiers;real life applications
base classifiers;attribute values;classification accuracy;data streams;real-world applications;test instance;instances;stream data;statistical information;effective classification;data streams;credit card fraud;base classifier;mining data streams;classifier;classifier
regression tasks;noise handling;classification tasks;machine learning;reasoning capabilities;inductive logic programming;relational data;learning performance;model selection
cost sensitive classification;utility function;classification;cost-effective;heuristic search;markov decision process;classification algorithms
systems require;dynamic programming;genetic algorithm;active power;time series;finite state machine;dynamic programming approach;clustering methods;dynamic programming approach;finite state
classification;text corpora;semantic level;text classification;document representations;text classification;document representation
data set;tree matching;pattern tree;discovering frequent;tree mining
classification problem;classification quality;space requirements;generalization ability;select features;support vector machines;margin;low cost
query patterns;xml data management;incremental mining;query patterns;incremental mining;incremental updates;structure mining
markov random field;markov random field model;window size;classification;spam filtering;weighting schemes;markov random field model
real-life data sets;stream-mining;statistical techniques;data streams;statistical estimation;data streams;mining data streams
association mining;multi-relational data mining;main-memory;multi-relational;multi-relational;large collections
classification rule;unseen data;classification rules;selection techniques;evaluation measures;classifier
closed patterns;closed patterns;microarray data;real-life;large number of;mining frequent;frequent (closed;discover patterns;pattern mining algorithms;microarray data;gene expression data
clustering;clustering;multiple data streams;data stream environment;multiple data streams;multi-resolution;space constraints;data streams
modeling tool;spatiotemporal data;prediction problems;network traffic;markov chain;environmental data;markov model
clustering;clustering algorithms;training set;times higher;instance-based learning;training examples;clustering approach;nearest neighbor classifier;rates;supervised clustering;decision boundaries;nearest neighbor;nearest neighbor;uci datasets;classifier;main idea
data mining;decision tree;real-world;data streams;data items;stream mining;instances;data stream;decision tree;limited number of;data streams;class labels;labelled data;demand-driven
database;machine learning algorithms;network topologies;machine learning;decision tree;machine learning techniques;ad hoc;wireless networks;classifier
response times;domain-specific;anomaly detection;outlier detection;categorical attributes;data sets;data sets;link-based;distance-based;rates;false positive
svd based;numerical experiments;search term;singular value decomposition
clustering;clustering problem;hierarchical clustering;user query;pre-defined;web search engines;web-page
classification learning;learning algorithm;local patterns;query-driven;pattern discovery;learning performance;benchmark data sets;query-driven
evolutionary algorithms;clustering;gene-expression data;gene-expression data;genetic algorithm;gene-expression;finding optimal;bioinformatics datasets;evolutionary algorithm;clustering
matrix factorization;association rules;principle component analysis;data mining;database
data set;data transformation;data mining applications;model construction;classification accuracy;data sets;feature selection;computationally expensive
mining frequent itemsets;frequent patterns;labeled graphs;frequent itemsets;labeled graphs;graph mining;transactional data
high-volume;customer segmentation;segmentation techniques;customer behavior
numerical values;collaborative filtering;missing values;real data;web search results
model complexity;mining algorithms;decision trees;decision-tree;principal components;data stream;decision trees
clustering;genetic algorithms;multi-objective;multi-objective;clustering approach;united states;large itemsets;association rules mining;association rules;fuzzy association rules;fuzzy sets
user preferences;recommendation systems;collaborative filtering;recommendation methods;feature-based;user's preference;nearest-neighbor;cold-start;collaborative filtering;data sparseness
discovery algorithms;frequently occurring;frequent subgraph;large number of;algorithm called;large graph;large graphs;graph datasets;discovery algorithm;discover patterns;connected components;frequent subgraph
clustering algorithms;density-based;density-based;database
domain knowledge;data source;daily activities;association analysis;long-term;highly sensitive;missing values;sensor-network
event types;probability distributions;temporal patterns;temporal patterns
search results;training set;training data;classification;database;learning algorithm;naive bayes classifier;domain-specific;labeled training examples;search engine;labeled training examples
trade-offs;decision tree;naive bayes;operator;error rate;loss functions
meaningful clusters;clustering;overlapping clusters;gene expression data;similarity measurement;subspace clustering;subspace clusters;high dimensional space
clustering;clustering algorithms;spatial data;real data sets;density-based clustering algorithm;clustering accuracy;spatial database;data sets;clustering;adaptive strategy;density-based clustering algorithm;spatial databases
support levels;mining algorithms;frequent episodes;mining process;event sequence;computationally expensive;prefix tree
spam detection;network intrusion;classification;test instances;large number of;machine learning;instances;log-linear models;classification problems;classifier
frequent itemset;hidden patterns;classification;real datasets;data warehousing;continuous attributes;unsupervised algorithm;missing values;mining tasks;pca-based
predictive models;complete information;instances;accurate models;missing data;classifier
bayesian network;parameter learning;bayesian network;projection-based;conditional probabilities;feature vectors
multi-modal;multi-modal;domain-independent;video summarization;graph-based;video summarization;news video;domain-specific;tf/idf;video retrieval;retrieval techniques
discriminant analysis;kernel methods;feature extraction methods
black-box;visualization tools;high quality;vector machine;data sets;visualization methods;parameter tuning;very large datasets;graphical representation of
density-based;data representation;data points;outlier detection;detection method;data representation;detection method;electronic commerce;density-based;large datasets;credit card fraud;detect outliers
score function;high potential;optimization approach;quantitative association rules;gradient descent;locally optimal;association rule;real-world data sets;association rules;quantitative association rules;weighted sum of
data set;spatial objects;spatial features;density estimation
multi-relational;multi-relational data mining;data cubes;database relations;entity types
clustering;higher accuracy;similar objects;similarity measure;clustering high dimensional data;high accuracy;clustering algorithm;dimensional data;nearest neighbors
clustering;local minimum;incremental clustering
takes into account;information content;logic-based;ranking model;ontology-based
probability estimates;decision trees;real-world data mining applications;classification accuracy;naive bayes;conditional independence;learning algorithms;accurate ranking;decision tree learning algorithm;decision tree;accurate ranking;mining algorithm;conditional independence;high classification accuracy
document categorization;dimensional space;classification;information retrieval;training data;classification results;basis vectors;latent semantic indexing;document categorization;dimension reduction;document representation
kernel matrix;large-scale;learning phase;learning algorithm;classifier;sparse kernel;classification problems;classifier
databases;highly heterogeneous;database;closed frequent;databases;tree patterns;np-complete;closed frequent
ensemble learning;local models;meta-model;models built;data sets;algorithm called;greedy algorithm;statistical models
browsing behavior;current web;clustering techniques;web logs;web site;web data;large sample;web data
clustering;instance;large number of;representation language
data arrives;typically performed;main memory;streaming data;data stream;hidden markov model;linear scan;evolving data streams;high speed
clustering;interactive exploration;clustering methods;automatic generation of;frequent closed;mining algorithm;document set
hill climbing;monte carlo;classification;naive bayes;accurate ranking;ranking performance;accurate ranking;markov chain;naive bayes;classification algorithms
learning rules;learning algorithm;data sets;common patterns;data sets;small size;exhaustive search
attribute-based;mining framework;peculiar data;peculiar data;interesting rules;data mining
multiple queries;resource consumption;continuous data streams;memory usage;input streams;scheduling strategies;sliding-window;query operators;operator;multiple streams;operator;data stream systems;scheduling strategy;queries involving;applications involving;negative results
stream-processing engine;stream-processing
queries issued;query patterns;query pattern;tree patterns;mining process;management systems;incremental computation;mining algorithms;data structure called;query results;user queries
data-item;high-confidence;ip network;hash-based;update streams;data structure;continuous data streams;query classes;application domains;lower bounds;data stream;space usage;estimation techniques;general form;data streams;estimation algorithms;memory resources;special case;rapid-rate;arise naturally in
user preferences;energy efficiency;aggregate data;sensor networks;sensor network;sensor nodes
web collection;large scale;web pages;retrieval systems;information retrieval
semantic web;knowledge representation;social networks
semantic web;knowledge representation;formal language
information retrieval" workshop;xml data;ir methods;xml retrieval;information retrieval
extracting information from;molecular biology;sequence data;text analysis;easy access;scientific literature;sigir 2003 workshop on;computational models;information technology;textual information
retrieval methods;computational resources;query processing;huge volumes of data;data placement;information retrieval workshop
question answering;large text collections;natural language;question answering;information retrieval
statistical approaches;ir systems;query expansion;retrieval results
multi-modal;context-aware;information retrieval;retrieval performance;ir research;relevant information;context models;multi-lingual
geographic information retrieval;geographic information;key words;search engines;geographic information retrieval;unstructured data
large numbers of;web sites
schema mapping;sharing data;data normalization;data exchange;schema matching;semantic integration;semantic heterogeneity;disparate sources;record linkage;semantic web;real-world entities;data warehousing;databases;semantic heterogeneity;schema elements
domain ontology;lessons learned;mapping problem;schema mapping;schema mapping
heterogeneous information sources;context information;dcm;large scale;deep web;query interfaces;schema matching;semantic correspondences;complex relationships;schema matching
business applications;xml schemas;schema matching;xml schema
data sharing;semantic mappings
schema matching;schema matching
data sharing;information systems;semantic knowledge;semantic integration;case studies;information exchange
information sources;database design;integration systems;powerful tools for;information integration;machine learning techniques;global schema
software agents;semantic issues;application integration
ontology-based;information-integration;semantic integration;semantic integration;databases;semantic-integration
mobile devices;data access;mobile users;commercial products
database technology;xml & databases;data base;international conference on;database management
acm sigmod;xquery implementation
xml databases;data management problems;database;database;information retrieval;xml databases;data management;information retrieval;data management
databases;web pages;web usage mining;wide range;natural language processing;web content;web page;machine learning;web content;web mining;web usage;data mining;access patterns;8;information retrieval;structure mining;web data
world wide web;extracted information;valuable information;information extraction;information quality;html documents;automatically detected;error rate;desired information;relational data
learning algorithms;feature vector;page segmentation;content analysis;spatial features;content features;hierarchical structure;web mining;learning problem;vision-based;web page;user study
world wide web;information resources;large-scale;knowledge management;instances;semantic web;knowledge acquisition
clustering;text mining;mutual reinforcement;textual content;multi-source;sentence extraction;bipartite graph;clustering algorithm;online news
information diffusion;low-overhead;long-running;information propagation
free text;hidden web;knowledge bases;web query interfaces;database;html pages;unstructured data;web services;databases;structured data;unstructured data;web forms
probabilistic framework;extraction task;named entities;generative models;features including;labeling problem;domain-specific;databases;html pages;rule-based;extract information from;conditional random fields;web data
large scale;query interfaces;deep web;schema matching;source-specific;ad-hoc;databases;query translation
classification model;multi-objective;ad-hoc;data mining;attribute selection;multi-objective optimization
clustering;clustering algorithms;data sets;fast algorithm;data points;high dimensional;categorical data;data clustering;algorithm called;subspace clustering;dimensional data;objective function;subspace clustering algorithms;high dimensional spaces
average precision;supervised learning;squared error
predictive model;neural network
solution space;predictive models;gradient boosting;classification approaches
tuning parameters;classification problem

training data;large-scale;vector machine;block-based;data blocks;learning tasks;learning problem;prediction task;classification algorithms
neural networks;objective functions;neural network;squared error;neural network;objective function
squared-error;average precision;prediction task;classification
text mining;large amounts of;free text;database;natural language processing;text data;machine learning;kdd conference;data mining;information extraction
acm sigkdd;web site;complete information
workshop report;link analysis;acm sigkdd;link analysis
workshop report;multi-relational data mining;acm sigkdd international conference on;multi-relational;knowledge discovery;multi-relational data mining;finding patterns;data mining;structured data
workshop report;semantic web;international workshop on;knowledge discovery;semantic web;web data
multimedia data mining;acm sigkdd international conference on;international workshop on;knowledge discovery;data mining;kdd2004 workshop
workshop report;web usage analysis;acm sigkdd international conference on;knowledge discovery;web mining;data mining;web mining
mining temporal;acm sigkdd international conference on;data mining;sequential data;knowledge discovery
workshop on data mining;acm sigkdd international conference on;data mining;drug design;knowledge discovery;data mining;gene expression;biological data;high throughput
workshop on data mining;data mining standards;privacy preserving data mining;predictive model;data mining;markup language
artificial intelligence;artificial intelligence
data acquisition;data replication;wide range;sensor data;data management;data storage;query optimization;management systems;international workshop on;sensory data;sensor networks;review process;sensor networks;data management;data processing;network data;database;information processing;streaming data;data management techniques;query processing;data management
low bandwidth;sensor readings;query results;linear model;communication cost;spatio-temporal;large number of;hardware technology;sensor devices;power consumption;sensor networks;query processing;multiple queries;applications requiring
large numbers of;kalman-filter;estimation technique;data-stream;estimation error;adaptive sampling;sensor networks;central server;rates;sampling rate;continuous data;data characteristics;sampling technique
data stream;motion tracking;distributed data streams
physical environment;low-bandwidth;sensor networks;push-based;confidence level;confidence levels;area network;data management
sampling methods;distributed computation;sampling algorithm;worst-case;uniform sampling;sensor databases;randomized algorithms;building block for;sensor networks;random sampling
wavelet decomposition;network connectivity;data reduction;optimization problem;user-defined;sensor networks;network topology;query engine
network processing;sensor networks;network lifetime;database systems;energy-efficient;simulation study;monitoring applications;sensor networks;sensor nodes;data dissemination;data transfer;event data
data analysis;wireless sensor network;case study;power consumption;sensor databases;query processing;sensor networks
query optimization;access control;large scale;qualitative analysis;cost-based optimization;sensor networks;sensor network;optimization techniques;data transmission;processing queries
network processing;computational power;network applications;data model;resource management;communication bandwidth;sensor nodes;sensor networks;application developers;sensor networks;automatically generate;high-level;low-level
ad-hoc;data streams;query processing and optimization;mobile devices;information retrieval;query processing capabilities;information processing;sensor networks;data stream management systems;data streams;wireless) networks;increasing importance
database technology;network applications;active rules;active rules;query processing in;environmental monitoring;sensor databases;query processing;wide range;data aggregation;sensor networks
spatio-temporal;power supply;distributed processing;energy efficient;environmental monitoring;sensor networks;query processing;sensor network;issue queries;retrieve information;energy-efficient;spatio-temporal queries
homeland security
nearest neighbor;sensor node;index structures;knn queries;great potential;spatial queries;sensor networks;query processing;location-aware;knn) queries
web documents;web pages;sliding window;web collections;summarization techniques;dynamic content;multi-document summarization
sample complexity;data analysis;distance-preserving;dimensionality reduction;kernel methods
information systems;data privacy
search systems;web search;real-world-entities;search process;data model;fine-grained;real-world entities;graph models;graph structure
domain knowledge;markov logic networks;automatically learning;data cleaning;real-world;real-world;statistical techniques;machine learning;relational structure;data mining systems;multiple types of;integration process;soft constraints;multiple sources;probabilistic graphical models
data mining;data analysis;pattern recognition;machine learning
search space;negative association rules;strong correlation;associative classifiers;market-basket analysis;correlation coefficient;classification model;association rules;negative association rules
plans;knowledge discovery;rule extraction;databases;problem-solving;knowledge discovery process
11;clustering algorithms;regularization;maximum likelihood estimation;10;fuzzy clustering;expectation maximization;em algorithm
clustering;clustering algorithms;singular value decomposition;combining multiple;topic discovery;combining multiple
reduction techniques;window size;data flows;sliding window;data reduction;hierarchical structure;data stream;data streams;query answering;sliding windows;histogram-based;range queries
data source;raw data;data mining tools;source data;data mining;mining patterns;change frequently
classification;spatial data;spatial relations;classification;domain specific knowledge;real-world;multi-relational;tightly integrated with;probabilistic approach;associative classifier;data mining system;naive bayes classifiers;association rules;associative classification;spatial database;object relational;relational data;spatial reasoning
graph partitioning;clustering;parameter-free;real-life datasets;outlier detection;information-theoretic;group structure;principal components;social networks;problem size;numerous applications
theoretical foundation;classification accuracy;test sample;classification error;machine learning;decision rule;decision making;classifier;game theory
clustering;clustering;tree-based;real data;xml documents;xml document;xml trees
monte carlo sampling;markov chain;sampling scheme;time series;microarray data
training set;learning algorithms;target distribution;learning curve;text classification tasks;real-world;feature selection methods;naive bayes;learning tasks;information retrieval;small training sets;logistic regression;learning models;support vector machines;class distribution
probabilistic model;randomized algorithm
feature representations;document classification;high-accuracy;cognitive load;real-life;supervision;text classifiers;text classification
automatically generates;domain experts;test set;fold cross-validation;classification;database;accurate classifiers;protein sequences;error rate
hierarchical clustering;data mining
knowledge discovery;compared with conventional;text databases;text mining techniques;machine learning
clustering;large amounts of;local sites;density-based;huge amounts of data;high quality;user-defined;clustering approach;distributed clustering;clustering;complex data;clustering algorithm;density-based clustering algorithm;clustering quality
web documents;web pages;sliding window;web collections;summarization techniques;dynamic content;multi-document summarization
dimensional space;efficient computation;large databases;3;skyline operator;operator;3,15,8,13,2;efficient computation;skyline objects
feature ranking;relevant features;constraint satisfaction;feature ranking;algorithm named;feature selection;ensemble approach;statistical model;machine learning and data mining
databases;classification;privacy concerns;privacy preserving data mining;distributed sources;data warehousing;data mining;multiple sources;classifier
kernel pca;kernel matrix;feature extraction method;classification;extracted features;vector machine;feature extraction;large number of;classification performance;data set;memory requirement;mapping function
clustering;clustering algorithms;web documents;spectral analysis;data set;text collections;text collection;data characteristics;clustering process;biological data
window size;synthetic datasets;event types;event sequence;constraint-based mining;window sizes;2;local maximum
feature space;knowledge discovery;tedious task;tf/idf;information retrieval;data set;case study;feature generation;knowledge discovery process
bayesian model;attribute values;training data;confidence intervals;decision support;predictive performance;classifier;class probabilities;naive bayesian
graph mining;graph isomorphism;frequent subgraphs;real-world;apriori-based;candidate set;graph data;frequent subgraph;hash-based;apriori-based;huge number of
interesting rules;interestingness measures
information systems;classification;classification;spatial datasets;spatial datasets;knowledge discovery;decision tree learning;entropy measure;geographic information;spatial relation;spatial databases
feature selection techniques;classification;feature construction;classification performance;knowledge discovery;feature selection;data dimensionality;feature weighting;feature selection techniques
clustering;search space;classification;clustering results;optimization problem;clustering process;biological datasets;traditional clustering algorithms
feature space;spam filtering;spam filtering;naive bayes;text categorization;huge amounts of;computational cost;classifier
frequent itemset;parallel algorithm;programming model;communication overhead;high degree of;mining process;frequent itemset mining;data flow;candidate generation
classification scheme;clustering;classification;unsupervised learning;large databases;knowledge discovery;gaussian distribution;databases
clustering;real data sets;density-based;spatial data;spatial clustering;clustering method
text mining;text mining;large scale;life science;interaction networks
theoretical analysis;classification learning;data sources
association rule mining;data providers;private information;privacy preserving;association rules
mining patterns;complex patterns
sequence data;clustering
learning process;learning rules;data obtained from;aggregated data;inductive logic programming;source data
relevant documents;hierarchical clustering;user query;search engine;web page;web-page
inter-transaction;discovering frequent;temporal databases;complete set of
data mining method;human movement;temporal patterns;time series;numerous applications in;data mining method
clustering;web directories;short queries;user query;web search engine;rough set;dynamic nature of;large number of;4,3;search results;clustering performance;search engines;document clustering;web search results;search engine;clustering;search results;rough set approach
access pattern;web usage mining;web site;user-defined;sequential patterns;mining frequent;1;hidden knowledge;business intelligence;2;wide range;real life;mining process;access patterns;log data;web usage data;web logs
data analysis;time series
normal form;special attention
web sites;access logs;recommender systems;database management;web sites;web server;highly dynamic
text documents;human interaction;document collections;knowledge creation
data mining;machine learning and data mining;data analysis;machine learning
intrusion detection system;anomaly detection;real-time monitoring;training data;web pages;data mining algorithms;detection rate
query answers;web pages;ranked list;takes into account;hierarchical clustering;1;search engines;correct answer;query results;web-page;web users
classification decisions;active learning;supervision;text classification;cognitive load;text classifiers;text classification
1;probabilistic models;context-free;machine learning;logic programming;hidden markov models;inductive logic programming;bayesian networks;4;relational learning;5
web mining;web personalization;usage data
data source;data mining tasks;integrated environment;user interface;data preparation;data mining system;data mining;data mining system
access paths;join operations;database;relational operators;conventional techniques;join operators;join algorithms;operator;temporal databases;auxiliary
database;querying xml data;xml fragments;sql queries;xml data;xml query;formal framework for;data model;relational tables;management systems;query performance;xml database systems;query processing;xml database;relational databases;query processing algorithms;query result;query conditions;xml schema
inherent uncertainty;semantic web;web resources;matching process;formal model
data sharing;data management system;schema design;management architecture;semantically meaningful;share information;large-scale;database;data integration systems;data management;schema evolution;easily extensible;semantic mappings;schema information;query answering;data integration
web service;description logics;extra information;instance;knowledge based;web services;service discovery
domain knowledge;mediator systems;concept-based;query formulation;concept-based;language called;keyword-based search;data integration;meta level
large-scale;query evaluation;database;information sources;modeling approach;answer quality;query translation
database;originally designed;commercial dbms;data-warehouse;data processing;data-centric;database engines;stream-processing
tag based;business operations;increasing number of;business processes;real-world situations
data streams;theoretical analyses;error guarantees;data sources;space usage;data stream management system;uniformly distributed;data streams;high-speed data streams;error bounds;data distribution
11;massive data streams;randomized algorithm;22;efficient computation;fundamental problem;data stream;5;8;databases;sensor networks;data aggregation
unified framework;high quality;intrusion detection;similarity queries;monitoring queries;data streams;window sizes;multi-resolution;interesting patterns;data stream;traffic management;data streams;sensor networks;indexing scheme;click streams;data-centric;space usage;queries efficiently
corpus-based;multiple domains;schema matching;corpus-based
duplicate records;data values;matching algorithm;real-world;schema matching;schema matching;data sets;data integration
data values;information systems;data provenance;meta-data;query language;data items;store data;data transformations;answer questions;schema elements
space complexity;geographically distributed;randomized algorithm;database;data reduction;sloan digital sky;distributed queries;large data sets;scientific databases;network traffic;caching techniques
growing number of;continuous query processing;publish/subscribe systems;maintenance cost;database;view maintenance;performance gains;incremental maintenance;data warehousing;view maintenance;base tables;base table;batch processing
plans;rates;data stream management system;adaptive approach;data stream systems;data characteristics;join queries;continuous queries
approximate answers;sensor nodes;data acquisition;energy efficient;network nodes;user queries;sensor networks;data-centric;sensor networks;user query
data sources;stream query processing
sensor-network;information systems;plans;low-cost;real-world;data sets;optimization problem;user query;probability distributions;computational resources;sensor networks;query processing;query processor;optimization techniques;correlated attributes
sensor nodes;multi-dimensional;energy-efficient;data-centric;aggregate queries;query optimization techniques;query processing in;sensor networks;query processing techniques;design space;sensor networks;processing queries;data organization;energy-efficient;data storage
tf*idf;adaptive query processing;plans;benchmark data;increasing number of;xml queries;query evaluation;xml repositories;query answers;document structure;efficiently computing;adaptive approach;query plan;approximate matches;adaptive algorithms
result sets;large numbers of;management systems;query processing in;intermediate results;large number of;network nodes;query processing in;distributed processing;exact match;database objects;query routing;super-peer;simulation results
data objects;large graphs;nearest neighbor;rnn queries;euclidean spaces;large graphs;optimization techniques;query returns;query point;nearest neighbors
privacy guarantees;privacy-preserving;random variables;real datasets;high-accuracy;random perturbation;random-perturbation;itemset support;data records;privacy-preserving;data mining process;frequent-itemset mining;condition number
classification;sensitive information;share information;individual privacy;continuous attributes;privacy requirements
data-management;census data;input data;finding an optimal;optimization algorithm;input parameters;data privacy;wide range;np-hard
database;query result caching;database replication;web sites;scheduling strategies;intensive workloads;dynamic content;data consistency;database replicas;dynamic content;load balancing
content-based filtering;simulation study;large-scale;overlay-network;network topologies;monitoring applications;fine-grained;high-volume;data streams;data dissemination;rates;data delivery;highly-distributed
maintenance cost;large database
tree based;tree based;update propagation;data access;update propagation
data values;database technology;xml repositories;query performance;native xml;xml document;data repositories;relational databases;optimization techniques
estimation accuracy;statistical summaries;schema-based;xml data;frequent updates;incrementally maintaining;incremental maintenance;xml-based;xml queries
handle arbitrary;update cost;xml processing;worst-case;xml data;tree-structured;xml documents;data structures;xml data
data values;synthetic data sets;inverted lists;databases;update-intensive;search queries;relational databases;relational databases;change frequently;update-intensive
real data sets;np-complete problem;efficient storage;massive volumes;scientific data;scientific applications;index structure;run length;high energy physics;database;indexing techniques;rates;scientific databases;compression ratio;databases;query processing algorithms;unlike conventional;indexing methods
vehicle tracking;data source;database;vehicle tracking;trajectory data;data management techniques;data pre-processing;data mining;data management techniques
replication;replication
clustering;clustering algorithms;total number of;accurately identify;real datasets;domain knowledge;optimization problem;objective function;projected clustering;clustering accuracy;accurately detect;semi-supervised;theoretical analysis;clustering algorithm;semi-supervised;gene expression;low-dimensional;projected clusters
clustering;clustering-aggregation;large data sets;clustering method;instance;clustering aggregation;theoretical guarantees;clustering aggregation;clustering categorical data
high-dimensional datasets;categorical data;categorical datasets;complete search;algorithm called;subspace clusters;subspace clusters
xml document collections;space overhead;path expressions;xml documents;index maintenance;incremental maintenance;search engines
tree structures;data distribution;false alarms;join operations;tree structures;xml data;xml queries;post-processing;indexing methods;query processing;xml indexing;querying xml data;xml indexing;matching algorithm;space complexity;query equivalence
matching algorithms;worst-case;pattern queries;index structures;directed acyclic graphs;pre-computed;directed acyclic graphs;query processing;graph data;graph matching;pattern matching;stack-based;increasingly popular
xml documents;xml documents;skewed;relational databases
path expressions;data types;query plans;pattern matching;data structures and algorithms
evolving data;search performance;poor performance;data updates;index structures;index structure;streaming data;moving object;moving object databases;index structures
query response time;semantic caching;user experience;semantic caching;query types;spatial queries;mobile clients;limited number of;performance gain;spatial queries;simulation results
mobile clients;mobile clients;test results;ad-hoc
window queries;search algorithms;indexing techniques;knn queries;location-based services;spatial index;spatial index;search trees
increasing number of;storage scheme;database;main memory;computing devices;storage model;computing devices;query execution plan;database management system;operator;data characteristics;data management;high complexity
multi-version;database systems;real-life applications;database applications;isolation level;fault-tolerance;replication;update intensive;serializability;concurrency control;concurrency control;excellent performance;control algorithm
data structures;unlike earlier;storage systems;meta-data
lock;high-priority;statistical analysis
xml tree;database;xml data;data exchange;probabilistic xml;data sources;xml documents;data integration;computing devices;data management system;probabilistic xml;real world;data integration;main idea
web-based;complex queries;relational algebra;query language;relational calculus;heterogeneous data sources;query language;object-oriented;user interface;xml data sources;query languages;entity-relationship
query optimizer;database;finding optimal;real-life;worst case;user-defined functions;brute-force algorithm;subexpressions;np-hard
database;spatio-temporal;spatial objects;moving objects;moving objects;moving objects databases;spatio-temporal
encoding scheme;skyline query;pruning technique;performance guarantees;skyline computation;data stream;skyline points;skyline queries;efficiently computing;data streams;sliding windows;data distribution
multi-dimensional;data analysis;pattern recognition;data processing;vice versa;outlier detection;data clustering;detection algorithm;data sets;data analysis;clustering methods;signal processing;data mining algorithms
unified framework;information loss;medical data;medical data
semi-honest;data analysis;data confidentiality;data analysis;proposed protocol;encrypted data;multi-party;distributed data;multi-party
query optimization;query cost;select-project-join;rewriting rules;on-line analytical processing;decision support applications;view maintenance;query performance;materialized views;incremental maintenance;data warehousing;olap operations;huge volumes of data;transformation rules
state transitions;data warehouse;data warehouses;state space;search problem;state-space
distributed data sources;plans;view maintenance;data sources;view maintenance;distributed environment;join queries;inter-related
multi-dimensional;focused primarily on;mining frequent itemsets;data cube;sequential patterns;regression model;association rules;transactional database;dimensional space;dimensional space
semantic annotation;statistical model;semantically related;template-based;semantic web;hand-labeled;instances;semantic concepts;html documents;automatically identifies;spatial locality;html documents
text classification;text classifier;classifier;vice versa;unlabeled documents
large amounts of;database contents;statistical summaries;content summaries;web databases;valuable) information;database;web-accessible;database selection;survival analysis;text databases;databases;text databases
fast approximate;multi-level;multi-dimensional;text data sets;distance measure;random samples;data sets;data set;high-dimensional data sets;similarity search;similarity queries;protein sequence;data objects;nearest neighbors
skewed data;nn) queries;nearest neighbor;indexing approach;tree-based;moving objects;moving objects;query processing;location-based;cost model;nearest neighbor queries;grid-based
incremental evaluation;memory requirements;spatio-temporal databases;large number of;highly scalable;queries issued;nearest neighbor queries;shared execution;location-aware;theoretical analysis;nearest neighbor queries;tree-based;concurrent queries;continuous queries
conventional methods;workflow execution;intermediate result;web server;remote sites;mobile applications;content delivery;intermediate results;cost model;wide range;simulation results
similarity analysis;symbolic representation;global information;distance function;database;time series;multi-resolution;interesting patterns;time series;precision/recall;text-based;retrieval techniques;clustering accuracy;synthetic data
evaluation cost;estimation method;spatio-temporal;workload-aware;query-driven;moving objects;aggregate" query;incur high;update cost
xml schemas;path expression;sql queries;efficient sql;query translation;xml queries;data set;relational systems;xml views;xml document;query containment;query translation
xpath queries;xpath processing;optimization techniques;native xml
rdf data;rdf databases;aggregate queries;massive amounts of;aggregate queries;view maintenance;aggregate queries over;resource description framework;relational dbms;algorithm to compute;web data
online aggregation;resource consumption;large volume;network management;stream data;ip-network;randomized algorithms;heavy hitters;huge number of;processing power
handle complex;query evaluation;data-mining;data structures;tree structure;database application;digital libraries;set-valued attributes
frequent itemset mining;extraction task;open source;data stored;access methods;frequent itemset;relational dbms;data distributions;data representation;indexing technique;frequent itemset mining;data mining;support threshold;relational dbmss;fp-growth;relational dbms;stored information
input data;frequent items;real-world;optimization problem;error guarantees;data structures;distributed data streams;worst-case;distributed data streams;na篓ýve;na篓ýve
distributed stream processing;high-availability;stream-processing systems;designed to support;distributed environments;high-volume;data streams;data-processing;high availability;stream-processing
pair-wise;continuous query processing;large numbers of;load distribution;stream processor;pull-based;computing environments;stream-processing systems;load distribution;greedy algorithm;push-based;correlation based;np-hard
archived data;data compression;large-scale;replication;stored data;file systems;hard disk
information systems;storage systems;storage layer;data migration;short term;long term
space utilization;data-placement;data migration;space utilization;data skew;data replication;data-placement;skewed;mathematical analysis
ranking functions;user profile;user preferences;user profiles;real users
long-running;sql queries
data cleaning;real-world;real datasets;duplicate elimination
data structure;state transitions;xml filtering;long-running;main memory;xml queries;highly accurate;cache performance;xml filtering;data-intensive
data structure;false positive;xml data;complex applications;path queries;2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 18;path queries
publish & subscribe system
data objects;black box;open-source;database;object-relational;application level;object-based;database access;object relational;enterprise applications;databases;database access;hand-written;mapping systems
information systems;cost functions;query processing;optimizer;database;dynamic nature of;data sources;cost model;query plan;cost-based query optimization;remote sources;information integration;query cost;federated queries;cost function;load distribution;query routing;query routing;network latency
clustering;db2 universal database;db2 udb;query processing;database schema
high-speed;database;text search;high-precision;search applications;text search
communication network;database;geo-spatial;united states;traffic information;retrieval method;retrieval method
xpath expression;xml stream;xml data;xml streams;large number of;high scalability;xml stream
data-mining method;estimation method;detection method;transaction execution;data collected from;distributed systems;em algorithm;distributed systems
sql/xml;xml instances;xml processing;database;native xml;data processing
optimizer;database management systems;database;text search;execution strategies;query processing in;data sources;cost-based;data access;query processor;execution engine;microsoft sql server
fine-grained access control;language constructs;relational database systems;privacy policies;relational database management systems;personal data;privacy-preserving;databases;preserving privacy
inter-transaction;fully operational;data loss;database management systems;database;database transactions;database management system;operator;standard database
event processing;real-world;fault management;fault management;stream processing;management systems;service providers;data stream;rates;large amounts of data;multiple streams;data stream processing
general purpose;fault-tolerance;industrial applications;management systems;storage systems;storage management;database performance;database management systems
data analysis;graph mining;data mining and knowledge discovery;data mining techniques;knowledge discovery;data mining;network data;network data
end-user;web pages;text analytics;large-scale;product review;entire document;natural language processing techniques;web services;document-level;opinion) mining
business operations;predict future
meta-search engine;meta-search engine;web based;search terms;query relaxation;keyword-query;cross-media;image search;cross-media;search engine;keyword query
8;million web pages;web search engine;data types
frequent itemset mining;pattern discovery;data characteristics;databases
operator;schema translation
data structure;polynomial-space;xml streams;xpath processing;sequential scan;xpath processing
xml documents;semantic information;web services
sensor networks;sensor measurements;sensor networks;latent variables;sensor network
database;spatio-temporal;query language;data model for;multimedia objects;text content;model called
database;database;human motion;prohibitively expensive;database-centric;relational data;similarity-based;motion sequences;human motion;long sequences
data model for;graphical representation of;spatial objects;spatial objects
data stream;xml query languages;data stream;query processing;query processing
multi-level;years ago;database;main memory;stored data;databases;memory utilization;database performance
web pages;web service;database theory;data extraction;industrial applications;theoretical results;logic-based;web data extraction;query languages
xml documents;ordered trees;core xpath;closely related;temporal logic
correct answer;sheds light on;document types;xml transformations;tree automata
xml documents;conjunctive queries over;active xml;query evaluation;web services
growing number of;web information retrieval;web pages;retrieval techniques;news search;web search engines;web search engines;hyperlink structure
classification;database;database applications;databases;partial rankings;rank aggregation;similarity search;meta-search;rank aggregation;algorithm to compute
world wide web;special case;algorithm performs well;dynamical systems;linear dynamical systems;information retrieval;ranking algorithms;link analysis;web searching
data-driven;web pages;web services;database;web sites;web site;web services;temporal logics;user input
data exchange settings;computational complexity;database systems;database design;np-complete;data exchange;source schema;schema mappings;fixed-point;information integration;fundamental problem;tuple-generating dependencies;schema mappings;schema mapping
query language;semantic web;language called;semantic web;resource description framework;query processing;databases;query containment;query languages
query language;topological properties;spatial datasets;spatial data;relational calculus
objects moving;spatiotemporal queries;general form;road networks
lower bound;replication;range query;real datasets;theoretical foundations;disk access;disk accesses;query response times;replication;databases;spatial data;theoretical bounds;range queries;load balancing
clustering;clustering performance;random walks;mixture model;similarity measures
frequent itemset;computational complexity;mining algorithms;database;np-complete;frequent itemsets;data mining
clustering;taking into account;projective clustering;clustering algorithm;dimensional data;local minima;objective function
approximate answers;multiple dimensions;multi-dimensional;data dimensionality;dynamic-programming;approximate query processing;error metric;error guarantees;query answers;user queries;approximation guarantees;relative error;large amounts of data;error metrics;absolute error
xpath queries over;lower bound;memory requirements;xml streams;query language;lower bounds;communication complexity;memory utilization;graph-theoretic
query evaluation;labeled trees;conjunctive query;conjunctive queries over;tree structure;wide range;conjunctive queries;node labels;conjunctive queries over;data management
query optimization;multi-dimensional;synopsis structures;plans;database;information-theoretic;cost estimation;queries involving;probabilistic guarantees;pre-computed;error propagation;space-complexity;join queries;space complexity;database systems
query optimization;cost functions;optimization technique;optimizer;22, 25;constraint satisfaction;database theory;database queries;decomposition methods;commercial dbms;instance;query plans;query-optimizers;conjunctive queries;cost function;query evaluation;structural properties
close connection;xml processing;regular expressions;context-free;type checking;static analysis;formal language;tree automata
data integrity;database;minimum number of;data privacy;np-hard;approximation ratio
data exchange problem;structural properties;data exchange;target instance;source schema;local structure;instance;relational calculus;query answering;query answering;target instances;data exchange
autonomous systems;taking into account;queries posed;data integration;query answering
sample set;provably optimal;multiple streams;traffic monitoring;adaptive sampling;environmental monitoring;error bounds for;sample points;pose queries;data streams;sampling scheme
generation algorithm;data stream management system;multiple streams;data stream systems;continuous queries;continuous queries
order statistics;order-statistics;data values;worst-case;communication costs;brute force approach;aggregate queries;network topology;communication cost;power consumption;sensor networks;sensor data;central server;algorithms for computing;network lifetime;algorithm to compute;total cost
space requirement;sliding window;limited space;algorithms require;randomized algorithms;variable-size;sliding windows;efficient approximate;fixed-size
query optimization;recursive queries;path expressions;conjunctive queries over;query classes;query containment;query containment;database systems
access patterns;answering queries;equivalent query;query classes;conjunctive queries
bounded treewidth;graph theory;database theory;relational database systems;finite model;select-project-join;finite structures;conjunctive queries
multi-valued;real-world;complete set of;data model;application domains;instance;operator;inference rules
range queries over;real valued;estimation techniques;synthetic datasets;kernel density;variable size;database;real-life;data exploration;query optimization;multidimensional datasets;local density;attribute values;multimedia databases;databases;real numbers;random sampling;data distribution
schema evolution;object-oriented;actual values;class hierarchy;dynamic environment;object-oriented databases
query execution;information systems;xml fragments;queries over xml;execution model;xml streams;performance gains;efficient querying;path expression;data streams;database engines;external sources;memory consumption
data objects;hash-based;storage devices;minimum number of;uniform distribution;storage capacity;algorithm called;low complexity
high-dimensional;intrusion detection;high-dimensional space;detection problem;outlier detection;data set;distance-based;dimensional data
computation cost;computing environment;query processing;computing environments;mobile devices
update processing;external memory;real-life applications;predicting future;wireless networks;indexing techniques;dynamic environment;large number of;query performance;mobile objects;moving objects;databases;index nodes;embedded systems;moving object databases;range queries
database;log records;database records;database updates;key-range;concurrency control
access method;real-life applications;aggregate information;range search;spatial objects;index structure;index nodes;maximum weight
database theory;formal language;alzheimer's disease
object-relational database systems;von neumann;database
acm sigmod
web service;web service;web services
clustering;clustering technique;database administrator;query response time;database
aggregate queries;query execution;data-stream processing;sliding-window;data streams
data types;spatiotemporal queries;object-based;pattern analysis;minimum set of
database research;database research
acm sigmod;databases;international workshop on;databases;international workshop on
acm sigmod;database;database;databases;databases
database;formal methods;programming languages;5;hardware architecture;atomicity;fault tolerance;transaction processing
data engineering;san diego;data engineering;sensor networks;mobile access;data management
aggregate queries;decision-support systems;4, 16;data sets;data warehousing;relational databases;21;22;aggregate queries;1;2;sensor networks;data analysis;mobile computing;data items;constraint databases;high energy physics;olap queries;12;information systems;data warehouses;database;databases
virtual organizations;virtual organizations;database research;information integration;databases;data security

world wide web;international conference on
semantic web;mobile devices

human-centered;human-centered
internet users;web sites;privacy policies;web site;lock;credit card;personal information
similar queries;semantically related;correlation coefficient;semantic similarity between;input query;wide range;search engine;search engine queries
duplicate detection;applications including;error rates;click streams;data streams;9
personalized recommendation;similarity metric;collaborative filtering algorithm;recommender systems
web applications;replication;data replication;web applications;performance gains;data-intensive;area network
web access;content distribution;increasing number of;large number of;low-bandwidth;multimedia content
distributed data sources;multiple queries;pull-based;database;query results;markov chains;data items;probabilistic approach;web data;pull based;query processing techniques;multiple sources;continuous queries;distributed data;decision making;continuous queries
web page;dynamically generated;result page;content features;search result;search engines;extraction accuracy;fully automatic;search engine;wrapper generation;web crawling;result pages
tree matching;web pages;web site;large number of;data items;database table;data records;machine learning;visual information;web data extraction;pattern discovery;web page;diverse domains;extracting data from
world wide web;semantic content;end-users;tree edit distance;semantic web;web content;web pages;html documents;web browser
clustering;ranking algorithm;commercial search engines;news sources;ranking measures;sliding window;search engine
search results;human-generated;web pages;automatic extraction of;relevance ranking;semantic similarity;semantic relationships between;information-theoretic measure;hierarchical structure;semantic relationships;semantic information;user studies;similarity measures
search results;web search;semantic associations;information-theoretic;ranking schemes;semantic web;relevance model;complex structures;complex relationships;knowledge base
execution environment;application integration;web services;instances;web services composition;goal-oriented;business process;web services;ad hoc;web service;application development
business processes;atomicity;control flow;web services;composite web services
web service;web service;web services;temporal constraints;consistency constraints
software development;reusability;xml applications;transformation rules;transformation rules;xml schema;xml document;low-level
web applications;high level;classification;driven web applications;business processes;modeling language;code generation
fine grained;xml based;wide range
web services;domain ontologies;service descriptions;semantic web;reasoning tasks
clustering;document structure;domain-independent;content providers;web browsing
service oriented;data representation;autonomous systems;case study;semantic web;real world
automatically generates;increasing complexity;web applications;programming model;type checking
semantic annotation;semantic indexing;textual descriptions;web pages;speech recognition;world wide web;knowledge management;meta-data;low quality;broadcast news;higher quality;semantic web;automatic speech recognition;semantic annotations;web news;information extraction from;temporal data;high precision;web interface
pruning strategy;compression methods;compression rate;storage overhead;pruning technique;search algorithms;improving web search;retrieval precision;pruning method;search engines;query processing;search effectiveness;search engine;average precision
search results;mining associations;web search engine;search-engine results;real-world;random samples;random sample;object-oriented;complex queries;search engine;faceted search;result set;efficient approximate;query results;web search engines;query terms
response times;query log;frequently occurring;inverted lists;search engine;caching scheme;lower level;web search engines;query processing in;data sets;search engines;level caching;query terms;web crawl;performance gains;index compression;web search engines;caching techniques;online algorithms for
xquery queries;evaluation strategies;queries over xml;original document;web services;query languages;xml query language
xml processing;programming languages;xml data;data representation;xml-based;low-level;xml schema;higher level
user queries;performance gains;semantic caching;semantic caching;query containment
context-aware;sensor networks;web services;design issues
web pages;learning systems;semantic web;semantic web;semantic relationships;query languages;navigation paths
data-driven;web service;data sources;decision-making
web-based;segmentation algorithm;success rate;topic segmentation
semantic annotation;pattern-based;web pages;large number of;semantic web;linguistic patterns;named entity;semi-automatic;generation process;pattern matching;semantic annotations
web sites;pattern mining;product features;customer reviews
semantic annotation;web pages;design method;design process;semantic knowledge;web sites;web engineering;semantic) knowledge;web page
search results;multi-lingual;statistical analysis;search engine
data flows;multi-step
search results;large volume;users' interests;web pages;web search;singular value decomposition;clickthrough data;real-world;multi-type;data set;higher-order;personalized web search;search engine;latent factors;web page
tuning parameters;web pages;search engine;search engine's;user experience;answer quality;user-centric;search queries;search engines;information access;search engines;user-centric;web page;web crawling;web data
intermediate results;proposed protocol;signature scheme
formal methods;trust model;prior knowledge;overlay networks;large-scale
generated dynamically;web pages;context-free;program analysis;dynamically generated
web corpus;web documents;large numbers of;large-scale;user queries;search engines;search engine;natural language-processing;web search engines;natural language
web portals;ir) techniques;ontology-based;information sharing;ir model;information retrieval;description logic;semantic web technologies;semantic information;semantic search;keyword-based search
clustering;web pages;link structure;clustering method;social network;agglomerative clustering;search engine;hand-labeled
hash function;computational power;increasing number of;brute force;financial services;web browser;allowing users to
web-based;agent based;privacy policies;privacy preferences;privacy policy;fine-grained;fine grained;privacy preferences
user visits;web browsers;user studies;context-sensitive;web browsing
web sites;measurement data;network monitoring;web applications
world-wide web;web applications;network flow;large-scale;valuable information;probability distributions;information flow;statistical analysis;data collected;web traffic
hybrid approach;web sites;large number of;web servers
service selection;selection problem;computationally hard
large-volume;content distribution;information dissemination;graph-based;diverse sources;instance;publish/subscribe system;additional information
real-life
spam detection;real-world graphs;21;web graph;mathematical analysis;transition matrix;markov chain
search methods;web objects;domain-independent;web search;user queries;application domain;current web;ranking results;document-level;object-level;object level;object-level;link analysis
markov chain;web graph;faster convergence;stationary distribution;performance gains;probability values
query formulation;search engine;instance;search engines;web browsers;web users
web pages;page segmentation;mobile devices;statistical model;ad-hoc;semantic concepts;ontology-based;web browsing;user evaluation;machine learning
web browsing;web content;web browsing;mobile users;storage devices
application area;rdf graphs;web application;case study;semantic web;information consumers;formally defined;task-specific
conceptual modeling;database;reasoning tasks;databases;semantic web;description logics;logic programming;semantic web;constraint-based;application scenarios
semantic web;development environment;large number of;description logic
similarity information;26;search algorithms;multi-step;monte carlo;database;19;similarity functions;20;similarity search;distributed architecture;link-based;hyperlink structure
locality-sensitive hashing;main memory;database systems;text corpora;performance guarantees;main-memory;data distributions;search queries;text data;skewed;communication cost;similarity search;indexing scheme;dimensional data;disk-based;web search engines
10;web graph;equivalence relation;np-complete;web graphs;2;keyword search;web page
incremental maintenance;maintenance algorithm;views defined;view maintenance;],*,//,vars. the algorithm consists of two processes. 1) the dynamic execution flow of an xslt program is stored as an xt (xml transformation) tree during the full transformation. 2) in response to a source [xml data
16;query language;relational database management systems;template-based;7;template based;application development;code base
web services;xml processing;web service;extensible markup language;application servers;xml documents;xml-based;xml document
web applications;type checking;application domain;data types;semantic web applications;temporal constraints;xml schema
xml documents;document type definitions;xml schema;regular expressions
semantic annotation;web pages;meta-data;xml based;web engineering;semantic web applications;xml content;semantic web;web applications;automatically generate;web application;xml schema
simulation results;information sharing;event-driven;large-scale;web servers;web server;memory size;competitive performance;multi-threaded
rates;low overhead;internet applications;internet applications;highly scalable;decision making
web service;automatically generate;web services;web services;composite web services
accessing data;xml data;data exchange;active xml;xml documents;real-life;web services;xml schema
nearest-neighbor search;skyline computation;progressive processing;database systems;database;skyline computation;user preferences;skyline points
special attention;access methods;replication;relational model;efficient computation;relational database systems;window functions;query optimizers;user groups;scalability problems;data cubes;operator;access structures;sql extension
query optimization;data collection;data acquisition;sensor devices;power consumption;sensor networks;query processing;query processor
computational complexity;graph theory;data exchange problem;data exchange;decision problem;source schema;data exchange settings;instance;answering queries;query processing;source data;algorithms for computing;target instances;data exchange;np-hard
set cover;result sets;np-complete;minimum description length;linear order;databases;data organization;multi) query optimization
database management systems;selectivity estimation;anomaly detection;frequent items;performance guarantees;database;data structures;data mining;synthetic data
tree-edit distance;document streams;vector space;worst-case;xml data;streaming xml data;sketching techniques;stream processing;streaming model;real-life;upper bound;tree-edit-distance;similarity joins;average-case
world wide web;program committee;search engines;international conference on
response times;web pages;visual attention;user interface;mobile applications;user study
network management;digital content;real-world;real-world situations;digital content;web content
world wide web;semantic web;semantic web;working group
information discovery;simple queries;meta-data;free form;large sets of;data centric;faceted search

mobile communications;cost-effective;mobile devices;automated techniques
web-based;web based
world wide web;user click
large scale;current web;active databases;information dissemination;agent systems;internet-scale;data management
web engineering;web technologies;web engineering;design process
service-oriented;web services;web services
search results;clustering;open-source;ranked list;hierarchical clustering;search engines;search engine;privacy preserving
supervised learning methods;enterprise search;classification model;svm models;classification;heuristic rules;ranking svm;baseline methods;regression model;retrieval method
web pages;current commercial;web graph;spam pages;ranking systems;page importance;seed set;ranking algorithms;ranking results;link-based;search engine;link analysis;increasing importance;web traffic
web pages;classification;link analysis;information retrieval;randomized algorithms;web page;times faster than
web pages;web search engine;23;web search engine;users' queries;helping users;linguistic information;return results;web searching;linguistic analysis;web search engines;natural language text
human users;web sites;web service;web sites;web service;web service;web services
supply chain;service oriented;complete set of;business transactions;supply chain management;wide range
breadth- first search;web pages;web graphs;search engines;web page;web crawling
stock market;search engines;search engines
broadcast news;user interfaces;data mining;online news;information retrieval
decision support;analytical model
mobile communications;mobile internet;visual content
machine learning;text processing;product search
web-search engines;web-search results;search engines;web-page;search engine
search engine
search algorithm;highly ranked;search efficiency;simulation results
high-speed;graph based;highly interactive;highly accurate;massively multiplayer online games;scalable distributed;huge number of
xpath queries;xml documents;entire document;fast retrieval;xml document;queries involving
xml schemas;xml instances;xml based;xml documents;case study;xml-based

optimal path;web pages;web usage;web navigation
xml data;raw data;multi-relational databases;xml documents;association rules mining;xml document;confidence levels
query patterns;data warehouses;clustering technique;data warehouse;query pattern;xml documents;queries issued;query pattern;patterns discovered
1;transition matrix;markov chain;web graph
external memory;web navigation;search process;visualization tool;search history;navigation paths
index size
service provider;related information;user-context;markup language
search results;web objects;pagerank algorithm;web pages;web documents;commercial search engines;search engines;multimedia objects;ranking mechanism
web portals;tree-structured;hierarchical clustering;automatic generation of
web based
access patterns;visualization tool;traversal patterns;graph-based
information flow
web pages;adaptive filtering;user feedback
multimedia applications;multimedia documents;optimization problem;web resources;resource management;capacity planning;optimal performance
evaluation model;search engines;web-based;search engine
large scaled;genetic algorithm;genetic algorithm;np-complete;directed graph;high quality;propagation model;bandwidth consumption;information retrieval;query routing;network topology
web applications;web engineering;web technologies;web application;web applications;rates;ad hoc
object-level;fine grained;web retrieval;object level;web retrieval
web documents;segmentation algorithms;web applications;information retrieval;naive bayes classifier;content information;visual information;semantic structure;web page;information extraction;segmentation algorithm
quality assessment;web sites;web site;end user;web site
web browser;web sites;web page;web site;web server
search algorithms;search engines;taking into account;power-law;search engine
network models;neural networks;web graph;neural network model;graph structures;neural networks
web based;2;search engine
web service discovery;large-scale;semantic matching;link-based;web services;service discovery
automatic generation of
incomplete information;link structure;algorithm called;web pages
related information;web content;web content

web pages;taking into account;high accuracy;classification;image-based;vector machine;text-based;accurate classification;tree structured;web image;web images
information systems;instances;web information systems;semantic web;development environment;data transformations;semantic web technologies
web pages;web users;real data;strongly correlated;2;link analysis;traffic data;web traffic;traffic analysis
document structure;web page;web pages
user navigation;mobile-internet;navigation patterns;predictive accuracy;making predictions;large sample;mobile-internet
information overload;long term
web services;programming languages;service-oriented;instance;service-oriented;web services;high-level;programming paradigm;programming language
web service discovery;web service;xml based;semantic web services;knowledge representation;agent systems;web services
end-users;web applications;user-interface;web site;web applications
web documents;web documents;user interface;relational information;web document;simple algorithm
web services;web service;data set;data sets;information visualization;web services
algorithms for computing;low-rank
semantic web;web service;information processing;information retrieval;multi-agent;semantic web;semantic issues;web resources;web services;web contents;information retrieval systems;semi-automatic
1;ranking scheme;markov chain;adjacency matrix
web-based;web forums;information extraction;search engine;implementation issues;extraction process
web pages;link structure;web site's;topic hierarchy;web site;web site's;tedious task
xml documents;modeling language
external sources;web content;web sites;web contents;web servers
domain knowledge;multi-step;domain ontology;takes into account;knowledge-based;semantic web services
clustering task;clustering method;entity extraction

network structure;instances;semantic web;query log;bayesian network
meta data;meta data;social networks
mobile applications;mobile environment;mobile computing;context-aware
database;database technology;transaction management
cut algorithm;web pages;visual cues;tabular data;html documents;web page
xml technologies
meta-search engine;search engines;open source
feedback controller;web server
web browsers;web based;web pages;web services
pre-defined;multi-class classification;hierarchical classification;classification methods;web content;labeled documents;hierarchical structure;class labels;automatically learning
web browsers;readability;web page;web browsing;web pages

xml schemas;semantic level;xml query;query language;xml query languages;xml documents
small groups;open source;provenance information
hybrid approach;sentence level;semantic tags;web pages;knowledge-based;information extraction;annotated corpus;context free;semantic web;training data;high accuracy;data quality;training corpus;knowledge-based systems
boolean queries;completeness;text search;xml queries;xquery implementation;keyword search;query results
web searches;content analysis
indexing approach;structural joins;main memory;structural join;indexing technique
link structures;link structure;open world;world wide web
term space;weighting schemes;text categorization;term weighting scheme;term weighting scheme;support vector machines;benchmark data sets
mathematical model;user profile;web-application;short-term
virtual environments;virtual environments;large number of;semantic web;resource description framework;world wide web
semantic web
multi-strategy;counterpart;semi-)automatic;ontology mapping;semantic relationships between;ontology mapping
web pages
web-sites;hit-rate;large number of;user behaviour;data-sets;web-site;access patterns
web based;web logs;finite state machine;web log
theoretical foundation
search engines;transition probability;click streams;search engine
relevance feedback;web search;web search;upper-bound;relevant documents
text classification;document-level;classifier;reuters corpus
design method;web applications;hierarchical clustering
high-level;users interact with;web applications;event-condition-action;web applications
visual similarity;visually similar
database;database entries;databases
xml documents;large document collections

highly dynamic;real-world;internal state;web server;web servers
service discovery;design principles;service selection;web services
neural networks;decision tree;ensemble methods;text classification;svm classifiers;support vector machines;weak classifiers
search engines;learning scheme;query routing;web search
question-answering;web contents
end users;application domain;query language;semantic web;user profiles;semantic relations;knowledge base
meta-data;automatically created;higher-level;lower level;data management system
web services;semantic web;real life;web resources;high-level language;composite event
meta-learning;web contents;web pages;web contents
web pages;rule-based;usage data;data processing;user-centric;ad hoc;traffic data

web service;model-based diagnosis;web services;fault management;web service composition
social network
web-based;web based;semantic content;frequency distribution;privacy threat;linguistic features
xml documents;xml schemas
accuracies;search topic;web search;temporal dynamics;probabilistic models
clustering;user community;collaborative filtering;real-world;data set;content information;share similar;gaussian distribution;user groups;model estimation
classification problems;nearest neighbor;large-scale;naive bayes;skewed;text classification;support vector machines
web directories;training data;classification;training examples;large-scale;classification performance;main idea;text classification
reusability;web-based;reference model;design principles;diverse set of;learning systems
clustering;web portal;content extraction;threshold values;search engine
semantic mappings;data sharing;xml schemas;automated techniques
labeling scheme;query nodes;xml queries;query performance;holistic twig join;twig pattern;twig query;xml database;pattern matching
business applications;security requirements;web services;service-oriented;service-oriented;security requirements;web services
pagerank algorithm;random walk;5;special case
clustering algorithms;web pages;information resources;web communities;web sites;information retrieval;content features;unified framework;machine learning
desired behavior;collection selection;real-world applications;text collections;information retrieval
web services;web services;transaction management
web service;web service composition;business process;web services;software components;main components
content delivery
clustering;retrieval performance;result page;high quality;selection method;information retrieval;data cleansing;ir research;feature analysis;web page
local search;classification;specific information;web applications;web resources;human activities;location-based;increasingly popular
fine-grained;ontology-based;ontology-based
clustering;phrase based;web pages;classification;vector machine;content information;svm) based
web content;user-friendly
rule-set
machine learning methods;random walk;stationary distribution;content delivery;web page;random walks
web pages;semantic web;early stage;semantic web;ontology language;data sets;resource description framework
building blocks for;web-based;modeling approach
xml documents;web applications;hierarchical structures;rule-based;visual interface;1;xml transformations;increasingly popular
web service;semantic web technologies;semantic web services;description logics;greedy algorithm
queries over xml
web-based;service-oriented;extensible markup language;schema integration
application level;web applications;mobile computing;low level;web application
cold start;ontology-based;ontology-driven;semantic web;user models;content management
text classification;data mining;wide range;online discussion;large-scale
deep web;deep web;web data
web search engine;dynamic environment;relevance judgments;search effectiveness;statistical tests;hypothesis testing;web search engines;test collection
optimization techniques;web services
domain specific;web sites;instance;human eye;data repository;meaningful information
2,4;web caching;computational resources
high-quality;web search;web search
efficient representation;web service;case studies;automatically generate;real world;composite web services
web documents;web pages;web log mining;traversal patterns;data mining;support threshold;markov chain
search engines;real data sets;decision tree;anchor text
user interfaces;xml schema
program analysis;context-sensitive;database;database queries;large number of;program analysis;highly optimized;web applications;binary decision diagrams;databases;context-sensitive;language called;data access
data exchange settings;regular expressions;data exchange;source schema;xml documents;instance;hierarchical structure;answering queries;basic properties;theoretical foundations;query answering;data exchange;relational data
satisfiability problem;data values;upper bounds;completeness;xml document
query languages
query rewriting;completeness;information-theoretic;query language;conjunctive queries;query languages
peer data management systems;database schemas;schema mappings are;database design;data exchange;database management;schema mappings are;relational schemas;information integration;high-level;schema mappings;metadata management
operator;intermediate results;relational algebra
2o(n, o(n;close connection;upper bounds;query language;combined complexity;query languages
incremental algorithm;block-based;ranking functions;information integration;operator;algorithms for computing;join operator
xml documents;security analysis;security policies;data protection;access control
query answers;private information;sum queries;data set;model called;posterior probability
clustering;11;total number of;computational power;database;principal component analysis;database entries;increasingly large;perceptron algorithm;databases;real-valued
extra information;original data;customer data;sensitive attributes
tuple generating dependencies;instances;data exchange;data exchange problem;block size;source schema;instance;source data;functional dependencies;algorithms for computing;data exchange;np-hard
tuple generating dependencies;computational complexity;target instance;np-complete;data exchange;data exchange settings;instance;conjunctive queries;special case
data exchange;data management;data integration;schema evolution
data objects;sufficient conditions;database;optimization problems;instance;databases;desirable properties;8;negative results
clustering;meta-search engine;local search;algorithm performs;correlation clustering;hierarchical clustering;objective functions;text-based;web searches;clustering algorithms
graph-based;oltp systems;isolation level;concurrency control mechanism;serializability;control mechanisms;application logic;serializable
queries over xml;query evaluation;xml streams;xpath queries;lower bounds
approximation methods;uniform distribution;approximate query processing;aggregate queries;error guarantees;approximation method
xpath queries;external memory;input data;distinguishing feature;main memory;lower bounds;streaming data;computation model;lower bounds;communication complexity;random accesses;intermediate results;9;auxiliary
network processing;computational power;data acquisition;operator;stream query processing;sensor networks;query processing;data streams;operator;takes place;queries involving;data transmission
approximate) answers;ip network;hash-based;continuous data streams;estimation problem;size estimation;duplicate elimination;high-confidence;instance;space usage;update streams;data streams;memory resources;network-monitoring;traffic analysis
complete information;data stream processing;data stream;high accuracy;massive amounts of data;answer queries;estimation techniques;communication networks
theoretical framework;type checking;theoretical foundation;intermediate results;xml query languages;document transformation;pattern matching;finite state;type inference
general case;active xml;data exchange;active xml;xml documents;high complexity;finite state;web services;xml trees;xml schema
query results;information integration;open-world;integration systems
statistical properties;digital data;private data;sharing data;census data;medical data;data privacy;privacy-preserving data publishing;database/privacy
synthetic data;size estimation
search cost;data sets;computation cost;similarity search;distance measures;similarity search;similarity query processing;time-series
multi-dimensional;heavy hitters;hierarchical data;data warehouses;multi-dimensional data;false positives;data set;data stream;data streams;heavy hitters;space complexity
concise representations;complete set of;frequent itemset;implication problem;relational databases;finite difference;inference rules
discrete event systems;large volumes of;query optimization techniques;databases;optimization techniques;data intensive
support levels;closed patterns;class label;mining frequent;model-free;data mining;class labels;risk factors
high-quality;acm international workshop on;data engineering;selection process;data engineering;san diego;mobile computing;data engineering;reference point;sensor networks;mobile access;acm international workshop on;databases;location-based;program committee;data delivery;data management;mobile access
streaming algorithms;high speed data streams;sampling-based;internet traffic;database;sampling algorithms;data stream;monitoring applications;network management;complex queries;algorithms produce;data stream management system;operator;sampling process;high speed;sampling algorithms;ip network;random sampling
node failures;distributed stream processing;fault-tolerance;trade-offs;operator;replication;distributed stream processing system;fault-tolerant
data-distribution;synthetic data;large-scale;storage space;communication network;aggregate functions;remote sites;monitoring applications;communication cost;approximation guarantees;traditional database systems;distributed-streams;worst-case;heavy-hitters;prediction models;distributed streams
randomized data;private information;original data;privacy preserving;principal component analysis;privacy-preserving data mining;pca-based
social security;real-life;databases
data objects;expected number of;computationally hard;partial information;sensitive information;decision makers;prior knowledge;knowledge discovery;decision makers;frequent set;anonymized data;benchmark datasets
query execution;taking into account;return results;database;optimization problems;takes into account;database;search problem;queries issued;mobile phone;query answering;helps users
attribute values;personal information management;information spaces;single class;real-world;diverse set of;personal information
user interfaces;semi-structured;general-purpose;information retrieval;growing importance;domain-specific;users' search;semistructured data
query execution;complex queries;optimizer;plans;data distributions;skewed;execution plans;actual values;query optimizers;estimation errors;subexpressions;plan choices
query optimization;multi-dimensional;query optimizer;database;query execution;estimation technique;cardinality estimation;optimization process
query optimization;relational algebra;database;query engines;plan space;execution model;relational database systems;sampling-based;data model;relational model;query processing;query plans;operator;principled framework;rank-aware;plans
attribute values;low cost;database;np-complete;equivalence classes;minimal-cost;real data;cost-based;greedy algorithms;multiple sources;record-linkage
rewritten) query;sql queries;query answers;data consistency;databases;commercial database systems;key constraints
data transformation;relational schemas;prior model;instances;language-independent;database schemas
query execution;optimizer;database systems;materialized view;microsoft sql server;materialized views;select-project-join;base tables;microsoft sql server
relational database system;nested queries;relational algebra
false positives;classification;classification;query evaluation;attribute domains;skyline queries;partially-ordered;margin
information systems;information filtering;database;information filtering;index structures;update streams;context-aware;adaptive indexing
access paths;database systems;database tuning;physical design;input query;increasingly complex
information systems;synthetic data;database;performance goals;materialized views;exploratory queries
privacy guarantees;privacy preserving;data perturbation;privacy-preserving;perturbed data
data analysis;database systems;computationally hard;data warehouse;efficient computation;large number of;commercial database systems;data analysts;typically requires
disk-block;real-world datasets;database applications;massive datasets;wide range;data streams;multidimensional data
synthetic data;frequent items;real-world;energy-efficient;tree-based;sensor networks;worst case;sensor network;rates;extensive simulation;path-based
greedy heuristics;data analysis;naturally leads to;synthetic data;optimization problems;data streams;optimization problem;traffic data;high speed data streams;data stream management systems;user-queries;efficiently computing;aggregation queries
query evaluation;evaluation strategy;memory usage;easily extensible;aggregate queries;memory space;data stream;evaluation techniques;data streams;operator;input streams;stream systems
index structures;range query
concurrency control mechanism;increasing demand for;basic algorithm;user transactions;key range
sql/xml;xml query languages;relational systems;data management;xml database;query languages;relational database management system;relational data
expected number of;attribute values;statistical properties;solution space;stationary distribution;result tuples;data streams;input streams;random walks
theoretical analysis;remote sources;join results;data distribution
query evaluation;multiple queries;materialized views;intermediate result;sharing opportunities;commercial dbms;instances;operator;relational dbms;concurrent queries;query engine
large volume;fall short;index entries;indexing techniques;query response time;theoretical analysis;indexing methods
completeness;access control;data publishing;user queries;relational databases;query results;join queries
response times;cluster based;database systems;fault-tolerance;isolation level;replication;data replication;data consistency;fault-tolerance;database replicas;tuple-level;concurrency control;update transactions
data values;data source;duplicate detection;similarity measure;real-world;detection method;data model;xml elements;complex data;relational data;duplicate detection
source schema;xml document;semi-structured data;path-expression;incremental maintenance;view maintenance;path-expressions;xml documents;materialized views;incremental maintenance;specification language;views defined;main features;source data;auxiliary data;typically requires;relational data
13;15;source documents;intermediate results;indexing techniques;xml documents;twig pattern;3;2;xml document;twig patterns;8;pattern matching;xml query processing
high-dimensional;spatial proximity;real-life datasets;concept called;clustering process;correlation clusters;databases;dimensional data
query evaluation;mobility patterns;wireless communication;spatial queries;communication cost;generic framework;location update;moving objects;query results
edit distance;detection techniques;distance functions;distance function;moving object;edit distance;similarity search;rates;similarity-based;pruning techniques;pruning power
business data;xml data;xml query;xquery engine;query language;data model;aggregation queries;relational systems;working group
xml updates;structural join;efficient query evaluation;ad-hoc;real-world;large number of;labeling schemes;query performance;xml documents;algorithms require;query processing;xml database;join algorithms
world wide web;13;labeled trees;xml documents;xml databases;html documents;keyword search;keyword search;user-friendly
data-driven;web applications;temporal properties;database;driven web applications;common properties;optimization techniques;interactive applications;model checking;database queries;high-level
search results;high-quality;4, 8;ranking function;search engines;search engine;major search engines;quality metric;formal framework
input data;probabilistic guarantees;join algorithms;query processing;disk-based;aggregate function
long-running queries;sql queries;worst-case;estimation problem;worst case;error bounds;estimation algorithm
end-user;approximate query processing;database systems;database;database queries;statistical estimation;probabilistic guarantees;data-base;estimation accuracy;analytic processing;incomplete data;sampled data;inference problem
network analysis;data analysis;sensor monitoring;stream mining;realistic data;naive implementation;rates;times faster than;data streams;theoretical analysis;relative error;object tracking;resource consumption
memory footprint;memory bandwidth;taking into account;large data streams;stream mining;stream-processor;cpu-based;data streams;sliding windows;mining data streams;data transmission
mining algorithms;frequently occurring;dna sequences;interesting patterns;case study;periodic patterns
data objects;query retrieves the;nearest neighbor;multiple continuous queries;dynamic environments;query points;nn queries;query point
monitoring applications;optimize queries;algorithms for computing;query processors;continuous queries
continuous queries over;potentially infinite;classification;data structures;plans;aware query processing;query semantics;query operators;query processing in;query processing;data streams;aware query;data stream systems;input streams;sliding windows;continuous queries
training data;gene expression data;19;rule-based;classification methods;bioinformatics datasets;classification method;discovered rules;mining algorithm;gene expression;classifier;association rule mining algorithms
clustering;application domains;internal structure;distance function;database;similarity measure;time series;piecewise linear;similarity matching;data stream;problem domain;matching process;databases;rule discovery;pattern matching;internal structure;finite state
parameter values;overlapping clusters;expression patterns;microarray data;graph-based;compact representation;algorithm called;microarray datasets;gene expression;clustering quality
distance measure;approximate nearest neighbor;vector space;database;image database;query-sensitive;trade-offs;query-sensitive;retrieval accuracy;time-series;computationally expensive;databases;handwritten digits;embedding methods;large database;distance) measures
video databases;spatio-temporal;graph matching;edit distance;query loads;tree-based;subgraph isomorphism;indexing method;temporal features;clustering algorithm;spatio-temporal;data structure;np-complete;data structures;index size;data structure called;metric spaces;distance measure;computational complexity;graph-based;spatial objects;spatial features;tree structure;video data;multimedia data;graph indexing
network bandwidth;high-dimensional;video sequence;total number of;feature vector;database;similarity measure;similarity measurements;similarity search;efficient indexing;user interests;high complexity;fast retrieval;video content;video sequences;optimal performance;video database;model called
scheduling strategies;duplicate elimination;cost-sensitive;instances;operator;path queries;sequential scans
computational complexity;lower bound;similarity measure;computation cost;tree-structured data;tree edit distance;tree-structured data;edit distance;massive datasets;similarity search;high complexity;structure information;similarity query processing;theoretical analysis
search performance;database systems;efficient search;exact matching;feature sets;similarity search;feature selection;complex structures;pairwise similarity;structured data;graph databases;filtering algorithm;feature set
data warehouse;information integration;multiple data sources
multiple sources;data integration;database
large enterprises;meta data;data integration;configuration management
accessing data;information discovery;domain-specific;data sources;information consumers;enterprise systems
application integration;configuration management;meta-data;data exchange;mapping generation;business intelligence;meta-data;information technology
xml queries;relational schemas;lessons learned;query languages;implementation issues;schema mappings;graph representation
large volume;access control;database;data model;wide range;unstructured data
web services;database
structural information;xml technologies;schema design;data structure;databases
data warehouse;data structures;data sources;data stored in;pose queries;query languages;efficient sql
sql/xml;xml query;tightly integrated with;xquery processing;xquery processing
recursive queries;average case;transitive closure;sparse graphs;selection conditions;sparse graphs
distributed environments;high-volume
data transformation;data sources;service oriented;data access;cost effective
enterprise application;service-oriented;1, 2;information technology;metadata management
conceptual modeling;application integration;business processes;web service;web engineering;industrial applications;web applications;web services;web applications;code generation;data-intensive;multi-party
service oriented;database systems;sql server;database architecture;database engine;microsoft sql server
database technology;feature set;event processing;real-world;real world;oracle database
database;application servers;instance;databases;application server;scheduling algorithm
11;mining data streams;high level language
design principles;large databases;databases
structural patterns;mining frequent patterns;graph mining;pattern-mining;real data sets;pattern mining;constraint-based mining;mining algorithms;index structure;mining frequent;broad applications;5;constraint-based;disk-based;graph databases
query optimization;multi-player;stream processing engine;resource management;stream processing;high availability
data management problems;large-scale;real-world;business logic;event detection;complex events;data generated from;high-level;low-level;complex event processing
data sharing;access control rules;1, 6;database;sharing data;access control;database servers;service providers;data encryption;data dissemination
large numbers of;sql queries;query semantics;data sources;multiple data sources;3;data item;poor quality;false positives
approximate answers;data analysis;wavelet transformation;scientific applications;web-services;multidimensional datasets;statistical analysis;web-service;data storage
document-centric;8;xml documents;10;databases
keyword based search;complex queries;database;natural language interface;query interface;application domains;xml database;query languages;natural language
selection predicates;database;data model;integrated environment;xml query languages;xml documents;schema mappings;input/output;schema information;xml query language
user interactions;real-world;data representation;3;ontology matching;xml schema;graphical interface
web services;application requirements;data warehouses;data transformation;legacy systems;data exchange;instance;xml transformations;data transformations;databases;multiple information sources
formal framework for;database management systems;vice versa;large scale;mapping problem;structured data sources;token-based;schema matching;ad-hoc;information flow;relational data;heuristic search;relational data;information sharing
world wide web;data residing;information systems;14;information sources;distributed databases;application domains;information integration;data warehousing;huge number of;global schema
duplicate records;attribute values;domain-independent;real-world scenarios;data warehouse;data cleaning;decision support;data quality problems;data quality;data cleansing;microsoft sql server;data mining;microsoft sql server;data entry
1, 8, 6;personal information management;personal information management;information management;data management
data-driven;piecewise-linear;linear model;database;indexing structures;compressed representation;clustering method;motion sequences;modeling approach;databases;classifier;human-motion
query interfaces;online databases;deep web;2;data access;databases;web sources
index tuning;sql server;database design;database tuning;physical design;materialized views;database tuning;tuning tools;relational database management system;microsoft sql server
optimizer;database;db2 udb;rates;plan choices;query optimizers;dynamic databases
complex queries;open-source;database;data distributions;execution plan;skewed;query execution plans;5;plan choices;actual values;query optimizers;subexpressions;execution plans
database;storage structure;sql server;sql server;moving objects;databases;immortal db;concurrency control
database schema;database;source data;databases;relational databases;additional information
large amounts of;database systems;xml data;relational database management systems;xml database systems;semistructured data;markup language;database vendors
sql/xml;xml storage;query processing;schema evolution;document-level;data type
enterprise wide;application integration;indexing methods;xml content;content management;diverse domains;application server;oracle database
xml storage;database management;sql server;relational database management systems;relational data;microsoft® sql server
random graphs;finite model;database research;database applications;data exchange;data mining;data acquisition;information extracted from;exploratory queries;sensor networks;ir-style;databases;query answering;information disclosure;diverse range of;data integration;data sources
information systems;database tuning;cost-effective;total cost;information technology;total cost
image databases
data mining;visualization techniques;case studies;databases
open source;commercial search engines;search engines;open source;data mining
evaluation methodology;text collections;ad hoc;retrieval task;evaluation measures
database;information retrieval;scientific literature;test collections;high-throughput;information access;massive amounts of data
1;plans;complete information;retrieval techniques;text retrieval
multimedia databases;acm international workshop on;acm international workshop on;multimedia databases
web applications;acm international workshop on;international conference on;database;web information and data management;acm international workshop on;knowledge management;end users;web information and data management
multimedia retrieval;video retrieval;retrieval effectiveness
information retrieval research;information retrieval research;information retrieval
relevance feedback;relevance feedback;relevance feedback;vector space model;xml retrieval
ir systems;extensible markup language;xml tags;xml elements;relevant document;logical structure;digital libraries;data repositories
ad hoc;natural language;retrieval systems
relevance feedback;relevance feedback;object-based;object-based;information retrieval
world wide web;skewed;search engines;web crawling;information retrieval methods
personalized search;general-purpose;machine learning;search engines;information overload;artificial intelligence
music information retrieval;music retrieval;information content;information retrieval
learning environments;information retrieval
language model;information retrieval;instances;language modeling;search engine;query terms
ir) models;vector-space model;information retrieval;large amounts of data;image acquisition;information retrieval
semi-automatic
summarization method;document genre;document genre;source documents
web information retrieval;short queries;text retrieval systems;retrieval effectiveness;word sense disambiguation;word sense disambiguation;web retrieval
probabilistic models;information retrieval;textual description;multimedia retrieval
world wide web;gather information;relevance information;information retrieval;relevance feedback;ir) systems;implicit feedback;interactive information retrieval
clustering;multimedia content;video retrieval;text queries;visual features
category specific;object category;image segmentation;category specific;instances;markov random fields;graph cut;object categories
feature points;object recognition;nearest neighbor;object recognition;quadratic programming problem;classification rate;shape matching;cost function;matching cost;shape matching
computational efficiency;patch-based;input data;large amounts of;video data;learning algorithm;large number of;missing data;input images;probability models
object recognition;object class;object parts;similarity-based;visual appearance;automatically learn
object recognition;statistical models;lower computational cost;geometric structure;computational cost;tree-structured;recognition performance;object classes;spatial structure;spatial constraints;statistical models
image classification;learning algorithm;decision trees;classification method
image quality;low resolution;image pixels;image data
human vision;video camera;dynamic events
model complexity;linear model;finite element method;learning problem;active appearance models;dynamic scenes
tracking algorithms;multi-modal;object recognition;estimation method;random variables;camera motion;diverse set of;activity recognition;image pixels;spatial context;graph cuts;dynamic scenes;object detection;moving objects;high levels of;object detection;dynamic scenes;detection accuracy
15;gradient based;partial differential equations;low level vision;pattern recognition;density functions;image restoration
classification;remote sensing;hybrid method;wide range;classification method;pre-processing
modal analysis;spatio-temporal;user-defined;spatial resolution;canonical correlation analysis;visual information;linear programming;audio-visual
small groups
image retrieval;exhaustive search;vision applications;probability distributions;accuracies;information theory;data structures;database;shape retrieval;speed ups

color image;decomposition method;basis functions;local) color;input image;basis functions;color segmentation
markov network;belief propagation;tensor-based;markov random fields;images captured;wide range;belief propagation;dense set of
detailed comparison;refinement process;energy minimization;noisy data;graph cut;uniformly distributed;graph cut;wide range;dense set of;matching cost;map estimation
physics-based;image database;statistics-based;correlation method;combined method
feature points;segmentation methods;embedding methods;perspective images;higher-dimensional;perspective images;theoretical results
object boundary;image segmentation;edge-preserving;edge preserving;graph-theory;classification problems
partial differential equations;target image;locally optimal
multiple images;shape information;matching accuracy;feature descriptor;global context;global context;local descriptors
kernel density;similarity measure;spatial information;similarity measure;target image;image sequences;tracking algorithm;linear order;color histogram;similarity measure between;kullback-leibler divergence;feature spaces;object tracking;local minima;motion models;similarity measures
corner detection;corner detection;general setting;detection methods;medical images;optical flow
low rank;low-rank;missing data;missing data;perspective camera;base-line
multi-view;multi-view
linear combination
similarity measure;random samples;matching method;similarity function;worst case;algorithm exploits;computational savings
real-time tracking;kernel-based;high dimensional;probability density functions;kernel-based;particle filtering;video sequences;density functions;object tracking;automatically determined;high dimensional space;monte carlo
hand tracking;propagation model;particle filtering;sequential monte carlo;particle filtering;hand tracking;bayesian formulation;tracking method
real-world;tracking algorithm;data collected;markov model;dynamic environment
image sequence;dynamic scenes;moving object;moving objects;detection problem
optical flow;segmentation method;motion sequences;motion segmentation
temporal dependencies;video frames;probabilistic framework;conditional random field;contextual constraints;filtering algorithm;image sequences;observed data;image sequences;object segmentation;motion cues;video sequences;segmentation method;efficient approximate;object segmentation;conditional random field
large scale;appearance model;long sequences;basic assumption
feature extraction;personal identification;personal identification;image regions;effective classification;higher accuracy;error rate
level-set;multiple objects;basis functions;medical images;level set;constraint-based;desirable properties;finite-element;constraint-based
positive results;kernel-based;similarity measure;tracking objects;kernel-based;image regions;moving objects;transformation parameters;tracking method;object tracking;measure called
operator;appearance models;accurately predict
motion analysis;matching algorithm;motion estimation;articulated objects;perspective images;point correspondences;articulated object;real images
clustering;line correspondences;clustering technique;moving objects;motion segmentation;line correspondences;dynamic scenes;motion segmentation
real-world;multi-view;feature point;scene points
line segment;complementary information;stereo matching;broader range;line segments;line segments;topological structure
document images;optical character recognition;document images;document image;geometric model
fast marching;surface normals;human knowledge;global solution;local patches;energy functional;synthetic and real images
matching algorithms;depth map;explicitly modeling;graph-theoretic;multi-camera;cost function;matching cost
regularization;learning algorithm;original data;classification;boosting algorithm
information content;image features;color information;salient features;saliency detection;local neighborhood;feature detectors;salient features;color image
discriminant analysis;feature set;face detection;multi-view;eye detection;multi-view;object detection;eye detection;fisher discriminant analysis;classifier;brute-force
classification;object category;background clutter;globally optimal;category recognition;feature detectors;instances;sparse representation;feature types;semi-supervised;efficient learning
tracking algorithms;motion capture;nearest-neighbor;propagation algorithm;human motion;observation model;motion estimation;regression methods;video sequences;data-base;inference algorithms
computational complexity;statistical models;computationally hard;particle filter
shape space;motion analysis;statistical models;human movement;linear combination;spectral analysis;human activities;motion capture data
dimensional space;similarity measure;motion field;motion estimation;video sequences;video segments
iterative) algorithm;linear models;prediction errors;human motion;model parameters;human motion;linear systems
variational formulation;image features;distance function;finite difference;level set;level set function;level set;energy functional;object boundaries;real images;curve evolution;variational formulation
image segmentation;wavelet-based;real data;real-valued;wavelet-based;observation model;em algorithm;semi-supervised;fast algorithms;image segmentation
real images;scale-invariant;texture segmentation
real images;image segmentation;image segmentation;image gradient
additional cost;level sets;shape analysis;level set
multi-modal;feature level;similarity measure;multiple views;multi-view;multiple view;personal identification;multiple classifiers;personal identification;similarity computation;level fusion;level fusion;human faces;integrates multiple
discrete cosine transform;aerial images;error metric;level set;1;curve evolution;aerial images;level set
classification problems;optimization design;post-processing;face detector;optimization algorithms;high-level;classifier
approximate nearest neighbor;nearest neighbor classification;nearest neighbor;trade-offs;classification accuracy;retrieval accuracy;database;euclidean spaces;shape context;distance measures;nearest neighbor classification;wide range;computationally expensive;vector space;brute force search;classifier;similarity measures
training set;training examples;training data is;generalization performance;aggregation methods;regularization;human faces;training sets;object categories;fully automatic;training datasets;semi-supervised;labeled examples;object categories
probabilistic modeling;face images;face recognition;pose variation;test images;probabilistic model;database;robust face recognition;face recognition;local patches;probabilistic approach;face image
scale-space;low frequency;multi-view;spatial distribution;intensity values;multi-scale;image matching;feature descriptor;nearest neighbour
closed form solution;image space;manifold learning;extra information;prior knowledge;euclidean distances;low dimensional;information derived from;cost function;dimensional data;image data
face images;discriminant analysis;cluster-based;learning algorithms;lower-dimensional;higher-order;supervised dimensionality reduction;feature selection;computational cost;classification problems;higher order;small sample size
class labels;density estimation;mutual information;face detector;boosting framework
learning process;face verification;loss function;high degree of;similarity metric;input space;training samples;face database;similarity metric;face verification
face recognition;independent components;scene structure;facial image;multiple factors;test image;independent components
face recognition;face databases;vector machine;discriminant analysis;real-world applications;fisher discriminant analysis;kernel methods;margin;excellent performance;support vectors;face database
object recognition;edge detection;feature detection;equivalence classes;discriminative power;vision algorithms;color space;feature detectors;detection results;weighting scheme
kernel density;multiple objects;intermediate results;particle filter;data association;visual tracking;density function;image sequences;particle filter;multiple object tracking;multiple object tracking
clustering;data set;face recognition;feature vector;linear subspace;high dimensional;face images;1;subspace analysis;subspace analysis;low dimensional;mixture model;optimal parameters;mixture models;local linear;face database
face images;face recognition;image sets;recognition rate;face recognition;data set;parametric model;imaging conditions;single image;image sets;low-dimensional;recognition problem
markov random field;energy functions;input image;labeling problem;graph cut;input images;multi-class
image retrieval;efficient indexing;content-based music;local descriptors
monte carlo;data association;detection algorithm;single target;tracking applications;particle filter;markov chain;auxiliary
training data;event detection;baseline methods;hidden markov model;event detection;data streams;semi-supervised;semi-supervised;audio-visual
density estimation;tree-based;data-structure;joint probability;tree models;multi-target tracking
scale-space;automatically selecting;wide range;cross-validation;image denoising;computational cost;image denoising;noise levels;natural images;model selection
dynamic environments;image analysis;incoming data;eye movements;visual analysis;low-level;video streams
training set;training data;image patches;inference problem;propagation algorithm;random samples;graphical model;confidence scores
singular value decomposition;kernel-based;subspace learning;face databases;kernel discriminant analysis;subspace learning;spatial structure;dimensionality reduction;kernel-based
unsupervised classification;learning mechanism;input data;topological structure;learning mechanism;probability density;similarity threshold;data distribution
semantic concept;extraction task;semantic video;unlabeled data;large scale;multi-view;data set;large number of;algorithm called;semi-supervised learning algorithms;training data;manually annotated;semantic concepts;training samples;concept detection;semi-supervised learning;multi-view;concept detection;semi-supervised;semi-supervised;unlabeled set
classification performance;object recognition;object classes;discriminative models;training examples;single class;classification tasks;object parts;generative models;generative models;learn models;visual recognition;object categories;multi-class;prior knowledge;object classes;learning method
classification scheme;classification tasks;classification performance;similar features;classification method;performance gains;classification task;object categories;selecting features
multiple classes;multi-class classification;classification problem;face detection;decision boundary;boosting algorithm;face detection;multi-class;lighting conditions;multi-class boosting;learning framework
image patches;classification;database;scene categories;test image;scene recognition;image content;spatial context;classification rate;33;image understanding;semantic categories;scene classification;image region;lighting conditions;probability density;mixture models;classifier;loopy belief propagation
classification;unlabeled data;classification decisions;classification;training examples;performance gains;training set;background modeling;incoming data;moving objects;detection methods;principal component analysis;labeled examples;classifier trained on;classifier
benchmark data;object class recognition;object model;classification performance;appearance model;training error;probabilistic semantics;boosting algorithm;object class recognition;instance;model parameters;learning problem;supervised learning;classifier
training images;local features;spatial relations;object categorization;scale invariant;object classes;generative model;object categories
multiple classes;exhaustive search;hypothesis space;object instances;binary classifiers;object detection;handwritten digits;classifier
generative model for;object appearance;generative model
clustering;unlabelled data;image segmentation;classification;unlabelled data;pairwise constraints;optimization process;model fitting;maximum entropy;cost function;class labels;semi-supervised;classification problems;soft constraints
matrix factorization;singular value decomposition;convex programming;minimization problem;convex programming;real data;prior knowledge;matrix factorization;missing data;missing data
probabilistic segmentation;gaussian mixture model;applications including;color information;image deblurring;color transfer;image space;probabilistic segmentation;optimal number of;probability distribution;algorithm produces;color transfer;natural images;expectation-maximization
cut algorithm;automatic segmentation;similar objects;interactive segmentation;positive results;graph cut;shape priors;graph cut;natural images;shape priors;user input
14;theoretical properties;8, 24;graph cuts;prior models;algorithm requires;probability density;segmentation algorithm;image segmentation
robust estimation;point correspondences;algorithm exploits
digital cameras;intrinsic parameters;aspect ratio;projection matrix
equivalence class;multiple images;light source
similarity function;operator;algorithm to compute;feature detection
low-cost;sensor data;motion estimation;multiple sources;feature tracking;feature selection
physics-based;shape recovery;dynamic environments;dynamic environment
error rates;large number of;similar results
multiple target;search process;data points;higher level
hierarchical organization;high-level;modeling task;classification;activity recognition;generalization error;training data;hidden markov model;human activities;markov model;markov model
kernel based;state space;classification problems;classification;classification;spatio-temporal;vector machine;classifier;databases;kullback-leibler divergence;auto-regressive;image space;auto-regressive
appearance model;face recognition;instance;image-based;learning algorithm;generic model;video sequences;online learning;transition probabilities;video-based;face tracking;video-based;human faces;low-dimensional
closed world;face recognition;test image;refinement step;partial occlusion;high recall;rates
low-dimensional representation;video frames;semi-supervised learning;manifold learning;supervision;temporal coherence;dynamic scenes;low-dimensional
data set;pixel level;small training sets;real-world scenes;high precision
object recognition;high-quality;human detection;svm based;database;gradient based;human detection;feature sets;pose variations;test case
learning process;discriminant analysis;classification problem;unsupervised learning;classification;illumination conditions;real data;matching problem
shape prior;object recognition;image information;image segmentation;sampling algorithm;probabilistic framework;shape similarity;shape prior;image regions;stochastic process;1;matching process;synthetic and real images;shape retrieval;markov chain;object segmentation;monte carlo
input image;illumination conditions;indexing methods;database
mapping function
dynamic programming algorithm;dynamic programming;stereo matching;stereo matching;error rate;processing speed
algorithm performs;probabilistic framework;real-time tracking;computationally intractable;face tracking;observed data;visual tracking;solution space;particle filter;kalman filter;objective function
theoretical foundation;markov network;target tracking;linear complexity;video sequences;object detector;fixed point;target tracking
face recognition systems;error rate;face recognition;face recognition;high resolution
image retrieval;image retrieval;data collection;retrieval results;data set;retrieval algorithms;retrieval systems
monte carlo;real-world;state-space;partial occlusion;fully automatic;particle filter;observation model;bayesian framework;markov chain
multiple objects;tracking results;dynamic scenes;prior knowledge;multiple objects;video sequences;dynamic scenes
multi-camera;fusion method;ground-based;multi-camera
graph theoretical;action based;action recognition;temporal properties
high-confidence;lock;appearance models;object appearance;tracking applications;fully automatic;unified framework;tracking results

face recognition systems;face recognition;sketch recognition;locally linear embedding;face recognition system;local linear
semi-supervised learning;object detection;object detection;large variations;semi-supervised learning algorithms;great promise;object detection;context-based
gaussian noise;neural network;test images;image denoising;image denoising;squared error;noise levels;natural images
tracking algorithms;deformable objects;markov network;local shape;statistical model;computationally tractable;algorithms for computing;model training;shape variations
structural information;natural language processing;information-theoretic;weighted graph;video stream
deformable objects;instances;filtering algorithm;algorithm combines
large numbers of;geometric structure;point correspondences;image pairs
error-prone;data representation;completeness;depth estimation;feature matching
image features;image noise;vision problems;operator;model fitting;image noise
multiple cameras;detection process;stereo matching;geometric information;specular reflections;error analysis;moving objects;background modeling
vector machine
multi-camera;pose estimation;multiple images
depth map;depth map
algorithm generates;mixture model;search process;motion models;real images;robust estimation;motion models
density estimation;vision algorithms;user supplied;cost function;classifier;projection based;times faster than
image segmentation;vector machine;image categorization;feature mapping;multiple-instance learning;region-based;instances;low-level features;distance function;class label;image categorization;classification algorithm
stereo-vision;mobile robotics
human face;free form;principal component analysis
unsupervised learning;multiple objects;video sequences;low resolution;object models;input sequence;input images;connected components;video sequences
spectral clustering;similarity measure;similarity matrix;clustering methods;tensor decomposition;motion segmentation;similarity measures
feature extraction;object identification;hierarchical model;graphical model;scene analysis;multi-camera;object detection;dynamic bayesian network;expectation propagation;robust estimation
learning phase;unlabelled data;recognition rate;learned models;body parts;probabilistic models;motion model;human motion;faster convergence;probabilistic model;human motion
clustering;ground truth;temporal properties;semantic labels;database;visual concepts;video annotation;ground "truth;similarity measure;news video;machine learning;visual concepts;feature selection;natural language processing;accurate tracking;video annotation;low-level;information gain
optical character recognition;readability;scene text
11;background model;gaussian mixture model;video surveillance;color images;video surveillance;motion information
pose variation;motion analysis;sparse set of;motion parameters;point correspondences;shape matching
markov random field;training images;generative model for;prior knowledge;algorithm called;4;single image;mrf model
machine learning methods to;classification approaches;brain images;machine learning;human brain;machine learning techniques;magnetic resonance imaging
pre-computed;additional information
tracking algorithms;image based;static images;particle filters;energy functional;level set;particle filtering;higher dimensional;filtering algorithm
tracking algorithm;density based;graph cuts;flow field;graph cuts
visual tracking;visual tracking;cost function;image regions;matching score
training phase;multiple cameras;camera parameters;map) estimation;low dimensional;real world scenarios
regularization;fast algorithm;level-set;multiple objects;level sets;level set;partial differential equations;real-time tracking;object boundaries;video tracking
image-region;statistical properties;synthetic data;information-theoretic;higher-order;entropy measure;image collections;information-theoretic;image processing
camera motion;unified framework;unified framework
image denoising;image denoising
prior model;single-image;incomplete data;automatically determined;instances
low frequency;vector field;generative model for;data set;generative model;high frequency
surface reconstruction;energy functional;local features;regularization;level set
graph partitioning;ground truth;image segmentation;average case;medical images;segmentation method;shape priors;optimization procedure;shape models;prior information;weighted graph
distance measure;exemplar based;mutual information;feature level;object level;feature sets;prior information;image patches;feature set
tracking algorithms;global model;image features;probabilistic framework;image noise;feature extraction;dynamic model;feature values;graphical model;tracking algorithm;generative model;feature tracking;filtering algorithm
video sequence;autonomous navigation;ground truth
data matrix;noisy data
multiple views;incomplete data;stochastic sampling;color information;stochastic sampling
high-resolution;high quality;image registration;range scans;images captured
discriminative training;discriminative training;object recognition;object classes;image patches;recognition performance;image patch;error rates;log-linear models;image patches;error rate;automatically learning
object recognition;12;face recognition;kullback-leibler;object recognition;strong classifier;2;selected features;learning method;7;5;binary pattern;13;weak classifiers
error rate;density estimation;false positive;detection problem;error rates;decision-making;detection rates;joint probability;density functions;classification problems;optimal strategy
multiple instance learning;image segmentation;image regions;recognition performance;test images;probability distributions;training images;learning problem;image annotation;image annotations;image annotation;retrieve images
object classes;discriminative learning;classification;graph-cut;large-scale datasets;maximum-margin;markov random fields;automatically learn
kernel functions;image pixels;training examples;kernel functions;kernel function;kernel functions;automatically constructed;prior knowledge;explanation based learning;level features;high-level;learning task
image retrieval;scale-space;kernel-based;image regions;database;image description;image sets;image description;feature spaces;optimization techniques;image segmentation
classification;temporal information;correlated features;conditional random fields;model averaging;error rates;hand-drawn;feature selection technique;posterior distribution;maximum likelihood;conditional random fields;recognition problem
discriminant analysis;face recognition;kernel selection;selection problem;overfitting problem;unified framework;discriminant analysis;higher dimensional
lower dimensional;classification;image database;image analysis;principal component analysis;video sequences;higher dimensional;higher order
illumination conditions;linear subspace;input image;single image;face recognition;recognition rates;multiple sources;wide range;single image;lighting conditions;human faces;low-dimensional
lower level;information extracted from;facial expressions;low level;facial expressions;latent variable models;statistical models;human face
object recognition;local features;svm classifiers;database;feature representations;object recognition;local features;recognition tasks;multiple types of;competitive performance;local image
image retrieval;image retrieval;global optimal solution;graph model;optimization problem;global optimization;relevance feedback;image database;retrieval performance;general-purpose;learning method;low-level;semantic information;learning method
object detectors;object detection;false positive;classifier;detection rate
image retrieval;image retrieval;semantic concept;high-level;information needed;learning process;level features;visual features;semantic concepts;region-based;pairwise classification;relevance feedback;long-term;low-level features;training images;low-level;region-based;semantic concepts;semantic information;high-level;learning method
markov random field;scene points;ground truth;belief propagation;video sequences;temporal coherence;loopy belief propagation;motion estimation;video sequences;ground truth data
object recognition;training data;image features;discriminative models;large scale;classification;object recognition;data set;object detection;probability theory
face recognition;recognition performance;component analysis;face recognition;commercial systems;image representations;visual learning;small size;training sample
feature trajectories;object recognition;support vector regression;motion tracking;motion tracking;outlier detection;linear combination;linear combination;outlier detection;quadratic programming;video sequences;robust estimation
large numbers of;object classes;large-margin;saliency detection;object detection;object class;generalization ability;object detector;object detectors
regularization;stereo algorithm;parameter estimation;belief propagation;iterative algorithm;spatially-varying;map estimation
feature points;local image features;local features;database;test image;recognition task;image data;recognition accuracy;classification process;3;local image features;recognition process;9;feature point;classifier
image retrieval;image retrieval;active learning framework;unlabeled data;active learning framework;real-world;retrieval performance;semi-supervised learning;semi-supervised;theoretical analysis;semi-supervised;support vector machines;labeled and unlabeled data;active learning algorithm;color images
minimum description length;search algorithms;synthetic data;image data;minimum description length;shape models;shape models;medical images;user interaction;information theoretic;model building
matrix factorization;regularization;closed-form solutions;optimization strategies;low-rank;objective functions;matrix factorization;missing data;cost function;missing data;average-case
object class recognition;error rates;multi-layer;4;spatial relationships;spatial information;texture features;spatial features;classifier;shape context;object class recognition;global features;local features;feature types;4, 12, 13;strong classifier;training sample
high level;data fusion;database;magnetic resonance;algorithms require;segmentation-methods;segmentation algorithms
surface shape;textual content
data representation;spatio-temporal;memory requirements;low-rank;dimensionality reduction techniques;compression ratio;approximation algorithm;dimensional data;special case;object classification;information loss
spatial correlations;higher order;higher order;natural images;scale invariant;power law;long range;natural images
region based;medical data;region-based;level set;edge information;deformable models;segmentation algorithm
sparse representation;dynamic-range
human eye
computational efficiency;image pixels;dynamic programming;dynamic programming;database;optimization method;tree structure;dynamic programming;stereo correspondence;stereo correspondence
graph-cuts;weighted graph;graph-cuts;scene reconstruction;computationally tractable;multi-view stereo
optimization algorithm;energy minimization;stereo matching;belief propagation;soft constraint;stereo images
error-prone;computational efficiency;state space;dynamic programming;stereo matching;dynamic programming;graph cut;video sequences;ground truth data;graph cut
highly sensitive;information contained in;fourier transform
svm classification;color space;post-processing;classification
clustering;data-driven;complex structure;database;medical databases;database;selected features;joint distribution;feature selection;learning problem;optimization criterion;classifier
medical applications;classification;medical data;multi-dimensional;application domains;likelihood ratio;model fitting;semi-automatic;likelihood ratio
feature space;face recognition;feature vector;training data is;recognition performance;face recognition;multi-dimensional;feature vectors
information extracted from;face recognition systems;face recognition;face recognition;input images
face images;face recognition;face verification;database;homeland security;pose variations;human faces;classifier
face representation;5, 6, 7;applications including;face representation;generative model;face model;high resolution;appearance model;image processing;high resolution
explicitly model;low-resolution;image patches;high quality;locally linear embedding;high-resolution
algorithm performs;local context;image features;global optimization;model parameters;maximum likelihood;prior knowledge;likelihood function;partial occlusion
video sequences;temporal coherence;classification problem;strong classifier;weak classifiers
visual tracking;kernel approach;general setting;kernel-based;theoretical guarantee
distance function;closed form solution;motion model;feature tracking;image sequence;point correspondences;closed form solution;optical flow;motion segmentation;motion models;motion segmentation
clustering;video sequence;multiple subspaces;motion model;optical flow;optical flow estimation
training set;unsupervised learning;local regions;scene categories;hierarchical model;supervision;9, 17;scene categories
face recognition;training data;recognition rate;face recognition;edge preserving;lighting conditions;light source;face database
single image;multi-point;expectation maximization algorithm;expectation maximization algorithm;single image
noise model;low cost;meaningful information;human eye
spatio-temporal;reliable detection;labelled data;spatial domain;classifier;motion segmentation
learning process;training data;visual content;classification;distance measure;image content;image pairs;distance measures;classification errors;ad hoc
support vector machines;machine learning methods;classification;linear discriminant analysis;machine learning;facial expressions;feature selection;fully automatic;support vector machines;feature selection techniques
small sample;training set;training data;vision systems;generalization error;case study;face detection;training error;real data;boosting algorithm;effective tools;sample set;error bounds;training samples;boosting algorithms;classification error;error bounds for;theoretical results;strong classifier;weak classifiers;training sets
face recognition;decision trees;nearest neighbor;random subspaces;random subspaces;recognition task;face space;random subspaces;single classifier;image data;weak classifiers;recognition problem
false positive;error rates;digital camera;detection algorithm;automatically detected;detection rate
prediction accuracy;training data set;pattern recognition;target values;input features;machine learning;multiple categories;algorithm produces;image categorization;dimensionality reduction
facial images;visual cues;human activity;scale space;probabilistic approach;low resolution;high-resolution
structured light;confidence measure;scene geometry;structured light
color histogram;color space;direct access to;shape information;reflectance model
clustering;local features;recognition tasks;voting scheme;minimal-cost;invariant features;feature distribution;feature sets;image matching;invariant features;nearest neighbor search
spatio-temporal;reliable detection;labelled data;spatial domain;classifier;motion segmentation
detection algorithms;gaussian mixture model;automatically constructed;typically requires;foreground detection;statistical modeling;statistical models;shadow detection
feature space;classification methods;object detection;detection performance;object detection;classifier
gradient-based;local optimization;dynamic programming;optimization method;regularization;search strategy;local optimization;global search;shape model;shape models;local shape;local minima;search space
surface normals;flow field;minimization problem;multiple views;flow fields;image sequence;multiple frames;motion estimation;motion segmentation
patch-based;image patches;classification;patch-based;vector machine;image regions;classifier
image points;measurement noise;motion model;monte-carlo sampling;statistical analysis;multi-body;multiple-view;objects moving;motion models;giving rise to;optimal set of
probabilistic modeling;medical imaging;clinical data
fast marching;point cloud;point cloud;finite difference;synthetic data;level set;shape recovery;finite element;level set;image segmentation
video sequence;segmentation problem;high-quality;graph cuts;multiple frames;motion segmentation
multi-level;effectively learn;mixture model;prediction accuracy;classifier training;higher-level;large-scale;concept hierarchy;unlabeled samples;unlabeled samples;em algorithm;image databases;classifier
ordering constraints;object model;discriminative power;object detection;markov random field;feature descriptors;input image;object detection;individual features;spatial constraints;object detection;belief propagation;matching problem
dynamic programming;classification;database;landmark points;shape descriptors;human motion;shortest path;databases;shape descriptor;shape matching
classification;training examples;database;classification accuracy;classification;large database
local structures;statistical models;local features;classification;class models;classifiers trained on;small training sets;rates;decision boundaries;statistical models;mixture models;character recognition;object classification
em algorithm;feature points;mixture model;mixture model;multi-view;unified framework;view based;multi-modality;model parameters;multi-view;feature point;face alignment;face alignment
low-dimensional;prior knowledge;importance sampling;monte carlo;belief propagation;markov network;data driven;estimation problem;visual cues;single images;hand labeled;human pose;data driven;probabilistic inference;human pose;human pose estimation;human body;pose parameters;belief propagation
12;expression patterns;quantitative analysis;image noise;learning algorithms;large number of;expression patterns;shape model;spatial patterns;maximum likelihood;comparative analysis;gene expression
ground truth;features extracted;vehicle tracking;feature-based;data association;visual recognition;object tracking
training set;exhaustive search;spatial relations;object class recognition;contextual information;pattern discovery
feature points;classification technique;classification problem;training phase;deformable objects;nearest neighbor classifier
spectral clustering;motion capture;human subjects;parameter estimation;motion capture data;feature selection;distance constraints;motion capture data
perspective images;intrinsic parameters
frequency domain;step procedure;registration algorithm;fourier domain
utility function;variable selection;makes sense;dimensionality reduction;variable selection;highly correlated;selection methods;variable selection;dimensionality reduction;dimensionality reduction
object boundaries;stereo images;mutual information;stereo matching;matching method
single-view;image motion;image sequence;subspace clustering;articulated object
video sequence;image registration;imaging conditions;similarity measure;multi-view;scene flow;large datasets;image sequences;prediction error;motion estimation;multi-view;video sequences;dynamic scenes
data point;intra-class;face recognition;intra- class;graph embedding;artificial data;linear discriminant analysis;data set;inter-class;dimensionality reduction;margins;graph embedding;separability;dimensionality reduction;data distribution
clustering;clustering;weighted graph;instance;vision problems;theoretical analysis
algorithm learns;face recognition;nearest neighbor rule;test data;data points;manifold learning;optimization problem;pattern classification;classification problems;low-dimensional
classification;unseen data;latent dirichlet allocation;unsupervised learning;probabilistic models;data set;labeled data;medical images;image information;latent variable models;mixture models;automatically discover
markov random field;training data;natural scenes;machine vision;image database;potential functions;image denoising;approximate inference
6, 23, 8, 25, 12;taking into account;17, 10;synthetic and real images
shape recovery;real world;image sequences
depth maps;input data;input image;energy minimization;depth maps;real world;images acquired
faster convergence;level set;image sequences;feature-based;surface reconstruction
energy function;multi-view stereo;depth map;graph cut;high accuracy;multi-view stereo;graph cut
data obtained from;surface shape
camera parameters;light source;synthetic and real images;light source
image pixels;arbitrary shape;np-hard problem;matching methods;visual correspondence;real images;visual correspondence
hill-climbing;human body;computational complexity;tracking problem
graph cuts;energy functions;23, 24;graph cuts;energy minimization;graph cut
theoretical analysis;cost function;condition number;maximum likelihood;low resolution
stochastic model;real-world;activity recognition;computational cost;recognition accuracy;hidden markov models;hidden markov model;human activities;particle filter;approximate inference
subspace analysis;face recognition;database;recognition performance;linear discriminant analysis;face recognition;recognition accuracy;subspace analysis;normal distribution;null space
multi-stream;mutual information;hidden markov model;processing power;maximum entropy;multiple streams;multi-stream;decision making;audio-visual
real images;obtained by combining;visual cues
camera motion;optimal algorithms;local minima;motion estimation
convergence properties;image features;image database;local minima;human face;image data;estimation process;cost function;reflectance model;optimization algorithm;higher level
object recognition;recognition performance;object recognition;image datasets;obtained by combining;object categories
clustering;nearest;density estimation;nearest neighbor;image segmentation;classification;feature space;analysis reveals;density estimator;data clustering;statistical pattern recognition;kernel density estimation;kernel density estimation;clustering
object recognition;graph theoretic;point cloud;refinement process;point sets;free energy;point cloud;graph-theoretic
high-dimensional;computational complexity;real-life applications;image features;matching process;geometrical constraints;algorithm takes;geometric constraints
face recognition;database;multiple cameras;face model;key points;management systems
data set;range data;personal identification;range images;personal identification
feature space;discriminative features;classification;appearance model;feature values;particle filtering;feature selection;particle filter;video sequences;feature based;object representation
clustering;model complexity;parameter optimization;density estimation;optimization framework;parameter estimation;unsupervised feature selection;feature selection;maximum-likelihood;density estimation;expectation propagation;bayesian approach;multivariate data;unsupervised feature selection;model parameters;posterior distribution;expectation propagation;bayesian approach
genetic algorithm;multiple objects;tracking objects;appearance models;object level;spatial distributions
pair-wise;tracking results;closed-form solution;feature selection;select features;iterative algorithm
appearance model;video sequence;nearest-neighbor;principle component analysis;image space;compact representation;appearance models;object appearance
dynamic programming;control points;stereo matching;data sets;dynamic programming;matching process;computational cost;matching problem;control points
face recognition;training images;database;feature extraction;linear discriminant analysis;face recognition;training samples;small sample size;databases;fisher discriminant analysis;excellent performance;face database
gradient descent
tracking results;particle filter;tracking performance;generative model;particle filter;bayesian framework
final step;optimization problem;projection matrix
simple linear;independent motion;synthetic data;multiple views;ground truth;articulated objects;image sequences
light source;appearance-based;light sources
ground truth;spectral embedding;database;segmentation algorithms;input parameters;precision/recall;segmentation algorithm;segmentation quality;segmentation algorithm
graph partitioning;high-quality;image segmentation;long-range;increasingly large;image processing;segmentation algorithm
pair-wise;structured light;object recognition;range data;range images;range data;feature descriptors;feature descriptor;range data;laser range
illumination conditions;low-dimensional;remote sensing image;database
multi-level;image sequence;feature points;concept called
local search;kernel-based;spatial information;region-based;tracking results;higher order
variational approach;segmentation problem;19;shape prior;level set function;shape priors;level set;shape prior;3;5;7;prior shape;level set
nearest;nearest;high dimensional data sets;vector machine;classifier;data set;small sample size;classifier
user interface;2;image segmentation;3
markov random field;markov network;belief propagation;markov random fields;images captured;wide range;dense set of
vision-based;facial features
training images;distributed computation;image database;feature extraction;wavelet-based;statistical modeling;statistical significance
human behavior;ir researchers;information retrieval research;ir techniques;information retrieval;multimedia retrieval;annual international acm sigir conference on;ad hoc;reviewing process;information retrieval;question answering;program committee
web site;color images;database

search results;response times;internet search;search algorithms;web search engine;commercial search engines;information retrieval;search engines;ir research;search engine;real world;web search engines
document space;classification;intrinsic dimensionality;text clustering;algorithm called;data preprocessing;geometrical structure;manifold structure;latent semantic indexing;euclidean structure;basis functions
latent semantic indexing;test collections;term space;related terms;low-dimensional
iterative process;cluster-based;language models;feedback documents;ad hoc retrieval;language-modeling approach;query processing
precision-recall;trec data;relevant documents;retrieval performance;highly correlated;maximum entropy;average precision
factors affecting;search task;performs poorly;search experience;relevance feedback;ir systems
context information;document summaries;document collection;context-sensitive;test set;interactive information retrieval;retrieval accuracy;context-sensitive;statistical language models;implicit feedback;retrieval models;retrieval performance;retrieval algorithms;major limitation;information retrieval;test collection
14;text-based image retrieval;target image;term feedback;term feedback;text retrieval;allowing users to;positive effect
relevant documents;search process;information retrieval;relevance feedback;user queries;ad hoc information retrieval;selection algorithm
web search;web search engine;collection selection;search engines;large amounts of data;quality measures;collection selection;result quality
hybrid approach;auxiliary;evaluation methodology;information retrieval methods;web sites;selection methods;kullback-leibler divergence;search interface
retrieval effectiveness;federated search;federated search;search engines;search engine;search technique
response times;relevance scores;monte carlo;ir systems;decision problem;large number of;distributed information retrieval;utility theory;highly sensitive;product information;decision model;distributed ir
utility function;relevance feedback;parameter optimization;adaptive filtering;recall oriented;relevance feedback;topic detection;filtering methods;logistic regression;text retrieval;objective function
news sources;probabilistic model;news events;event detection;generative model;unified framework
training data;collaborative filtering;cluster-based;data sparsity;collaborative filtering algorithms;higher accuracy
web documents;data dimension;learning algorithm;feature selection techniques;reuters corpus;storage space;feature extraction;text categorization;text categorization;text data;information retrieval;feature selection algorithm;solution space;feature selection;feature selection;objective function;information gain
data objects;web objects;sparseness problem;web pages;web search engine;heterogeneous sources;similarity measurement;user click;query log;web page
gene ontology;nearest neighbor;special attention;database;text categorization;weighting schemes;gene ontology;classifier
latent variable model;prediction accuracy;markov chain;collaborative filtering;collaborative filtering;scientific articles;implicit feedback;relevance feedback;hidden markov models;eye movements;information retrieval;monte carlo
clickthrough data;relevance judgments;decision process;implicit feedback
ir systems;information retrieval;test collections;highly reliable;average precision;information retrieval systems;effectiveness measures
data mining;world wide web;web pages;years ago;replication;large-scale;million web pages;large number of;web servers;instances;data sets;automatically generated
user profile;web pages;web search;high quality;personalized web search;web pages;million web pages;web page
ranking algorithm;web information retrieval;link analysis;web graph;link structure;ranking algorithms;algorithm called;web page;hierarchical structure;web pages;link graph;link analysis
high-quality;web search engine;clickthrough data;manually annotated;summarization methods;hidden knowledge;web page;web-page
multi-document summarization
document summaries;summaries generated;online news;source documents;multi-document
complex queries;text retrieval systems;large scale;optimization strategies;inverted lists;search engine;language modeling;query times
tuning parameters;trec data;document ranking;language models;document-centric;evaluation strategies;wide range;query processing;vector space model;retrieval effectiveness
low bandwidth;clustering;natural language text;text categorization;mobile applications;processing power;natural language
high-dimensional;trec collections;inverted lists;query processing;query processor;expansion terms;query expansion;query terms
trec web track;supervised machine learning;web page;automatic extraction of;html documents
prediction accuracy;training data set;multiple labels;target values;information retrieval;data sets;category information;multi-label;multi-label;latent semantic indexing;latent semantic indexing;dimensionality reduction
text data;text classification;geometric structure;euclidean space;positive definite;text classification;support vector machines
classification;problems require;single label;mutually exclusive;maximum entropy;classification algorithm;classification algorithms
human behavior;ir researchers;information retrieval research;ir techniques;information retrieval;multimedia retrieval;annual international acm sigir conference on;ad hoc;reviewing process;information retrieval;question answering;program committee
retrieval function;relevance information;relevance information;term frequencies;long queries;formal framework
parameter learning;takes into account;generative models;information retrieval;language modeling approaches;ad hoc retrieval;probabilistic retrieval model;average precision;document-pairs;test sets;linguistic features
trec collections;language modeling approach;information retrieval;language model;language models
language-model;web search;language model;language models;high probability;ad hoc information retrieval
ranking techniques;content-oriented xml retrieval;xml retrieval;highly relevant documents;relevant documents;large number of;element retrieval;xml documents;result set;test collection
overlay networks;hash table
10;web page;conditional random field;training data is;semi-structured;labeled training data;linear-chain;extract information from;context free;training data;specific problem;error rate;long range;9;semantic information;markov chain;question answering;extraction process
rewrite rules;web-based;transliteration;edit operations;increasing number of;text data;information retrieval;web data
classification method;classification;collective classification;maximum entropy;local classifiers;classifier;classification algorithm
mixture model;valuable information;document frequency;named entity;mixture models;learning environment
user studies;audio-visual;text analysis;classification;video summarization
document classification;language identification;database;vector machine;latent semantic analysis;feature selection methods;error reduction;document categorization;classifier
decision trees;word recognition;language model;recognition performance;document retrieval;highly skewed;highly accurate;retrieval performance;database;training samples;decision trees;image classification;retrieval results;classifier
ad hoc;training data;hidden markov model;pattern matching;question answering;sentence retrieval
information retrieval research;relevant documents;retrieval methods;test collections;question answering systems;ad hoc;question answering
density-based;mutual information;query expansion;question answering;dependency relations;passage retrieval;qa) systems;retrieval methods;expectation maximization;statistical models;question answering
web information retrieval;relevance propagation;web pages;web search;traditional information retrieval;real-world;search applications;propagation model;generic framework;content information;search engines
relevance scores;training data;document relevance;feature values;query independent;document content
search results;false positives;false positive;high accuracy;large-scale;search queries;potential impact;query frequency;search queries;web usage;query logs;human knowledge
correct answers;error rate;information retrieval;user study;information retrieval
search results;document collection;modeling task;categorical data;search process;information retrieval;data set;document genre;retrieval systems;document type
search algorithms;web search;related information;user's interests;relevance feedback;user interests;web pages;web search results
ambiguous queries;query length;retrieval performance;query expansion;additional information
trec collections;probabilistic model;information retrieval;retrieval performance;parameter tuning;term frequency;language modeling approach
markov random field model;formal framework for;training data;term dependencies;text features;markov random fields;web collections;ad hoc retrieval;average precision
search space;retrieval functions;parameter settings;information retrieval;retrieval models;retrieval performance;retrieval function;retrieval framework;optimal performance
structured document retrieval;information retrieval
user navigation;takes into account;vector-based;fully automatic;web page;average precision
search results;ranking scheme;ranking algorithm;search performance;web search results;link graph
document collection;trec data;learning algorithms;relevant content;query difficulty;distributed information retrieval;search engine
target language;term weights;data-sparseness;translation probabilities;term association;vector-space;expectation maximization;machine learning;algorithm combines;cross-lingual;cross-language information retrieval
document collection;cross-language retrieval;dictionary-based;parallel corpora;automatically generated;cross-language information retrieval
dictionary-based;translation probabilities;query words;cross-language information retrieval;statistical model
content-based search;parameter estimation;feature-vectors;image-collections;retrieval results;manually annotated;hidden markov models;hidden markov model;low-level;training) images;image-collection;image annotation;test collection;content-based retrieval;visual features;content-based image retrieval
meta-search engine;search results;web search;community-based;collaborative web search;search sessions;select relevant;search behaviour
semantic annotation;semantic classes;database centric;classification problem;semantic labels;database;probabilistic model;probabilistic retrieval model;image databases;retrieval systems;retrieval methods;training images;parameter tuning;feature selection;computational cost;visual feature;higher accuracy than;image annotation;classification problems;semantic segmentation
document collection;relation extraction;queries submitted to;search engine;query logs;question answering
relevant information;world-wide-web
trec data;retrieval performance;average precision
high correlation;precision-recall;trec data;retrieval performance;highly correlated;average precision
ad-hoc retrieval;query term;cognitive model;information retrieval;3;4;6;query expansion;higher order;selection methods
web queries;rates;query processing
text representation
search systems;training data;classification;web collection;web queries;manually labeled;web queries are;query logs;general-purpose;combined method;user queries;topic-specific;return results;databases;supervised learning
web search engine;fusion strategies;relevance judgments;search result;search engines;visual features;search engine;collection statistics;web search engines;visual features;test collection
general problem

meta-search engine;search results;web search;community-based;collaborative web search;search sessions;select relevant;search behaviour
exact computation
search engines;search engines;completeness;collected data;reference model
topic tracking;vector-space;topic model;document set
index structures;large collections;end user;query throughput;high quality
probabilistic latent semantic analysis;matrix factorization;instances;1;document clustering;4;analysis tasks;5
cross-language;text retrieval
natural language queries;database systems;sql queries;document retrieval;query reformulation;query operators;structured information;text documents;vast amounts of data;natural language
document collection;information access
ir researchers;cross-language;search techniques;indexing techniques;latent semantic analysis;data base;search technique
search results;document space;relevant documents;query formulation;high quality;retrieval results;relevance feedback;search engines;visual representation
trec data;document level;information retrieval
search task;retrieval effectiveness;ad-hoc search;relevance judgments;query difficulty;supervised machine learning;document representations;recall-oriented;collection statistics;web search engines;visual features;documents retrieved
large number of;semantically similar;language modeling;similarity measures
cross lingual information retrieval;translation model;parallel corpus;statistical model;cross language retrieval;information retrieval techniques;cross lingual
image retrieval;retrieve images
image retrieval;attribute values;attribute values;retrieved images;relevance feedback;graphical user interface;search interface;multi-faceted
1, 2;aspect model;latent dirichlet allocation;semantically meaningful;naive bayes;generative models;3, 4;discrete data;optimal parameters;supervised classification;data generated from
query term;query term;search engine results;document term;search engine
profile-based;event tracking;information retrieval;feature selection;tracking method;profile-based
query term;document retrieval;relevance feedback;retrieval performance;ad hoc;query terms
social relationships;document content;information sharing;information sharing;inter-relationships;information sharing
feature space;statistical model;multimedia information;information extraction
cross-language information retrieval;retrieval results
ranking algorithm;statistical properties;relevance ranking;takes into account;link structure;temporal behavior;information retrieval;ranking algorithms;topic model;html documents
search results;retrieval effectiveness;central server;relevance judgments;user feedback;performance metric;addressing this problem;search engine;implicit feedback;human relevance judgments;web search engines
text classification;classification accuracy;training data set;support vector machines;text classifiers
topic detection;agglomerative clustering
clustering;web search results;real-world;external knowledge;word senses;instances;web search results;labeled training
collaborative filtering;language model
low-quality;pagerank algorithm;trec data;high-quality;web sources;real world
weighting function;relevance information;language model;trec-6 data;probabilistic retrieval model;trec-7 data;context-based
enterprise search;world wide web;free text;test collections;information retrieval systems
classification scheme;genetic programming;genetic algorithms;classification;majority voting;vector machine;similarity functions;digital library;text classification
data-driven;machine translation;cross-lingual information retrieval;cross-lingual;query expansion;web crawling
similarity metrics;content-based music;automatically extracts;music collections
mobile user;natural language queries;web search;user interfaces;question answering;mobile devices;search paradigm;web pages;web services;search engine;text messages;search engine queries;linguistic analysis;keyword based;natural language
text summarization;mobile devices
long term;data publication;web service;data store;user interface;web mining;web crawl;data mining;web mining;high speed
international conference on;record linkage;data distributions;statistical techniques;information retrieval;information quality;quality metrics;database-backed;international workshop on;information systems;information quality;query processing;database;data quality;information systems;data processing;quality-aware;acm sigmod;data quality problems;knowledge discovery;data preparation;database-centric;information quality;databases;data integration
conference series;plans;data mining and knowledge discovery;knowledge discovery;acm sigkdd international conference on;data mining;kdd conference;knowledge discovery;acm sigkdd;data mining;program committee;acm sigkdd international conference on;knowledge discovery
question-answering systems;general setting
commercial applications
scale-free;world wide web;drug design;large-scale;world wide web
bayesian-network;classification;selection process;training process;classifier performance;network model;classification accuracy;model generation;magnetic resonance;problem domain;bayesian network;limited number of;computational cost;image data;markov blanket;variable selection;hidden variables;image analysis;data set;tree structure;magnetic-resonance;classifier
approximation error;low-rank;document matrix;query distribution;query-dependent;latent semantic indexing;latent semantic indexing
unlabeled images;multi-level;high level;effectively learn;mixture model;classifier training;hierarchical classifier;parameter estimation;penalty term;concept hierarchy;image domain;em algorithm;image classification;semi-supervised;statistical learning;model selection
real-world datasets;extraction algorithm;linear support vector machines;medical images;rule extraction from;optimization problem;decision support systems;linear classifiers;black-box;optimization criteria;classifier
clustering;clustering algorithms;clustering results;pair-wise;efficient computation;inter-relationships;heterogeneous data;heterogeneous data;bipartite graph;real data;text mining;high-order;real-world applications;semi-definite programming
clustering;high-dimensional datasets;classification;intrinsic dimensionality;density based;low-rank;low-dimensional;high dimensional spaces
database queries;graph data;large graph;database
clustering;black box;classification;ensemble methods;clustering method;text data;clustering techniques
hand-crafted;web pages;predictive power;automatically generated
streaming algorithms;error guarantees;original data;relative error;data streams;general problem
spam detection;data set;false positives;combination methods;false positive;specific algorithms;user behavior;prohibitively expensive;wide range;high accuracy;real world;machine learning and data mining;classifier
complete model;black-box;visualization technique;data sets;case studies;logistic regression;internal structure;support vector machines
bayesian network;large-scale;sampling-based;database;bayesian networks;databases;large database
text mining;naive bayes classifier;misclassification costs;naive bayes;highly skewed;naive bayes;classification performance;high scalability;high confidence;document collections;feature selection;data mining;document-level;classifier
large-scale;tree-based;motion model;efficiently extract;tracking problem;observation data;temporal data
clustering;probabilistic model;optimization strategy;data clustering;data set;exploratory data analysis;model parameters;competitive performance;combining multiple;model selection
detection algorithms;feature set;real life data sets;high dimensional;outlier detection;detection algorithm;outlier detection;data records;databases;detecting outliers
domain experts;gene expression data sets;data sets;small sets of;gene interactions;support vector machines;gene expression;neural networks
real graphs;discovered patterns;small-world;single snapshot;existing graph;network evolution;wide range;information networks
clustering;clustering;binary data;data points;large data sets;document datasets;clustering methods;market basket;clustering model
text mining;text mining;temporal patterns;multiple domains;scientific literature
clustering;global model;interpretability;maximum entropy;classification;probabilistic model;conditional independence;heterogeneous data sources;learning tasks;knowledge integration;mathematical formulation;probabilistic models;global models;iterative algorithms;privacy issues;takes into account;high quality;maximum likelihood;closed form solutions;learning framework;feature sets
false positives;spatio-temporal;time series;scan statistics;spatial region;rates;detection methods;sales data;motivating application
interaction data;real data sets;customer segmentation;expression patterns;gene expression data;complete set of;data sets;data mining problem;data mining methods
ranking svm;clickthrough data;real-world;retrieval functions;relevance judgments;ranking function;implicit feedback;preference judgments;search engine logs;web search results;search engine;preference data;user study
gradient descent;loss functions;statistical method;loss function;prediction performance
algorithm performs;support vector machines;feature space;19;linear programming;training sets;large datasets;text classification;latent semantic indexing;supervised learning;mixture models;low-dimensional;dimensionality reduction
utility function;discovered patterns;discovery algorithms;induction algorithm;sampling-based;diverse set of;iterative process;pattern mining;prior knowledge;interesting rules;sampling strategy;learning task;classifier
business process;learning algorithm;machine learning;real world;increasingly popular
partial orders;score function;partial order;heuristic method;genomic data
domain knowledge;web objects;web pages;term space;optimal matching;deep web;structure information;object representation;web object;web object;web page;query terms
association mining;training data;learning models;data instances;information extraction;statistical learning;entity recognition;natural language processing;data--driven;patterns discovered;learning performance;conditional random fields
profile-based;frequent patterns;real datasets;mining process;frequent-pattern mining;frequent-pattern mining;interpretability;quality measure;huge number of;generative model
discovered patterns;mining closed;graph theory;large scale;closed frequent;pattern-growth;mining process;biological datasets;biological networks;social networks
anonymity-preserving;data collection;data mining methods;large number of;original data;meaningful results;random perturbation;cryptographic techniques;randomized response;privacy-preserving;data mining;data mining problem;mining tasks
clustering;high-dimensional;application requirements;real-life applications;database schema;related information;user specifies;feature extraction;clustering task;relational clustering;data mining task;multiple relations;numerous applications;relevant features;clustering
informative samples;partial orders;labeling effort;effectively learn;classification;active learning;ranking function;information retrieval;machine learning;ranking functions;relational databases;support vector machines;large-margin;excellent performance;labeled data;sampling technique
redescription mining;redescription mining;association rule mining;closed itemsets;constraint-based;conceptual clustering;data mining problem
private information;randomized data;classification problem;classification;privacy-preserving
predictive features;feature selection methods;predictive model;feature selection;generated dynamically;error reduction;feature selection
search results;customer satisfaction;finding similar;graph partitioning;similar documents;finding similar;content management
high-dimensional;detection problem;multi-dimensional;feature space;anomaly detection;feature space;qualitative reasoning;kernel function;detection method;human experts;probabilistic reasoning;incoming data;data obtained from;principal component;behavior model;time-series;expert systems
prediction problem;prediction techniques;case study;feature extraction;selection process;online auctions;collect data;machine learning;fine-grained;online auctions;online auction;classification algorithms
large-scale;online discussion;large volumes of;textual data;data mining technologies;wide range;text classification
ensemble framework;automatic extraction of;large scale;majority voting;large quantity of;query interfaces;schema matching;semantic correspondences;web data;databases;ensemble approach;data quality;web" sources;matching accuracy
clickstream data;data mining methods;similarity measure;mined patterns;mining process;web usage mining;user profiles;evolving data streams;noisy data;clustering approach;web data;similarity measures
knowledge discovery;learned models;statistical relational learning;domain experts
duplicate records;duplicate detection;mixture model;database;record linkage;data quality;data cleaning;data sets;kdd community
prediction problem;training examples;data pre-processing;rates;machine learning algorithms;high dimensional feature space
human behavior;semantic content;interactive exploration;latent dirichlet allocation;prediction accuracy;information dissemination;content analysis;classification methods;social network;machine learning;instance;adaptive algorithm;natural language processing
text mining;data cleaning;svm based;baseline methods;text filtering;support vector machines
mixture components;learning algorithm;time series;network monitoring;network management;hidden markov models;wide range;event correlation;test statistics;optimal number of
feature extraction;large-scale;maintenance costs;data mining tasks;great potential;learn models;data mining;data mining and machine learning;model building
specific algorithms;classification;high dimensional;decision support;classification methods;test instance;instance;decision process;interpretability
clustering algorithms;overlapping clusters;synthetic data;23;real world datasets;overlapping clustering;biological datasets;gaussian mixture models;exponential family;clustering model
mining framework;association rule mining;pattern mining;hidden markov models;data sets;hidden markov model;model parameters;association rule;frequent pattern mining;model building
text mining;information stored in;optimization techniques;classification;applications including
text mining;occurrence frequency;concept hierarchy;interestingness measure;web mining;user evaluation;association rules;association rules;discovered rules;user-oriented
clustering;profile based;hidden markov model;event detection;event detection
sequential pattern mining;distributed memory;sequential patterns;sequential pattern;broad applications;data mining task;pattern set;sequential pattern mining;closed sequential patterns
clustering;overlay networks;user behavior;overlay network;user preference;knowledge discovery;social networks;dirichlet processes;anonymized data
search results;dynamic programming;text documents;directed acyclic graphs;large collections of;efficiently identify;minimum cost
partial orders;ordinal data;naïve;computational geometry;partial order;2;association rules
real data;link analysis;graph laplacian;kernel-based;kernel methods
clustering;valuable knowledge;privacy-preserving;private information;data mining;privacy concerns;collected data;data items;privacy-preserving distributed;database technologies;data mining;distributed data mining;vertically partitioned data;privacy-preserving data mining
complex queries;multiple queries;synthetic datasets;query results;multiple datasets;knowledge discovery;data mining tools;data mining system;queries involving;mining tasks;frequent pattern mining
fast algorithms;protein structure;synthetic datasets;frequent patterns;large-scale;social network analysis;graph-based;discovering frequent;mining process;graph datasets;discover meaningful;topological patterns;noisy datasets;formally defined;approximate matches;discovering frequent
web objects;web usage mining;rating data;semantic features;personalized recommendations;data sets;recommendation accuracy;content features;user interests;web site;usage information;user models;semantic relationships;latent dirichlet allocation;semantic information;maximum entropy;web users
cold start;probabilistic latent semantic analysis;collaborative filtering;collaborative filtering;information retrieval;latent semantic analysis;information retrieval;model called
text features;high accuracy;support vector machines
graph partitioning;clustering;algorithm requires;kernel-based;data mining applications;spectral methods;clustering approaches;memory requirements;theoretical guarantee;large graphs;kernel k-means;refinement step;graph clustering;clustering algorithm;cluster sizes;million edges;objective function;graph clustering;graph cut
clustering;data analysis;specific algorithms;rating matrix;data matrix;data matrices;dyadic data;clustering framework;fundamental problem;clustering algorithm
intrusion detection;classification;spam filtering;classification tasks;linear classifiers;real data;2;learning problem;classifier
data generation;synthetic datasets;high rate;instances;computationally expensive;classification;rare-class;modeling technique;instance;7;9;class labels;class label;conditional independence;data mining;rare class;manual labeling;network intrusion detection;heuristic algorithm;feature sets;high-cost;high cost
forward selection;regression method;regression tasks;classification;large-scale;latent features;predictive modeling;simple heuristics;optimization problem;regression methods;requires solving;sparse kernel;support vector machines;loss functions;computational burden;benchmark datasets
temporal order;unsupervised discretization;knowledge discovery;time series;discretization methods;knowledge discovery;hidden markov models;real life;higher accuracy than;kullback-leibler divergence;probability distributions
text mining;natural language processing techniques;tree mining;knowledge discovery;dependency analysis;data collected;ordered trees;natural language;free trees
face recognition systems;distance functions;fuzzy objects;density-based clustering;uncertain data;sensor databases;real-world data sets;location based services;data mining algorithms;density-based clustering algorithm
collaborative filtering;large-scale;real-world;social network;information services;online communities;similarity measures
algorithm generates;clustering;real-world;expectation maximization;hierarchical clustering algorithm;document clustering;higher quality;document collections;human supervision;clustering approach
text mining;language pairs;cross-language;text documents;similar topics;natural languages;text corpora;information integration;cross-lingual;main idea;generally applicable
extreme values;regression problems;instance;target variable;squared error;prediction error;distribution function
distance functions;distance function;data instances;information retrieval;context-dependent;data mining;contextual information
prediction model;prediction methods;raw data;data streams;common practice;takes place;mining data streams;benchmark data sets
association) patterns;distance metrics;temporal information;temporal patterns;spatial neighborhood;scientific datasets;real datasets;simple algorithm;spatio-temporal;scientific data
nearest neighbor;classification;manifold learning;neighborhood graphs;wide range;low dimensional;dimensional data;graph construction
answer set;search space;candidate patterns;frequent patterns
high-dimensional datasets;synthetic datasets;categorical datasets;complete search;algorithm called;subspace clusters
fast response;data rates;10, 14;sensor technology;time series;random projections;data sets;data stream;long-term;sliding windows;pearson correlation;fast response
statistical methods;interaction data;increasing complexity;data characteristics;synthetic data;detection accuracy;expectation maximization;original data;prediction error;observation data;high dimensionality
synthetic data sets;error rates;false positive;information discovery;emerging area;test cases;rule-based;data mining techniques;data sources;application domains;data set;knowledge discovery;data sets;homeland security;privacy issues;diverse set of;credit card;multiple sources;semantic graphs
supply-chain;lessons learned;data mining;enterprise-wide;complex systems;case studies;common interests;data mining;analyzing data;decision making
pattern sets;medical data;data set;pattern discovery;real world;discovery problem
domain experts;visualization tools;temporal patterns;domain knowledge into;knowledge acquisition;error prone;temporal characteristics of;text mining techniques
clustering;feature space;feature vector;domain experts;post processing;sample set;bayes classifier;feature vectors
database;parameter estimates;clustering procedure;databases
clustering;color features;unsupervised classification;event data;raw data;image data;surveillance video;multi-camera;spatial distribution;color histogram;images captured
prediction methods;classification;poor performance;data mining techniques;data mining;predictive power;real data;time series;historical data;bayesian networks;short term;machine learning techniques;accurate predictions;enterprise systems
clustering;clustering algorithms;simulation result;clustering problem;simulation studies;text clustering;simulation data;clustering techniques;standard benchmarks;clustering model
pattern-based;microarray data;distance function;closely related;data sets;search problem;similarity search;microarray analysis;expression levels;distance metrics;similarity matching
free text;document management;user experience;access pattern;databases;query evaluation;information retrieval;ranking schemes;data sources;search engines;search engine;unstructured data;business intelligence
declarative query language;access paths;existing indexes;database;database;query language;semi-structured documents;semantic web;distinctive features;search services;navigational queries
query answers;low-overhead;tracking framework;large-scale;storage space;communication network;aggregate queries;general-purpose;error guarantees;monitoring applications;communication cost;remote sites;data-analysis;distributed-streams;prediction models;distributed streams
anomaly detection;data distributions;traffic monitoring;data stream;stored data;data stream management systems;data streams;heavy hitters
query processing in;rank-based;nearest neighbor;communication overhead;communication cost
database operations;data-structure called;multi-threaded;database performance;database
node failures;query execution;query processors;join processing;query plan;data layout;select-project-join;7;operator
data structures;high level;database systems;low-level;limited number of
source documents;instance-level;heuristic algorithms;information integration;np-complete;xml schema
web databases;query forms;application requirements;mapping problem;web sources;schema matching;helping users;broad applications;query rewriting;databases;query translation;web" sources
relational databases;database;instance;databases
materialized views;semantic caching;xpath queries;xml query language;query optimization
query evaluation;optimizer;access methods;query optimizer;low-cost;optimization strategies;xml query;wide range;index structures
clustering;xml indexes;twig queries;processing algorithms;xml data;xml query;highly optimized;query performance;query processing algorithms;answering queries;query processing techniques;answering queries;disk-based;path queries
execution plans for;database;relational model;scientific applications;stream data;high-volume;high volume;numerical data;data flow;high-level;limited resources;parallel execution;stream queries;continuous queries
synthetic data;management systems;real data;instances;association rules;numerous applications
clustering;detection problem;classification technique;text classifier;success rate;probabilistic approach;hong kong;text streams;clustering approach;parameter free;text classification;clustering techniques;data mining task;real life;bursty features;text classification;parameter free;partially supervised;topic detection;positive examples
labeling scheme;query nodes;disk access;single label;xml queries;query performance;holistic twig join;twig pattern;twig query;xml database;intermediate results;pattern matching
access pattern;tree-pattern" queries;classification;performance guarantees;tree-pattern queries;structural characteristics;storage model;matching process;tree-pattern
hash based;twig patterns;document filtering;incoming documents;post-processing;xml documents;user profiles
running times;skyline algorithms;large data sets;worst case;skyline queries;relational databases;average-case;nearest neighbors
skyline computation;skyline query;efficient computation;data mining;skyline queries;efficiently computing;operator;preference queries;naïve;multi-criteria decision making
skyline computation;fundamental problem;multi-criteria decision making;skyline objects;skyline operator
xquery queries;larger datasets;queries efficiently;data model;streaming data;user-defined;large datasets;data streams;code generation;static analysis;query plan;processing queries
xml stream;synthetic data;schema-based;execution model;xml streams;semantic query optimization;optimization techniques
query optimization;relational queries;estimation problem;relational operators;xml query;cost estimation;xml queries;analytical models;cost model;statistical learning;query operators;real-world data sets;data sets;query optimizer;cost models;table scans;query workload;xml data;operator;statistical learning
edit distance;labeled trees;hierarchical data;approximate matching;hierarchical structures;integrating data from;efficient approximation;data items;autonomous data sources;databases;structural information;real world;tree edit distance
relevance ranking;large text collections;query workloads;text queries;data characteristics;relational dbms;text processing
position information;sequence matching;database size;index structure;query performance;query processing;databases;information retrieval;query length
xpath queries;schema-based;xml data;sql queries;operator;optimization techniques;query translation
12;plans;pattern tree;duplicate elimination;query plan;tree based
query nodes;tf*idf;structural heterogeneity;xml repositories;query answers;data structures;query processing;scoring methods;high precision
conjunctive predicates;distribution information;attribute values;query execution plans;optimizer;selectivity estimation;db2 udb;query plans;valuable information;joint distribution;query execution times;plan quality;cost-based;maximum entropy;selectivity estimates;query optimizers;ad hoc;cardinality estimates
indexing schemes;spatial data;database systems;access methods;database applications;index structures;relational database systems;query types;data type;relational databases;processing queries
query optimization;distance function;selectivity estimation;database;database applications;probability distribution;similarity functions;large data sets;query string;real data sets;selectivity estimation
query optimization;real-life and synthetic data;synopsis construction algorithms;approximate query answering;broader range;space complexity
compact representation;streaming environment;wavelet decomposition;huge amounts of data;massive data sets;efficient approximation;data set;data stream;space requirements;wavelet-based;error metrics;streaming environments
dynamic programming algorithm;greedy approach;quadratic programming;optimization problem;minimum description length;greedy algorithm;dynamic programming approach;query results;olap applications;np-hard
matching algorithm;materialized views;views defined;algorithm relies on;normal form;key constraints
fundamental properties;quality-aware;completeness;finer grained;data quality;materialized views;fine grained;query processing;cache management;optimizer;consistency constraints
database;relational tables;xml databases;xpath query;view selection;performance gains;caching techniques
query evaluation;optimizer;plans;nested queries;user-defined;sort order;evaluation techniques
completeness;transitive closure;pattern queries;directed acyclic graphs;stack-based;query processing;graph data;pattern matching;stack-based
forward search;keyword search on;real data;efficiently extract;spreading activation;perform poorly;search algorithm;graph databases;keyword search on
search engines;link spam;link spam;web pages;ranking algorithms
web documents;web pages;xml format;relevance ranking;takes into account;semantic tags;ranked retrieval;query engine;text content;web search engines;web data
data exchange;xml query languages;data model;xml documents;information embedded in;document collections;xml collections
tree structures;relational dbms;main memory;commercial products;query processing;column-oriented;high availability
protocol called;update transactions;replication;database;transaction execution;fine-grained;databases;serializable;transaction processing
data structure;frequent pattern mining algorithms;performance bottlenecks;data locality;data mining;spatial locality;prefix tree;temporal locality;multi-threaded;frequent pattern mining
8;application systems;parallel execution;database
service provider;query execution;data updates;data-mining;query execution;real-world;database;query types;data set;data management;data mining application;outsourced databases;selection predicate
semantic) web services;semantic web services;web services;composite service;relational database;real world;composite web services
access pattern;retrieval precision;selectivity estimation;data collections;evaluation strategies;xml data;document level;index structure;xml documents;ranked retrieval;xml elements;query processing;random accesses;query conditions;query engine
network bandwidth;query execution;attribute values;distributed data sources;data collections;real-world;related algorithms;performance gains;bandwidth consumption;result-quality;computational costs;data repositories;quality measures;network latency;result quality;query response times
clustering;classification;time series;meaningful results;human action;wide range;real world;motion-capture
tree structure;update operations;range queries;fault tolerance;queries efficiently
update intensive;min-cost;content distribution;assignment problem;real-world;data item;combinatorial optimization;matching problem;np-hard
skewed data;overlay networks;application requirements;database applications;index structures;overlay network;information retrieval;theoretical analysis;databases;load-balancing;efficient construction
hidden variables;time-series;case studies;data processing;data streams;time-series;pattern discovery;pattern discovery
mining results;11;computational complexity;frequent patterns;pattern sets;closed frequent;pattern mining;frequent-pattern mining;measure δ (called;minimum set of;theoretical bounds;np-hard
spam detection;world wide web;web pages;massive graphs;search engine;link spam;web search engines
general purpose;database;data mining processes;incremental maintenance;xml database systems;very large datasets;database table;large database
approximation algorithms;online aggregation;query execution;selection queries;aggregate functions;databases;sql queries
statistical properties;low-overhead;plans;input data;execution strategies;adaptive algorithms;query processing;query processor;query optimizers;database systems
node failures;data collection;event-detection;database;event detection;data tables;sensor network;wide range;join queries;sensor networks
search results;high-quality;high quality;search result;search engines;search engine results;ranking methods;web page;objective function;result quality
edit distance;indexing structures;single attribute;data cleansing;index nodes;query string;approximate match;multi-attribute;index structure;numerical attributes;numeric attributes;existing database
answering queries;query semantics;databases
cache performance;hash-based;join algorithms
large number of;data sources;distributed data sources;join queries;query processing
join algorithm;optimizer;join results;join processing;join algorithms;hash-based
matching algorithms;vehicle tracking;vehicle tracking;incremental algorithm;trajectory data;traffic management;road network;map-matching;quality measures
nearest;network distance;search algorithm) for;path length;real-life datasets;road network;nn queries;moving objects databases
query evaluation;nearest neighbor;range search;spatio-temporal;index structure;index structures;pattern queries;pattern queries;spatio-temporal
semi-honest;information sharing;database;privacy preserving;semi-honest;protect privacy;sharing information
information loss;desired level of;large number of;spatial locality;data set;data records;personal data;high dimensionality;privacy preserving data mining;high dimensional space
schema information;formally-defined;computationally hard;relational table
multi-dimensional;probability density functions;access method;uncertain data;uncertain database;probability density function;imprecise data;range search
dynamic programming algorithm;spatio-temporal;range query;cost model;tree structure;index structures;range queries
nearest;spatial region;pruning techniques;large number of;algorithm called
web documents;large-scale;sampling techniques;data model;optimization problem;data sets;databases
imprecise data;query semantics;aggregation queries over;data model
exploratory data analysis;data cubes;aggregate function;model trained
ground truth;schema matching;real-world domains;higher accuracy than;matching systems;matching accuracy
pruning techniques;21;mapping language;7;data sources;mapping composition;schema evolution;operator;schema mappings
real-world;detection accuracy;dynamic environments;user queries;data sources;multi-source;data integration systems;semantic mappings;false alarms;data integration
15;14;data cube;database;19;decision support;query language;relational model;2;database vendors;9;13, 7;database systems
base table;user friendly;optimizer;query rewrite;database server;materialized views;data warehousing;base tables;functional dependencies
large-scale;grid computing;data warehouse;large scale;data warehouses;grid computing;grid-based;oracle database;high availability;oracle database
data generation;management systems;case study;data management;comparative analysis;biological data
node failures;query optimization;query execution plans;database;network monitoring;query operators;distributed settings;data stream management systems;multiple streams;memory utilization;data feeds;low-level
fuzzy classification;fuzzy classification;customer relationship management;query language;query language;relational databases;fuzzy logic
data generation;access methods;database;ad-hoc;data normalization;data distributions;real data;optimization strategies;query workloads
data sizes;storage engine;large data sets;main memory;database
database applications;databases;increasing number of;database
object tracking;large scale;dynamically changing;rfid data;data model for;management systems;business processes;monitoring queries;derived data;data transformation;rfid technology;temporal data;data management systems;data management system
supply-chain;radio frequency identification;storage scheme;bitmap index;asset management;rfid-based;tree-based;tracking applications;databases;high volume;rfid) based
high levels of;support vector machines;data mining techniques;large volumes of data;oracle database;mining algorithm;data mining technology;statistical analysis;high accuracy;support vector machines;commercial databases;oracle database
high-speed;tree structures;xml storage;db2 universal database;relational structures;sql/xml;native xml;relational database systems;data model;relational tables;xml documents;database;native xml;data management;db2 universal database;xml indexes;xml schema
rewrite rules;query result;querying xml data;type checking;sql server;xml data;language constructs;large number of;optimization techniques;path-based;enterprise applications;cost-based;xquery implementation;data type;xml schemas;relational database system;query language;xml schema;relational operators
query optimization;classification;selectivity estimation;xml data;query feedback;workload-aware;tree structure;bayesian classifier;exact match;data management system;xml tree
service-oriented;distributed applications;application-level;web services
user interface;scalability problems;computational model;models built
rdf data;sql queries;database applications;rewritten query;user-defined;materialized views;rdf) data;resource description framework;sql query;querying capabilities;efficient sql;relational data
query optimizer;database;fine-grained;cost models;plan choices;query optimizers;parametric query optimization
database research;database systems;historical data;database
great success
detection techniques;instance-based;data fusion;ad-hoc;real-world;multiple representations;schema matching;conflict resolution;multiple times;missing values
business process;business activities;business processes;distributed environment
data stream processing;grid-based;data volumes;data streams;huge volumes of data;network traffic;observational data
local search;web search;web search engine;overlay network;search engine;local database
database;xml data;databases;highly dynamic;mapping rules;automatically generated;query translation
network bandwidth;evolving data;database;query operators;probability distribution function;correct) answers;limited resources
database servers;response times;3, 2;database;query result caching;database applications;database server;web content;web application;user requests;database;web pages;data stored in;data sources;query results
web-based;internet users;management systems
schema translation;high-level;high-quality;instance;source schema;instance-level;relational schemas;object-oriented;user interface;relational tables
8;web accessible;data delivery
data sharing;data sharing;update propagation;event-condition-action;database;relational database management systems;update processing;databases;answer queries
query processing;tracking framework;similar behavior;sensor data;data stream management systems;rates;data stream management system;sensor network;tracking framework
streaming applications;emerging area;data stream;query processing;data streams;result quality
large volume;multiple data streams;classification decisions;classification;mining data streams;data mining;data stream mining;data streams;data characteristics;resource constraints;high speed
store data;personal information management;operating systems;data integration;personal information;data processing;data storage;data management;problems arising
growing number of;query rewriting;text search;xml repositories;answer ranking;query answers;user queries;user profiles;relevant answers;keyword search
deep web;search interfaces;search interface
xml documents;xml data;query rewriting;digital data;functional dependencies
query engines;query processors;query language;data model;instances;query languages;relational model
meta-data;xpath query;xml documents;indexing problem;xml elements;query processing;xml data;query answering
13;database management systems;query optimizer;data independence;logical level;disk-resident;access path;data item;application programs;selection algorithm
data analysis;query language;application domains;query engine;domain model
continuous query processing;continuous query;streaming data;distributed processing;stream data;2, 8, 9;2, 3;continuous queries
ranking queries;multiple criteria;multimedia databases;ranking queries;query results;increasing importance;emerging applications;relational database management systems;similarity queries;ranking function;data mining;relational database management system;web databases
pattern based;heterogeneous data;integrated environment;user-defined
web databases;databases;large number of;scientific databases;world wide web
sql queries;efficient querying;data consistency;databases;commercial database systems;key constraints
distributed data sources;query language;data model;data sources;data access;high-level;global schema
data residing;service calls;database;database design;databases;poor quality
query optimization;synopsis structures;database systems;classification;data stream management;efficient computation;approximate query answering;large number of;event detection;database;data stream;wide range;data analysis;data mining;synopsis construction algorithms
knowledge-driven
traditional databases;text mining;enterprise search;web search;document level;search applications;databases;information extraction;search engines;document structure;search engines;meta data;search engine;result set;document content;xml search;structured data;information extraction techniques;data aggregation
main idea;content distribution;overlay networks;large-scale
growing number of;1, 2, 3, 4, 5, 6, 7, 9;vice versa;database;text search;ir techniques;xml repositories;information retrieval;xml documents;ir research;query languages;query results;structured data;unstructured data
database research;hong kong
clustering;query routing;sharing data;xml data;keyword-based;replication;increasing attention;data management
data rates;data stream mining;data stream;data sets;web logs;wide range;sensor networks;network traffic;mining data streams
multi-dimensional;business information;semi-structured data;business data;data representation;application domains;xml documents;wide range;analytical processing;online analytical processing;tree model
clustering;large number of;power-law
core xpath;xml document;conjunctive queries

database research;database systems
closely related;giving rise to;databases
acm sigmod;program committee;constraint databases
pattern recognition;database management systems;large volumes of data;scientific applications;international workshop on;inductive databases;management systems;knowledge extraction;data volumes;data processing;data mining
acm sigmod;relational databases;san diego;database
data mining;data mining application;sql server;data mining;data mining technologies;data mining;data mining based;microsoft sql server
information systems;taking into account;semi-)automatic;semantic web services;web services;modeling techniques;composite web services
petri-nets;database;business process management;workflow management
database technology;workflow systems;workflow management;scientific applications;tightly integrated with;high energy physics;databases;data-intensive;data management;data-manipulation
high-level;workflow systems;data collections
scientific workflow;semantic web;rates;artificial intelligence;database
domain experts;workflow systems;computing environments;application programs
data provenance;large-scale;storage capacity;data provenance;scientific workflow;data management
ad-hoc;scientific data;distributed environments;image data
grid computing;workflow management systems;grid computing;large data sets;complex applications;application scenarios;workflow systems
data analysis;xml data management;image analysis;xml-based;xml database;metadata management;data-intensive;data storage
algorithm performs;grid environment;real-world;scientific applications;scientific workflow;scheduling strategy;grid environment
data structure;fp-tree;database;ypxs;rule mining;phnty;mining process;wide range;data mining;association rule;real world;bx;large database
information systems;xml format;ontology language;instances;data interchange;ontology mapping;semantic interoperability
web databases;database research;database management;mobile computing;content-based retrieval;database research;document images;moving object;multimedia databases;web resources;data management
data management


database theory;database queries;instances;artificial intelligence;structural properties;query answering;game theory
acm sigmod;february;storage systems;database
3;2;4
text mining;text mining;natural language processing;natural language processing
text mining;natural-language;unstructured text;data-mining techniques;named entities;information extraction;text corpus;text corpora;product descriptions;information extraction;structured data
prediction accuracy;test set;recognition tasks;classification;instances;expression recognition;learning systems;data set size;skewed;instance;filtering techniques;named entity;entity recognition;supervised classification;class distribution
unknown word;entity recognition
hybrid approach;extracted information;distributed databases;information extraction;higher-order;distributed sources;special case;higher order;distributed environment;association rule mining;information extracted from;mapping function;text mining;textual data;association rule mining;textual documents;distributed data mining;higher order;unstructured text;databases;association rule;mining algorithm;global schema
support vector machines;classification;large-scale;classification accuracy;web-page classification;text categorization;skewed;data analysis;theoretical analysis;support vector machines
data-driven;domain experts;visualization tools;data mining and machine learning;domain knowledge into;data representation;knowledge acquisition;temporal patterns;text messages;error prone;temporal characteristics of;computing systems;text mining techniques
large amounts of;feature set;molecular biology;information extraction;modeling techniques;natural language processing;biomedical research;task-specific;general purpose
text mining;unstructured information;rule-based;statistical techniques;natural language processing;specification language;text mining techniques
text mining;search results;unstructured information;large document collections;data structures;higher degree of;knowledge discovery;document matrix;natural language processing;statistical analysis
hidden patterns;logistic regression;specifically designed for;binary classification problems;naive bayes classifier;naive bayes classifiers;bayes classifier;accurate predictions
workflow management;software architecture;rule-based;data-centric;incremental updates;rule-based
data structure;disk-based;poor performance;suffix tree;worst-case;construction algorithm;main memory;large number of;buffer management;larger datasets;algorithm's performance;data sources;science applications;tree construction;disk-based;approximate string
growing number of;the protein databank. wiley, new york;nucleic acid res. 1(28), 45–48 (2000);data analysis;data source;nucleic acids res. 28(1), 41–44 (2000);propagation algorithm;update propagation;life sciences;data model;case study;data sources;data sets;heterogeneous databases;source data;databases;data freshness
large numbers of;optimization technique;plans;real-world;service descriptions;large number of;integrate data from;user queries;data sources;bioinformatics datasets;automatically generating;web services;execution engine;tuple-level;automatically generate;optimization techniques;data integration
human body;target detection
human body;traffic monitoring;synthetic aperture
human body
face images;face recognition;applications including;face recognition algorithms;recognition performance;robust face recognition;face recognition;correlation filters;image data;low resolution;correlation filters;databases;feature analysis

human body;likelihood function
multi-modal;feature detection;multi-perspective;body parts;video based;camera views
matching techniques;human body;vision based;case study
human body
human body

database;tracking algorithm;moving objects;graph matching;em algorithm;excellent performance
facial expression recognition;human body
human body
vision-based;human body
temporal information;activity recognition;human motion;single image;lighting conditions;activity recognition
search results;object localization;image based;efficient search;multiple views;multi-view;image space;feature-based;optimization problem;classifier;solution space;geometric structure;multi-view;single object;classifier;object classification
matching algorithm;similarity measurement;moving object;moving object;moving objects;limited number of
erroneous data;algorithm's performance;background model
appearance model;classification;human detection;false alarm;human detection;object detection;video surveillance;color histogram;human body;optimal design
human body
human body
human body
statistical properties;image enhancement;video stream;image regions;information gathered;lighting conditions;image processing;video streams
natural scenes;classification;maximum entropy model;accuracies;database;1, 19;data sets;visual information;wide range;main components
human body;natural scenes
detection algorithms;vision applications;takes into account;detection algorithm;object detection;np-complete;greedy algorithm;object detection;pre-processing
dynamic environments;indoor environment
specially designed;low-cost;real-world;indoor environment;machine-vision;1;2;location information
multi-camera;human body
human vision;meaningful information
ground truth
ground truth;automatic segmentation;image segmentation;segmentation techniques;large datasets;maximum likelihood;segmentation algorithms
human body
multiple object tracking;tracking performance;real-life;object tracking;addressing this problem
object recognition;recognition algorithm;natural scenes;scale-invariant feature transform;database;image database;object recognition;1;2
human body;projection based
object recognition;total number of;class models;feature-based;instances;recognition systems;bayesian framework;multi-class;object detector
human body;false positive;face verification
data set;recognition rate;intensity images;range images;images acquired

image sequences;human body
spatial correlations;appearance-based;feature detection;learning scheme;expectation- maximization;particle filtering;graphical model;probabilistic representation;visual feature;low-level features;object models;statistical analysis;efficient learning;visual feature;belief propagation;high-level;statistical learning;learning method;spatial correlation
human body;nearest neighbor
object class recognition;human body;image features
regularization term;discriminant analysis;classification;learning algorithms;linear discriminant analysis;regularization;discriminant analysis;learning theory;training samples;small sample size;theoretical analysis;support vector machines;selecting features
learning process;closed-loop;discriminative power;point features;reinforcement learning;visual features;local features;selecting relevant;control problem;visual features
linear features;human body;image data
feature space;estimation accuracy;synthetic data;image features;gaussian process latent variable;solution space;pose estimation;gradient method;single image;pose estimation;human body
unlabeled data;training data is;mixture model;performance degradation;face detection;semi-supervised learning;bayesian network classifiers;face detection;semi-supervised;theoretical analysis;semi-supervised;classifier;unlabeled set
classification;higher-level;clustering process;stereo vision;image motion;object detection;basic assumption
face images;human body;region-based
high-dimensional;clustering;unsupervised learning;mixture model;selection criteria;real data;prior knowledge;dirichlet mixture;dirichlet distribution;dimensional data;recognition problem;dimensionality reduction;expectation-maximization
multi-modal;multi-modal;gaussian kernels;local maximum;linear combination;image analysis;individual objects;medical images
human body
human body;bayesian approach;background modeling
training set;classification;classification;linear transformation;knowledge-based;classification algorithms;depth images;knowledge-based;human generated;knowledge based;lower-dimensional;recognition rate;support vector machines;feature vectors;classification algorithm
real-world;data collected;bayesian learning;bayesian learning
'orgchart'
motion analysis;video based;video based
human body;object detection
stereo vision;processing algorithms;real-world applications;stereo vision;environmental conditions;high-volume;image domain;depth estimation;image data;stereo camera
computational complexity;high computational complexity;false positives;reliable detection;background modeling;optical flow;image sequences;region-based;detection performance;vehicle detection;optical flow;motion information;vehicle detection
stereo vision;real world;stereo matching
vision-based;vision-based;texture analysis
error-metrics;human body;motion estimation
state space;training data set;shape descriptor;linear combination;hidden markov model;human body;video streams
human body;dynamic bayesian network
user interfaces;vision-based;human body;mobile devices
motion capture;global model;motion capture;input space;pose estimation;image sequence;automatically detects;motion capture data
markov models;human body;hand tracking
signal processing;wavelet-based
vision-based;estimation methods;motion estimation
support vector machines;human body
estimation method;classification;face model;vector machine;motion model;graphical model;facial features;facial expressions;human face
interface design;object recognition;mutual information;classification;video sequence;facial image;image domain;imaging conditions;eye detection;feature matching
open-source;low-cost;feature-based;eye movements;tracking algorithm;video-based
human body;context based
multiple layers;image features;object parts;dynamic model;articulated objects;robust tracking;image sequence;articulated object;video sequences
human body
unsupervised classification;classification framework;human action;recognition accuracy;19;video-based;human action
human body
selective attention;human subjects;evaluation metric;selective attention;human eye;video sequences;negative result
search performance;human learning;image features;target object;eye movements;low-level;visual search
learning mechanism;visual search;16;selective attention;visual search;1;target detection;search tasks;5, 7;2;search efficiency;4
mobile robotics;classification accuracy;mobile robotics;scene recognition;classification rate;classification results;context-based;classifier;context-based
object-based;object-based;salient regions;visual attention;visual attention
scene analysis;human body
object recognition;human body
human body;decision-theoretic
selective attention;human body
markov decision process;object recognition;image descriptors;image based;database;action sequences;state space;object detection;local descriptors;real world;information theoretic;global information

parametric model;dynamic range;light sources
simulated data
key features;large number of;evolutionary computation
optimization framework;human vision;error metric;input images;human vision;dynamic range
deformable objects;high-resolution;require expensive;single images;stereo vision;moving objects;dynamic scenes;low-level
human body
human body
virtual environments;geometrical constraints;high-resolution;image quality
human body
structured light;edge detection;structured light;real world scenes;shape recovery;vision problems;accurately detect
structured light;vision systems;structured light;light sources;perspective projection;active vision
visual information
real world;real world
human body
dimensional data;tracking algorithm;real world
web-browsing
human body
sensor technology;face recognition;increasing attention;face recognition
local features;face verification;depth map;principal component analysis;vector machine;face verification;depth maps;range data;face verification
target recognition;simulated data;security applications;articulated objects
face recognition;human body
human body
pose variation;energy function;security concerns;registration algorithm;point sets;face recognition;computational complexity;optimization techniques
face recognition;recognition algorithm;fully automatic;similarity measure;correlation coefficient;database;face recognition;hash table;recognition rate
data set;intensity images;range images;images acquired
shape model;range images;large number of;range images;shape model
hand-held;scene reconstruction;scene reconstruction;stereo camera;automatic generation of
human body
motion tracking;moving) targets;moving target;stereo matching;matching process;video camera;moving targets
human body
semi-automated;security applications;data acquisition
vision systems;algorithms require;vision algorithms;case study;power consumption;wide range;lighting conditions;computing power
human body
synthetic aperture;synthetic aperture
signal processing;vision systems;image processing;field programmable gate arrays
synthetic and real images;vision algorithms;input image;software systems;8;hard disk;resource utilization;field programmable gate
description language;9;3;modeling technique;vision applications
tracking results;tracking objects;communication bandwidth;sensor networks;tracking applications;computational requirements;higher level
human body;low-level
edge detection;image data;embedded systems;low cost;low cost
large numbers of;expression patterns;high throughput;expression data;wavelet analysis;large data sets;data obtained from;data sets;temporal data
large scale;human body
pattern recognition;pattern recognition
feature space;highly relevant;gene expression data;supervised methods;data set;data sets;small sample size;fundamental problem;gene selection;gene selection
fast marching;spatio-temporal;detection process;quantitative analysis
data extraction;energy functions
latent structure;human body
forward selection;approximation algorithms;discrimination power;microarray data;classification;gene selection;gene selection
human body;classification
interaction data;high throughput;linear programming;data set;normal form;building block
human body
large numbers of;expression patterns;high throughput;expression data;wavelet analysis;large data sets;data obtained from;data sets;gene expression analysis;temporal data;wavelet transformation
face recognition systems;databases
human body;classification
human body
feature analysis;human body;correlation filters
error rates;lessons learned;face recognition;face recognition
face representation;face recognition;face verification;distance metrics;face representation;appearance based;appearance based
recognition accuracy;face recognition;genetic algorithms;feature extraction method;linear transformation;fitness function;baseline algorithm;color features;principal component analysis;color space;face recognition;color features;rates;databases;face verification
face recognition;face recognition;landmark points
human body
processing speed;image enhancement;computational methods;multi-scale;reflectance model;high quality;face recognition;color images;images captured;lighting conditions;reflectance model;dynamic range
face images;feature space;dimensionality reduction technique;database;face recognition;intrinsic dimensionality;low-dimensional space;face recognition;dimensionality reduction technique;mathematical model;facial feature;high dimensional space
operator;multi-view;high quality;image set;face database
state university;facial feature;face detector;test/database;local image
face recognition;support vector machines;human body
recognition algorithm;matching algorithm;classification;face recognition algorithms;features extracted from;similarity scores;image data;recognition algorithm;motion estimation
input image;face recognition;training images;local features
discriminant analysis;human body;face verification
systems require;face recognition;face databases;feature extraction;recognition rate;range images;image representation;efficient algorithms to;large number of;face recognition;computationally intensive;facial features;databases;real world;ground truth;existing database
face images;face recognition;recognition algorithm;matching algorithm;database;data sets;distance metric;storage requirements;optimal performance
face recognition;database;depth images;face recognition;automatically extract;base-line
clustering;tree structures;mutual information;tree structure;detection accuracy;eye detection;statistical model;independent component analysis;object detection;feature subset;eye detection
deformable model;face recognition;facial expressions;human body
face recognition;classification techniques;face recognition;face recognition algorithms
low-dimensional space;face recognition;recognition algorithm;face recognition algorithms;distance matrix;baseline algorithm;closed form solution;linear discriminant analysis;match scores;independent component analysis;principal component analysis;distance computations
face recognition;human body
multi-modal;multi-modal;bayesian network;graphical models;surveillance systems;parameter estimation;em) algorithm;human motion;moving objects;expectation-maximization;neural network;feature level
face recognition;database;recognition performance;eye detection;face recognition system;recognition accuracy;eye detection;face alignment;detection rate
machine learning;international conference on;machine learning;inductive logic programming;review process;machine learning
apprenticeship learning;reinforcement learning;finite-state;linear dynamical systems;continuous-state;optimal performance;optimal policies
data-gathering;training data;active learning;active learning;objective functions;finding optimal;hidden markov models;sequential data;state estimation;active learning algorithms
monte carlo;markov chain;exact computation;bayesian inference
prediction accuracy;training set;nearest neighbor;decision boundary;decision rule;training sets;worst case;desirable properties;nearest neighbor rule;competence;learning speed
data points;classification;large-scale;kernel matrices;kernel matrix;matrix decompositions;low-rank;low-rank;learning problems;kernel methods;learning task;simulation results
distributional clustering;clustering;local optimization;real-world datasets;clustering algorithms;mutual information between;document clustering;learning scheme;objective function;information theoretic
binary classification;cost-sensitive learning;classification tasks;learning tasks;cost-sensitive classification;binary classifier;error rate;multi-class;learning task;classifier;cost-sensitive
real-life datasets;multi-instance;multi-instance;decision tree learning;tree learning;decision tree learners
high-dimensional;low-dimensional representation;input data;data points;prediction task;dimensionality reduction;low-dimensional;dimensionality reduction;additional information
clustering;spectral clustering;learning parameters;optimization framework;semi-supervised learning;cluster model;hidden structures;intrinsic structure;clustering algorithm;real world;cluster-models
ensemble learning;process model;training data;dynamical systems;comprehensibility;process models
ranking function;cost function;gradient descent;search engine;ranking functions;test results;neural network
conditional likelihood;classification;dynamic bayesian networks;naïve bayes;topological structure;simulated data;scoring function;support vector machines;network topology
probabilistic framework;sequential nature;principal components;hidden markov models;independent components;finite state
web-based;prediction problems;probability distribution;probability density function;machine learning;probability distributions;normal distribution
multi-agent;learning algorithm;globally optimal
density estimation;segmentation methods;image segmentation;variational approach;graphical models;markov random fields;exact inference;bayesian formulation;variational bayesian;bayesian framework;real world;approximate inference;model selection
preference relations;bayesian methods;preference learning;multiclass problems;gaussian processes;bayesian framework;preference learning;kernel approach;classification approach;likelihood function;model selection;benchmark datasets
support vector;optimization problems;numerical experiments;training samples;benchmark datasets
classification techniques;computational biology;natural language processing
learning algorithms;learning agent;learning algorithm;reinforcement learning algorithm;wide range;learning agents
structured output;structured prediction;parameter estimation;large margin;computational cost;classification algorithms
face recognition;linear discriminant analysis;discriminant analysis;discriminant analysis;dimensional data;dimensionality reduction
real-world;highly correlated;special case;search algorithm for
function approximation;instance-based learning;instance-based;relational reinforcement learning;lazy learning;regression algorithm
state transitions;gaussian process;gaussian process;action selection;policy evaluation;generative model for;reinforcement learning;reinforcement learning;temporal difference;gaussian processes
probability distribution;ensemble classification;monte carlo;ensemble classifier
clustering;svm algorithm;similarity measure;clustering performance;supervised clustering;clustering algorithm;support vector machines
kernel function;kernel functions;graph kernels;prediction error;expert knowledge;graph representation
regression problems;classification;tree-based;parameter free;cross-validation;computational overhead;test case;interpretability;tree models
kernel learning;data source;classification;regression problems;optimal combination of;hierarchical model;variational bayes
feature space;hill climbing;edge detection;classification;image processing;data set;machine learning;large space of;feature selection;image processing
learning strategies;inference procedure;learning systems;reinforcement learning;machine learning;question answering systems;natural language
mutual information;np-complete;gaussian processes;real-world data sets;local structure;gaussian processes
clustering;clustering;cost functions;local search;unsupervised learning;classification;artificial data;global search;algorithm called;class information
learning algorithms;locally linear embedding;clustering task;statistical analysis;low dimensional;covariance matrix;principal component analysis;real world
real data sets;intrinsic dimensionality;small sample sizes;large data sets;convergence rates;random samples;intrinsic dimensionality
clustering;clustering algorithms;lower bound;mixture model;predictive distribution;agglomerative hierarchical clustering;dirichlet process;ad-hoc;real-world data sets;distance-based;probabilistic model;distance metric;bayesian hierarchical;hypothesis testing;inference method
active learning;performance guarantees;online learning;online learning;perceptron algorithm;structural properties
support vector;pairwise classification;classification methods;kernel-based;classification
synthetic data sets;data points;data streaming;change detection;data stream;data streams;benchmark data sets
amino acids;nearest neighbor;recognition problem;learning algorithm;protein fold;classification method;multi-class;protein sequence;multi-class;class prediction
domain knowledge;correct answers;planning domain;test set;plans;planning problems;training set;typically requires;planning systems;huge number of
machine learning;evaluation methodology;information extraction;information extraction;machine learning
term weights;weighting schemes;information retrieval;word frequency;language modeling;category information;automatically learn
multiple classes;binary classification problems;boosting algorithm;binary classifiers;overfitting problem;learning problem;multi-class;uci datasets;coding scheme
computational properties;bayesian network;optimization criteria;discriminative learning;classification;naive bayes;naive bayes;structure learning;classification performance;parameter learning;bayesian networks;maximum likelihood;classification problems;classifier;learning method
support vector;support vector;special case;contingency table;classification
correlation clustering;theoretical analysis;error bounds for;clustering;statistical test
interactive learning;reinforcement learning;reinforcement learning algorithm;automatically learn;classifier;local descriptors
markov decision processes;great promise;causal relationships;state variables
test set;error rate;unlabeled data;error bounds
text classification;linear svm;feature selection;feature selection method
ensemble learning;ensemble members;single class;instances;algorithm called;high precision
state space;synthetic data;classification;variable selection;complexity bounds;conditional distribution
algorithm performs;markov logic networks;knowledge-based;markov networks;inductive logic programming;real-world domains;relational databases;knowledge base
training data;worst-case;data sets;pruning methods;online learning;prediction algorithms
clustering algorithms;clustering results;vector data;input data;spectral learning;graph-based;pairwise constraints;supervision;objective function;semi-supervised clustering;data sets;vector-based;markov random fields;semi-supervised;kernel k-means;semi-supervised;special case;kernel approach;graph clustering
human brain;classifier;machine learning techniques
training examples;prediction performance;learning algorithm;hidden states;classification performance;learning performance;classifier
information needed;training data;classifier;training data is
regression problem;unlike standard;gaussian process regression;gaussian process regression;convex optimization problem
nearest;learning curve;meta-learning;information gathered;small samples;classification algorithms
real data sets;data source;auxiliary;partially labeled;active learning approach;logistic regression;logistic regression;labeled examples;supervised learning;classifier
computational efficiency;bayesian network;predictive accuracy;amino-acid;graph model;long-range;protein fold;conditional random fields
large numbers of;scoring function;synthetic data;databases
clustering;bayesian network;models learned;classification;probabilistic learning;naive bayes;belief propagation;naive bayes models;wide range;context-specific;gibbs sampling;bayesian networks;benchmark datasets;probability estimation;naive bayes models
score distributions;machine learning;data set;confidence level;benchmark data sets;learning method
dcm;classification;text documents;text data;document collections;dirichlet distribution
state space;graph theory;large state spaces;basis functions;reinforcement learning;mathematical model of;theoretical basis for;operator;laplace-beltrami
binary classification;classification;optimization problem;linear programs;support vector machines;support vectors
state space;real-time dynamic programming;large state spaces;performance guarantees;upper bounds;optimal value function;planning algorithms;lower bounds;wide range

data structures;highly competitive;feature space
test data;training set;ground-truth;learning algorithm;reinforcement learning;reinforcement learning;policy search;supervised learning;control policy;high speed
network routing;real world situations;reinforcement learning;weight vector;multi-criteria;optimal policies;multiple objectives
synthetic data;conditional distributions;probabilistic models;expectation maximization;gradient descent;small sets of;real-world;random variable
nearest;generalization performance;real life datasets;feature spaces;support vector machines;support vectors
posterior probabilities;learning algorithms;naive bayes;neural nets;maximum margin;supervised learning
learning agents;multi-agent;learning agents;multi-classifier;individual agents;machine learning
term dependencies;em algorithm;graphical models;euclidean distances;graphical model
object recognition;database;real-world;recognition accuracy;local descriptors;local information;information theoretic
parameter learning;mutual information;bayesian network classifiers;structure learning;objective functions;naive bayes classifier;data sets;accurate classifiers;computationally expensive;maximum likelihood;structure learning of;classification rate;classifier
classification;ad-hoc;classification systems;cross-validation;binary classifiers;cost-based;optimization criteria
multi-dimensional;independent component analysis;subspace analysis;subspace analysis;spanning trees;sample points
bayesian model;expectation maximization algorithm;unlabeled data;error rates;text documents;manual labeling;partially labeled;data-sets;joint distribution;classification accuracies;classification models;feature values;training documents;class labels;text classification;labeling process;labeled examples;training corpus;classification algorithms
relevance vector machine;kernel method;test cases;gaussian processes;relevance vector machine
multiple instance learning;learning algorithms;data sets;false-positive;logistic regression;support vector machines;multiple instance
real world datasets;randomly generated;tree induction;decision tree learner;information gain
gradient-based;matrix factorization;optimization method;maximum margin;low-rank;matrix factorization;prediction problems;maximum margin;semi-definite programming
plans;markov decision processes;action space;algorithms for computing;efficient approximate;optimal policies
decision tree learners;boolean functions
integer linear programming;dynamic programming algorithm;semantic role labeling;inference procedure;linear programming;conditional random fields;integer linear programming;hidden markov models;conditional random fields;inference process
classification models;multilabel classification;kernel-based;classification;exponential family;hierarchical text classification;optimization algorithm;learning algorithms;markov network;classification model;training sets;maximum margin;predictive accuracy
em) algorithm;hidden variable;step size;baum welch;speech recognition;expectation maximization;exponential family;hidden markov models;theoretical basis for;expectation maximization;maximum likelihood
clustering;underlying data distribution;sample size;density-based;lower bound;kernel density;distance metrics;semi-supervised learning;convergence rate;density based;data samples;shortest path;density-based;upper bound;semi-supervised classification;path-length
conditional likelihood;mixture components;classification problem;classification;linear transformation;model estimation;reduction techniques;low-dimensional;exponential family;supervised dimensionality reduction;desired accuracy;mutual information between;class labels;information needed;mixture models;feature vectors
domain knowledge;vector machine;machine learning methods;learning problem
local features;semidefinite programming;data points;spectral methods;sparse matrices;locally linear embedding;data sets;low dimensional;intrinsic dimensionality;dimensionality reduction
matrix factorization;class models;data analysis;gradient descent;model selection
state-space;hidden markov models;model parameters;fully connected
graphical structure;latent variable models;graphical models;latent variables;hidden variables
graph partitioning;state space;reinforcement learning;state transition;automatically creating;low computational cost
unlabeled data;semi-supervised setting;point cloud;semi-supervised learning;classification tasks;machine learning;unlabeled examples;supervised learning algorithms;data points;semi-supervised;increasing attention
data collection;gene expression data;algorithm performs well;active learning;simulated data;gene expression analysis;time-series;objective function;active learning algorithm
memory footprint;synthetic data sets;density estimation;real data sets;predictive distribution;sampling based;linear classification;expectation propagation
large amounts of;analysis tasks;svm training;large scale;large scale;support vectors;classification performances;svm classifiers;sufficiently high;training algorithm
theoretical analysis;markov decision processes;worst-case;optimal policies;performance metric
domain knowledge;domain knowledge;learning algorithm;training examples;domain knowledge into;large margin;explanation based learning;desirable properties;class labels;support vector machines;classifier
cost function;gradient descent;margin;regularization;databases
state space;sample size;sampling based;monte-carlo;optimal value function;approximation error;decision problems;transition probabilities;generative model;operator
temporal-difference;multi-step;td networks;td) networks;world knowledge;learning algorithm;temporal-difference;td(λ) networks;computational cost;future events
correct answers;convex optimization problems;training data;optimization problems;large margin;graph-cuts;prediction models
gaussian kernels;learning approaches;local models;incremental learning;local models;locally weighted
approximately optimal;regression problems;point sets;vector machine;large data sets;space complexity;support vectors
regularization;classification;semi-supervised learning;learning algorithm;exponential family;regularization framework
document classification;unsupervised classification;human effort;classification;concept hierarchies;simpler models;text documents;generative model for;regularization;text corpora;model parameters;efficient retrieval of;web data;inter-related
data point;missing data;basis functions;level set;eigenvalue problem;globally optimal solution;dimensional data;shape model
cross-validation;protein structure;conventional techniques;general-purpose;classification
probabilistic latent semantic analysis;markov random field;language models;hidden variables;smoothing techniques;context sensitive;context free;markov random fields;mrf model;language model;language modeling;mrf) model
perfect information;action selection;reinforcement learning;sampling" technique;computational cost;decision making
sufficient training data;globally optimal;data generated from;markov models
gaussian mixture model;incomplete-data;classification;parameter estimation;real data;expectation maximization;missing data;logistic regression;em algorithm;density function;variational bayesian;density functions;incomplete data;feature vectors;classification algorithm
learn models;monte carlo;dynamical systems;learning algorithm;temporal difference;em algorithm
detection problem;theoretical framework;false positive;face detection;classifier;features selected;detection performance;learning problem;fisher discriminant analysis;select features;classifier;detection rate
classification;vector machine;kernel function;learning algorithms;svm) training;large margin classifiers;classifier
learned knowledge;prior distribution;hierarchical bayesian model;bayesian approach;relational learning;model parameters
multiple tasks;multi-task;linear models;text categorization;em-algorithm;gaussian processes;multi-label;multi-task learning;bayesian framework
probability estimates;classification;classification accuracy;naive bayes;learning algorithm;naive bayes;structure learning;instances;bayesian classifiers;accurate ranking
clustering;distance measure;clustering results;taking into account;probability distributions;optimal matching;large number of;soft clustering;instance;distance based;unified framework;desirable properties
labeled instances;real-world;directed graph;clustering method;directed graphs;labeled and unlabeled data;classification problems;clustering approach
web sites;automatically extract;linear-chain;web information extraction;crf model;real world;web page;information extraction;conditional random fields
linear program;regularization;inductive learning;computational cost;inductive inference;graph-based;graph-based semi-supervised learning;large data sets;graph laplacian;semi-supervised learning;graph-based;manifold structure;labeled and unlabeled data;classification problems;mixture models;unlabeled set
entropy-based;clustering;vector space;classification methods;large margin;semi-supervised classification;logistic regression;decision boundaries;semi-supervised;low-dimensional
learning process;training set;learning algorithms;feature selection algorithms;feature selection algorithm;feature selection algorithms;feature subset;dimensional data
search systems;supervised learning;unlabeled data;classification;classification;large-scale;web collection;semi-supervised learning;web queries are;query logs;general-purpose;combined method;user queries;topic-specific;return results;databases;learning approaches
domain experts;visual vocabulary;domain-independent;classification accuracy;text documents;medical images;image collection;tf/idf;data mining;visual vocabulary;ir methods;extracting features from;high classification accuracy
duplicate records;record pairs;effectively learn;distance functions;addressing this problem;real-world datasets;record linkage;basis functions;linear combination;streaming data;similarity function;record linkage;online learning;addressing this problem;databases;learning method;clustering
information theory;information-theoretic;association rule;post-processing;interestingness measure;information-theoretic measure;specially designed;association rules;association rule discovery;interestingness measures;association rule
shortest paths;graph kernels;shortest-path;positive definite;classification;shortest-path;data mining algorithms;kernel function;increasing number of;complex objects;graph models;instances;computationally expensive;graph data;general problem;graph kernels;data mining algorithms;np-hard
sequential pattern;transaction data;mining algorithms;tree structure;spatial regions;spatio-temporal;sequential patterns;mining frequent;mobile objects;instances;line segments;spatio-temporal;sequential patterns
feature space;accurate models;anomaly detection;anomaly detection;time series;time series;data set;real-life
clustering;intrusion detection;network traffic;categorical attributes;objective functions;optimization problem;frequent itemsets;network data;association analysis;information loss
clustering;data point;attribute values;unlabeled data;data points;synthetic data sets;categorical data;cluster based;attribute values
clustering;clustering;vector space;distance measure;time series;time series;distance metric
clustering;clustering algorithms;density-based;random-walk;random-walk;density-based clustering;time series;time series;density-based clustering;noise model;noise levels;quality measure;noise model
link analysis techniques;web graph;structural properties;navigational patterns;recommendation algorithms;web site's;web personalization;web pages;personalized recommendations;related algorithms;link analysis;end user;usage data
detection algorithms;detection algorithm;rates;time series;real-world
constraint-based mining;post-processing;mining process;frequent itemsets;transactional database;pattern set;interestingness measures
real-world datasets;decision trees;decision tree;posterior probabilities;decision tree construction;decision tree;hypothesis space;loss functions;increasing number of;cost-sensitive;probability distribution;high accuracy;probability estimation;posterior probability
concise representations;frequent patterns;frequent itemsets;dataset characteristics;adaptive algorithms;frequent itemsets;data mining;frequent closed
database;tree mining;tree-structured;xml database;databases;high probability;tree-structured
text mining;feature space;vector space;classification;database;knowledge management;textual features;optimization problem;similarity graph;latent semantic indexing;hierarchical structure;learning tasks;low dimensional;intrinsic structure;real-world data sets;common practice;latent semantic indexing;textual documents;labeled documents
dynamic programming;sequential pattern;general-case
classification models;high dimensional datasets;mining process;algorithm called;data mining;transactional data
data mining applications;instance;4, 14;ranking performance;naive bayes;lazy learning;1;instance;7;uci datasets;auc;real-world;conditional independence;naive bayes;machine learning and data mining;classification algorithm;12;16;19;18;accurate ranking;accurate classification;machine learning;classification accuracy;algorithm called
support levels;frequent itemset mining;fp-tree;multi-pass;real datasets;frequent itemset mining;large number of;streaming data;frequent itemset mining;frequent itemsets;apriori algorithm;algorithm requires;data mining
learning process;training set;learning algorithms;feature selection algorithms;feature selection algorithm;feature selection algorithms;feature subset;dimensional data
clustering;detection algorithms;anomaly detection;data cleaning;time series;time series;discovery algorithm;time series;data mining;diverse domains
data-driven;dimensionality reduction technique;nearest neighbor;data samples;data visualization;linear transformation;computational biology;test cases;locally linear embedding;geometric structure;real life
pagerank algorithm;web pages;web search;web graph;linear algebra;anchor text;higher-order;singular vectors;higher-order;linear algebra;tensor decomposition;link analysis;hyperlink structure;hits algorithm
meaningful clusters;clustering;clustering methods;local density;real-world;data dimensionality;generic framework;generic framework;subspace clustering;traditional clustering algorithms;dimensional data;dimensional data;algorithms typically;subspace clusters;subspace clustering algorithms;subspace clustering
clustering;data objects;global model;complete information;gaussian distributions;data mining techniques;local models;data mining tasks;distributed applications;model-based clustering;data repository;data set;distributed environments;mining algorithms;distributed sources;model-based clustering;data mining algorithms;distributed data;distributed environment
frequent itemsets;memory usage;data streams;memory space;frequent itemsets;data stream;prefix tree;data mining process;memory utilization
mining algorithms;frequent patterns;tree structure;database;tree nodes;frequent-pattern mining;apriori-based;tree structure;tree based;incremental mining;database transactions;transaction database
clustering;consensus clustering;clustering ensembles;multiple clusterings;multiple clusterings;data mining
search space
protein structure;classification;computational biology;multi-stage;wavelet decomposition;protein structure;classification results;wavelet-based;protein structures;feature vectors;wavelet transformation
minimal optimization;nearest point;vector machine;quadratic programming problem;fast computation;support vector machines;nearest point
specific applications;machine learning methods;record linkage;complex relationships;domain specific;machine learning;domain-specific;matching method;common-sense;simple models;expert systems;hand-written;record linkage is
statistical dependence;group structure;related entities;instances;data sets;hidden structures;relational data;performance gains;relational learning;class labels;classification tasks;inference techniques
large numbers of;machine learning algorithms;machine learning;unlabeled examples;data sets;decision boundary;active learning algorithms;active learning algorithm
human users;greedy algorithm;mutual information;data volume;representative set;massive data;relative entropy;heuristic algorithm;representative set;information-theoretic;real datasets;large database
parameter-free;spatial data;synthetic datasets;spatial data mining;minimum description length;large datasets;spatial correlation;binary features
discovering frequent;temporal intervals;temporal intervals;database;real datasets;discovering frequent;pattern mining;depth first search;synthetic datasets;network data;sign language
data mining;remote sensing images;extracting information from;image database;remote sensing;image data sets;spatial information;remote sensing;image databases;information extraction;case study;image databases;remote sensing image;databases;remote sensing data;mining patterns
data analysis;evaluation metric;regression models;performance metric;case study;ranking performance;regression model;regression models;correlation coefficients;evaluation measures;size estimation
multi-class problems;classification models;nearest neighbor;recommender systems;collaborative filtering;binary classification problems;classification methods;classification problem;classification models;recommender systems;customer behavior;multi-class;recommender systems;classifier;linear support vector machines
linked data;data collection;multi-stage;classification;multi-stage;instances;class label;individual classifiers
genetic programming;bayesian network;operator;user-defined functions;network model;functional dependency
transaction data;binary data;association analysis;confidence measure;special case;continuous data;conditional probability
support vector machines;classification;classification;spatial information;linear programming;human experts;alzheimer's disease;feature selection;classification approach;classifier;alzheimer's disease;feature selection
graph partitioning;anomaly detection;real datasets;random walk;detection algorithm;anomaly detection;neighborhood information
frequent itemsets;database;sharing data;large databases;frequent itemsets;data mining
clustering problems;hidden markov models;web-log data;synthetic data;real-world
discriminant analysis;support vector machines;feature extraction;linear discriminant analysis;tensor based;convex optimization;classification problem;learning machines;supervised learning;machine learning and data mining
user profile;random walk;database;ranking measures;directed graph;random walk;interestingness measure;random walk;association rules;query expansion
latent variable model;face images;high-dimensional space;synthetic data;gaussian process;relational model;relational model;handwritten digits;dimensionality reduction
scalable solution;real life data sets;classification;sensitive information;finding an optimal;template-based;data set;template-based;domain values;data mining;classification problems
prediction accuracy;classification models;update cost;synthetic data;ensemble classifiers;streaming data;costly process;data streams;mining data streams;classifier;data distribution
frequent itemset mining;approximate algorithm;np-complete;frequent itemset mining;frequent itemsets;real-life;privacy issues;databases;association rules
data stream;false dismissal;high rate;time series;data streams
discriminative training;protein function;classification;markov models;sequence classification;biological sequence;gradient based;counterpart;text classification;data sets;likelihood function;sequence classification;support vector machines;maximum likelihood;markov model
clustering;clustering;multi-dimensional;data obtained from;spectral clustering;temporal information;reduction techniques;time series;spectral analysis;hidden markov models;time series;sensor network;data mining;time-series;real data;high levels of;model-based clustering
discriminant analysis;sample size;data mining and machine learning;linear discriminant analysis;learning theory;small sample size;data dimensionality;theoretical analysis;support vector machines;dimension reduction
prediction problem;classification;closely related;problem domains;problem domains;pruning method;pruning algorithm;problem domain;semi-definite programming
human users;visual data mining;data distributions;large-scale;visual data-mining;large number of;quality function;data sets;data distribution;data mining algorithms;comparative analysis;actionable knowledge;discovered rules
classification quality;classification;large-scale;text documents;high dimensional data sets;high dimensional;text data;3;standard svm;text classification;dimensional data;memory consumption;special case;training error;classification algorithm
rule evaluation models;rule sets;human experts;objective rule;data mining;rule evaluation support method;learning algorithms;post-processing;post-processing;mined results;rule evaluation support method;learning models;data mining process;objective rule;uci datasets
data mining applications;discovering association rules;real life;multiple attributes;database
instance;rates;classification;learning tasks;learning problems
frequent itemset mining;frequent itemset mining;3;low support;database
adaptive clustering;nearest neighbor;distance function;past experience;learning approaches;past experience;cluster quality;adaptive clustering;case study;instance-based learning;adaptive clustering
regularization;classification models;unlabeled data;linear programming;boosting algorithm;classifier;column generation;basis functions;semi-supervised;labeled data;mixture models;labeled and unlabeled data;benchmark datasets
search algorithm for;search algorithm for;subspace clusters;inter-relationships;dimensional data;subspace clusters
collaborative filtering;recommender systems;recommender systems;collaborative filtering
clustering;relevant attributes;selection process;feature selection;feature selection
multiple data streams;avoid redundant;memory usage;data streams;mining process;prior knowledge;transaction databases;multiple streams;mining sequential patterns;mining algorithm;sequential pattern mining;sequential pattern;data distribution
data perturbation;classification;data owners;3, 1;sensitive information;sensitive data;privacy preserving data mining;1;3;privacy preserving;data perturbation;data owner
computational framework;large numbers of;classification approach;feature selection
parameter values;training data;threshold values;classification;association rule mining;accurate classification;higher accuracy
fault detection;classification;data mining techniques
decision trees;feature vector;learning algorithm;class label;learning algorithms;naive bayes;sample selection bias;data set size;sample selection bias;logistic regression;bayesian classifiers;margin;classifier;class probabilities;training sample
data sets;biologically relevant;database
semi-honest;privacy-preserving;data mining techniques;databases;semi-join;data warehousing;privacy-preserving;data mining;data encryption;frequent pattern mining;data integration;frequent pattern mining
data mining approach;data mining algorithms;data mining approach;application area
cost effective;classification;class label;cost estimation;selected features;data stream;data stream;feature selection;classification method;cost-effective;cost-effective;decision model;classification task;classifier;feature selection
clustering;web-based;lower computational cost;collaborative filtering;recommender systems;content distribution;1;svd based;matrix factorization;computationally expensive;rapid rate;clustering algorithm;collaborative filtering;collaborative filtering;clustering
unlabeled data;real-world;classification systems;text classification;rates;false-positive;label-sets;text classification;label-set;real world;high precision
clustering;overlapping clusters;discovering frequent;closed itemsets;complete set of;databases;closed itemsets;frequent closed;complete set of
clustering;clustering results;semi-supervised learning;clustering criteria;structural properties;clustering algorithm;semi-supervised
community discovery;community discovery;social network;large graphs;real-life;community discovery;million edges
collecting data;data sets;customer relationship management;large-scale;confidential information;collected data;data set;data sets;data collection;data analysis;association rules;high-level;decision making;information loss
point based;optimization methods;convergence rate;data set;data sets;em algorithm;em algorithm;probabilistic models;cluster models
real-world data sets;ensemble classifiers;ranking performance;data mining and knowledge discovery
decomposition method;theoretical framework;probability density functions
random walk;spatial data;free-form;real datasets;random walk;spatial region;likelihood ratio
text mining;relation extraction;text documents;semantic knowledge;ontological knowledge;domain-specific;case study;domain-specific;text collection;text documents;ontological knowledge;semantic relations;ontology learning;complex structures
mining patterns;mining patterns;data mining
web search results
classification;unsupervised feature selection;label assignment;frequent itemset mining;unlabeled documents;clustering algorithm;occur frequently
clustering;gaussian mixture model;mixture model;post processing;main memory;data set
binary classification;data mining tool;logistic regression;logistic regression;regularization;data mining task;parameter tuning;large datasets;data preprocessing
distance functions;fuzzy objects;density-based clustering;uncertain data;large data sets;sensor databases;uncertain data;uncertain objects;location based services;data mining algorithms;density-based clustering algorithm
clustering;clustering algorithms;high-dimensional;pairwise constraints;supervision;data set;data sets;semi-supervised clustering;metric learning;algorithm learns;clustering algorithm;semi-supervised;higher accuracy than
context sensitive;distance measures;context-sensitive;weighted sum of;context-sensitive
time series;time series
clustering;clustering algorithm;gene ontology;ontology-based;clustering algorithm
regression algorithm;quality metric;pattern recognition
lower bound;frequent patterns;closed patterns;probabilistic models;databases;large databases;closed) patterns;data mining;association rules;average number of;upper bound
cost-sensitive learning;high cost;imbalanced datasets;cost-sensitive;learning approaches
text representation;singular value decomposition;high-order;high-order;tensor space;text representation;text classification;mathematical framework;dimension reduction
11;frequent itemset;itemset mining;frequent itemsets;real data;frequent itemset mining;frequent itemsets;data sets;noisy data;noise model;large itemsets
real-world datasets;parallel algorithms;density-based;network intrusion;database;outlier detection;parallel algorithms;local outliers;machine learning;data cleaning;density-based;distance-based;parallel algorithm;distance-based
support vector machines;vector machine;vector machine;large data sets;compression method;accurate classifiers;support vector machines;random sampling
clustering;clustering approaches;pattern analysis;spatial locations;social interactions;spatial clustering;data mining techniques;temporal information;clustering methods;data collected
ranking algorithm
accuracy compared to;predictive models;training data;classification tasks;feature values;prohibitively expensive;classification model;expected utility;cost-effective;missing values
web applications;extraction task;search engine;web crawlers;search engines;web mining;search engine
partial orders;sequence data;mining frequent closed;mining frequent;sequential patterns;1;data mining task;sequential pattern mining
clustering;clustering;high-dimensional;text data;desirable properties;model selection
predictive accuracy;distributed data;rule discovery;supervised learning;learning task;distributed data;relative accuracy
face images;face recognition;face databases;face recognition;landmark-based;principal component analysis
data point;categorical data;classifier
predictive accuracy;structural properties;large social networks;social network;social networks;structural properties;social networks
constraint-based mining;constraint-based mining;search space
skewed data;skewed data;selecting features;na篓ýve bayes;text classification;decision trees;support vector machines;feature selection;text classification;dimensional data;select features;information gain;classifier;feature selection
mining algorithms;frequent patterns;branching factor;branching factor;9;mining algorithm
benchmark data;intrusion detection;false positive;multi-objective;multi-objective;intrusion detection;rule-based;evolutionary computation;evolutionary computation;agent-based;network data;network traffic;agent-based;detection rate
transaction data;density-based;frequent itemsets;frequent itemsets;density-based;subspace clustering;numerical experiments;subspace clustering
mining task;web sites;extracted information;real-world;web sites;feature values;highly dynamic;online auction;learning method;product features;learning problem;semi-supervised;web page;automatically extract;decision making
schema integration;frequently occurring;deep web;rule-based;large number of;optimization problem;clustering aggregation;np-complete;real-world data sets;approximation algorithm;clustering aggregation
data analysis;data-analysis;data sets;correlation-based;correlation coefficients;clustering;large number of;time series;principal component analysis;feature-subset-selection;statistical properties;data analysis;precision/recall;real-world data sets;search technique;correlation-based;similarity searches;time series;application domains;correlation-based;data set;similarity search
high-dimensional;markov blanket;markov blanket;preprocessing phase;feature selection;data mining;feature selection problem;optimal set of
algorithm performs;instance;spatial relationships;pattern mining;spatial datasets;pattern mining;spatial features;instances;computational cost;pattern discovery;mining algorithm
training data;training examples;probability estimation;data sets;binary classification problems;probability estimation
data sharing;local constraints;limited bandwidth;data sets;data set;aggregated data;data privacy;generative model;distributed data
algorithm performs;ensemble methods;vector machine;base learners;margin;classifier
synthetic datasets;high dimensional datasets;outlier detection;high dimensional datasets;problem domains;detect outliers;detecting outliers;high dimensional;high dimensional space
pruning techniques;classification;class values;tree mining;tree patterns;predictive power;accuracies;comprehensibility;classification;rule set;tree patterns
cost functions;decision trees;classification;misclassification costs;objective functions;optimization problem;cost-sensitive classification;cost function;cost-sensitive
clustering;density estimator;data mining methods;data warehouse;online algorithms;mined patterns;data warehouse;hierarchical clustering;industrial applications;raw data;data structures;hierarchical clustering methods;data mining algorithms;discovered knowledge;clustering method
classification;classification
false positives;historical data;statistical tests;large-scale;data mining applications;detecting anomalies;business intelligence;empirical bayes;process control;error rate;detect anomalies
classifier ensemble;classifier fusion;fusion strategies;image database;theoretical guarantees;classifiers trained on;classifier fusion;classifier
growing number of;resource-constrained;information sources;relevant documents;emerging applications;distributed information retrieval;result set;resource constrained;source selection;multiple sources;network applications
xml documents;query language;formal description;special attention;xquery expressions
large number of;information management;relational database management systems;data sources;databases;data management
scientific data management;data stores;data volumes;data quality
sensor technology;main memory;on-line analytical processing;commercial products;stream processing;high-volume;data processing;processing requirements;real world;high-level;relational dbmss
average number of;database
12;database
historical data;database systems;database
database research

data management;information dissemination;ubiquitous computing;sensor networks;distributed systems;databases;event-based;data management
database;information retrieval;data management systems;databases
international conference on;database;databases;international workshop on;data engineering;databases;international workshop on
1;xml query;query language;xml documents;2;working group
multimedia databases;database;acm sigmod;data mining;time series;databases;database performance
ir research;information retrieval
rates
content analysis;information access;short term;natural language texts
workshop report;massively parallel;information retrieval;information access;computing systems;distributed systems;information retrieval workshop
workshop report;confidence scores;difficult queries;query difficulty
sigir workshop on;theoretical framework;formal methods;similarity functions;information retrieval;graph theory;information retrieval;metric spaces;retrieval applications
multi-modal;context-free;information retrieval;ir research;multi-lingual;multi-media
workshop report;image retrieval;multimedia information retrieval;multimedia retrieval;information retrieval workshop
text summarization;international conference on;topic segmentation;information retrieval applications;document level;passage retrieval;information retrieval;1;3;2;acm international workshop on;real-world applications;question answering
xml element retrieval;element retrieval;relevance ranking;information retrieval
clustering;search interfaces;workshop brought together;information retrieval;exploratory search;information visualization;information exploration;information seeking;cognitive processes
approximate answers;multiple dimensions;directly optimize the;data dimensionality;wavelet decomposition;synthetic data sets;dynamic-programming;real-world;approximate query processing;query answers;optimization algorithms;approximation guarantees;relative error;large amounts of data;error metrics;absolute error
completeness;relational algebra;f;transitive closure;xml documents;core xpath;xml document
query processing;database;sampling-based;index maintenance;feature extraction;mining process;xml documents;indexing problem;chemical compounds;graph indexing;data mining;graph queries;frequent pattern mining;large database;database updates
data exchange settings;computational complexity;schema mapping;np-complete;data exchange;source schema;fixed-point;fundamental problem;query answering;schema mappings;tuple-generating dependencies
query evaluation;optimizer;relational queries;query pattern;client applications;predict future;database servers;cost-based;training phase;context-based;total cost
1991;query optimization;synopsis structures;plans;database;information-theoretic;cost estimation;data distributions;queries involving;probabilistic guarantees;error propagation;space-complexity;join queries;space complexity;database systems
real-world datasets;data mining tasks;entity types;link structure;data mining;instances;multiple types of;machine learning algorithms;distributed data;relational data;link mining
network analysis;linked data;web pages;single-object;data mining techniques;heterogeneous networks;collective classification;social networks;link prediction;semantic information;social network analysis;mining tasks;link mining
network analysis;clustering coefficient;sampling algorithm;knowledge discovery;sampling algorithms;data mining;complex systems;share similar;real world networks;network topologies;statistical significance;topological properties;sampling algorithms;separability
network analysis;network models;temporal link prediction;machine learning techniques;event-based;data set;ranking algorithms;relational information;data mining;network data;event-based
local optimization;social network analysis;conjugate gradient;kernel functions;social network;dynamic model;latent space;update rule;low dimensional;real-world
link discovery;link prediction;perform poorly;prediction task;prediction models
graph partitioning;relevance scores;relevance score;anomaly detection;real datasets;random walk;detection algorithm
multi-relational;rdf graphs;maximum flow;weighting schemes;discovery algorithm;discovering patterns;higher quality;pattern discovery
multi-relational data mining;graph-based;multi-relational;logic-based;data mining system;inductive logic programming
link analysis
link mining;mining tasks;event detection;application requirements;link mining
training set;short queries;noise level;answer set;acm sigkdd international conference on;related information;knowledge discovery;search queries;web site;data mining
base classifiers;training data;training phase;classification;statistics based;ensemble classifiers;machine learning;search engines;web pages;classification problem;classification results;category information;classification task
main idea;training documents
search engines;search engine;query terms;classification;search engine queries
workshop report;link discovery;acm sigkdd
workshop report;multi-relational data mining;acm sigkdd international conference on;multi-relational;knowledge discovery;finding patterns;data mining;structured data
workshop report;biological entities;information from multiple sources;clinical practice;sequence databases;gene expression
workshop report;data mining methods;international conference on;anomaly detection;ad-hoc;data mining processes;data mining;knowledge discovery;anomaly detection;data mining systems;data mining;workshop on "data mining
workshop report;data mining standards;workshop on data mining;data mining standards
workshop report;web usage analysis;web usage analysis;acm sigkdd international conference on;knowledge discovery;web mining;data mining;web mining;web mining;multi-faceted
workshop report;clustering;open source;data mining methods;classification;open source data mining;open source data mining;frequent pattern mining
real-world;active learning;cost-sensitive learning;acm sigkdd international conference on;data mining;international workshop on;knowledge discovery;data mining;data mining applications
knowledge discovery;acm sigkdd international conference on;multimedia data mining;data mining;international workshop on
frequent itemset;measure called;minimum support;association rule mining;candidate itemsets;generation process
data stream management systems;window queries;continuous query;sliding-window;data streams
database technology;database;privacy concerns;single attribute;protect privacy;building block
association rule mining;stream data;static databases;traffic monitoring;emerging applications;data stream;click streams;data streams;data distribution;high speed
rewrite rules;relational algebra;sql queries;sql query;xml query;user query;xml view;relational database
data structure;data characteristics;efficient representation
basic concepts;application domains;access control
log-structured;query processing;rates;moving objects;file systems;data gathering
database;programming languages;programming language;database
web service;grid computing;data access
acm sigmod;multi-agent systems;databases;database
cost model;database instance;query reformulation
case study;knowledge base;cross-domain
fuzzy classification;nearest neighbor classification;task specific;distance measures;gradient descent;nearest neighbor classification;real life;cost function;class labels
genetic programming;prediction model;classification;margin;kernel functions;optimal kernel;svm classifiers;support vector machines;classification task;classifier;model selection
cross-validation;cross-validation;classifier;training examples;linear systems
rough set;classification;classification;data set;decision rule;training samples;rule extraction;rough set theory;approximation algorithm
clustering;mutual information;classification;word pairs;joint distribution;feature selection;chi-square;feature selection;contextual information;feature selection;information gain
instance selection;unlabeled data;classification;active learning;semi-supervised learning;jensen-shannon divergence;classifier;labeled data;active learning approach;selection criterion;desired level of;jensen-shannon divergence;classifier
minimum number of;machine learning;knowledge acquisition;comprehensibility;machine learning;knowledge acquisition;machine learning
low-quality;data integrity;class noise;classification;instances;instance;data quality;case study;noisy data;data mining;dependent variable
tracking algorithm
local geometry;face recognition;linear subspace;high dimensional;face databases;locally linear embedding;face recognition;lower cost;operator;laplace-beltrami;nearest neighbors
spatial locations;classification techniques;classification;classification;independent variables;spatial analysis;machine learning;prediction error;spatial) data;dependent variable;support vector machines;artificial neural networks;geographic information systems
software development;bayesian network;high risk;bayesian belief;belief networks;software quality;learning method;small-scale
statistical properties;pattern recognition;features extracted from;pattern recognition;shape features;parallel algorithm;network structure;parallel algorithm
database;data mining applications;data mining;data mining;data-centric;user groups;business intelligence;data preparation;oracle database;data mining;data-centric;high-level;model selection;predictive analytics
intrusion detection;computing infrastructure;network security;high accuracy;intrusion detection;maintenance cost;data mining;data mining-based;data mining-based;database-centric;intrusion detection systems;intrusion detection systems;data transformations;data analysis;oracle database
web-based;meta-learning;classification;increasing number of;meta-learning;data mining;machine learning algorithms;model selection
clustering;intrusion detection;network security;classification;intrusion detection;wireless networks;real-world;instances;distance-based;data mining;wireless network;wireless networks;clustering approach;clustering
false positives;similarity measure;support-vector machines;probabilistic model;machine learning;similarity measure between
selection criteria;data obtained from;selection criteria;synthetic data;expression levels
clustering;data objects;microarray data;microarray data;database;biological processes;takes into account;hierarchical clustering;clustering performance;clustering process;real-life;clustering methods;clustering algorithm;clustering algorithm;global structure;computing platform;clustering
test data;amino acids;hidden markov model;computational methods;prediction methods
classification accuracy;training data;vector machine;classifier;classification
computational efficiency;high-dimensional datasets;data analysis;open source;machine learning techniques;pattern recognition
data-driven;data-driven;clinical data;rules generated;data sets;machine learning techniques;knowledge-driven;knowledge-driven
required information;classification technique;classification quality;classification;large scale;increasing number of;incoming documents;multi-label classification;data mining technology;associative classification;high accuracy;associative classification;multi-label;multi-label;classification algorithms
ensemble learning;decision tree;drug design;learning algorithms;drug design;machine learning;machine learning techniques;test case;machine learning
clinical practice;mathematical programming;classifier;left ventricle;feature selection technique
search algorithms;classification;classification;data mining techniques;rates;selection methods;classifier;selection algorithm;selection methods
learning mechanism;prior knowledge;reinforcement learning;prior knowledge
high degree of;perform poorly;learning framework;magnetic resonance images;classification
regression trees;auc;support vector regression;average error;linear programming;regression problem;censored data
learning process;auc;neural networks;vector machine;fuzzy logic;nearest neighbor classifier;statistical method;artificial neural network;logistic regression;machine learning techniques;risk factors;machine learning approaches
predictive accuracy;network topology;metabolic networks;inductive logic programming;learning models;metabolic networks
classification;spectral methods;spectral analysis;data sets;semi-parametric;bayesian framework;probability density;information extraction;higher order
data points;optimization problems;semi-supervised learning;large number of;graph laplacian;semi-supervised learning;manifold structure of
local patterns;data mining;database
binary attributes;feature construction;depth first search;machine learning;efficient construction;propositional satisfiability;algorithms typically;relational data
text mining;text mining;free text;high dimensional;high dimensional;text mining;real-world;data mining;data set;data mining;clustering algorithm;clustering algorithm
clustering;clustering algorithms;graph partitioning;graph-based;clustering criteria;web log data;data items;data set;clustering methods;evolutionary approach;clustering algorithm;clustering algorithm based on;real world
pair-wise;valuable information;organizational structure
clustering;clustering;training set;common features;test set;classification;classification;cluster-based;classification performance;rough sets;class label;takes place;cluster analysis
feature space;digital information;text retrieval;information retrieval;dimensionality reduction techniques;independent component analysis;noise reduction;text retrieval;principal component analysis;high dimensionality;latent semantic indexing
context-aware;semantic knowledge;text data;knowledge representation;retrieval algorithms;formal model
machine translation;natural language processing techniques;sign language;image sequence;machine translation;image recognition
computing environment;text categorization;text categorization;giving rise to;text classifiers;grid environment;reuters-21578 corpus
operator;genetic algorithms
evolutionary computation;graph structure
statistical methods;sample set;training set;test set;high accuracy;genetic algorithm;data mining;algorithm generates;data set;classification rules;preprocessing phase;genetic algorithm;data mining;association rules mining;pre-processing;test sets
genetic programming;higher level;gene expression;regression problems;programming language
genetic algorithms;generalization error;genetic algorithm;fitness function;machine learning;data driven;machine learning
base classifiers;medical applications;medical databases;training set;training instances;classifier ensembles;class distribution
linear projection;classification;classification;frequency-domain;frequency domain;kullback-leibler;spatial features;linear features;purity;spatial domain;feature vectors;local region
artificial data;algorithm named;1;5;boosting algorithms;single classifier;classifier;classifier
base classifiers;training process;base learners;classifier
support vector machines;vector machine;training data;training algorithm
domain knowledge;data objects;domain ontology;extraction process;pattern mining;sequential patterns;mining process;abstraction-level;frequent pattern mining;pattern extraction
transaction data;frequent patterns;approximate matching;exact matching;mining process;data structures;real life;algorithm relies on;frequent pattern;transaction database
underlying structure;vast number of;business data;statistical tests;survey data;association rule discovery;real-life;association rules;association rules;pre-processing
frequent itemset;frequent itemset;tool called;tree-based;visual interface;data mining;transactional database;graphical user interface;user-friendly
large numbers of;naive algorithm;conjunctive views;rewriting algorithm;theoretical results;decision support systems;scalable solution;data warehousing;maximally contained rewritings;conjunctive queries;data integration systems;5
base table;database;virtual view;view updates;instance;base tables
training examples;aggregate views;aggregated data;views defined;aggregate views;general problem;data mining problems
efficient computation;data sets;efficient computation
data cleaning;real datasets;textual similarity;similarity functions;similarity function;data cleaning;operator;operator;similarity joins;similarity joins
sampling methods;data warehouse;sample data;data-set;data set;data sets;parallel processing;sampled data;random sampling
uncertain data;conjunctive views;maximally contained rewritings;conjunctive queries
data cleaning;join algorithms;approximate match;declarative framework;score distribution;approximate match;query results
query times;probability distribution;index structure;databases;object identification;feature values;feature vectors;distance measures;similarity search;query processing;databases;feature vectors;result quality
lower-bound;special case;road network;user-defined
approximation techniques;multimedia databases;completeness;high quality;large databases;approximation techniques;image databases;fast computation;query processing in;multimedia databases;data mining;multimedia database
indexing schemes;main memory;index structure;real-life;query performance;data sets
xml data;xml tree;labeling schemes;update cost
join algorithms;complex queries;optimization techniques;execution plans;xquery engine
schema design;normal forms;data model;query performance;query processing;normal form
discovered patterns;domain knowledge;search space;domain-independent;taking into account;application domain;real-world;pruning strategies;data set;data mining
multiple databases;mining task;synthetic datasets;frequent patterns;multiple datasets;query plan;instances;applications involve;query plans;real world;mining tasks
approximate query processing;streaming environment;database;sampling-based;sampling-based;query processing in;olap queries;summary statistics;probabilistic guarantees;estimation accuracy;numerical attributes;dimensional data
single-dimensional;optimal algorithms;general-purpose;greedy algorithm;approximation algorithm;higher-quality;np-hard
pair-wise;domain-independent;synthetic data;database;similarity queries;upper bounds;similarity measure;time series;similarity queries;instance;finding an optimal;market-basket data;answer set;order preserving;general problem;historical data
sample size;data summarization;random samples;large margin;data sets;biased samples;cardinality estimation;biased samples;network traffic
query optimization;query rewriting;rewriting queries;relational algebra;frequent itemset;relational database systems;set containment;data mining;operator;optimizer;normal form;join operator
theoretical analysis;multi-dimensional;frequent updates;location-dependent;disk accesses
virtual machine;query execution engine;java-based;database;virtual machines;query execution engine;data set;execution engine;operator;query-specific;query evaluation
conjunctive views;maximally contained rewritings;conjunctive queries
join algorithm;internal state;information sharing;data providers;information leakage;query processing;databases;applications requiring
data residing;query response time;database operations;privacy preserving;data integration;data sources;privacy preserving;query processing;privacy-preserving;answer queries;autonomous data sources;data integration;query processing
systems require;algorithm performs;lower bound;similarity scores;database;large databases;match scores;dictionary-based;input text;recognition systems;extraction accuracy;entity recognition;total cost
effectively exploit;training data;database;unstructured text;manually labeled;multi-relational;semi-markov;conditional random fields;inference algorithms;relational databases;statistical models;unstructured data;data integration
query answering over;effectively exploit;databases;query execution;database;ad-hoc;real-world;external knowledge;query answers;probabilistic approach;databases;query answering;data integration
minimal cost;real data sets;type information;dynamic programming;np-complete;input-output;web service composition;web services;rule based
data-driven;web applications;high-level language;web sites;data model for;concurrent updates;high-level language
database systems;database;applications ranging from;user requirements;user requests;transaction management;user-centric;database systems
indexing schemes;tree structures;multi-dimensional;database;multi-dimensional data;tree structure;query processing;range queries;load balancing
database;database engine;storage structure;historical queries;databases;immortal db;concurrency control;recent database;database engine
world wide web;ad hoc;base) relations;database;relational model;data model;replicated data;data management;web services;databases;application developers;query results;mobile devices;application code;data management
automated design;large-scale;database design;highly skewed;physical design;database design;tuning tools
range query;subgraph queries;answer set;indexing technique;index structure;subgraph isomorphism;edit distance;similarity queries;chemical compounds;query processing;subgraph queries;graph queries;graph queries;high accuracy;structured data;graph indexing;index size
cardinality estimates;query execution engine;optimizer;histogram construction;database;highly accurate;query feedback;query plan;data distribution
cardinality estimation;large numbers of;information systems;plans;query optimization;information retrieval applications;building block for;cardinality estimation;data items;internet-scale;highly distributed;highly accurate;databases;internet-scale;highly-distributed;indexing structures;key constraints;load balancing
pair-wise;semi) automatically;database;probabilistic model;peer data management systems;heterogeneous databases;knowledge creation;real-world;automatically-generated;data integration;implicit assumption
sampling approach;high-quality;block-level;sampling-based;large scale;dynamic nature of;database;aggregation queries;random sample;digital media;highly correlated;sampling techniques;databases;ad-hoc;aggregation queries;random walks
overlay networks;join queries;network nodes;query processing in;continuous query;query processing;internet-scale;network traffic;join queries;overlay networks
matching techniques;deep-web;query interfaces;matching systems;query interfaces;highly accurate;deep web;instances;data instances;web sources;question answering;automatically discover;matching accuracy
web databases;distance metrics;web databases;query relaxation;answering queries;categorical attributes;functional dependencies;user study;high precision
web databases;user interfaces;query interfaces;query interface;search engine;query interfaces;algorithm to compute;user interface;search engines;databases;access information;multiple sources
web sources;query forms;theoretical framework;hidden web;finding an optimal;web databases;web service;structured data from;high quality;graph traversal;selection techniques;database;data records;selection techniques;np-complete problem;structured information;databases;web sources;database servers;web search engines
sensor nodes;sensor readings;data collection;event-detection;probabilistic models;real-world;database research;spatial correlations;data reduction;data collection;sensor networks;sensor network;probabilistic models;sensor networks;query processors;result quality;simple models
stream-processing systems;multi-query optimization;large-scale;business logic;overlay network;stream-processing systems;query operators;operator;operator;stream-processing engine;performance goals;stream-processing
xml streams;vice versa;xml queries;xml streams;approximate query answering;functions defined;user-defined;instance;data streams;data stream management system;data streams;query languages;optimization techniques;memory management
high speed data streams;data streams;compression technique;data elements;data stream;relative error;worst case;space requirement;data streams;relative error
labeling scheme;natural language processing;large repositories of;ordered trees;tree model;query engine
xml documents;trade-offs
query optimization;xpath queries;selectivity estimation;real-world datasets;xml queries;data structures;xml elements
conjunctive views;maximally contained rewritings;streaming data;conjunctive queries
data rates;query processor;stream query processing;network monitoring;prohibitively expensive;data sources;rates;network monitoring;query processor;query results
service provider;update processing;real-life and synthetic data;distributed monitoring;high speed;emerging applications;space constraints;distributed environments;data streams;heavy hitters;resource constraints;traffic analysis
sparse data;query efficiency;sparse" data;storage utilization;relational database management systems;data sets;sparse datasets;wide range;query performance;vast amounts of
data analysis;specifically designed to;database;high-end;compression techniques;disk storage;query processing;storage systems;data mining;commercial) database;data-intensive;information retrieval systems;compression schemes
databases;specific problem;database
real data sets;synopsis structures;xpath queries;test cases;cardinality estimation;cardinality estimation;wide range;cardinality estimation;path queries
query optimization;structural joins;structural join;plan selection;xml data;xml query;theoretical analyses;xml elements;structural information;structural join;estimation techniques;xml query processing;estimation methods;size estimation
clustering;twig queries;real-life;data distributions;document structure;xml content;xml databases;xml elements;xml database;selectivity estimates;unified framework;xml content
computational cost;estimation problem;cost estimation;skyline operator;skyline queries;cardinality estimation;requires solving;skyline operator;microsoft sql server
multi-dimensional;multi-dimensional data;efficient computation;database;real data;effective pruning;issue queries;relational database
mobile user;multiple criteria;skyline query;skyline queries;data stored;ad hoc networks;resource-constrained;mobile devices;skyline queries;mobile ad hoc networks
search space;main memory;window size;online mining;data streams;aggregate functions;interesting patterns;complexity bound;aggregate queries over;multi-granularity;data streams;theoretical analysis;limited number of
real-world datasets;query optimization;sensor readings;linear program;plans;sampling-based;optimize queries;linear programming;sampling-based;prohibitively expensive;probability distributions;sensor networks;query processing;planning algorithms;result set;sensor networks
relational database system;database systems;database;free-form;query language;keyword-based;query answer;databases;relational databases;query results
query execution;window queries;data collection;trade-offs;sensor networks;query processing;query processing techniques;network topology;extensive simulations;sensor networks
desirable properties;efficient computation;moving objects;dense regions
real data;aggregate functions;memory requirements;aggregate function;computational cost
mining closed;search space;pruning methods;frequent subgraphs;stock market;complete set of;graph databases;graph mining algorithms;highly correlated;frequent closed;computing power;graph databases
algorithm finds;complete set of;graph mining;partition-based;frequent subgraphs;main memory;database;dynamic environment;databases;graph mining;algorithms typically;partition-based
labeling scheme;labeling process;reachability queries;label space;sparse graphs;labeling schemes;large graphs;massive datasets;wide range;applications involve;queries efficiently;xml indexing;reachability queries
nearest neighbor;rnn queries;storage scheme;database;ad-hoc;missing values;ad-hoc;nearest neighbors;query results;query returns;nearest neighbors;synthetic data
received increasing attention;nearest neighbor;continuous spatio-temporal queries;unique features;range query;query points;nearest neighbor query;location-aware;nearest neighbor;optimization techniques;query point;nearest neighbors
data structure;xpath queries;xml data;xml streams;complexity bound;compact data structure;query processor
shortest paths;nearest-neighbors;network distance;nn queries;minimum number of;database;upper bounds;shortest path;query processing;road networks;computational cost;network data;costly process;query processing
approximate answers;approximation algorithms;labeled trees;theoretical analyses;tree pattern;tree patterns;high confidence;streaming data;error bounds;xml documents;algorithm called;labeled trees;relative error;real datasets;data streams;tree pattern
real datasets;performance gains;sliding window;data stream applications;data stream applications;mathematical model;data stream;data streams;query processing techniques
evaluation cost;data formats;databases;query language;data model for;scientific databases;databases
data structure;supply chain;data analysis;radio frequency identification;data cube;rfid data;abstraction level;management systems;data sets;takes place;object tracking
clustering;feature space;high-dimensional feature space;image retrieval;retrieve images;feature vector;visual features;content-based image retrieval;retrieval results;relevance feedback;search results;nearest neighbors;similar images;content-based image retrieval;low-level;content-based image retrieval
nearest;nearest neighbor;multiple layers;nearest;real datasets;spatial objects;spatial queries;query point
data describing;moving objects;moving object;moving object;join algorithm
life sciences;complex queries;relational algebra;selectivity estimation;database;sequence data;real-world;biological sequences;optimization framework;query plans;pattern matching;biological sequences;vast amounts of
partition-based;large scale;indexing techniques;similarity search;graph queries;distance constraints;graph databases
clustering;pattern-based;biclustering algorithms;density-based;gene expression data;algorithms require;subspace clustering;expression levels;gene expression
service oriented;database;simulation studies;database applications;software systems;finer-grained;logic based;service-oriented;application systems;computing infrastructure;total cost
information systems;business process;clinical information;business process;information flow;business processes
database technology;financial data;database research;real-life;database technology;core components
rdf data;data structure;life sciences;database;directed graph;database;data model;relational tables;web resources;digital libraries;resource description framework;spatial network;metadata management
relational models;query semantics;relational schemas;view-based;rewriting algorithm;answering queries;data integration;relational databases;database integration;semantic constraints;relational data
data integrity;ontology-based;ontology based;database systems;related terms
hierarchical data;classification;data model;unified framework;open source;data integration
result sets;querying xml data;aqualogic data services platform;data exchange;relational database systems;data sources;service-oriented;xml-based;query results;sql queries;xquery expressions
13;database systems;query optimizer;database;natural languages;query operators;cross-lingual;query engine;14
memory footprint;database;memory usage;durability;transaction throughput;databases;persistent database;transaction processing
line segment;database applications;space-partitioning;data sets;space-partitioning;performance gains;disk-based
world wide web;classification problem;extraction task;web sites;classification methods;data sparsity;training data;web data;entity recognition;web data;automatically generate;decision making
load balancing;materialized view;data placement;auxiliary;database;query workload;database server;db2 udb;response times;materialized views;db2® universal database;database systems;data placement;base tables;load balancing;database systems
ad-hoc;databases
database tuning;total cost;information systems;information technology;cost-effective
statistical properties;data analysis;census data;private data;sharing data;medical data;data privacy;privacy-preserving
biological networks;database management;xml data;data structures;similarity search;emerging applications;complex structures;similarity search;traditional database;data mining;complex structures;data management
processing units;memory bandwidth;computational power;database;graphics processing units;data mining applications;general-purpose;data management;data volumes;query processing algorithms;processing power
data values;streaming applications;skyline computation;algorithm called;data sets;skyline operator;operator
algorithm performs;pairwise classification;duplicate detection;xml data;xml elements;xml data
relevance feedback;search space;real life data sets;user feedback;relevance feedback;multimedia retrieval;random accesses;sequential scan
class label;decision support applications;classification problem;classification problem;data mining application
high-confidence;association rule mining;information-theoretic;association rule mining;quantitative attributes;information-theoretic;mutual information between;attribute sets
pruning strategy;computational efficiency;search space;data mining technique;window size;synthetic data;frequent patterns;index structure;emerging patterns;real data;solution space;window sizes;efficient discovery of;fixed-size;data streams;efficient discovery of;application scenarios
high dimensional data sets;interesting patterns;data sets;statistical analysis;text processing;dimensional data;dimensional data;real world applications;data mining algorithms;huge number of;transactional data
lower bound;real-life datasets;time series;time series;periodic patterns;periodic patterns
database;databases
service provider;database operations;database operations;security analysis;data sets;2;6;database outsourcing;privacy risks
distributed databases;sensitive data;minimum number of
1, 2;security constraints;application domains;1;environmental conditions;temporal constraints
structured data;high accuracy
automatically extracted;language model;classification performance;text classification;latent semantic;classification algorithms;classification task;automatically discover;document set
web databases;count-based;query interface;query interfaces;schema matching;schema matching;greedy algorithm
great significance;web objects;highly heterogeneous;specific information;extraction task;information extraction techniques;web data management;information extraction;extraction accuracy;web sources;object-level;web object;extraction algorithm;conditional random fields
business process;information systems
query forms;database schema;query loads;query workload;database;similar"queries;similar" queries;clustering algorithm;relevant information;database schemas;data organization
systems support;view update;update operations;relational systems;native xml;xml views
xml data;conjunctive views;maximally contained rewritings;conjunctive queries
optimization techniques;nested queries;xpath queries;subexpressions
applications including;database;materialized views;incremental maintenance;base data;query processing
database systems;control mechanisms;native xml;2, 3, 4, 6, 5;concurrency control;concurrency control;5;3, 7, 8;increasingly popular
clustering;sql/xml;historical data;database systems;database;relational databases;indexing techniques;temporal queries;xml views;relational database;databases;relational databases;query conditions
query execution;query optimization;database tuning;databases;query cost;query processing in;selection" queries;databases;extensive simulation
databases;database application;ad-hoc;database applications;mobile ad-hoc network;data replication;replication;data replication;databases;occur frequently
high-dimensional;complete information;feature vector;synthetic data;information retrieval;test collections;nodes represent;wavelet-based;relevant information;super-peer;feature vectors;internal nodes;retrieval method
data access;data access
application domain;databases;resource discovery;moving objects;mobile ad-hoc network;local information
window queries;data stream management;memory usage;continuous data streams;user-defined;query answers;window sizes;data stream systems;memory management
spatial data;data set;data stream;rapid rate;data streams;5, 6
join algorithm;communication cost;sliding window;join algorithms;data stream;distributed data streams;distributed data streams
data warehouse;data cleaning;sensor data streams;stream processing;sensor devices;data streams
11;stream processing systems;traffic control;stream processing engine;input data;query results;stream processing engine;1, 2, 5, 6;data sources;stream processing;3;data stream;financial services;high-level;data feeds;data entry;traffic analysis
cpu cost;data streaming
sensor node;energy-efficient;sensor networks;sensor nodes
spatial locations;index structure;3;multi-resolution;retrieval results;similarity search;2;7;6;8;higher-quality
sensor networks;energy-efficient;query result;sensor networks;environmental monitoring
burst detection;large number of;data stream applications;burst detection;data structures;window sizes;1;real world;search algorithm
supply chain management;black box;optimize queries;query processors;monitoring applications;operator;query processor;user-defined functions;continuous queries
sensor readings;spatial aggregation;spatial regions;database applications;query evaluation;spatial region;power consumption;sensor networks;spatial aggregation
data structure;databases;time series;time series;similarity queries;similarity search;similarity queries;databases;support threshold;queries efficiently;user-defined
storage management;data migration;storage management;large-scale
materialized view;range query;database table;database
query results;real life applications;path expressions;xml data;xml query;query language;xml query language;query processing in;query formulation;user-friendly;query performance;considerable effort
optimal performance;transaction processing
database systems;database;data partitioning;distributed database;distributed transactions;update transactions
network traffic;data stream;data stream management system;data streams
multi-user;database;database management;query language;data model;object-oriented;object-oriented;distinctive features;database management system
instance;data warehouses;data warehouse;business process;external information;data warehouse;common practice;multidimensional data
data sharing;nearest neighbor;nearest neighbor query;distributed processing;location-dependent;location-based services;query results;spatial databases
relevant documents;data publishing;ir systems;content-based retrieval;selection method;selecting informative;2;content-based retrieval
complex queries;database;database applications;sharing opportunities;sharing opportunities;massive datasets;data warehousing;intermediate results;operator;database engines;instances;concurrent queries
'orgchart';query processing;spatial networks
efficient parallel;indexing schemes;parallel computation;cost model;data cubes;meta data;data cubes;query engine
1;computational resources;grid environment;query processing;query processing
network distance;spatial networks;location-based services;objects moving;data mining algorithms;spatial network;visual exploration of
1;traffic flow
stream processing;continuous queries over data streams;data stream processing;real-world applications
recommendation systems;applications including;database;large scale;databases;database systems
conjunctive views;maximally contained rewritings;conjunctive queries
structural relationships;security policies;security views;security constraints;access control
databases;ad-hoc;transaction management;mobile ad-hoc network;database transactions;database application;databases;mobile ad-hoc network;mobile ad-hoc network;transaction management
reduction techniques;search space;pattern mining;constraint-based mining;constraint-based;query language;user-defined;interesting patterns;constraint-based;pattern discovery;pattern discovery
schema mapping;database;data exchange;view update;business processes;constraint-based
database size;database;data sources;data integration;efficiently computing;key constraints
schema integration;ad hoc;schema mapping;schema integration;schema matching;instance-level;semantic correspondences;instances;common knowledge;data integration
web interfaces;web sources;query interfaces;search engines;search engine;knowledge sources
ontology-based;modeling task;application domain;context-aware;domain ontology;ontology-based;data management;domain models
multiple data sources;ontology-driven;semantic matching;schema matching;data instances;schema matching;share similar;databases;semantic information;database schemas;ontology-driven
query rewriting;algorithm to compute;efficient search;query language;query rewriting;data model;semantic web;physical resources;navigational queries;web accessible
data objects;similarity searches;management systems;database;management systems;multimedia database;multimedia database;content-based image retrieval
information systems;existing database;database;video data;semantic concepts;computational costs;high-level;multimedia retrieval;video database;learning strategies;markov model;modeling methods;multimedia database
segmentation problem;image databases;multimedia databases;query processing;image similarity;retrieval task;multimedia database
matching algorithm;similarity measure;temporal information;video search;index structure;video object;user query;temporal information;content-based video;video retrieval
clustering;similarity query;information systems;navigation systems;clustering technique;clustering method;time series;moving object;similarity queries;moving objects;motion patterns
digital images;nearest neighbor;image features;classification accuracy;color space;candidate set;semantic categories;color histogram;image collections;image classification;image classification;nearest neighbors
music retrieval;query processing;tree index;similarity-based
domain knowledge;content analysis;efficient representation;database;document understanding;automatically generated;databases;search algorithm;large database;document centric
video sequence;classification;classification framework;principal component analysis;pca-based;classification algorithms
data structure;store data;scalable distributed;data structures
access control;dynamic environments;decision support;trust management;trust management
game theoretic;large data sets;theoretical results;data structures;theoretical result;answer queries
web services;computing environments
real world;virtual communities
data objects;multiple users;multi-user;database
stream processing systems;output quality;distributed stream processing system;dynamic systems;large-scale
query optimization;multi-query optimization;multiple queries;network processing;intermediate results;multiple continuous queries;optimization problem;network nodes;query plans;operator;stream systems;distributed data
distributed algorithm for;simulation study;network resources;overlay network;high-throughput;data streams;data dissemination;high volume
intrusion detection;streaming algorithms;internet traffic;streaming algorithms;data streaming;traffic data;network monitoring;internet traffic;distributed data;extensive simulations
intrusion detection systems;network security;generalized queries;database;query language;query interface;sql queries;query language;case study;intrusion detection system;aggregate views;network intrusion;instances
estimation accuracy;frequent items;anomalous behavior;traffic data;data set;data sets;data stream;cardinality estimation;memory resources;estimation methods
cost estimation;large number of;emerging applications;network management;highly distributed;ad hoc querying;query planning;ad hoc querying;data aggregation
resource sharing;specific applications;content distribution;web content;network monitoring;web caching;distributed systems
data management systems;similarity queries;query types;data organization;large-scaled;instance;similarity queries;exact match;structured data
node failures;search terms;query patterns;large-scale;text search;data store;data migration;data partitioning;query load
long-running queries;sensor network;applications including;database;large volumes of data;database applications;rule-based;network applications;information processing;environmental monitoring;sensor networks;sensor data;sensor network;databases;application logic;continuous queries;data processing;active database;distributed environment
storage schemes;classification;query types;storage capacity;sensor networks;sensor network;sensor networks;storage management
world wide web;web pages;social network analysis;link structure;information retrieval;page rank;3;9;web page
ground truth;aggregation algorithm;noise level;aggregation methods;rank aggregation;rank aggregation;markov chain;statistical framework
hybrid approach;ontology-based;service descriptions;description logics;highly dynamic;distributed environments;user request;similarity-based;service discovery;service discovery;automated techniques
software development;ontology-based;general-purpose;service-oriented;web services;software components;pre-processing
similar documents;clustering method;similarity function;document clustering;traditional clustering;document clustering
search results;web pages;visual cues;spatio-temporal;web information extraction;information extraction;user interface;search strategies;web page;increasing amounts of;temporal data;complex relationships;search behavior
real-life;web sites;web-site;web interface;web pages
web directories;web documents;relevant documents;query logs;query log;search engine;user clicks
world wide web;user navigation;log data;data processing;web usage;user session;data mining techniques;valuable information;web servers;required information;user sessions;pre)processing;web site;data processing;relevant information;web usage mining;web server;web usage data;web users;web usage mining
clustering;clustering;matching algorithms;large scale;schema matching;schema matching;data-type;semantic mappings;optimization techniques;matching problem;schema matching
sampling method;web interfaces;query interfaces;text descriptions;large number of;schema matching;schema matching;data sources;domain values;noisy data
theoretical framework;data sources;inference algorithm;web pages;real-life;domain-independent;extracting data from
web documents;tree structures;tool called;mapping rules;html documents;semantic interpretation;user-friendly;semi-automated;user input
text mining;raw data;decision-making;automatically extract;extracted data
search results;automatic extraction of;news items;data collected from;search result;search engines;extraction accuracy;search engine;automatically extract
higher precision;web search;hash table;fully operational;specific queries;real-world;search result quality;result quality;web crawl;search engine;typically rely on;rich information;web search;web data
major search engines;semi-structured data;search engine;search engines;web contents;search engine;frequently updated;web data
query language;semantic web applications;semantic web;relational database;relational databases;relational data
taking into account;synthetic data;content distribution;np complete;replication;heuristic method;low level;content providers;distributed data;data management
case studies;world wide web;ontology-based;knowledge bases;inference engine;databases;human experts;semantic web applications;database schemas;database content;data resources;semantic web;ontology-based;ontology language;databases;query answering;multiple domains
web service;dynamic nature of;web service composition;logic-based;web services;composite service;composite services;service discovery;service selection
human effort;weighted graph;trust management;social network analysis;semantic web;semantic web;unique characteristics;multiagent systems;artificial intelligence;trust model
database technology;storage structures;data model;physical design;instances;semantic web;knowledge base
data centric;access patterns;data sets;long-term;semantic information;working group
semantic web;database
semantic web;database schema
traditional information retrieval;ir systems;irrelevant documents;ir) systems;word frequency;retrieved documents;web search engines
data sources;physical resources;semantic model;biological data;data model
workflow management systems;large-scale;data flow;log data;high-level;resource utilization;low-level
large volumes of data;data acquisition;grid services;resource discovery;data movement;data processing;databases
social science;large-scale;data flows;massive amounts of;large-scale;raw data;case studies;data flows;data management
scientific workflow;data sources;database management system
web services;semantic interoperability;execution engine;data flow;workflow engine;semi-automated
web-based;information systems;user actions;workflow management systems;related data;workflow management;information systems
handle complex;control-flow;real-world;control-flow;generic framework;data-centric;scientific workflow
data exploration;visual representations;data exploration
data-driven;scientific applications;derived data;data quality
scientific workflow;high volume;simulation framework;data transfers;multi-scale
dynamic environments;service descriptions;dynamic environments;semantic web;semantic web services;web services;service discovery;service discovery
semantic web;web services;distributed systems;basic concepts
optimization framework;content distribution;data types;tree-based;overlay network;performance goals;wide range;data dissemination;data dissemination;high-level;overlay networks
context aware;stream processor;traffic monitoring;end users;data streams
parallel processing;context-free;context-free;streaming data;xml content;hardware architecture;automatically generated;field programmable gate
ontology driven;training methods;classification
data source;access control;web service;aqualogic data services platform;data sources;xml-based;query processor;xml schema
regular expressions;xml data;data exchange;multiple documents;xml documents;efficient querying;5;data integration;memory consumption;xml schema
xml documents;data types;xml document;xml schema
human effort;large scale;upper bounds;schema matching;retrieval systems;scalability problems;matching systems;effectiveness measures;answer sets;test collection
xml documents;sequence alignment;life sciences;xml documents;context-specific;contextual information;protein sequences
structural joins;structural join;memory usage;context-aware;xml streams;stream processing;operator;tuple-level;xml stream;processing queries
xpath queries;rule set;xquery expressions;xpath query;xpath query;heuristic method;xml query language;execution times
xml documents;document-centric;context-free;textual content;xml documents;document-centric;xml document
document collection;semi-structured documents;document collections;clustering methods;semi-supervised;clustering;clustering results;semi-structured documents;semi-supervised clustering;structural information;highly relevant;prior knowledge;xml documents;structural information;clustering algorithm;textual documents;clustering model;web applications;xml format;major source of;constrained clustering;supervision;clustering task;logical structure;digital libraries
xml documents;event-based;poor performance
privacy policy;privacy policy;real life;confidential information;usage information
database;sharing data;data integration;privacy preserving;homeland security;privacy preserving;data privacy;data integration;data mining;data integration
data values;additive noise;data mining techniques;private information;sensitive data;privacy preserving data mining;privacy preserving;perturbed data;random perturbation;continuous data
privacy-aware;database server;location-based services;private information;user location;database servers;spatial region;location information;user privacy;location-aware;location-based;location-based;spatio-temporal queries;privacy threat;main idea

clustering;clustering;classification;data mining techniques;record linkage;multiple sites;databases;hidden information;pair-wise;privacy preserving data mining;database;privacy preserving;data mining tools;data mining;data mining models;association rules;data objects;distributed environments;privacy preserving;privacy preserving
clustering;generalization hierarchies;user defined
service provider;data analysis;user queries;random walk;markov chain;iterative process;probability distribution;data analysis;markov chain
database contents;nested queries;relational algebra;database;databases;aggregate queries;execution strategies;aggregate queries over;query processing;databases;relational databases;sql queries;encrypted database;query processing cost
service oriented;information exchange
xml databases;multiple queries;private information;query language;static analysis;xml databases;user queries
information systems;computing environments;personal data;privacy issues;data protection;transactional data
object oriented;management systems;data objects;query language;complex relationships
handle complex;theoretical framework;category-based;information processing;heterogeneous data;data set;category-based;structured data
multimedia databases;event-driven;application domain;data management;enabling users to;data management
large number of;real-life;multimedia data;efficient retrieval
competitive performance;motion information;privacy concerns;machine learning methods to
object model;external sources;semantic network;patient data;efficient querying;primary goal
conceptual modeling;taking into account;medical records;high-level;representation language
decision making
file systems;high-level;file systems
multiple communities;kernel-based;rank documents;multiple communities;generation process;latent semantic indexing;citation graph;weighted graph
xpath queries;xml instances;xml format;large-scale;xml data;data partitioning;xml queries;query workload;relational tables;xml databases;query processing;xml database;parallel processing;sql queries;xml fragments
dimensionality reduction techniques;singular value decomposition;fourier transform;data representation;time series;time series;data set;similarity searching
clustering;join algorithms;xml data;matching problem;xml documents
mining frequent itemsets from;fp-tree;frequent itemsets;huge amounts of;3, 4, 6, 10;2;noisy data;data mining;6, 10;noisy data;real world
text mining;text data;mining algorithm;sequential pattern mining;sequential pattern;customer reviews
frequent itemset;multi-dimensional;web sites;multi-dimensional;data mining;data-mining tasks
data objects;clustering;feature extraction method;classification;outlier detection;class label;high dimensional data sets;clustering methods;clustering method;high accuracy;class labels;classification problems;benchmark data sets;sensitive attributes
similar objects;outlier detection;financial data;stock market;time series;time series
database;data mining system;mining process;database
distance measure;index structure;time series;fast computation;similarity search;databases;time-series;efficient similarity search
concurrency control;large-scale;control method;low frequency;tree structure
large number of;stored data;data volume;data management
database;large scale;fixed size;data structures;scientific databases;databases;multidimensional data
databases;basic concepts;database systems;database
optimizer;sql queries;sql queries;commercial dbms;share common;optimization techniques
high throughput;target object
query keywords;web pages;clustering method;time series;web archives
search results;web pages;web spam;search engines;link-based;ranking methods;link spam;link analysis techniques
current location;local information;search engines;visual content
local appearance;keyword queries;web search;user interface;web search results;semantic relationships;web retrieval
search results;desktop search;web pages;web search;link structure;pagerank algorithm;text data;web pages;web contents
web pages;web graph;large-scale;web pages;web archives;web crawling
web search;link structure;semantic relationships;human experts;named-entities;semantic relationships;ranking documents;complex relationships
relational model;relational databases;nested queries;operator;sql queries
semantic annotation;semantic annotation;web pages;data-extraction;semantic annotation;data-extraction;semantic web;ontology-based;human intervention;web data
query execution;window queries;streaming applications;continuous query;data stream management system;efficient query evaluation;data stream management systems;query language;query optimization;data stream management systems;sliding-window;continuous queries
data sharing;ad-hoc networks;window queries;query results;database;simulation results;spatial queries;mobile clients;location-dependent;databases;location-based;query load;spatial queries;information exchange;nearest-neighbor queries
query evaluation;structural join;plans;database;1, 22;holistic twig join;management systems;3, 6, 16;concurrent updates;query processing;xml database;operator;relational dbmss;xml query processing;query processing
algorithm produces;query routing;ant colony optimization
high-level;software tools;user requirements;web service composition;composite web services
data loss;real-world;data stream management systems;data processing;ad hoc;data stream management systems
video data;domain-specific;temporal structure;temporal structure
protein structures;discovered patterns;real-life problems;network intrusion detection;data describing;temporal patterns;data mining;temporal data;web personalization;data set;instance;data sets;data mining techniques;scientific domains;temporal patterns;data mining;spatiotemporal data;application domains;scientific data
data volume;data stream management;high-volume;massively parallel;stream processing;sensor networks;stream queries;data streams;high-volume;measurement data;stream queries;data stream management system
large scale;distributed stream processing system;distributed stream processing
speech recognition;signal processing;database;database;emotion recognition;video sequences;audio-visual;audio-visual
classification;sequence mining;frequent itemset mining;data mining;building block for;clustering;open source;data mining methods;international workshop on;frequent itemset mining;data mining problems;source code;graph mining;data mining processes;huge number of;frequent pattern mining;frequent pattern mining algorithms;outlier detection;representative set;open source data mining;program committee;frequent pattern mining
text mining;text mining;biological entities;data mining research;gene expression analysis;protein structure;data mining techniques;information from multiple sources;clinical practice;drug design;sequence databases;extracting knowledge from;data mining;program committee;international workshop on;gene expression;drug design
graph theory;social science;information retrieval;wide range;natural language processing;multiple sources;link analysis;pattern analysis;recommendation systems;link discovery;hidden structure;international workshop on;social network analysis;social networks;spatial databases;graph theory;link discovery;data mining;information technology;data mining techniques;link discovery;machine learning;homeland security
high quality;sensor technology;acm international workshop on;data engineering;acm sigmod;selection process;data engineering;san diego;mobile computing;data engineering;mobile environment;mobile access;acm international workshop on;databases;location-based;program committee;real world;data management;mobile access
personal information management;large number of;information management;semantic integration;data sources;data integration systems;semantic relationships;digital libraries;query answering;diverse applications;data management
satisfiability problem;tree structure;implication problem;reasoning tasks;data values
data structure;running times;0,1;range queries;simulation results
schema mapping;target instance;data exchange;decision problem;source schema;schema mappings;combined complexity;instance;data complexity;data exchange
tuple generating dependencies;data exchange problem;data exchange;database;general setting;9;data exchange
schema mapping;target instance;tuple-generating dependencies;source schema;instance;target instances;instances;schema mappings;source instances
closed world;taking into account;data exchange;databases;source schema;incomplete information;instance;answering queries;evaluation techniques;theoretical foundations;query answering;target instances;data exchange;open world
query evaluation;sufficient conditions for;sql queries;sql query;bag-set semantics;query equivalence;sufficient condition for;set-semantics
total number of;database systems;database;np-complete;sql queries;query processing and optimization;query-containment;bag-set semantics;conjunctive queries;query containment;set semantics
data-driven;internal state;database;web service;web services;local database;user input
end-users;web information systems;tree-structured;instance;data access;ad hoc
knowledge bases;expected-utility;probabilistic reasoning;statistical information;irrelevant information;intelligent agent;decision theory;database systems
information content;0,1;information-theoretic measure;database design;information-theoretic;normal forms;instance;normal form
bounded treewidth;database design;single attribute;algorithms produce;relational schema;functional dependencies;np-hard
ranking scheme;web pages;web graph;link structure;18;graph models;real-world;power law distribution
approximate answers;multiple users;privacy guarantees;computational power;large-scale;private information;partial knowledge;private data;complex queries;log log o(m
clustering;approximation algorithms;social security;15;16;database records;data records;individual privacy;increasing importance
sensitive information;query classes;query-answering;privacy-preserving;conjunctive queries;answer queries;query containment
finding an optimal;relational databases;data complexity
planning problems;conjunctive queries;queries posed;computationally hard;query plan
filter ordering;high-speed data streams;sensor networks;query optimization;database
greedy algorithms;extreme values;anomaly detection;database applications;resource-constrained;objective functions;optimization problem;complex systems;sensor networks;query plans;query optimizers;data stream systems;rates;objective function;np-hard
tree-structured data;compute nodes;conjunctive queries;query languages;processing queries;processing queries
relational algebra;worst-case;relational database systems;data item;join result;algorithms for computing;internal memory;np-hard
unlike standard;data structure;tree) data structure;compression scheme;range queries
query evaluation;lower bound;monte carlo;input data;lower bounds;memory size;relational algebra;large data sets;decision problems;data stream model;data streams;error bounded;external memory;data complexity;random access;worst case;computation model;streaming data;memory space;random accesses;9, 10;sequential scans
clustering;instance;million nodes;frequent patterns;approximation quality;web communities;application domains;large graphs;instances;20, 8;space requirement;data streams;sample sizes;sampling algorithms;space complexity
skewed data;extreme values;update cost;theoretical properties;data streams;high speed
data stream;sensor networks
data structure;12;lower bound;14;16;data streams;mobile computing;data streaming;fundamental problem;distributed systems
frequent items;data structures;sliding windows;sliding window;variable-size
statistical properties;naïve;communication overhead;real-life applications;sampling-based;frequent items;large number of;sketch-based;data set;communication cost;data sets;wide range;central server;high accuracy
information dissemination;high probability;sensor networks;overlay network
xml documents;xpath expression;labeled tree
transitive closure;operator;core xpath
database;query language;relational operators;view updates;view update;handle arbitrary;view definition;functional dependencies
clustering;clustering problems;objective function;np-complete;clustering algorithm
motik, 2005;world wide web;years ago;gutierrez et al., 2004;de bruijn et al., 2005;database;answer set;additional constraints;query language;data model;ontology language;hayes, 2004;patel-schneider et al., 2004;graph theoretical;resource description framework;chen et al., 1993;prud'hommeaux and seaborne, 2006;query answering;knowledge representation;formal semantics
query answering over;functional dependencies;ontology-based;open-world;database;query language;database theory;databases;conjunctive queries;view-based;incomplete information;query processing in;information systems;information access;query answering;relational databases;query languages;query containment;data exchange;data integration
database systems;tree search;index structures;distributed search;effective search;multi-attribute queries;tree structure;update cost
data sharing;database instances;instance;data consistency;provenance information;concurrency control;ad hoc;data integration
data stream applications;false positive;data structure;limited space;streaming data;duplicate elimination;data stream;rates;upper bound;simple algorithm;streaming environments
bag-set semantics;query optimization;query evaluation;completeness;relational queries;19;general setting;1, 8;aggregate queries;base relations;view-based;materialized views;answering queries;execution plans for;cost-based;conjunctive queries;set semantics;aggregation functions;query evaluation;optimization approach
query optimization;query rewriting;database;relational database systems;user-defined;view maintenance;user-defined;aggregate functions
naïve;plans;ad-hoc;query plans;aggregate queries;query operators;query processing;query processor;upper-bound;provably optimal;principled framework;rank-aware
data arrives;data independence;open source;database systems;database;real-world;databases;frequent updates;sensor networks;traditional database;probabilistic models;data store;frequently updated
problem instances;matching algorithms;join operations;database;problems arise;instances;data sets;file systems;resource allocation
query execution;query evaluation;query processing;naïve;application area;database;application-level;database research;query engine;optimization opportunities;23, 24, 33;distributed systems;overlay networks
forensic analysis;efficiently extract;hash functions;forensic analysis;database
data residing;database;index structures;dynamic environments;analytical models;outsourced databases
fine-grained access control;query optimizer;coarse grained;database applications;access control;query plan;query plans;fine grained access control;information leakage;user-defined functions;microsoft sql server;fine-grained access control
pattern-based;pattern matching;object tracking;network applications;real-world;temporal patterns;event detection;sensory data;sensor networks;map matching;energy-efficient;data distribution
minimal cost;energy-efficient;data collection;sensor networks;sensor data
extreme values;trade-offs;real-world;energy-efficient;selection queries;local constraints;sensor networks;dynamic systems;fundamental problem;query processing algorithms;query result;network topology
skewed data;intrusion detection;heavy hitters;statistical summaries;data stream management system;ip network;skewed;data stream applications;parameter estimates;data stream;stream data;traffic data;data streams;model fitting;power-law;quality guarantees;high speed
large amounts of;approximation techniques;distributed computation;random variables;selectivity estimation;aggregate queries;spatial joins;pseudo-random;exact computation;data-streaming
synthetic data sets;relational queries;selectivity estimation;semi-structured;construction algorithm;selectivity estimates;summarization techniques;data set;real-life;graph-based;relational database;join queries
privacy guarantees;heuristic approaches;sensitive information;data publishing;privacy requirements;additional information
protect privacy;preserving privacy;sensitive data
service provider;web applications;data updates;database;database;sensitive data;static analysis;query answers;web applications;credit card;cost-effective;data-intensive
completeness;access methods;information sources;web service;query formulation;sql server;integration systems
large-scale;information extraction;specific topics;execution plans for;completeness;query optimizer;unstructured text;cost-based;text databases;execution plans for;graph theory;plans;textual data;real-life data sets;cost model;text database;task-specific;ad-hoc;specific properties;execution plans;model parameters;search engine
search results;focused primarily on;web search;query processor;real data;large sets of;search engines;query processing in;local search;query processing;data processing;search engine;web search engines;query processing
distributed monitoring;communication overhead;detecting anomalies;linear combination;observed data;theoretical results;monitoring applications;sensor networks;extensive simulations
communication overhead;real-world;feature selection;data streams;highly scalable;distributed data streams;information gain
continuous query processing;multi-dimensional;dimensional space;join queries;ad-hoc queries;spatial join;sliding window;data elements;query processing in;query processing;data streams;data stream systems;spatial join;continuous queries
memory bandwidth;memory-intensive;database management;high-end;takes into account;main memory;large databases;communication bandwidth;cpu-based
database systems;costly process;user transactions;database
query throughput;complex queries;long-running queries;continuous query;input data;main memory;query plan;data streams;operator;memory intensive
search space;database;constrained optimization;databases;margins;structured data;processing queries
database;text search;real datasets;product review;search engines;document collections;keyword query;keyword search;document scores
context-sensitive;processing queries
structured databases;automatic selection;large number of;data items;information retrieval;optimization problem;rank aggregation;ranked) list;query results;user study
quality-aware;continuous query;classification;overlay network;network topology;bandwidth consumption;quality-aware;rates;computing resources;data streams;graph construction;data delivery
stream processing;stream processing systems;stream processing;stream processing engine;database;data locality;large-scale;streaming data;stream processing;linear road;processing nodes;distributed environments;data streams;linear road;distributed environment;resource allocation;data management;performance bottlenecks
xquery queries;complex queries;xml queries;xquery processing;large numbers of;tree pattern queries
source nodes;xml data;multi-dimensional;xml element;update cost;0,1;security policies;meta-data;index structures;data representation;xpath query;index structure;xml elements;multi-level;efficiently identify;meta-data;benchmark data sets;quality guarantees
query evaluation;structural join;evaluation strategy;twig pattern;higher throughput;pattern queries;join operators;xml document;holistic twig join;management systems;input sequences;user access;xml database;rates;data consistency;xml query processing
data sizes;database technology;relational algebra;evaluation strategies;optimization strategy;xquery processing;relational tables;xml documents;lessons learned;xml update;main features;xml database;data management
skyline query;update cost;frequently updated;databases;frequent updates;query cost;dynamic environment;skyline queries;data mining;issue queries;preference queries;multi-criteria decision making;skyline objects
real data sets;skyline algorithms;data set;skyline points;skyline queries;decision making;high dimensional space
data objects;received increasing attention;nearest neighbor;storage overhead;distance function;real-world;index structures;scientific applications;data set;lower bounds;vector data;metric spaces;nearest neighbors;nearest neighbor search
linear transformation;index structure;indexing techniques;large databases;lower-bounds;similarity search;efficient similarity search;dimensionality-reduction technique
general purpose;database;general-purpose;provenance information;databases;source databases;database systems
index structures;query execution;local search;efficient indexing;massive volumes;disk page;indexing techniques;scientific applications;query performance;application domain;high resolution;query processing;data layout;analysis tool;data management;data volumes
keyword queries;search effectiveness;database;text search;application level;growing rapidly;query language;text data;information retrieval;keyword-based search;ir) style;text databases;relevant answers;relational databases;relational databases;keyword search;real world;database schemas;keyword search on
keyword queries;xml queries;text queries;document structure;evaluation techniques;query languages;xml search;query terms
processing cost;large-scale;query semantics;database server;query types;publish/subscribe systems;database servers;database;publish/subscribe system;design space;network traffic;primary goal
replication;concurrency control;update transactions;serializability;database
optimization framework;data types;tree-based;overlay network;performance goals;wide range;real-world;data dissemination
selection predicates;query optimization;network intrusion detection;query workloads;aggregate queries;real data;data streaming;monitoring applications
query retrieves the;main memory;long-running queries;dynamic environments;query types;space requirements;rates;data stream model;databases;window size;sliding windows;fixed-size
local patterns;window sizes;meaningful patterns;multi-scale;time series
subspace analysis;query processing strategies;data cube;data cube;dominant relationship;efficient computation;skyline computation;data set;dominant relationship
database systems;database;query workload;column-oriented;compression schemes;databases;column-oriented;storing data
sql queries;database design;automatic selection;physical design;performance tuning;relational database system;microsoft sql server;database vendors;total cost
data services;aqualogic data services platform;aqualogic data services platform;data sources;service-oriented;key features;data delivery
xml data;domain specific;general-purpose;query operators;relational data;programming language
software development;web applications
web pages;current commercial;machine learning
anonymized data;false positives;historical data;window size;privacy-preserving
attribute values;string matching;large databases;ad hoc;data quality
data sizes;relational database;business intelligence applications
generated automatically;large databases;visual representations;time series;visual analysis;1;formal language;data sources;visual representation;relational databases;query languages;microsoft sql server;low-level
large volume;xml processing;push based;database;data stored in;oracle database;source data;relational databases;database engine;oracle database;continuous queries
allowing users to;xml-based
fast approximate;query optimization;queries involving;db2 universal database;current commercial;join results;efficient computation;db2 udb;cardinality estimates;base-table;fact table;cardinality estimation;join result;common practice;query results;database systems
large-scale;data sets;data management;structured data;highly scalable;data management
data-description;helps users;data formats;relational tables;data sources;enormous amounts of;data analysts;ad hoc;vast amounts of data;query engine
collecting data;low-cost;mobile devices;spatial queries;sensory data;data management;data management system
support vector machines;graph mining;vector machine;application domains;graph kernels;graphical user interface;graph data;data mining;graph mining;graph kernels;increasing importance;data management;nearest neighbor search
data transformation;data integration;data integration
database applications;databases;database;database application
data services;aqualogic data services platform;xml query;aqualogic data services platform;data sources;relational tables;query operators;fine-grained;service-oriented;stored procedures;data access;web services;query optimizations;relational views
error-prone;scientific discovery;scientific data;data management
query answer;database management system;trajectory data;continuous spatio-temporal queries;object relational
language independent;real-world;data sources;information systems;ontology alignment
databases
approximate query answering;database systems;database;random samples;approximate query processing;optimization criteria;data analysis;query processing;query answering;sampling scheme;expert knowledge;error bounds;sql extension;open-source
data-driven;web applications;vice versa;computing power;application server;language called;web browser;application code;high-level;programming models;application development
database;context-aware;multi-channel;cross-media;information stored in;mobile access;databases;content management
network models;database systems;probabilistic models;database server;wide range;interactive visualization
data collection;optimization framework;data types;application-level;tree-based;overlay network;optimization model;performance goals;wide range
data-driven;conjunctive predicates;estimation process;web applications;optimizer;maximum entropy;database;complex queries;real-world;partial knowledge;valuable information;joint distribution;query execution times;ibm db;query execution plans;ad hoc;cost-based query optimizer;selectivity estimates;relational database management system;plan choices
conjunctive predicates;estimation process;query execution plans;optimizer;maximum entropy;database;real-world;partial knowledge;selectivity estimates;valuable information;joint distribution;query execution times;ibm db;complex queries;ad hoc;cost-based query optimizer;cardinality estimation;relational database management system;plan choices
acoustic features;neural networks;classification;information processing;emerging applications;classification process;databases;low-level
open-source;text search;xml data;keyword search over;xml views;xml query language;data management system
database systems;database;large scale;knowledge management;cross-lingual information retrieval;information retrieval techniques
location-based queries;database;data availability;citation;limited bandwidth;computing environments;location-based service;data access;location-based services;caching) scheme;physical objects
keyword queries;data store;database;high-precision;information extraction techniques;information retrieval;data set;search engines;semantic search;search engine;keyword search;semantic search;search engine
cost-effective;query load;query performance;temporal patterns;database
query results;keyword search on;databases
software tools;database;web communities;extracted information;extraction process;information extraction;machine learning;unified framework;information extraction from;prior knowledge;unstructured data;extraction techniques
web-based;xml processing;programming languages;xml applications;xml data;emerging applications;large datasets;data-intensive;programming paradigm
duplicate records;record linkage;information retrieval;record linkage;databases;machine learning;similarity measures
query forms;large scale;deep web;aggregate data;information integration;information access;online databases;data management
machine learning;international conference on;international conference on;machine learning;learning theory;review process;program committee;machine learning;program committee;artificial intelligence;poster session
high-dimensional;large numbers of;reinforcement learning;theoretical results;model-free;real-life;continuous-state;policy search;optimal performance;markov decision process
potential function;financial data
semi-supervised;learning problem;higher order;graph theoretic;higher order
regularization;classification;ranking function;real-valued;graph data;ranking functions
data mining techniques;machine learning;counterpart;principal components;canonical correlation analysis;principal component analysis;em algorithm;maximum likelihood;pre-processing
regularization;kernel selection;optimization problem;greedy algorithm;learning task;benchmark data sets
large state spaces;temporal difference learning;multi-agent;temporal difference;decision problems;rates
clustering;data points;random walk;statistical model;data driven;transition probability;markov random walk;iterative algorithm
supervised learning;sample complexity;classifier;active learning algorithm;active learning
sufficient conditions for;data points;kernel function;similarity-based;kernel functions;large margin;similarity functions;machine learning;similarity function;problem domain;learning problem;learning task;high dimensional space
machine learning
penalty term;graphical models;multivariate gaussian;large-scale;convex optimization;coordinate descent;scale poorly;genomic data;covariance matrix;problem size;maximum likelihood;optimization techniques;interior-point methods;model selection
data structure;nearest neighbor;intrinsic dimensionality;point set;data set;tree data structure;nearest neighbor queries;machine learning;metric spaces;brute force search
maximum likelihood;graph model;probability distribution over;small-world;power-law;data set;graph models;algorithms for computing;maximum likelihood;real-world graphs;model selection criterion;model selection
state space;predictive models;topic models;time series;large document collections;document collection;latent topics
optimization problems;combinatorial optimization problem;probability distribution over;predictive distribution;learning problem
traditional models;efficient optimization;decision processes
regression algorithm;error reduction;semi-supervised;semi-supervised;semi-parametric
learning algorithm;input output;semi-supervised learning;feature representations;support vector;relational learning;semi-supervised learning;output variables;semi-supervised;loss functions
clustering;data point;clustering structure;spectral clustering;approximation error;linear order;principal component;connected-components
supervised learning methods;decision trees;large-scale;naive bayes;neural nets;logistic regression;supervised learning algorithms;supervised learning
cost functions;semidefinite programming;general-purpose;memory intensive;low-dimensional;dimensionality reduction;np-hard
hierarchical classification;hierarchical classification;stochastic model;real-world;evaluation scheme;general case;instance;synthetic datasets;svm classifier;classifier;support vectors
optimization technique;unlabeled data;classification;optimization problem;objective function;decision boundaries;semi-supervised;support vector machines;local minima;semi-supervised
convergence properties;classification;instance;multiple-instance learning;optimization problem;multi-instance;instances;data sets;regularization framework;multi-instance learning;kernel methods;loss functions
theoretical analysis;convex programming;learning algorithms;support vector machines;support vectors
utility function;learning algorithms;bayesian approach
reinforcement learning algorithms;specifically designed to;reinforcement learning problems;making predictions
classification;mixture model;decision boundary;nearest neighbor classifier;real-world data sets;local classifiers;data distribution;classifier;training sample
precision-recall;highly skewed;machine learning;algorithm's performance;decision problems;operator
clustering;clustering;computational complexity;data analysis;cluster analysis;high dimensional data sets;linear discriminant analysis;principal components;clustering techniques;low dimensional;cluster analysis;dimensional data;clustering algorithm called;local minima;spectral graph;dimensionality reduction
gradient-based;matrix factorization;maximum margin;ensemble approach;great promise
planning algorithms;reinforcement learning algorithms;supervised learning techniques;induction algorithms;decision tree;reinforcement learning problems;incremental version;markov decision processes;tree induction;learning problem
classification;database;conditional distributions;learning algorithm;artificial datasets;naive bayes classifiers;efficient learning
evaluation methodology;user preferences;preference learning;data collections;real-world;blocks-world;data collection;learning problem;kernel density estimation;learning method;positive examples
clustering;global solution;real-life datasets;covariance matrix;principal component analysis;principal component analysis
dcm;text documents;exponential-family;maximum-likelihood;clustering documents;exponential family;takes into account;clustering algorithm;expectation-maximization;times faster than
prediction accuracy;protein function;posterior probabilities;prediction methods;statistical model;tree based;protein sequences;graphical model
optimal policies;transition probabilities;reinforcement learning;markov decision process
learning process
computational complexity;approximation method
training data;classification;probability models;input space;bayesian inference;logistic regression;uniformly distributed;kullback-leibler divergence;bayesian framework
object recognition;graphical models;classification;text data;information retrieval;poisson model;graphical model
regression trees;inference problem;classification;ensemble methods;tree-based;biological network;interpretability
game theoretic;regularization;spam filtering;quadratic programming;labeled data;specifically tailored;recognition tasks;noise model;classification task;classifier
bayesian model;monte carlo;latent features;instance;mobile phone;markov chain;binary features
speech recognition;training data;recurrent neural networks;classification;sequence data;real-world;post-processing;learning tasks;input data;recurrent neural networks
kernel functions;text corpora;document clustering;dimensional data;svm classifier;kernel methods
kernel learning;sparse data;kernel matrix;kernel-based;training examples;learning algorithms;feature representations;text classification;support vector machines;feature vectors;natural language
transductive learning;graph cut
domain knowledge;small sample;training data;unlabeled data;classification;kernel function;labeled samples;image retrieval;boosting algorithm;training samples;image similarity;semi-supervised;learning method
suffix tree;markov models;partially observable;tree-based;hidden state;arbitrarily complex
greedy algorithm;classification model;classification;active learning;real-world;batch mode active learning;batch mode active learning;unlabeled examples;medical image;efficiently identify;manual labeling;uci datasets
real-world;multi-class classification
high-dimensional;linear model;fmri) data;t,t;probabilistic models;time series;fmri data;process models;process models;magnetic resonance imaging;prior knowledge;cognitive processes
multitask learning;learning algorithms;average error;data compression
high-dimensional;low-dimensional space;dimensionality reduction technique;dynamic programming;basis functions;component analysis;reinforcement learning;temporal difference;state space;automatically constructing;lower-dimensional space;supervised learning;basis function;control problem;markov decision process
regularization term;regularization;recognition rate;real-world;prior knowledge;data set
selection problem;kernel selection;linear discriminant analysis;selection method;interior-point methods;optimal kernel;fisher discriminant analysis;machine learning;high dimensional feature space;convex optimization problem
linear classifier;kernel-based;classification;conditional distributions;labeled data;computationally tractable;linear classifiers;linear classification
smoothing methods;state-spaces
reinforcement learning;learning tasks;knowledge transfer
data point;order statistics;classification;data association;data association;multiple topics;hidden markov models
kernel learning;running times;data points;kernel matrices;kernel matrix;low-rank;learning tasks;scale poorly;learning problems
latent variable model;gaussian process;latent space;dimensionality reduction techniques;data sets;low dimensional;probabilistic pca
training data;predictive accuracy;hypothesis space;learning algorithms;knowledge-based;optimization problem;prior knowledge;support vector machines
base classifiers;query specific;classification;bayesian classifiers;obtained by combining;classification results;query-specific
reuters corpus;database;probabilistic model;tf-idf;gaussian kernel;weighting scheme;topic classification;text classification
graphical models;sufficient condition for;energy functions
special case;synthetic data;parameter estimation;kernel function;heterogeneous data sources;vector machine;optimization procedure;multiple kernels;protein function;generative model;maximum entropy;kernel methods;large-margin;classifier;optimization algorithm
point-based;region-based;partially observable markov decision process;optimal value function;expectation maximization;partially observable markov decision processes;variational bayesian;region-based;maximum likelihood
base classifiers;classification problem;margin;boosting algorithm
latent dirichlet allocation;document classification;topic models;text data;finer-grained;discrete data;mixture models;increasingly popular
clustering;spectral clustering;algorithm iteratively;multi-type;spectral clustering algorithms;relational clustering;hidden structures;web mining;low dimensional;data objects;relational data;clustering
clustering;vision applications;data points;vision problems;subspace clustering;clustering methods;cost function;dynamic scenes;synthetic data
state space;sample size;random walk;policy evaluation;highly optimized;policy evaluation;markov decision processes;efficiently computing;operator;long term;direct computation
ensemble pruning;generalization performance;pruning method;classification problems
high-order;potential functions;image denoising;prior models;markov random fields;color images
clustering;data set
supervision;mutual information;objective function;dimensionality reduction
relevance ranking;classification;svm-based;generalization performance;real-world;learning tasks;selection algorithm;feature selection;combinatorial optimization problem;sparse linear;np-hard
learning tasks
window size;potentially infinite;markov models;markov models;linear-chain;input sequence;higher accuracy;fixed-size;structured information
explanation-based learning;planning domains;hierarchical structures;learning algorithm;knowledge-based;complex tasks;learned knowledge;hierarchical task
state space;learning algorithm;reinforcement learning;reinforcement learning;large-scale
training set;training data;svm training;nearest-neighbor;instances;training instances;support vector machines;boundary detection;support vectors
singular value decomposition;lower-dimensional;machine learning problems;classification;training examples;naïve bayes;data set;learning problem;support vector;support vector machines;learning performance;classifier;dimensionality reduction;classification algorithm
bayesian model;reinforcement learning;partially observable markov decision process;optimal value function;reinforcement learning;online learning;point-based
learning process;unlabeled data;test data;semi-supervised learning;semi-supervised learning;learning algorithms;graph-based;data set;instance;multiple-instance;multiple-instance;semi-supervised;learning method;content-based image retrieval
domain knowledge;logistic regression;multivariate gaussian;learning task;learning problems;labeled data;automatically constructing;covariance matrix;text classification;error reduction;supervised learning;transfer learning
classification;classification;concept classes
prediction problem;cost functions;dynamic programming;goal-directed;imitation learning;maximum margin;fast algorithms;maximum margin
linear program;markov random field;synthetic data;estimation problem;convex optimization;quadratic programming;belief propagation;approximation guarantees;map estimation;quadratic program;linear programming
training set;training sets;real data;multiple-view
base-classifier;base classifiers;training data;training examples;boosting methods;boosting algorithm;margins;margin;classifier
individual features;discriminative features;image sequence;state variables
database;main memory;mining process;databases;sufficient statistics;data-mining algorithms
algorithm performs;minimization problem;dynamical systems;linear dynamical systems;vector-valued;real-valued;expectation maximization;kalman filter
predictive accuracy;low variance;rule learning;training error;learning algorithms;large margin;rule sets;optimization procedure;model selection;efficient computation;error rate;capacity control;convex optimization problem
sequence labeling;efficient inference;highly accurate;dynamic programming;worst case;inference algorithms
cost functions;structured domains;classification;cost-sensitive learning;expected cost;cost-sensitive learning;markov networks;cost function;cost-sensitive;maximum entropy;link structure;link prediction;real-world;classifier;cost-sensitive classification
attribute values;cost-sensitive learning;medical diagnosis;missing values;case study;actual values;real-world;test examples;total cost
object recognition;classification;data points;learning procedure;uci datasets;vector machine;character recognition;theoretical justification;maximum weight;maximum margin;support vector machines;margin;classifier
factor analysis;search procedure;search algorithm for;continuous variables;low rank;hill-climbing;covariance matrix;operator;latent variable models;bayesian learning
internal state;reward function;explicitly modeling;markov decision process;learning agent
kernel machines;kernel machines;svm algorithm;unlabeled data;kernel-based;real world datasets;loss functions;convex optimization problems;optimization problem;optimization framework;local minima;semi-supervised;margin-based;classification algorithms
classification learning;statistical properties;training data;feature subset;classification;classifier learning;training dataset;classification performance;feature selection;feature subset;dimensional data;selection bias;feature selection
competitive performance;classification;classification
clustering;principal component;computationally tractable
prediction performance;major components;game tree search;learning algorithm;probability distribution over;numerous applications in;pattern extraction
computation cost;model-free reinforcement learning;reinforcement learning;markov decision process;takes place;optimal performance;finite state
classification;learning-theory;concept class;hypothesis class;efficient learning;learning environment;classification algorithms
bayesian network;high computational complexity;learning algorithms;classification;decision tree;learning algorithm;bayesian network classifiers;structure learning;decision tree learning;decision trees;context-specific;conditional probability
high-dimensional;data analysis;dimensionality reduction method;linear dimensionality reduction;multimodal data;supervised dimensionality reduction;local structure;fisher discriminant analysis;class labels;dimensionality reduction
margin-based;learning algorithm;computational costs;multiclass problems;microarray datasets;feature weighting;margin;objective function;convex optimization problem
differential evolution;vector machine;gradient descent;kernel space;support vector machines;objective function;support vectors
linear space;tree based;algorithm requires;memory requirement;suffix tree
algorithm performs;factor analysis;relevant features;synthetic data;high dimensional;learning approaches;regression methods;linear regression;variational bayesian;learning problems;dimensional data;automatically detects;query points;locally weighted
expectation maximization algorithm;exact inference;markov decision processes;optimal control;probabilistic inference;belief state;reward functions;ad hoc;optimal policies;continuous state
clustering;statistical methods;graph data;mixture model;feature vector;feature space;spectral graph;regularizer;graph kernels;data processing;data structure called;em algorithm;high dimensionality;clustering;text processing;increasingly popular
real world datasets;data collection;sampling algorithm;active learning;irrelevant features;relevance assessment;data acquisition;unlabeled examples;labeled examples;class labels;active sampling;random sampling;sampling algorithms;classifier;information theoretic;feature relevance
optimizer;optimization method;conditional random fields;large data sets;limited-memory;conditional random fields;inference techniques
predictive accuracy;language model;probabilistic model;topic model;data sets;topic modeling;em algorithm;latent topic
data point;synthetic data;text classification tasks;sample data;semi-supervised learning;label propagation
regularization;support vector regression;regression function;classification;solution path;solution space;support vectors;computational cost;support vector;desired accuracy;piecewise linear;model called
regularization;linear constraints;relative entropy;boosting algorithm;boosting algorithms;margin
binary classification;labeled examples;labeled data;large margin;prior knowledge
state representation;parameter estimation;dynamical systems;closed form solutions;gaussian kernel;model parameters;kernel methods
closed-loop;state representation;action sequences;answer questions
nearest-neighbor;accurate classifiers;time series;classification
spectral methods;locally linear embedding;sparse matrix;dimensionality reduction;maximum variance
medical applications;sampling algorithm;hierarchical dirichlet process;dirichlet process mixture;nonparametric bayesian;biological data
discriminative training;baum-welch;semidefinite programming;unsupervised learning;markov networks;hidden markov models;unsupervised algorithm;maximum margin;local minima;structured learning
taking into account;data points;sensitivity analysis;feature mapping;locally linear embedding;real life;prior information;semi-supervised;semi-supervised;dimensionality reduction
null space;data dimension;null space;sample size;linear discriminant analysis;linear discriminant analysis;supervised dimensionality reduction;theoretical analysis;dimensional data;pre-processing;applications involving;dimensionality reduction
greedy search;unlabeled data;active learning;real-world;active learning;text categorization;alternating optimization;regression model;preference learning;efficient optimization
user preferences;preference learning;regression tasks;gaussian process;regression problems;counterpart;hierarchical bayesian model;ranking functions
spectral clustering;image segmentation;approximation quality;low rank;kernel matrix;sampling based;kernel principal component analysis;spectral embedding
clustering algorithms;real world;voting scheme
computational efficiency;classification;naive bayes;bayesian classifiers;attribute-values;naive bayesian
java-based;query optimizer;data types;query language;buffer management;storage manager
query execution;query execution plans;black-box;query language;data stream applications;data stream processing;data stream management system;linear road;continuous queries
indexing schemes;spatiotemporal queries;database;storage space;index structure;indexing techniques;query efficiency;temporal dimension;indexing scheme;query performance;historical queries;index nodes;index structures
data types;query language;network model;moving objects;query processing;objects moving;moving objects databases
video surveillance;distance function;similarity measure;index structure;time series;similarity measures;data mining research;distance measures;noisy data;efficient retrieval;time-series;precision/recall;real-world applications;longest common subsequence;increasingly popular
data objects;nearest;existing indexes;mobile computing;real data;location-based services;search space;nn search;construction algorithm;nearest-neighbor search;hybrid method;nn) search;computing environments;nearest-neighbor;cost model;solution space;search cost;object-based;compact representation;nearest-neighbor queries;nn queries;query point
classification performance;database;large databases;interestingness measure;item sets;low-complexity;association rules;association rule discovery;confidence measure;query expansion;scale “free
data sets;database;real-life;large-scale;pattern mining;time series;summarization techniques;similarity search;error bounds for;nn queries;databases;data streams;dimensionality reduction
tree structures;document retrieval;index structures;language called;information retrieval;xml-based;query processing;semistructured data
information-seeking;information processing
cross language;image collection
web applications;international conference on;end users;database;acm sigmod;web information and data management;acm international workshop on;knowledge management
information resources;relevance ranking;relevant documents;exact matching;irrelevant documents;information retrieval;user queries;search engines;search engine;web search engines
resource sharing;search methods;keyword queries;information systems;large scale;information retrieval;retrieval models;information retrieval systems
evaluation methodology;document collection;relevance information;document sets;retrieval task;ad hoc
content-oriented xml retrieval;ad-hoc retrieval;ad-hoc;relevance feedback;information access;natural language processing;xml retrieval;scoring methods;test collection
1;relevance feedback;3;2;4;relevance feedback
end-users;natural language queries;database;ranked list;ir systems;xml query;xml elements;ad-hoc;formal language;natural language processing;xml information retrieval;query languages;natural language
markup language
ad hoc information retrieval;semantic associations;retrieval performance;language modeling approach;ad hoc information retrieval;language modeling
document collections
transaction data;data mining application;customer relationship management;workshop on data mining;data mining and knowledge discovery;real-world data mining applications;data mining applications;real-world;data mining;large databases;3;1;lessons learned;2;case studies;4;databases;dimensional data;software development
medical records;domain knowledge;probabilistic framework;data repository;high-quality;databases;data extraction;patient data;probabilistic reasoning;data mining;clinical data;multiple sources;poor quality
1;density estimation;kernel-based;classification;machine learning;united states;case study;analysis task;statistical analysis
world wide web;classification model;information sources;textual content;image search;web content;search engines;multiple information sources
data mining system
clustering;source code;hidden patterns;open source;large volumes of data;application server;object-oriented;knowledge acquisition;data mining;semi-automated
text mining;large amounts of;unlabeled data;multi-view;semi-supervised learning algorithms;labeled data;product descriptions;databases;single-view;classification problems
service quality;data source;business process management;web-based;data mining techniques;decision makers;business processes;customer support;data mining;business process management
market basket analysis;market basket;data driven;decision-making
customer base;classification model;neural networks;classification;data mining;data mining;operator;cluster analysis;information technology
data-driven;high-dimensional;decision support;knowledge discovery;chemical compounds;noisy data;general problem
data mining;spatial data mining;spatial data mining
data values;data analysis;real-world;missing data;large datasets;missing data;disguised missing data
valuable knowledge;classification;mining association rules;data mining;hidden information;sparse datasets;worst case;frequent closed;frequent closed;memory consumption;optimization strategies
implementation issues;fuzzy logic;database
database systems;data structures;object-relational;database;large databases;scientific computing;object-relational database;management systems;commercial database;object-relational database systems;database vendors
temporal aggregates;database;temporal aggregates;standard sql
software development;service-oriented
rates
management systems;data base;data processing;data base
acm sigmod;information systems;information quality;international workshop on
united states;wdas;distributed data;distributed data
life sciences;medical research;molecular biology;san diego;international workshop on;1;2;data management;life science;program committee;international workshop on;working group;data integration
web information and data management;acm international workshop on;knowledge management
information systems;database systems;data management;databases
query answering;databases
acm sigmod;database;database management systems;customer support;databases
problem instances;search space;database instances;database;distributed databases;view definitions;materialized views;answering queries;data warehousing;conjunctive queries;data integration
distance functions;distance function;database;temporal information;spatio-temporal;query types;mobile devices;data reduction;distance-function;approximation technique;location-aware;spatio-temporal;error bounds;spatio-temporal queries
nearest;nearest;answering queries;nearest neighbor queries;moving objects;query returns
bayesian network;dependency trees;partial knowledge;bounded memory;special case;dependency trees;efficient learning
query optimization;xpath queries;query optimization;xml queries;query processing and optimization;storage model;xml documents;transformation rules;xml data;logical-level;document databases
ir research;ir researchers;query formulation;text categorization;information retrieval;incomplete information;annual international acm sigir conference on;technical program;semi-automatically;topic areas;user studies;information retrieval;question answering;reviewing process;wide range
clustering;quantum theory;vector space;linear algebra;von neumann;linear subspaces;hidden variable;transition probability;probabilistic models;operator;simple algorithm;conditional probability;metric spaces
organizational structure;game-theoretic;large-scale;search techniques;information retrieval applications;social network;small-world;3, 5, 7, 8, 13, 18, 21;social networks;graph models;local information;link analysis;information exchange;social media;information content;focused primarily on;depends crucially on;social-networking;large social networks;2, 6, 16, 17;1;social networks;9;4;27, 28;10, 11;20, 23;social network analysis;19, 24, 25;26;long-term;information retrieval;social networking;network topology;10;12;15;structural features;federated search;mathematical models;long-range;game theory;network structure;distributed ir;large-scale social networks
spam detection;prediction accuracy;user preferences;web search;large-scale;real-world;user behavior;implicit feedback;search result;web search results;search engine;query-dependent;user interaction
query evaluation;total number of;relevant documents;average precision;information retrieval tasks;instance;search engines;trec web-track;information retrieval;question answering;search tasks
web search;user interactions;ranking process;large scale;improving web search;user behavior;user feedback;ranking algorithms;implicit feedback;web search engine
information retrieval;similarity metrics;instance;baseline methods;social networks;similarity measures;similarity measures
temporal information;information retrieval;real dataset;structure information;web data;clustering algorithm;detection task;linguistic features
document collection;cost effective;formal models;probabilistic models;expert finding;knowledge based;trec enterprise;excellent performance;unsupervised techniques
retrieving information from;spoken document retrieval;compact representation;retrieval performance;automatic speech recognition;search effectiveness;recognition systems;error rate;average precision;posterior probability
features extracted from;database;mixture models;low-level;databases
vector space;hierarchical approach;retrieval accuracy;indexing scheme;information extraction;retrieval models;structure information;modeling approach;music retrieval
computational complexity;web sites;web search;browsing behavior;random walk;random walk model;web pages;web applications;theoretical analysis
document ranking;language models;cluster-based;relevant documents;retrieval performance;main idea;graph-based
ranking approaches;topic distribution;web search;link structure;random walk model;ranking schemes;link-based;multiple datasets;link analysis
information seeking;negative results;knowledge-based;term-based;performance gains
closely related;relevant documents;probabilistic models;probabilistic information;language modelling;tf-idf;retrieval models;poisson model
weighting function;mutual information;related terms;retrieval accuracy;retrieval scores;information retrieval;similarity function;retrieval models;data sets;retrieval model;language modeling;query expansion;term matching;document set
large corpora;logistic-regression;margin;logistic regression
web search;classification;applications including;users' queries;user queries;classifier
document collection;retrieval effectiveness;data fusion;data fusion;training process;probabilistic approach;trec ad hoc;result set
search results;web search;user supplied;relevance feedback;rates;web search results;web-graph;similarity measures;query refinement
high quality;relevance models;relevance feedback;pseudo-relevance feedback;retrieval performance;retrieval algorithms;collection statistics;average precision;language modeling
model parameters;mixture model;retrieval accuracy;data sets;statistical language models;pseudo-relevance feedback;retrieval models;feedback documents;language modeling approach;pseudo-relevance feedback;mixture models;query terms;main idea
semantic smoothing;semantic smoothing;translation model;language models;smoothing method;context-sensitive;ontology-based;retrieval performance;em algorithm;language modeling approach;contextual information;context-sensitive;average precision;language model;query terms
computational complexity;search algorithms;latent dirichlet allocation;ad-hoc retrieval;language model;cluster-based;topic model;topic models;information retrieval;gibbs sampling;generative model;cluster-based retrieval;language modeling framework;approximate inference;machine learning;trec collections
quadratic programming;loss function;relevant documents;document retrieval;ranking svm;large number of;ranking svm;information retrieval;gradient descent;loss" function;ranking documents
modeling assumptions;selection problem;trec collections;translation model;retrieval results;training methods;statistical models;query translation;cross-language
search results;cross-language;information retrieval;special case;cross-language information retrieval;translation probabilities
probabilistic framework;language models;probabilistic model;language model;probabilistic model;question answering
random walk;random walk on;multi-document summarization;bipartite graph;markov chain;question answering
community based;textual features;question answering;service providers;document collections;web services;retrieval model;quality measure;answer questions;language modeling
data objects;mutual reinforcement;latent semantic analysis;collaborative filtering;semantic relations;text categorization;text clustering;latent semantic analysis;multiple types of;semantic space;latent semantic analysis;data sparseness
application area;classification;language constructs;text documents;business intelligence;sentiment classification;pattern discovery;supervised learning;identification problem
negative transfer;learning tasks;real-world applications;text classification;machine learning;classification task;classifier
xml) information retrieval;precision-recall;web ir;xml retrieval;precision-recall;information retrieval;related documents;ir) metrics
real-world;relevance judgments;large sets of;information retrieval;high confidence;test collections;retrieval evaluation;information retrieval evaluation;average precision;test collection
correct answers;document collection;search algorithms;web search;web crawl;relevance information;relevant documents;trec collection;search engines;test collections;search effectiveness;search performance;document set
high precision;algorithm finds;web pages;large-scale;large scale;3;4;web search engines;projection based
navigation patterns;web ir;web sites;web crawlers;web site;digital library;page content
ranking algorithm;large-scale;web forums;topic hierarchy;fine-grained;content information;retrieval performance;link-based;automatically generating;link graph;automatically generated;specific topics;automatically created
ranking techniques;ranking function;ranking algorithms;convergence speed;link-based;page importance;graph data
collection size;collection statistics;prediction accuracy;information retrieval techniques
query classes;document collection;training data;human knowledge;combination weights;performance gains;retrieval tasks;query independent;combination methods;multimedia retrieval;meta-search;combining multiple;model selection;web retrieval
past queries;parameter values;user modeling;federated search;information retrieval;ad-hoc;training data;information access;future queries;long-term;user modeling;search efficiency
quality-aware;high-quality;database size;database;sampling process;text database;sampling framework;higher-quality;text databases
query throughput;text retrieval systems;large-scale;rates;data volumes;load balancing;computing systems;load balancing
text retrieval systems;index maintenance;main memory;disk accesses;index construction;text collections;text collection
data structure;response times;text search;semi-structured;semantic tags;million documents;data structures;query processing;search engine
query evaluation;pruning methods;performance guarantees;pre-computed;ranked queries;average precision
natural language queries;local context;vice versa;density based;dependency relations;passage retrieval;short queries;occur frequently;query expansion;query terms
search results;main components;query difficulty
clustering;document sets;intrinsic dimensionality;ranking 200 queries;search queries;search effectiveness;average precision
weighted graph;text analysis;semi-supervised classification;cluster membership;prior knowledge;clustering task;training samples;document clustering;semi-supervised;cluster analysis;clustering model
clustering;generative model for;text clustering;user feedback;clustering tasks;text clustering;clustering methods;clustering algorithm;clustering model;user input
search task;detection algorithms;duplicate detection;database;data cleaning;constrained clustering;document detection;instance-level;information retrieval;relevant" documents;clustering process;clustering approach
computationally intractable;relevant documents;probabilistic models;information retrieval;probability ranking principle;information retrieval systems
relevance score;high accuracy;ranked list;learning algorithm;information retrieval;data set;discounted cumulative gain;high precision
search results;search term;xml fragments;high-precision;search strategy;semantic search;text corpora;semantic information;semantic search;query language;high precision;xml fragments
user-generated;information retrieval;relevance feedback;retrieval performance;recall-oriented;pseudo-relevance feedback;expansion terms;query length
search systems;search results;document similarity;relevance feedback;search tool;retrieval performance;average precision;query-biased
human behavior;large-scale;upper bound;regression model;performance gains;ad hoc retrieval
classification problems;unlabeled data;semi-supervised setting;local minimum;large scale;data mining applications;semi-supervised learning;large number of;linear svms;information retrieval;large scale;optimization procedure;classification tasks;sparse datasets;partially-labeled;linear classifiers;support vector;semi-supervised;interpretability;labeled examples
real-world datasets;graph structure;classification;graph-based;data items;distance metric;training samples;graph based;graph-based;text classification;svm classifiers;feature spaces;text documents;similarity measures
domain knowledge;domain knowledge;training examples;classification;learning approaches;training data is;text categorization;logistic regression model;prior distribution;data sets;training sets;text classification;posterior distribution;building classifiers;supervised learning;bayesian framework;classifier
complete model;probabilistic framework;background model;collaborative filtering;rating data;data sparsity;large number of;user-item;prediction quality;similar items;similar users
category specific;personalized recommendations;collaborative filtering;traditional collaborative filtering;personalized recommendation;information flow;social network;instance;information flow;information access;access patterns
singular value decomposition;linear model;recommendation systems;false alarm;detection method;collaborative filtering algorithms;svd-based;filtering techniques;rates;information overload;detection rates;low-dimensional
ir effectiveness;test collections;statistical significance;test results;ir metrics;rank correlation
trec-6 ad hoc;confidence intervals;ir evaluation;similar characteristics;information retrieval evaluation;effectiveness measures;average precision;test-collection;test collection
trec data;large-scale;low variance;relevance judgments;retrieval evaluation;retrieval systems;prohibitively expensive;statistical method;highly accurate;statistical technique;random sampling
genetic programming;web pages;instance;ranking functions;web page;average precision
classification performance;search procedure
data-rich;web site;user privacy;general problem;privacy risks;popular items
context sensitive;document understanding;word frequency;multi-document;sentence extraction
digital libraries;information graphics;valuable knowledge;digital library;information graphics
web documents;text summarization;summarization method;user request;information-seeking;mobile devices;hierarchical structure;mobile phone;summarization methods
clustering;search results;information retrieval applications;textual content;search engines;web search engines;clustering
document collections;information contained in;image processing;information processing;wide range
document relevance;large scale;relevance judgments;retrieval evaluation;retrieval systems;random sample;limited number of;average precision
document retrieval;xml element retrieval;xml elements;xml retrieval
formal representation;ir models;xml retrieval;retrieval systems
semantic features;diverse set of;hierarchical structure;question answering systems;log-linear models;question classification;classifier;machine learning approaches
web search;community-based;community-based;post-processing;community members;search engine;web search
evaluation methodology;document sets;relevant documents;test collections;document set;test collection
term proximity;retrieval effectiveness;ad-hoc retrieval;large text collections;text collection;retrieval method;term proximity
web search;web log
vector space;classification;tensor space;large scale;tensor space;naive bayes;data sets;information retrieval;data set;tensor space;algorithm called;document space;theoretical analysis;latent semantic indexing;support vector machines;vector space model;high dimensional space
test collection;ad hoc queries;information retrieval;large-scale
relevance scores;logistic regression model;training data;retrieval systems
provide answers;qa systems;provide feedback;addressing this problem;information seeking;correct answer;question answering
text documents;text segmentation;linguistic features
information extracted from;auxiliary;content management
search engine results;relevance judgments;evaluation metric
ranking algorithm;web page;summarization method;graph based;web pages
hybrid method;objective function
historical data;trec collections;score distributions;score-based;rank aggregation;rank aggregation
search systems;search tools;identify patterns;search behaviour
human users;expansion terms;query expansion;user generated;retrieval systems
test data;spatial relationships;text descriptions;information retrieval;information retrieval;retrieve images
relevance feedback;user interfaces
joint probability;rule based;instances;limited number of;rule based
learning scheme;information retrieval methods;text categorization
pseudo-relevance feedback;average precision;difficult queries
retrieved documents;information-seeking;element retrieval;xml retrieval;text retrieval
optical character recognition;data mining;document images;information processing;document structure;test collections;information access;text retrieval;test collection
temporal information;topic tracking
user experience;digital libraries;queries issued;search experience
search systems;speech retrieval;automatic speech recognition;retrieval effectiveness
retrieval effectiveness;structured documents;weighting model;information retrieval;retrieval performance;trec2005 enterprise
document retrieval;question answering systems
feature vector;content-based retrieval;content-based video;visual feature;video retrieval;user study;visual features
web search;language models;sequence data;user behavior;prediction performance;search sessions;query reformulation;user models;language modeling approach;user actions;prediction task;query terms
numerical values;news sources
training set;training set size;classifier;nearest neighbors
relevant information;document set;novelty detection
relevance feedback;ambiguous queries;web search engines;user intent;query logs
result diversification;search results;personalized web search
generic model;xml elements;retrieval model
relevant document;evaluation metric
model order selection;clustering problem;cluster ensembles;document clustering;document representations
maximum likelihood;document frequency;average precision;language modeling;language modeling
high level;sentiment analysis;text classification tasks;genre classification;classification accuracy;text classification
queries issued;search engines;search engine
ad hoc;information access
search result;search engines;query language;xml-retrieval;search engine
xml-retrieval;element retrieval
interactive query expansion;query expansion;query terms;query formulation
label information;multi-label learning;graph-based;multiple types of;text categorization;semi-supervised learning algorithms;unlabeled documents;training documents;label propagation;label propagation;multi-label
xml documents;semi-structured documents;document content;vector space model;similarity measures
multi-task;optimization method;information retrieval methods;search engine;retrieval functions;information retrieval;user queries;risk minimization;retrieval function;learning problem;statistical models
retrieval performance;concept-based;text retrieval;data sets;concept-based
sentiment analysis;natural language text;sentiment classification
java-based;probabilistic information;information retrieval;relevance feedback;retrieval model;contextual information;query expansion;machine learning approaches
web pages;plans;related information;electronic documents;context-free;personal information
visual feature;video retrieval;content-based retrieval;visual features;feature vector
kdd conference;data mining and knowledge discovery;knowledge discovery;acm sigkdd international conference on;data mining;knowledge discovery;data mining;program committee;acm sigkdd international conference on
large numbers of;classification;wireless communication;fine-grained;fine grained;sensor networks;data mining
statistical queries;spatial regions;data structures;data tables;large datasets;sufficient statistics
knowledge discovery;data mining
correlation clustering;clustering;data analysis;probability distribution;correlation clustering;post-processing;data set;correlation clusters;clustering algorithm;clustering model
algorithm learns;optimizer;network flow;relevance feedback;stationary distribution;maximum entropy;learning problems;feature vectors
approximately optimal;approximation algorithms;streaming algorithms;spatial data;data points;massive data;scan statistics;data sets;grid-based;approximation algorithm;negative results
clustering;directly optimize the;classification;database;segmentation techniques;large databases;segmentation problem;distance-based;computational cost;optimization techniques;separability;locally optimal
structural features;large-scale;large social networks;user-defined;decision-tree;social networks;network structure;social networking;social sciences
clustering;intrusion detection;ground truth;density-based;classification decisions;outlier detection;data set;outlier detection;detect outliers;detecting outliers;ad-hoc;hypothesis testing;clustering model;clustering structure
clustering;clustering algorithms;clustering;spectral clustering;clustering technique;information-theoretic;data set;minimum description length;point set;input parameters;grid-based;data set size;information-theoretic;clustering algorithm;real world;space partitioning;real world data sets
anonymity-preserving;data collection;large number of;cryptographic techniques;data mining;mining algorithm
frequent itemset;mining algorithms;hash-based;sequence mining;pattern mining;frequent itemset mining;large data sets;data sets;scalable solution;graph mining;frequent pattern mining
frequent patterns;real-life datasets;categorical attributes;efficient algorithms to;interesting patterns;discretization methods;rank based;numerical data;numerical attributes;support measures
network analysis;mining algorithms;discovery algorithm;biological networks;high-throughput;network motifs
large-scale;web communities;specific domains;complete search;search engines;highly scalable;small-scale
clustering;nonnegative matrix factorization;real world
parameter values;decision trees;kernel-based;data summarization;independent variables;kernel functions;data mining;large number of;regression problem;training data;wide range;regression methods;benchmark datasets
training set;test set;fold cross-validation;generalization error;computational cost;data sets;real-world applications;cross-validation;sample selection bias;stationary distribution;risk minimization;data mining;classification algorithm;test sets;classification algorithms
test set;supervised machine learning;class distribution;classifier;classification
clustering;data mining results;theoretical results;frequent sets;instances;chi-square;data mining;margins;markov chain;data mining algorithms;meaningful information;high-dimensional 0-1 data sets
semi-structured;text mining;mining frequent patterns;semi-structured data;probabilistic model;data mining approaches;predictive power;real data;probabilistic modeling;learning scheme;training data;web mining;data mining;data mining applications;ordered trees;mining patterns;real datasets;space complexity
kernel machines;kernel machines;logistic regression;classification;active learning;classification approaches;classification scheme;quadratic programming problem;kernel learning;kernel learning;semi-supervised;kernel machines;supervised learning;labeled and unlabeled data;learning method
search space;frequent subgraph mining;frequent subgraph;large databases;subgraph mining;structured objects;pattern discovery;mining algorithm;graph databases
high-dimensional;cutting plane algorithm;sparse data;linear support vector machines;regression problems;large number of;linear svms;word-sense disambiguation;drug design;decomposition methods;optimization problem;applications involve;machine learning techniques;text classification;classification problems;large datasets;cutting-plane algorithm
mutual information;information-theoretic;correlated patterns;quantitative attributes;databases;mining associations;quality measure;correlated patterns
quality measure;feature set;binary data;database
data mining tool;data sets
labeled data;classifier;topic segmentation
density estimation;data points;model parameters;range data;kullback-leibler divergence;em algorithm
selection predicates;view based;target class;data mining tasks;workload-aware;anonymization techniques;individual privacy;data privacy;algorithms typically
high-dimensional;random projections;vector space;approximate algorithm
context information;data mining application;minimum support;real-life applications;olap operations;rule mining;minimum confidence;data sets;class association rules;log data;data mining algorithms;actionable knowledge;discovered rules
data structure;multi-dimensional;sparse data;contrast patterns;high dimensional;binary decision diagrams;human experts;emerging patterns;binary decision diagrams;highly scalable
data objects;community structures;unsupervised learning;unsupervised learning;real datasets;clustering approaches;hidden structures;applications involve;data mining;cluster structures
data analysis;classification quality;data compression;high quality;image analysis;data elements;tensor-based;original data;tensor decomposition;diverse domains;analysis tool;numerous applications in
semantic classes;frequent patterns;efficient algorithms to;selecting informative;data mining task;structured information;similar patterns;semantic relations;frequent pattern;semantic annotations;frequent pattern mining
clustering;greedy heuristics;dynamic programming;data points;mobile-phone;algorithms produce;sequential data;segmentation algorithms
network structure;real data sets;efficient approximation;space complexity;relational data
distance computation;computational efficiency;distance functions;distance function;distance preserving;linear programming;large number of;optimization problem;local minima;computationally expensive;data mining;explicitly models;compares favorably with;machine learning algorithms;low-dimensional
user defined;network flow;anomaly detection;real datasets;matrix decompositions;high-order;streaming data;large datasets;tensor analysis;data cubes;feature selection;fully automatic;principal component analysis;dimensional data;latent semantic indexing;interesting patterns;higher order;dimensionality reduction;social networks
real-world data sets;hierarchical model;hierarchical classification;classification;classification
memory capacity;large databases;distance-based;upper bound;distance metric;memory consumption;metric space
query nodes;algorithm finds;viral marketing;social network;multiple dimensions;multi-source;gene regulatory networks
database design;sensitive information
topic model;low-dimensional
significant rules;association rules
disk block;frequent patterns;evaluation functions;significant patterns;traditional database;greedy algorithm;pattern set;np-hard
cross-validation;discriminant analysis;classification;sample size;high dimensional;class distributions;candidate pairs;classification methods;discriminant analysis;machine learning;classification performance;candidate set;data mining;dimensional data;regularization
multi-task;learning algorithm;pattern recognition;input data;classification;label information;learning problems;principal component analysis;principal component analysis;learning tasks;information retrieval;model called;data mining;probabilistic pca;semi-supervised;dimensionality reduction;semi-supervised setting
tree based;machine learning methods;text classification tasks;vector machine;total number of;text classification;markov chain
clustering;pattern-based;cut algorithm;focused primarily on;semantically similar;event-driven;high quality;web log data;event detection;clustering process;data collected from;web content;vector-based;event detection;log data;real world;web search engines;web search engine
data record;web data extraction;probabilistic model;information extraction;data records;web data extraction;conditional random fields
computational complexity;classification problem;classification;active learning;outlier detection;classification methods;data set;outlier detection;data sets;density estimation;detection methods;estimation methods
real data sets;hardware technology;sensitive data;data records;privacy preserving;heuristic algorithm;data processing;data mining;sufficiently high
association rule mining;associative classifier;class distribution;support measure;classification
applications including;social interactions;computational framework;social interactions;finding patterns;social networks;social interaction
database;real-world;adaptive approach;real-world entities;query processing;databases;processing queries
storage space;computational power;test sets;learning models
detection algorithms;attack detection;recommender systems;large number of;machine learning;user profiles;classification;classification approach;user input
massive data streams;training data;limited memory;online learning;linear svm;stream processing;online learning;chi-square;feature selection;feature selection methods;select features;margin;information gain
clustering;clustering algorithms;real data sets;agglomerative hierarchical clustering;generic framework;high accuracy;evolutionary clustering;evolutionary clustering
partial orders;data mining;real data;query processing;large datasets;concept class;total order
correlation-based;interrelated data;data mining methods;multiple views;real-world;multiple view;data mining;mining process;knowledge embedded in;learning mechanism;wide range;multiple relations;real-world data sets;relational databases;data mining algorithms;data mining approaches;relational data;vast amounts of
survival analysis;online stores;user's interests;online stores;extract information from;recommendation methods;maximum entropy;log data
accuracy compared to;diverse set of;dynamic model;online auctions;online auction;dynamic environment;key features
association rule mining algorithms;binary attributes;regression models;logistic regression;association rules;numeric attributes
mining closed;closed frequent itemsets;closed itemsets;mining frequent;mining frequent;data stream;frequent itemsets;transaction databases;data streams;association rules
knowledge engineering;human effort;labeled data is;real customer;classification accuracy;text classifier;text categorization;classifier;sentiment-classification;large document collections;machine learning;machine-learning;statistical models;text processing
redescription mining;search procedure;instance;case studies;large datasets;data mining;mining algorithm;problem called
online social networks
svm classifiers;learning algorithms;support vector machines;classifier;classification
statistical approaches;real-life
shortest paths;sampling methods;sample size;sampling method;large graph;large graphs;sample sizes;graph patterns;sampling strategies;random-walks
clustering;pair-wise;clustering algorithms;closely related;partially ordered;clustering methods;clustering algorithm;clustering
visual data mining;visual data mining;machine learning;real-life;information visualization;computational complexity;large datasets;data mining process;visualization techniques
text mining;context information;mixture model;probabilistic latent semantic analysis;probabilistic model;text collection;comparative analysis;mining tasks
data cube;on line analytical processing;similarity metric;visual representations;real data;data cubes;data representations;tedious task
search space;mining algorithms;temporal patterns;time series;time series;data set;knowledge discovery;partial order;knowledge representation;temporal reasoning
clustering;order statistics;data point;data points;classification;clustering algorithms;large scale;accuracies;classification tasks;large margin;real world and synthetic datasets;large datasets;high probability;support vector machines
text documents;named entities;graphical models;topic models;making predictions
detection algorithm;data mining approach;raw data;semantically meaningful
clustering;classification;image-based;gene expression analysis;gene expressions;excellent performance;content-based image retrieval
naïve;collaborative filtering;cold-start;data sets;personalized recommendations;naïve
clustering;customer relationship management;data stream;digital library;transition model;topological properties
web documents;world wide web;text documents;machine learning;statistical analysis;theoretical analysis;pattern matching;natural language
query language model;search accuracy;clickthrough data;user's search;long-term;search history;retrieval performance;long-term;contextual information;web search;rich information;language modeling
discriminant analysis;kernel-based;large data sets;classification accuracy;massive data sets;feature extraction;kernel principal component analysis;fisher discriminant analysis;approximation algorithm;maximum margin;kernel methods;margin;space complexity
frequent itemset;frequent itemsets;random variables;end-user;probabilistic models;real datasets;large number of;post-processing;probabilistic approach;markov random fields;level-wise;mrf model
real-world datasets;accurate classifier;historical data;classification;stochastic model;mining data streams;class distributions;decision support;instances;rare class;training data;data streams;algorithm produces;class distribution;classifier
large-scale;query logs;search engine logs;user activity;query logs;user clicks
large amounts of;unlabeled data;classification;labeled training data;time series;data sources;labeled examples;accurate classifiers;semi-supervised;semi-supervised
privacy preserving data publishing;general case;np-hard;sensitive information
training set;support vector machines;approximate algorithm;svm training;incremental algorithm;optimization problem;matrix factorization;fine-grained;support vector machines;matrix-factorization;training algorithm
distance computation;distance-based outlier detection;sampling algorithm;outlier detection;real-life datasets;distance-based;distance computations
clustering;structural patterns;item-set;linear model;candidate patterns;frequent patterns;synthetic data sets;prior knowledge;interesting patterns
clustering;clustering problem;clustering results;clustering validation;clustering algorithms;uniform distribution;data distributions;clustering method;entropy measure;cluster sizes;key features;validation measures;data distribution
real data sets;synthetic data sets;categorical data;data set;anonymized data;query answering;applications involving
clustering;clustering results;cluster quality;clustering methods;bipartite graph;graph representation;clustering approach;clustering quality
mining algorithm;graph mining;frequently occurring;complete set of;search spaces;valuable knowledge;internal structure;optimization techniques;graph databases;graph database
real world;classification;search space;pruning strategies
window size;attack detection;recommender systems;time series;collaborative filtering algorithms;wide range;heuristic algorithm
association rule mining;association rules;frequent itemsets;database
graph structures;predictive accuracy;convex optimization problem;link information;text features;risk minimization;prediction models;collective inference;document categorization;graph regularization;web-page
operator;real dataset;mining (frequent;pruning techniques
enterprise search;data warehousing;data integration;data streams
data miners;financial services;data mining
data mining methods;database;social network analysis;probabilistic models;search engine;information processing;joint inference;information extraction;emerging patterns;data mining;information extraction;transfer learning;conditional random fields
text mining;customer satisfaction;data mining;large databases;case studies;data mining
ct images;linear program;redundant features;support vector machines;false positive;linear programs;medical images;linear discriminant analysis;boosting algorithm;computational costs;column generation;data sets;rates;classification method;unified framework;feature selection process;classification approach
data collected by;test data;remote sensing;high priority;decision tree;vector machine;event detection;hand-labeled;svm classifiers;dynamic events;classifier;decision making
text mining;large numbers of;human effort;free-text;clustering method;training sets;class distribution;customer-support
lessons learned
open source;search space;graph analysis
classification models;large scale;large collections of;feature generation;semantic features;meta learning;regression models;data mining;audio features;music collections
data mining application;decision trees;classification techniques;naïve bayesian;classification;class attribute;classification systems;data set;case study;data sets;domain experts;data mining system;class association rules
user interactions;classification;search experience;user behavior;domain-specific;machine learning;search result;web search results;ranking methods;web search engine
data sets;missing values;data mining
reusability;web-based;specifically designed for;data structures;database;knowledge discovery;object-oriented;knowledge discovery;data mining;high-level
search space;sequential pattern mining;mining framework;gene expression data;low support;order-preserving;real datasets;cluster model;long patterns;computational costs;massive datasets;small groups;bounded number of;expression levels;gene expression analysis;gene expressions;special case;subspace clusters
software development;decision tree;cost-sensitive learning;cost-sensitive;learning approaches;high cost;limited resources
software development;text mining;open-source;ensemble methods;large number of;data pre-processing;machine learning;case studies;data stream mining;complex data;graphical user interface;design decisions;distributed data mining;mining tasks
clustering;domain experts;public data;access logs;learned models;cross validation;private data;decision tree learning;data mining system;logistic regression;complex relationships;locally weighted
cost sensitive learning;directly optimize the;classification;classification;class label;ranking algorithms;instance;gradient descent;true positive;data mining projects;constrained optimization;classifier
text mining;link mining;data mining;web mining;data mining
data values;mining algorithms;vertically partitioned;data points;database;data distributions;data reduction;sensitive data;privacy preserving data mining;euclidean distances;distance-based;privacy-preserving;perturbed data;random perturbation;secure multi-party computation;increasingly popular
learning approaches;pattern-based;supervised learning techniques;mining framework;sensitive information;semi-supervised learning;database security;database;prediction accuracy;multi-relational databases;data mining;access control;nearest neighbor;semi-supervised
service provider;false positives;privacy-preserving;access control;storage space;privacy preserving;privacy-preserving;limited resources
computational complexity;small groups;heuristic approaches;large datasets;np-hard problem;database records;fixed size;multivariate data;information loss
privacy preserving;vertically partitioned;privacy issues;data items
virtual organizations;privacy-aware;database systems;marketing strategies;minimum set of;refinement process;access control;privacy policies;goal-oriented;business process;business processes;databases
motion parameters;gradient-based;optimal kernel;theoretical results;visual tracking;tracking performance;scale-invariant;optimal kernel;theoretical analysis;kernel-based
tracking algorithms;video sequence;multiple objects;object tracking;long sequences;higher level
tracking methods;local minima;contour based;fourier domain
video sequence;image noise;image sequences;1;facial features;facial expressions;pose parameters
texture analysis;classification;learning approaches;region-based;geometric model;perspective camera
recognition performance;learning algorithm;shape features;algorithm learns;incremental learning;object detection;object detectors;object categories
object recognition;feature level;template matching;object categorization;object recognition;feature selection;object categories
unlabeled images;local features;unsupervised learning;image features;clustering technique;supervision;automatically learn;input images;efficiently computing;features selected;object categories
clustering;object class;object classes;background clutter;high dimensional;generative model;object class;matching methods;reliable detection;detection approach;wide range;hierarchical representation;probabilistic model;recognition problem;excellent performance;object categories;generative model
tracking methods;auxiliary;efficient computation;real-world;computationally intensive;robust tracking;data mining;false alarms;auxiliary
clustering;feature points;motion analysis;feature trajectories;moving objects;moving objects
feature trajectories;object tracking;spectral clustering;feature trajectories;articulated objects;data sets;minimum spanning tree;articulated object;articulated objects
appearance model;motion analysis;motion tracking;appearance model;image regions;motion parameters;closed form solution;appearance variations;maximum likelihood
statistical properties;global optimal solution;measurement noise;group structure;moving objects;positive definite;covariance matrix;tracking method;detection rate
person tracking;dynamic programming;finding optimal;long sequences
statistical region-based;segmentation methods;statistical region-based;spatial correlation
error-prone;automatic segmentation;hidden markov model;graph cut;video sequences;segmentation accuracy;optical flow estimation;classifier;segmentation algorithm
closely related;image based;globally optimal;closed form solution;natural image;natural image;sparse matrix;high quality;closed form solution;cost function;segmentation algorithms;natural images;sparse linear;user input
object boundary;weighted graph;linear programming;shortest path;shortest path;segmentation algorithm;weighted graphs;shortest path
linear classifier;database;classification accuracy;linear discriminant analysis;spatio-temporal;shape descriptors;magnetic resonance images;classification performance;deformable model;statistical pattern recognition;principal component analysis;deformable model
modeling framework;spatio-temporal;accurately predict;prior knowledge;dimensionality reduction techniques;high dimensionality;motion models
appearance model;object tracking;human body;mixture model;appearance model;body parts;tracking results;estimation accuracy;typically rely on;visual appearance;likelihood function;filtering algorithm
uncertain information;body parts;prior knowledge;recognition performance;hidden markov models;video sequences
point correspondences;registration algorithm;point cloud;image processing;estimation problem;point sets;image data;image sequences;tracking problem;kalman filter;multi-view stereo
clustering;approximation methods;learning procedure;generative models;computationally intensive;clustering performance;generative model;fast computation;model estimation
object recognition;discriminative models;discriminative models;training data is;large-scale;generalization performance;generative models;objective functions;generative model;7;multi-class;semi-supervised;unlabelled data;2, 3;synthetic data
ensemble learning;pattern recognition;classification;frequency domain;ensemble methods;training process;machine learning;online learning;image sequences;human machine;high accuracy;image content;frequency domain;weak classifiers
high-dimensional;feature space;data points;classification;distance measure;image datasets;classifier;image datasets;sparse datasets;labeled dataset;high-dimensionality;concept class;supervised learning;public domain;classifier
fast algorithm;kernel-based;linear subspace;kernel principal component analysis;feature representations;feature selection algorithms;classification performance;salient features;numerical experiments;feature analysis;data representations;sparse kernel;recognition problem
multiple cameras;multiple views;training samples;pose estimation;pose estimation;learning method
reference data;motion capture;particle filters;test data;particle filter
data structure;video sequence;spatial distribution;18;image patch;robust tracking;object model;8, 6;computational cost;histogram-based;combining multiple;multiple image
case-based reasoning;video sequence;visual tracking;case- based reasoning;face tracking;object tracking;human face
underlying structure;pose estimation;image sets;image analysis;training examples
25;video segmentation;data description;11,13,14,24;hidden variables;simpler models;model parameters;generative models;conditional distribution;free energy;iterative learning;inference techniques;modeling approach;posterior distribution;local minima;model selection;multiple models
feature space;discriminant analysis;dimensional space;classification accuracy;discriminant analysis;high dimensional feature space;kernel discriminant analysis;feature vectors
pattern recognition;classification;positive-definite;linear discriminant analysis;principal components;theoretical basis for
feature space;face recognition;similarity threshold;similarity measure;similarity based;machine learning algorithms;margin;large variations;human beings;margin
low rank;low rank;iterative algorithms;iterative algorithms
hybrid approach;point-based;automatically extracted;region features;registration algorithm;image-based;feature extraction;feature-based;point set;matching) algorithm;point sets;region features;6;image registration;image noise
local geometry;shape representation;scale-space;shape descriptor;shape representation;local shape;shape information;image matching;variational framework
synthetic and real images;finite difference;level set function;edge information;deformable model;boundary detection
image patches;linear transformation;gaussian noise;visual correspondence;change detection;matching method;common assumption;image compression;feature matching
mutual information;statistical dependence;random variables;spatial information;image matching;video tracking;image description;histogram-based
shape representation;matching algorithm;shape retrieval;hierarchical structure;shape retrieval;specific features
total number of;classification;training error;cost-sensitive;boosting algorithms;error rate;total cost;error bounds;classification algorithms
object recognition;unlabeled data;image segmentation;local neighborhood;optimization problem;semi-supervised classification;labeled data;image information;intrinsic structure;labeled and unlabeled data;general problem
clustering;matrix factorization;classification;objective function;sparse representations;cardinality constraints;recognition accuracy;human perception;separability;dimensionality reduction
face recognition;lower dimensional;classification;supervised dimensionality reduction;recognition performance;feature subspace;face recognition;margin based;vector-based;small training sets;dimensional data;margins;subspace learning;margin;margins;high dimensional space
computational complexity;object classes;highly structured;image datasets;object categorization;statistical properties;natural images;statistical learning;object categories
object recognition;range images;multi-resolution;highly correlated;high resolution;large datasets;feature point;multi-resolution
vision applications;markov network;optimization algorithm;stereo matching;belief propagation;robust tracking;vision problems;stereo matching;flow field;image pair;spatially varying;low-level
matching scheme;point-based;local affine;wide range;hash table;local image;local affine;local affine
image patches;additive noise;sparse representation;image denoising;image denoising;training algorithm
single image;prior model;edge detection;image based;noise level;noise estimation;upper bound;noise estimation;algorithms require;single image;noise levels
face images;face recognition;face verification;local regions;face recognition algorithms;databases;high dimensionality;face verification
spectral clustering;spectral methods;data clustering;data sets;similarity matrix;7;automatically learn
synthetic datasets;gaussian process latent variable;image sequences;dimensionality reduction;auto-regressive;learning method
conditional likelihood;generic model;training images;conditional random field;parameter estimation;database;19;maximum likelihood;learning algorithm;feature detectors;articulated objects;3;training data;6;8;deformable models;deformable models
10;propagation algorithm;22, 19;sampling method;belief propagation;stereo vision;algorithm requires;sampling process;6;2, 7
conditional random field;crf) model;estimation problem;state-space;human pose;video sequences;continuous state
training data;image features;gaussian process;requires minimal;regression model;human eye;semi-supervised;semi-supervised;bayesian learning
low-resolution;image features;image features;stochastic model;real data;closed-form solution;input images;regularization framework;sampling rate;video frames
training images;image segmentation;training examples;object parts;region-based;learned model
object recognition;mutual information;recognition rate;category recognition;illumination conditions;video database;object appearance;main idea;category recognition;line segments
context sensitive;learning phase;graphical) model;context free;graph representation
body parts;major source of;camera motions;fully automatic;data association
training set;training data;gaussian process;human motion;model averaging;human pose;density function
computational complexity;diffusion process;multi-dimensional;shape context;distance measures;histogram-based;local descriptors
image segmentation;binary data;component analysis;image data;principal component analysis;image data;character recognition
training data;feature selection method;classifier;visual tracking;training methods;recognition tasks;object detection;feature extraction methods
conditional likelihood;mixture components;discriminative learning;classification;sequence data;bayesian network classifiers;gradient boosting;human motion;classification performance;time-series;discriminative learning;sequence classification;classification problems;learning task
feature space;feature sets;classification;feature space;classification accuracy;hierarchical clustering;automatic methods;classification methods;independent component analysis;feature selection;computational efficiency;principal component analysis;support vector machines;clustering algorithms;competitive learning
pre-defined;temporal order;activity recognition;clustering;instances
bayesian model;object parts;inference algorithm;object segmentation;large sample
kernel pca;training data;level-set;image segmentation;multiple types of;prior knowledge;segmentation results;prior shape;segmentation method;principal component analysis;image information;segmentation algorithm
regularization;image noise;domain knowledge into;large number of;regularization;shape priors;local shape;object boundaries;shape features
image retrieval;similar objects;video tracking;generative model for;graph cuts;global constraint;wide range;image pairs;image pair;optimization scheme;global constraint;np-hard
error rate;training set;object category;classification;large-scale;object categorization;recognition task;invariant features;instances;feature vectors;test set;gaussian-kernel;support vector machines;object instances;classifier;object categories
clustering;pattern recognition;game-theoretic;clustering approaches;generic framework;game-theoretic;game theory;arise naturally in
map-estimation;map-estimation;object recognition;facial images;image registration;face recognition;depth map;markov random field;em-algorithm;vision problems;generative model;diverse domains;model parameters
probabilistic framework;natural scenes;information sources;learning tasks;instances;contextual information;character recognition
distance metrics;heterogeneous sources;retrieval applications;motion tracking;stereo matching;distance metric;distance metric;maximum likelihood;similarity estimation;video sequences;benchmark data sets;similarity estimation
weakly supervised;training set;segmentation methods;image segmentation;supervision;weakly supervised;image regions;visual concepts;segmentation algorithm
bayesian network;automatic segmentation;algorithm to compute;interactive segmentation;object boundary;automatic segmentation;constraints imposed by;joint probability distribution;bayesian network structure;boundary points;conditional probability;boundary detection
counterpart;level-set;level sets;segmentation techniques;shape information;image understanding;object boundaries
iterative learning;optimization problems;graph cuts;graph cuts;2;multi-label;visual perception
cost function;globally optimal;globally optimal;medical images
objective function
pattern recognition;real data;linear subspace
face detection;classification tasks;object detection;upper-bound;object detection;classifier;classification error
video frame
monte carlo;search result;automatically creating;salient regions;input images;visual information;bayesian framework
fast marching;shape representation;vision applications;finite difference;computational complexity;highly accurate;wide range;accurate tracking;higher order;nearest neighbors
computational efficiency;object recognition;random fields;closely related;quadratic programming;graph cuts;belief propagation;markov random fields;map estimation;subgraph matching;np-hard
graph partitioning;clustering;image segmentation;clustering;image segmentation
minimization problem;energy functions;graph cuts;inverse problem;graph cuts;medical imaging;mr images;markov random fields;theoretical analysis;mr images
statistical measures;natural images;scale invariant;low level;higher order;multi-scale
fast marching;shortest paths;parameter-free;synthetic datasets;vector field;control theory;control theory;magnetic resonance imaging
image analysis;image data;control mechanisms;deformable models;high-level;decision making
camera motion;computational complexity;video rate;ground truth;real data
synthetic and real images;multi-perspective;baseline algorithm;high quality;depth map;image pairs;images captured;stereo camera
object model;synthetic and real images;image-based;contour-based;contour-based;multiple models;point correspondences;object reconstruction;camera parameters
face images;face recognition;database;takes into account;linear constraints;human faces;shape variations
training data;high-frequency;multi-resolution;database;local patches;tensor space;facial expressions;high-resolution images;low-resolution;video sequences;bayesian framework;facial image;high-resolution;multi-resolution
automatically discovering;mri data;manifold structure of;level set function;left ventricle;level set;data sets;image set;manifold structure of;mr images;segmentation algorithm
segmentation problem;level sets;local) minimum;region-based;level set;gradient descent;mr images
local features;iterative process;image labeling;scene understanding;image labeling;image recognition
classification;multi-channel;mri data;tree-based;mr images;classification approach;classifier
clustering;model order selection;image segmentation;model order selection;parameter tuning;model order selection;sampling scheme;classifier
human-generated;weighting function;training data;image segmentation;vision applications;application level;learning algorithm;machine learning;reference image;segmentation quality;image segmentation;segmentation algorithm
scene structure;motion parameters;intrinsic parameters
unlike prior;typically rely on;motion field;motion field;spatio-temporal
high level;semantic features;level features;low quality;high quality;quality assessment;image search;low-level features;low recall;quality assessment;level features;classification rate
human perception;texture analysis;general purpose
image region;spatio-temporal;moving object;instances
huge number of;energy function;computational cost;optimization problem;belief propagation;global optimization;optimization scheme;exemplar-based;objective function
ground truth;segmentation methods;image segmentation;graph-based;general-purpose;level-set;test images;segmentation accuracy;real images;image-segmentation;image segmentation
11;markov random field;novelty detection;graph cuts;belief propagation;graph-cuts;precision-recall;loopy belief propagation;graph cut;novelty detection;video sequences;posterior distribution;belief propagation
clustering;em) algorithm;kernel density;feature vector;fixed-point;spatial structure;clustering algorithm;average number of;image segmentation
clustering;clustering;frequently occurring;euclidean spaces;segmentation method;motion segmentation
instance;segmentation algorithm;main idea;temporal constraints;motion segmentation

segmentation techniques;image region;single-image;image regions;single image
conventional techniques;large numbers of;unknown environments;particle filter
multiple views;sufficient conditions for;point correspondences;synthetic data
cost function;optimization techniques;vision problems;local minima;optimization problems
salient features;real world;ground truth;surface reconstruction;shape reconstruction
image points;takes into account;geometrical constraints;constraint logic programming;geometric model
real data;image motion;perspective camera;shape priors;segmentation algorithm
feature point;feature points;real data;optimization framework
optimization algorithm;global optimality;real data;global optimization;pose estimation;globally optimal solution;provably optimal;local minima
video sequence;synthetic data;closed-form solutions;euclidean structure;camera parameters;partial occlusion;accuracies;euclidean structure

large numbers of;graph structure;multi-view stereo;high quality;multi-view stereo;input images;stereo algorithm;graph embedding
efficient computation;dynamic programming;labeling problems;dynamic programming;labeling problem
evaluation methodology;ground truth;image datasets;multi-view;high-accuracy;reconstruction algorithms;shape models;multi-view stereo;benchmark datasets;reconstruction algorithms;multi-view stereo
optical flow;optical flow;optimization process
selective attention;closed-form solution;video surveillance;video surveillance;real-world applications
multi-camera;manifold learning;manifold learning;additional information;multi-camera
camera motion;optimization problems;optimization framework
point features;high precision;motion estimation
hand-held;particle filtering;particle filter;motion models;auxiliary
image points;images acquired
single images;line segments;single image;perspective camera;multi-view
tracking method;tracking algorithm;data association;feature vector;data structure
prediction accuracy;distance function;color space;object-tracking;surveillance video;moving object;data mining;lighting conditions
classification;image region;spatial context;temporal context;tracking performance;probabilistic pca;tracking method;multi-target tracking
clustering;independent motion;supervised learning;image features;image space;data association;general case;detection algorithm;data driven;clustering framework;clustering algorithm;primary goal
visual tracking;video sequence;illumination conditions;basis functions;face tracking;linear combination;visual tracking;shape model;visual tracking;low dimensional;unified framework;sampling framework;ground truth
high-speed;ground truth;image sequences;prior knowledge;image sequences;wide range;estimation algorithms;extended-kalman-filter;vision-based;image sequence;motion model
point sets;jensen-shannon divergence;probability density functions;jensen-shannon divergence;point set;data set;probability distributions;data sets;density functions;takes place;point-sets
shape models;compression scheme;model selection
fully automatic;fourier transform;point sets;data sets;final step;fourier domain;arbitrarily large
global illumination;face image;manual labeling;algorithm iteratively;view-based;wide range;single image;deformable model;perspective projection
regularization;image features;structural constraints;markov network;image noise;markov network;face alignment;face alignment
multi-kernel;kernel-based;motion tracking;video sequences;lighting conditions;kernel-based
tracking algorithms;approximately optimal;kernel-based;ad-hoc;tracking results;tracking problem;design space;feature spaces;target tracking;kernel-based
image patch;dynamic programming;feature detection;iterative process;highly compressed;feature tracking;interactive applications;dynamic programming;data stored in;feature tracking
low-quality;object tracking;large number of;data association;data association;shortest path;object level;video sequences;object tracking
tracking algorithms;multiple-target;dynamic model;data association;image sequences;multi-target;image data;tracking algorithm
visual tracking;robust tracking;vision problems;theoretical analysis;multiple sources;information integration
classification problem;background clutter;spatial region;feature selection;feature selection;motion cues;motion cues;motion information;motion segmentation
image representations;appearance-based;image representation;vision systems
ground truth;million images;background clutter;local regions;database;large number of;retrieval quality;local region
local features;database;scene categories;image representation;scene categorization;high accuracy;scene categories;large database
spatial information;image sequences;feature distribution;tracking problem;tracking results
image registration;registration algorithm;images acquired;multi-modality;image registration;bayesian framework;prior knowledge;bayesian framework
multi-level;low-resolution;facial expression recognition;data obtained from;images captured;image quality;active appearance models;stereo camera;human faces;shape variations
clustering;feature points;independent motion;image registration;graph cut;sparse set of;multi-label
graph cut;moving objects;multiple images;dynamic range
matching algorithms;image matching;vision applications;database;point features;matching methods;invariant features;color information;image matching;image matching;real images;additional information
human subjects;binary data;texture features;relevance feedback;model parameters;bayesian framework;content-based image retrieval;bayesian framework;content-based image retrieval
classification techniques;classification;spatial relations;object localization;object boundaries;background clutter
computational complexity;nearest neighbor classification;nearest neighbor;classification;distance function;category recognition;vector machine;object recognition;classification rate;data sets;training images;category recognition;distance functions;support vector machines;benchmark data sets
image understanding;image understanding;object detection;vice-versa;detection methods;object detector
mathematical framework;context-sensitive;vision systems;bayesian network;image set
bayesian network;inference problem;multi-target tracking;bayesian network;labeling problem;high complexity;similarity measures;ground truth;multi-target tracking
point-based;motion estimation;scene geometry;long-range;real-world;motion estimation;multiple types of;optical flow;feature tracking
multi-camera;scene flow;multi-camera;optical flow;scene flow;scene flow
data collected from;main components;point cloud;geometric model
model fitting
fast marching;path planning;large scale;landmark points;landmark-based;compression scheme;memory requirement;artificial intelligence
ground truth;stereo vision;test images;stereo vision;input images;matching method
spatial correlations;multi-view stereo;free energy;em-algorithm;moving objects;generative model;input images;real-world scenes;multi-view stereo;model parameters
multi-view;stereo algorithm;multi-view stereo;window-based;depth maps
object recognition;probabilistic model;image-based;dirichlet process;training process;hierarchical model;perspective projection;object instances;object categories;visual features;stereo images
dynamic bayesian network;prior knowledge;model assumes;single image;dynamic bayesian network
linear model;synthetic data;16;3, 2;shape models;random noise;factorization method
image pairs;mutual information;shape reconstruction;shape reconstruction
instances;video sequence;perspective camera;expected values
point cloud;large-scale;range data;range images;range data;human interaction;range scans
edge-information;image based;global analysis;color images
structured light;light-source;light source
depth map;multi-camera;compared with conventional;large number of
matching algorithms;machine vision;human vision;stereo matching;scene points;high resolution
energy- minimization;stereo matching;belief propagation;data set;loopy belief propagation;matching problem
surface normal;propagation algorithm;belief propagation;inference mechanism;contextual information;belief propagation;belief propagation;geometric constraints;stereo correspondence
specular reflections;specular reflections;synthetic and real images;stereo correspondence
cost functions;reference image;target image;cost functions;optimization problem;belief propagation;stereo matching;cost function;stereo matching;mrf model;markov random field;color segmentation
minimum spanning tree;tree based;region-based;tree structure;ranking algorithms;dynamic programming;dynamic programming;benchmark datasets;stereo algorithm
nearest-neighbor;surface reconstruction;surface reconstruction;matching process;operator;surface reconstruction
body parts;distance measure;point cloud;moving object;multi-view
line segment;camera parameters;bayesian approach;computed tomography;line correspondences;image noise
virtual camera;video sequence;camera motion;euclidean reconstruction;real-world;moving object;intrinsic parameters;multiple objects;segmentation method;moving objects;objects moving;video sequences
cost function;distance transform;graph cuts;graph cuts
point-based;image registration;success rate;image-based;image data;medical imaging;video tracking;computed-tomography;image space;estimation method
probabilistic framework;markov network;object parts;articulated objects;loopy belief propagation;multiple information sources;low-level
object recognition;classification;matching algorithms;highly accurate;global optimization;matching problem;recognition rates
specific algorithms;tensor-based;dynamic analysis;image regions;tracking algorithm;image processing;pattern extraction
multi-level;registration algorithm;large scale;optimization method;extraction process;iterative optimization;mr images
spatially varying
design space;cost functions;multi-view stereo;synthetic aperture
face images;face recognition;low-resolution;information contained in;face image;vector representation;high resolution;human faces;high resolution
face images;face representation;face image;scale-space;feature vector;face representation;discriminative power;nearest-neighbor classifier;linear combination;decision rule;databases;adaptive control
face recognition;depth map;baseline algorithm;invariant features;depth maps;human faces
face recognition systems;matching scheme;face recognition;image-based;matching accuracy;deformable model;face model;large database
clustering;distance measure;face recognition;voting scheme;database;linear models;classification;classification;local models;face images;video sequence;classification decisions;test images;distance-based;image set;test image;video sequences;human faces;clustering procedure;multiple images
false positives;training data set;vector machine;loss functions;multiple-instance learning;region-based;data sets;region-based;learning problem;image annotation;learning strategy;image annotation
training images;linear model;effectively learn;object class;supervision;landmark-based;object classes;object shape
image retrieval;image retrieval;distance metrics;distance metrics;contextual constraints;component analysis;data instances;contextual constraints;contextual information;distance metric learning
local features;classification;classification;classification methods;visually similar;visually similar;fully automatic
clustering algorithms
real data sets;training examples;negative examples;object detection;object detection;support vector machines;margin;natural images
machine translation;training set;local image features;image features;multiple objects;test images;training images;individual features;visual features
face recognition;surface normal;data compression;database;generalization performance;data normalization;face recognition;recognition process;principal component analysis;face alignment;search algorithm;dimensionality reduction
face recognition;database;19;facial expression recognition;facial expression recognition;facial expressions;feature distribution;range data;feature distribution;human face
shape information;face recognition;input image;face recognition;shape information;real world
face images;face recognition;pattern recognition;feature set;database;recognition performance;learning algorithm;robust face recognition;boosting algorithm;feature selection algorithm;facial features;wide range;feature selection;appearance variations;face database;feature selection
face recognition;random variables;face model;face recognition;expectation-maximization algorithm;single image;face database
recognition performance;discrimination power;face recognition;pose variation;face recognition
clustering;training data;objective function;multi-view;computational framework;strong classifier;intra-class;discriminative features;object detection;detection performance;multi-view;individual classifiers;exemplar-based;classifier
visual similarity;classification;database;excellent performance;visual vocabulary;visual vocabulary;nearest neighbour;classifier
text search;transition model;dynamic programming;text queries
free text;web pages;visual cues;clustering method;supervision;visual information;wide range;local image
video sequence;statistical translation;unknown word;training examples
face images;graph structure;graph based;detection method;real life;graph based;similarity graph
high dimensional;web images;text-based;web-scale;machine learning;data mining technologies;similar images;image annotation;content-based search;visual features
human detection;salient features;image representation;human detection;feature selection;variable-size
depth images;depth images;low-resolution;rates;color images;low-resolution;low efficiency
face images;low-resolution;test set;face detection;detection method;face detector;real world;detection rate
clustering;fully automatic
underlying structure;classification;hidden-state;conditional independence;hidden states;hidden structures;hidden markov model;generative model;recognition tasks;hidden state;conditional random fields
operator;database;large number of;segmentation algorithm
local optimization;dynamic programming;mixture model;large number of;sequential monte carlo;hierarchical model;tree-structured;stochastic search
ct images;ct images;15;convergence rate;medical imaging;detection algorithm;2;generative model
probabilistic latent semantic analysis;classification;classifier;generative model;automatically extracts;local descriptors
image-based;classification;classification;multiple objects;image-based;large number of;tree structure;binary classifiers;object detection;classifier
face recognition systems;algorithm parameters;recognition rate;similarity scores;recognition performance;similarity measurement;face recognition system;recognition systems;em algorithm;individual queries;intrinsic structure;face alignment
face images;face image;features extracted from;face model;linear discriminant analysis;face recognition;face recognition system;data sets;rates;image set;single classifier
local appearance;multi-aspect;shape model;articulated objects;training samples;wide range;5;human body
object recognition;multi-view;shape model;object instances;object class;multi-view;single-view;object class
object classes;image segmentation;similar object;image datasets;automatically discovering;topic discovery;image collections;text analysis
clustering;4, 20, 7;object categorization;object category;22;local regions;text documents;highly competitive;object categorization;local patches;hierarchical structure;object classes;object categories
data-driven;complex events;multiple objects;complex events;accurate tracking;occur frequently;event recognition
dynamic bayesian network;probabilistic framework;classification techniques;classification;dynamic nature of;temporal evolution;extraction techniques;facial feature;semantic relationships
graphical models;training examples;domain expertise;temporal sequence;data types;individual nodes;anomaly detection;supervised approach;annotated data;labeled data;unsupervised fashion;internal structure;learning method
clustering;training examples;multiple views;classification methods;multiple cameras;action recognition;semi-supervised;high-level
matching scheme;video sequence;video frames;human action;linear programs;small sets of;human actions;matching problem
clustering;spectral clustering;training images;unsupervised learning;linear programming;test images;pruning method;computationally expensive;image labeling
data collection;hidden markov models;data analysis;temporal patterns;sample size
appearance model;discriminant analysis;appearance-based;feature space;tensor based;data set;appearance models;structure information;high dimensionality
human motion;explicitly models;linear systems;classification algorithm
markov random field;conditional random field;parameter estimation;hidden variables;activity recognition;video surveillance;maximum likelihood;activity recognition;monitoring systems
data sequences;inference algorithms for;dynamic systems;dynamic systems
general problem;image sequences;action recognition;database
human activities;context-free;context-free;representation scheme;image sequences;human activities;human activities;high accuracy;recognition rate
unlike standard;medical image;data points;multi-label learning;vision applications;multi-label learning;real-world;scene analysis;single class;multiple labels;traditional classification;explicitly models;class labels;label propagation;label propagation;precision/recall
mutual information;classification;optimization method;feature extraction;classification performance;solution space;gradient method
input space;low dimensional;dimensionality reduction;dimensionality reduction;high dimensional
human detection;background clutter;generative model;human motion
local features;discriminative power;multi-view;test images;object detection;matching process;local features;local affine;features selected
similarity function;motion estimation;database;temporal information;large number of;training process;left ventricle;similarity function;detection algorithm;tracking algorithm;motion estimation;appearance variations;motion estimation
estimation technique;density estimation;image registration;level sets;mutual information;intensity values;parameter tuning;spanning trees;mutual information;probability density;mixture models
multi-modal;image registration;kernel density;processing algorithms;representative set;mutual information;prior knowledge;image registration;bayesian framework;image pairs;low-level

optimization algorithm;real data;surface reconstruction;dense set of;convex optimization problem;expectation-maximization
image features;light sources;high quality;shape recovery;surface reconstruction;local image;stereo images
single view
reflectance model;synthetic data;real data
real-world
spatially varying;sparse set of;high-frequency
closely related;semidefinite programming;synthetic and real images
illumination conditions;synthetic data;camera motion;multi-view;surface reconstruction;lighting conditions
real images;image space;spatial distributions;color space;color information
scene structure
scene geometry;additional constraints;scene structure;specular reflections;image motion;expectation-maximization algorithm;optical flow
object recognition;classification;classification;face detection;large number of;tree-structured;search strategies;binary) classifiers;classifier
edge detection;scene geometry;multiple views;multi-channel;multi-channel;target object;scene structure;detection algorithm;single image;lighting conditions;vision-based;visual features;detection rate
graphical models;feature detection;sketch-based;graphical model;detection results;object detection;object representation;object categories;increasingly popular
feature space;image processing;feature points;image specific
independent motion;video segmentation;model offers;joint distribution;video sequences;higher-order;image sequence;motion patterns;motion patterns;high-level
statistical approaches;theoretical framework;kernel density;probability estimates;background modeling;motion analysis;moving objects;feature selection;video surveillance;boosting algorithm;feature selection
face images;face recognition;database;takes into account;linear constraints;human faces;shape variations
local affine;texture analysis
shape representation;scale space;shape representation
low-resolution;color image;edge-preserving;learning procedure;computational framework;image super-resolution;multi-scale;test cases;image super-resolution;high-resolution
spatially varying;blurred image;pre-processing
edge detection;large number of;natural images;object boundaries;low level;edge detection;supervised learning algorithm;supervised learning;boundary detection;classification algorithm
image enhancement;noise reduction
input image;valuable information;feature descriptor
depth map;user interaction;spatially-varying
weighting function;real-world;training set;linear regression;image denoising;single image
specular reflections;vision problems;color space;motion estimation;color information
specific applications;kernel-based;classification;kernel matrix;path-based;classification tasks;nearest neighbor classifier;graph laplacian;labeled examples;image sets;synthetic data;real-world;graph laplacian;pattern recognition;weighted graph;classification;similarity measure;graph-based;machine learning;object classification;classifier;object classification
clustering;scene analysis;surface normals;surface normal;scene geometry;scene analysis;scene points;image sequence;clustering algorithm;light source;clustering
video sequence;summarization method;video frames;visual information;graph cut;summarization methods;video content;optimization techniques
modeling framework;high speed;navigation systems;vision algorithms
surface normals;class information;single view;object classes;single-view;natural images
generative-discriminative;hybrid approach;multi-view;svm-based;linear combination;hybrid method;detection method;object detection;detection performance;prior works;object detection;support vector machines;model called;likelihood ratio
compact representation;monte carlo;local shape;compact representation;shape retrieval;input query;sampling strategy;shape retrieval;multi-class;text analysis;object categories
large numbers of;object classes;class models;shape information;visual words;databases;missing information
human body;body parts;image region;prior models;human pose-estimation;propagation algorithm;pose estimation;tree-structured;real-valued;graphical model;inference algorithms
natural scenes;visual search;feature maps;visual features;target detection;background clutter
scene geometry
structured light;object appearance;visual features
visualization tool;structured light;motion tracking;visualization technique
hand-held;ad hoc;euclidean structure;calibration method
dynamic range
image-region;exploratory analysis;clustering approaches;spatial neighborhood;fmri data;iterative clustering;benchmark datasets;compares favorably with
cognitive states;training data;classification;classification;feature selection method;classification methods;machine learning;statistical information;discriminative features;generalization ability;svm classifier;building classifiers
data-driven;data analysis;classification;temporal patterns;manifold learning;time series;low dimensional;magnetic resonance imaging;dimension reduction
feature space;image registration;spatio-temporal;temporal dimension;mr images;mr images;spatio-temporal
context information;probabilistic framework;hidden markov model;identification method;hidden markov model;search algorithm;images acquired;control parameters
segmentation methods;perceptual information;image features;real-world;image segmentation;perceptual information;visual perception;segmentation algorithms
edge detection;ground-truth;hierarchical classification;segmentation techniques;natural images;geometric structure;natural images;low-level
computational complexity;global constraints;database;contour extraction;refinement process;natural image;multi-scale;contour extraction;natural images;multi-scale
image regions;natural images;database
edge detection;feature detection;finding task;multiple objects;detection method;image data;statistical model;object boundaries;natural scenes;approximation algorithm;general problem;special case;np-hard
line segments
high-resolution;video streams
energy function;dynamic range;local color;color space;gradient descent;spatial locations
point cloud;target object;point correspondences
high quality;digital camera;high-resolution images;matching method;vision-based;video camera
real images;color images;high-frequency
local minima;shape priors
long term;pair wise;input data
illumination conditions;dynamic range;classification;database;prior knowledge;content analysis;image analysis;expectation maximization;information extraction;analysis task;classification results;visual appearance;class distribution;manually labeled
markov random field;local optima;parameter estimates;prior model;mr images;mr images;maximum likelihood
mathematical framework;noise model;error analysis
shortest paths;markov chain;edge detection;globally optimal;contour extraction;objective function
human vision;computational models
edges represent;explicitly represented;hierarchical organization
real images;low level
feature points;local affine;feature descriptors;perspective projection
real data sets;manifold learning;data sets;generative model;motion patterns;dimensional data;special case;effective tools
context information;matching scheme;local features;global information;spatial information;matching methods;feature-based;local features;nearest neighbor;feature matching
data set;object recognition;image patches;recognition rate;object class;patch-based;recognition task;target object;object class recognition;instances;statistical method;combinatorial optimization;statistical models
local image features;local features;sparse set of;classification;large-scale;features extracted from;object databases;vector machine;recognition performance;recognition tasks;databases;classifier;object categories
semantic classes;markov random field model;classification;image collection;invariant features;classification problem;image-specific;contextual information
object classes;similar objects;imaging conditions;database;single class;target function;single image;single view
high-dimensional;parameter space;sign language;statistical estimation;tracking applications;optical flow;deformable models
random sample;multiple subspaces;geometric model;statistical techniques;statistical estimation;image sequences;image sequence;multiple subspaces
data point;ensemble techniques;classification problem;motion estimation;ensemble method;extensive simulations
regression algorithm;regression problem;cost functions;sampling algorithms;projection based
markov random field;facial images;model assumes;face recognition;image pixels;em-algorithm;robust estimation;regression problem;generative model;robust estimation
optimization technique;parameter estimation;parameter estimation;genetic algorithm;multi-view;minimum number of;prior information
human visual system;object recognition;recognition performance;voting scheme;object recognition;feature descriptor;contextual information;context-based
bayesian network;training images;database;probabilistic model;shape model;scene analysis;rates;object representation;real world;image categorization
multi-modal;geometric information;multi-modal;scene reconstruction;reconstruction error
laser range;object-based;appearance-based;false positive;database
pixel-level
optimal performance;generic framework;video object;segmentation results;image sequences;moving objects;evaluation criteria;segmentation algorithms;video object;segmentation quality;segmentation algorithms
probabilistic latent semantic analysis;object recognition;spatial information;visual words;object detection;iterative algorithm;latent semantic
face images;category specific;markov random field;multi layer;high level;belief propagation;graphical model;training data;low level vision;multiple frames;patch based;supervised learning;image restoration
detection algorithms;computational complexity;discriminative features;color information;detection process;image content;global features;salient regions;salient points;image information;local information;local feature
handle complex;edge detection;temporal information;spatio-temporal;patch-based;filtering techniques;spatio-temporal
objects moving;video sequence;moving objects;activity recognition;classification
image points;face recognition;classification;linear dimensionality reduction;spatial information;linear dimensionality reduction;case studies;feature selection;feature selection
spatial relationships;region-based;low-level features;scene classification;region-based;loopy belief propagation;scene classification;object detectors;fully connected;generative model
scene categorization;vector machine;scene categorization;competitive performance;low level features;natural images
clustering;search results;image features;content based image retrieval;image search;search engines;search engine;keyword query
detecting anomalies;event recognition;syntactic structure;anomaly detection
view based;low resolution;aspect model;high level;tracking framework;multiple views;image data;facial features;shape model;shape information;deformable model;facial features;facial expressions;face image;shape models;feature analysis;appearance variations;shape model;feature tracking
graphical models;sequence data;probabilistic models;event detection;state transition;hidden markov models;multi-modality;event detection;high-level;conditional random fields
real data sets;motion analysis;image based;temporal information;multiple images;camera motion;image sequences;image sequence;reference image;higher dimensional;motion segmentation
modal analysis
moving object detection;false positives;scene understanding;false alarms;true positive;scene understanding;scene understanding;segmentation algorithm;object segmentation;motion segmentation
motion analysis;weighted graph;image-feature;equivalence relation;real-world;equivalence relations;image sequence;clustering algorithm
boosting framework;object recognition;discriminative features;local features;database;decision boundary;object category;large numbers of;selected features;feature selection algorithm;nearest-neighbor;lower computational cost;feature selection;individual features;class labels;nearest neighbour;strong classifier;classifier
deformable objects;image segmentation;numerical results;object class;patch-based;region-based;object detection;local patches;object class;semi-supervised;learning framework;pixel-level
data collection;machine learning methods to;hand-crafted;eye movements;operator;movement data;eye movements;human eye;operator;multi-scale
clustering;image feature;video frames;image features;image-feature;real-world;scale invariant feature transform;time series;euclidean distances;image sequence;wavelet-based;video sequences;feature vectors;video frame
signal processing;classification;decision rules;detection method;classification tree;single image
empirical mode decomposition;time series;data set;images captured;time-series;spatial domain;pre-processing;classification rate
pre-defined;composite events;high-complexity;spatio-temporal;event detection;composite events;multi-camera;moving objects;video surveillance;higher level;high-level;spatio-temporal;camera views
visual similarity;segmentation problem;image segmentation;vice versa;semantic similarity;boundary detection;instances;semantic content;manual labeling;boundary detection;semantic similarity
image retrieval;object recognition;classification;machine learning
multiple classes;classification;loss function;multiclass classification;originally designed;boosting algorithm;classification problems;classifier
active learning;information contained in;active learning algorithms;post-processing;high-level;learning process;clustering;clustering results;semi-supervised learning;learning scheme;video annotation;informative samples;active learning;semantic concepts;training samples;limited number of;svm classifier;training set;svm-based;video annotation;low-level features;margin;semi-automatic
human brain;desired properties;medical data;medical imaging;high resolution;storage requirements
control systems;combining multiple;pattern based
theoretical analysis;error rate;feature vector;security level;database
face images;local features;quality assessment;quality assessment;image quality;recognition rates
training set;vector machine;database;similarity scores;similarity scores;large number of;multi-classifier;fusion methods;svm)-based;selection criterion;classifier;likelihood ratio
baseline methods;aspect model;test set;video data;retrieval performance;maximum entropy;static images;motion cues;semantic information;video annotation
clustering;classification;scene analysis;visual information;novelty detection;visual information;audio-visual
semantic concept;text search;semantic concepts;news video;multimedia retrieval;semantic concepts;classifier
tracking results;real-time tracking;surveillance systems
graphical model
motion analysis;fast algorithm;computational power;belief propagation;monitoring applications;image motion;power consumption
matching algorithms;image sets;color image;image data;data set;matching algorithm
face images;face recognition;appearance-based;feature sets;feature vector;face databases;face recognition;block-based;appearance based;distance measures;feature selection;discrete cosine transform;local features;training set;feature vectors;feature selection
object recognition;classification techniques;scale invariant feature transform;database;pattern recognition;image descriptors
information content;completeness;sample size;sample size;iterative algorithm;information theoretic
computational efficiency;recognition rate;database;feature extraction technique;spatiotemporal data;image sequences
dynamic programming;optimization process;rates;cost function;real world;recognition problem
high throughput;feature detectors;hardware architecture;feature detectors;local affine;multi-scale
software architecture;design process;high level
optical flow;high-level;embedded systems;power consumption
field-programmable gate arrays;verification framework;vision systems;vision algorithms
video stream;vision algorithms;real-world applications;environmental conditions;vehicle detection;high-level;embedded systems
vision systems;stereo camera;range data;background modeling;stereo vision;real-world;stereo camera;visual analysis;programming interface
free text;accurately identify;free-text;biometric recognition;input features;classifier
database;shape descriptors;high-order;multiple times;scale invariant;2;wide range;computational cost;images acquired;high order
face recognition systems;face recognition;data collection;face recognition algorithms;recognition performance;face recognition;human operators
face recognition systems;face images;face recognition;database;graph matching
face recognition;appearance-based;classification;recognition performance;face recognition;local image;appearance-based;face database
automatically extracted;selection process;prior knowledge;image sequences;bayesian approach;bayesian framework
classification performance;matching algorithm;classification;viewing conditions;database;expected error;low resolution;moving object;match scores;environmental conditions;rates;pattern matching;classifier
pose variations;training images;test set;personal identification;similarity scores
automatically detects;human face;magnetic resonance imaging
perspective projection;1;video tracking;motion segmentation;estimation algorithm;segmentation algorithms
independent motion;imaging conditions;camera motion;object class;critical task;prior knowledge;moving objects;moving objects;video surveillance;moving object detection;moving object detection
color information;object localization;moving objects;accurately detect;video sequences;moving object detection
frequency-domain
feature space;face recognition;appearance-based;feature space;numerical results;linear discriminant analysis;face recognition;independent component analysis;principal component analysis;feature analysis
face recognition;kernel function;face recognition;kernel discriminant analysis;databases;human face;kernel based
discriminative training;distance measure;face recognition;recognition algorithm;feature set;database;large-scale;vector machine;face recognition;feature analysis;facial expressions;pose variations;support vector machines;dimensionality reduction;face database
training set;facial images;test image;robust face recognition;face recognition;test images;higher-order;facial expressions;multiple factors;tensor decomposition;optimization techniques;factorization method
computational complexity;classification;data compression;similarity measure
high quality;digital camera;high-resolution images;matching method;vision-based;video camera
object boundary;object tracking;kullback-leibler;tracking algorithm;region features;video sequences;object tracking
multi-modal;background model;posterior probability;particle filter
tracking methods;shape matching;support vector regression;surveillance systems;kernel-based;video-based;detection results;target detection;target detection;unified framework;video-based;false alarms;confidence level;learning scheme;video sequences;target detection;threshold values
matching score;face recognition;recognition rate;database;recognition performance;multi-view;human face;data set;high resolution;human face
recognition performance;recognition systems
pose variations;training images;test set;personal identification;similarity scores
face recognition;facial images;classification;database;test image;information contained in;face recognition;pose variations;bayesian framework;human face
face images;face recognition;facial images;data record;database;face recognition;database;illumination conditions;image data;data fusion;face database
high-dimensional;state space;particle filters;multi-aspect;markov process;particle filters;tracking performance;image sequences;target detection;tracking problem;observation model;target tracking;target detection;monte carlo
mutual information;training phase;feature-level;image pair;feature representation;object segmentation;level fusion;relevant information;mutual information;object segmentation;object shape
classification;vector machine;representation scheme;machine learning techniques;real world;support vector machines;classification task
target detection
clustering;deformable objects;moving object;adaptive clustering;moving objects;multiple object tracking
multiple view;data association;fully automatic
classification performance;classification;fusion strategies;information sources;face detection;multiple view;classifier;information fusion;error rate;ensemble approach
recognition performance;distinguishing features;multi-modal;personal identification
input parameters;classifier;classifier
regularization;vector space;statistical model
synthetic data;shape variations;shape features;medical imaging;shape analysis;shape analysis;wavelet transformation;wavelet transformation
web-based;multiple users;tracking algorithm;concept map;problem solving
automatically detects;training images;manual labeling
feature points;sign language;video sequences;missing data;motion patterns;simple algorithm
low-dimensional space;activity recognition;human motion;pose estimation;particle filter;activity recognition
decision trees;test image;single image;scoring function;single image;motion information
ground truth;manifold structure;linear space;manifold learning;growing importance;statistical analysis;manifold structure of;manifold structure;high dimensionality;hypothesis testing
false positives;high level
spatial distribution;magnetic resonance imaging;fmri data
classification;classification;large number of;prior knowledge;filtering step;spatial distribution;shape models;spatially varying;shape model
image quality;real-time tracking;level-set;visual-tracking;imaging conditions
probabilistic framework;particle filters;visual cues;tracking results;particle filters;visual cues;database
distance measure;information content;classification problem;kernel-based;database;human actions;human action recognition;sparse representation;salient points;image sequences;salient points;specifically tailored
accuracies;high accuracy;image data
supervised methods;linear subspace;linear subspace;unsupervised learning
appearance-based;multiple cameras;multi-view;pose estimation;multiple views;pose estimation;multi-view;appearance-based;exemplar-based;single-view
video segmentation;sign language;instance;vision problems;hand tracking;video sequences
human movement;high degree of;human-centered;human movement;accurate tracking;hand tracking;vision-based;vision-based
magnetic resonance;causal relationships
numerical experiments;magnetic resonance images;multi-resolution
image registration;basis functions;inverse consistency;test images;4;image registration
automatically determines;desirable property;left ventricle;deformable model;image sequences;deformable model
test images;finite element model;registration algorithm;great potential
high-dimensional;regularization;brain images;wavelet-based;statistical model;prior distribution;shape models;wavelet-based;pca-based
face recognition;training data;active learning;input image;classifier trained on;face model;face detector;face detection;human intervention;face database
real world situations;highly interactive;propagation algorithm;region-based;graphical model;missing data;partial occlusion;belief propagation
maximum likelihood;poor performance;classification;sign language;linear regression;recognition accuracy;recognition systems;vision-based;sign language
computational complexity;video surveillance;video sequence;video surveillance;preserving privacy;simulation results
face image;facial images;automated methods;ad-hoc;video data;large-scale;algorithm combines;data sets;video sequences;data utility
intra-class;random projections;fixed length;feature vector;random subspaces
image registration;image alignment
objective functions;information theoretic
mr images;statistical techniques;shape analysis;canonical correlation analysis
linear model
medical image;energy minimization;graph cuts;energy functional;medical images;segmentation algorithm
match scores;match scores;face recognition system;security issues;privacy issues
public domain;automatically extracted from;user privacy;point cloud;database
dynamic events
generic model;video sequence;high quality;modeling tool;object class;images captured;video-based;pose estimation;high accuracy;single image;video-based
low-cost;image acquisition;images captured;image acquisition
em) algorithm;training data;level sets;linear combination;probability density function;signed distance;level set;shape information;data sets;shape priors;medical images;density function;em algorithm;level set;expectation-maximization
conditional probability;unlabeled data;training phase;gaussian process;manually labeled;fast algorithm;brain images;labeled data;brain images;semi-supervised;graph regularization;semi-automated
em) algorithm;markov random field;automatic segmentation;local regions;spatial information;classification methods;markov random field;brain images;probability distribution;segmentation method;brain images;evaluation criteria;expectation maximization

virtual camera;expert knowledge;video streams
video sequence;matching algorithm;multi-view;moving object;visual representation;highly compressed;video sequences;dynamic scenes
natural scenes;image space;sampling method;sampling method;video-rate;image-sensor;multi-layer;visual perception
multi-view;direct comparison;computationally expensive;light sources
information systems;rates;video frame;visual information;content providers;information technology
mutual information;information-theoretic;natural image;gaussian noise;mutual information between;natural images
ct images;local minima;user input;user supplied;fast computation
magnetic resonance;computational efficiency;image sequences;free form
operator;real-world;image analysis
mri data;medical imaging;data sets;step procedure;positive definite;tensor product;approximation algorithm;mri data
geometric relationship between;17, 18;shape features;instances;shape models;modeling approach;closed form solution;magnetic resonance imaging
training set;information theory;natural images;natural images
human subjects;segmentation algorithms
global consistency
variational approach;shape similarity;level sets;level-set
texture segmentation;texture analysis
distance measure;image segmentation;graph cuts;graph cuts;segmentation results;algorithm takes
vision applications;spectral graph;energy minimization;optical flow;interactive segmentation;image restoration
information systems;spatio-temporal;mobile devices;data stream processing;query processing in;sensor databases;rfid technology;data warehousing;real world;data management
query optimization;multi-query optimization;continuous query;plan generation;query optimization
data values;input data;data locality;operator;heterogeneous network;implementation issues;join operator
indexing techniques;multi-dimensional;range queries;network monitoring;network monitoring
domain knowledge;domain specific;data warehouses;data cleaning;information-theoretic;anomaly detection;change detection;data stream;network traffic
massively distributed;large-scale;evaluation strategies;query processing in;structured data;data management;nearest neighbor search
small world;desired properties;small-world;7;uniformly distributed
gene ontology;1;3;2;search algorithm;protein interaction networks
false positives;decision trees;prediction performance;error rates;decision trees;bayesian networks;gene ontology;high-throughput;bayesian networks
query expansion;query expansion;test collection
automatic extraction of;training examples;negative examples;machine learning;fully automatic;databases;automatic extraction of;positive examples;benchmark data sets;related words
medical records;medical records;feature extraction method;free-text;domain ontology;semi-structured;structured information;high accuracy;semi-structured;information extraction system
classification;database;manually labeled;svm) classifiers;vector machine;combined method;multi-document summarization
database;data acquisition;sensor networks;sensor nodes;temporal data;vast amounts of
data collections;data sets;sensor data;physical location;network data;data storage;sensor data
spatio-temporal;power supply;analytical models;energy efficient;query processing in;sensor networks;sensor network;sensor networks;energy-efficient;spatio-temporal queries;sensor networks
security concerns;document structure;electronic documents;storage model;xml documents;service providers;data privacy;query processing;document content;query results;processing queries
individual privacy;privacy concerns;personal information;privacy policies;privacy policies
internet users;privacy concerns;privacy issues;personal information
vector space model;similarity measure
basic operations;preserving privacy;basic operations;special attention;database;outsourced databases;application domains;security issues;tree structure;privacy-preserving;modern database;search trees;basic operations;search trees;search trees;security issues
data encryption;privacy requirements;personal data;data owners
bayesian network;parameter learning;vertically partitioned;privacy-preserving;database;data mining applications;privacy concerns;bayesian network;distributed data;vertically partitioned data;bayesian network structure;privacy-preserving;databases;data mining problem;mining algorithm;vertically partitioned data;privacy-preserving data mining;learning bayesian networks
privacy-aware;error-prone;high level;role-based access control;access control;large enterprises;change detection;automatically) generated;control flow;event- condition-action;object oriented;diverse domains
information systems;service oriented
privacy-aware;fine-grained;ontology-based;data items
classification;stereo vision;stereo vision;vision based;video sequences;classifier
image processing;image sensor
long distance;sensor fusion
tracking algorithms;markov random field model;traffic flow;statistics based;spatio-temporal;traffic monitoring;traffic flow;tracking algorithm;mrf model

blind source separation;word recognition;frequency domain;fast-convergence;independent component analysis;rates;blind source separation
speech recognition;word recognition;noise estimation;estimation error;error rate;estimation method
clustering;speech data;data clustering;data driven
speech recognition;data collection;word recognition;speech recognition;database;speech data;data collected;working group
recognition accuracy;linear regression
case study;database;high-availability;database
service oriented;large scale;data store;database architecture;xml database;service discovery
database systems;data integration systems;data integration;data transmission;data integration;plans
relational databases;distributed file system;relational database systems;distributed file system;database administrators;storage requirements;disk space;storage structures
maintenance cost;giving rise to;data-intensive applications;storage management;storage management;information technology
index tuning;database management systems;database architecture;agent architecture;software agents;index tuning
indexing schemes;indexing schemes;ad-hoc queries;database tuning;dynamic environments;index structure;query processing;index tuning
database administrator;existing indexes;database operations;management systems;database;xml data;maintenance costs;xml queries;xml databases;instance;xml database;xml indexing;schema information;xml indexes
database systems;database;database tuning;database tuning;resource selection;database applications;database administrators;commercial database
end-users;sql queries;database design;data mining problems;database;databases;database administrators;database schema;huge number of
data updates;database
frequent itemset;data mining technique;data mining techniques;item set;apriori-based;frequent itemsets;transaction database;association rule;association rule;information technology
search space;database;pattern mining;mining process;sequential pattern;data set;algorithm called;frequent sequential patterns;sequential pattern mining;sequential pattern;large database
sequential pattern mining;sequential patterns;optimization problem;algorithms for computing;sales data;sequential pattern mining;sequential pattern
data objects;business data;data sets;data sets;detect outliers;data mining;detecting outliers
clustering;web documents;clustering approach
data objects;10;clustering results;feature extraction method;feature extraction;clustering method;input parameters;high-dimensional data sets;clustering methods;user-friendly;clustering
ontology based;content management;metadata management
search results;web search;classification;tree" based;web searches;query keywords;personal information
xml databases;xml documents;update operations;regular tree;ordered trees
extensible markup language;access control;xml elements;security level;access control
schema matching;database schemas;schema matching;data transformations;schema matching
digital images;fourier transform;frequency domain
automatic extraction of;color information;color information;time-series;video streams;video streams
relevance feedback;video data;video retrieval;information retrieval
generation algorithm;database;database schema;xml documents;database;xml document;relational database
semantic query optimization;semantic knowledge;optimization strategy;semantic query optimization;xml databases;transformation rules;control strategy;databases
xml data;cache management;tree-structured;xml databases;cache management;relational databases;xml query language
replication;query processing;replication
search results;simulation studies;query processing;query processing
search algorithms;sequential data;search algorithm;search algorithm;database
clustering;continuous query;optimization method;optimization method;multiple continuous queries;query plans;query processing;data streams;continuous queries;multiple continuous queries;processing queries
load balancing;amino acid;high speed;frequent patterns;load balancing
compared with conventional;relational table;relational tables;relational tables
information systems;mobile environment;mobile devices;information retrieval;user interface;location information;location-based services;location-based;information retrieval;user's location
information service;location-based service;mobile environment;recommendation service
content management
service provider;location-based services;highly accurate;location privacy;location-based service;trajectory data;position data;location-based services;location privacy;personal information
geographic information;event-driven;spatio-temporal;data model;real world;event-based
graph-based;navigation systems;application scenarios;graph-based

relational database;geographical information;supervision;database
social networking;social network;mobile users;network applications
web crawling;simulation study;cultural heritage;web crawling;data acquisition
web pages
web-graph;link information;web servers;link information;information retrieval

micro-array data;pattern-based;recommendation systems;pattern-based clustering;micro-array data;order-preserving;significant patterns;subspace clustering;clustering methods;clustering
1;sequence alignment;database;19;service-oriented;distributed data;grid computing;search tool;service-oriented;data-intensive;life sciences
expression patterns;inference problem;large-scale;genetic algorithm;large number of;large-scale;time-series;evolutionary algorithm
protein-protein interaction network;protein interaction networks;protein interaction networks
evaluation criterion;computationally hard;link structure;web communities;3;tree construction
link structure;web page;web search;result set

web information retrieval;information retrieval systems
retrieval method;search engine
clustering;clustering;dna sequences;sequence data;efficient retrieval of;dna sequence
relational data sources;structural heterogeneity;information sharing;large-scale;ad-hoc;mapping problem;structured data sources;dynamic environment;autonomous data sources;mapping problem;main components
efficiently identify;false positive
specially designed;clinical data;database;clinical information;web server;mobile devices;data integration;clinical information;health information
image retrieval;user interface;conceptual model;database
large-scale;database design;storage structure;database;data sources;analysis tasks
life sciences;human genome;information sources;heterogeneous data;information integration;heterogeneous databases;data description;databases;grid-based;data management;information integration
data placement;frequency distribution
replication;load distribution;storage systems
efficient parallel;detection algorithms;complex objects
mobile user;nearest;data accesses;rule set;multiple sets of;data migration;data migration;mobile users
efficient parallel;detection algorithms;complex objects
modeling method;multi-tier;instances;main idea;high-level;area network
performance degradation;pre-processing;access method;low cost
indexing structures;index structure;data set;indexing structure;line segments;perform poorly;biomedical applications;indexing methods
instance;human body;query operations;database;human motion;database application;high resolution;human motion;modeling method
data streams
statistical queries;multimedia applications;database;search paradigm;index structure;copy detection;similarity search;retrieval systems;indexing structure;similarity search;content-based video;similar features;large databases;performance gains;range queries
spam detection;image processing;image processing;image data
storage management;control method
relational databases;business rules;query processing engine;natural languages
access methods;database;path expressions;index structures;query processors;twig queries;tightly integrated with;query processor;unified framework;index structures
stream processor;1;multi-sensor;data streams;sampling rate;optimization techniques;multi-sensor;resource usage
multiple cameras;content analysis;audio-visual
clustering technique;vector space;high dimensional;tree-based;vector spaces;metric space;sequential scan;times faster than;high-dimensional;answer set;disk accesses;nearest neighbor queries;linear scan;similarity searches;metric spaces;access structures;real data sets;similarity searches;distance function;data set;query processing;random accesses
query execution;query processing;vice versa;intermediate results;similarity retrieval;retrieval systems;data set;real life;quality guarantees;local descriptors;data chunks;result quality;content-based image retrieval
image retrieval;multi-level;feature vector;multi-level;pre-defined;data structure;feature vectors;content-based image retrieval
join algorithm;structural joins;encoding schemes;path expressions;xml data;xml queries;structural joins;xml documents;structural relationships;filtering techniques
query evaluation;fine-grained;access control mechanism;access controls;xml data;access control;query evaluation;fine-grained;xml database;query processor;access control;structural information
web pages;user specifies;change detection;large-scale;general-purpose;web pages;instances;distributed environments;web page
query evaluation;query engines;cost-based;ad-hoc;optimization approach;scalability problems;query processing;query plans;query optimizations;query engine
image retrieval;relevance feedback;mathematical formulation;vector machine;image content;learning tasks;user feedback;relevance feedback;relevance feedback;retrieval performance;low-level;long-term;user feedback;content-based image retrieval;visual features;content-based image retrieval
image retrieval;image retrieval;sequential scan;image sets;content-based image retrieval systems;indexing method;logic-based;poor quality;human face
image retrieval;image description;user experience;taking into account;image collection
image retrieval;multi-level;image features;multimedia applications;classification;classification accuracy;classification task;clustering method;classification rules;low-level features;real life;association rules;classification method;association rules;image classification;image classification;mining algorithm;focus primarily on;low-level
web pages;classification;content filtering;predictive model;association rules;web page
databases;information systems;information theory;sensor networks;data management;databases;distributed systems
query operators;query execution engine;xml indexes;xml processing;database;xml data;access paths;xml documents;query processing;query plans;query engine;programming interface
xml schemas;heterogeneous data sources;schema matching;application domains;xml documents;xml elements;databases;quantitative analysis;additional information
join algorithm;nested queries;storage schemes;xml query;require expensive;reconstruction algorithms;xml documents;xml view;reconstruction algorithm;multiple relations;relational databases;query result;stack-based
database;query language;data model;formal model;data type;xml schema
entity relationship;relational databases;meta-data;2;databases;relational databases;user-friendly;xml schema
indexing schemes;multi-dimensional;main memory;nearest neighbor;international conference on;database;acm sigmod;databases;similarity retrieval;international workshop on;multimedia retrieval;data sets;multimedia databases;database;large amounts of data;databases;nearest-neighbor;storage management;query points;database researchers;vice-versa
structured data;information management;unstructured information
enterprise applications;main-memory;master data;enterprise application;service-oriented;data management techniques;data streams;data management
fundamental problem;stored data;multiple streams;data streams;query results;join queries;join operator
clustering;continuous query processing;large numbers of;high-quality;selection queries;large number of;data structures and algorithms;join queries;continuous queries
data objects;handle arbitrary;nearest neighbor;highly dynamic;shortest path;shared execution;road networks;distance metric;query results;query sets;nearest neighbors
mapping composition;relational operators;user-defined;mapping composition
schema mapping;target instance;data exchange;schema mappings;data sources;information integration;automatic generation of;schema mappings;programming paradigm
concise representation;schema mapping;schema mappings are;data exchange;source schema;real datasets;algorithm produces;algorithms for computing;high-level;schema mappings;data integration
approximation algorithms;total number of;data store;query performance;instance;native xml
data redundancies;data structure;storage cost;schema design;database;xml data;refinement process;data manipulation;partition based;xml databases;functional dependencies;real life;data transfer;functional dependencies;efficient discovery of;benchmark datasets;stored information
target class;xml schemas;regular expressions;web service;xml data;learning algorithm;incremental computation;regular expression;instance;data sets;xml-documents;document type
sensitive values;data analysis;average error;sensitive data
large databases;queries posed;sensitive data;statistical databases;inference techniques
data cleaning;rfid data;window size;sliding-window;data streams
correct answers;query processing capabilities;radio frequency identification;rfid-based;rfid data;provide answers;data set;data analytics
multi-dimensional;resource-constrained;outlier detection;data distributions;memory requirements;real data;streaming data;density-based;sensor networks;sensor data;sensor networks;complex applications
synthetic data;database;query relaxation;selection queries;query answer;queries involving
query evaluation;xpath queries over;xpath queries;boolean functions;query processing;structural information;network traffic;maintenance algorithm;distributed systems
large numbers of;distributed data sources;database;data store;computing environments;consistency constraints;instances;distributed sources;query results;monitoring systems
score function;data accesses;sufficient condition for;ranking function;indexing problem;high complexity;worst case;score functions;ranked queries;indexing methods
hierarchical data;edit operations;index structures;tree patterns;real data;large sets of;incremental maintenance
xml databases;graph theory;feature vector;twig queries;structural properties;feature-based;xml documents;twig pattern;xpath query;xpath queries over;real data sets;evaluation techniques;pruning power;indexing technique
multiple queries;xml data;main-memory;query-engines;takes into account;optimization techniques
4, 16;encoding scheme;twig queries;query results;intermediate results;tree-pattern queries;post-processing;xml documents;data sets;query processing;twig query;tree-pattern;duplicate elimination;query processing techniques;pattern matching;xml query processing;8
theoretical framework;xml fragments;large number of;query semantics;keyword-based;xml document;xml documents;query processing cost;efficient retrieval of;real-world;keyword search;query result
matching techniques;attribute-level;schema evolution;data exchange;schema matching;matching process;data transformations;selection conditions;schema mapping
database systems;database;user effort;desirable properties;automatically generate;schema elements
data values;training set;database;multi-column;schema translation;instances;real-world and synthetic datasets;iterative algorithm;databases
business process;query language;business process;business processes;real life;web services;high cost;formal model
query optimization;service calls;web services;query optimization;sharing data;optimal plan;execution plan;general-purpose;web service;select-project-join;data "chunks;web services;web service;np-hard
systems require;graph-structured data;personal information management;file content;semi-structured;data model;data model;data model for;structural information;3;personal information management;data streams;highly heterogeneous;20;structured data;relational data;16, 14, 47
real-world and synthetic datasets;storage schemes;data cube;space utilization;real-world applications;data cubes;very large datasets;great promise
database;data model;fact table;imprecise data;large datasets;aggregation queries
memory footprint;data model;language called;data records;fundamental problem;multidimensional space;aggregation functions
web pages;web search;multiple sites;web mining;computationally expensive;link graph;web search engines
data objects;clustering;linkage-based;efficient clustering;pairwise similarity;multi-typed;high efficiency;hierarchical structure;multi-granularity;clustering methods;power law distribution;relational database;semantic information;high cost
spam detection;web pages;web graph;instances;link-based ranking;search engines
query answering;performance tradeoffs;ad hoc;general form;synthetic data sets
multi-dimensional;query results;data cube;sql server;ranking function;user's preference;data blocks;query processing;ranking functions;computational model;ad hoc;rank-aware;data access
cost model;server logs;applications ranging from;upper bounds;large margin;disk accesses;ranked retrieval;sensor data;random accesses;performance gains;building block for;index scans;data integration;query processing
database systems;database;query workload;data representation;intensive workloads;performance tradeoffs;analytical model;databases;column-oriented;database workloads;query engine
physical design;upper bounds;lower bound;database systems
scheduling policies;multi-stream;unlike standard;multiple continuous queries;real data;data stream management systems;query plans;continuous queries
high-dimensional;function approximation;high dimensional;real-world;index structures;large number of;index structure;computational resources;mathematical models
black box;continuous query processing;query processors;user-defined;query operations
clustering;filtering systems;state representation;xml filtering;path expression;streaming xml data;instances;memory requirement
query answering;tree pattern queries;information integration;query optimization
materialized view;incremental maintenance;base data;view definition;view maintenance
query optimization;sample size;sample-size;sampling algorithm;database;bounded-size;incrementally maintaining;data items;random sample;information integration;base data;sampling scheme;data-mining tasks
mining task;total number of;classification;sampling algorithm;theoretical properties;data stream;sampling process;sample points;storage requirements;data mining problems
resource sharing;similar queries;large numbers of;multi-query optimization;data stream management system;query workload;multiple continuous queries;diverse range of;fine-grained;window sizes;monitoring systems;window-based;data streams;high volume;memory consumption;join queries;join operator;stream queries;continuous queries
data objects;synthetic data sets;search algorithms;nearest neighbor;answer set;similarity search;information retrieval;multimedia retrieval;similarity search;disk based;data mining;similar objects
search space;pruning technique;intermediate result;exact answer;spatial region;lower-bound;query returns;exact answers;spatial databases
local regions;accurately predict;great potential;predictive model;massive datasets;wide range;large datasets;cost-effective;machine learning algorithms
business information;database;text documents;text document;data sources;structured information;high accuracy;knowledge management;unlike prior;structured data
web databases;concept hierarchies;user interfaces;query interface;query interfaces;semantic web;html tables;multiple sources;access information;inference rules
query optimization;very large datasets;high quality;large datasets;brute-force approach;anomaly detection;query formulation;database;memory requirements;xml documents;real-world and synthetic datasets;data management;fundamental problem;real-world;relational data;data integration
data warehouse;data redundancy;data replication;distributed database;high availability;replicated data
database systems;database;database replication;isolation level;database servers;replication
large numbers of;completeness;data stored;replication;data availability;large scale;large-scale;network management;data sets;highly distributed;hash table;real-world;explicit feedback;completeness
web applications;standard sql;query processing;join operator;efficiently computing
data point;real-world datasets;skyline query;skyline queries;data points;tree-based;query point;spatial attributes;skyline queries;margin;query points
privacy-aware;nearest;high quality;spatial regions;database server;location information;query processing;location-based services;query processor;location-based;privacy requirements;main components
stream processing systems;distributed stream processing;query operators;operator;computing devices;rates;operator;network traffic;short-term
decision-making;data loss;high rate;data stream applications;lower level;desired level of;data stream applications;query processing;data processing;databases;data stream systems;query results;computational overhead;data management;data stream management systems
unlike earlier;window size;query plan;query answers;rates;data stream management systems;data streams;operator;aggregation queries over;sliding windows;query results;result quality
frequent closed;mining algorithms;mining frequent closed;synthetic datasets
materialized views;incremental maintenance;data cubes;data cube;data warehouses
multi-dimensional;data cube;production line;low support;rfid data;supply chain;individual objects;higher level;abstraction level;abstraction levels
query execution;encoding scheme;false positive;compression techniques;query response times;query processing;run-length;commercial databases;approximate results
efficient querying;compressed data;compression ratio;sort order;coding scheme
compact representation;estimation accuracy;monitoring data;real-world;error metric;monitoring applications;instances;higher accuracy
distance measure;similarity measure;distance measures;partial occlusion;real world;longest common subsequence
distance computation;query workloads;data distributions;knn queries;network topology;network nodes;parameter settings;road network;euclidean space;road networks;databases;mathematical analysis;spatial network;query processing
databases;similarity measure;main memory;reference-based;edit distance;database;similarity search;sequence databases;reference-based;sequence database;distance computations;long sequences
synthetic data sets;data cleaning;performance guarantees;set-similarity join;real-life;operator;similarity joins;correct answer;high similarity
dynamic programming algorithm;dynamic programming;search space
query optimization;classification problem;database queries;conjunctive queries;data model;meta-level;semantic web;schema information;integration systems;query containment
data lineage;input data;database;uncertain data;databases;relational operations;probabilistic databases;query processing in;base data;query processing;uncertain data;relational databases;query results;data management
real-life applications;algorithm exploits;probability distribution over;information extraction;probabilistic databases;imprecise data;databases;statistical models
data analysis;workflow management;data quality;semi-automatically;personal data;data processing;processing requirements
search results;automatic extraction of;dynamically generated;result page;user query;search result;search engines;search engine;automatically generate;web crawling;result pages
keyword queries;storage devices;index structure;keyword-based;query performance;search queries;range queries;search engine;keyword search;extensive simulations
redundant information;query pattern;special properties;valuable" information;query processing;low overhead;theoretical analysis;multiple queries;query results;stored information
query optimization;search space;optimizer;cost-based;cost-based;relational database system
relational data sources;data services;aqualogic data services platform;service-oriented;query processing in;data sources;query processing;query processing techniques;query processing in;xml schema;data services
decision support system;decision support;decision support;transaction processing
test results;large volumes of;association rule mining;index structures;data mining;business intelligence;distributed architecture;analytical processing;data mining;business data;test results;large repositories of;structured data;online analytical processing;online mining
lock;document processing;update transactions
control theory;memory management;cost-benefit analysis;fast convergence;database
high level;large-scale;real-world;model generation;case study;production data;natural language processing;mining algorithm;distributed systems
user experience;wireless communication;high-quality
home network;high-end;home network;home network;individual users
low power;massive data;information processing;low-power;random access;high density;high speed
declarative query language;sql/xml;database;structural information;execution model;query processing techniques;xml documents;xml db;rule based;xpath processing;relational database system
sql/xml;xml indexes;xml queries;querying xml data;database vendors
management systems;workflow management systems;content management
information management;information technology;database
keyword queries;high quality;database;databases;user interface;wide range;relational database;information retrieval techniques;keyword search over;user-friendly;query processing
optimal plan;semi-structured data;access control;xml data;access control;view selection;control strategies;control strategy;specification language
complex data;retrieval engine;similarity queries;relational dbms;key features
prohibitively expensive;approximate query answering;interactive exploration;database;large data sets;approximate query answering;large databases;query answers;xml databases;query processor;semi-structured;cost-effective;approximate results
visualization techniques;web image;database;web image;large-scale
false positives;user interface;candidate matches;schema matching;matching systems
query execution;domain ontology;multiple data sources;semantic web;information integration;low level;retrieve information;query engine;query processing
accessing data;query optimization;optimization technique;features including;relational queries;database;federated queries;relational data sources;federated queries;data sources;4;query plan;query processing
data-driven;schema mapping;target instance;data exchange;source schema;schema elements;human input;high-level;schema mappings;schema mapping
rdf data;systems support;data exploration;context-aware;scientific applications;instances;databases;query languages
domain knowledge;data structure;image databases;pre-computation;statistical analysis;ontological knowledge;image annotation;user-friendly;visual features
density function;density estimation
large graphs;extraction algorithm;multi-resolution;visualization tools
sql queries;data sources;xml-based;sensor data;sensor network;sensor networks;network data;sensor networks;application development;graphical interface
multiple layers;network applications;database;sensor networks;sensor network;database systems
query optimization;optimization techniques;schema-based;xml streams;semantic query optimization;incremental maintenance;query optimization techniques;queries over xml;knowledge embedded in;sensor networks;query optimizations;xml stream;online transaction processing
image retrieval;image retrieval;high dimensional;image database;relevance feedback;distance measures;similarity search;similar images;existing database
xml processing;xml applications;candidate-set;indexing scheme;3;xml document
web content;content extraction
access control;xml queries;rewritten query;large number of;prohibitively expensive;answering queries;xml views;query engine;answer queries;query answering;data integration
query capabilities;data model;reconstruction algorithms;systems biology;path-based;data management system;data storage
xml view;view update;application scenarios
web-based;monitoring data;database;stream mining;fully operational;data center;mining tasks;time-series;input streams;user-friendly;monitoring systems
graph model;streaming applications;database schemas;data generation;test data
xml documents;xquery queries;xquery engines
database systems;database schema;database applications;great potential;global optimization;3;multi-tier;automatic generation of;code generation;database schemas
personal information management;digital camera;database technology;mobile phone
processing units;memory bandwidth;computational power;database;graphics processing units;data mining applications;general-purpose;data management;data volumes;query processing algorithms;processing power
data mining;gene expression data;large volumes of;stock market;databases;time series;time series;database;databases;time-series;information retrieval
sampling algorithms;data sets;massive data sets;randomized algorithms;large data sets
1
review process
20;xml schema;semi-automatic;logical level;xml databases;web information systems;conceptual model;xml db;development process;stored information;xml schema
ensemble learning;growing number of;training data;training data is;classification;classification accuracy;learning algorithm;application domains;data stream;sufficient training data;data streams;variance reduction;individual classifiers;application scenarios
heterogeneous systems;ontology mapping;semantic web applications;ontology mapping
information systems;database research;computational biology;databases
software development;information systems;specific domains;database applications;data warehouses;international workshop on;1;object-oriented;web applications;modeling language;lessons learned;modeling language
data set;data item;provenance information
information systems;international conference on;huge amounts of data;scientific applications;international workshop on;data engineering;workshop brought together;data flow;massive data sets;effective tools
acm sigmod;data streams;web services;data provenance;database
ground truth;schema matching;real-world domains;higher accuracy than;matching systems;matching accuracy
data structure;frequent pattern mining algorithms;performance bottlenecks;data locality;spatial locality;prefix tree;temporal locality;multi-threaded;frequent pattern mining
matching algorithm;materialized views;views defined;algorithm relies on;normal form;key constraints
database applications;load balancing;database
skyline algorithms;worst-case;design choices;skyline queries;relational databases;average-case
conjunctive predicates;distribution information;attribute values;query execution plans;optimizer;selectivity estimation;query optimization;db2 udb;query plans;valuable information;joint distribution;query execution times;plan quality;cost-based;maximum entropy;selectivity estimates;query optimizers;ad hoc;cardinality estimates
imprecise data;query semantics;aggregation queries over;data model
pair-wise;pair-wise;black-box;database;real-world;matching records;diverse sources;instance;information integration;computationally expensive;databases;additional information;information integration
1;3;2;power law;mathematical analysis
multiple users;database;knowledge management;information retrieval;homeland security;security applications;knowledge discovery
similarity query;query rewriting;complex queries;query execution plans;database size;relational database management systems;similarity predicates;similarity queries;complex data
spatio-temporal;upper bounds;realistic data;search problem;similarity search;video surveillance
pair-wise;nearest-neighbor search;amino acids;feature vector;wavelet decomposition;distance matrix;retrieval accuracy;query interface;database;spatial features;processing queries;data sets;times faster than;efficient retrieval of;memory utilization;structural similarity
speech recognition;dynamic programming;matching algorithm;tree structures;document retrieval;low level;fundamental problem;string similarity;times faster than
text summarization;knowledge based;statistical analysis
classification;diverse set of;regression models;sentiment analysis;real-world data sets;competitive performance
clustering;community-based;real-world;data collected from;web content;case studies;online content;query keywords;online communities;automatically discover;community structure
singular value decomposition;graph structure;synthetic data sets;web sites;data set;higher-order;common interests;count-based;trend analysis
average precision;search engines;average precision;poor performance;search effectiveness
xml retrieval;xml element;content-oriented xml retrieval;retrieval evaluation;statistical tests
result sets;taking into account;data collections;confidential information;information retrieval;case study;web-based;web search engines;web search;search interface;low-quality
trec data;relevance judgments;retrieval systems;6;evaluation measures;average precision
input tuples;join results;memory constraints;sliding window;data stream;approximation technique;exact computation;data streams;join result;result set;input streams;online algorithms;objective function
clustering;high-dimensional;temporal locality;data stream model;low-dimensional space;input space;streaming data;detection approach;data stream clustering;traditional clustering algorithms;data streams;clustering algorithm;kernel methods;real-world datasets;kernel-based
accurate classifier;classification problem;naïve;classification;classification;classification accuracy;sensor readings;classifier;bayesian classifier;data streams;conventional methods;classification algorithm;sliding-window;join operations
erroneous data;biological databases;text-mining;biological sequences;databases
knowledge-discovery;total number of;15;candidate-generation;binary attributes;memory capacity;massive data;candidate pairs;real data;data set;candidate set;highly correlated;correlation coefficients;computational cost;algorithm produces;randomized algorithm;pruning power;massive data sets;correlation analysis
data structure;supply chain;data analysis;radio frequency identification;rfid) technology;rfid data;supply chain management;data sets;business processes;compression based;takes place;abstraction level
search results;statistical summaries;trade-offs;data items;search result quality;query routing;web data
pruning methods;document collection;text retrieval systems;language model;document-centric;large text collections;main memory;retrieval tasks;ad-hoc;pruning method;kullback-leibler divergence;retrieval effectiveness
ranking process;structural constraints;large volumes of data;query semantics;pruning strategies;retrieval systems;information retrieval systems;web retrieval
classification;kernel methods;feature vector;classification;similarity measure;large volumes of data;application domains;object classification;large datasets;benchmark datasets;string kernel;data management
multiple classes;query execution;classification problem;classification;document classification;large scale;search engine;classification accuracy;search-engine;document corpora;svm classifier;classifier
data mining technique;document classification;classification;textual features;rule set;link-based;digital library;multi-criteria
class information;web page classification;web page classification;classification accuracy;information retrieval;web mining;link graph;unlike prior;web page;web data
pattern-based;sentence level;named entities;detection approach;novelty detection;specific topics
pair-wise;clustering;social interactions;social network;transition matrix;social networks;markov chain
clustering;web pages;web ir;classification;costly process;classification methods;web mining;performance gains;web page;resource usage
automatically learns;machine translation;training set;domain-independent;natural language text;instances;semantic knowledge;classification rules;semantic categories;cross-lingual;semantic interpretation;question answering
similar queries;specific information;community-based;helping users;community members;knowledge based;personalized web search;search engine
web search;local context;specific information;natural language processing techniques;private information;user's interests;user query;user profiling;search engine;expansion terms;personal information
search systems;information-seeking;implicit feedback;relevance feedback;retrieval performance;search tasks;additional information
world wide web;commercial search engines;special case;multi-task;optimization method;information retrieval methods;search engine;retrieval functions;regularization;user queries;reproducing kernel hilbert space;risk minimization;retrieval function;learning problem;statistical models
sensor readings;tree based;storage scheme;ad-hoc queries;data-centric;data-centric;sensor networks;uniformly distributed;distributed algorithms;load balancing
euclidean distance between;data stream environment;synthetic datasets;multiple streams;knn queries;time series;high efficiency;user queries;similarity search;data stream management system;data streams;main memory;conventional methods;streaming environments;nearest neighbors;query processing
xml processing;event processing;applications - including;data stream mining;user-defined;data stream;wide range;data stream management systems;time-series;aggregate functions
clustering;distance measure;matching algorithms;euclidean distances;time series
clustering;hierarchical clustering algorithms;clustering algorithms;text documents;hierarchical clustering;text clustering;text document
clustering;unique features;clustering results;clustering structure;clustering validation;similarity measure;efficient clustering;real datasets;high quality;cluster validity;domain-specific;clustering framework;clustering algorithm;clustering;clustering algorithms;transactional data
domain-specific knowledge;web objects;multiple communities;high-quality;vertical search;intermediate results;web communities;ranking problem;detection algorithm;fusion methods;search engine;web object;user studies;web forums
search task;data fusion;expert search;retrieval performance;relevant expertise;document representation;trec 2005 enterprise
cold start;user profile;specific information;information retrieval;user profiling;data sets;modeling approach;explicit feedback;bayesian hierarchical;implicit feedback;user studies
access methods;skyline queries;skyline queries
high-dimensional;skyline queries;skyline computation;pruning strategies;performance degradation;skyline queries;adaptive strategy;low-dimensional
completeness;database systems;approximate query answering;rank-aware;exact answers;query operators;peer data management systems;query processing;query processing techniques;distributed environments;distributed data
operator;structural properties;business intelligence;large graph;massive graphs
tree structures;multiple data sources;heuristic approaches;sufficient conditions for;database;tree pattern;query language;tree-pattern queries;tree-structured;information extracted from;data sources;high complexity;data management techniques;query languages;querying capabilities
candidate patterns;mining algorithms;mining frequent itemsets;web usage mining;xml processing;tree mining;real datasets;database;data structures;wide range;frequent subtrees;data mining;rates
text mining;human subjects;hand-crafted;instances;semantic web;real-world;diverse applications
text mining;clustering;document collection;document classification;large scale;locality-sensitive hashing;feature extraction;spectral analysis;management systems;higher-order;latent semantic indexing;retrieval techniques;similarity measures
instance;concept space;related concepts;knowledge discovery;domain specific;retrieval tasks;application domains;frequency information;similarity computation;relational database;additional information
query times;document level;search experience;search applications;document level;implementation details;query processing;result set;key parameters
relational dbms;disk access;aggregate functions;standard sql;statistical analysis;user-defined functions;memory management;machine learning and data mining;programming interface
knowledge management;source documents
knowledge management;case study;knowledge-intensive;document classification;process models
regularization term;regularization;data points;classification;linear discriminant analysis;linear discriminant analysis;high-dimensional datasets;cross-validation;candidate set;small sample size;regularization parameter;support vector machines;dimension reduction;model selection;overfitting problem
search results;domain experts;domain specific;general purpose;concept-based;large number of;health) information;domain specific;large databases;domain specific knowledge;information retrieval;domain-specific;computationally tractable;computationally expensive;concept-based;retrieved documents;takes into account;computational models;readability
parameter values;relevance propagation;discrimination power;multiple sets of;link structure;link structures;ranking method;propagation model;retrieval results;content information;test collections;link-based;ranking methods;wide range;link information
information retrieval models;query term;context models;term frequency;information retrieval
ranked list;query performance prediction;trec test collections;measure called;query performance
web databases;query result;database;ranking scores;user query;user's preferences;final ranking;domain independent;ranking method;query result;user feedback;query results
cost functions;directly optimize the;rank-based;scoring functions;cost function;ranking functions;training set;search heuristics
major search engines;query interface;trec collection;random sample;main idea;low-variance
text summarization;high-volume;text summarization;frequency distribution;data reduction
concise representation;search space;database instances;database;databases;conflict resolution;edit distance;distance measures;update operations;domain experts;scientific databases;efficiently computing;relational databases
query-specific;document summarization;semantic associations;query-independent;spanning trees;summarization method;query result;web search engines
view updates;source database;11, 14;views defined;data provenance;source databases;5;np-hard
clustering;base table;rewritten query;relational queries;vice versa;query optimization;query-evaluation;query optimization;optimization algorithms;materialized views;select-project-join;schema information;semantic information;optimization framework
query processing;block-level
training set;classification;active learning;training sets;text classification;classifier
document classification;language models;classifier;classification performance;high dimension;text classification;classification algorithms;document representation;automatically discover;classification algorithms
multi-level;feature extraction;web search;machine learning methods;maximum entropy model;web queries;click data;feature selection methods;naive bayes;linear svm;gradient boosting;query log;feature selection;error rate;vector machine;classification approaches;user click;navigational queries;machine learning approaches;search engine results
clustering;relevant documents;ranking list;learning algorithm;irrelevant documents;labeled documents;information retrieval;intrinsic structure;label propagation;semi-supervised;initial retrieval
data source;language models;textual information;clustering techniques;human activities;virtual worlds;language modeling
clustering;scientific domains;decision support;real data;user interests;information loss
clustering;multi-level;relational query processing;parameter values;block size;theoretical results
set valued attributes;database size;simple queries;stock market;inverted files;indexing scheme;set-valued data;set containment;set-valued attributes;data mining;set-valued attributes
data values;attribute values;moving-objects;database management systems;simple queries;uncertain data;probability density function;join processing;join algorithms;probabilistic threshold;nearest-neighbor queries;multiple relations;databases;biological databases;optimization techniques;join queries
frequent itemset;real world datasets;database;association rule mining;frequent itemsets;databases;association rule;integer programming;transactional data
privacy preserving;private information;distributed data mining;privacy concerns;distributed databases;sequential patterns;sequential patterns;privacy preserving;databases;real world applications;preserving privacy;sequential pattern mining;privacy preserving;multi-party
data structure;query string;database;similar results;approximate string
learning process;classification models;multiple sites;content features;selection methods;web page
clustering;clustering problem;web documents;data set;structure information;purity;document collections;clustering algorithm;traditional clustering
relevance feedback;initial query;xml retrieval;structural constraints;user feedback;relevance feedback;semi-structured documents;xml documents
classification accuracy;pattern-based;frequent patterns;text categorization;association patterns
holistic twig join;xml data;matching method;twig pattern
search space;nearest-neighbor;nearest neighbor queries;high recall;query result;metric spaces
text classification
multi-stage;markov chains;markov chains
keyword queries;database;text streams;keyword search;query keywords;keyword search on
subspace clustering
efficient similarity search;main features;time series;dimensionality reduction;high sensitivity
compact representation;frequent patterns
inherent uncertainty;retrieval models
translation model;parallel corpus;training process;cross-language information retrieval;parallel corpora;cross-language information retrieval
text mining;text summarization;data analysis;cluster ensemble;gene expression;high quality;text clustering;multi document summarization;expectation maximization;gene clusters;ensemble approach;gene expression
precision/recall;information retrieval
image retrieval;efficient search;brain images;image information;latent semantic indexing;ir methods
web directories;significant rules;probabilistic model;knowledge discovery in databases;matching method;association rule;textual documents
optimization techniques;data stream;data stream management systems
continuous queries over;hill-climbing;search space;streaming data;query operators;monitoring systems;input streams;stream queries;continuous queries
clustering;semantic similarity;time series;machine learning;search queries;query logs
search space;naïve;gene expression data;microarray data;data collected from;subspace clustering;expression levels;gene expressions;similar characteristics
multiple databases;nearest neighbor classification;nearest neighbor;privacy preserving;data mining tools;databases;information disclosure;classifier
term weights;domain-independent;mutual information;multi-task;text segmentation;text analysis;similar documents;source documents;text segmentation;multi-task
frequent sequential patterns;dynamic nature of;sequential pattern mining;sequential patterns;database
data sets;private data;wide range;application scenarios;private data
similarity measure;average precision;data collections;context based;relevance judgments;relevance feedback;retrieval model;initial retrieval;probability estimation
web objects;machine learning;search performance;vertical search;anchor text
search results;relevant pages;large-scale;web search engine;large number of;user queries;query logs;search intent;frequently occurring
web documents;search-result;web search;user's perspective;hierarchical structure;classification method;web search;training corpus
filtering systems;information organization;traditional information retrieval;named entities;link detection;link detection;topic detection;document representations;document representation
computational complexity;density estimation;kernel density;data streams;streaming data;statistical model;data mining;processing requirements;processing cost;building block;data distribution
clustering;detection algorithms;kalman filter;classification;structural properties;real datasets;time series;human motion;specialized algorithms;pre-processing;feature vectors;detection task
pre-defined;dynamically generated;search applications;statistical techniques;topic hierarchies;search result;mining algorithm;topic hierarchies
semi-automatically;instance;semi-automatic
hierarchical data;related data;large scale;time-series;interactive visualization;visual analysis
high-dimensional;databases;search process;indexing scheme;relevance feedback;retrieval performance;similarity-based;retrieval method
clustering;search results;query-specific;similarity scores;trec-7 test collections;context based;document clustering;retrieved documents;clustering;query-specific;confidence level
web pages;diverse set of;web spam;web spam;manually annotated;reference collection
faceted search;exploratory search;personal information management;information access
information retrieval;web searches;link-based;search engine results;search engine;content providers
web portals;information access;multi-document summarization;digital libraries;question-answering;cross-language information retrieval
geographic information retrieval;spatial indexing;relevance ranking;access methods;geo-spatial;query interfaces;search engines;geo-referenced;geographic information retrieval
sigir 2006 workshop on;sigir 2006 workshop on;xml element retrieval;xml element retrieval
text retrieval;text retrieval
search systems;sigir 2006 workshop on;decision-making
workshop report;technical program;open source;information retrieval systems;information retrieval workshop
acm symposium on;growing number of;information access;information access;information retrieval
domain-specific;information retrieval;retrieval tasks;information retrieval workshop
text summarization;large-scale;simple heuristics;document collection;information retrieval;relevance feedback;retrieval performance;retrieval model;graph based;semantic interpretation;vector space model;machine translation;retrieval effectiveness;text based;relevant documents;semantic knowledge;simple heuristics;ir) systems;natural language processing;knowledge representation;similarity measures;natural language text;specific information;semantic analysis;statistical model;document frequency;conceptual graphs;information retrieval;average precision;weighting schemes;retrieval models;final ranking;term frequency;search engine;query expansion technique;statistical models;question answering;query terms;higher level
high probability;real-world;moving objects;spatiotemporal data
selection methods;ensemble selection;ensemble selection
large corpora;latent semantic analysis;latent semantic analysis
search space;closed itemsets;pattern mining;mining process;pruning strategies;interesting patterns;frequent itemsets;random noise;end user;closed itemsets;large number of;frequent pattern mining
object recognition;object-recognition;potential function;real-world;memory requirements;inference algorithm;belief propagation;input image;belief propagation;synthetic data
spatial datasets;clustering algorithm;association rule mining;real-world;spatial datasets;spatial knowledge;case study;efficient discovery of;uniformly distributed;association rules;grid-based;spatial data mining
anomaly detection;large-scale;higher order;higher-order;data mining;supervised learning;higher order
data mining;large space of;database systems;data summarization;great promise for;entire process;data mining;machine learning;model construction;algorithm selection;exploratory mining;exploratory mining
data mining methods;high throughput;gene expression;data mining approaches;machine learning;gene expression;biological data;gene expression data
minimization problem;density-based;large-scale;outlier detection;document retrieval;database;information retrieval;information theoretic;knowledge discovery;database;databases;artificial data;objective function;information theoretic;distance-based
state space;markov chain;monte carlo;state space;time series;knowledge discovery;bayesian approach;modeling approach;poisson model;observation model;auto-regressive;modeling method;model parameters
web pages;synthetic data;web sites;machine learning;graph patterns;mining patterns;classification task
collaborative recommendation;collaborative recommendation;proximity measures;database;similarity matrices;similarity measure;von neumann;graph kernels;nearest;rule based;answer questions;random walks
ensemble classifier;classifier;classification performance;text classification;classifier;combination function;text classification;ensemble approach;individual classifiers
clustering;pair-wise;information theory;web image;heterogeneous data;information theory;heterogeneous data;multiple types of;high-order;visual features;low-level;data mining;real-world applications;web images
domain knowledge;feature space;itemset mining;feature vector;database;frequent subgraphs;probability distribution;prior probability;lower bounds;statistical modeling;closed frequent;statistical significance
automatically learns;classification model;classification;classification;association rule mining;rule-based;learning algorithm;classification systems;rule-learning;classification rules;learned model;text classification;uci datasets
utility function;prediction problem;classification;classification problem;hierarchical classification;hierarchical classification;hierarchical classification;search tree;expected utility;user behavior;information retrieval;decision-theoretic framework;class hierarchy;expected utility;topic hierarchy;search behavior
clustering;clustering;underlying structure;data mining and machine learning;high quality;clustering methods;cluster analysis;real datasets
ranking algorithm;overlapping clusters;highly variable;social network;data set;high precision and recall;clustering framework;pairwise similarity;overlapping communities
classification;heterogeneous sources;unlabeled examples;large scale;financial data;outlier detection;training classifiers;large scale
taking into account;real-world;predictive power;evaluation scheme;machine learning;data mining approach;feature selection;detection rates;classifier
clustering algorithms;multi-dimensional;taking into account;cluster analysis;medical data;time series;time-series;time-series;test results;cluster analysis;multidimensional space;structural similarity
training data is;training dataset;machine learning applications;ranking performance;artificial datasets;real-world data sets
closed itemsets;real-world;mining frequent;dimensional data
diverse applications;automatically created;visualization tools;operating systems;data mining
data objects;density-based clustering;interesting patterns;cluster quality;multi-instance;instances;data sets;multi-instance;feature vectors
implicit assumption
frequent itemsets;tree structure;frequent sets;tree structure;data stream;sensor networks;click streams;data streams
clustering;learning algorithms;schema mapping;similar object;record linkage;data mining tasks;similarity functions;automatically learning;similarity function;record linkage;large datasets;pairwise similarity;distance computations
graph mining;database;social network analysis;parallel processing;execution times;shared memory;real world datasets;problem statement;graph data;memory consumption;link analysis;increasingly popular
clustering;clustering algorithms;test problems;meta clustering;meta clustering;clustering criteria;case studies;squared error;automatically generating;clustering;diverse set of;supervised learning;meta clustering;clustering quality
naïve;candidate patterns;spatio-temporal;pattern mining;mining algorithm;spatio-temporal
relevance feedback;semantic video;high level;multimedia data mining;video data;spatio-temporal;learning algorithm;real-life;real-world applications;surveillance video;database;learning process;semantic video;surveillance video;low level;video content;video retrieval;temporal data;visual features;content-based image retrieval
data structure;suffix tree;frequent itemsets
clustering;clustering;natural language processing techniques;frequent itemset mining;clustering performance;class labels;automatically generate;readability
analysis tool
algorithm called
association mining;database;multi-tier;multi-tier;association rules;association rules
real-world;program committee;social network
auc;class-imbalance learning;class-imbalance learning
closed frequent itemsets;large number of;mining frequent itemsets;high accuracy;concise representations
probability estimates;auc;active learning;classification accuracy;active learning;machine learning;active learning algorithms;unlabeled set
completeness;database;data quality problems;soft clustering;data quality;data sets;quality measures;data quality;data management
clustering;distance measure;data mining methods;database;data mining approaches;user-defined
text mining;computational complexity;algorithm to compute;optimization problem
clustering algorithms;data mining methods;end-users;massive data;trade-offs;heuristic algorithms;data description;data mining;interpretability
misclassification costs;cost-sensitive learning;cost-sensitive;real-world applications;data sets;cost-sensitive learning;class distribution
search engine;similarity measurement;storage space;arima model;related queries;query logs;text queries;information retrieval;time series;moving average;arima) model;query logs;auto-regressive;arima model;dimensionality reduction;large database
complete information;randomized algorithm;probabilistic segmentation;probabilistic segmentation;segmentation quality;random walks
gene expression;gene expression
parameter tuning;frequent-itemset;mutual information;chi- square;interesting itemsets;closed itemsets;high quality;clustering method;clustering quality;frequent itemsets;clustering performance;document clustering;interestingness measures;document clustering;high dimensionality;interesting" itemsets;dimensionality reduction;closed frequent itemsets
parameter estimation;real datasets;representation scheme;scientific datasets;high accuracy;temporal evolution
data arrives;data structure;incremental algorithm;synthetic data sets;data types;data streams;sensor data streams;monitoring applications;sensor devices;rates;wavelet-based;data streams;periodic patterns;sliding windows;sliding windows
detection algorithms;posterior probabilities;probability estimates;outlier detection;score distributions;probability estimates;threshold selection;labeled examples
contextual information;data mining;collected data;customer behavior
clustering;clustering algorithms;data point;dense regions;relevant data;generative model;clustering algorithm;traditional clustering
bayesian information criterion;computational biology;sequence data;tree models;probability distribution;real-life;markov chains;tree models
database;database queries;large graph;large datasets;data mining;association rules;mining associations
information contained in;training data;text categorization
false positives;graph mining;intermediate result;large number of;sql query;textual similarity;contextual information;real world
state transition;highly correlated;efficient algorithms to;time series;time series
data analysis;data samples;instance;massive datasets;generative model;real images;class labels;multidimensional data
object based;generic model;labeled data is;structural constraints;object identification;labeled training data;similarity measure;object identification;similarity functions;attribute-based;real-life;object identification;structural information;pairwise constraints
clustering;pattern-based;relation extraction;large corpora;high-recall;extraction process;large number of;selection method;supervision;weakly supervised;instances;relation extraction;automatically identifies;high precision
em algorithm;em) algorithm;probabilistic models;data mining;data set;bayesian classifier;large data sets;data sets;missing data;data mining;em algorithm;semi-supervised;incomplete data;expectation-maximization
semantic networks;extraction algorithm;training sets;classifier;semantic network
training data;large number of;cross-domain;naïve bayes
weighting function;data mining results;data mining;data mining;data set;text classification;retrieval performance;text retrieval;pseudo-relevance feedback;data mining based;query terms
clustering algorithms;customer base;customer segmentation;finding an optimal;statistics-based;customer behavior;segmentation method;computationally tractable;distance-based;data mining;np-hard;combinatorial optimization problem;customer data;transactional data
ordinal data;web search results;prediction performance;dimension reduction;specifically designed to;dimension reduction
data set;cluster based;large-scale;sampling based;large data sets;kernel function;vector machine;pattern classification;hierarchical clustering algorithm;kernel based;real world data sets
concept-based;similarity measure;concept-based;text clustering;statistical analysis;clustering quality;text mining techniques;text clustering
ranking results;graph data;temporal information;web spam;web search engines;temporal information;classification model;temporal features;link spam;link spam;web data
numerical experiments;optimization problem;support vector;training samples;real-world data sets;data mining;support vector
clustering;concept called;domain-independent;missing information
feature space;support vector machines;collaborative filtering;feature selection method;unstructured text;real-world;reduction techniques;unstructured data;classification accuracy;classification accuracies;feature vectors;document corpora;text classification;feature spaces;classification approach;document representation
false positives;document collections;large-scale
anonymity-preserving;privacy-preserving;data mining results;frequent patterns;ad-hoc;share information;pattern mining;data-mining results;privacy issues;data mining;secure multi-party computation;privacy-preserving data mining;distributed environment
biological networks;structured domains;probabilistic model;learning algorithm;link prediction;data sets;biological network;network evolution;social networks;probabilistic model;link prediction;network evolution;network structures
incremental algorithm;query patterns;frequent patterns;real-life datasets;frequent updates;xml queries;mining frequent;incremental mining;xml databases;user queries;transaction database;query patterns;frequent sequences;xml queries;incremental updates
clustering;nonnegative matrix factorization;matrix factorization methods;matrix factorization;clustering
music information retrieval;music information retrieval;data set;information retrieval systems;clustering algorithm
real-world;high cost;data mining
high-dimensional;tensor analysis;multi-aspect;window-based;real datasets;data model;tensor analysis;environmental monitoring;data stream;window-based;multi-aspect;correlation analysis;multiple aspects
ct images;hybrid approach;medical applications;classification model;texture features;context-sensitive;obtained by applying;computed tomography;segmentation algorithm
pattern recognition;distance function;distance metrics;voting scheme;machine learning;ensemble classifier;classifier;nearest neighbor classifier;nearest-neighbor;exploratory data analysis;ensemble method;feature selection;feature vectors;distance metric;ensemble technique;distance metrics;benchmark data sets;data mining problems;tabu search;feature vector
information contained in
search space;frequent subgraph mining;candidate patterns;frequent subgraphs;frequent subgraph mining;mining algorithm;margin
stream mining;classification tasks;resource management;resource allocation;performance metric;binary classifiers;optimization problem;false alarm;classifier
pair-wise;classification;amino acids;wavelet decomposition;classification;distance matrix;object retrieval;support vector machines;database;feature vector;excellent performance;nearest-neighbor;wide range;biomedical applications;feature vectors;similarity measures
clustering;redundant features;graphical models;optimizer;conditional random field;conditional distributions;coarse-grained;log-linear models;large number of;information extraction;speed-ups;logistic regression;log-linear models;text classification;limited-memory;feature vectors
clustering;dimensional data;large data sets;projected clustering;relevant attributes;projected clustering;uniformly distributed;clustering algorithm;high dimensional spaces;low-dimensional;projected clusters
pruning strategy;search space;itemset mining;graph structure;database;memory usage;closed itemsets;frequency counting;low support;mining frequent;pruning strategies;mining algorithms;large databases;graph-based;real-world data sets;frequent closed
clustering;data objects;pruning methods;efficient clustering;probability density function;uncertain data;uncertain objects
clustering;nearest neighbor;outlier detection;algorithm (requires;real-life datasets;noisy data;concept based;clustering algorithm
entropy-based;synthetic data sets;context-awareness;sensory data;entropy measure;data streams;simple algorithm;real world;entropy based
simple algorithm;item set;similar items;weighted graph;intrinsic structure
computational complexity;classification model;support vector regression;classification;solution path;learning algorithms;vector machine;semi-supervised learning algorithms;piecewise linear;solution path;labeled examples;computational cost;support vector;regularization parameter;semi-supervised;manifold regularization;semi-supervised classification;manifold regularization
kernel regression;unlabeled data;training data is;data mining applications;regression methods;graph-based;semi-supervised learning algorithms;machine learning;unlabeled examples;kernel regression;semi-supervised;labeled examples;semi-supervised;algorithm named;semi-supervised classification
learned models;high-dimensionality;high-order;interesting patterns;time-series;motion capture data;hidden markov model;data mining;time-series;temporal structure
feature space;large data streams;real-world applications;temporal data mining;feature spaces;ensemble approach;temporal data mining
input parameters;data mining approach;web based;data mining approach;business logic
real datasets;time series;cross-correlation;time series;local patterns;wide range
collecting data;interaction networks;bayesian inference;model parameters;high degree of;communication networks
nearest-neighbor search;data point;data points;outlier detection;density based;reference-based;large datasets;very large datasets;theoretical analysis;highly scalable;nearest neighbors
occurrence frequency;svm classifiers;classification;anomaly detection;feature space;learning approaches;anomaly detection;anomaly-based;clustering algorithm;svm classifiers;feature spaces;text classification problems;high speed
classification;classification accuracy;ensemble methods;classification methods;bayesian classifiers;ensemble classification;multiple relations;local classifiers;relational data
hybrid approach;incomplete data;learning bayesian networks;em) algorithm;learning algorithms;data set;data sets;bayesian networks;bayesian networks;evolutionary algorithm;incomplete data;missing values;evolutionary algorithm;expectation-maximization
distance measures;regularization;distance functions;compare favorably with
text mining;text mining;pattern-based;text documents;keyword-based;discovered patterns;frequent sequential patterns;databases;text mining;helps users
brute-force;correlation coefficient;large databases;large databases;data sets;strongly correlated;correlation coefficients;upper bound;strongly correlated
clustering;clustering;shape space;similar objects;spanning trees;partitioning scheme;image collections;low dimensional;projection based
low rank matrix approximation;classification;cost effective;feature extraction;text classification;centroid based;text classification
partial orders;partial orders;binary data;partial order;total order;interesting orders;erroneous data
parameter space;em) algorithm;model-based clustering;mixture model;expectation- maximization;real datasets;expectation maximization;region based;multivariate data;data mining;algorithm takes;maximum likelihood;model-based clustering;mixture models;likelihood function
graph partitioning;clustering;graph theoretic;clustering documents;bipartite graph;clustering algorithm;clustering approach
clustering;clustering;matching algorithm;monte carlo;parameter estimation;clustering algorithms;information-theoretic;generative model for;topic models;overlapping clustering;clustering documents;hierarchical bayesian model;empirical bayes;efficient approximate;markov chain;inference techniques;latent dirichlet allocation
topic distribution;community mining;topic model;topic hierarchy;similarity-based;latent topics;blog data;cosine similarity
kernel learning;database;object recognition;approximate algorithm;kernel-based;training examples;kernel principal component analysis;feature extraction;selection method;unsupervised learning;kernel principal component analysis;selection methods;class labels;uci datasets;effectively learn
user profiling;web service;rule-based;user profiling;web services;rule-based;usage data
high-dimensional;feature space;black box;feature extraction method;classification;extracted features;data mining processes;feature extraction;extraction techniques;instance;classification accuracies;decision tree;remote sensing image;dimensional data;clustering
semantic smoothing;tf*idf;model-based clustering;language model;smoothing method;text retrieval;cluster quality;semantic smoothing;statistical translation;document clustering;similarity-based;document clustering;context-sensitive;class-specific;cluster models;clustering quality
data cleansing;base learners;classification;classification;information sources;1;3;classifier ensemble;base learners;2;supervised learning;classifier;classifier ensemble
clustering;clustering algorithms;distance function;hierarchical clustering algorithms;data bubbles;large data-sets;real-life;high accuracy;clustering
similarity measure;query-sensitive;similarity measure;query-sensitive;similarity measure;similarity measures;relevance feedback;content-based image retrieval;content-based image retrieval
data sets;peculiar data;peculiar data;data set;peculiar data;multiple categories;detect outliers
database;markov networks;databases;data mining process;markov logic;ad hoc;markov logic;probabilistic graphical models
boosting framework;regularization;numerical experiments;classification;regression problems;large-scale;generalization performance;gradient boosting;data sets;computational requirements;combining multiple;regression tasks
multiple classes;multi-class problems;classification;genetic algorithm;misclassification costs;class distribution;learning algorithms;multiple classes;cost-sensitive;classification performance;boosting algorithm;classification performances;imbalanced data;imbalanced data sets;class distribution;classifier;problem domain
dependency structure;large number of;binary data;binary data
real graphs;graph partitioning;storage cost;closely related;relevance score;weighted graph;random walk;pre-computation;large graphs;random walk;low-rank matrix approximation;disk-resident;community structure
nearest neighbor algorithm;classification;classification;classification accuracy;stream mining;nearest neighbor classifier;instance;computational resources;real world;diverse domains;class prediction
em algorithm;query language model;weighting model;document retrieval;prior distribution;pseudo relevance feedback;baseline methods;databases;em algorithm;external data;structured queries;learning framework;structured queries
greedy search;local) search;classification;minimum support;decision tree;associative classifiers;large number of;global search;test instance;associative classifier;associative classification;decision tree classifier;associative classification;error rate;counterpart
itemset mining;interesting itemsets;mining process;frequent itemset mining;linear transformations;aggregation operator;support threshold;itemset mining;mining patterns;candidate generation;aggregation functions;interestingness measures
clustering;time-series;high accuracy
classification;classification;svm-based;kernel functions;build models;vector-based;chemical compounds;ranked-retrieval
parameter tuning;regularization;data analysis;regression models;linear regression;automatic feature selection;parameter tuning;prediction error;regression model;linear regression;objective function
class information;temporal locality;classification;input data;data acquisition;original data;stream data;classifier
clustering;classification;locality-sensitive hashing;brute force;shape analysis;search algorithm;diverse domains
class information;lower dimensional;data points;learning approaches;data streams;linear discriminant analysis;data set;algorithm called;subspace learning;feature selection;principal component analysis;dimensional data;subspace learning;classification problems;highly scalable;information gain
high-dimensional;data point;upper bounds;nearest neighbor;genetic algorithm;efficiently computing;outlier mining;genetic algorithm;detection problem;fitness;real-life datasets;databases;dimensional data;optimal number of;sampling technique
cross-validation;auc;large-scale;probability estimation;large number of;highly accurate;skewed;class membership;algorithm selection;cost-sensitive learning;error rate;data mining problem;probability estimation;separability
prediction accuracy;high dimensional;irrelevant features;false alarm;data mining techniques;large number of;collected data;highly accurate;skewed;sample selection bias;quality control;skewed
sequential data;association rule;real world applications;medical research;association analysis
real data sets;global optimal solution;lower bound;clustering method;information retrieval;data sets;data mining;lower bound;high dimensionality;machine learning
kernel learning;kernel methods;kernel-based;computational savings;learning algorithms;large number of;applications requiring;perceptron algorithm;structured objects;storage requirements;support vector machines;structured data
pruning strategy;mining algorithm;database;memory usage;databases;association rules;memory consumption;mining algorithm;sequential pattern mining;sequential pattern
decision trees;time series;decision trees;input variables;classification problems;classifier;arise naturally in
mining task;case study;model called;mixture model;text documents
semantic smoothing;training data;semantic network;text classification;text classification;data sparseness
frequent sets;pattern mining;mining frequent;frequent set;prior knowledge
graph-structured data;mining algorithms;graph mining;pattern mining;real-world;increasing number of;pattern mining;time series;application domains;interesting patterns;data mining;dynamic graphs

position information;feature selection algorithm;algorithm combines;feature selection technique;protein sequences;protein sequences
location-based;spatio-temporal;probabilistic learning;temporal features;common interests
prediction accuracy;secondary structure;classification;database;protein sequence;class prediction
gene expression data;simulation studies;gene expression;adaptive algorithms;model-free;essential information;maximum number of;computational burden;data complexity;gene regulatory networks;gene expression data
distance measure;classification problems;classification;learning process;real datasets;genetic algorithm;time series;time series;distance measures;locally optimal;optimal parameters;machine learning;data mining algorithms
test set;classification;cross validation;selection process;data set;replication;model selection;real world;gene expression;classification methods;model selection;feature selection
prediction accuracy;support vector machines;computational methods;classification;cross validation;structure information;high cost;support vector machines;structural features
cross validation;prediction accuracy;structural information;vector machine;principal component analysis
learning process;search space;learning process;genetic programming;regression problem;gene expression;building block
artificial neural networks;high accuracy
kernel pca;data points;microarray data;kernel pca;vector machine;kernel principal component analysis
dimensional data;random projection;nearest neighbor classification;feature set;random projection;image data sets;principle component analysis;principal component analysis;predictive performance;micro array data;data sets;nearest neighbor classification;computational cost;dimensionality reduction methods;dimensional data;nearest neighbor classifier
feature set;database;require expensive;classifier;hidden markov models;computational overhead;feature selection;obtained by combining;recognition rates;feature selection
clustering;document collection;document clusters;weighting schemes;machine learning;frequency information;document clustering;term frequency;data streams;term weighting scheme;term weighting scheme
information filtering;helping users;user preference;data records;latent semantic indexing;missing values
hierarchical agglomerative clustering;similarity measure;semi-supervised learning;hierarchical clustering;algorithms require;partitional clustering;hierarchical agglomerative clustering;labeled examples;quality metric
specific information;web content;web content;information processing;state transition;web content;hidden markov model
document streams;document streams;mixture components;potentially infinite;mixture model;process model;sliding window;dirichlet process;huge amounts of;generative model;sliding windows;high-level;mixture models;trend analysis
facial expressions;instances;face recognition;facial expressions;recognition accuracy
ensemble classifiers;feature vector;classification;medical diagnosis;cross-validation;ensemble methods;feature subsets;semi-automated
pattern classification;pattern recognition;classification;multi-layer;recognition task;detection rates;monitoring applications;condition monitoring;pattern recognition;support vector machines;feature vectors
training data set;pattern recognition;machine learning;data set;machine learning applications;supervised machine learning;line segments;line segments;artificial data sets;breast cancer;training algorithm
naïve bayes;machine learning;data set;environmental conditions;machine learning techniques;image processing
training data;test set;neural networks;predictive modeling;decision tree algorithms;instances;data sets;comprehensibility;rule extraction;rule extraction from
database;recognition performance;principal components;visual information;principal component;kernel space;input variables;regression model;numerous applications
probability estimates;decision trees;classification;classification;probability estimation;naive bayes;naive bayes classifier;tree induction;6;9;probability estimation
randomly generated;concept learning;real-valued
learning agents;learning agent;knowledge-based;rule refinement;rule refinement;problem solving;positive examples
information content;information bottleneck;jensen-shannon divergence;data organization;large number of;feature values;log analysis;data sets;semi-supervised learning;semi-supervised;data organization;feature vectors
data arrives;training data;neural networks;learning algorithms;incremental learning;incremental learning;classification problems;training phase
agent based;genetic algorithm;multi-agent;real-world;trading agents;trading agents;automatically generate;mechanism design
high levels of;supervised learning;test data;classification;large-scale;imbalanced data;negative examples;false positives;cross-validation;data sets;rank-order;rates;high accuracy;support vector machines;positive examples
multi-class classification;support vector machines;solution path;highly competitive;regularization;sparse representations;feature selection;multi-class;multi-class
classification model;hierarchical approach;kernel selection;learning machines;support vector machines;classification task
multi-class problems;binary classification;multi-class;vector machine;classifier
edit distance;classification;database;text categorization;weakly supervised;supervised learning
clustering;real data sets;fusion strategies;color space;multiple-view;classifier;target detection
sparse learning;relational algebra;random variables;sparse learning;query language;data model;data set;instance;regression model;databases;observation data;probability distributions
genetic programming;decision trees;texas hold'em;data set;human players;high accuracy;machine learning techniques;data mining approach
detailed comparison;nearest neighbor;real-world;actual values;dependent variable;missing values
data analysis;machine learning;knowledge discovery;formal concept analysis;data mining;information retrieval
robot learning;robot learning;image processing;web browser
noise levels;microarray experiments;microarray data;linear combination;data set;rates;statistical analysis;microarray data;missing values;estimation methods;local similarity
real graphs;discovered patterns;real networks;degree distribution;graph evolution;sparse graphs;small-world;single snapshot;existing graph;network evolution;temporal evolution;wide range;information networks
sensitive attributes;sensitive information
clustering;clustering problem;correlation clustering;instance;clustering aggregation;large datasets;approximation algorithm;np-hard;theoretical guarantees;clustering aggregation;clustering categorical data
attribute-based;data characteristics;database;real-world;data redundancy;real-world entities;query processing;relational information;databases;clustering algorithm;pairwise similarity;knowledge extraction;relational data;similarity measures
schema matching
file systems;database engine;open-source;multi-core
xml search;20;database;text search;2, 15, 17, 20, 19, 47, 26, 32, 24;information retrieval;xml content;3;ir research;highly structured;querying xml documents;query languages;query results
multi-dimensional;database technology;lock;index structure;accurate tracking;moving objects;margin
schema evolution
data services;relational model;query language;data model;entity framework;replication;entity framework;data model
ad-hoc;distributed architecture;information theory;large networks;data sources;data management;sensor networks;application development;databases;sensor networks;long term;competence;data management
computing infrastructure;acm sigmod;international workshop on;data management techniques;modern hardware;database performance;data management
acm international workshop on;information systems;data warehousing and olap;large amounts of;decision support;scientific databases;data warehousing and olap;data management
workshop report;data engineering;international conference on
web pages
relational model;application domains;database

query optimization;database;acm sigmod;scientific data;database research;digital libraries
complex queries;data types;data exchange;load-balancing;data intensive applications;queries involving;exact-match
spatial data;relational algebra;data types;spatio-temporal;data types;temporal data;sql extension
domain knowledge;clustering problem;attribute values;clustering results;clustering algorithms;constraint-based mining;efficient algorithms to;constrained clustering;data clustering;data items;data mining systems;clustering quality;objective function;clustering
high-dimensional;multi-level;multi-dimensional;data points;dimensionality reduction method;higher precision;highly scalable;synthetic datasets;dimensionality reduction;low-dimensional;dimensionality reduction
key points;main memory;object-relational;cache performance;highly competitive;main memory;xml databases;object-oriented
physical characteristics;cache performance;main memory;relational operations;relational database management systems;data layout;relational databases;relational data;storage devices
individual records;fixed-length;compression techniques;compression techniques;random access;disk space
clustering;database;automatically extracting;clustering method;motion model;behavior patterns;color images;temporal features;motion patterns;high precision
statistics-based;privacy concerns;hierarchical structure;web mining;web browsing;distributed data mining
real data sets;synthetic data sets;categorical data;data set;anonymized data;query answering;applications involving;information loss
training data;training set size;predictive accuracy;training data is;training examples;data set;classification;classifier performance;data mining process;progressive sampling;machine learning and data mining;classifier
spatial correlations;ct) images;data mining tasks;evaluation functions;medical imaging;small training sets;computed tomography;multi-instance learning
training set;classification;dependent variable;computed tomography;classification task;neighborhood information
supervised learning techniques;fold cross-validation;takes into account;confidence values;error rate;classifier
data structure;parameter optimization;nearest neighbor;classification;decision boundary;nearest-neighbor;evolutionary algorithm
text mining;large-scale;data mining;kdd-2006 conference;data mining;link mining
databases;data mining methods;data mining;biological information;large amounts of data;data mining;gene expressions;biological data
workshop on data mining;real-world;data mining;business applications;kdd-2006 workshop;business applications;business processes;data mining
predictive model;kdd workshop;markup language;data mining standards
workshop report;web usage analysis;web usage analysis;acm sigkdd international conference on;knowledge discovery;web mining;data mining;web mining;web mining
temporal data;sequential data;temporal data mining;data mining techniques
workshop report;data mining research;multimedia data mining;multimedia data mining
workshop report;acm sigkdd;temporal data mining
workshop report;data mining applications;workshop brought together;real-world;acm sigkdd international conference on;international workshop on;knowledge discovery;data mining;data mining process
clustering;clustering;heuristic method;data points;clustering algorithms;clustering problem;partition-based;streaming data;similarity function;original data;similar" objects;data streams;traditional clustering
years ago;database;information integration;data warehousing;vision based;data transformations
data manipulation;data model;database systems;database researchers;database
data-driven;large numbers of;intelligent behavior;indexing techniques;large number of;data management;dynamic behavior;artificial intelligence;query processing
keyword queries;semantic relationships between;query efficiency;inverted lists;heterogeneous data sources;data items;user interaction;indexing techniques;pose queries;data-integration systems;unstructured data;schema elements
high-density;database;flash-based;database servers;computing devices;digital cameras;transactional database;unique characteristics;disk-based;data storage
approximation algorithms;social security;sensitive information;protect privacy;np-hard;approximation ratio
past queries;relevance ranking;database;private information;tree-based;information retrieval;record linkage;ranking algorithms;generative model;query result;text processing
database;data stored in;database systems;forensic analysis;database table
attribute values;ad-hoc;database applications;disk access;tree-structured;index nodes;query processing;efficiently computing;ranking functions;threshold algorithm
large-scale;keyword queries;ir techniques;retrieval effectiveness;database;relational database systems;query evaluation;text data;relational tables;databases;search result;data stored in;ir-style;ranking method;keyword query;relational databases;query processing
clustering;bitmap index;sql queries;data mining applications;database queries;post-processing;information retrieval;clustering quality;query processing;efficient clustering;query conditions;query results;structured data;retrieval applications
relational data sources;world wide web;selection problem;structured databases;data source;keyword-based search;database;structured data sources;ranking methods;distributed databases;keyword-based;data sources;service oriented;databases;traditional database;relational database;keyword query;relational databases;real datasets;keyword based;summarization method
domain knowledge;user) feedback;conflict resolution;data sources;query-driven;query processing;data structures and algorithms;user feedback;data integration
web-based;relational queries;applications ranging from;collected data;materialized views;data sources;information integration;answering queries;query results
cardinality estimates;query optimization;high-quality;materialized view;query execution;estimation technique;process control;cardinality estimation;produce accurate;cardinality estimation;user-defined functions;microsoft sql server;random sampling
approximate answers;distributed computation;analysis reveals;aggregate queries;sketching techniques;sketching techniques;statistical analysis;statistical analysis;data-streaming;theoretical bounds
incoming data;estimation techniques;computational costs;error bounds;order statistics
data sharing;expected number of;data availability;baseline algorithm;communication cost;data replication;scalable solution;concurrent updates;efficient retrieval of;performance gains;typically rely on
data distributions;search performance;highly skewed;index structure;large sets of;data items;fault-tolerant;range queries
data sharing;data-sharing;distributed architecture;data management applications;databases;data organization;operating systems
processing units;high-speed;comprehensive evaluation;data stream;data streams;network-traffic;primary goal
medical applications;matching algorithm;database applications;real datasets;huge amounts of;statistical information;stream data;problem domain;data streams;pattern growth;data items;data arrive
streaming xml data;xpath queries over;streaming algorithms;streaming environment
query processors;limited memory;query semantics;large-scale;data cleaning;real-world;probabilistic database;databases;probability distribution over;environmental monitoring;application domains;stream-processing;probabilistic data;data sets;information integration;probability distributions over;data streams;aggregate queries;unlike conventional;multi-sensor
xquery engines;xml tree;applications including;access control;xquery update;wide-range;xml documents;user queries;xml update;6
growing number of;graph-structured data;worst-case;search strategy;bi-level;memory requirements;query processing;keyword search;query keywords;keyword search on;block level;query processing
xpath queries;database;great success;query performance;join algorithms;xpath processing;optimization techniques
xml data;xml keyword search;data structures;keyword search;complex data;keyword search;query language;web users
cardinality estimation;database;database schema;query optimizers;test query;base tables;database management system;test database;databases;operator;test case;query-aware;query results;wide range
selection predicates;data source;language models;data cleaning;large number of;information retrieval;hidden markov models;databases;similarity predicates;scoring methods;data quality
energy-efficient;mobile devices;data centers;energy-efficiency;wide range;energy efficiency
closely related;database management systems;database;database management;physical design;database;database management system;storage systems;storage management;storage devices
data warehouse systems;table scan;database schema;parallel processing;cpu cost;database scans;table scans;database performance;trade offs;query processing
relational model;data values;query result;relational queries;metadata management
open-source;database;static databases;high-volume;query processing;dynamic environment;relevant data
small size
multiple sources;pairwise constraints;search space;real data
logical reasoning;28;schema matching;data instances;1;ontology matching
rewriting queries;database;data transformation;view maintenance;object oriented;data management;data access;databases;answer queries;relational database;persistent storage
query optimization;relational model;bi-level;information integration;relational databases;source databases;distributed database
selection predicates;linear algebra;sharing opportunities;matrix decompositions;aggregate functions;distributed queries;heuristic algorithms;query optimizations;aggregation functions;aggregation queries;np-hard
stream processing;compression methods;streaming environment;large-scale;detection accuracy;document processing;event detection;stream processing;user interface;limited resources
query evaluation;boolean queries;xpath queries;performance guarantees;counterpart;xpath queries over;network traffic;query returns
network processing;sensor readings;monitoring queries;selection queries;physical environment;event detection;sensor networks;sensor data;sensor networks;join queries
query execution;complex queries;multiple queries;cost-based optimization;sharing opportunities;nested queries;optimization problem;query plan;materialized views;minimal overhead;query processing;query optimizers;microsoft sql server;subexpressions
systems require;information systems;query relaxation;real-world;databases;information management;object-relational database;users' queries;data structures;data sets;structured information;ir systems;query relaxation
database;database server;long-running;high-priority;operator;database-centric
dynamic programming;threshold-based;time series;time series
large numbers of;position information;indexing approach;location update;moving objects;location-based
clustering;clustering algorithms;density-based;database;line-segment;minimum description length;trajectory data;line segments;clustering algorithm
keyword queries;query processing;online users;dynamic environments;memory requirements;homeland security;data sources;formal semantics;data streams;operator;keyword search on;application scenarios;keyword search on
considerable effort;product development;dynamic nature of;aggregate data;real data;analytical processing;highly correlated;analytical processing;numerical data;on line-analytical processing;query languages;keyword-based search
world wide web;database;random walk;provide answers;random sample;user queries;hidden databases;underlying data distribution;databases
user preferences;database queries;automatically constructed;cost-based;real life;query-result
vector space;data stored;data sources;privacy preserving;privacy requirements;record matching
database;real-world;privacy risk;anonymization techniques;databases;anonymized data;information technology
issue queries;public data;product information;distributed processing;private data
data publication;aggregate information;real data;theoretical results;privacy preserving;theoretical analysis
query optimization;database systems;database;uncertain data;aggregate query;decision support;aggregate queries;decision support applications;machine learning;probability distributions;probabilistic inference;general form;probabilistic inference;belief propagation
incomplete information;relational algebra;database;real-world;query language;query optimization;incomplete information;query processing in;data management;decision support queries;query languages;relational database management system;data management applications
analytic processing;database;ad-hoc;aggregate query;approximate query processing;exact answer;query processing in;exploratory data analysis;relational database;database systems
content-based filtering;optimization approach;xml data;relevant data;xml document;data dissemination
clustering;data structure;maintenance cost;publish-subscribe systems;large number of;minimal overhead;clustering algorithm
queries involving;systems support;publish/subscribe systems;large number of;xml streams;query language;join processing;join processing;event processing;query processing techniques;xml stream;join queries;concurrent queries
cardinality estimates;optimizer;set intersection;skewed;search engines;query processing;data distribution
dynamic programming;search space;cost-based;database systems;search strategies
multi-version;lower bound;tuple level;user-defined;cost-based query optimizer;query optimizers;selection predicate
parallel database;query execution plans;optimizer;data warehouses;intermediate results;materialized view;database;parallel databases;olap queries;real-world;parallel dbms;estimation errors
schema-design;sparse data;schema design;database systems;large number of;data set;data sets;data management;ad hoc queries
tree based;tree construction;suffix tree;dna sequences;main-memory;biological sequence;data skew;algorithm called;human genome;databases;disk-based;massive data
indexing schemes;graph-structured data;reachability queries;scale-free;index structure;query performance;large graphs;existing graph;million nodes;relational database management system;graph indexing
database;np-complete problem;frequent subgraphs;main memory;subgraph isomorphism;query performance;query answers;answer set;query processing;closed frequent;graph queries;indexing technique;graph databases
web site;search engine
web data management;database management;database research;data management;databases
large-scale;web data;web data management;web-scale;development environment;data management system;data processing;search engine;data management
data manipulation;database systems;database;language constructs;higher-level;entity framework;multi-tier;application servers;service-oriented;entity framework;data access;data-centric
1;object-oriented;object/relational;object/relational;data-centric
data objects;data objects;service oriented;data source;programming model;community based;heterogeneous data sources;aggregate data;application developers;lower cost;data formats;programming models
data volume;low-cost;high reliability;application servers;transaction throughput;database systems;service provider;large number of;organizational structure;high volume;recent database;highly scalable;large database;database management systems;high scalability;data integration;database;large scale;complex applications;database management system;resource utilization;data integration
cluster based;database operations;fault tolerant;database size;optimization method;data mining;intra-query;relational operations;data warehousing;database management system;databases;information resources;parallel dbms
related information;data warehouse
unstructured information;data warehouse;meta-data;real-life;customer data;text mining techniques
data type;xquery update;xml db;xml update;sql/xml
xml data;multi-user;xml document;database;xml applications;real-world;xml databases;domain-specific;concurrency control;xml database;databases;database functionality;relational database;transaction processing;transaction processing
13;structural join;database systems;database;xml data;data model;1;join algorithms;ibm db;6;database functionality;relational databases;aggregation functions;database vendors
xml format;ibm db;database vendors;enterprise data;data server;database schemas;relational dbms;document representation;relational data
data stream processing;data streams
access control;large scale;database;enterprise applications;access control;optimization techniques;application server;oracle database
database systems;access control;database;data-collection;database vendors;total cost
query complexity;query optimizer;database;ad-hoc;sql server;database research;execution strategies;execution strategies;decision support systems;data volumes;cost-based query optimizer;automatically generated;query performance;microsoft sql server
clustering;data mining tool;statistical models;table scan;linear models;database;sql queries;linear regression;data set;large data sets;data sets;user-defined functions;statistical models;relational dbms
selection techniques;search systems;search engine;highly scalable;large-scale
parallel implementation;map-reduce;programming model;real-world;map-reduce;relational operations;machine learning;join algorithms;search engines;model called;data processing;relational data;relational algebra
multi-dimensional;fall short;business information;large datasets;data updates;database;business information;data warehouse;data extraction;information processing;data warehousing;automatically generating;data processing;databases;database-centric;real world;high-level;decision making
data services;data access;data-centric
database operations;general purpose;database;graphics processing units;design decisions;query engine;programming model
data structure;relational query processing;cache performance;counterpart;query processing;query processor
takes into account;microsoft sql server;physical design;low overhead;tuning tools;data characteristics
data manipulation;data-access;domain model;object-based;entity framework;entity framework;conceptual model;relational database;database schema;application development;higher level
cardinality estimates;query optimization;query plans;query feedback;quality control;execution plans;cardinality estimation;user-defined functions;microsoft sql server
query execution;query evaluation;relational database management system;monitoring queries
virtual machine;web services;active learning;virtual machines;multi-tier;performance goals;computing resources;database applications;resource utilization
uncertain databases;data uncertainty;query processing capabilities;search strategies;databases
virtual machine;unique features;user interfaces;hand-crafted;user defined functions;sensor network;query optimizations;sensor nodes;real world
data stream systems;data stream management systems;data streams
stream processing;estimation problem;data stream;data streams
real-life;overlay network
multiple data streams;data stream management;sensor data;high degree of;mobile devices;stream processing;fine-grained;traffic management;data stream;relevant data;data sources;large amounts of data;operator;health monitoring;distributed environment
high speed data streams;query processing engine;event processing;query language;rates;complex event
clustering;classification;database;user queries;stored data;data mining system;data stored in
web databases;web-databases;scheduling policies;end users;database;web sites;multiple dimensions;user-centric
allowing users to;quality-aware;raw data;decision-making;data exploration
search engines
data-driven;web-based;user-centric;web applications;programming model
continuous queries over;web service;information extraction;stream processing;web services;continuous queries
databases
sql queries;xml data;main memory;xml documents;change detection;xml document;relational database;relational databases;database engines
query expansion;large collections of;xml documents;ranked retrieval;search engine;structured data
web corpus;large-scale;document retrieval;data-rich;entity search;search engines;search engine
data objects;information discovery;query results;link structure;heterogeneous data sources;1;original data;labeled data;queries efficiently;operator;querying capabilities
science applications;data-intensive;information quality;information quality;data quality
meta-data;partial knowledge;data model;query-driven;constraint-based;scientific data
database;query interface;enterprise-wide;data complexity;biological information;prior knowledge;size estimation
general purpose;distributed computation;overlay-network;distributed applications;grid computing;highly distributed;data processing;distributed environment;programming language
database technology;relational algebra;xquery expressions;xml data;xquery processing;view definitions;high-volume;database query processing;relational database;relational database system;plans
domain knowledge;pre-defined;user interactions;data collections;natural language interface;query translation
database technology;event processing;valuable information;complex patterns;information overload;vast amounts of data
probabilistic databases;prototype systems;scientific data;database research;databases
large scale;mining large graphs
data-driven;data-centric;database systems;databases
multiple layers;access control;data-centric;aggregate data;sensor networks;query processing;query processing techniques;sensor databases;database researchers;design issues
selection process;high quality;mobile computing;data engineering;data management;acm international workshop on;program committee;mobile users;mobile access
tree structure;path length;path queries;indexing structure;adjacency matrix;graph structured data;search algorithm;graph structured data
bayesian networks;conditional independence;bayesian network structure;bayesian networks
frequent subgraphs;2;graph mining;graph databases;tree nodes
evolving data;data describing;complex data;complex objects;time-series;complex data
clustering;order statistics;data analysis;similar objects;large-scale;efficient clustering;web search results;computational cost;hierarchical clustering methods
mining closed;real world;databases;search strategies;tree-structured
scale-free;community structure
support vector machines;learning problems;maximum margin;supervised methods;training data;7;semi-supervised;semi-supervised;classification problems;semi-definite programming;classification algorithms
interpretability;decision tree;classification accuracy;vector machine;sampling method;risk management;statistical method;credit card;support vector machines;neural network
computational complexity;real-world;linear programming;vector machine;quadratic programming problem;support vector machines
classification;classification accuracy;linear programming;classification methods;data mining approach;data mining approach;behavior analysis;classification method;principal component analysis;multi-criteria;dimension reduction
process model;multiple criteria;knowledge management;knowledge management;linear programming;data mining;data mining;data mining process;process models
clustering;meaningful clusters;high-quality;data mining and knowledge discovery;electronic documents;document clusters;optimization procedure;document clustering;comprehensibility;data mining;clustering algorithm;related documents
clustering;clustering;similarity measure;multi-stage;data mining;greedy algorithm;high-dimensional data sets;data mining;association rules
mining results;users' interests;mining algorithms;database;pattern tree;subtree patterns;mining process;xml databases;subtree patterns;frequent induced;mining algorithm;frequent induced;web log
real-world datasets;13;12;tree structured;database;tree mining;mining frequent;schema matching;web information systems;conceptual model;frequent subtrees;candidate generation;tree model;ontology matching
mining closed;mining closed;pruning techniques;computational biology;tree structures;database;tree patterns;mining frequent;algorithm named;xml databases;frequent subtrees;frequent induced;frequent induced;free trees
occurrence frequency;computational complexity;vector space;extraction algorithm;keyword extraction;large-scale;chi-square;term frequency;linguistic features;linguistic features
matching techniques;increasing number of;information extraction;data sets;search engines;web mining;string matching
visualization techniques;support vector machines;kernel-based;decision tree algorithms;kernel function;data mining;interval data;artificial datasets;large datasets;data mining;data type;test results;kernel methods;continuous data;kernel-based
large-scale;decomposition method;complex networks;real data;fine-grained;extraction methods;efficiently extract;complex networks;coarse-grained;community structure
rule evaluation models;learning algorithms;case study;rule evaluation support method;classification rules;learning algorithms;post-processing;human experts;mined results;data mining process;meta-learning;rule evaluation support method;data mining;learning scheme
attribute reduction;data mining based;rough set;rough set theory;mining process
pattern recognition;graph database;computational biology;tree mining;pruning techniques;xml databases;completeness;candidate generation;graph databases;free trees
customer relationship management;database;data cleaning;data mining;decision tree;data mining;data mining based;data integration
rough set;classifier;neural network;rough set theory;medical images;neural network
predictive modeling;wide range
optimization algorithms;data points;optimization problems;large number of;multiclass classification;classification methods;graph kernels;multiclass classification;large scale;laplacian matrix;numerical results;semi-supervised;semi-supervised;learning method
computation costs;data sets;radial basis function;classifier
logistic regression;predictive models;logistic regression;weighted average;information theory;logistic regression;model selection
map) estimation;regression problems;ensemble members;combination weights;pruning algorithm;pruning algorithm;expectation-maximization algorithm;weight vector;classification problems
complex network;graph theory;parallel algorithm;real-world scenarios;complex network;complex networks;massive data;high efficiency;data mining;parallel algorithm
databases;decision trees;mutual information;classification;classification;decision tree;single class;missing values;missing attribute values;decision tree;probability distribution;classification process;classification results;takes into account;missing values
genetic algorithms;classification;classification;genetic algorithm;imbalanced datasets;imbalanced datasets;genetic algorithms;rule extraction;fuzzy rules;uci datasets
clustering;domain experts;medical applications;discriminative features;feature selection techniques;classification;real-world datasets;spatio-temporal;principal component analysis;fmri data;time series;support vector machines;spatial features;data mining task;feature selection technique;data collected;feature subset selection;spatial features
clustering;parameter values;data mining techniques;finding clusters;arbitrary length;spatial databases
multi-level;spatio-temporal databases;association patterns;spatio-temporal databases;transaction databases;spatio-temporal;association rules;market basket analysis;market basket analysis
vector data;compression ratio;data compression;database;multi-resolution;mobile computing;multi-resolution;road network;vector data;compression scheme;road networks;databases;query processing;road network;compressed representation;application domains;compression schemes
graph mining;spatio-temporal;network management;algorithm called;dynamic graphs;spatio-temporal;relational data
search engine;similarity measurement;storage space;arima model;related queries;query logs;text queries;information retrieval;time series;moving average;arima) model;query logs;auto-regressive;arima model;dimensionality reduction;large database
statistical queries;maximum likelihood estimation;classification;candidate models;computation cost;remote sensing;extended abstract;classification accuracy;computational overhead;computational cost;statistical queries
moving objects;moving object
association rules;spatio-temporal;search space;real world;association rule
data mining applications;data mining and knowledge discovery;data mining and knowledge discovery;data mining process;data mining tasks;software & systems;data mining;complex data;distributed data mining
data repository;classification;maximum entropy model;maximum entropy model;real-valued;feature representation;feature representation;convergence speed;incomplete data
information systems;common properties;equivalence relations;rough sets;rough set theory;rough sets
clustering;clustering;high-frequency;topological structure;geometric structure;document set
association rules;logical properties;multi-relational data mining;data mining
patterns mined;mining algorithms;theoretical foundations;search algorithm for;high dimensional
rough set theory;decision-making;incomplete information;continuous attributes;rough set theory;continuous attributes
contingency table;contingency table;original matrix;sample size;linear combination;expected values;statistical analysis
web pages;web page
clustering;genetic algorithms;genetic algorithm;clustering technique;time series;time series;fitness;clustering approach;wavelet transformation
knowledge discovery;hypothesis generation;text documents
knowledge engineering;matching accuracy;data mining methods;textual data;free-text;text understanding;general-purpose;case study;documents retrieved;ensemble method;classification errors;relevant information;classifier;learning method;machine learning methods
human experts;genetic algorithm
service quality;event data;business process;business processes;business processes;high-level;enterprise systems
prediction techniques;prediction accuracy;decision support;time series;association rules mining;data mining;association rules;prediction task
domain knowledge;supervised learning;semi-supervised clustering;clustering accuracy;clustering process;clustering task;semi-supervised clustering;clustering algorithm;semi-supervised;semi-supervised clustering
markov chain;time series;data analysis;high-order
sponsored search;directed graph;semantic relationships;search engines;search engine;search engine
privacy-preserving;real-world scenarios;data collections;large scale;data sets;privacy-preserving;personal information
frequent itemsets;frequent itemsets;decision theory;raw data;association rules
data set;privacy guarantees;data mining results;decision tree;real-world;privacy preserving data mining;naive bayes classifier;data sets;real-world;privacy preserving data mining;privacy preferences;real world data sets
data objects;attribute values;high-accuracy;svd-based;data perturbation;principal component analysis;dimensional data;nonnegative matrix factorization;latent features;matrix decomposition;breast cancer;binary classification;data mining;cross validation;high-accuracy;vector machine;optimization problem;privacy preserving data mining;data privacy;data utility;benchmark datasets;low rank;protect privacy;data utility;learning method
census data;higher-quality;search strategy;finding optimal;information loss
semi-honest;semi-honest;secure multi-party computation;secure multi-party computation;distributed data;privacy-preserving data mining
pattern set;data mining;frequent pattern mining;sensitive information;frequent pattern mining
missing values;distributed databases;privacy-preserving;privacy-preserving;decision tree
clustering;privacy guarantees;classification;data mining techniques;outlier detection;data mining applications;privacy concerns;distributed data;privacy preserving;nearest neighbors;data mining;nearest neighbor search;data mining algorithms;privacy preserving;privacy preserving;nearest neighbor search
data mining tasks;nearest neighbor classification;privacy-preserving;data mining;privacy concerns;collect data;data privacy;privacy-preserving;data mining
data mining;data mining tasks;data mining tasks;mining association rules;business intelligence;business intelligence;high precision;data mining;storage requirements;decision making
latent structure;latent structure
auxiliary
pattern mining;pattern mining;tree structured data;data obtained from;evolutionary algorithm;tree model
knowledge discovery;distinctive features;large amounts of
clustering;clustering;business opportunities;cluster analysis;business opportunities;cluster analysis
product feature;relies heavily on;mining algorithm;domain knowledge;knowledge-based
remote sensing images;image segmentation;remote sensing;features extracted from;semi-structured;semi-) structured;satellite images;high-resolution;knowledge discovery
instance;semantic web;web pages;classification methods;classification
text mining;free text;input data;large text collections;domain ontology;text-mining;mining process;domain ontology;text collection;textual documents;textual documents;corpus-based
data representation;retrieval performance;statistical analysis;text retrieval;concept-based representation;quality measures;graph representation
search results;data structure;unsupervised learning;related data;federated search;feature extraction;information extraction;domain-specific;web crawling;data mining;high-level;information extraction;database table
model complexity;auto-regressive
information systems;software tools;spatial information;data mining;valuable information;data mining technologies;data sources;geo-referenced;census data;decision makers
applications including;association analysis;association rules;association rule;association analysis;real world data sets
clustering;cluster-based;total number of
web documents;classification approach;semantic similarity between;semantic web;web document;web document;classification approach;semantic similarity
stream mining;parameter settings;window sizes
land cover;streaming data;data set;change detection;heuristic algorithm;moving average
clustering;probabilistic latent semantic analysis;historical data;semi-structured;clustering method;latent semantic analysis;compression-based
frequent item sets;web-mining;frequent patterns;data sets;7;pattern set;frequent pattern mining
clustering;xml documents;clustering results;encoding methods;xml documents;document structure;high dimensionality;feature vectors
ontology-based;completeness;candidate itemsets;association rule mining;concept hierarchy;large databases;data mining research;association rule mining;mining algorithm;data mining system;large datasets;domain ontology;fuzzy association rules;huge number of
xml documents;frequent patterns;semi-structured data;xml mining;large number of;xml documents;data warehousing;association rules;discovered knowledge
text mining;tree based;classification;real life datasets;user-defined;spectral analysis;classification accuracies;tree based;character recognition;diverse domains;automatically learning;user input
automatically extracted from;web pages;anchor text;link structure;web graph;web graph;ranking algorithms;web graphs;web page;page content
query evaluation;potentially infinite;sliding window;data items;mathematical model;data stream;sliding windows;continuous queries;continuous query
clustering;kernel density;unsupervised clustering;spatio-temporal;data acquisition;streaming data;high efficiency;streaming data;clustering data streams
na篓ýve bayes classifier;naive bayes classifiers;na篓ýve bayes;online learning;naive bayes classifiers;classification problems
linear models;predictive power;time series;computational costs;user queries;data mining;real world
clustering;clustering;labeled samples;learning algorithm;classification methods;data stream mining;data set;unlabeled samples;mining data streams;classifier
learning-curve;learning algorithm;progressive sampling;progressive sampling;sample size;machine learning algorithms;class distributions;learning algorithms;sampling method;data mining research;large data sets;data sets;databases;imbalanced data sets;processing speed
sampling methods;potentially infinite;sampling algorithm;small sample;data streams;streaming data;distance based;data streams;random sampling
clustering;biological datasets;density-based clustering algorithm
entropy-based;complete set of;frequently occurring;dna sequences;discovering frequent;sliding window;mining frequent;dna sequences;input sequence;sliding windows
feature space;classification;great success;generative model;protein sequences;interpretability;classifier
data point;consensus clustering;overlapping clusters;microarray data;microarray data;clustering algorithms;real world datasets;overlapping clusters;10;obtained by applying;model selection;consensus clustering
gene regulatory networks;large-scale;gene regulatory networks;time series;expression data;expression data;data mining approach;data mining approach;gene expression
databases;gene ontology;ontology-driven;prior knowledge
clustering;update processing;tree based;high-quality;3;quality guarantee;4;clustering algorithm;graph clustering;worst case
mining results;mining closed;frequent itemsets;closed frequent itemsets;online mining;data streams;sliding window;streaming data;closed frequent itemsets;highly accurate;data streams;incremental updates;mining data streams
sliding window;sliding window;sequential patterns;incremental mining;sequential patterns;incremental mining;data streams;mining sequential patterns;mining data streams
clustering;clustering;clustering algorithm;real data sets;data stream;traffic flow;data stream;click stream;continuous attributes;clustering methods;clustering problems;sensor networks;machine learning and data mining;mining tasks
risk management;vector machine;sensor data
risk management;vector machine;sensor data
quality control;significant rules;information systems;decision support;data mining
clustering;microarray data;similarity measure;grey relational;data sets;selecting informative;gene selection
clustering;11;microarray data;similar behavior;data set;data matrix;1;takes place;gene expression;objective function;np-hard
clustering algorithm;graph-theoretic;protein interaction networks
received increasing attention;singular value decomposition;context-aware;scientific data;large collections of;life science;visual exploration of;real datasets;visual exploration of
common features;molecular biology;evolutionary process;databases;data type;data type;user-centric;data mining algorithms
interaction data;specific features;scale-free;small-world;information flow;protein interaction networks;clustering methods;information flow;protein interaction networks
risk management;database;machine learning;risk management
network model;key features;real world;community mining
knowledge acquisition
network management
risk analysis;neural networks;genetic algorithm;risk factors;risk analysis;principal component analysis;neural network
clustering;clustering;classification;data clustering;fuzzy clustering;supervised classification;rough sets
11;high dimensional;outlier detection;dimension reduction;similarity measurement;time series;correlation coefficient;hierarchical agglomerative clustering;similarity based
information loss;gene expression data;classification;expression data;selecting informative;accurate classifiers;gene selection;simulation results;gene selection;bioinformatics research
clustering;clustering algorithms;categorical data;clustering
human brain;traditional clustering
data structure;multi-relational data mining;meaningful patterns;biological networks;unsupervised learning;common features;graph-based;learning algorithms;long-term;biological network;relational learning;supervised learning;graph-based data mining;biological data;graph representation
high throughput;early stage;data sets;high-throughput;feature selection;data mining;feature selection
maximum likelihood;data processing;noise estimation
classification performance;classifier;classification
support vector machines;classification;classification approaches;version space;version space;6;support vector machines
years ago;pattern tree;mining association rules;composite items;composite items;association rules;association rules;discovered rules
data mining;extracted knowledge;knowledge discovery;knowledge discovery process;data mining;knowledge discovery process
user interface;case study;user interface;interestingness measures;interesting rules;medical knowledge;interestingness measures
ontology-based;instance;user interface;information retrieval;retrieval methods;semantic web;ontology-based;association rules;association rules;high-level;retrieval method
model complexity;sample size;graphical model;data mining system;data mining;sample sizes
classification;raw data;data cleansing;data sets;originally designed;image classification
gene expression data;learning framework;gene regulatory network;dynamic bayesian network;time series;higher-order;dynamic bayesian network;gene expression;gene regulatory network
statistical significance;statistical properties;gene expression;data processing;microarray data
feature space;classification techniques;classification;classification;feature extraction;large number of;effective classification;pattern recognition;classification problems
automatic extraction of;database;biological processes;extraction rules;information extraction;biological information;data mining
mining algorithms;minimum support threshold;knowledge representations;database;xml mining;frequent subtrees;mining frequent;queries posed;instances;web mining;tree structured;databases;association rule discovery;frequent induced;mining algorithm;frequently occurring
association rules;supervised learning;11, 9;statistical measures;entropy measure;3;classification rules;data sets;2;production rules;4;formal concept analysis;data mining;production rules;support measure;5;discovered knowledge;discovered rules
decision trees;association rule discovery;classification approaches;classification systems;data sets;incremental learning;accurate classifiers;associative classification;data mining;associative classification;classification approach;classification algorithms
clustering;fitness function;high accuracy;real-life data sets
mobile communications;real datasets;mobile communication;hidden layer;prediction method;neural network
growing number of;intrusion detection;multiple criteria;mathematical programming;classification;network intrusion detection;false alarm;linear programming;mathematical programming;multiple criteria;network intrusion detection;machine learning;classification accuracies;rates;intrusion detection;perform poorly;classification problems;classifier
training set;large number of;data mining
classification;labeled instances;class label;learning algorithm;instance;data preprocessing;feature selection;data mining;classification accuracy;learning problem;real-life datasets;classification algorithms
predictive accuracy;high quality;vector machine;image analysis;classifier performance;svm) classifier;error prone;density-based clustering algorithm
feature subset;classification;feature extraction;classification results;selection methods;classification method;clustering algorithm;classification results
decision trees;closed patterns;data mining techniques;structured data;decision tree construction;realistic data;effective pruning;real world;structured data
data base;manufacturing process;temporal patterns;manufacturing process;supervision;modeling approach;markov chain
probabilistic modeling;data analysis;classification;probability distributions;probability distribution;probability distribution;data analysis;data mining;random variable
search space;massive graphs;main-memory;main memory;query types;query evaluation;united states;shortest path;batch processing;disk-based;path queries
service oriented;distributed computing;event-driven;business process;service-oriented;business operations;web services;event-based
query execution;access control mechanism;access control;hierarchical structures;xml data;instance-level;tightly integrated with;xml databases;query processing in;query processing;databases;control mechanisms;query plan
line segment;nearest neighbor;synthetic datasets;data points;pre-computation;data-partitioning;multidimensional datasets;database updates;nearest neighbors;approximate results
document filtering;theoretical properties;xml streams;query language;query processing;data transformations;memory consumption;xml stream
query evaluation;large data sets;data model;probabilistic databases;probabilistic data;managing data;theoretical result
constraint satisfaction problems;conjunctive queries
database instances;conjunctive query;databases;conjunctive queries;view definitions;database theory
probabilistic databases;set semantics;conjunctive queries;relational algebra;databases
external memory;lower bounds;query processing;data stream model;massive amounts of data;internal memory
approximation algorithms;attribute values;decision trees;decision tree;entity identification;maximum number of;general problem;probability distribution over;np-hard;probability distributions over;fault detection;decision trees;greedy algorithm;approximation algorithm;average number of;special case;diverse applications;relational table
regular tree;binary pattern;conjunctive queries;path expression;tree automata
transitive closure;query containment;instance;query containment
querying xml data;xml tree;xml publishing;upper bounds;xml publishing;tree nodes;complexity classes;query languages;query results;xml trees;relational data;relational queries
database;frequent items;parallel processing;distributed processing;memory resources;data management systems;random sampling
document corpus;similar objects;synthetic data;data points;nearest neighbors;image search;query processing;theoretical analysis;query point;data management;query processing cost
closed world;data exchange;source schema;data exchange settings;instance;conjunctive queries;data exchange
database schemas;schema mapping;schema mappings are;data exchange;tuple-generating dependencies;instances;operator;negative results;high-level;schema mappings;data-exchange
peer data management;data exchange;data complexity;data integration;data exchange;data management
database queries;programming language
pattern-based;xml schemas;xml tree;rule set;implication problem;regular expressions;rule sets;reasoning problems
bounded treewidth;bounded treewidth;data complexity;finite structures;relational schema;recognition problem
rule-based;query equivalence
query languages;database
data integrity;instance-based;implication problem;instance;xml update;xml document
query answering;core xpath
multiple queries;multiple continuous queries;shared execution;performance gains;adaptive strategy;np-hard;continuous queries
lower bound;large data streams;data stream processing;algorithm requires;worst case;data streams;approximation algorithm;sliding windows
ordered trees;hierarchical structure;context-free;finite-state;tree automata
search engine;streaming algorithms;20;data-centric;probability distribution over;data stream;probabilistic" data;probabilistic data;domain values;sensor networks;data stream model;data streams;algorithms for computing
streaming algorithms;anomaly detection;statistical summaries;data streams
applications ranging from;event processing;query semantics;design choices;complex event processing;event streams;event processing;formal framework
contingency table;nearest;fall short;20;synthetic data;raw data;privacy guarantees;privacy-preserving;special case
labeled trees;xml data;probabilistic information;data model;general case;3;decision procedure for;correct answer;tree model
query evaluation;conjunctive query;probabilistic database;probabilistic databases;conjunctive queries;complete classification
naive algorithm;relational queries;data complexity;probabilistic data;joint probability;sufficiently high;np-hard
linear model;sample size;real datasets;prediction errors;model trees;prediction error
mining algorithms;frequently occurring;dna sequences;interesting patterns;case study;periodic patterns
frequent patterns;database;temporal association rules;theoretical properties;interesting patterns;prior works;mining associations;frequent 2-itemsets;short-term
personalized search;preserving privacy;web search;personalized search;search applications;user privacy;privacy-preserving;search services
test statistic;statistical significance;information retrieval evaluation;test statistics;rank correlation;additional information
component analysis;information retrieval research;retrieval method
evaluation methodology;retrieval effectiveness;xml retrieval;test collections;scoring methods;content-oriented xml retrieval
xml document collections;structured document retrieval;multimedia information retrieval;text-based;logical level;multimedia retrieval;xml elements;retrieval performance;multimedia objects;relevant information;xml element retrieval;ad hoc;xml structure
xml information retrieval;ir systems;test collections;end-users
1;xml retrieval;features extracted from;search process;xml documents;2;search behaviour
retrieval tasks;document types;xml collections
clustering;xml documents;xml mining;xml mining;classification
main findings;ir techniques
federated search;retrieval model;search engines;hidden information
search techniques;large-scale;federated search;text search;computing resources;digital libraries
tree structures;data warehouses;storage structures;query processing in;compression method;column-oriented
query language
clustering;similarity query;nn queries;similarity measure;accuracies;database;time series;hierarchical model;data set;data sets;filtering methods;point correspondences
description language;data sets;synthetic data;general-purpose
simple queries;xml data;increasing number of;query language;data model;application domains;domain-specific;xml-based;visual interfaces;9;user profiles;xml query language;user communities
data values;attribute values;20;xml processing;type-checking;database theory;xpath queries;schema design;40;static analysis;xml documents;4;32, 37, 40, 39;28, 31;core-xpath;query languages;static analysis
web applications;database systems;database;acm sigmod;xml query languages;semantic web services;modeling language;object-oriented databases

data bases;databases
image retrieval;multimedia information retrieval;classification;large-scale;image processing;large quantity of;multimedia information retrieval;machine learning;sensor networks;security applications;databases;video retrieval;multimedia data
snodgrass;database systems
ir research;program committee;web search;classification;formal models;machine learning;annual international acm sigir conference on;information retrieval;incomplete information;annual international acm sigir conference on;wide range;review process;topic areas;user studies;information retrieval;question answering;reviewing process;multi-media
service provider;asset management;organizational structure
personalized search;keyword queries;web queries;adaptive algorithms;ambiguous queries;query keywords;query expansion;personal information;web retrieval
user profile;trec collections;retrieval effectiveness;user's interests;user query;user-centric;query-specific;information retrieval;ir model;language modeling;contextual factors
evaluation methodology;web pages;personal information management;privacy issues;personal information management;personal information
multiple queries;news events;evaluation scheme;user feedback;fine-grained;incremental learning;adaptive filtering;novelty detection;semi-automatic
user-item;collaborative filtering;recommender systems;correlation coefficient;data sparsity;prediction quality;collaborative filtering algorithms;missing data;similarity computation;prediction algorithms;similarity threshold
user profile;personalized recommendation;large number of;hierarchical model;user feedback;user profiles;model parameters;bayesian hierarchical;user modeling;algorithm converges;computationally expensive
reusability;low-cost;relevance judgments;retrieval evaluation;retrieval tasks;test collections
information retrieval evaluation;information retrieval evaluation
retrieval effectiveness;discriminative power;average precision;ir evaluation;test collections;discounted cumulative gain;ir metrics
training data;labeled instances;machine learning techniques;instances;labeled data;feature selection;classifier performance;text classification;support vector machines;classifier;feature weights
search results;clustering;relevant documents;user's perspective;cluster labels;search engine;log data;query words;search logs;clustering
clustering;clustering;regularization;regularizer;regularization;information retrieval;clustering documents;document clustering;clustering methods
unstructured text;usage patterns;web pages;automatic extraction of;identification method
multi-modal;multi-level;query specification;hierarchical classification;large-scale;classification framework;positive results;boosting algorithm;image databases;classifier training;image collections;multi-task learning;low-level;image classification;image annotation;visual features;salient objects
image retrieval;unlabeled images;nearest neighbor;database;image space;relevance judgments;retrieved images;relevance feedback;relevance feedback;training samples;graph laplacian;active learning algorithm;geometrical structure;higher precision;optimal design;classifier;optimal design;content-based image retrieval
web search;generation process;fixed size;data structures;search engines;query biased;search engine;compression method;search engine users;total cost
search results;readability;web search;user behavior;search behavior;search result;search engine;web search engines;query terms
document cluster;clustering algorithms;multiple documents;document clusters;document clustering;clustering algorithm
initial query;web searches;search engines;search engine results;search behavior;query log
search results;web search;query topic;user behavior;web search;user study;browsing behavior;related queries
search results;geographic information retrieval;local search;location-dependent;search services;automated techniques
query throughput;evaluation strategies;document retrieval;main memory;disk access;retrieval systems;query processing;information retrieval systems
data-access;trade-offs;query answers;instance;search engines;rates;query results;web search engines;query log
search results;pruning techniques;large-scale;query loads;search engines;million web pages;web search engines;result quality;vast amounts of
term weights;mutual information;topic segmentation;topic segmentation;multiple documents;topic detection;similar documents;multi-document;special case;topic detection
feature trajectories;time series;event detection;spectral analysis;detection algorithm;3;document frequency
term weights;training data;event detection;named entity;event detection;multiple streams
web documents;web pages;duplicate detection;machine-learning techniques;document detection;search evaluation;text analysis;high precision
web search engine;classification;user experience;higher classification accuracy;query classes;machine learning;search advertising;web search results;search engine
click logs;transition probability;relevance information;relevant documents;random walk;relevance judgments;random walk model;image search;search engines;document pairs;random walks;click graph
search results;information seeking;information-seeking;retrieval effectiveness;interactive information retrieval
browsing behavior;web search engines;query operators;user groups;relevant information;search engine users
language models;cluster-based;relevant documents;term feedback;information retrieval;relevance feedback;term feedback;feedback documents;language modeling approach;query expansion;term-based;retrieval accuracy
trec 10 web track;learning algorithm;retrieval systems;computationally expensive;globally optimal solution;support vector;machine learning;average precision
feature space;ranking results;ranking svm;information retrieval;instances;special case;information retrieval
search results;commercial search engines;training data;optimization framework;user click;relevance judgments;machine learning;user queries;data sets;search engines;objective functions;search engine;ranking functions;preference data;supervised learning
proximity measures;trec test collections;language model;information retrieval;retrieval performance;retrieval models;highly correlated;retrieval model;document relevance;query terms;term proximity
worst-case;black box;random variable;query aspects;posterior distribution;pseudo-relevance feedback;retrieved documents
term dependencies;markov random field model;retrieval effectiveness;query suggestion;relevance models;information retrieval;relevance feedback;query expansion technique;data sets;markov random fields;pseudo-relevance feedback;query expansion;language modeling
trec test collections;language models;smoothing methods;query likelihood;poisson model;query generation;information retrieval;probabilistic model
question answering;human judgments
question answering;external knowledge;question answering
graphical models;prediction model;probabilistic graphical model;answer ranking;information retrieval;joint model;natural language processing;joint probability;logistic regression model;question answering
retrieval effectiveness;question answering;qa systems;highly ranked;retrieval task;semantic information;question answering;retrieval techniques
7;data collections;4;discounted cumulative gain;statistical significance;8;average precision
data-driven;test results;test collections;specifically designed to;test collection
highly ranked;text retrieval systems;relevance judgments;relevance judgments
ranking problem;web search;web search engine;loss function;ranking method;algorithm named;information retrieval;ranking problem;ranking function;machine learning techniques;upper bound;ranking methods
training data;baseline methods;loss function;instance;document retrieval;automatically created;learning algorithm;ranking svm;boosting algorithm;information retrieval;loss functions;ranking model;ranking models;discounted cumulative gain;training process;classification errors;directly optimize the;benchmark datasets;average precision
genetic programming;document collection;machine learning methods;genetic programming;web collection;trec-8 collection;tf-idf;statistical information;term frequency;ranking functions;average precision
ranking results;loss function;classification;feature selection method;similarity scores;ranking models;information retrieval;training instances;feature selection;feature selection methods;optimization problem;feature selection
growing number of;content-based filtering;bayesian methods;machine learning methodology;content-based filtering;computational cost;large-scale;performance gains;classification performance;spam filtering;text classification;high cost;support vector machines;benchmark data sets
spam detection;clustering;commercial search engines;web pages;web graph;spam pages;large-scale;web spam;link-based;search engine results;base classifier;classifier;web data
pagerank algorithm;web pages;algorithm called;web graph
music information retrieval;image retrieval;specifically designed to;database;rank order;large databases;text-based;real-valued;data set;model parameters;expectation-maximization algorithm;kullback-leibler divergence;class labels;music retrieval;multi-class;conditional probabilities
music information retrieval;web-based;web pages;large-scale;natural language;complementary information;web retrieval;vector space;meta-data;search engine;music collections;similarity measures
retrieval performance;term selection;information retrieval
trec collections;query suggestion;search engine;query suggestion;dictionary-based;input query;cross-lingual;query logs;cross-lingual;query translation;cross-language information retrieval;query log
breadth-first search;anchor text;large-scale;large data sets;link-based;text retrieval;average precision;web-page
network analysis;ir evaluation;bipartite graph;information retrieval evaluation;trec 8 data;average precision;effectiveness measures
statistical methods;world wide web;web pages;web page classification;classification;database;link structure;early stage;document-term;matrix factorization;content information;web mining;adjacency matrix;web page;compact representation;low-dimensional
text retrieval systems;relevant documents;federated search;selection methods;search effectiveness;text retrieval
information retrieval applications;sampling technique;multiple queries;sampling methods;selection methods
federated search;relevant information;retrieval accuracy;information retrieval
real-world datasets;large-scale;text search;text search;text collections;document collections;index size;web archives;optimization problems
search methods;hash-based;duplicate detection;great success;design principles;large document collections;similarity search;text retrieval;hash-based;feature vectors;hash functions
pattern queries;database;makinen-navarro, 2007;manning et al., 2007
prediction techniques;web search;real-world;query types;retrieval tasks;classifier;test collections;performance prediction;prior information;query performance;query performance prediction
organizational structure;sparse data;test set;retrieval tasks;retrieval methods;hierarchical structure;retrieval models;knowledge-intensive;language modeling
sponsored search;web search engine;user experience;syntactic features;web page;page content;result pages
search result quality;search session
query expansion;result sets;relevant documents;ir systems;relevance judgments;document term;information retrieval;user queries;test collections;ir) systems;query expansion
document scores;high-dimensional space;spatial analysis;prediction performance;large-scale;relevance judgments;retrieval systems;retrieval performance;performance prediction;performance prediction;data analysis;retrieval scores;information retrieval;information retrieval systems
multiple criteria;data fusion;decision rules;information retrieval;distinctive features;rank aggregation;information retrieval
trec data;ctr;language models;position information;retrieval performance;structure information;length normalization;term proximity
data set;information embedded in;blog data;feature selection method
speech recognition;handle arbitrary;retrieving information from;speech data;broadcast news;query processing;query terms
domain knowledge;classification;classification accuracy;label assignment;supervised machine learning;label sets;automatic speech recognition;text classification;retrieval applications
retrieval effectiveness;index terms;spoken document retrieval;mutually exclusive;retrieval performance;term frequency;error rate;posterior probability
context sensitive;web search;web search engine;query term;accurately predict;information retrieval tasks;context sensitive;language modeling;discounted cumulative gain;search engine;web search;query traffic;query terms
clustering;clustering algorithms;data source;classification approaches;content extraction;language-independent;rule based;machine learning approaches;word segmentation
domain-specific knowledge;passage retrieval;retrieval model;knowledge-intensive;information retrieval
keyword queries;text corpora;worst-case;information retrieval applications;data distributions;information retrieval;efficient indexing;databases;power-law
complex queries;basic operations;efficient search;worst-case;pattern queries;text queries;million documents;user interface;query processing;querying capabilities;query engine;index size
indexing structure;user queries;text retrieval;large document collections;query-driven;bounded number of;index size;retrieval quality;query logs;theoretical analysis;document collections;network traffic;query engine
local structures;classification;document representations;document classification;class-specific
natural languages;skewed
multiple classes;clustering results;graph theory;globally optimal solution;optimal matching;cluster quality;edge weight;document clustering;locally optimal;evaluation strategy;bipartite graph;evaluation measures
1;information retrieval;similar results;language model
trec collections;retrieval systems;data sets;test results;rank correlation;topic structure
binary classification problems;error-correcting output codes;text categorization;text classifier;multi-class;classifier
evaluation measure;text segmentation;user oriented;user-oriented
broadcast news;performance gain;text-based;entropy based
automatic evaluation;average precision;citation graph
ranked list;information resources;relevant documents;results merging;communication costs;federated search;source-specific;federated search;training documents;optimization method
strong correlation;query length;information retrieval
pseudo relevance feedback;summarization techniques
probability ranking principle;probability ranking principle;retrieval effectiveness
data-driven;matching methods;logistic regression;high recall;semantic information;question answering;event-based;term-based
access patterns;desktop search;temporal patterns;user study;tool called
search systems;user's interests;web search
relevant documents;main findings;information retrieval;document structure;xml document;relevant information;ad hoc
question answering
language-model;language models;query-expansion;relevance models;language modeling framework;query expansion;model selection
web browser;web pages;classification;web page
feature sets;classification
topic segmentation;topic segmentation
user's search;behavioral patterns;query reformulation
relevant information;digital library;information access;information retrieval
consensus clustering;voting scheme;cluster ensembles;cluster membership;small scale;cluster ensemble;clustering problems
global features;scale space;retrieval function;retrieval systems
text classification;real-world;semantic level;classification;problems require
ranked list;document retrieval;xml elements;retrieval task;relevant information;average precision
probabilistic model;estimation problem;document frequency
sample sizes;ir systems;average precision;wide range
information retrieval;model averaging;test collections;kullback-leibler divergence;latent semantic indexing;vector space model
search results;search engines;ranking results;search engine
search results;enterprise search;clickthrough data;retrieval accuracy;document types;trec enterprise;scoring function;term frequency;task-type;document type
search results;information sources;federated search;text search;hidden information;federated search;search engines;information source
query efficiency;query relaxation;ranking svm;ranking function;ranking svm;search quality
web pages;web search;distributed search;collection selection;user query;selection methods;ranking functions;collection selection
relevance feedback;user profiles;user click;online video
completeness;ir evaluation;ir evaluation;information retrieval;test collections;statistical significance
shape retrieval;shape matching;retrieval results
strong correlation;users' satisfaction;web search results;effectiveness measures
content-based filtering;filtering systems;multiple criteria;incoming documents;information overload;multi-criteria;select relevant;multiple aspects
ad-hoc retrieval;inverted files;pruning technique;term-based
training data;active learning;machine learning;competing methods;named entity;information extraction
classification problem;retrieval effectiveness;classification;web queries;training classifiers;sufficient training data;retrieved documents;classifier;query terms;pre-retrieval
random sample;test collections;relevance judgments;large collections
clustering;clustering;large number of;short texts;clustering accuracy;information overload;similar items;additional features
collection size;search interfaces;content summaries;distributed search;collection selection;collection size;logistic regression;estimation algorithms;maximum likelihood;web data
database;video summarization;automatically generated;higher accuracy;video content
test collection;text-based;patent retrieval;retrieval methods
markov random field;retrieval models;sentence retrieval;query terms
ad-hoc queries;ad-hoc;result pages;retrieval task;relevant documents
document retrieval;information retrieval systems;index terms;retrieval effectiveness
clustering;feature space;small training sets;information derived from;text classification;classifier
training set;news sources;online news;user interface;news items;power law;classification errors;user feedback;classifier trained on;classifier
language model;large scale;semi-structured;relevance models;relevance models;semi-structured documents;real-world
online product reviews;aggregation function;opinion mining;context dependent;product features
sentence retrieval;redundant information;similarity measure;query likelihood;tf-idf;retrieval techniques;retrieval model;novelty detection;modeling techniques
high-dimensional;image retrieval;high-dimensional;vector space;tf-idf;retrieval performance;classification problem;visual vocabulary;naíve bayes;retrieval techniques
topic segmentation;web page;segmentation algorithm
clustering algorithms;optimization framework;clustering algorithms;information retrieval applications;monte-carlo;optimization problem;document clustering;complex structures;traditional clustering
finding similar;finding task;natural language
active learning;real-world;classification tasks;active learning;text categorization;learning performance;classification algorithms
large document collections;retrieval strategies;stochastic sampling
retrieved documents;expert search;language models;search problem
ranking algorithm;term weights;random walk;trec collections;graph-based;random walk;tf-idf;information retrieval;information retrieval
web-search;test queries;query logs;search effectiveness;query logs;pseudo-relevance feedback;query type;query refinement
large number of;average precision;rank correlation
correlation coefficient;test collections;information retrieval research;threshold values;information retrieval

window size;language modelling;retrieval tasks;instance;retrieval performance;term dependency;term dependency
question-answer;question answer;answer ranking;question answer;link analysis;mechanism design
search log;web search;queries submitted to;web search
nn) queries;protein structure;nearest neighbor;high-dimensional feature space;dimensionality reduction;dimensionality reduction methods;efficient similarity search;query processing strategy;dimensionality reduction
relevant document;structured document retrieval;xml documents;semi-structured documents;2;4
informative samples;query rewriting;sponsored search;active learning;query rewriting;user queries;algorithm called;sponsored search;relevance model
web queries;internet traffic
sponsored search;major search engines;business model;long-term;relevant information;web search engines
specific topics;learning paradigm;higher order;web searching
web pages;relevant pages;relevant documents;content similarity;web page;web users
maximum likelihood estimation;language model;language models;feature selection technique;language modeling approach;em algorithm
temporal information;temporal dimension;multi-document summarization
spam pages;search engines;ranking systems;web pages
feature representation;filtering systems;filtering techniques;spam filtering
web page;multiple communities;query-specific;ranking performance;multiple topics
search results;navigation patterns;query suggestion;query suggestions;query logs;search sessions;query suggestion;query log;search engine
relevance feedback;data collected from
multiple users;retrieval performance;users' queries
search techniques
retrieved documents;ir systems;ir) systems;interactive information retrieval
text mining;detection algorithms;hidden markov models;mutual information;detection approach
test set;random sample;query log;real world;search evaluation;query sets;test collection
image retrieval;image indexing;image quality;web page;user study;visual features
document collection;query processing;clustering structure;classification;large-scale;text search;automatically generated;specifically tailored;cluster-based retrieval;retrieval effectiveness
active learning;relevant documents;decision boundary;active learning;irrelevant documents;low recall
relevant information;information discovery;multiple queries;exploratory search
search services;profile-based;information sharing;user modeling;explicit feedback;user modeling;context-based
search tool;search tools;search engine;retrieval precision;high-precision
nearest;pqrs;regular expressions; ;mno;regular expression;search engine
user experience;indexing techniques;cultural heritage;database
user interfaces;multimedia retrieval;cultural heritage;gps data;visual content
count-based;classification tasks;text categorization;text categorization;feature-based;compression scheme;classifier;extracting features from
duplicate detection;retrieval tasks;wikipedia articles;similarity search;retrieval performance;search interface
ranking algorithm;search term;web pages;geographic information;relevance ranking;tf*idf;major components;web-sites;user query;ranking schemes;web site;search engine;web page
search results;high dimensionality;search engine;document ranking;search engine
search task;search results;completeness;search topic;users' satisfaction;interactive information retrieval;ir systems;plans;ir techniques;domain knowledge;information retrieval;web searching;3;1;4;2;evaluation measures;interactive information retrieval;retrieval performance;web search engines;effectiveness measures
information retrieval methods;information retrieval
structured documents;document retrieval;information retrieval;probabilistic models;retrieval task;probabilistic models;information retrieval
web pages;personal information management;human activity;mobile devices;ad-hoc;semantic content;information retrieval systems
fuzzy set;spatial information;information retrieval;question answering;spatial reasoning;text documents;geographic information retrieval;qualitative reasoning
term weights;world knowledge;information retrieval;text retrieval;query routing;query expansion
multi-modal;data collections;multimedia information retrieval;machine learning methods;score distribution;data fusion;query-independent;retrieving relevant;fusion methods;multimedia retrieval;query-classes;query-dependent;takes place;high-level;low-level
data mining and knowledge discovery;knowledge discovery;acm sigkdd international conference on;data mining;kdd-07 conference;knowledge discovery;data mining;program committee;acm sigkdd international conference on

data-driven;data sets;data analysis;data mining
2, 8, 16, 23;large-scale;private information;social network;1, 22;12, 19, 24, 28;social sciences;social media;social systems;20, 21, 18;29;social networks;7;9, 26, 27, 30;problem-solving;4, 5, 10, 11, 13, 15, 25;information-seeking;time-series;individual privacy;3, 17;privacy-preserving data mining;interaction data;information systems;social network data;network structure;6, 14
real data sets;confidence intervals;data warehouses;expected values;statistical model;sales data;web traffic
training set;sparse data;large scale;data mining applications;sampling method;tree-structured;perform inference;rates;real-world;contextual information;markov model
large scale;real-world;latent factor models;linear regression;generalized linear models;simulation studies;statistical method;dyadic data;logistic regression;latent factor model;latent factors;local structure;model fitting
classification problem;data bases;classification;molecular biology;string data;market basket data;large data sets;hidden markov models;data streams
clustering;xml documents;clustering algorithm;xml data;data representation;data sets;xml documents;data records;databases;structural information;synthetic data sets;data mining problems
text mining;numeric data;product feature;digital camera;business intelligence;sales prediction;data set;product space;model parameters;product features;tensor product;image quality;sales data
structural learning;data analysis;question arises;causal modeling;temporal information;time series;data set;instance;graphical models;related algorithms;real world;temporal data;model selection
vector space;semantic relationships between;query logs;user behavior;query log;bipartite graph;semantic relations
batch-learning;ranked list;real-world;stream data;online learning;data stream;data streams;learning task
structural patterns;user-item;user ratings;recommender systems;collaborative filtering;rating matrix;data set;compare favorably with;matrix factorization;user preferences;formal model;similar items;similar users;iterative algorithm
search results;search space;total number of;scalability problems;partitioning scheme;similarity-based;similarity based
classification technique;classification;vector machine;time series;optimization model;multi-dimensional;correct classification;nearest neighbor rule;fold cross validation
clustering;feature space;data mining tasks;high-dimensional space;data points;distance metric learning;low-dimensional space;kernel function;learning algorithms;observed data;separability;learning algorithm;distance metric;kernel learning;linear projection;low-dimensional;dimensionality reduction
clustering;clustering algorithms;data record;density-based clustering;data-stream;data stream;stream data;data stream clustering;data streams;cluster structure;clustering quality;high-speed
document clustering;singular value decomposition;singular vectors;makes sense;svd;parallel corpus;cross-language information retrieval;clustering documents;similar topics;similar documents;document matrix;semantic space;latent semantic analysis;cross-language information retrieval
clustering;spectral clustering;cost functions;synthetic data sets;clustering problem;evolutionary clustering;clustering results;clustering result;clustering methods;long-term;clustering problems;clustering data streams;short-term;evolutionary clustering;temporal smoothness;clustering quality
temporal properties;optimization problems;temporal dynamics;constrained optimization;blog data;discover meaningful
feature vector;regression problems;hidden structure;parametric model;los angeles;em algorithm
stochastic processes;temporal data;temporal logic;temporal data mining;data mining
clustering;result sets;relevance feedback;result set;automatic query expansion;current web;log analysis;pseudo relevance feedback;query aspects;search engines;web search queries;search engine;ranking documents;interactive query expansion
hill-climbing;user preferences;heterogeneous sources;database;automatically extracting;real-world;conference on knowledge discovery and data;database records;feature-based;edit distance;distance measures;annotated data;databases;multiple sources;learning method;similarity measures
clustering;mining task;unlabeled data;classification;learned knowledge;learning algorithms;machine learning;classification performance;labeled data;classification results;produce high quality;labeled data from;real world applications
attribute values;training data;anomaly detection;real world datasets;categorical attributes;detecting anomalies;categorical datasets;probabilistic approach;unlabelled data;probability model
classification;feature selection method;worst-case;unsupervised feature selection;performance guarantees;data sets;feature selection;feature selection methods;text classification;theoretical guarantees
clustering;clustering;sufficient conditions;emerging area;constrained clustering;data mining research;data set;clustering solution;incremental clustering;np-hard;negative feedback
clustering;classification;regression problems;multimodal data;independent variables;cluster assignment;local minimum;data matrix;cost function;step procedure;wide range;customer behavior;complex data;dependent variable;collaborative filtering;prediction models;synthetic data
regularization;label information;unsupervised learning;uci datasets;von neumann;spectral clustering algorithms;distance metric;reproducing kernel hilbert space;operator;semi-supervised;random walks;learning framework;weighted graph
clustering;domain experts;domain ontologies;mining framework;data mining results;classification;large-scale;real-world;hierarchical clustering;generic framework;association rules mining;data mining;classification results
modeling assumptions;markov random field;generative/discriminative;unlabeled data;hybrid approach;text classification tasks;semi-supervised learning;semi-supervised classification;semi-supervised learning methods;labeled data;semi-supervised;labeled and unlabeled data;classification task;semi-supervised classification
data set;large-scale;small groups;community structure
text corpus;corpus based;algorithm called;feature-based;bursty features
clustering;instances;set covering;rule set;dual problem;clustering problems;rule learning;minimum number of;graph-based;decision rules;instance;distance constraints;frequent pattern;data mining;model complexity;graph coloring;benchmark datasets;data mining problems;generic algorithm;frequent pattern mining
clustering;data-driven;constrained) clustering;data points;application requirements;data structure;cluster model;real datasets;constrained clustering;clustering result;sensor networks;clustering methods;clustering problem;clustering;objective function;data-driven
trajectory patterns;spatio-temporal;pattern mining;real data;moving objects;sequential pattern mining
prediction problem;data mining method;query response time;data mining methods;database;large scale;convergence rate;margin based;image database;max margin;data mining;output variables;data mining problem;learning framework;highly scalable;max margin;multimedia database
binary data;low complexity;special properties;makes sense;real data;frequent itemsets;entropy measure;pattern discovery;tree-based;dependency structure
clustering;text mining;classification;unsupervised clustering;clustering method;data sets;optimal number of;clustering performance;chi-square;distance-based;databases;latent semantic indexing;citation graph
graph connectivity;large-scale;language models;likelihood ratio;topic detection;document collections;graph analysis;high-level;citation graph;trend analysis
quality guarantee;dual problem;construction algorithm;data set size;error metric;data mining;space budget;efficient construction
search space;synthetic datasets;database;correlation coefficient;correlation mining;great success;application domains;graph databases;candidate set;scientific domains;graph data;heuristic rules;correlation mining
spam detection;high-precision;naive bayes;document term;document representation;feature selection;text classifiers;length normalization;classifier;high precision
outbreak detection;water distribution;real-world;outbreak detection;share common;greedy algorithm;cost-effective;blog data;times faster than
high potential;mining algorithms;real-life applications;closed patterns;equivalence classes;equivalence class;frequent itemsets;emerging patterns;chi-square;statistical significance;test statistics;depth-first search
estimation accuracy;synthetic data;microarray data;gene expression;random projections;efficiently computing;dimension reduction;projection matrix
clustering;effectively exploit;clustering results;clustering algorithms;boosting framework;pairwise constraints;large number of;data clustering;data clustering;spectral clustering;clustering algorithm;data representations
software development;pruning strategy;temporal patterns;pattern mining;application server;multiple times;case study;real datasets;mined patterns;mining algorithm;sequential pattern
data objects;clustering;probabilistic framework;markov random fields;clustering algorithms;probabilistic model;multi-type;large number of;relational clustering;exponential family;relational clustering;semi-supervised clustering;web mining;cluster structures;clustering tasks;relational data;graph clustering
real data;large datasets
text mining;text data sets;topic models;optimization problem;labeling problem;mutual information between;text collections;topic model;kullback-leibler divergence;user study
database;human relevance judgments;topic model;finding task;language model
clustering;clustering algorithms;attribute-based;real datasets;graph) data;complementary information;prior knowledge;clustering methods;real world entities;cluster analysis;objective function;relationship data
unseen data;probabilistic graphical model;interesting patterns;large document collections;multi-scale;model generation
real-world datasets;decision trees;exhaustive search;decision tree;ranking function;constraint-based;decision tree learners
high similarity;association analysis;case study;data sets;biological data;prediction algorithms;protein interaction networks;association analysis;protein interaction networks
ranking algorithm;movie database;recommender systems;collaborative filtering algorithm;search tools;search queries;filtering techniques;ranking method;ranking methods
clustering;resource consumption;classification accuracy;high quality;user interests;multiple topics;topic tracking;topic detection;user-feedback;operating parameters;higher quality;user feedback;user study
clickthrough data;web corpus;user behavior;bayesian approach
clustering;clustering problem;total number of;classification;mixture model;generative process;evolutionary process;mixture models;generative models;topic hierarchy;19;algorithm performs;real data;clustering algorithm;theoretical results;mixture models
probabilistic generative model;dirichlet prior;mixture model;variational bayes;meta-data;topic classification;specific properties;multiple topics;prior distribution;knowledge discovery;mixture model;model parameters;dirichlet distribution;generation process
clustering algorithms;data sets;user's preferences;hierarchical clustering;recommendation systems
greedy strategy;active learning;data mining applications;user behavior;active learning methods;relevance feedback;online advertising;document retrieval;margin-based;positive class
text documents;document similarity;analysis task;databases;document databases;language-modeling approach;likelihood ratio
concept-based;text categorization;statistical analysis;term frequency;quality measures;error rate;graph representation
attribute values;training examples;real-world data mining applications;cost-sensitive learning;data mining and machine learning;data acquisition;cost-sensitive;uci datasets;total cost
clustering;spectral clustering;web pages;data source;synthetic data;molecular biology;learning algorithm;constrained clustering;heterogeneous data sources;optimal combination of;semi-supervised clustering;term frequencies;problem setting;real-world;eigenvalue problem;total cost;clustering approach;graph clustering;clustering method
em) algorithm;labeled samples;test set;test data;mixture model;spam filtering;semi-supervised learning;labeled training data;unlabeled samples;classifier;data sets;test-set;expectation maximization;real-world data sets;selection bias;conditional independence;learning method;unlabeled set
multi-dimensional;data points;multi-dimensional data;test statistic;data set;change detection;data sets;statistical test
special case;document summarization;hypothesis generation;named entities;data set;graphical models;graph matching;link analysis;query terms
parameter-free;real datasets;user-defined;large graphs;dynamic networks;diverse domains;information theoretic
intrusion detection;detecting anomalies;weighting schemes;algorithm learns;computational overhead;false alarms
high-dimensional;clustering algorithms;data dimension;sparse data;labeled instances;semi-supervised clustering;clustering method;unsupervised clustering;instance;clustering performance;semi-supervised clustering;supervision;real-world data sets;dimensional data;semi-supervised;expert knowledge;semi-supervised
greedy heuristics;dynamic programming;exhaustive search;social interactions;real-world;social network;social networks;np-hard;community structure
source code;machine learning problems;linear support vector machines;conditional random fields;risk minimization;logistic regression;data-locality;highly scalable;times faster than
high-quality;query pattern;social network;large graphs;exact match;pattern matching;indexing methods
proximity measures;clustering;graph mining;computational efficiency;directed graphs;link prediction;mining tasks
regression trees;regression trees;predictive accuracy;real data sets;decision tree algorithms;pre-computed;regression models;highly scalable
data mining;pattern-based;12;database;databases;emerging patterns;data mining;real world;data distribution
simulated annealing;genetic algorithms;neural networks;optimization problems;stochastic gradient descent;data mining and machine learning;local minimum;gradient descent;machine learning;gradient descent;bayesian networks;target function;data mining;decision model;secure multi-party computation;gradient-descent
text mining;database;text streams;database research;data set;data sets;multiple streams;patterns discovered
probabilistic framework;hidden variable;model generalizes;latent space;data types;component analysis;large number of;real-world applications;principal component analysis;prediction task;dimensionality reduction
real data sets;data mining problem;multi-criteria decision making;multidimensional space;application scenarios
clustering;problem remains;classification;classification;large-scale;rare-class;class distributions;data sets;data set;supervised learning algorithms;rare class;real-world data sets;accuracies;rare classes;support vector machines;class distribution;clustering
graph partitioning;viral marketing;similarity measure;real datasets;algorithm called;clustering algorithm;clustering algorithm;network clustering
counterpart;learning process;classification performance;classification problem;data samples;multiple labels;label space;multi-label learning;feature subspace;algorithm called;multi-label classification;shared subspace;multi-label;classifier;synthetic data
clustering;motion capture;time series;time series;novelty-detection;rule-discovery;data mining algorithms
kernel learning;feature space;discriminant analysis;multi-class;quadratic programming;kernel function;kernel matrix;quadratic programming;kernel discriminant analysis;equivalence relationship;interior point methods;computationally expensive;learning problem;kernel methods;binary-class;kernel matrices
high-dimensional;discovered patterns;summarization method;data mining techniques;similarity measure;visual patterns;semantically meaningful;meaningful patterns;visual features;text data;image data;frequent itemset mining;frequent itemsets;clustering scheme;image databases;real images;discover meaningful;metric learning
3, 23, 24;question answering
data structures;graph model;statistical method;user queries;search result;search engines;domain independent;search engine;web crawling;result pages
vertical search;database;large collections of;extraction accuracy;high accuracy;generated dynamically;wrapper generation
data record;semantic labels;natural language processing techniques;probabilistic model;text fragments;tree structure;text content
behavioral patterns;interaction graphs;interaction graphs;diffusion model;link prediction;evolving networks;event-based
archived data;application area;data mining techniques;vector machine;limited bandwidth;lessons learned;data mining;analyzing data;memory resources;dynamic events
production systems;knowledge sharing;social network;markov decision processes;social networks;learning framework
link formation;class label;relational models;data pre-processing;statistical models;knowledge discovery;pre-processing;relational data
specific applications;data values;data analysis;explicitly represented;domain experts;anomaly detection;outlier mining;large data sets;real data sets;correlation analysis;disguised missing data;missing values;heuristic approach
end-users;sample size;data mining techniques;evaluation criteria;controlled experiments;large amounts of data;variance reduction;controlled experiments
network size;data updates;large-scale;real-world;learning algorithm;distributed classification;information gathered;general form;dynamic networks;local classifiers;distributed classification
nearest-neighbor;wide range;conditional distribution;regression-tree
complex structure;large-scale;real-world;accurately predict;classification method;operator;real world;kernel based
large scale;pattern discovery;language-independent;relevant pages;seed set
entity extraction;conditional random field;real-world;image-based;document images;large collections of;information retrieval tasks;modeling techniques;data mining approach;data mining;human interaction;image space;question answering;named entities;multi- channel
clustering algorithms;gaussian mixture model;training phase;classification;cluster model;expectation maximization;originally designed;data processing;data streams
detection algorithms;multi-dimensional;case study;data cube;change detection;large data sets;large datasets;data mining;baseline models
tracking algorithms;data mining application;training data;unlabeled data;machine learning methods;large amounts of;labeled training data;conditional random fields;mobile objects;domain constraints;learned model;sensor networks;sensor network;data mining;em algorithm;semi-supervised;semi supervised learning
mining results;domain experts;frequent patterns;large volume;temporal information;mining temporal;event data;interesting patterns;temporal data
large scale;clinical practice;machine learning techniques;real world;machine learning;classification algorithm
selection problem;takes into account;machine learning;sample set;stock data;competitive learning;real-world
malware detection;classification;decision tree;vector machine;naive bayes;fp-growth;malware detection;rule based;data mining based;classifier
world-wide web;conflicting information;web sites;search engines;information providers;algorithm called;web site;problem called;information source
knowledge discovery in databases;data mining;data mining
access methods;access methods;database size;disk access;similarity search;commercial dbms;similarity search
multi-valued;approximation techniques;historical data;data values;historical information;real data sets;sensor networks;discrete cosine transform;error metrics;sensor nodes;limited resources;low-level
clustering structure;classification;anomaly detection;scalable solution;data mining;network traffic;support vector machines;clustering;large amounts of;hierarchical agglomerative clustering;anomaly detection;hierarchical clustering algorithms;large data sets;intrusion detection system;data points;intrusion detection;network resources;hierarchical clustering;data records;clustering analysis;training set;support vector machines;training process;data set;boundary points;clustering analysis;classification algorithms
data structures;additional cost;cost model;data structure
query evaluation;sql queries;probabilistic model;optimization algorithm;efficient query evaluation;query semantics;information retrieval;probabilistic databases;monte-carlo simulation;arbitrarily complex;approximation algorithm;data complexity
data analysis;spatial data;database;query workload;scientific data;scientific applications;long-running;storage capacity;36;relies heavily on;data records;multi-attribute;storage manager;storage management;hard disk
increasing demand for;application integration;intelligent agents;programming languages;data management;web services;databases;information overload;relational databases;fully connected
database;relational model;relational operators;1;relational" dbmss;relational database
location dependent;massively distributed;large scale;digital information;ad-hoc;location information;sensor networks
mathematical models;higher-level;database technology;information services
prediction accuracy;product recommendation;information filtering;recommender systems;database;filtering techniques;real-world;trade-offs;recommendation algorithms;content information;web logs;web site;similarity information;digital library;databases;end user;user behaviors
taking into account;database;search techniques;database applications;similarity metric;relevant objects;search sessions;relevance feedback;similarity search;search region;database objects;relevant information;benchmark datasets
query evaluation;synthetic data;object-relational;base relations;object-relational database;regular expression;databases;operator;recent database;join operator
domain experts;end-user;database selection;document collections;modeling techniques;document retrieval;collection selection;document-type;database selection;logical database;online databases;natural language;world wide web;trec data;textual data;results merging;databases;domain-specific;high level;database;relevance judgments;distributed environments;search engine
type information;recursive queries;synthetic datasets;xml data;xml queries;real-life
query evaluation;xpath queries;main-memory;query evaluation;worst case;query processing algorithms
database;selection process;xml documents;data stored in;xml document;database engines;database engine
query execution;increasing number of;execution-plan;query optimizer;xml data;expected cost;relational database systems;xml views;relational databases;relational data
business data;ubiquitous computing;database servers;data access;data confidentiality;databases;encrypted database;data access
preserving privacy;database research;database systems;databases
database;original data;high probability;database relations;unique characteristics;desirable properties;relational databases;real world applications;relational data
parameter space;parameter values;cost functions;plans;query optimizer;query plan;multiple times;parametric query optimization;piecewise-linear;piecewise linear
clustering;query optimization;complex queries;optimizer;plan selection;query optimization;computationally intensive;similar queries;plan choices;future queries;query optimizers;classifier
query optimization;cost functions;main memory;database operations;database;main-memory;database research;access patterns;cost models;operator;disk-based;estimation problem
query results;ad-hoc;emerging applications;streaming data;data streams;query processing techniques;continuous queries;ad hoc queries
monitoring applications;application area;data processing;data management applications;human operators
xml stream;xquery language;query processor;xquery processing;data (streams;data streams;xml stream
instances;database;xml databases;data structures;path queries;real world data sets
approximation algorithms;search space;synthetic data sets;regular expressions;regular expressions;database;large databases;multi-dimensional data;index structure;tree nodes;spatial locality;query string;databases;tree-based;sampling-based;application scenarios
join algorithm;structural joins;structural join;join algorithms;path expressions;xml queries;structural joins;xml documents;structural relationships;document structure
instance;data points;skyline queries;skyline queries
query retrieves the;nearest neighbor;line segment;dataset characteristics;analytical models;query processing;nearest neighbor search
join algorithm;object-relational;data mining applications;join processing;join algorithms;generic framework;similarity joins;user interaction;quality guarantees
database technology;database systems;modeling technique;match-making;15;partial order;preference queries;preference queries;soft constraints;standard sql
data mining;production systems;multi-dimensional;data analysis;high-quality;potentially infinite;data cube;regression analysis;dynamic environments;multi-dimensional;multiple times;stream data;data streams;time-series
traditional databases;massive data streams;multiple data streams;basic operations;massive data streams;large scale;approximation method;databases;data stream;sensor networks;stream data;data processing;data streams;approximation technique;large quantity of
algorithms for computing;market basket;frequent itemsets;network monitoring;data streams
data structure;synthetic data;fourier transform;stock market;sliding window;time series;making decisions;efficient computation;grid-based;data streams;real world;direct computation
search results;short queries;initial query;inverted files;web search engines;search engines;query processing;search engine users;result pages
fully operational;commercial search engines;web pages;temporal locality;web search engine;large-scale;data manipulation;large sets of;web search engines;user queries;data preparation;data tables;disk accesses;parallel execution;data storage;external-memory;disk-resident;data access
higher precision;database contents;critical task;hidden web;search interfaces;content summaries;database;query interface;distributed search;web-accessible;hierarchical classification;document frequency;text databases;databases;statistical summaries;selection algorithm
distance measure;indexing techniques;time series;database
index structures;large number of;query cost;query processing;random accesses;performance gain;index nodes;total cost
database;ad-hoc;query workload;storage model;storage model;reconstruction algorithms;storage manager;high cost
world wide web;query execution plans;selectivity estimation;selectivity estimation;memory constraints;path expressions;xml data;data exchange;internet scale;query feedback;workload-aware;queries over xml;summary statistics;real data sets;estimation accuracy;path expression
data mining;order statistics;selectivity estimation;query optimization;user interfaces;application level;real datasets;database server;database;underlying data distribution;rates;performance guarantees;databases;estimation algorithms;query result;data distribution
forward selection;synopsis structures;depends crucially on;path expressions;xml data;real-life data sets;data elements;graph-structured xml;heuristic algorithm;optimization problem;selectivity estimates;correlation patterns;specification language;np-hard
access control;tree-structured data;xml data;data representation;data items;data item;control problem
multi-level;query evaluation;xml element;control policy;twig queries;xml data;data exchange;access control;xml documents;user queries;access control model;security level
rule-based;security applications;semantic relationships
world-wide web;8, 11;data source;large-scale;real-world;sampling-based;computational resources;data items;change detection;data sources;data warehousing;great potential;data-intensive
real-world;dynamically changing;data items
search results;internet search;distributed search;formal model for;internet search;bandwidth consumption;query frequency;internet-scale;network topology
web portals;bandwidth consumption;web technologies;data delivery;application servers
relevant content;view maintenance;dynamic content;data stored in;databases;application programs
database systems;business data;database server;data access;performance gains;data-intensive;application server
large numbers of;high quality;data warehouses;data cleaning;real datasets;data warehouse;false positives;duplicate elimination;similarity functions;edit distance;domain-specific;domain independent;multi-attribute;real world
hierarchical organization;schema mapping;target values;semantically meaningful;design choices;relational schemas;source data;semantic relationships;high-level;source schemas;web data
data warehouse;database applications;schema matching;user effort;semantic correspondences;data sources;real-world;schema matching
data center;data repositories;search space;scientific databases
life sciences;plans;secondary structure;large volumes of;database research;query language;query evaluation;query performance;data sets;specific problem;protein sequences
traditional databases;xml data;case study;query operations;database;uncertain data;probabilistic xml;information extraction;probabilistic data;data obtained from;relational systems;data base;databases;natural language;data management;probability model
web directories;web pages;decision tree;text classification tasks;cache performance;test instances;naive bayes;instances;data sets;training instances;statistical pattern recognition;memory requirement;sequential scans;text classification;support vector machines;low-dimensional;classification algorithm
information discovery;keyword queries;plans;database schema;np-complete;greedy algorithm;execution plan;relational databases;keyword search;subexpressions;plan execution
mining results;high level;synthetic datasets;input data;association rule mining;high degree of;privacy concerns;mining association rules;mining process;data privacy;data mining
ctr;workflow execution;business logic;business process;temporal constraints;transaction logic;constraint logic programming;resource allocation
database systems;main-memory;transaction execution;intensive workloads;wide range;disk-based
transaction models;programming languages;web servers;fine-grained;mobile agent;programming language
dimension hierarchies;drsn, mrb, ks;fact table;data warehousing;query processing;business intelligence applications;real world applications;relational database management system;path-based
concurrency control;large volume;multi-dimensional;serializability;database;large volumes of data;data warehouse;on-line analytical processing;concurrency control mechanism;data sources;data warehouses;source data;control mechanisms;batch processing;multi-version
minimum description length;data analysis;data cube
base table;data cube;similar behavior;aggregate functions;locally optimal;algorithms for computing
accessing data;naive algorithm;aggregation algorithm;database;aggregation methods;disk access;disk accesses;computation model;data set;memory requirement;buffer size
database;user defined functions;db2 udb;incremental maintenance;view maintenance;aggregate functions;incremental maintenance
data arrives;efficient algorithms to;rnn) queries;nearest neighbor;exploratory analysis;decision support;traffic monitoring;error guarantees;data sets;data stream;exact computation;data stream model;data streams;nearest neighbor
large volume;xml-document;aggregation algorithm;query optimization;tree pattern;document filtering;tree patterns;space budget;xml documents;space requirements;matching process;tree-pattern;upper-bound;data dissemination;specification language
sql queries;data exchange;xml views;xml document;relational database;relational data
multi-version;simulation study;takes into account;transaction throughput;rates;transaction management;data delivery
operator;production systems
database design;total cost
resource consumption;database;storage model;cryptographic techniques;log-structured;wide range;memory consumption;limited resources
positive results;large-scale;database;distributed database systems;data layout;databases;perform poorly;execution times;database vendors;online transaction processing
database access;multi-channel;plans;gather information;databases
data repository;meta-data;data quality;data repositories;development process;data gathering
life sciences;specially designed;context-sensitive;data sources;development environment;computing resources
lock;index scans;query planning
search results;score function;aggregation algorithm;join operations;join algorithms;database applications;trade-offs;database;join operators;retrieval systems;commercial database systems;search engines;query processing;operator
memory-intensive;database systems;input data;database administrators;database applications;database queries;commercial database systems;database application;database;commercial systems;operator;configuration parameters;increasingly complex;memory management
xml databases;database research;database technology;real-world scenarios;database;textual features;query processors;query types;xml data management;xml query languages;xml documents;data analysis;xml database;query processor;xml query processing;data management;ad hoc queries
extended abstract;information sources;semi-structured;real-world;xml documents;databases;data integration
partial orders;customer satisfaction;application integration;optimizer;large-scale;search engines;query answering;operator;preference queries;high-level;kie;standard sql
information management;data management;information management
end-users;access logs;analysis reveals;web sites;user requests;web log mining;clustering algorithm;log data;web site's;query expansion
web pages;database systems;database;electronic commerce;driven web applications;web application;web applications;network latency
web portals;domain knowledge;web portal;web site;web page;automated techniques
database technologies
mobile agents;road network;mobile agent;data management system;traffic data
web-based;life sciences;graph structure;data collections;database applications;data sets;document search;large graphs;data sources;content management
traffic information;active functionality
object-relational;database management systems;database management system
relational databases;complex queries;keyword-based search;answer ranking;databases
data objects;information systems;data sources;autonomous systems;data interchange;distributed data sources;relevant information;xml technologies;specification language;distributed systems;global schema
active learning;integrating data from;execution model;similarity functions;selecting informative;record pairs;domain-specific;cluster-based;multiple sources;manual effort
data mining results;business intelligence;acm sigkdd;data mining;decision making;real-world;domain-driven;intelligent systems;international workshop on;enterprise applications;domain driven data mining;actionable knowledge;domain driven data mining;data-driven;real-world;pattern mining;business processes;information technology;social security;data mining techniques;international workshop on;knowledge discovery;domain driven data mining;information technology;business users;solving complex
domain specific;open source;unstructured information;data mining standards;data mining standards;streaming data;international workshop on;predictive model;streaming data;kdd workshop;data mining standards;markup language
point-based;suffix tree;knowledge representation;interval data;mining algorithms;pattern mining;time series;general concept;partial order;interval-based;noisy data;multivariate data;temporal data;sequential pattern
workshop report;inductive databases;data mining;knowledge discovery in databases;knowledge discovery;constraint-based;international workshop on;inductive databases;knowledge discovery;machine learning;query languages;international workshop on
high-throughput;medical research;data mining
logical reasoning;fall short;hypothesis generation;expression data;observed data;gene expression analysis;gene expression data
nearest-neighbor search;data structure;classification;data structures;large data sets;nn search;similarity search;databases;classifier
association rule mining;data mining technique
common interests

xml databases;data resources;data integrity;copyright © 2007 john wiley & sons;access control
data source;heterogeneous sources;semantic correspondences;copyright © 2007 john wiley & sons;data sources;information integration;grid environment
end-user;copyright © 2007 john wiley & sons;data replication;managing data;data transfers;data management
copyright © 2007 john wiley & sons;distributed database;software tools;data model;data centers
implementation issues;distributed computation;copyright © 2007 john wiley & sons;distributed environments;access patterns;temporal data;distributed systems
clustering;clustering algorithms;parameter-free;spectral clustering;clustering technique;real-world;clustering method;minimum description length;point set;input parameters;fully automatic;information-theoretic;clustering algorithm;real-world datasets
semantic annotation;semantic classes;frequent patterns;data mining applications;efficient algorithms to;selecting informative;structured information;similar patterns;semantic relations;prior knowledge;frequent pattern;semantic annotations;frequent pattern mining
network datasets;data mining tool
process model;markov process;unsupervised learning;real-world datasets;human activity;traffic monitoring;time-series;time-series;bayesian framework;threshold-based;traffic data
clustering;data mining results;margin;theoretical results;spectral analysis;frequent sets;instances;chi-square;statistical tests;high-dimensional 0--1 datasets;margins;markov chain;data mining algorithms
data-storage;data rates;data storage
complex data;information integration;information integration;usage scenarios;data placement
database;specific algorithms;data-centric
space budget
web pages;optimizer;web queries;query interface;relational operators;relational tables;complex queries;cost-based;text documents;execution engine;query results;directed graph
semantically related;indexing techniques;query language;query answers;information-retrieval techniques;search engine
content distribution;dynamically changing;push based;streaming data;instances;communication) cost;sensor data;data streams;real world;data items;traffic data;web users
query patterns;frequent patterns;candidate patterns;xml data;query response time;xml queries;management systems;algorithm called;efficient retrieval of
clustering;clustering algorithms;summary statistics;algorithm requires;clustering problem;large volumes of data;scalability issues;clustering process;data stream;data stream clustering;evolving data streams;clustering algorithm;synthetic data sets;large volume
trend detection;synthetic data;frequent patterns;regression analysis;pattern growth;temporal patterns;frequency counting;pattern mining;real dataset;data streams;data streams;temporal patterns
systems support;database systems;data types;text data;performance degradation;databases;database engines;text processing;standard database
database systems;large number of;user query;query processing;databases;relational databases;distributed environment
query execution;document collection;pruning techniques;ranking techniques;response times;cluster-based;inverted lists;million web pages;search engines;database researchers;search engine;ranking functions;hyperlink structure;term-based;web search engines;query terms;query processing
tree structures;instances;query engines;main memory;xml query language;xml documents;tree structure;model checking;native xml;xml document;databases;query languages;path queries
received increasing attention;ad hoc;xpath queries;xml tree;xpath expression;xml queries;operator;xpath queries;optimization techniques;np-hard
instances;xml queries;xml query language;xml documents;instance;core xpath;xml document;operator;tree pattern queries;path queries
search techniques;document collection;xml tags;relevant documents
xml storage;rewriting rules;primary key;data sets;xml document;functional dependencies;semantic information;semantic constraints
xml data
query evaluation;main-memory;memory requirements;static analysis;xml documents;original document;databases
structural joins;querying xml documents;simple queries;inverted lists;tree-based;typically involves;cost model;information retrieval;native xml;query processing;query plans;pattern matching;xml query processing
concise representation;xquery queries;plans;efficient query evaluation;xml data;tree pattern;xquery expressions;tree patterns;xml database;optimize queries;xml query language
xml streams;query engines;xml data;query language;memory requirements;finite-state;path queries;highly scalable;linear scan;large amounts of data;query processing techniques;xml trees;tree automata
large numbers of;xml filtering;post-processing;xml elements;high-volume;query processing
holistic twig join;structural joins;algorithm performs;xml queries;xml documents;twig pattern;worst case;xml document;cpu cost;selection predicate
query evaluation;information sources;highly variable;execution plan;support queries;join operators;instances;rates;remote sites;operator;input streams;join queries;join operator
continuous query;execution strategies;analytical models;query operators;shared execution;data streams;operator
answer quality;query plan;data stream;data sources;data stream;rates;query plans;push-based
database systems;overlay networks;database research;relational queries;distributed queries;distributed environments;distributed systems;database technologies;database query processing;simulation results;query engine;massively distributed
optimization technique;query plan;rates;data stream management systems;computing resources;resource allocation
database;moving-average;query language;data model;instance;query languages;optimization techniques
human interactions;web accessible;web interfaces;search interfaces;search engines;multiple sites;web interfaces;user queries;search engines;matching accuracy;produce high-quality;databases;semi-automatically;allowing users to
cost-based query optimization;dynamically changing;estimation error;attribute sets;query feedback
world wide web;web pages;web documents;link analysis;link structure;high quality;user behavior;keyword-based;search engines;valuable information;query result;web page
selection problem;selection problem;web content;database servers;web caching;data freshness;sheer volume of;adaptive algorithm
temporal locality;query operations;cache performance;index structures;main memory;tree-structured;stream processing;concurrent updates;search performance;times higher;fixed-size
database operations;database systems;query workload;main-memory;storage model;algorithms for computing;index scans;storing data;data storage
high-dimensional;synthetic data sets;main memory;database systems;database;data mining applications;database;instance;data mining techniques;data volumes;data mining;operator;dimensional data;limited resources;low-dimensional;data intensive
multi-dimensional;data analysis;pattern recognition;data points;22;multi-dimensional data;data clustering;high-dimensional spaces;data preprocessing;connected components;signal processing;real world
clustering algorithms;vector space;summarization method;clustering structure;application domain;data bubbles;sufficient statistics;hierarchical clustering;data set;vector data;summarization methods;clustering algorithm;metric spaces;clustering quality
clustering;online aggregation;heavy hitters;hierarchical data;applications including;decision support;total number of;data stream;network management;hierarchical structure;stream data;randomized algorithms;data streams;heavy hitters
11;25;multi-dimensional;data distributions;data cube;data structure;aggregation methods;multiple dimensions;tree structure;2;data warehousing;6
rewrite rules;query optimization;optimizer;database systems;coarse-grained;database;database applications;database server;optimization problem;implementation details;coarse-grained
main memory;sliding window;join processing;continuous queries over data streams;sliding windows;processing cost
nn) queries;nearest neighbor;moving objects;databases;nearest neighbor queries;query result
database;main-memory;query processors;sort order;xpath processing;relational dbms
data feed;parameter values;statistical techniques;data analysis;business decisions;database;data cleaning;data quality problems;decision support;data quality problems;data quality;ad-hoc;service providers;local region;databases;network traffic;poor quality;network data;data feeds;problems arising
integrity constraint;data mining-based;semantically meaningful;databases;data quality problems;data quality;database;machine learning techniques;learning algorithm
handle arbitrary;pattern detection;fall short;meaningful patterns;synthetic datasets;query processing engine;algorithms require;long-range;stream mining;9, 5, 22;interesting patterns;communication bandwidth;sensor devices;user intervention;long-term;auto-regressive;modeling method;limited resources;data gathering;resource consumption
data sharing;mapping composition;mapping language;optimization methods;data sources;answering queries;data stored in;query processing;semantic mappings
information systems;xml schemas;dynamic environments;query capabilities;view definitions;mapping generation;data sources;explicitly models
materialized view;lock;view maintenance;rates;materialized aggregate;control mechanisms;concurrency control
multi-dimensional data;higher throughput;main-memory;database;frequent updates;query performance;direct access to;enormous amounts of;index nodes;data management applications
similarity searches;discrete data;high dimensional;metric spaces;continuous data;retrieval performance;sequence data;sequence databases;linear scan;discrete data;tree construction;similarity searches;indexing technique;indexing methods;data distribution
input data;xquery engine;xml data;query language;xml queries;xml documents;xml) data;performance tradeoffs;xml document;xml query language;relational data
clustering;query optimization;optimization technique;sufficient conditions for;access methods;data warehouse;avoid redundant;dimension hierarchies;aggregate queries;fact table;optimization techniques
operator;high accuracy;query optimizer;cardinality estimation
query optimization;access paths;optimizer;data samples;automatically discovering;large database;clustering techniques;query processing;data mining;sample sizes;functional dependencies;query-optimization;relational data
storage devices;cache performance;main memory;computing platform;data layout;relational data;large datasets;physical characteristics;data placement;storage systems;persistent storage;database management systems
query execution;lower bound;execution model;memory requirements;data centric;computing devices;evaluating queries;data management
database systems;database;ibm db;specific characteristics;query plans;access patterns;storage manager;storage management;transaction processing
content-delivery;query execution;web applications;database;query processing capabilities;enterprise application;query execution plans;database caching;high volume;database table
query logs;user behavior;database
oltp systems;information systems;transaction processing
ranking queries;join results;join algorithm;database engine;query processors;join operators;query operators;scoring function;queries efficiently;relational databases;join queries;execution plans
parameter space;cost functions;plans;query optimizer;query plan;parametric query optimization;parametric query optimization
parameter values;database;optimization problems;index structures;efficient approximation;aggregate functions;provide answers;desired accuracy;objective function
query retrieves the;access method;unique features;index structure called;takes into account;spatio-temporal;static data;moving objects;cost models;optimal performance
search space;real-life applications;range search;euclidean spaces;spatial database;query processing in;spatial networks;databases;spatial queries;spatial network;nearest neighbors
query optimization;approximate algorithm;applications including;storage space;real datasets;spatial datasets;density based;real world datasets;high accuracy;exact answers;np-hard
query optimization;query evaluation;plan selection;integrity constraint;6;query results;processing queries
fine-grained;operator;processing requirements;data stream;high-volume;data streams;operator
text-search;keyword queries;relevance ranking;free-form;real data;relational database management systems;query-processing strategies;keyword query;keyword search over;ir-style;multiple relations;information retrieval;relational databases;querying capabilities;structured data;keyword search
meta data;formal description;data integration;database schema
large numbers of;information sources;data providers;real-world;data sources;distributed environment;distributed systems;processing queries
database;sampling techniques;reduction techniques;approximate query processing;categorical data;aggregate functions;aggregate function;base data;numerical data;numerical attributes;relative error;robust estimation;random sampling
xml tree;control policy;access control;real datasets;rewriting rules;xml documents;instance;cryptographic techniques;data owner
matching score;dynamic programming;biological sequence;data set;search tools;data sets;query sequence;tree index;search algorithm;biological sequences
privacy guarantees;privacy-preserving;access-control;real-life;privacy-preserving;content providers;content providers
large volumes of;historical data;data compression;specifically designed for;query workloads;data warehouse;sql queries;compression techniques;raw data;compression technique;query performance;data warehouses;data management;large amounts of data;performance gains;disk storage;relational tables;relational data
commercial applications;location data;real datasets
base table;complex queries;optimizer;query optimizer;poor performance;sql server;statistical information;execution plans;cardinality estimation;plans
clustering;clustering;multi-dimensional;database systems;access methods;database;relational tables;block based;minimal overhead;query processing;index scans;table scans
graph structures;implementation issues;structural features;database;multi-dimensional data;query results
database systems;database;replication;source data;databases;relational data
data streams;xml query language;xquery engine
xml documents;xml schemas;database;object-relational) database;data model for;object-relational database;xml document;fundamental problem;xml db;relational data;xml schemas;xml schema;increasingly popular
traffic monitoring;data management;mobile users;real world;content providers;data management system
xml documents;web service;web browsing;web services;higher level
data warehouses;database technology;database
data warehouses;data warehouse;database;mobile devices;distributed transactions;mobile users
web-based;xpath queries;xml data;xml indexing;user interface;query processing;relational database;xml indexing;efficient sql;query engine
query rewriting;completeness;open-source;database;type-checking;xml schema
schema translation;instances;web data;traditional database;semistructured data;web data
mobile user;spatial data;data providers;spatial information;dynamically changing;mobile objects;location-based services;location-based;mobile users
hierarchical classification;data mining techniques;threshold based;statistical information;query processing;data integration
relational model;query optimizations;standard sql;scalability problems;window functions
data cube;data cube;equivalence classes;equivalence relation;efficient computation;data warehouses;user interface;data structures;incremental maintenance;answering queries;data warehousing;data warehousing and olap;operator;query answering
index tuning;database;ad-hoc queries;database tuning;dynamic environments;cost model;query-driven;query processing;addressing this problem;index tuning
network bandwidth;large distributed;individual nodes;semantic caching;application domains;query response times;internet-scale;answer queries;high-volume;sensor network;internet-scale;computing power;query routing
simulation data;data generation;large-scale;spatio-temporal;time series;access patterns;relational dbmss
semantic web;database technology;semantic web
data grid;data grid;web service;structured data;grid services;storage systems;databases;database access;data management systems;metadata management
web applications;web applications;web services;data-centric
data management techniques;query processing;sensory data;data processing;database;query optimization;wide range;information processing;streaming data;data acquisition;management systems;data replication;sensor networks;data management;sensor network;sensor networks;network data;data storage;energy-efficient;data management;sensor data
event types;sensor networks;vehicle tracking;sensor networks;sensor node
multi-query optimization;multiple queries;wireless sensor network;data acquisition;cost model;aggregation queries;optimization algorithms;cost-based;sensor networks;sensor data;sensor network;subexpressions
sensor devices;sensor networks;sensor network;metadata management;real world;inference rules;physical objects
protocol called;simulation study;energy efficiency;large networks;scheduling algorithm;sensor networks;7;data delivery;data aggregation
update transactions;large scale;concurrency control mechanism;concurrent execution;continuous queries;data management;sensor data
high quality;physical environment;communication bandwidth;environmental conditions;power consumption;query results
24;13;15;information filtering;rfid data;data stream management systems;event correlation;sensor data
web-based;control systems;historical data;monitoring data;database;stream mining;large number of;data centers;data center;time-series;input streams;threshold-based;monitoring systems
large distributed;database;databases;distributed databases;database servers;databases;distributed database
semantically similar;database;large number of;complex structures;data sets;data integration systems;data management;semantic mappings;data integration
association rules;data mining
graph structures;high dimensional;association rule mining;main memory;optimization problem;instance;hamming space;interactive visualization;instances;multi-faceted
bitmap index;high cardinality;compression schemes;compression techniques
tree index;database applications;construction algorithm;buffer management;larger datasets;algorithm's performance;data sources;large datasets;disk-based;tree construction;approximate string
xpath queries over;xpath queries;database;answer set;communication costs;xml database;algorithms for computing;xpath processing;answer sets
sql/xml;matching algorithm;xml fragments;data values;xml queries;large collections of;user query;query processing;xpath expression;query result;xml query processing
schema-free;querying xml data;keyword-based search;stack-based;partial knowledge;database queries;schema-free;multiple data sources;document structure;schema-free;xml document;aware query;xml structure;data integration
access control rules;database;sharing data;real datasets;access control;database servers;xml documents;service providers;data dissemination
real data sets;security requirements;sensitive information;data security;xml publishing;xml documents;data-publishing;xml document;information leakage;common knowledge
privacy policy;database;large databases;databases
query evaluation;database;np-complete;query processing;additional constraints;tree pattern queries;query languages
computational complexity;semantic caching;xpath queries;knowledge bases;database;database queries;xml queries;query reformulation;data sources;peer data management systems;data integration;query containment;query containment;data management applications
path expressions;efficient sql;query translation;path expression;sql queries
query evaluation;xpath queries;xml data;optimization problem;rewriting algorithm;xpath query;evaluation strategy
tree pattern queries;optimization techniques;query language;optimization framework
data stream;discrete data;data streams
excellent scalability;pull-based;real-life datasets;motion model;data sources;data stream;sensor data
mining frequent itemsets from;high speed data streams;false positive;limited memory;minimum support;frequent items;frequent itemset mining;frequent itemsets;computationally intractable;false-positive;high accuracy;data streams;memory consumption;frequent item(set;high speed
xml documents;historical information;temporal data;indexing scheme;query performance
query optimization;query evaluation;streaming xml data;xquery language;schema-based;main memory;query processing;data streams;event-based
query optimization;higher accuracy;estimation accuracy;frequency distribution;selectivity estimation;data structure;path expressions;xml data;trade-offs;estimation error;dynamic environments;cost-based;upper bound;internet applications;estimation techniques;space budget
relational database systems;xml processing;xquery expressions
query optimization;xml format;xml data;business objects;xml documents;business applications
view updates;xml query languages;xml view;xml views;relational database;relational databases;relational views
data arrives;query optimization;synopsis structures;synthetic data sets;optimal algorithms;approximate query answering;storage utilization;real-life;linear space;data sets;dynamic programming;greedy algorithm;approximation algorithm
query optimization;provide evidence;real life data sets;approximation algorithms;approximate query answering;histogram construction;representation scheme;relative error;wavelet based;theoretical analysis;absolute error
synthetic data sets;frequently occurring;streaming environment;communication costs;remote sites;real-life;update streams;cardinality estimation;estimation algorithms
approximate answers;random sample;result tuples;data streams;join result;input streams;sliding-window;stream joins
resource sharing;resource sharing;large numbers of;performance guarantees;wide range;data streams;aggregation functions;sliding-window;continuous queries
historical data;sampling techniques;multiple representations;data reduction;archived data;data stream management systems;data streams;query engine;random sampling
growing number of;web pages;completeness;pull-based;vice versa;information sources;information dissemination;real-world;web sources;information from multiple sources;general-purpose;information providers;data mining;push-based;data streams;query engine;continuous query
growing number of;underlying structure;web-service;web services;similarity search;search problem;text fragments;similarity search;web services;search engine;high recall;keyword search;software components;finding similar
closed-loop;classification;data warehouse;machine learning;web-site;user feedback
answer queries;web crawlers;web directories;web search engines
web databases;high precision and recall;instance-based;database;integrating data from;query interfaces;schema matching;user queries;domain-specific;matching techniques;cross validation;schema-matching;web databases;extracting data from;instance-based
internet search;user queries;high quality;web server;web servers;data set;web crawlers;search engines;pagerank algorithm;search engine;query results
search systems;distributed monitoring;query workloads;query processing;query processor;internet-scale;popular items;analytical model
parallel databases;optimal algorithms;large number of;range queries;large-scale
network models;query processing;distributed stream processing;query operators;distributed environments;aware query processing;processing queries
data sharing;data sharing;correct answers;related data;autonomous data sources;query translation
relational database) systems;management systems;6, 11, 9;streaming data;1;linear road;stream data;data management system;historical queries;relational database;linear road;8;query results;stream systems;data management
completeness;relational algebra;database;simple queries;data mining;query language;data model;application domains;user-defined;multiple streams;syntactic structure;data streams;operator;query languages;stream queries;continuous queries
hash functions;database;extra information;storage engine;database management system;information disclosure;additional information
database;database;design choices;static analysis;data structures;query string;query processing;queries efficiently;sensitive) data
high-dimensional;multi-dimensional;data analysis tasks;data cube;data cube;data warehouses;data sets;olap operations;online analytical processing;text processing;high dimensionality;high dimensional space
high-dimensional;synthetic data sets;data cube;data set;data cubes;low dimensional;operator;high dimensionality;vast amounts of data
link analysis;database;directed graph;web graph;valuable information;link-based ranking;web pages;relational database;relational databases;link analysis
keyword search;labeled graphs;authority flow;synthetic datasets;databases
world wide web;search engine's;spam pages;large number of;human experts;web spam;link structure;semi-automatically;seed set
sensor-network;sensor readings;taking into account;database;real-world;declarative queries;data acquisition;optimization problem;provide answers;data sets;sensor networks;performance gains;real world;modeling techniques
data analysis;type-checking;software architecture;real-world;query language;data model;query processing;data-centric
large numbers of;data rates;xml filtering;publish/subscribe systems;xml data;overlay network;internet-scale;xml-based;internet-scale;data dissemination;highly scalable
image retrieval;approximation algorithms;vector space;exhaustive search;computational geometry;match scores;vector spaces;retrieval quality;computational cost;resource allocation;nearest neighbors
final ranking;case study;database;data cleaning;multiple attributes;efficient algorithms to;single attribute;data cleaning;query string;approximate match;data base;approximate matches
web corpus;information discovery;query evaluation;data collection;general-purpose;multidimensional datasets;probabilistic guarantees;high probability;building block for;threshold algorithm;multidimensional data;data items;index scans
high-end;storage manager;online transaction processing;database servers;transaction processing
search performance;free space;key range;tree nodes;file systems;log-structured
join algorithm;storage schemes;generally applicable;relational operators;storage model;join algorithms;real-life;random access;algorithm called
memory utilization;management architecture;data layout;dynamically changing;database application;access patterns;data placement;data organization;storage devices
end-user;23, 24;databases;privacy policies;personal data;6;data management
service provider;query evaluation;privacy guarantees;data partitioning;worst-case;information leakage;statistical measures;data privacy;range queries;privacy-preserving;database outsourcing;database outsourcing;relational table;margin;data management;sensitive attributes
high-speed;intrusion detection;streaming model;stock market;domain specific;streaming data;domain-specific;computing environments;confidence levels;sensor data
nearest neighbor;data points;pre-computation;data-partitioning;multidimensional datasets;database updates;nearest neighbors;approximate results
high-dimensional;join algorithm;real life datasets;nearest neighbor;knn) join;data mining;join processing;knn-join;similarity search;distance computation;databases;dimensional data;margin;query results;similarity join;cpu cost
spatial proximity;tree based;database systems;continuous variables;database;frequent updates;moving-object;large quantities of;emerging applications;moving objects;queries efficiently;nearest neighbor queries;data management;continuous queries
data-driven;motion capture;real datasets;brute force approach;motion capture data;similarity search;databases;human motion;human-motion
edit distance;false positives;distance functions;distance function;time series;pruning strategies;edit distance;databases;lower bound;pruning power
query answers;multi-dimensional;data distributions;nearest neighbor;limited memory;real data sets;query processor;data stream applications;data set;memory utilization;query processing;nearest neighbors;nn queries;data streams;databases;absolute error;query point;data arrive;adaptive indexing
database;real-world;nearest-neighbor;data sources;real-world entities;databases;geographic information systems
materialized view;query result;fuzzy sets;prior knowledge
shortest paths;distance computation;network connectivity;knn queries;pre-computation;computationally expensive;spatial networks;road networks;databases;real-world data sets;spatial network;nearest neighbors;nearest neighbor search
high-dimensional;clustering;cluster structure;classification;large volume;high dimensional data streams;clustering method;similarity search;high-dimensional data sets;projected clustering;data stream;stream data;clustering methods;data streams;dimensional data;synthetic data sets;highly scalable;projection based;clustering quality
sql queries;probabilistic model;optimization algorithm;efficient query evaluation;query semantics;information retrieval;probabilistic databases;monte-carlo simulation;arbitrarily complex;approximation algorithm;efficient query evaluation;data complexity
clustering;indexing schemes;tree based;dimensional space;data points;database;uncertain data;index structures;probability density function;threshold queries;efficient computation;index structure;data item;threshold queries;probability values;indexing methods
database;probabilistic models;information retrieval;domain independent;ranking functions;query results;structured data
relational databases;storage scheme;biological data;sql queries
query optimization;database systems;database;real-world applications;compact data structure;sql extension;operator;relational databases;query plan;database engine;indexing methods
plans;database;sql queries;data representation;medical imaging;scientific datasets;discrete data;optimization techniques;spatial databases
result sets;attribute values;naïve;database systems;multi-objective;query processing in;specialized algorithms;multi-objective;databases;exact matching;objective functions;algorithm maintains;query processing in;database scans;instance;query processing;skyline queries;database objects;result set
query execution;adaptive query processing;adaptive query processing;1;query plans;long-term;8;query engine
interesting orders;cost-based query optimization;space requirements;additional cost;preprocessing phase;avoid redundant
ordering constraints;stream query processing;generally applicable
query execution;database management;database applications;db2 udb;data model;ibm® db;source data;result set;set semantics
query optimization;data analysis;query execution;execution strategies;tabular data;data transformations;query processor;microsoft sql server
optimization techniques;access plan;access control;sql query;database research;application domains;relational database management systems;access control;relational database management systems
traditional databases;real-world datasets;computational cost;data types;real-world;protein structures;commercial dbms;case study;query processing;databases;complex data;commercial database;spatial queries;commercial databases;geographic information systems
stock market;database server;update-intensive;storage engine;high-volume;data streams;concurrency control;highly scalable
access pattern;3;decision support system;data distributions;selection process;data sets;query performance;data set;decision support systems;business intelligence;query workload;enabling users to;high-level;query sets
domain-specific knowledge;ontology-based;ontology-driven;semantic matching;database applications;indexing scheme
keyword based search;semantic associations;databases;information retrieval;keyword search;semantic web;relevant information;information overload;biomedical research;retrieval techniques
clustering;hybrid" approach;search space;database;database design;db2 udb;physical design;materialized views;ibm® db;additional features
sql tuning;query optimization;high level;query optimizer;database;databases;execution plan;sql tuning;manual tuning;real customer;cost-based;access path;execution plans;performance tuning;large volume;access structures
construction algorithm;search engines;final step;search engine;search quality;global analysis
query rewrite;xml storage;relational algebra;database;queries over xml;xml publishing;query performance;xml documents;object relational database;1;query engine;xml db;object relational;relational data;relational queries
document-centric;xml data;data-centric;data model;relational tables;data stored in;query processing;relational databases;relational database;query performance
database administrator;query execution plans;query optimizer;database;query workload;basic concepts;db2 udb;query performance;case study;implementation details;ibm® db;query processing;query feedback;relational dbms;tedious task
automated design;clustering;single-dimensional;search algorithm for;query cost;storage structure;db2 udb;human experts;decision-support systems;multiple dimensions;solution space;data sparsity;relational databases;database table;indexing methods;data storage
lessons learned;data acquisition;1;business process;business processes;sensor networks
radio-frequency identification;rfid technology;rfid) technology;rfid data;data management
database systems;database;large volumes of data;complex systems;configuration parameters;human intervention;microsoft sql server
genomic data;database;large-scale;case study;data sets;high-throughput;managing data;genomic data;processing requirements;special case;application area
large scale;data sets;data warehousing;data integration;database
databases;databases;computing environment;personal information;database
data warehousing
database;database technology;data warehouse
grid computing;large databases;database systems;transactional data
clustering;multi-level;database workloads;years ago;database systems;database;modern hardware;modern database;main memory;disk accesses;multi-threaded;database;database application;stored data;storage systems;databases;data placement;memory utilization;data-intensive;database performance;working set
data security
domain knowledge;expression patterns;gene expression data;real-world;integrated environment;3;analysis task;graphical interface;clustering methods;gene expression data sets;interactive exploration;biomedical applications;clustering procedure;bioinformatics research
frequent itemsets;frequent itemset;classification algorithms;association rule;frequent itemset
ground truth;classifier ensemble;streaming model;data stream mining;data stream;learned model;decision-tree;data streams;credit card;data chunks
specially designed;gene expression data;minimum support;visual analysis of;biological datasets;2;chi-square;expression levels;association rules;association rules;association rule mining algorithms
high-dimensional;high-dimensional;data points;detection problem;data point;effectively identify;learning process;dimensional data
false positive;visualization tools;database;data mining;large databases;time series;visualization tool;databases;time-series
databases;conjunctive queries;database
high-speed;data structure;storage systems;data storage
xml documents;relational databases;xml queries;relational database;querying xml documents
ranking techniques;semantic associations;information retrieval;search engines;semantic relationships;semantic associations
web sites;web site;data intensive
similar queries;query optimization;optimizer;feature vector;tree-based;1;ibm db;2;future queries;query optimizers;commercial database;classifier;clustering
real-world;query plans;intermediate results;query plan;databases;operator
conjunctive predicates;benchmark data;query execution plans;optimizer;database;database administrators;wide range;automatic generation of;query optimizers;functional dependencies;analysis tool
rewrite rules;optimizer;database applications;database server;control strategies;coarse-grained;coarse-grained
database schema;sql server;microsoft sql-server;desirable properties;database schemas;relational dbms
low power;data acquisition;aggregate data;data stream;sensor networks;distributed systems;data management;query processing
query processing;optimizer;sensor networks;data stream management systems;sensor network;optimization process;data stream management system
sensor data is;operator;stream queries;data processing;data streams
interaction data;database;application called;temporal behavior;1;domain-specific;data streams;graphical user interface;querying capabilities;spatio-temporal queries;continuous queries
video data;spatio-temporal;low-level features;database management system
large numbers of;database server;location-aware;incremental evaluation;spatio-temporal;query processor;computing environments;mobile objects;moving objects;location-aware;data streams;continuous spatio-temporal queries;query processing algorithms
ir techniques;37;speech data;information retrieval;speech retrieval;natural language processing;natural language processing
relational data sources;data residing;data source;database;database schema;ontology language;conceptual model;information access;relational database;schema design;structured data;formal semantics;data loss
growing number of;search methods;structured documents;storage space;information retrieval methods;search engine;xml-documents;digital libraries;document structure;retrieval performance;xml information retrieval;information retrieval techniques;xml information retrieval;structural information;vector space model
enterprise search;web search;link structure;enterprise search;information retrieval;semantic web
ensemble learning;intrusion detection;intrusion detection systems;base classifiers;network intrusion detection;false alarm;feature representations;behavior patterns;classifiers trained on;multiple classifiers;high cost;error rate;single classifier;feature sets
large graphs;knowledge engineering;large graph;clustering algorithm
data mining techniques;target domain;takes into account;knowledge-base;knowledge based;context-dependent;corpus-based;similarity measures
clustering;clustering algorithms;taking into account;instance;large number of;real data;management systems;event correlation;clustering algorithm;recognition problem
11;10;automatically extracted;training phase;relevant documents;linguistic features;14
human effort;knowledge bases;knowledge-based;real users;information retrieval;prohibitively expensive;document collections;specifically tailored;search engine;knowledge-bases;information retrieval tasks;knowledge base
business decisions;streaming environments;user-defined;sensor data streams;data quality;business applications;data processing;resource constraints;window-based;decision making
domain knowledge;local structures;unlabeled data;classification;feature space;classification framework;real-world;naïve bayes;online news;classification process;learning process;partially labeled;test sample;traditional classifiers;classifier;discovered rules
description language;causal relationships;event-driven;event processing;rule-based;knowledge base;supply chain management;complex event processing;temporal constraints;complex event;stock market
query evaluation;semi-structured data;xml format;hierarchical structures;retrieval techniques;highly correlated;query languages;huge number of
database administrator;database systems;database;physical design;databases;total cost
formal framework for;theoretical foundation;digital library;theoretical foundations;information overload;personal information management
question answering;cross language;document retrieval;information retrieval;specific context;qa systems;text retrieval;question answering
statistical translation;cross language information retrieval;linear combination;cross language information retrieval;confidence measure;query translation
score function;pruning strategy;matching algorithm;database;graph matching;social network;large graph;biological network;data mining;subgraph matching;pairwise similarity
application domain;share information;service-oriented;semantic web;semantic web services;formal description
structural constraints;semi-structured;relevance assessment;information retrieval;semi-structured documents;key words;information retrieval
knowledge management;development process;information exchange;knowledge management
10;web applications;web usage mining;database;web queries;web sites;realistic data;closed itemsets;access patterns;web log
salient features;internal node;online communities;optimization problem
monitoring applications;data structure;heavy-hitters;answer quality;multiple dimensions;data items;real data;view based;data stream;streaming data;data streams;heavy hitters;online algorithms
data point;probabilistic framework;mixture model;random variables;model assumes;high-dimensional datasets;machine learning;multimodal data;data mining
clustering;learning process;high-dimensional datasets;structural properties;clustering results;clustering accuracy;clustering framework;clustering algorithm;benchmark datasets;result quality;clustering
database research;database technology;database
25;regular expressions;19;xml schema;6, 19, 20, 21;xml schema;tree automata
text mining;text mining;textual data;text representation;text collections;semantic issues;similarity measures;natural language
query optimization;database systems;database;acm sigmod;main-memory;data streaming;data mining;information technology
acm sigmod;information science;database
large scale;social networks;user experiences;online communities;database researchers;structured information
database technology;database;databases;international workshop on;high energy physics;data sets;databases;numerical data;internal structure
accessing data;structured databases;free-form;keyword-based;information retrieval;original data;logical database;databases;relational databases;query keywords
learning process;user preferences;xml fragments;xml data;vector machine;naïve bayes;xml documents;learning mechanism;level features;xml search;feature vectors;web data
response times;query optimization;multi-dimensional;relational queries;database;ir) researchers;ir systems;text retrieval;query language;information retrieval;information retrieval models;databases;information retrieval models;relational tables;resource usage
relational algebra;large-scale;main findings;retrieval tasks;probabilistic relational;retrieval models;operator;high-level;probability estimation;unstructured data
search terms;attribute values;database systems;database;query language;query language;retrieval systems;query processing in;database query processing;traditional database;traditional database;query languages;queries involving;information retrieval;information retrieval systems;query processing
query execution;xml retrieval;large-scale;query relaxation;aggregation function;query language;xml query languages;word-sense disambiguation;query conditions;ranked retrieval;ir-style;semistructured data;query expansion;retrieval engine;query processing
access patterns;business processes;distributed systems
low-cost;database researchers
database design;database technology;database systems
synthetic data sets;large data sets;uncertain data;data set;instances;data sets;uncertain data;uncertain objects
twig queries;xml data;data complexity;probabilistic data;probabilistic xml;probability threshold
dimensional space;synthetic data;database;real-world;constraint language;olap queries;real-world applications;imprecise data;domain constraints;aggregation queries over;query answering
probabilistic data;query evaluation;probability distribution;database;query optimization;theoretical results;probabilistic databases;materialized views;materialized views;query processing;answer queries;information exchange
specific applications;response times;search space;dynamic programming;matching algorithm;false positive;database;large number of;real datasets;multi-core;index structure;approximate matches;query workloads;xml indexing;longest common subsequence;query processing
open source;xquery engine;data model;window functions;input sequence;linear road;java-based;continuous queries
pattern language;materialized views;database queries;xml queries;7
communication patterns;data source;open-source;xquery update;semi-join;efficient querying;data sources;service oriented;web services;distributed database;formal semantics
service provider;algorithm performs;database;association rule mining;data owners;private information;data transformation;data owner;transactional data
outsourced data;data outsourcing;access control
service provider;continuous query processing;long-running queries;data streams;indexing scheme;databases;completeness;range queries;data owner
end-users;window queries;key features;data structures;aggregation queries over;theoretical analysis;sliding windows;space partitioning
large numbers of;plans;distributed stream processing;query workloads;output quality;optimization problem;distributed stream processing system;query results;continuous queries
estimation method;statistical model;lower bounds;memory requirement;maximum likelihood;distributed streams
query retrieves the;main memory;data structure;ad-hoc;dynamic environments;distributed databases;incrementally maintaining;pruning strategies;data set;ad-hoc;evaluation strategies;scoring function;data streams;query answering;incremental updates;indexing technique;ad-hoc queries
edit distance;selectivity estimation;database applications;edit distance;string matching;similarity measures
search algorithm for;data stored;database;xml documents;stored data;databases
user input
base table;database systems;view maintenance;materialized views;maintenance cost;view maintenance;response times;materialized views;query processing;microsoft sql server
data values;finite domain;semantically related;data cleaning;implication problem;schema matching;static analysis;counterpart;heuristic algorithms;general setting;7;9;functional dependencies
domain knowledge;query rewriting;database;virtual view;knowledge management;ibm db;knowledge representation;data management;unified framework;database engine;microsoft sql server;relational data;managing data
high cardinality;input tuples;skyline computation;data elements;underlying data distribution;evaluation techniques;data characteristics;data distribution
skyline algorithms;close connection;skyline queries;data points;index structure called;data dimensionality;skyline query;decision making
real-world and synthetic datasets;search space;skyline queries;efficient computation;pre-computed;data set;skyline queries;computational cost;algorithm (called;query returns;query point
real data sets;high-quality;fixed-length;support queries;edit distance;main idea;string matching;approximate queries;approximate string
database;makes sense;data quality;automated methods;statistical method;data consistency;6;automatically-generated;functional dependencies;user interaction
great significance;real-world datasets;external sources;large datasets;record matching;negative examples;data sources;business intelligence applications;real world;string matching;record matching
factors affecting;hash-based;aggregation operator;multi-core;run length;data structures;query processing;hash table;adaptive algorithm;database operations;high accuracy
query execution;data accesses;database systems;multi-core;real-world;sharing opportunities;database;decision-support queries;execution engine;concurrent queries;database systems
low-overhead;data processing;programming model;multi-core;high-end;join operators;operator;high scalability;data stream;high-throughput;data stream management systems;data transfers;column-oriented;rates;times higher;stream joins
high priority;search-engine;search engine;theoretical analysis;simple algorithm;theoretical guarantee
web corpus;global information;document retrieval;data-rich;entity search;search engines;wide range;conceptual model
web pages;plans;database;case study;data sources;human effort;high accuracy;integrate data from
rdf data;web-based;rdf databases;vertically partitioned;query times;large-scale;scalability issues;database;semantic web;real-world applications;semantic web;scale poorly;column-oriented;prior art;data management
lower bound;data sequences;sequence matching;data sets;query sequence;databases;time-series;matching algorithm
similarity-based;synthetic data sets;euclidean distance between;web search;distance function;data mining;time series;lower bound;dimensionality reduction techniques;multimedia retrieval;similarity search;similarity queries;search efficiency;databases;time-series;efficient similarity search;high dimensionality;pruning power;piecewise linear;lower bounds
high-dimensional;multi-dimensional;data analysis;dimensional space;market segments;data cube;anomaly detection;real-world;anomaly detection;time series;predict future;expected values;large data sets;time-series;high-dimensional space;search algorithm;detect anomalies;higher level
true values;streaming environment;partial information;trade-offs;time series;time series;original data;real data
online aggregation;data management problems;extreme values;outlier detection;large number of;data set;query processing;distance join;data management;bayesian approach
data objects;synthetic datasets;multi-dimensional data;specialized algorithms;real datasets;decision support;ranking functions;skyline queries;data analysts;query returns
databases;data items;performance gains;threshold algorithm;general problem;database instance
query execution;query evaluation;twig queries;join operator;join) operator;keyword-based;join algorithms;structural-joins;answering queries;query processing;xml document;relational database;np-hard;weighted graphs;computationally expensive
security requirements;database;database server;sensitive data;performance degradation;data encryption;data warehousing;high cost;aggregation queries
data publishing;prior knowledge;anonymization algorithm;database instance;sensitive information
data publication;data publishing;real datasets;extra information;privacy preserving data publishing;individual privacy;model called;information loss
access control rules;query evaluation;great flexibility;fine-grained access control;access control;database;fine-grained;query modification;fall short;query answering;databases;relational databases
real-world datasets;spatial data;data types;spatial objects;data model;building blocks for;probability distribution;data type;high-level;indexing technique;low-level
service provider;nearest;maximum number of;nearest neighbor queries;metric space;problem called
large amounts of;database technology;database systems;frequent updates;main-memory;main memory;tree-based;update operations;cost model;spatial-index;rates;indexing technique
query language;business processes;interesting patterns;business processes;execution engine;minimal overhead;optimization techniques;formal model
rewrite rules;business activities;database;control strategy;data dependencies;data stores;graph model;multi-stage;increasing number of;unified framework;business process;business processes;data access;data processing;performance gains;optimization process;data management;control flow
transaction logic;database transactions;logic-based;web services;semantic web services;data flow;service discovery;transaction logic
randomized algorithms;multiple sites;data sources;aggregate queries over;query processing;randomized algorithms;query results;data integration
web databases;query rewriting;query patterns;optimization framework;database;network resources;missing attribute values;selectivity estimates;reformulated queries;user query;naïve bayes;functional dependencies;query processing;databases;high recall;query processing techniques;web environment;high precision;database query processing
query execution;17, 22;pruning techniques;17;23;rewritten query;information management;semantic integration;data sources;information integration;search engine;query results
data sharing;20;related data;data exchange;view maintenance;database instance;provenance information;data provenance;schema mappings;data integration
query complexity;efficiently computing;data-integration systems;imprecise data;data sources;answering queries;data integration;source data;information extraction techniques;semantic mappings;schema mappings;data integration
domain knowledge;data extraction;large numbers of;context-aware;matching accuracy;deep web;data sources;extraction accuracy;extraction task;context-aware;context awareness
adaptive query processing;plans;synthetic datasets;learned models;plan space;real datasets;efficient algorithms to;query-processing algorithms;environmental monitoring;execution plans;data streams;future events;statistical models;plan-selection;continuous queries
scheduling policies;taking into account;data-sharing;query loads;sharing opportunities;individual queries;data sizes;wide range;disk storage;disk scheduling;index) scan
query execution;benchmark data;long running;data warehousing;decision support queries;incur high;long running
quality metrics;spatial indexing;database;indexing techniques;anonymization algorithm;data set;anonymization techniques;data sets;tree index;incremental updates;spatial index
multi-dimensional;sensitive information;single attribute;privacy preserving;data anonymization;information loss
computational efficiency;sensitive information;data publishing;external knowledge;high confidence;social networks;privacy requirements;personal data
service provider;database;increasing number of;outsourced data;lower cost;query results;database engine
factors affecting;navigation systems;large scale;search strategy;pre-computation;patterns mined;competing methods;road network;distance-based;road networks;traffic data
pair-wise;synthetic data sets;temporal intervals;temporal properties;efficient algorithms to;large collections of;blog posts;vast amounts of data
taking into account;synthetic datasets;database;index structures;query types;real dataset;spatial attributes;query processing;skyline queries;data mining;query results;decision making
contingency table;real-world datasets;data values;attribute dependencies;database;large number of;commercial products;query feedback;large space of;data mining;query optimizers;dependency structure;space budget;query-feedback
process model;data collection;application-level;missing data;sensor networks;bayesian approach;sensor networks;bayesian inference
synthetic data sets;statistical models;high level;network size;numeric data;index structure;index structure;hidden markov models;sensor networks;low level;nn queries;real-world;communication costs;markov chains;range queries;query sequence
xml filtering;publish-subscribe systems;xml query;incoming documents;xml documents;indexing structure;upper bound;xml filtering
processing cost;network traffic;publish/subscribe systems;large-scale
real data sets;quality guarantee;approximation methods;database;large data sets;large databases;exact answer;theoretical guarantee;instances;tree data structure;linear complexity;query answering;synthetic data sets;answer questions;quality guarantees
query optimization;query evaluation;input tuples;query optimizer;plans;estimation method;join operators;join processing;execution plan;joint distribution;estimation algorithms;data sets;depth estimation;scoring function;relevant answers;base tables;operator;depth estimation;principled framework;speed-ups
probabilistic approach;data sets;information retrieval;real data sets;multi-attribute
clustering;graph models;sampling techniques;np-complete;index construction;graph data;greedy algorithm;feature selection process;pruning power;test data
structural patterns;indexing methods;high-quality;graph database;vast number of;graph-based;index structure;mining process;large graph;indexing method;index construction;feature selection;subgraph isomorphism;databases;query processing;cost-effective;pruning power;graph indexing;graph indexing;query processing cost
high-dimensional;entropy-based;high-dimensional datasets;query results;space requirement;indexing techniques;large number of;search quality;efficient indexing;similarity search;sensor data;hash table;locality sensitive hashing;dimensional data;indexing scheme;content-based search
network monitoring;multi-level;communication overhead;large-scale;takes into account;hierarchical algorithm;optimization problem;workload-aware;aggregate queries over;extensive simulations;high accuracy;distributed data streams;data distribution
information systems;large-scale;query allocation;high efficiency;baseline methods;load balancing
high degree of;similar objects;data collections;metric spaces;range query;data movement;network latency;communication cost;similarity search;similarity queries;nearest neighbor queries;query processing;real-world;computational overhead;bandwidth consumption;super-peer
regular expressions;xml data;inference algorithm;sufficiently large;xml documents;data sets;xml schema;document type definitions;real-world;automatically generate;positive examples;xml schema;increasingly popular
evaluation cost;structured databases;database;sql query;complex queries;database schema;schema-based;inter-relationships;databases;aware query;keyword search;query conditions;result quality
structural information;clustering quality;semistructured documents;structural similarity
optimization framework;ad-hoc;real-world;query optimization techniques;text data;information extraction;data sets
data set;unstructured data;structured queries
enterprise search;5;large parts;open-source;virtual view;semi-structured;information retrieval techniques;emerging applications;data set;search queries;base data;xml views;xml database;integration systems;keyword search over;query results
data objects;generic model;convex optimization;index structures;database research;query types;real-world applications;data set;query processing;space partitioning;queries involving;objective function
search space;optimal plan;plans;query optimizer;database;performance guarantees;np-hard problem;expected-cost;large number of;multi-dimensional;parametric query optimization;plan choices;query optimizers;database engines;query templates;query processing
problem instances;query optimization;running times;estimation accuracy;optimizer;query optimizer;index selection;database design;cost estimation;cost model;physical design;query cost;computationally expensive;estimation technique
individual queries;false positives;workload management;data warehouses;customer service;execution times;business intelligence;fundamental problem;workload management;fuzzy logic;online transaction processing
source code;data processing;scientific data;database research;relational operators;relational operations;data provenance;high-level
execution environment;business process;business processes;ad hoc;abstraction level
data bases;international conference on;database;query workload;decision support;data model;performance metric;decision support systems;9;optimization techniques;transaction processing
database;data warehouse;stream processing;data processing;sc, sbc;relational dbms
world wide web;semantic web services;systems support;problem solving;multi-core;programming model;database;data model;emerging applications;real world domains;reasoning capabilities;service-oriented;data management;data management;problem solving;databases;data-intensive applications;real world;semantic web;relational data;data services
computational overhead;compression technique
stream processing systems;continuous query;efficient computation;semantic constraints;window functions;wide range;complex event processing;base tables;batch processing;continuous queries;commercial databases;continuous queries
multi-modal;rates;large-scale;large number of;distributed stream processing
clustering;multi dimensional;multiple attributes;db2 udb;clustering scheme;data warehousing;query processing;database engines;multi dimensional
query execution plan;optimizer;sql queries;sql query;upper bound;query language;aggregate functions;cost-based;query processing;databases;arbitrarily complex;result set;query results;oracle database;approximate results
data sharing;query execution;query throughput;data source;execution model;data sources;sharing opportunities;distributed queries;remote sources;query engine;data transfers;combining multiple;disk page;data integration
query optimization;heterogeneous sources;aqualogic data services platform;user experience;integrating data from;user-defined
query optimization;large numbers of;feature set;access methods;sql queries;test cases;access paths;query execution;return results;generation process;test cases;test case;database engine;microsoft sql server;database systems
database;database applications;application code;microsoft sql server;application developers;tuning tools;relational dbmss
false positives;real world;sufficient conditions for;database;sql query;large number of;unlike earlier;application code;real world applications;commercial databases;serializable
data mining tasks;tree based;fp-tree;multi-core;lock;data locality;multi-core;frequent itemset mining;fine-grained;frequent pattern;frequent pattern mining
increasing number of;average case;main memory;large number of;fine-grained;data transfer;data transfers
clustering;response times;database management system;database management systems;db2 udb;index scans;decision support systems;table scans;database engines;index scan;table scan
large volume;hierarchical organization;complex queries;web-based;sharing data;scientific data;database;data model;scientific data management;virtual view;data integration;central server;local sites;unified framework;biomedical research;scientific data;data management
user interfaces;web-based;database contents;database systems;multi-resolution
matching algorithm;schema matching
generic model;information systems;10;modeling language;common task;operator;role based;xml schema
schema integration;1, 2, 6, 8, 10;schema integration;source schemas;integration process;schema mappings;semi-automatic;schema elements
complex data;xml data;search results;xml keyword search;search engine
data independence;query reformulation;data management system;load-balancing;semantic interoperability;schema mappings;data management system
preference elicitation;preference elicitation;increasing complexity;related queries;easy access
completeness;computation costs;data items;outsourced data;access patterns;keyword search;conjunctive keyword
public data;private data;data flows;product information;query processing in;distributed processing;issue queries
data fusion;data fusion;conflict resolution;final step;integration process;schema mappings
interactive exploration;high degree of;main memory;mobile devices;query response times;data sizes;growing importance;data sets;user interface;query processing;distance functions;data server
relational databases;relational calculus;highly structured;natural language
trade-offs;real-world;accuracies;moving objects;communication costs;query processing techniques
view" based;higher-level;workflow systems
enterprise data;user-interface;execution engine;business users;data feeds;data integration
clustering;temporal order;temporal information;spatial information;compact representation;content-based video;wide range;high accuracy;context information;large database
query evaluation;xml streams;dynamic analysis;main-memory;xquery engine;static analysis;buffer management;memory consumption
rewrite rules;optimizer;cost-estimation;database;optimization approach;rule-based;estimation process;cost-based;5;4;control strategy;graphical user interface;coarse-grained
learning curve;18;relational database systems;xml-based;databases;relational algebra;type inference;programming language
open-source software;increasing complexity;open-source software;large scale;large scale;high frequency;content management
growing number of;large amounts of data;data warehouse;main memory;3;data volumes;data processing;distributed sources;histogram-based;data management
graph structure;instance;web graphs;web mining;social networks;social communities;large-scale distributed systems
reusability;faceted search;information systems;relational dbms;data management
query optimization;complex queries;interactive exploration;database;application called;biological sequence;sequence databases;declarative framework;databases;life science
unique features;information discovery;text streams;query answer;ranking functions;high volume;text analysis;spatio-temporal
text search;increasing number of;temporal dimension;text collections;text collection;web archives;keyword query
data warehouse;data warehouse;query response time;global optimization;base tables;ad hoc;ad hoc queries
compact representation;sql queries;4, 1, 3, 2;query language;language called;incomplete information;main features;incomplete data;data management system
adaptive query processing;data streams;adaptive query processing;data management;query plans;data integration
multi-dimensional;general concept;spatio-temporal;time series;online services;databases
information systems;trade-offs;privacy policy;location information;data privacy;data management;privacy-preserving;location based service;data management systems;location privacy;main components
access pattern;data integrity;completeness;data services;sensitive data;outsourced data;service providers;data encryption;data outsourcing;data confidentiality;data management;data access
life sciences;data migration;long - term;united states;data confidentiality;data management
query optimization;graphical models;database research;selectivity estimation;database;probabilistic graphical models;data mining;data representation;machine learning;compact representation;joint probability distribution;sensor networks;probabilistic graphical models;data analysis;databases;data privacy;information extraction;image processing;data integration
data-driven;question arises;xml retrieval;application scenarios;unstructured text;semi-structured;data model;web content;information retrieval;user interface;ir research;web data;databases;digital libraries;keyword search;xml content;online communities;web data;search interfaces
information integration;data warehousing;schema evolution;long-term;schema mappings;data integration
database research
world wide web;social networking;allowing users to;databases
document processing;programming languages;acm sigmod;search applications;xquery implementation;international workshop on;technical program;web services;databases
cosine similarity;database;data cleaning;database applications;real datasets;increasing number of;tf-idf;complex queries;similarity predicates;string matching;data integration
high correlation;total number of;query pattern;cardinality estimation;cardinality estimation;actual values;high accuracy;algorithm relies on;algorithm produces;result set;heterogeneous data sources;optimization strategy

acm sigmod;database theory;database
acm sigmod;xquery implementation;web site;international workshop on
medical records;data warehouses;knowledge management;information management;data quality;long-term;health information;knowledge management;healthcare data;unique characteristics;information technology
information integration;mobile computing;international conference on;web-services
user requests;real world;search engine
ir researchers;ir research
vector space;retrieval models;information retrieval research;retrieval model;information retrieval
search results;temporal information;information space;user experience;information retrieval;information retrieval systems
enterprise search;trec enterprise;real-world;development process;search tasks;test collection
large-scale;information retrieval;information retrieval;real-world applications;large-scale

training data;machine learning;automatically learn;information retrieval
search task;world wide web;information sources;diverse range of;web search engines;information providers;web browsing;user groups;information seeking;information-seeking
world wide web;similarity search;increasing attention;duplicate detection
search engines;web queries;web searching;user behaviour;web searching
multimedia information retrieval;visual search;large scale;meta-data;multimedia information retrieval;information retrieval;geo-referenced
large-scale;information retrieval;large-scale distributed systems;large-scale distributed systems;information retrieval
graphical models;probabilistic models;ir) model;information retrieval;15, 9;2;bayesian networks;probabilistic models;information retrieval;artificial intelligence
focused retrieval;sigir 2007 workshop on;sigir 2007 workshop on;focused retrieval
sigir workshop on
information access;international workshop on
query log analysis;query logs;personal data;data analysis;search engine
information retrieval workshop
rfid technology;supply chain;world knowledge;feature generation;information retrieval
classification;named entities;information extraction;databases;question answering;2, 3
machine translation;training data;sentence-retrieval;training corpus;post-processing;information extraction;novelty detection;sentence retrieval;question answering;sentence retrieval
web pages;document structure;web pages;document genre;search engine;centroid-based;classifier
multi-stream;molecular biology;dynamic bayesian networks;event sequences;real-life;multi-attribute;multi- attribute;attribute dependencies;monitoring systems
skewed data;class noise;imbalanced data;class distributions;data mining research;imbalanced datasets;skewed
clustering algorithm;probability density function
frequent patterns;frequent patterns;uncertain data;tree-based;interesting patterns;counterpart;real-life;transaction databases;uncertain data;mining algorithm
interaction data;gene ontology;false positive;high-throughput;accurately detect;data integration;similarity measures
greedy search;bayesian network;learning algorithm;bayesian information criterion;high dimensional;learning algorithm;bayesian network;learning algorithms;greedy algorithm;feature selection method
graphical models;database;probabilistic model;probabilistic information;probabilistic databases;stream query processing;managing data;data management system
probabilistic databases;probabilistic database;attribute level;probabilistic databases;data sources;sensor data;probabilistic inference;inference techniques
input data;classification;markov models;input parameters;hidden markov models;sensor networks;data mining
9;multi-source;bayesian networks;models learned;domain models
growing number of;data streams;storage utilization;data streams;sliding window;data elements;data stream;data streams;storage requirements
intrusion detection;intrusion detection systems;pattern recognition;network intrusion detection;hierarchical classifier;hierarchical classifier;intrusion detection;behavior patterns;individual classifiers;error rate;false alarm;high cost;classifier
minimal optimization;incremental learning;semi-supervised;support vector machines;radial basis function;semi-supervised
ground truth;declarative queries;data stream applications;change detection;statistical method;window-based;data streams;hypothesis testing
window size;change detection;generation process;test statistic;data stream applications;change detection;7;data streams;approximate algorithm
high-speed;high-speed;transliteration;data streams
multiple data streams;memory resources;network data;multiple streams
event processing;multiple types of;event detection;data stream;event detection;data stream;complex event processing;score functions;unified framework;simulation results
manually labeled;multiple types of;category information;corpus-based;knowledge creation
clustering;text mining;classification problem;classification;sentiment classification;related words
tree matching;web pages;web data extraction;data extraction;data records;web site;web data extraction;web page
clustering;heterogeneous systems;discover meaningful

clustering;domain experts;association rule mining;real-world;case study;data sets;association rules;association rule;spatial data mining;association rule
clustering;running times;temporal data;similarity measure;spatio-temporal;execution times;clustering validity;compact representation;moving objects;similarity measure between;temporal data;segmentation algorithm
data structures;query patterns;aggregation techniques;image databases;databases
pattern mining;spatio-temporal;representation language;sequential patterns;temporal logic;sequential patterns;temporal data;spatio-temporal
stock market;web users;high precision;learned model;data mining
community structures;large-scale;real-world;social network;hierarchical bayesian model;social networks;large-scale social networks
feature space;feature- space;classification;meta-data;visual features;extraction accuracy;meta-data;visual features;probability model
clustering;clustering;density-based;color space;image data;automatic generation of;automatically generate
semantic annotation;semantic annotation;large volumes of;artificial neural networks;large collections of;fine-grained;high- quality;semi-automatic;multimedia data;semi-automatic
clustering;text documents;greedy algorithm
tensor space;video data;temporal order;tensor space;data sets;activity patterns;human activities;video sequences;low-dimensional;activity patterns
similar queries;keyword queries;similarity measure;query expansion;query expansion;search engines;digital library;search engines;semi-automatically;log data;query expansion;natural language
temporal relations;sensor data is;interesting patterns;knowledge discovery;data mining;real world
spatio-temporal;data mining;time series;application domains;data set;data sets;knowledge extraction;missing data;principal component analysis;missing values
clustering;taking into account;spatial clustering;12, 13, 14;spatial location;normal distribution;covariance matrix;likelihood function;similar characteristics
clustering;classification;anomaly detection;real datasets;time series;time series;similarity search;sensor networks;data mining;temporal data
sensor data;spatial knowledge;spatial knowledge;prior knowledge
training examples;regularizer;data mining applications;sampling strategy;vector machine;classifier;baseline methods;svm classifier;svm classifiers;labeled examples;classification task;objective function;classification error
transfer learning;test data;target domain;learning task;positive examples;entropy based;transfer learning;additional information
generation algorithm;relevant features;secondary structure;secondary-structure;biologically relevant;secondary-structure
statistical methods;statistical approaches;sequence patterns;vector machine;cross-validation;hidden markov model;human genome;data mining tools;gene expression;prediction models;computational methods
multiple databases;free text;database;databases;databases;high recall
factor analysis;classification;land cover;discriminant analysis;regression tree;land cover;factor analysis;spatial data mining
prediction accuracy;data mining technique;remote sensing;correlation coefficient;tool called;prediction methods;knowledge-based;regression tree;cross-validation;decision support systems;data mining techniques;predict future;spatial patterns
classification scheme;supervised learning methods;real datasets;geographic regions;semi-supervised learning;classification;remote sensing;land cover;accuracies;class distributions;learning algorithm;large number of;hybrid approach;parameter estimates;training samples;remote sensing images;databases;semi-supervised;statistical model;classifier;key words
mining task;apriori algorithm;itemset mining;complex relationships
spatio-temporal
digital images;pattern analysis;large collections of
text mining;microarray analysis;high-throughput;gene expression data;derived data
software architecture;discrete event systems;database
objective rule;case study;data pre- processing;data mining process;medical databases;data mining;post-processing;test database;time-series;algorithm selection;extraction methods;time- series;data mining;time-series;rule induction methods;data mining processes;pattern extraction
data mining;data analysis;clinical data;real-world;data mining;medical data;temporal dimension;mining process;sequential patterns;case study;4;extracted knowledge;sequential pattern mining
classification;decision support;image data;supervised machine learning;essential information;image classification;image data
trajectory patterns;data mining methods;trajectory patterns;database;spatio-temporal;privacy concerns;problem statement;position data;privacy preserving;road network;moving objects databases;np-hard
clustering;privacy-preserving;nn queries;classification;privacy-preserving;privacy- preserving;large data sets;data sets;outlier- detection;vertically partitioned data
clustering;data streams;privacy-preserving;clustering algorithm;clustering algorithm;distributed data streams
database;database queries;aggregate queries;modern database;privacy requirements;statistical databases
semi-honest;trade-offs;data mining applications;privacy-preserving distributed;privacy-preserving;data mining algorithms
vertically partitioned;logistic regression;databases;distributed databases;statistical analysis;logistic regression;data collected;vertically partitioned;data mining algorithms;privacy-preserving data mining
clustering;data values;unsupervised learning;matrix factorization;privacy preserving;benchmark dataset;nonnegative matrix factorization
clustering;cluster analysis;medical data;time series;trajectory data;time-series;cluster analysis
image features;image database;decision-support;data mining;decision-making;medical imaging;data mining;margin
learning algorithms;learning algorithm;collected data;intelligent systems;data collected from;data mining;health monitoring
clustering algorithms;clustering results;prediction model;database;large quantities of;decision-tree;data set;knowledge discovery;classification results;databases;time-series;clustering validity;data-mining techniques;classification algorithms
database;data mining;decision tree;data mining;chi-square;logistic regression;data mining;risk factors
decision support systems;medical applications;feature extraction;high computational cost;data fusion;image database;content-based retrieval;data mining;image data;human brain;complex systems;data preparation;real life;data management;data mining;storage requirements;ad hoc;database schema;unstructured data;query results
simulated annealing;movie database;artificial data;generative model for;data clustering;generative model;relational data;model parameters
machine learning methodology;community based;optimization problem;agent- based;expectation-maximization;learning performance;style algorithms
classification;pattern recognition;relevant features;distance metrics;distance metric learning;learning algorithms;feature-based;machine learning;database;data preprocessing;feature selection;model-building;data mining;distance metric;dimensional data;benchmark datasets;learning method;nearest neighbor
estimation accuracy;network size;network applications;association rule mining;estimation technique;traffic monitoring;estimation process;data freshness;sensor data;data mining
clustering;clustering;color information;cluster analysis;color space;algorithm named;data mining;optimization criteria;cluster analysis
power law;data collection;training set size;learning curve;sample size;error rates;decision- tree;power law;classifier;power law;classifier performance;data mining process;error rate;classifier;benchmark datasets
data objects;high-dimensional;data mining technique;data points;distance-based outlier detection;distance function;categorical data;categorical data;mining results;distance-based
input space;objective function;feature selection;svm) classifier;support vector;support vector machines;classifier
predictive accuracy;cross- validation;random samples;vector machine;global optimization;cross-validation;statistical method;support vector;observation data
evolutionary algorithms;algorithm's performance;fitness;simulation results;fitness
linear program;multiple criteria;classification;classification;linear program;vector machine;multiple criteria;regularization;real-life;mathematical framework;quadratic program;data mining problems;objective function
clustering;high-dimensional datasets;document clusters;algorithm called;document datasets;clustering algorithm;document categorization
neural network;predictive modeling;neural network;network topologies;neural networks
score based;classification;classification;association rule mining;rule mining;weighting scheme;association rules;classifier
active learning;logistic regression;semi-supervised setting;special structure;kernel matrices;active learning;accurate classifiers;learning method;logistic regression;data points;semi-supervised;active learning algorithms;semi-supervised;class probabilities
classification problem;linear model;classification;classification;database;data set;real life;classification method;credit card;mathematical framework;high dimensionality
frequent itemsets;frequent itemset;discovery algorithms;frequent itemsets;frequent itemsets;decision diagram;database;candidate itemsets;prefix tree;association rules;data mining;data mining;association rules;benchmark datasets;source database
collaborative filtering;user-item;collaborative filtering;collaborative filtering;nonnegative matrix tri-factorization;matrix factorization;nonnegative matrix tri-factorization;benchmark data sets
key points;data sets;large number of;execution times
high-dimensional;latent space;low dimensional;input data;real world data sets
mining task;mining algorithms;real-world;large number of;sequential patterns;algorithm called;discover patterns;wide range;higher quality;sequential pattern
semi- definite programming;semi- supervised learning;training data;unsupervised learning;optimization problems;numerical results;measurement noise;maximum margin;support vector;learning problems;support vector;semi-supervised;support vector machines;semi-definite programming;classification algorithms
topic models;topic models;large document collections;algorithm called;unsupervised fashion;large collections;highly scalable;long-distance
latent dirichlet allocation;topic models;document collections;speed-ups;unsupervised fashion;em algorithm
clustering;clustering;data analysis;low support;hierarchical clustering;market-basket;data structures;market-basket data;market-basket;clustering methods;partitional clustering;clustering approach
search methods;regularization;random variables;pairwise constraints;regularization;supervision;semi-supervised clustering;clustering performance;semi-supervised clustering;text clustering;prior information;bayesian framework;pair- wise constraints
public domain;pattern recognition;feature selection algorithms;machine learning;selection methods;feature subset;feature selection methods;feature subset selection;learning performance
high utility;web pages;web usage mining;database;information service;pattern mining;real-world;pattern mining;service providers;user preferences;traversal patterns;databases;mining" algorithm;web page;rich information;decision making
link prediction;collective classification;correct classification;object classification;link prediction;wide range;real world domains;object classification
clustering;clustering;multi-type;clustering approaches;data objects;heterogeneous data;heterogeneous data;high- order;higher-order;web mining;low dimensional;dimensional data;real data;clustering algorithm;high-order;information retrieval;higher order
conditional entropy;structural patterns;graph-based data;graph-based data;anomaly detection;real-world;detecting anomalies;graph-based;anomalous behavior;graph- based;specific domains;data mining;entity/relationship
large graph;large graph;database
feature space;string kernel
classification problem;networked) data;related data;network classification;instances;relational data;classification algorithms
clustering;data mining technique;minimum support;frequent subgraphs;clustering technique;graph-based;graph-based;requires minimal;document clusters;document clustering;text documents;document clustering;human interaction;text mining techniques
pair-wise;information theoretic;weak classifiers;classifier;term dependency;feature selection;text classifiers;benchmark data sets;term dependency
synthetic data sets;classification;instances;classification accuracy;data set;collective classification;network structure;local structure;classifier;relational data
clustering algorithms;networked data;clustering method;real data;input parameters;hierarchical structure;hidden structures;social networks;clustering algorithm;metabolic networks
normal distributions;neural network;real dataset;science applications;spatial structure;clustering algorithm;observation data;neural network
network models;dynamic programming;statistical properties;random variables;probability density function;observed data;dynamic programming;statistical analysis;random variable;clustering algorithm;maximum likelihood;dynamic programming;distance-based
clustering;clustering algorithm;clustering analysis;database
sampling methods;learning algorithm;vector machine;machine learning;applications involve;gene interactions
fuzzy classification;data point;fundamental properties;fuzzy-classification;classification;semi-supervised learning;supervised learning;unsupervised learning
case study;interaction networks;expression data;case study;biological network;high-throughput;protein-protein interaction network;interaction networks;multiple sources;biological network;inference algorithms;protein-protein interaction network
statistical methods;multi-level;clustering results;microarray data;multi-level;expression data;prior knowledge;data set;independent component analysis;biologically relevant;selection methods;statistical test;knowledge-driven;knowledge-driven
numerical experiments;gradient-descent;reinforcement learning;reinforcement learning;cost function;optimal strategy;optimal control
medical records;pattern recognition;unsupervised learning;index terms;data collections;feature extraction;data sets;information retrieval;knowledge discovery;data sources;pattern recognition;complex patterns;clustering methods;databases;data repositories;human body;discovery algorithm
pre-defined;optimization techniques;genetic algorithm;classification accuracy;optimization method;selection method;optimization problem;candidate set;numerical results;svm classifier;support vector machines;kernel parameters;objective function
regularization term;classification;data points;large scale;vector machine;incremental version;large datasets;high dimensionality;test results;classification algorithms
kernel learning;label information;svm based;kernel matrix;label information;classification performance;computationally expensive;handwritten digits;svm classifier;memory intensive;separability;semidefinite programming
genetic programming;svm algorithm;numerical experiments;vector machine;hybrid model;kernel functions;kernel function;kernel functions;classification problems;svm) algorithm
hidden markov models;machine learning;hidden markov model;protein sequences;objective function
structural learning;learning algorithm;high throughput;learning algorithms;bayesian network;breadth-first-search;computational complexity;np-hard;bayesian network;bioinformatics research
gene ontology;pattern recognition;classification;knowledge embedded in;semantic knowledge;feature extraction;semantic similarity;expression data;expression data;data set;data sets;expression levels;extraction algorithm;dimension reduction;classifier;high dimensional space
classification;microarray data;sample size;classification accuracy;integrating data from;data integration;data integration;statistical analysis
kernel regression;nearest neighbor;gene expression data;radial basis function;linear discriminant analysis;gradient descent;logistic regression;bayes classifier;base classifier;support vector machines;artificial neural networks;classifier;learning vector quantization
state space;maximum likelihood estimation;spatial-temporal;neighborhood structure;time series;kalman filter
learning process;regularization;unlabeled data;classification;underlying structure;maximum margin;long running;1;semi supervised learning;kernel framework;semi-supervised;regularization
genetic programming;fixed length;genetic algorithms;regression problems;differential evolution;parameter optimization;real-valued;tree structure;evolutionary algorithm;gene expression
genetic programming;decision trees;genetic programming;single attribute;decision tree;internal node
instance;genetic algorithms;recognition rate;genetic algorithm;decision rules;key words;instances;genetic algorithms;knowledge acquisition;data mining;rough sets;rough sets;fold cross validation
recurrent neural network;gradient based;higher order;recurrent neural networks;recurrent neural networks
evolutionary process;genetic algorithms;genetic algorithms
nearest-neighbor search;nearest-neighbors;local search;genetic algorithm;fast convergence;combinatorial optimization problem
gene) selection;regularization;high throughput;classification;classification framework;dimensional data
total number of;search algorithm for;high dimensional;closed sets;patterns mined;mining algorithms;optimization techniques;constraint-based;mining algorithm
itemset mining;input data;frequent patterns;level- wise;generic framework;interesting patterns;frequent itemsets;data sets;transaction database;frequent pattern mining
multiple databases;partition based;database;association rule mining;association rule mining;navigation paths;databases;association rules;dimensional data;information technology
large datasets;rule set;cluster-based;rule generation;rule extraction;association rules;association rules
frequent itemsets;categorical data;clustering process;frequent itemsets;clustering categorical data;hierarchical clustering algorithm;clustering algorithm called;clustering categorical data
xml documents;learning algorithms
dynamic programming algorithm;memory usage;incremental algorithm;finding optimal;incremental version;burst detection;hidden markov models;streaming data;user activity
data-driven;training set;phase transition;phase transition;learning algorithm;general case;relational learning;learning problem;relational learning;heuristic search
problem solving;learning agent;negative examples;knowledge-based;application domains;problem solving
generative-discriminative;sufficient conditions;discriminative models;counterpart;numerical experiments;unified framework;error rate;classifier
training set;unlabeled data;distance metrics;learning procedure;active learning;learning tasks;data set;unlabeled examples;scale poorly;text classification;instances
pre-defined;multi-level;markov random field;video segmentation;image segmentation;user-defined;energy minimization;segmentation results;theoretical analysis;label propagation;natural images
recognition rate;shortest-path;optical character recognition;image-based;document images;shortest path;character segmentation;segmentation accuracy;document images;character segmentation
feature space;models learned;generalization performance;text documents;knowledge learned;text classification;text classification;classification task
synthetic data;probabilistic model;behavior patterns;machine learning;decision making;community based;style algorithms;decision making;expectation-maximization
variational inference;early stage;hidden markov models;hidden markov model;usage information;automatically discover
web scale;training data;high accuracy;database;context-sensitive;web text;higher accuracy;large database
principal components analysis;pattern recognition;covariance matrix;databases;databases;covariance matrix
ubiquitous computing;hidden-markov-models;indoor environment;recognition rate
agent based;simulation study;agent based;multi-agent;data set;reinforcement learning;machine learning;1;data sets;2
classification;vector machine;machine learning;homeland security;accurate predictions;process models;machine learning;learning method
real-world;information extraction
attribute values;classification model;rough set;classification;decision rules;rule-based;classification;large number of;software quality;rule sets;case study;domain values;rough set theory;classification errors;high cost;software quality;rule-based;model selection
probability estimates;auc;classification trees;small sample sizes;class probabilities;probability estimation;class probabilities
recognition rate;potential function;temporal sequences;neural network;temporal sequence;implementation issues;neural network
supervised learning algorithm;numerical analysis;nearest neighbor;collaborative filtering;classification;data mining techniques;linear algebra;sparse matrices;vector machine;regression algorithm;condition number;condition number;direct computation;condition number;sparse matrix
evolutionary algorithms;increasing complexity;general-purpose;parameter estimation
additional knowledge;plans;partially observable;partial plans;learning algorithm;inductive logic programming;generate plans;situation calculus
design space;noisy data;accurate models;noisy data;high precision
case studies;hybrid model;data collections;large number of;time series;time series;data set;prediction method;moving average;artificial neural networks;artificial intelligence;moving average
machine learned;decision tree;neural nets;machine learned
medical records;linear support vector machines;large-scale;negative examples;instance;database;em algorithm;regularization parameter
training set;neural networks;regression tasks;generalization performance;regression tasks;accurately predict;input space;squared error;generalization ability;high dimensional spaces;neural network;high dimensional spaces
web-based;web based;control algorithm;motion control;motion control;web interface;machine learning techniques;robot learning;database management;machine learning approaches
web information retrieval;web based;web pages;language identification;language identification;machine learning;high accuracy;language identification;artificial neural networks;multi-lingual
naive bayes classifier;web based;users' preferences;user models;domain model
dimensionality reduction techniques;training data;active learning;label information;classification;active learning;training examples;active learning;classifier;dimensionality reduction techniques;document frequency;dimensionality reduction;nearest neighbour;principal components analysis;classifier;dimensionality reduction;unsupervised techniques
training images;object recognition;feature extraction;training data is;classification accuracy;component analysis;computational complexity;optimization problem;training data;computational cost;dimension reduction;low dimensional space
auc;data-mining;end users;real-world;data mining;user's perspective;machine learning;case study;data mining;data mining models
decision tree classifier;inference engine;rule refinement;data set;higher accuracy than;rule refinement
selection problem;real-world;learning algorithm;sampling method;clustering methods;clustering problems;clustering algorithm;clustering algorithm based on;hierarchical agglomerative clustering;model selection
nearest;knowledge bases;dataset characteristics;data mining;user's preferences;model selection
learning process;robot control;state spaces;high dimensional;reinforcement learning;reinforcement learning;action space;behavior model
evolutionary algorithms;problem remains;high dimensional;problem domains;reinforcement learning;temporal difference;state-space;learning problems;theoretical analysis
data mining and machine learning;classification problem;classifier;positive class
continuous data;synthetic data;gaussian distributions;fully supervised;mixture models;15, 16;bayesian networks;11, 13, 21;bayesian networks;learning tasks;logistic regression;semi-supervised learning;2, 8, 4;semi-supervised;mixture models;real world scenarios
training data;imbalanced data;decision trees;instances;genetic algorithms;data mining;classifier;classification model
clustering;clustering;energy function;input data;data points;approximately optimal;energy minimization;tabu search;image data;low-dimensional;synthetic data
bayesian network;learning bayesian networks;classification accuracy;naive bayes;search space;bayesian networks;naive bayes classifiers;network structure;classification task;classifier
clustering;clustering;cluster structure;true values;data set;bipartite-graph;document clustering;cluster structures;similarity measures
real world;training data;training process;databases
clustering;world wide web;cosine similarity;web site;similarity measure;clustering techniques;similarity search;search engines;similar documents;clustering;related documents;similarity measures;vector space model;vast amounts of
music recommendation;web sites;recommendation systems;personalized services;large number of;user's preference;users' preferences;clustering algorithm;clustering algorithm
ensemble learning;classification;ensemble classifiers;streaming data;underlying data distribution;streaming data;data streams;classification task;data chunks;streaming data
network traffic;feature extraction;feature extraction;cross-correlation
accurate models;relevant features;microarray data;gene clusters;large number of;microarray data analysis;data set;statistical significance;learning task
unsupervised classification;document images;unsupervised classification;document types;threshold based;clustering techniques;unsupervised algorithm;clustering algorithm;document image
classification;classification accuracy;classification performance;training samples;feature selection;feature selection methods;small samples;high dimensionality;support vector machines;training sample
high-level;representation language;knowledge base;negative examples;knowledge base
web-based;world wide web;statistical machine translation;sentence-level;parallel corpora;parallel corpora
inference method;parameter optimization;flow field;optical flow estimation;probabilistic framework;temporal coherence;computationally feasible;motion estimation;dynamic bayesian network;optical flow;noisy data;motion information;optical flow estimation
learning algorithms;learning algorithm
database;learning strategy;evaluation function;artificial neural network;brute-force
hybrid model;input output;neural network;neural network
regulatory networks;computational biology
data set;mathematical programming;mathematical programming;reduction techniques;dimensionality reduction techniques;relevant data;dimensional data;breast cancer
support vector machines;case study;taking into account;breast cancer;large number of
gene expression data;clustering solution;cluster analysis;data clustering;biological information;clustering methods;gene expression;computational methods
false positives;decision tree;classification;database;vector machine;probabilistic database;probability models;large number of;database;high-throughput;classifier;artificial neural network;linear discriminant analysis
false positives;test set;dna sequences;large number of;data set;svm classifier;support vector machines
patient data;neural networks;neural networks;neural networks
discovered patterns;data describing;data obtained from;natural language
medical records;provenance information;decision-making;index terms;information gathering;decision support systems;increasing number of;computational costs;computational requirements;simulation framework;data mining;semi-automatically;decision processes;decision processes
sample selection;free text;classification model;mutual information;test data;natural-language text;active learning;unstructured text;active learning;retrieval performance;12;active learning approach;unlabeled data;semi-supervised;labeled data;query points
clustering;clustering;key words;multiple factors;breast cancer
web-based;classification;database;web service;pre-computed;typically involves;multiple sequence alignment;input sequences;clustering algorithm;protein sequences;comparative analysis
web search;program committee;web search;international conference on;data mining
spam detection;social networking;information management;information management
search results;real world;relevance score;page content
query distribution;algorithm runs in;theoretical results
data structure;similar objects;reuters corpus;similar object;similarity functions;similarity function;linear space;query processing;nearest neighbor;pairwise similarity;data representations;nearest neighbors;nearest neighbor search
information discovery;search engine;text queries;complex data;business intelligence;databases;faceted search;enabling users to;relational data
market segments;query suggestion;relevant data;similar users;em algorithm;modeling techniques;search logs
training set;document retrieval;rank-based;learning algorithms;ranking svm;perceptron algorithm;document pairs;ranking functions
user navigation;web search;structural properties;web sites;user behavior;large sample;user clicks;traffic data;web users
training set;test set;machine learning;algorithms require;model parameters;discounted cumulative gain;ranking functions;document scores;ir metrics
data-gathering;click logs;relevance information;real-world;search engine;logistic regression model;search engine
web graph;pattern mining;data sets;frequent itemsets;designed to support;random access;compression scheme;specifically designed to;random walks;frequent pattern mining
statistical properties;scale-free;support vector regression;degree distribution;database;stochastic model;neighborhood structure;large-scale;poisson process;tree structure;network evolution;social networks;real world networks;poisson model;small world
social science;group members;online communities;diverse set of;long-term;online communities
neighborhood graph;real-world;common interests;bipartite graph;high-level;power-law;structural properties
search results;web directories;classification model;naíve;classification;large-scale;hierarchical structures;hierarchical classifier;classification methods;classifier;search result;training data;bayesian classifier;effective classification;web search results;class labels;ambiguous queries;classifier
false positives;web search;classification;web queries;heuristic rules;web queries are;dictionary-based;logistic regression;web queries;search quality;machine-learning algorithms;manually labeled;test sets;classification algorithms
training set;taking into account;document classification;document classification;database;learning strategy;document collections;digital library;temporal evolution;training sets;class distribution
wikipedia articles;occur frequently;baseline models
social media;high quality;high-quality;social media;classification framework;information sources;social interactions;content information;user-generated content;test case;community question/answering;social media sites
web search;vice versa
real-world;virtual communities;blog posts
detection techniques;data mining techniques;web spam;classification;natural language processing
language constructs;product review;user generated content;data set;linguistic patterns;context dependent;product features;customer reviews;opinion mining;natural language
search performance;monte carlo;specific information;modeling framework;decision-making;search advertising;search engines;rates;web search results;search engine;real world;markov chain
world wide web;semantic knowledge;concept hierarchy;statistical information;search engine;semantic information;search result pages
domain experts;tracking objects;relevance feedback;object detection;multimedia information;user preferences;data mining;multimedia data mining;digital media;hong kong;point based;multimedia information retrieval;graph mining;multimedia data mining;association rule mining;discover patterns;data processing;video streams;multimedia databases;data mining and knowledge discovery;database;international workshop on;homeland security;multimedia data
parameter settings;large number of;data mining
clustering;social media;clustering results;web pages;visualization tools;spectral clustering;information flow;influential nodes;ranking algorithms;social network;streaming data;clustering techniques;web site;graph analysis;temporal smoothness;long-term;temporal data;blog data;graph clustering;short-term
hidden markov models;search process;visual features;video content
multi-modal;mining task;mining frequent patterns;distance function;sensory data;declarative framework;mining tasks
classification;classification framework;real-world;learning algorithm;image data;integrating multiple;classification method;data mining;image classification;machine learning;learning framework;visual learning;mining tasks
detection techniques;appearance-based;svm-based;feature-based;key points;object detection;salient-points;pose variations
semantic video;database;video data;information contained in;time series;neural network;relevance feedback;semantic retrieval;learning framework;individual users;monitoring systems;content-based image retrieval
association rule mining;database;association rule mining;data mining research;frequent items;association rules;transactional database;association rule;fp-growth
redescription mining;biclustering algorithms;gene ontology;data mining applications;biological datasets;high-throughput;scientific datasets;data mining;relational database;data mining algorithms;multiple aspects
life sciences;web pages;knowledge bases;database;domain specific;web search engines;semantic web;web services;relevant information;automatically extract;automatically discover;natural language
high-dimensional datasets;lda model;classification;solution path;computational cost;gene expression;temporal patterns;feature extraction;linear discriminant analysis;linear regression;irrelevant features;equivalence relationship;feature selection;regularization parameter;spatial patterns;gene expression;model selection
high-dimensional;data set;discriminant analysis;gene ontology;naïve bayes classifier;gene expression data;classification;dimension reduction;linear discriminant analysis;discriminant analysis;expression data;classification performance;enormous amounts of;microarray analysis;sample data;high dimensionality;model fitting;gene regulatory networks;high throughput
semi-structured;text mining;mining frequent patterns;probabilistic model;real datasets;predictive power;real data;learning scheme;training data;web mining;large datasets;data mining;data mining applications;ordered trees;probabilistic models;space complexity
data lineage;input data;database;uncertain data;databases;relational operations;probabilistic databases;query processing in;base data;query processing;uncertain data;relational databases;query results;data management
efficient parallel;query optimization;sample size;sample-size;sampling algorithm;database;bounded-size;incrementally maintaining;data items;random sample;information integration;base data;data stream;sampling scheme;data-mining tasks;memory management
large numbers of;completeness;data stored;replication;data availability;large scale;large-scale;network management;data sets;highly distributed;hash table;real-world;explicit feedback;completeness
distributed algorithm for;algorithm combines;markov-chain;web pages;web search
data redundancies;data structure;storage cost;design) process;schema design;database;functional dependency;partition-based;data manipulation;xml documents;xml databases;data transfer;functional dependencies;normal form;efficient discovery of;stored information;xml schema;increasingly popular
keyword queries;join operations;index entries;keyword-based;query performance;search queries;range queries;keyword search;extensive simulations;real world data sets
radio frequency identification;sensor devices;data independence;rfid data;sensor devices;high-level;statistical framework;low-level
mapping composition;relational operators;user-defined;mapping composition
information overload;relevant information;decision makers;storage capacity;application domains
clustering;data mining technique;distance function;visualization techniques;clustering approaches;parameter settings;interesting patterns;clustering result;data resources;interactive visualization;visual analysis of;irrelevant attributes;subspace clustering;data mining;clustering algorithm;dimensional data;individual objects;clustering analysis;subspace clusters;real world;subspace clustering algorithms
group structure;visual analysis of;visual representations;social networks;visualization tools
clustering;visual analytics;data mining techniques;semantic similarity;valuable information;knowledge learned;knowledge discovery;candidate set;comprehensibility;real datasets;user studies;visualization techniques;graphical representation of;helps users;user studies
large amounts of;data analysis;visual analytics;movement data;financial data;stock market;data set;time series;large data volumes;visualization technique;visual analysis of;analysis tasks;complex data;clustering algorithm;solving complex
human perception;visual analytics;low cost;large scale;large amounts of data;computational methods;database operations;movement data;case studies;fully automatic;massive amounts of data;visual analytics;interactive visual;real datasets;mobility patterns
social network analysis;web mining
rating data;acm sigkdd international conference on;data mining;1;knowledge discovery;2;data mining
user ratings;low rank;sequence mining;time series;linear regression;edit distance;prediction method;squared error;similarity matrix;association rules;margin
test data;predictive power;logistic regression;key points;mathematical model;modeling approach;input variables
link prediction;classification approach;large-scale datasets;link prediction problem



recommendation systems;collaborative filtering;major components;machine learning;online services;major components;benchmark datasets
human knowledge;knowledge discovery;real-world;acm sigkdd;data mining;pattern mining;domain driven;knowledge discovery;domain driven data mining;data mining;real world;acm sigkdd international conference on
workshop report;agent based;network analysis;knowledge discovery;social network analysis;acm sigkdd international conference on;data mining;social network;social network analysis;web mining;data mining;web mining;kdd workshop;knowledge discovery
workshop report;mining framework;acm sigkdd international conference on;international workshop on;knowledge discovery;data mining
large volumes of data;data mining;data mining;international workshop on;online advertising;data mining;data mining problems
search systems;customer service;real world;confidence intervals;modeling assumptions
agent based;social networking;search experience;related algorithms;social network;network evolution;personal data;kdd 2007 workshop;data mining;machine learning approaches;emerging trends;social networks;multimedia content;online communities;world wide web;social web;social network analysis;web sites;user behavior;web mining;social networking;community discovery;behavioral patterns;data miners;social activities
random graph;personal information management;directed graphs;edges represent;gradient descent;transition probabilities;supervised learning
growing number of;predictive models;focused primarily on;data record;relational models;performance gains;temporal dynamics;relational data;bayes classifier;relational learning;temporal sequences;relational domains
social media;community structures;real networks;community detection;sparse graphs;social network;web pages;graph theory;real life;large-scale social networks;computational cost;real world;community detection;overlapping communities;worst case
web search;classification;community-based;individual nodes;link-based;network structure;community structure;link mining
probabilistic model;communication network;machine learning;simulated data;community based;expectation-maximization;style algorithms
takes into account;user behavior;mathematical analysis;mathematical model;allowing users to;social media sites
share information;social network;daily activities
network structure;unsupervised learning
extracting information from;random walk;data collections;semi-structured;database;hidden information;large collections of;social network;social networks;unstructured data
spectral clustering;spectral clustering;low quality;data sources;cluster sizes;location data;clustering quality
link information;classification;large-scale;link structure;link-structure;labeling problem;data sets;link-based;analyzing data;blog data;communication networks
directed graphs;recommender systems;spectral clustering;link structure;spectral methods;data set;social network;page rank;social networks;link-based;adjacency matrix;similarity metric;random walks
network analysis;automatically extracting;automatic methods;hierarchy levels;data mining;great promise;real world;user behaviors
workshop on data mining;information organization;large volumes of data;data mining;social network;international workshop on;social networks;data mining;information retrieval;natural language;data mining problems;online advertising
application requirements;data cleaning;optimization strategies;information management;business objects;case study;business intelligence;information integration
search spaces
web applications;management architecture;database;web application;database systems;data management;programming language
protocol called;multi-version;simulation study;desirable properties;update transactions;large scale;intensive workloads;replication;internet scale;concurrent updates;data processing;building block for;concurrency control;serializable;range queries
massively distributed;database;database records;original data;stored data;databases;query routing;low cost
specific algorithms;overlay networks;semantically related;semantic features;high degree of;large-scale;overlay network;incremental maintenance;peer data management systems;semantic mappings;overlay networks
type information;xquery language;database;xml data;complexity bounds;instance;data-centric;xml schema
xml documents;data structures;main memory;space usage;memory usage
np-complete
query rewriting;completeness;view definitions;source schemas;conjunctive queries;normal form;schema mappings;relational data
schema mapping;semantically related;schema matching;mapping generation;data sources;algorithms rely on;high precision
schema mapping;globally optimal;data transformation;schema matching;semantic matching;schema elements
storage devices;index entries;query performance;real-life;indexing technique;times faster than
cluster based;information systems;database;database replicas;automatic selection;high-availability;multi-tier;replication;service-oriented;data transfer;databases;data item;takes place;optimal strategy;transaction processing
tree structures;large-scale;digital content;tree structure;distributed environments;access path;managing data;large amounts of data;concurrency control;proposed protocol
optimizer;database systems;query optimizer;database design;query workload;higher degree of;physical design;objective function
storage cost;database systems;application requirements;database;replication;query load
materialized view;load distribution;auxiliary;database;query workloads;pre-computed;stochastic search;query execution;exhaustive search;greedy algorithms;disk space;database cluster;solution space;database systems
pruning strategy;local structures;false positives;pattern recognition;graph isomorphism;database;np-complete problem;graph-based;spectral graph;large graph;search problem;counterpart;subgraph isomorphism;4;pruning rules;verification framework;search efficiency
compression rate;resource consumption;transitive closure;large graph;large graphs;reachability query;numerous applications
shortest paths;shortest-path;database;large graph;user query;large graphs;road network;answer queries;temporal databases
graph structures;graph mining;frequent patterns;database;graph model;life sciences;subgraph isomorphism;post-processing;node labels;mining algorithm
music information retrieval;answer set;high degree of;index structures;scoring function;digital libraries;music retrieval;string matching
data point;encoding scheme;high-dimensional space;data points;nearest-neighbor;search techniques;partition-based;index structure;nn) search;high-dimensional spaces;high dimensional;nn search;tree index;clustering algorithm;dimensional data;query point;dimensional space
efficient querying;index structure;time series;time series;interactive applications;similarity search;meta data;query processing;dimensional data;real world;temporal data;dimensionality reduction;sensor data
data publishing;anonymization algorithm;data arrive
multiple objects;data dissemination;database operations;clustering algorithms;machine learning
network bandwidth;small groups;relevant documents;storage space;real-world;information leakage;excellent performance
search space;service calls;optimizer;web service;active xml;data elements;service-oriented;cost-based;web services;performance gains;optimization strategy
selection problem;global optimality;instances;data-analysis;problem instances;search space;2, 12;performance guarantees;query-evaluation;data-warehouse;2;optimum solution;programming model;on-line analytical processing;optimization problem;olap queries;approximation algorithm;12;high-quality;17;index selection;compare favorably with;auxiliary data;pruning algorithm
data structure;security concerns;database;databases;outsourced databases;data-base;service providers;data privacy;privacy issues;database outsourcing;database outsourcing
data objects;skyline query;database;multi-criteria decision making;effective pruning;skyline points;euclidean space;skyline queries;pruning techniques;metric space;metric spaces
synthetic data sets;user preferences;database;data items;instance;relevant data;pre-processing
algorithm performs;attribute values;multi-dimensional;threshold based;handle arbitrary;indexing structure;search space;computational cost;space partitioning;metric spaces;real world;similarity measures
data warehouses;equivalence class;data warehouse;selection queries;fact table;apriori algorithm;conjunctive queries;functional dependencies;support measure;level-wise;database table;relational table
multiple databases;query optimization;mining frequent itemsets;optimization framework;synthetic datasets;cost-based query optimization;database queries;pattern mining;query optimization techniques;multiple datasets;cost-based query optimization;query plans;cost-based;data mining;dynamic programming approach;frequent pattern mining
synthetic data sets;motion paths;communication overhead;classification;multiple objects;extraction process;motion paths;movement patterns;moving objects;applications requiring;margin;resource allocation
simple heuristics;representation language;data-base;object-oriented;real-world;database administrators;application developers;complex relationships
complex queries;database;query specification;query interface;user access;user study
keyword queries;database;dynamically-generated;logical database;information access;databases;database schemas;structured data
clustering algorithms;26;density-based;cluster structure;17;22;18;intra-class;data set;semi-supervised clustering;4;clustering methods;real-world data sets;class labels;semi-supervised;cluster analysis;data distribution
10;search algorithms;nearest neighbor;information dissemination;multi-channel;mobile devices;nn-search;19;optimization technique;query processing;nearest-neighbor queries
multi-dimensional;semi-structured data;search tools;takes into account;query interface;heterogeneous data;query performance;management systems;multi-dimensional;ir-style;allowing users to;text content;query conditions;personal information
query answers;private data;query results;real data;database
target instance;data exchange;data exchange settings;queries posed;conjunctive queries;query answering;query languages;data exchange
relational databases;query language;constraint language;databases;semantic query optimization;relational model;semantic web;data consistency;relational data;schema information;semantic interoperability;markup language;key constraints
databases;uncertain data management;uncertain databases;uncertain data;uncertain database;data mining tasks;search space;pruning methods;sensor data;traditional database;ranked queries;decision making
multimedia databases;search performance;run-length;query pattern;range query;tree index;index structure;time series;compression technique;disk page;space complexity;run-length;range search;7;compressed data;pattern matching;external-memory;biological sequences;arbitrary length
10;10, 14, 22;low frequency;xml data;keyword queries;xml documents;algorithm called;worst case;keyword search;high frequency;keyword search
distance function;database;answering queries;nearest neighbor queries;distance based;time-series;query results;frequently updated
local search algorithms;data dissemination;optimization techniques;optimization technique
false positives;database queries;fast response;collect data;sensor networks;sensor data;users' search;analyzing data;real world;approximate results
data arrives;main memory;join algorithms;approximate join;data streams;query processing techniques
load distribution;taking into account;incremental evaluation;takes into account;sliding window;join operators;network nodes;performance tradeoffs;query processing;network traffic
spatial datasets;tree based;brute-force;processing algorithms;decision support;data distributions;large datasets;computational cost;operator;algorithms for computing;spatial join
search engine
data mining methods;data miners;mobile phone;rates;specific characteristics;data mining
control policy;data-warehouses;data warehouse;data warehouses;business intelligence;memory requirement;control mechanism;workload management;online transaction processing
web applications;data mining techniques
data mining tool;complete set of;block size;application scenarios;application requirements
communication patterns;large-scale;social network analysis;real-world;communication network;social network;social ties;social ties;mobile phone;primary goal;spreading activation;social networks
parallel implementation;large-scale;computing environment;data stream;highly scalable;large data volumes;space-partitioning;rates;data stream management system;highly scalable;stream queries;speed-ups
end-user;data services;7
ontological knowledge;matching techniques;software components;user requirements;web services
context information;end-users;web applications;information services;web-based;web sites;mobile devices;mobile applications;web services;current location;web browser;change frequently
database;java based;context based;context-based;access control
heterogeneous data;cryptographic techniques;file systems;data confidentiality;application developers;data storage
control policy;access control
web pages;instances;semi-structured;web sites;large number of;instance;semantic web;search engines;database;search engine;real world;automatically extracts
mining task;concise representation;free text;mining frequent patterns;taking into account;large volume;temporal information;huge amounts of;knowledge workers;mining frequent patterns;temporal data;frequent pattern mining
individual queries;scientific data;increasing number of;query language;data-sets;distributed queries;data stored in;query plans;arbitrarily complex;query plan;query languages;life-science;scientific domains
web site;graph database;query engine;databases
inductive database;instance-based learning;rule-based;post-processing;data mining;clustering;classification models;relational model;software architecture;sql extension;activity relationships;pattern mining;gene expression analysis;pre-processing;relational data;database;multi-relational;query language;data model;distance measures;feature selection;knowledge discovery process
sensor technology;question arises;grid-based;sensor network;distributed data streams
virtual machine;database systems;database;virtual machines;database research;databases;physical resources;operating systems
completeness;database systems;database;large number of;application servers;complex applications;transaction throughput;service-oriented;sensor data;database technologies;query optimizers;traffic control;building block;database researchers;resource consumption
web service;key features;database management system;web services;databases
fast approximate;query optimization;data visualization;selectivity estimation;synthetic data sets;query workloads;keeping track of;minimum description length;data mining;query answering;model fitting;minimum description length
query result;instance;aggregate queries;probabilistic guarantees;error guarantees;monitoring applications;bandwidth consumption;sensor networks;sensor data;sensor nodes;sensor networks;query results;error bounds
semantic caching;real-world;web sites;query semantics;keyword-based;database-backed;web caching;query containment
multi-dimensional;access method;update cost;large number of;spatiotemporal queries;query performance;moving objects;query processing
mining algorithms;database;social network analysis;complete set of;time series;traditional clustering algorithms;location data
xml updates;update cost;xml data;internal node;labeling schemes;query performance;xml query processing
response times;database;moving object;sliding window;data items;join algorithms;stream query processing;rates;query processing;sensor network;data streams;time-series
domain knowledge;model complexity;unseen data;classification techniques;recommendation systems;predictive accuracy;hybrid approach;association rule mining;feature extraction;large number of;web search;support vector machines;markov model
evaluation cost;schema-free;keyword-based search;stack-based;partial knowledge;database queries;multiple data sources;document structure;schema-free;xml document;aware query;xml structure;data integration
physical design
database;correct answer;sql queries;4, 3
cosine similarity;database;data cleaning;database applications;real datasets;increasing number of;tf-idf;complex queries;similarity predicates;string matching;data integration
everyday life;context-aware;comprehensive evaluation;context models;context modeling
rdf data;database;structured data from;web sites;web site;web browser
database;acm sigmod;real data;information retrieval;user interfaces;web mining;databases
database design;distributed databases;active databases;databases;data management systems;data management
databases;databases;international workshop on;international workshop on
database;application domains;international workshop on;sensor networks;data management;sensor network;sensor networks;sensor networks;data management
dkvk;database;uncertain data;dkvkd;databases;data management
service selection;international workshop on
analysis reveals;web contents;xml-based
web service;semantic annotation;scalable solution;web services
web service composition;external information;web services;web service composition
clustering;probabilistic latent semantic analysis;web services;database design;service-oriented;web services;huge number of;learning method
context information;completeness;context-aware;semantic representations;similarity-based;context-awareness;application domains;semantic web;semantic web services;web services;real-world;vector spaces;service discovery;semantic similarity between
web service;context-based;semantic matching;makes sense;web service composition;web services composition;context-based;web services;term matching
mobile environment;mobile devices;logical structure
service discovery;automatically discover
computing infrastructure;data management techniques;database performance;data management;international workshop on
data sharing;database systems;database;ad-hoc;large-scale;distributed databases;databases;data management
xml databases;store data;database;index structures;xml documents;query processing;databases
business activities;business process;business processes;5;xml-based;execution traces
data management;data management
database
acm sigmod;databases;international workshop on
distinguishing features;online communities;structured data;databases
biological networks;social science;international workshop on;knowledge discovery;semantic web;chemical compounds;graph data;data type;complex structures;wide range;complex structures
privacy concerns
knowledge management;data warehousing and olap;acm international workshop on
historical information;database administrators;semantic web;information retrieval;databases
acm sigmod;large scale;database systems;international conference on
search results;high throughput;data centers;machine learning;data mining;data mining algorithms
database;query specification;database management;data sets;information visualization;filtering techniques;visualization techniques;query languages;visual exploration of
large volumes of data;data types;data mining approaches;streaming data;rates;high-level
main memory;computation cost;spatial datasets;pruning strategies;point set;service providers;euclidean distances;distance-based;bipartite graph;quality guarantees;spatial databases
tree index;database;spatio-temporal;frequent updates;moving objects;tree index;change frequently;human intervention;spatio-temporal;data distribution
shortest paths;network distance;shortest path;worst case;road networks;theoretical analysis;spatial network;nearest neighbors;spatial databases
total order;ground truth;applications involving;dynamic programming algorithm;fundamental problem
information discovery;web based;ad-hoc;real datasets;web sites;probabilistic framework;rank aggregation;query keywords;higher level
ranking queries;verification framework;database systems;data warehouses;execution model;aggregate queries;high-level;bounded memory;effective pruning;data cubes;candidate generation
minimum number of;real datasets;privacy concerns;application domains;social network data;network data
past queries;query result;database;sensitive information;real data;privacy guarantees;privacy preserving;statistical analysis;data analysis;protect privacy;query results
nearest-neighbor search;query execution;location based services;data mining techniques;information retrieval;user location;large number of;mobile devices;single snapshot;cryptographic techniques;privacy guarantees;location-dependent;protect privacy;location based services
set cover;computational complexity;multiple queries;performance guarantees;optimal algorithms;data streams;maximum number of;greedy algorithm;data stream systems;filter ordering;average number of;approximation ratio;np-hard
query evaluation;pattern matching;pattern queries;regular expression;evaluation model;stream query processing;click stream;financial services;event streams;performance gains;pattern matching
regular expression;network intrusion detection;network security;limited memory;content distribution;real-life data sets;regular expression;real-life;data streams;deterministic finite automata;deterministic finite automata
clustering;clustering algorithms;synthetic data;computational cost;sampling-based;main memory;large number of;data matrix;data matrices;decomposition methods;microarray analysis;large datasets;data mining;matrix decomposition
clustering;clustering;parameter-free;cluster structure;independent components;cluster model;clustering result;clustering approach;data set;independent component analysis;gaussian distribution;clustering methods;independent components;clustering method;data distribution
high-dimensional;feature space;computational complexity;multimedia databases;multimedia applications;similarity measure;high quality;approximation techniques;response times;dimensionality reduction techniques;similarity search;feature representation;query processing;feature representations;dimensionality reduction;real world data sets
search space;skyline query;uncertain databases;real-world;pruning methods;sensor data;skyline queries;uncertain data;query processing
efficient parallel;skyline points;database research;skyline query;data points;skyline computation;data partitions;query processing in;distributed settings;scalable solution;grid-based;skyline queries;partitioning scheme;space partitioning;pruning power;space partitioning
data structure;streaming environment;skyline computation;categorical data;streaming data;partially ordered;partially-ordered
low cost;database;database applications;frequent updates;general-purpose;high availability;multi-media
completeness;communication overhead;database;database applications;cayley;range queries;brute force;data management system;network structure;load balancing;data management systems;graph-theoretic;indexing methods;data distribution
efficient search;maintenance cost;query efficiency;query frequency;massive amounts of data;tuple-level;coarse-grained
supply chain;encoding scheme;storage scheme;sql queries;valuable information;query templates;rfid data;query performance;supply chain management;wide range;query processing;queries efficiently;rfid technology
tree structures;path expressions;xml data;xml query;xml queries;xml data management;rich semantics;relational data;schema evolution;tree-pattern;functional dependencies;xml structure;relational data
ranking scheme;query result;small size;query biased;web data;search engine;xml search;meaningful information
xml documents;design choices;caching scheme;distributed applications;query load
query optimization;xml query language;database
real data sets;dynamic programming algorithm;web search;fundamental problem;record linkage;index structures;fixed-length;index structure;query performance;high-quality;cost-based;cost-based;queries efficiently;lower bound;special case;approximate queries;approximate string
distance measure;matching algorithm;database;time series;data set;data sets;efficiently identify;brute-force search
real-world datasets;data rates;lower bound;sample size;large data streams;random samples;sliding window;uniform sampling;data stream;upper bound;sliding windows;sampling scheme;random sampling
search space;classification;real world datasets;temporal patterns;closely related;pattern mining;algorithm called;interval-based;hierarchical representation;promising candidates;real world applications;optimization techniques;classifier;additional information
graph structures;completeness;relational algebra;access methods;graph pattern matching;query language;search space;subgraph isomorphism;large graphs;graph data;operator;pattern matching;graph databases
highly compressed;real-life;data sets
mining framework;mined patterns;objective functions;significant patterns;kernel method;disparate sources;graph mining algorithms;graph patterns;graph data;data mining;increasing amounts of;pattern discovery;structural similarity
network analysis;world wide web;mining algorithms;approximate algorithm;filtering step;social network analysis;stock market;application domains;dense regions;parameter settings;wide range;large graph;frequent) pattern mining;highly sensitive;real datasets;dimensional space
private information;privacy-preserving data publishing;conditional probabilities;maximum entropy;sensitive attributes
data publication;real data;sensitive data;high confidence;sensitive values;privacy threat;sensitive attributes
large number of;xml stream;xml-based;application servers
desired properties;pruning techniques;source data;database;test queries;test cases;cardinality constraints;test database;databases;operating conditions;subexpressions
memory bandwidth;programming model;graphics processing units;parallel computation;general-purpose;join algorithms;cpu-based
complex queries;access method;instance;multi-instance;instances;cost-based;query processing;operator;buffer space
handle complex;dynamic programming;dynamic programming;join predicates
query performance;complex queries;fully automatic;database
clustering;statistical methods;aggregation methods;synthetic datasets;existing graph;np-complete;database;application domains;large graphs;graph datasets;summarization techniques;summarization methods;real world
graph-structured data;large-scale;twig patterns;large graphs;twig pattern;twig-pattern;computational overhead;worst case;large datasets;incur high;pattern matching
large graphs;graph structure;reachability queries;index) size;social network analysis;large number of;directed graphs;xml databases;web mining;main idea;real world graphs;computational cost;graph queries;real world applications;path-tree;graph databases;processing queries
query nodes;evaluation cost;minimization problem;xml data;efficient algorithms to;tree-structured;tree pattern queries;tree pattern queries
keyword queries;data partitioning;partitioning strategy;query workload;electronic documents;query answers;parameter tuning;sheer volume of
low cost;database systems;database;indexing approach;general-purpose;disk page;indexing method;databases;theoretical analysis;storage manager;application code;efficient access to;application programs;temporal databases;trend analysis
pattern-based;data analysis;data cube;sequence data;implementation details;data items;real-life;online analytical processing;traditional olap
data structure;disk-based;suffix tree;computational biology;pattern discovery;pattern matching;times faster than
ranking queries;synthetic data sets;sampling algorithm;ranking queries;uncertain data;probabilistic threshold;uncertain data;object tracking;probability threshold
handle complex;handle arbitrary;attribute-level;monte carlo;database;data uncertainty;uncertain data;sql queries;probability distribution over;continuous attributes;query plan;query answers;uncertain attributes;arbitrarily complex;probability distributions over;tuple-level;query processing techniques;query-result;probability values;monte carlo;database systems
query evaluation;closely related;twig queries;object-based;query efficiency;probabilistic xml;efficient approximate
naïve;14;46;event processing;rfid data;event detection;static analysis;1;hidden markov models;event streams
control algorithm;vice versa;serializable;data consistency;database management system;serializability;databases;automatically detects;serializable;database management systems
completeness;open-source;replication;database replication;long running;high availability;case studies;commercial database;data management systems
data objects;database management systems;large-scale;query operators;query processing in;distributed environments;skyline operator;super-peer;rank-aware;data volume
15;key range;approximation guarantees;data partitions;8;naïve;np-hard
attribute values;data collection;naturally leads to;confidence intervals;united states;dimensional data;multidimensional space;query results;online analytical processing;result quality
real data sets;sensor readings;gps data;discrete data;network applications;database;query processor;raw data;closed form solutions;databases;sensor network;data mining;continuous functions;measurement errors;brute-force
verification framework;text documents;named entities;input text;real datasets
data set;excellent scalability;naturally leads to;uncertain data management;randomly generated;frequent items;sampling-based;query types;streaming data;statistical information;probabilistic data;data sources;wide range;high probability;internal structure;synthetic data sets;data generated from
schema integration;schema based;schema integration;source schema;logical level;user interaction;allowing users to;taking into account;source schemas
utility function;perfect information;synthetic datasets;large-scale;real-world;candidate matches;schema matching;user feedback;data sources;wide range;result quality;decision-theoretic framework;data integration
involving multiple;personal information management;automatically created;produce high-quality;data sources;data integration systems;semantic mappings;data integration systems;multiple domains;human intervention;schema mappings;data integration
domain knowledge;attribute values;heterogeneous data sources;classification;decision support;knowledge-base;tree-structure;cost model;classification process;real-life;olap operations
database;database engine;database research;aggregate queries;keyword search over;databases;relational databases;sql queries;existing database
keyword queries;keyword-based search;semi-structured;real datasets;large collections of;data model;high efficiency;search engines;search effectiveness;ranking mechanism;keyword search;high accuracy;structured data;search efficiency
data sharing;data manipulation;query processing;database management systems;database;relational databases;distributed databases;keyword-based;space overhead;nodes represent;emerging applications;databases;keyword search;high cost;data storage
fundamental properties;highly correlated;keyword search over;relevant answers;complex data;main components
execution plans for;compact representation;database systems;query optimizer;query optimization;optimization strategies;physical design;input query;parametric query optimization;parametric query optimization;tuning tools
virtual machine;database management systems;query optimizer;virtual machine;virtual machines;instances;database systems;database management system;computing resources;physical resources;database workloads;resource allocation;database instances
query execution;database systems;data warehouse;storage layer;decision support;data warehouses;query processing;business intelligence applications;join algorithm;column-oriented
years ago;database systems;database;major components;main memory;database architecture;buffer management;databases;concurrency control;transaction processing;disk-resident;online transaction processing
20;22;database;31;workflow execution;prohibitively expensive;data item
data lineage;recursive queries;scientific data;data set;data lineage;provenance information;data provenance
multi-strategy;increasing complexity;large-scale;real-world;database schema;meta-clustering;instance;high degree of;database;databases;high-level;learning framework;data integration
query processor;real-world domains;meaningful results;information extraction
skewed data;parallel database;parallel algorithms;complex queries;data warehouses;parallel processing;increasing number of;data skew;multiple dimensions;management systems;16;query processing;processing requirements;data skew
benchmark data;sampling based;large databases;execution plan;highly accurate;query performance;query optimizers;commercial database;customer data;data management
cost-based optimization;sql/xml;query evaluation;query execution plan;data source;xpath expression;database;query optimization;data streaming;query plan;performance degradation;ibm db;ibm® db;concurrent execution;operator;optimization techniques
database systems;poor performance;database applications;access latency;database server;solid state;potential impact;computing devices;data structures and algorithms;transaction processing
design principles;spatial index;database;data types;business logic;sql server;user-defined;database;2;stored procedures;data access;data type;application developers;microsoft sql server
parallel database;data analysis tasks;programming model;ad-hoc;map-reduce;language called;prohibitively expensive;large data sets;open-source;data processing;data collected;low-level;plans
error prone;oracle database;large-scale
data-flow;stream processing;streaming applications;stream processing engine;communication overhead;large-scale;real-world;large number of;user-defined;stream processing;highly-optimized;code generation;distributed data;application development
query-independent;high speed data streams;large volumes of data;network traffic;high-rate;processing nodes;query sets;data stream;instance;distributed processing;query execution plans;partitioning scheme;data stream management systems;data streams;query-aware;query-aware
customer satisfaction;multiple objectives
database;database;minimal overhead;oracle database;test results;relational database management system;oracle database
large numbers of;web applications;business users;key features;data integration
rdf data;web applications;database;open data;semantic web;semantic web;information systems;relational databases;data management
geographic regions;schema-mapping;vertically partitioned;database;meta-data;enterprise applications;databases;common practice;total cost
spatial indexing;cost-based query optimizer;query optimizer;sql server;data types;microsoft sql server;location-aware;spatial indexing;spatial queries;microsoft sql server;plans
data-driven;data management techniques;relational operations
query processing;database systems;analytic processing;database
user preferences;takes into account;requires minimal;user feedback;source data;database engine
index structure;user-defined;similarity function;string transformations;efficient retrieval;record matching
key features;massively multiplayer online games;distributed nature of;central server;large-scale
missing values;uncertain databases;uncertain data;relational operators;probabilistic data;database management system;data types;database engine;sensor data
ranked list;text search;text queries;spatial regions;input text;search services
web-based;human knowledge;graph database;database;query language;object-oriented
user interfaces;wide range;provenance information;open-source
mobile user;spatio-temporal;query language;query processing;location-based services;databases;object-relational dbms
xml documents;xml schemas;xml schema;document type definitions
specific applications;data values;data analysis;explicitly represented;synthetic data sets;disguised missing data;user interface;case study;data sets;2;disguised missing data;missing values;heuristic approach
complex queries;query optimizer;highly structured;database;recommendation process;xml databases;increasingly large;xml database systems
schema mapping;schema mapping;data instances;data-sets;instances;high-level;data integration;schema elements
web service;confidence values;schema matching;optimization problem;query result;user interaction
schema mapping;schema mappings are;mapping generation;data examples;information integration;fundamental problem;semi-)automatically;design process;mapping systems
movie database;language models;graph-based;query language;keyword-based;semantic search;user interface;search engines;web sources;semantic information;knowledge base
schema mapping;semantically related;automatically selecting;schema matching;mapping generation;data sources;discovery algorithm;algorithms rely on;high precision
declarative query language;data structure;hierarchical data;molecular biology;database;reference data;data sets;temporal queries;database
4, 5;fully automatic;xquery engine;user experience;inference algorithm;data elements;3;statistical analysis;data processing;ad hoc;human intervention
data arrives;monitoring applications;stream processing;replication;data streams;operator
spatial regions;spatial region;moving objects;query processing;privacy-preserving;sensor nodes;location privacy;sensor networks
plans;programming languages;distributed applications;application servers;execution plans;application logic;existing knowledge;query processing
cost-based;publish/subscribe system
declarative query language;rfid-based;rfid data;event extraction;graphical interface;data collected;rfid technology;application developers;high-level
relevant information;user behaviors;social tagging;efficiently computing
publish-subscribe systems;minimal overhead;clustering algorithm
update processing;xquery engines;open-source;xquery engine;xquery update;trade-offs;databases;distributed transactions
xquery engine;programming language
clustering;information network analysis;interactive exploration;database;databases;user-friendly;information network;heterogeneous information networks;information networks
prototype systems;scientific data;database research;emerging applications;database
web applications;open source;database;data model;data model for;object-oriented;long-term;object/relational;object/relational
uncertain data management;database;uncertain data;database research;uncertain data;large collections of;probabilistic data;data collected;query answering
information fusion;query language;distributed database;sensor networks;information fusion
information filtering;collaborative filtering;recommender systems;recommender systems
dynamic programming;taking into account;data types;real datasets;complementary information;clustering model;tree structure;heuristic algorithm;social networks;databases;approximation algorithm;special case;cluster analysis;metabolic networks;relationship data
clustering;data point;high-dimensional datasets;density-based;dense clusters;information bottleneck;analysis reveals;dense regions;relevant data;generative model;clustering methods;dimensional data;special case;mixture models;highly scalable;density based clustering;iterative algorithm
data structure;mining frequent patterns;tree structures;real datasets;database;mining frequent;tree model;xml documents;tree structure;mathematical model;association rules mining;frequent subtrees;frequent induced;candidate generation;tree model
matching algorithm;competing methods;semantic similarity;applications involving;knowledge discovery;data sets;knowledge representation;corpus-based;string similarity;longest common subsequence
database systems;international conference on;acm sigmod;graph streams;program committee;program committee;database systems
human effort;database research;databases
detailed comparison;schema mapping;data exchange;source schema;decision problems;sufficient condition for;tuple-generating dependencies
instance;theoretical framework;basic operations;schema mapping;high complexity;target instance;data exchange;instances;decision problems;schema mappings;database instances
schema-mapping;database schemas;schema mapping;schema mappings are;data exchange;tuple-generating dependencies;conjunctive-query;logical level;query equivalence;integration systems;basic properties;high-level;query-answering;schema mappings;data integration;data-exchange
query complexity;database systems;join results;execution model;general case;join algorithms;problem statement;scoring function;rank join;np-hard
core xpath
pagerank algorithm;streaming model;graph streams;random walk;large graphs;probability distribution;web-graph
approximation ratio;approximation) algorithms;multiple queries;min-cost;min-cost;data stream;filter ordering;filter ordering;special case;total cost
data arrives;sensor readings;large data streams;sliding window;data quality;range queries;sliding windows;heavy hitters;data management
selection predicates;database;data cleaning;efficient approximation;query language;probabilistic databases;wide range;sensor data;efficiently computing;query results;conditional probabilities
query evaluation;data integrity;database;aggregate functions;data items;instances;probabilistic xml
query evaluation;real-life datasets;probabilistic database;query evaluation;markov networks;probabilistic databases;conjunctive queries;key constraints
closed world;lower bound;database;np-complete problem;data exchange;aggregate queries;relational schemas;aggregate query;schema mappings;incomplete information;upper bound;operator;algorithms for computing;data exchange
open-world;data exchange;schema mappings;large space of;instances;mapping composition;wide range;query answering;closed-world;data exchange;schema mapping
query containment;data exchange;database
relational databases;increasing demand for;real-life;data quality
database;database;database records;sensitive data;prior knowledge;probability distribution;databases;logical properties;prior-knowledge
24;data structure;encoding scheme;information-theoretic;data structures;query distribution;2, 5
clustering;approximation algorithms;mining algorithms;approximation algorithms;record linkage;uncertain data;probability density functions;data analysis tasks;sensor network;data mining;optimization criteria;clustering uncertain data
clustering;approximation algorithms;clustering problem;market-basket analysis;approximation algorithms;text mining;np-hard
data objects;distance measure;labeled trees;hierarchical data;distance measure;locality-sensitive hashing;similarity search;algorithms for computing
data-driven;complexity bounds;multiple queries;web services;database;parallel processing;input sequences;prior models;lower bounds;decision problems;database updates;web services
attribute values
query evaluation;transitive closure;combined complexity;path expressions;core xpath;operator;query containment
real-world;object model;xml update
formal framework for;access control;relational calculus;security applications;probabilistic xml;operator;query languages
convergence rate;data items;replication;rates;algorithm converges;access patterns;high probability;query routing;distributed systems
object-oriented;type inference;inference algorithm;type inference
lower bound;distributed computation;local constraints;temporal behavior;communication complexity;fundamental problem;real world;distributed data;geometric constraints
relational structures;tree-width;conjunctive query;functional dependencies;tree-width;np-complete;databases;relational databases
jim gray
search effort
jim gray
jim gray

jim gray
jim gray
jim gray;information processing
batch processing;online transaction processing;jim gray;transaction processing
jim gray
transaction processing;database
data-intensive;data analytics
1;transaction processing;transaction processing
nrc;microsoft sql server;jim gray;digital library;high resolution
databases;jim gray;sloan digital sky
jim gray;database
jim gray
data set;data server;data sets;jim gray
clustering;multi-dimensional;classification;time series;meaningful results;wide range;human actions;real world;motion-capture
web documents;web pages;information contained in;pattern mining;incremental mining;interesting patterns;web log mining;web logs;traversal patterns;data mining;support threshold;markov chain
spatial data;aggregate queries;data mining applications;spatial datasets;tree-based;sensor networks;query types;spatial databases;location based services;resource allocation;np-hard
anonymity preserving;minimum support threshold;large number of;effectively identify;source database;statistical significance;source data;extracted knowledge;data mining models;pattern discovery;frequent pattern mining;data mining results
real-world datasets;data cube;low-cost;query response times;fact table;great promise for;data cubes;query answering;fast computation;efficient construction
clustering;data structure;query evaluation;increasing number of;data points;hierarchical clustering;main memory;hierarchical clustering;search space;optimization problem;sparse data;data point;problems arising;greedy algorithm;high quality;multidimensional space;multidimensional data;data chunks;efficient access to
database engines;database engine;databases
frequent patterns;large number of;data mining;pattern-growth;coding scheme;frequent itemset mining;data mining;mining patterns;cpu cost;frequent pattern mining
type information;database systems;database;legacy systems;database transactions;database technologies;programming language
web objects;query evaluation;completeness;database management systems;database;software systems;application servers;user requests;web caching;database caching;constraint-based
real-world and synthetic datasets;threshold-based;disk-based;nearest neighbor;threshold values;real-world scenarios;real-world;distance measures;shortest path;iterative algorithm;nn queries;road networks;metric spaces
global model;training data;privacy-preserving;classification;naïve bayes classifier;predictive models;naïve bayes;distributed classification;classification performance;instances;data sources;privacy-preserving;databases;distributed data mining;classifier;privacy-preserving data mining
complex queries;query response time;storage space;range-sum queries;approximate query processing;on-line analytical processing;decision support systems;error estimation;data warehouse systems;high quality;error estimation;range queries
clustering;learning examples;classification;association rule mining;data mining algorithms;data mining;data mining problems
storage overhead;access control;relational database systems;labeling schemes;query modification;privacy preserving;role-based access control;relational databases
large-scale;large number of;instance;data management;memory consumption;embedded systems;data management
database functionality;major components;data management;database
web service;long-running;business transactions;application domains;distributed transactions;transaction processing
xml documents;lock;concurrency control mechanism;fine-grained;native xml;database management system;5
cost functions;plan execution;database;energy-efficient;database;application domains;database servers;database management system;energy efficiency;database management
database technology;query language;database technology
database management systems;generally applicable;general-purpose;database architecture;service-oriented;usage scenarios;data management;flexible architecture
database;database systems;database management system
data structure;data analysis;workflow systems;input data;scientific applications;web-services;regular expression;schema information;high-level
search space;large-scale;parallel processing;description logics;everyday life;agent communication;data integration
databases;ontology-based;ontology based;database
clustering;genetic programming;association rule mining;fixed-length;mining tasks;comprehensibility;web usage;high dimensionality;web usage data;data mining algorithms;low-dimensional;dimensionality reduction;search engine queries
web databases;completeness;large-scale;entity identification;correctly identify;instances;data sources;instance;entity identification;data integration
clustering;network size;information processing;information retrieval;content similarity;clustering algorithm;high cost;distributed ir
sql/xml;query execution plans;fine-grained;database;relational database systems;query optimization techniques;instance;xml document;xquery implementation;optimization techniques
statistical summaries;xml query languages;database design;structural summaries;structural summaries;xml document;query languages;xml query processing;size estimation
query evaluation;plans;database;data providers;spatio-temporal;dynamic environments;data sources;streaming data;service-oriented;query processing;diverse range of;data integration
theoretical analysis;storage space;query result;search strategies;additional information
database;competing methods;main memory;frequent updates;reference-based;distance measures;similarity search;database objects;dynamic databases;reference-based;databases;metric spaces;selection algorithm
query execution plan;spatio-temporal;dynamic environments;keeping track of;query operators;rates;data stream management system;continuous spatio-temporal queries;data streams;operator;input streams;spatio-temporal queries;continuous queries
mining task;search algorithms;mining frequent itemsets;minimum support;frequent patterns;database;closed itemsets;real data;mining frequent patterns;high efficiency;candidate itemsets;memory size;upper bound;database scans;closed) itemsets;memory consumption;level-wise;synthetic data
query retrieves the;key range;database;relative error;databases;query results;disk page;high precision
data structure;online algorithms for;small size;random samples;random sample;streaming data;data set;implicit assumption;high accuracy;large sample;disk-based;data management;random sampling
xml documents;query processing strategy;historical information;real-world;data model;indexing scheme;query performance;data model for;data structures;query processing;xml document;constraints imposed by;temporal data;xml query language
intrusion detection;databases;database management systems;sql queries;considerable effort;database;user behavior;mining process;database access;role based access control;specifically tailored;detection techniques;anomalous behavior;access patterns;relational databases;clustering algorithms
query optimization;selectivity estimation;database;database applications;large databases;similarity functions;probability distribution;query string;real data sets;distance metric;approximate queries;approximate string
access methods;query performance
mobile ad hoc network;application requirements;data availability;replication;data replication;data access;databases
communication overhead;privacy guarantee;privacy- preserving data mining;vertically partitioned data;privacy-preserving data mining;times faster than
mining frequent patterns;sample size;frequent itemsets;data streams;real data;power-law;prohibitively expensive;itemset support;cost-effective;frequent-itemset mining;pre-processing;power-law
service calls;active xml;xml documents;declarative framework;web services;stored information;data management
synopsis structures;optimal algorithms;approximate query answering;multi-resolution;range queries;range queries;real-life data sets;olap applications
instance;segmentation methods;database;semantic similarity;database schema;schema matching;semantic web applications;schema matching;frequency information;segmentation method;data integration;matching method;mutual information;matching algorithm;longest common subsequence;corpus-based;word segmentation
distance computation;nearest;network distance;real world datasets;multi-resolution;shortest path;query processing;nn queries;computational cost;query returns;query point;search region
received increasing attention;information retrieval research;classification;emerging topics;annual international acm sigir conference on;social tagging;information retrieval;sentiment analysis;annual international acm sigir conference on;review process;user studies;information retrieval;reviewing process;program committee
user experience;user studies
high degree of;protein function;heterogeneous sources;computational biology;information retrieval
search results;correct answers;web search;web search engine;iterative process;medical knowledge;linguistic knowledge
trec test collections;poor performance;optimal number of;long queries;related concepts;user interaction
content-similarity;document retrieval;retrieval results;case study;retrieval performance;query results
clustering;classification technique;data source;clustering technique;query types;2;specifically designed to;quality measure;web search engines;result quality
term weights;web pages;probabilistic graphical model;web sites;dirichlet process;inference algorithm;text fragments;content information;wide range;web mining;web sites
web search;web search engine;meta-search;machine learning;users' search;search engines;search engine;higher quality
disparate sources;evaluation metric;information retrieval;average precision;retrieval systems
test collections;great success;ir evaluation;test collections;information retrieval systems;test collection
completeness;large-scale;evaluation measure;training process;user queries;retrieval performance;discounted cumulative gain;trec test collections;average precision
modeling assumptions;recommender systems;collaborative filtering;detection algorithms;user feedback;matrix factorization;svd based;algorithm exploits;association rules;collaborative filtering
user preferences;collaborative filtering;database;collaborative filtering algorithms;ranking problem;data sets;real world;similar users
active learning strategies;aspect model;collaborative filtering;training examples;active learning;learning approaches;statistical inference;benchmark datasets;collaborative filtering
ranking algorithm;auc;unlabeled data;data collections;ranking measures;precision-recall;ranking function;labeled data;boosting algorithm;ranking functions;semi-supervised;average precision;classification algorithms
optimization methods;conventional methods;loss function;loss functions;upper bounds;ranking svm;ranking models;information retrieval;algorithm called;discounted cumulative gain;upper bound;evaluation measures;information retrieval;average precision;benchmark datasets
feature space;learning algorithm;nearest neighbor;training examples;nearest neighbor;ranking models;information retrieval;model construction;query dependent;ranking function;machine learning techniques;query-dependent;ranking model
estimation accuracy;feature vector;content-based retrieval;estimation technique;high-dimensional spaces;similarity search;search quality;efficient similarity search;cosine similarity
time series;linear model;search behavior;search behaviors
personalized search;users' interests;social annotations;automatic evaluation;user's interests;high quality;data sets;search quality;high cost;web page;term matching;social annotations
predictive models;search algorithms;large-scale;user behavior;relevance judgments;behavior patterns;log analysis;user intent;result quality
clustering;ranking approaches;document ranking;relevant documents;language model;document clusters;relevant-document;retrieved documents;query-specific;retrieval method
similarity measure;unified framework;word-sense disambiguation;clustering performance;text representation;text clustering;clustering methods;semantic relationships between;semantic relations;labeled data
nonnegative matrix factorization;document cluster;document space;similarity measure;real-world applications;document clustering;clustering problems;cosine similarity;posterior probability
ranking algorithm;feature extraction;large number of;information retrieval;test collections;logistic regression
training set;total number of;classification;training examples;model assumes;nearest-neighbor classifier;supervised classification;latent semantic analysis;result set
search space;search accuracy;spatial data;stored data;spatial constraints;transliteration;general problem
search results;test set;trec collections;language models;language model;vector-space;relevance feedback;test collections;positive examples;test collection;sampling strategies;special case;negative feedback
relevance feedback;training data;initial query;relevant documents;feature space;bayesian logistic regression;term space;active learning;logistic regression model;relevance feedback;retrieval performance;overfitting problem;user evaluation;variance reduction;unlabeled documents;active learning algorithm;feedback documents
trec collections;relevant documents;cluster-based;large-scale;retrieval accuracy;relevance feedback;document clusters;feedback documents;relevance model;retrieved documents;pseudo-relevance feedback;initial retrieval;main idea
supervised learning;trec collections;classification;classification process;pseudo-relevance feedback;feedback documents;pseudo-relevance feedback;expansion terms;additional features;unsupervised learning;retrieval effectiveness
transductive learning;test data;labeled data is;test queries;ranking performance;ranking algorithms;test) data;ranking algorithms;labeled data;ranking functions;supervised learning;information retrieval systems
training set;gaussian process;document retrieval;score distributions;information retrieval;data set;probability distribution;baseline methods;gaussian processes;discounted cumulative gain;random noise;neural network;regression model;document scores;ir metrics;document rankings;ir) metrics
training data;information retrieval applications;rank documents;additional information;information retrieval;ranking performance;query terms;ranking method;ranking functions;association rules;pre-processing;demand-driven;automatically learn;discovered rules
search results;learning process;web search;optimization problems;boosting methods;objective functions;machine learning;preference judgments;data sets;ranking functions;preference data;information retrieval
document summarization;query-sensitive;mutual reinforcement;query-sensitive;single-document summarization;sentence extraction;multi-document summarization
web documents;document summarization;graph-based;manually labeled;valuable information;document search;information retrieval tasks;tensor-based;sentence extraction;summarization methods;blog posts;web document
multi-document summarization;cluster-based;random walk model;random walk model;multi-document summarization;cluster-based;cluster-level;link analysis;document set
spectral clustering;multi-document summarization;semantic analysis;sentence-level;matrix factorization;data sets;similarity matrix;multi-document summarization;machine learning techniques
user interfaces;exploratory search;relevant information;information retrieval
continuous query;information sources;information filtering;real-life;user interests;distributed information retrieval;information filtering;query routing;blog data
user browsing;relevant document;click logs;document relevance;relevance information;click data;search engine;real data;cross-validation;12;search engine;browsing behavior
query intent;training data;vertical search;classification;semi-supervised learning;general-purpose;classification performance;user interface;feature representation;search engine results;search services;query words;click graph
12;link structure;federated search;document retrieval;query expansion technique;retrieval task;blog posts;pseudo-relevance feedback;ad hoc;query expansion
image features;training data;high-level;learning framework;low-level;web image;latent dirichlet allocation;textual information;indexing problem;distance metric;web images;content-based image retrieval;image retrieval;metric learning methods;semantic concepts;image annotation;distance measure;similarity measure;semantic similarity between;database;retrieval performance;learning method
relevance scores;short queries;retrieval accuracy;spoken document retrieval;statistical modeling;statistical models;retrieval method
query-independent;trec 2007 enterprise;document search;data set;retrieval task;query-dependent;language modeling framework;expansion terms;query expansion
search results;query refinement;web search;conditional random field;discriminative models;crf) model;generative models;search queries;baseline methods;query refinement;query words;query refinement
document level;query expansion;search process;implicit feedback;general case;search result;internal structure;query expansion
real-life problems;users interact with;decision-making;relevance assessment;user feedback;specific task;information seeking;cognitive processes
search results;additional knowledge;web search;business model;computational costs;preprocessing phase;search engine
relevance score;document ranking;theoretical foundation;ad-hoc;opinion retrieval;linear combination;ranking function;bayesian approach;topic-relevance;retrieval task;opinion retrieval;ranking functions;combining multiple;topic relevance
passage based;language model;probabilistic models;passage-based;unified framework;document retrieval;probabilistic model
score function;probabilistic framework;dcm;efficient retrieval;model parameters;retrieval accuracy;generative model;probabilistic models;accurate ranking;retrieval tasks;pseudo-relevance feedback;relevance model;probabilistic model;text documents;estimation algorithms;probabilistic retrieval model;modeling techniques;language modeling;ad hoc information retrieval
language modelling;query term;information theory;tf-idf;document length;relevance feedback;probability theory
real-world datasets;multiple domains;web pages;ranking methods
user browsing;probability distribution;markov process;edges represent;user behavior;page importance;instance;web content;baseline methods;web browsing;link graph;web users
storage space;link structures;web forums;web applications;data source
graph based;question-answer;sequential patterns;user generated content;question-answer;classification method;question answering
translation probabilities;language model;document retrieval;query likelihood;retrieval models;baseline methods;retrieval model
interface design;user ratings;prediction model;query suggestion;question answering;community question answering;answer ranking;collaborative question answering;user intent;information seeking;question answering
automatic extraction of;retrieval effectiveness;probabilistic model;verbose queries;web collections;search engines;document frequency;higher accuracy;query-dependent;natural language
search effectiveness;information retrieval;test collections;query type;ambiguous queries;ambiguous queries
relevant features;query log;web search results;machine learning techniques;automatically identifying;web search engine
ranking algorithm;mixture components;web pages;mixture model;mixture model;user-generated;large-scale;training documents;word clusters;posterior probabilities
search results;real-world datasets;search space;social tags;relevant content;social-tagging;user-generated content;query processing;interesting items;online communities;excellent performance
entropy-based;anchor text;high-precision;information retrieval;prediction" problem;association rules;tag-based;tag prediction
clustering;search results;free text;information systems;ranking techniques;post-processing;spectral analysis;content similarity;search engines;web search engines;retrieved documents;information retrieval systems
clustering;rank-aggregation;relevant documents;highly ranked;relevant-document;query-specific
growing number of;text documents;link information;content similarity;document clustering;clustering algorithm
high-dimensional;natural-language;web pages;matching algorithm;duplicate detection;web collection;web collections;similarity search;web archives;locality sensitive hashing;execution times
text analysis;web search
summarization methods
correlation coefficient;average precision;information retrieval
problem instances;domain knowledge;classification problem;heuristic approaches;parameter estimation;labeled features;input features;probabilistic models;training classifiers;regression models;machine learning;instances;data sets;labeled data;text classification;unlabeled instances;soft constraints;objective function
trec data;14;confidence intervals;large scale;sampling method;retrieval evaluation;relevance judgments;1;random sampling
graph structures;optimization framework;language models;smoothing methods;information retrieval;retrieval performance
language-model;web documents;class information;classification;large-scale;classification model;classification method;classification approach;search algorithm;classifier;classification algorithms
probabilistic latent semantic analysis;web applications;classification;target domain;probabilistic model;cross-domain;blog classification;labeled data;classification approaches;text classification;labeled and unlabeled data;classification algorithm
unlabeled data;active learning;active learning algorithm;text categorization;optimization problem;active learning methods;text corpora;iterative algorithm;greedy algorithm;data examples;support vector machines;learning method;np-hard
web pages;depends crucially on;classification;web graph;anchor text;classification error;class labels;web page classification
completeness;document selection;relevance judgments;information retrieval evaluation;cost-effective;evaluation measures
evaluation measures;accurately reflect;ir systems;evaluation measure;objective functions;user requirements;ranking functions;information retrieval evaluation;retrieved documents;evaluation measures;question answering;information retrieval systems;test collection
information seeking;test collections;relevance assessment;test collection
higher-order;test collections;ir evaluation;relevant documents
relevance judgments;relevant documents
explicitly model;trec collection;preference judgments;ranking algorithms;evaluation measures;evaluation measures

stochastic model;average precision;average precision;retrieval effectiveness;user behaviour

average precision;information retrieval systems;discounted cumulative gain
missing information;document collections;topic structure;clustering algorithm
information retrieval evaluation;text collections;information retrieval
search engine;query refinement
browsing behavior;text queries;retrieval systems;search queries;information retrieval models;user modeling
search results;query intent;web search;query classes;user intent;search result pages
obtained by combining;user profile;user profiles;naive bayes models;user opinions
user-item;recommender systems
personalized search;web search;information sources;personalized ranking;relevance judgments;personalized search;search history;user study
high level;search result;question answering;question answering systems;user preference
relevance information;relevance judgments;distributed information retrieval;document collections;distributed information retrieval;test collection
user profile;background model;language model;search experience;user queries;user interests;5;user study;long-term;aware search;query expansion;individual users
classification
classification performance;text categorization
information retrieval;retrieval performance;opinion finding;relevant documents;blog posts
search results;highly correlated;search engines;search engine
user community;clickthrough data;search results;search behaviour;document rankings
highly dynamic;tag prediction;real-world;community-based;large-scale
community question answering;real users
social media;web search;user experience;growing rapidly;qa systems;search engines;temporal evolution;question-answering;question answering;information source
low precision;relevant documents;social tagging;user preference;collaborative tagging;similarity based
web pages;product features;customer reviews
labeled examples;sentiment classification;classifier
supervised learning;spam filtering;semi-supervised learning methods;data sets;labeled data;ecml/pkdd;semi-supervised;support vector machines;labeled examples
trec 2007 blog track;opinion-finding;relevant documents
cross-lingual information retrieval;web queries;web log mining;query translation
web search;anchor text;web text;statistical model;data resources;query log;web search queries;multiple sources
expert finding;retrieval models;highly relevant;language modeling;finding task
collection size;real-world;web search;web crawl;search effectiveness
resource discovery;estimation) algorithm;link-based
web page;web search;daily activities;sensor nodes;web page;additional information
web information retrieval;retrieval effectiveness;ir) systems;selection method;retrieval performance;feature selection;test collection;search tasks;web retrieval
retrieval effectiveness;large-scale;language models;language model;data-sets;web data;web retrieval
improving web search;web search;search engine
visual-words;video retrieval;visual words;video indexing;classification
retrieval precision;document images;document image;connected components
human-generated;user generated;automatic speech recognition;user generated;user study
web browsers;retrieval systems;search tools;online video;video retrieval;search tasks;multi-faceted
semantic annotation;semantic analysis;web image;semantic labels;semantic analysis;data set;regression model;web image
search results;search queries;entity recognition;query terms;information retrieval
natural language

domain knowledge;information-retrieval;lower dimensional;relevant documents;latent space;document retrieval;information retrieval;latent semantic analysis;information retrieval performance
text mining;information retrieval
search terms;information extraction techniques;keyword-based;information extraction;retrieval performance;keyword-based;text retrieval;information retrieval;automatically extract
domain knowledge;domain experts;web search;query suggestion;domain expertise;search behavior
rewriting rules;information retrieval
expert finding;trec data;query terms
relevance propagation;multi-step;random walk;relevance propagation;expert finding;data sets
support vector machines;hierarchical classification;classification
open-source software;real-world;vector machine;information retrieval;retrieval accuracy;information retrieval
correlation coefficient;relevant features;character recognition;feature extraction method
parallel corpus;retrieval scores
relevance feedback;language modeling framework;regularization;language modeling
theoretical bounds;regularization;similarity measures;similarity measure;theoretical bounds
power-law distribution;short queries;query length;power-law;query sets
ir systems;rule-based;low complexity;test sets;query terms
relevance feedback;retrieval effectiveness;test collections;relevance models;language models
clustering;real-world datasets
sparseness problem;language models;language modeling approaches;language model;information retrieval;proximity-based;test collections;query terms
information content;test sets;language models
retrieval performance;automatic query expansion;query expansion
singular value decomposition;singular vectors;text categorization;hierarchical structure;document matrix;latent semantic indexing;latent semantic indexing
learning algorithm;minimal optimization;classification;learning algorithm;quadratic programming;subset selection;structural svm;structural svm
clustering;nonnegative matrix factorization;latent semantic indexing;closely related;posterior probability
text classifier
real-world datasets;support vector machines;information retrieval;unlabeled examples;text classification;support vector machines
semantic classes;similarity information;question classification;tree kernel;question classification;semantic features;diverse set of;question answering systems;support vector machines
sample sizes;collection size;distributed information retrieval
search engine;query logs;classifier;web users
text mining;xml mining;information retrieval;average precision;xml structure;positive effect
xml retrieval;scoring functions;document structure;retrieval quality;text retrieval;ir methods
document retrieval;element retrieval;retrieval tasks;xml documents;relevant document;retrieval task;xml element retrieval;comparative analysis
xml documents
entity ranking;retrieval results;finding task;graph-based ranking
search results;document retrieval;passage-based;ranking performance;ranking methods;ranking documents
markov random field;retrieval effectiveness;feature-based;term dependencies;phrase based;retrieval model;document collections;semantic information;feature set;term dependency;mrf) model
query results
entity ranking;2;ranking method;network traffic;entity extraction;query terms
pruning strategy;query processing in;search engines;web search engines;query processor
document collection;cross-language;vector-space;information retrieval tasks;document collections;cross-language
local information;web graph
world wide web;classification accuracy;learning tasks;topic modeling;additional features;classification task
relevance feedback;classification problem;learning algorithm;text categorization;classifier
graph partitioning;nearest neighbor;real-world;clustering performance;document datasets;document clustering;purity;high dimensionality;validation measures;nearest neighbors
clustering;clustering;document collection;graph partitioning;document content;clustering algorithms;information retrieval;document collections;clustering methods;algorithm produces
distance computation;feature construction;demographic information;distance-based;collaborative filtering
clustering;search results;mobile environment;input/output;web searching;web searching
cross-lingual information retrieval;web-interface;cross-lingual;retrieval results
user-generated content;online communities;common interests

relevance feedback;fine-grained;xml retrieval
search systems;emerging topics;high-level;text-based;multimedia information;open source software;web-based;3;information retrieval;search engines;real-life;information science;5;4;text retrieval;image description;retrieve information;content-based image retrieval;real world;conceptual model
information retrieval;information retrieval;document structure;xml-documents;result quality;structural similarity;relevant documents;indexing approach;bandwidth consumption;search engines;ranked retrieval;vector space model;retrieval quality;structural information;taking into account;structured documents;discriminative power;final ranking;information retrieval techniques;bandwidth consumption;ir) techniques;structural features;takes into account;ir techniques;xml elements;distributed environments;term frequency;search engine;content-based search;xml structure
relevance assessment;interactive information retrieval;relevance feedback;retrieval performance;retrieved documents;web searching;document relevance;information-seeking;1;3;2;5;4;information seeking;takes place;multi-modal;real-life problems;users interact with;decision-making;implicit feedback;user feedback;search behavior;negative feedback;relevance judgments;explicit feedback
data structure;statistical techniques;trec data;document content;natural language text;natural language processing techniques;ir systems;ir model;similarity measure;large number of;vector representation;information retrieval;ir research;term-based;structured queries;dependency trees;document representation;term matching;graph representation
search results;desktop search;query capabilities;search tools;user context;task-specific
retrieval algorithms
personalized search;predictive models;search interfaces;frequent patterns;sequence data;user's perspective;information retrieval;user activity;modeling techniques;large collections of;information retrieval systems;user queries;digital library;natural language processing;semantic space;individual users;user behavior;informative features;user interaction;interaction data;search sessions;digital libraries;user behaviors
workshop brought together;knowledge management;text analytics;text data;machine learning;diverse set of;natural language processing;program committee;information retrieval
book search
xml retrieval;multimedia documents;structured document retrieval;multimedia information retrieval;text-based;logical level;xml documents;multimedia retrieval;xml elements;retrieval performance;retrieval task;xml document;multimedia objects;relevant information;ad hoc;xml structure
clustering;xml documents;xml mining;xml mining
data mining;knowledge management;databases;sensor data streams;information retrieval;semantic web;personal information;knowledge management;natural language processing;digital libraries;pre-processing;information quality
web information and data management;knowledge management;international conference on;acm international workshop on;international workshop on
information retrieval research;information retrieval
ranking techniques;information retrieval research;ir systems;positive results;information retrieval;ir research;document representation
semantic annotations;information retrieval
expert search;desktop search;information retrieval;mobile devices;information retrieval
retrieval effectiveness;ir systems;information retrieval;ir) systems;international workshop on;user interaction
information retrieval;hong kong;patent retrieval;information retrieval
web pages;information retrieval methods;major source of;information retrieval;web content;search engines;retrieval algorithms
information retrieval
digital images;web pages;personal information management;digital information;design principles;designed to support;wide range;major limitation;personal information management;stored information;personal information
information retrieval tasks;web search;retrieval effectiveness;parameter estimation;textual features;statistical model;data sets;wide range;feature selection;ad hoc retrieval;information retrieval models;markov random fields;information retrieval;query expansion

optimization technique;wide range;population-based
optimization technique;pso algorithm;energy minimization;population-based
genetic algorithm
learning process;information entropy;learning environment;classification process
bp neural network;recommendation service;recommendation service;decision tree
association rules;association rules mining;collaborative filtering
intrusion detection;intrusion detection system;anomaly detection;data mining;data mining) technology;data mining
customer behavior;data describing;data mining;web log;customer behavior
mining algorithm;frequent item sets;minimum support;database;item set;frequent item sets;mining algorithm
trust model;trust model;semantic web;semantic web;information from multiple sources
supply chain management;supply chain;content analysis;collect data
service oriented;information service;content filtering;personalized services;valuable information;resource management;service oriented;resource management
electronic commerce;virtual communities;game theory
resource management;resource management
large enterprises;diffusion model;diffusion model

web-based
learning performance;facial expression recognition;intelligent tutoring system;intelligent tutoring system
run-length
evaluation model;evaluation index system;key words
object oriented;formal methods;database security;database security;object oriented database
decision-making;decision making;information technology
web traffic;web cache;web caching;caching techniques;web cache;network traffic;memory management;simulation results;web traffic;data access
sequence patterns;web cache;sequence mining;user behavior;ant colony algorithm;web page
information technology;information technology;risk factors
wireless sensor network;simulation experiments;network nodes;grid-based;sensor networks;selection algorithm
communication protocol
multi-channel;multi-channel
cost function;rates
detection algorithm;real-valued;synthetic datasets;selection algorithm;real-valued
increasing demand for;data mining tasks;data visualization;knowledge discovery;data mining;machine learning;knowledge discovery;database;knowledge acquisition;databases;data mining;data mining;data mining technology;artificial intelligence
software development;highly interactive;knowledge management;wireless communication;software systems;information processing
anomaly detection;intrusion detection system;network security;database;data mining
high utility;high utility;optimization technique;synthetic data;high dimensional datasets;inter-transaction;long patterns;hybrid method;dimensional data
software architecture;multi-core;video content
high frequency
incomplete information system;incomplete information system;incomplete information system;classification;rough set
local search;ant colony;optimization methods;optimization algorithm;optimization problem;global search;ant colony algorithm;evolutionary algorithm;constraint solving;objective function;geometric constraints
missing values;decision making;decision making
high-speed;intrusion detection;intrusion detection system;matching algorithm;high-speed
state transition
search space;mining algorithms;sequence mining;pruning strategies;frequent 2sequences;2;pruning strategies;frequency counting;sequential pattern mining;total cost
complex network;bayesian network;network intrusion;algorithm called;forensic analysis
unknown word;related words;unknown word;semantic retrieval;related words
web pages;web navigation;web navigation;optimization model;optimization model;web page;hyperlink structure
association rules;data mining
product information;closely related;data mining technology;data mining results;data mining
data resources;standard database
network security;network security;multi-sensor;network security;data fusion
early stage;rates;inverse problem
image sequence;information sharing
optical character recognition;index terms
textual content;classification;spam filtering
minimum number of
complex data;data mining
regression analysis;time series;key words;independent variables;multiple factors;dependent variable;time series
bp neural network;bp neural network;sample data;evaluation model
data mining techniques;instances;clustering problems;clustering algorithm;pso algorithm;pso) algorithm
spatial data;geographic information;data mining;association rules;association rule;association rule;data mining based
neural networks;neural network;clustering tasks;data mining;clustering analysis;neural network
regression trees;ground truth;training data;classification;decision tree;land cover;remote sensing;decision tree algorithms;classification accuracy;decision tree;high accuracy;data mining;remote sensing data;principal component analysis;data collected
commercial applications;implementation issues;video surveillance;artificial intelligence

storage cost
training samples;pso algorithm;neural networks;simulation results

security policies;intrusion detection system;core components;virtual machine;virtual machines;intrusion detection;flexible architecture
fuzzy neural network;neural networks;optimal control
remote sensing images;fusion methods;classification;classification;remote sensing;classification accuracy;multi-classifier;fusion methods;high accuracy;single classifier;multi-classifier;classifier
minimum support;association rules;data mining;theoretical analysis;association rules;communication networks
classification;classification;genetic algorithm;rule-based;genetic algorithm;machine learning;case study;training samples;rule discovery;massively parallel;artificial neural network;classifier
local sites;communication overhead;communication cost;distributed databases;distributed databases;frequent itemsets;data mining;association rule;mining algorithm
semantic web technologies;semantic web
sequence alignment;training data;text analysis
data mining process;knowledge discovery;database;knowledge discovery
game theory
clustering;domain experts;massive data;user interests;information retrieval
knowledge management;knowledge transfer;ontology-based;knowledge transfer
base line;base line;point set
data objects;data integrity;local context
uncertain information;multiple sets of;information fusion;network management;product space;unified framework
interface design;mobile phone;key words
search task
control policy;role based;user privacy;access control
remote sensing images;data fusion;independent component analysis;independent components;remote sensing;high-order;data redundancy;independent component analysis;obtained by applying;statistical analysis;remote sensing image;factor analysis;image processing
neural network;large amounts of;estimation method;neural network;modeling methods
competence
knowledge management;knowledge acquisition
optimization approach;case study;tree based
operator;vision-based;input images
optimization problems;numerical results;global optimization;optimizer;global optimization
mathematical models;data processing;information technology;information processing
scheduling problem;key words;lower bound;test problems;np-hard
noisy data;key words;data set;noisy data;clustering algorithm;clustering algorithm
low accuracy;high accuracy
case study;development process;multi-agent system;agent-based;distributed systems
knowledge acquisition;knowledge acquisition;external knowledge;knowledge acquisition;knowledge management
semantic level;ontology-based;information sharing;communication protocol
domain knowledge;process models;process model;model construction;database
knowledge management;knowledge management
business intelligence;database
domain knowledge;electronic commerce;knowledge management;electronic commerce;agent architecture;software components
process planning;database;knowledge discovery;object-oriented;knowledge acquisition;knowledge discovery
support vector regression;support vector regression;rough set;rough set;artificial neural network
stock market;vector machine;stock market;intelligent systems;artificial neural network
remote sensing images;remote sensing images;decision function;classification;remote sensing;land cover;kernel functions;svm-based;detection method;supervised classification;change detection;support vector machines;svm-based
supply chain management;kernel function;numerical simulation;vector machine
search performance;index terms;genetic algorithm;optimization problems;differential evolution;constrained optimization;constrained optimization;simulation results
customer satisfaction;svm algorithm;customer satisfaction;vector machine;vector machine;evaluation model;comprehensive evaluation;higher accuracy;evaluation model;classifier;classification algorithm
clustering;ant colony;ant colony algorithm;ant colony;large number of;key words;ant colony algorithm;heuristic function;ant colony algorithm;evolutionary algorithm;short-term;short-term;discrete optimization
semantic matching;exact matching;logic based;semantic matching;semantic web services;description logic;web services;ontology based
pattern based;application systems
qualitative analysis
operator;operator;business model
web-based;web usage mining;web usage analysis;data mining;web resources;data mining;web-log;data mining algorithms;learning environment;learning method
data warehouse;question answering;association rules;data mining;clustering algorithm;question answering
multi-level;dimensional space;graph embedding;relational information;independent components;massive data sets
impact analysis;time series

multi-unit;utility function;decision-making
numerical results
long-term;supply chain;supply chain;key words
detection algorithm;spatial resolution;spectral analysis
remote sensing data;remote sensing data;large scale
evaluation model;database performance;oracle database
spatial analysis;spatial analysis;spatial distribution;statistical models;quantitative analysis;local region
control algorithm;linear transformation;design method;control law;adaptive algorithm;simulation results
numerical simulation
neural network;control strategy;neural network
finite element model;finite element method
resource sharing;resource sharing;ontology-based;ontology-based;search algorithm;ontology based
user interfaces;test cases;large number of
retrieved documents;semantic similarity between;semantic graph;statistical model
information service;database;search engine;word segmentation
color information;face detection;color images;face detection;color images;detection performance;lighting conditions;human face;classifier;color segmentation

content management;content management;database
web-based;information systems;specific information;specific information;web technologies;learning environment;structure information;learning environment

information stored in
information service;resource sharing;long-range;information sharing;supervision
artificial neural networks;neural networks;genetic algorithm;internet traffic;genetic algorithm;forecasting model;forecasting model;internet traffic;neural network;neural networks
bp neural network;analytic hierarchy process;decision-making;bp neural network
1;digital signal;memory management
data mining and knowledge discovery;knowledge discovery;acm sigkdd international conference on;data mining;highly competitive;knowledge discovery;data mining;program committee;acm sigkdd international conference on
search advertising;search engine's
regularization;input features;large scale;regularization;coordinate descent;feature selection
large amounts of;object recognition;focused primarily on;image features;textual data;image content;image search;training data;machine learning techniques;web page;content-based image retrieval
pattern recognition
user actions;social systems;social network;time series;theoretical justification;social influence;social ties;high probability;simulation results;social networks
massive graphs;approximation algorithms;streaming algorithms;main memory;external memory;large-scale;clustering coefficient;large graph;large graphs;web graphs;social networks;theoretical analysis;sequential scans
synthetic data;database;entity identification;generative model for;supervision;joint model;label-sets;unsupervised fashion;em algorithm;document categorization
mining closed;mining algorithms;redundant information;closed patterns;frequent patterns;window based;sliding window;efficient representation;streaming data;data stream;closed patterns;mining frequent closed;data streams;frequent closed;low complexity
information diffusion;viral marketing;label information;synthetic datasets;improving accuracy;large networks;collective classification;labeled data;node labels;classification algorithm;approximate inference;network clustering;optimal set of
set cover;clustering problem;dynamic programming;web scale;document retrieval;search engine;greedy algorithm;hierarchical agglomerative clustering;clustering
principal components analysis;dimensionality reduction technique;data analysis;principal components analysis;unsupervised feature selection;scientific domains;selection problem;application domains;document-term;data matrix;linear algebra;24, 12;unsupervised feature selection
individual records;data-mining;data publishing;data mining;machine learning;databases;data-mining algorithms;algorithms rely on;privacy threat;sensitive attributes
statistical framework;automatically generating;search engine
search systems;auc;learning framework;search applications;feature maps;loss functions;data sets;ranking model;structured learning;discounted cumulative gain;max-margin;multi-criteria;average precision;navigational queries;structured learning
hybrid model;auc;training examples;spam filtering;feature space;naive bayes;naive bayes;generative model;logistic regression;false-positive
kernel learning;feature space;multiple kernel learning;redundant information;optimization framework;classification;linear program;data mining tasks;benchmark data sets;column generation;multiple kernels;eigenvalue problem;computationally expensive;kernel methods;subspace kernel;low-dimensional
collaborative filtering;collaborative filtering;large-scale;information retrieval;expectation-maximization algorithm;multiple types of;gibbs sampling;social networking;model training;parallel computing
text mining;optimal performance;classification;high dimensional;correlation coefficient;imbalanced data;feature selection method;large number of;information retrieval;classification performance;data sets;feature selection;decision boundary;feature selection methods;credit card fraud;small samples;skewed;classification problems;machine learning and data mining;classifier
simulation data;supervised learning methods;historical data;short term;prediction methods;semi-supervised learning;time series;data sets;hidden markov model;prediction problems;model trained;long-term;long term;semi-supervised;supervised learning;learning framework;decision making
algorithm finds;markov network;sensitivity analysis;time series;graphical model;data mining;systems biology;reconstruction algorithm;probabilistic graphical models
linked data;classification;record linkage;classification methods;clustering;training examples;real world situations;increasing number of;6, 7;record pairs;nearest-neighbour;training sets;nearest neighbour;training data;databases;vector machine;classification results;high quality;svm classifier;supervised learning techniques;large databases;accurate classification;data mining projects;svm) classifier;classifier
social network;instance;social ties;social influence;social networks;online communities;social interaction
individual records;pattern detection;training data;network intrusion;test data;real-world;categorical datasets;attribute values;similar patterns;accurately detect;normal behavior
search results;click logs;closely related;greedy approach;rates;average precision;theoretical guarantees
rewrite rules;equivalence classes;equivalence class;large-scale;algorithm learns;transformation rules;search engines;5;search engine
clustering;higher accuracy;fully-supervised;distance functions;learned metric;distance function;high dimensional;data mining applications;learning algorithms;data sets;domain-specific;nearest neighbor classification;latent semantic analysis;metric learning;low dimensional;learning problem;distance metric;nearest neighbor;baseline methods;collaborative filtering;efficient optimization
itemset mining;constraint programming;constraint programming;constraint-based mining;pattern mining;wide range
training set;training data;unlabeled data;molecular biology;database;conditional probabilities;real-world;negative examples;unlabeled examples;binary classifier;labeled examples;classifier trained on;classifier;positive examples
order statistics;cosine measure;hash functions;similarity measure;similarity metric;hash functions;similar images;computational cost;basic algorithm;high dimensional spaces
frequent itemset;frequent patterns;frequent patterns;search tree;decision tree;low support;discriminative patterns;minimum support;feature selection;memory consumption;frequent pattern;feature vectors;frequent pattern mining
training set;classification accuracy;machine learning;text classifier;file systems;document corpora;text classification;information retrieval
speech recognition;search cost;hidden markov models;hidden markov models;query sequence;times faster than
auc;training phase;classification;real-world;classification tasks;network classification;partially labeled;classification performance;collective classification;statistical relational learning;semi-supervised learning;real-world data sets;class labels
domain knowledge;preserving privacy;data publishing;external knowledge;keeping track of;instance;data privacy;anonymized data;auxiliary information
real data sets;data analysis;related entities;multiple documents;large document collections;document collections
training examples;neighborhood structure;data sets;intrusion detection;ensemble framework;learning task;predictive power;multiple models;knowledge transfer;text classification;local structure;classification model;specifically designed for;learning algorithms;knowledge transfer;transfer learning;locally weighted;spam filtering;classification accuracy;test examples;learning method;classification algorithms
numerical analysis;theoretical properties;real datasets;real data;original data;human genome;network data;mapping problem;overlapping communities
discovered patterns;frequent itemset;mining algorithms;frequent pattern mining algorithms;synthetic datasets;source code;real-life datasets;post-processing;real dataset;frequent itemsets;comprehensive evaluation;pattern mining algorithms;random noise;optimal parameters
clustering;latent variables;generative model for;explicitly models;dirichlet process mixture;unsupervised techniques
text mining;synthetic data;computational biology;sequence data;gene clusters;candidate set;sequence databases;pattern discovery;sequential pattern;model called
clustering;singular value decomposition;pattern recognition;data compression;quality assessment;principal component analysis;face images;learning tasks;finding patterns;high order;large datasets;data mining;factorization method;noise levels;numerous applications;hand-written;high order;social networks
network analysis;world wide web;cut algorithm;graph mining;security applications;diverse range of;data sets;limited number of;clustering methods;systems biology;real world;clustering approach
matrix decompositions;linear combination;factor matrices;matrix-decomposition;data-analysis tasks;matrix decomposition;interpretability
coordinate descent;stopwords;support vector machines;text categorization;logistic regression
multi-dimensional;generative process;similar topics;topic model;data sets;visualization methods;discrete data;em algorithm;latent semantic;unlike conventional;dimensional euclidean space
production systems;database schema;causal discovery;data-bases;automatically constructing;databases
high-dimensional;problems arise;protein function;multiple labels;binary classification problems;information contained in;input space;generalized eigenvalue problem;multi-label classification;shared subspace;web page;binary classifier;multi-label;document categorization;direct computation
real data sets;customer relationship management;synthetic data sets;categorical attributes;great potential;user preferences;multidimensional space;user's preferences
frequent itemset;synthetic datasets;clustering method;computational cost;local minimum;decision-tree;regression problem;frequent itemsets;probabilistic model
training set;linear support vector machines;large scale;linear svms;main idea;text classification;multi-class
local structures;large datasets;dynamic-programming;event sequences;sequence mining;event sequences;local patterns;produce high-quality;user activity;data description;event sequence;optimization problem;patterns discovered;real datasets
evaluation metric;latent factor models;recommender systems;collaborative filtering;implicit feedback
long-range;communication network;temporal dynamics;social network;network topology;social networks;computing systems;social networks
data objects;outlier detection;parameter selection;data set;outlier detection;data mining task;distance-based;detecting outliers;dimensional data;real world
historical data;synthetic data;large volumes of;event types;frequent episodes;model estimation;user-defined;predict future;user-behaviors;hidden markov models;generative model;event streams;event sequences;web browser;mixture models;search session
complete model;network models;online social networks;temporal information;parameter settings;maximum-likelihood;large scale;power law;network evolution;social networks;model free;degree distribution
efficient parallel;motion capture;learning algorithm;parallel algorithm;parallel computing;data dependencies;multi-core;hidden variables;parallel algorithms;increasing number of;linear dynamical systems;linear dynamical systems;visual tracking;hidden markov models;time series;data mining algorithms;multi-core
active learner;unlabeled examples;active learning;active learning;costly process;instances;labeled data;decision tree;learning algorithm;supervised learning;active learning algorithms
unlabeled data;label information;classification;classification framework;supervision;objective function;classification performance;cost function;labeled data;learning problem;labeled data from;intrinsic structure;labeled and unlabeled data;transfer learning;real world applications
text mining;training examples;training examples;text data;probabilistic approach;supervised methods;text collection;common task;application domains;mining tasks;multi-faceted
multi-class classification;boosting methods;gradient boosting;higher order;cost sensitive;convergence rates;cost-sensitive;multi-class;loss functions;theoretical guarantees
regression problem;indexing techniques;classification accuracies;online algorithms;moving average
real graphs;social media;connected component;generative model for;social network;weighted graphs;citation networks;connected components;internet traffic;weighted graphs
overlapping clusters;exhaustive search;density based;user-defined;optimization problem;data set;projected clustering;subspace clustering;approximation algorithm;dimensional data;subspace clustering algorithms;subspace clustering
lda model;highly scalable;modeling framework;link structure;topic models;graphical model;1;data sets;unseen data;4;bipartite graph;8;prediction task;computationally expensive
classification;discriminative learning;label information;classification;learning framework;partially labeled;data sets;labeled data;learning problem;quadratic optimization;margin-based
classification;data mining techniques;data mining;classification rules;databases;association rules
collapsed gibbs sampling;latent dirichlet allocation;lda) model;real world data sets;sampling scheme;million documents;text corpora;wide range;times faster than;gibbs sampling;real world;collapsed gibbs sampling;latent dirichlet allocation
graph mining;accuracies;application domains;graph data;weight vector;mining algorithm;learning task;text processing
text mining;speech recognition;nonparametric bayesian;graph structure;pitman-yor;graph model;semantic knowledge;information retrieval;knowledge discovery;graph clustering;baseline models;semantic relationships between;probabilistic generative model;hierarchical dirichlet process;nonparametric bayesian;syntactic structure
generative process;social network;skewed;social networks;mobile phone;operator;power-law
conditional entropy;real-world;sequence mining;search algorithm;markov model
label quality;increasing complexity;unlabeled data;low cost;data points;low-cost;amazon's mechanical turk;data miners;data items;data quality;multiple times;multiple labels;label-quality;data mining
fast approximate;symbolic representation;real world datasets;time series;multi-resolution;massive datasets;data mining algorithms
web based;information sources;efficient computation;computational cost;large number of;aggregate queries;matrix factorization;real-life;user-generated content;scalability issues;threshold algorithm
cluster structure;case study;instances;large data sets;data set;supervised approach;network intrusion;semi-supervised;network traffic
large-scale;optimization algorithms;predictive accuracy;database;optimization methods;sparse matrices;low-rank;matrix factorization;relational learning;model generalizes;relational schema;relational learning;multiple relations;rating prediction;matrix factorization methods
mixture components;learning algorithms;mixture model;data distributions;linear regression;real-life;rates;mixture models
classification performance;regularization;vertex set;spectral learning;generalization performance;large-scale;high-order;multi-label classification;large data sets;eigenvalue problem;computationally expensive;computational cost;multi-label;benchmark data sets
synthetic data;temporal information;structural properties;large scale;real-world;complex relationships
real graphs;accuracy compared to;detecting anomalies;low-rank;low-rank;finding patterns;adjacency matrix;real data;network traffic;dynamic graphs;sparse matrix;times faster than
united states;complex network;clustering coefficient;complex network;database
parameter estimation;convergence rate;document clustering;computing resources;gibbs sampling;em algorithm
real data sets;document classification;classification accuracy;semantic information;text data;text classification;high dimensionality;classification algorithms;document representation;natural language
joint model;heterogeneous sources;real-world;closely related;schema matching;database records;data set;information integration;requires solving;error reduction
information extraction from;internal structure;high precision and recall;training data;information extraction
clustering;high-dimensional;real-world;sparse data;clustering performance;information-theoretic;clustering criteria;incremental learning;incremental learning;data sets;information-theoretic;common practice;objective function;information theoretic
implementation issues;minimal optimization;classification;large-scale;takes into account;vector machine;false-positives;false-positive;dimensional data;support vector machines;extensive simulation;classifier
clustering;pruning strategy;approximation ratio;set covering;high level;false positive;synthetic datasets;database;transactional databases;frequent itemsets;approximation algorithm;np-hard;huge number of;transactional data
transaction data;individual privacy;high dimensional;data mining research;transaction databases;data mining
data mining method;outlier detection;probability density function;detection algorithm;algorithm called;normal distribution;theoretical analysis
shortest-path;directed graph;random walk on;link-based;expected cost;mining tasks;linear systems
discriminative training;linear models;loss function;error rates;binary classification problems;structural svms;highly optimized;information retrieval;approximation guarantees;training methods;large datasets;natural language processing;large-margin;random sampling
redundant features;microarray data;classification accuracy;feature selection algorithms;random sample;knowledge discovery;feature selection;kernel density estimation;dimensional data;high classification accuracy;small sample size
real-world datasets;mining algorithms;instance;classifier ensemble;data mining research;instances;stream data;kernel space;data streams;prior probability;conditional probability;data chunks;prediction models;classifier
error rate;upper bound;large number of;brute-force;computational cost
svm algorithm;total number of;average number of;real world datasets;decision boundary;vector machine;optimization problem;unlabeled examples;algorithm takes;semi-supervised;margin;semi-supervised;integer programming
integrating information from;information contained in;heterogeneous data sources;data sets;multiple data sources;data sources;multi-source;biologically relevant;gene selection
total number of;input data;database;candidate pairs;large databases;data sets;real-world applications;strongly correlated;upper bound;real-world data sets;memory space;buffer size
high-dimensional;detection problem;detection techniques;land cover;signal processing;case study;land cover;spatio-temporal;data mining techniques;control theory;change detection;data sets;high computational complexity;data mining approaches
em) algorithm;question-answering;bayesian information criterion;ranked list;link analysis techniques;expectation-maximization
clustering;query patterns;baseline methods;suffix tree;large-scale;query suggestion;context-aware;query suggestions;search engine;user's search;query suggestion;search queries;search engines;data sparseness;context-aware;search log;query sessions;search logs;query sequence
information graphics;human visual system
mining algorithms;sharing information;association rule mining;efficient inference;search engines;association rules;corpus-based;web search engine
clustering;ground truth;classification problem;template matching;vector machine;case study;svm) classifier;svm classifier;machine learning algorithms;template matching;additional information
customer satisfaction;structured databases;lessons learned;real-life;text data;text classification;business intelligence;label-sets;data mining;text classification;interpretability
data mining application;large distributed;large data sets;data sets;data mining;programming paradigm
knowledge sharing;feature extraction;ensemble classifier;knowledge discovery;sensor data;sensor measurements;data streams;prediction) models;kalman filter;main components
detection algorithm;related queries;detection process;spatial distribution;vast number of
transaction data;web content;statistical models;text-based
text mining;emerging trends;relation extraction;classification;scientific articles;decision support system;database;mesh terms;temporal dynamics;topic models;manually annotated;data sources;named entity;ranking functions;clustering
pre-defined;pattern discovery;history data;large collections of;large-scale
network applications;large scale;3;burst detection;user queries;hierarchical structure;high volume
clustering;source code;taking into account;language constructs;software systems;share similar;domain expertise;linguistic information;related concepts;text processing;code base
web databases;probabilistic framework;search services;social networks;modeling approach;digital libraries;search services;main features
radio frequency identification;database;random samples;rfid data;user-defined;item sets;business processes;sample sizes;rfid technology;estimation methods

event-driven;multiple views;interaction networks;dynamic nature of;interaction graphs;large graphs;knowledge discovery;case studies;visual-analytic;network data
prediction accuracy;multiple data sources;data source;demographic information;data fusion;mri data;heterogeneous data sources;heterogeneous data;kernel method;knowledge discovery;data sources;alzheimer's disease;complementary information;biomedical research;kernel framework;alzheimer's disease;selecting features
survival analysis;privacy-preserving;data mining techniques;lower dimensional space;linear programming;large-scale;real data;data sources;privacy preserving;random projection;specific problem;privacy-preserving;benchmark datasets;optimal performance;privacy-preserving data mining;feature selection
transaction data;real-world;build models;high accuracy;supervised learning;simulation results
text mining;web documents;web pages;document classification;document level;multiple-instance learning;relevant content;block level;opinion mining
large scale data mining;online social networks;related topics;large-scale;social networks;human beings;web search;social networks
frequent itemset;inductive database;database;tree mining;mining views;relational tables;data stored in;data mining;association rule;data mining algorithms
source code;open source;data cleaning;data-bases;record linkage;data sets;data mining projects;matching records;graphical user interface;multiple sources;record linkage
text documents;textual content;classification;natural language processing
customer satisfaction;customer relationship management;text classification;customer satisfaction;real-world;text data;growing importance;business intelligence;label-sets;databases;text classification;interpretability
specific applications;data values;data analysis;explicitly represented;synthetic data sets;disguised missing data;user interface;case study;data sets;2;disguised missing data;missing values;heuristic approach
high level;database;data mining techniques;integrated environment;data mining process;data mining models
product feature;review text;product review
dimensional data;result sets;interactive exploration;visualization tools;knowledge discovery;data mining;data mining techniques;interesting patterns;data resources;parameter settings;provide feedback;subspace clustering;low dimensional;traditional clustering;visualization techniques;knowledge discovery process
high volume;user activity
partially labeled;high accuracy
model complexity;sensor measurements;trade-offs;anomaly detection;real datasets;computational cost;matrix decompositions;high-order;streaming data;interesting patterns;tensor analysis;citation networks;higher-order;data cubes;feature selection;window-based;principal component analysis;dimensional data;latent semantic indexing;dimensionality reduction;social networks
privacy-preserving;classification;input features;vector machine;classifier;data matrix;privacy-preserving;svm) classifier;svm classifier;vertically partitioned data
expected number of;computationally hard;partial information;sensitive information;decision makers;prior knowledge;risk analysis;knowledge discovery;decision makers;frequent set;anonymized data;benchmark datasets;data owner
decision trees;security concerns;privacy-preserving;knowledge discovery;privacy-preserving;data mining;data-mining projects;vertically partitioned data
database research;acm sigmod;database research;conference series
pattern-based;data analysis;sequence data;traditional olap;online analytical processing;traditional olap
digital information;personal information management;personal data;user-centered
service oriented;taking into account;web services;web service;search result;web services;web service discovery
real-world;virtual communities;blog posts
closed world;nonmonotonic reasoning;database;databases;incomplete information;relational databases;relational databases;open world;extended logic programs
intrusion detection;data residing;intrusion detection;database server;user requests;database;database management system
large databases;information retrieval;trade-offs;information retrieval
large quantities of;raw data;higher order;knowledge discovery;original data;stream data;derived data;data mining;higher order;processing power
case study;social networks;information consumers;open-source;vast amounts of
classification tasks;data mining;data mining
privacy-aware;spatiotemporal queries;queries involving;analysis tasks;data warehouse;query results;qualitative analysis;decision support;time series;trajectory data;nearest neighbor queries;range queries;location-based services;traffic control;query engine
knowledge discovery;knowledge management;1;knowledge discovery;2;data mining
database research;years ago;information exchange;semi-structured data
query evaluation;xml data;databases;query types;cost model;highly optimized;query processing;conjunctive queries;query languages;building block;relational data;execution times
join operations;semi-structured data;twig queries;xml documents;cost-based;query processing;digital library;xml indexing;query result;indexing methods
rewrite rules;query rewrite;query evaluation;query patterns;structural join;xquery language;database;graph model;index structures;twig queries;xml query languages;join operators;holistic twig join;native xml;databases;join operator
memory footprint;pre-defined;optimization technique;xquery engine;semantic query optimization;xml streams;semantic query optimization;xml elements;optimization opportunities
schema-free;takes into account;search engine;data elements;5;semantic relationships between;9;8;xml trees;structured queries
automatically generates;web pages;web services;web service;active xml;xml view;dynamic content;virtual) view;declarative framework;web services;automatic generation of;web sites;web page;data integration;data intensive
data sharing;network routing;xml data sources;heterogeneous data sources;data representation;peer data management systems;query processing;semantic interoperability;semantic mappings;internet applications;query routing
analytical queries;web-based;data warehouses;relational model;business data;data cubes;decision-support applications;formal framework
unlabeled data;semi-supervised learning;labeled training data;semi-supervised learning algorithms;information extraction;entity recognition;relation extraction;natural language
classification;decision tree;classification performance;instances;ranking performance;5;probability estimation;probability estimates;sheds light on;classification models;loss function;probability estimation;closely related;1;instance;3;2;rates;4;6;operating conditions;auc;true positive;higher accuracy;training set;decision trees;classification;machine learning;classification performance;squared error;binary classifier;classifier
user queries;search engines;valuable information;web queries are
web search;information access;recommender systems;user experience;information access;personalized services;community-based;domain constraints;search engines;information services;relevant information;user studies;machine learning and data mining
statistical methods;domain experts;collapsed gibbs sampling;execution traces;topic models;machine learning;highly correlated;latent-dirichlet-allocation;latent topics;execution traces
target language;edit distance;approximate answers;learning paradigm
multiple dimensions;local density;nearest neighbor;classifier;active learning
lower dimensional;gaussian process regression;large scale;kernel matrix;large data sets;gaussian processes;learning problems;storage requirements
learned metric;tree edit distance;optimization algorithm;recognition task;tree structured data;tree-structured;structured data;tree model;image recognition
bayesian network;training data;random variables;real-world;uniform distribution;bayesian network structure;bayesian networks;maximum likelihood
feature space;sparse data;image segmentation;learning algorithm;level set;pattern classification;classifier;knowledge representation;decision boundaries;traditional classifiers;machine learning;machine learning algorithms;classifier
bayesian model;graphical models;markov models;partially observable;closely related;hidden markov models;transition probabilities;markov chain;dna sequence;training sample
context sensitive;unlabeled data;supervision;classifier;context sensitive;supervised learning algorithm;classifier
density estimation;active learning;active learning methods;uncertainty sampling;model parameters;error reduction
domain experts;accurate models;decision tree;active learning;active learning;decision tree;extracting knowledge from;active learning methods;competing methods;decision tree learning;algorithms produce;training sets;decision tree learner
clustering algorithms;text data;large number of;clustering accuracy;prior knowledge;semi-supervised clustering;supervision;clustering algorithm;ensemble approach;constraint sets
performance guarantees;directed graph;vertex set;worst case;total cost
clustering;spectral clustering;synthetic datasets;spectral clustering;unsupervised clustering;spectral methods;clustering method;hidden markov models;state variables;hidden markov models;time-series;structured data;clustering
biological networks;probabilistic logic;explanation based learning;explanation based learning;statistical relational learning;explanation based learning
learned knowledge;graph-based;reinforcement learning;formal language;graph-based;human input;transfer learning
positive examples;training set;classification;training examples;real-world;learning problem;large number of;instances;learning problems;unlabeled set;positive examples
relational model;learning approaches;relational models;data set;dependency analysis;relational data;real-world;gibbs sampling;maximum likelihood;structure learning of;missing values;relational data
confidence intervals;selected features;instances;parameter tuning;feature selection;uci-datasets;feature selection process
structure learning;large networks;bayesian networks;high accuracy;network structure;network structures;dependency structure
state space;continuous domains;global solution;reinforcement learning;prior knowledge;reinforcement learning;sample points;reward functions;planning tasks
gradient-based;numerical experiments;mutual information;loss function;gaussian process;predictive distribution;source separation;source separation;process models;temporal structure
simple linear;parameter values;training set;dynamic programming;discriminative learning;sequence labeling;real data;sequence labeling
regularization;loss function;regularizer;optimization methods;regularization;optimization problem;convergence speed;feature selection;optimization techniques;loss functions
generalized linear models;process model;bayesian inference;generalized linear models;prior distribution;predictive performance;bayesian inference;expectation propagation
data points;evaluation metric;real-world;model-selection;classifier;model selection
regression trees;training set;test problems;training process;algorithm called;regression methods
pattern recognition;machine learning methods;efficient computation;principal component analysis;large datasets;structured objects;computational burden
auc;classification problems;support vector machines
domain knowledge;clustering algorithms;decision trees;clustering trees;clustering trees;constrained clustering;clustering process;instances;constraint sets;instance level;hierarchical agglomerative clustering;instance level;clustering trees
multi-class problems;aggregation techniques;weighted voting;naive bayes;theoretical results;naive bayes classifiers;individual classifiers
markov models;classification tasks;language models;high-order;high-order;algorithm called;transition probabilities;dirichlet distribution;protein sequence;language modeling
model-free;partially observable markov decision processes;learning algorithm;belief states
multilabel classification;classification approaches;ensemble method;scene classification;single-label;ensemble method;multilabel classification;classifier
decision trees;predictive accuracy;decision tree;class distributions;ensemble methods;large number of;uci data sets;decision process;interpretability;classifier;comprehensibility
generalization error;major source of;boosting methods;data sets;bayesian classifier;boosting algorithms;real world;training sets
decision making;positive results;reinforcement learning;planning problem
learning paradigm;training process;unlabeled examples;semi-supervised;style algorithms;style algorithms
gradient-based;general purpose;policy gradient;model-free reinforcement learning;policy gradient;partially observable markov decision processes;long-term;limited-memory;probability distributions
auc;test instances;uci data sets;valuable information;statistical properties;classifier;ranking performance;classification tasks;model selection
prediction accuracy;naive bayes
continuous variables;linear models;accuracies;multi-target;model trees;target variable;model trees;multi-target;single-target
mutual information;classification;weighted voting;association rules;association rules;prediction models;association rule
document collection;information organization;hierarchical structures;extraction process;user oriented;hierarchical structure;document collections;semi-supervised;clustering approach;benchmark datasets
character recognition;learning examples;classifier;knowledge based;recognition systems;algorithm to compute
regression problem;kernel regression;artificial data
training set;gaussian mixture model;classification;decision boundary;classification tasks;binary classifier;classification problems;positive examples
cost sensitive classification;theoretical framework;feature extraction;cost sensitive;medical diagnosis;additional features;loss functions
action-based;probabilistic models;classifier;action-based;action models;action-based
search algorithm;bayesian networks;relational domains;relational data
probability estimates;attribute values;training data;classification;instance;probability estimation;naive bayes;instances;data sets;classification task
ranking algorithm;classification learning;loss function;label ranking;decomposition techniques;label ranking;multi-class;classifier
phase transitions;phase transitions;generalization error;interaction networks;learning algorithm;machine learning;data set;learning problem;simulation experiments
large number of;text categorization;text categorization;semi-supervised;text classification;semi-supervised;semi-supervised learning methods;text content;manifold regularization
logistic regression;relevant data;multi-task learning;classifier;problem called
rank aggregation;classification;data fusion;ranking functions;learning algorithm;linear combination;ranking models;information retrieval;instances;natural language processing;data mining;rank aggregation;ad hoc;retrieval systems
ensemble learning;base classifiers;decision trees;multi-objective;multi-objective;multiple target;ensemble methods;single target;predictive performance;ensemble methods;prediction tasks;decision trees
clustering;image segmentation;kernel-based;information-theoretic;information bottleneck;kernel-based
training data;data generation;training data is;motivating application;supervised learning;training instances;multi-class;class distribution;random sampling
markov decision processes;sequence labeling;dynamic programming;globally optimal;markov models;sequence labeling;information retrieval;reinforcement learning;ranking algorithms;decision process;natural language processing
base classifiers;computational complexity;pairwise classification;worst-case;classifier;pairwise classification;multi-class;individual classifiers
boosting framework;scale-space;regression problems;loss function;real-world;modeling technique;boosting algorithm;decomposition methods;scale-space;machine learning and data mining
clustering;computational complexity;large-scale;instance-level;constraint sets;produce high-quality;objective function;clustering quality
extraction task;high levels of;active learning;multi-view;real-world;active learning;naive bayes;learning phase;information extraction;feature sets;product descriptions;multi-view;semi-supervised;user feedback;sample-selection
high-dimensional;regularization;data analysis;linear models;large scale;missing values;principal component analysis;sparse data;variational bayesian;principal component analysis;accurate predictions;high dimensionality;missing values
function approximation;transfer learning;reinforcement learning;reinforcement learning problems;regression tree;supervised learning;transfer learning
clustering;instance;training set;class noise;instances;probability distribution over;class membership;instance;high confidence;class labels;classifier;class noise
forward selection;structural features;vast number of;structured data;scoring function;complementary information;stochastic local search;instances;combinatorial optimization problem;feature representation;linear classification;feature sets;structured data;feature set
closely related;cost-sensitive learning;learning algorithm;cost-sensitive;meta-learning;sample sizes;classification algorithms;sampling technique
probabilistic model;real-world;traffic flow;traffic management;probabilistic models;efficient learning
1;subspace analysis;independent components;high quality
kalman filter;parameter estimation;instance;outlier detection;high quality;robotic systems;sensory data;parameter tuning;kalman filter;expectation-maximization
domain knowledge;em) algorithm;state space;graphical models;graphical model;imitation learning;parameter learning;graphical models;imitation learning;graphical models;autonomous agents;probabilistic graphical models;expectation-maximization
neural networks;neural network;higher accuracy;real-world data sets;error rate;machine-learning algorithms;real numbers;continuous optimization
data sets;semi-definite programming;supervision;prior knowledge
graph embedding;feature extraction;graph embedding;real-world
evolutionary algorithms;genetic programming;multiple instance learning;multi-objective;multi-objective;genetic programming;learning algorithms;multiple instance learning;prediction problems;computational cost;multiple instance
refinement operator;rule learning;knowledge representations;stochastic local search;relational learning;semantic web technologies
unlabeled data;semi-supervised learning;labeled training data;semi-supervised learning algorithms;information extraction;entity recognition;relation extraction;natural language
classification;decision tree;classification performance;instances;ranking performance;5;probability estimation;probability estimates;sheds light on;classification models;loss function;probability estimation;closely related;1;instance;3;2;rates;4;6;operating conditions;auc;true positive;higher accuracy;training set;decision trees;classification;machine learning;classification performance;squared error;binary classifier;classifier
user queries;search engines;valuable information;web queries are
web search;information access;recommender systems;user experience;information access;personalized services;community-based;domain constraints;search engines;information services;relevant information;user studies;machine learning and data mining
reusability;hypothesis generation;large-scale;databases;machine learning;databases;machine learning;machine learning;hypothesis testing
pattern-based;training data;induction algorithm;large text collections;basic algorithm;high reliability;extract information from;weakly supervised;instances;data sparseness;information extraction;data sparseness;scientific databases
user preferences;visual content;image collection;graphical model;visual information;feature selection;short term;long term;content based image retrieval systems;visual features;generative model
classification learning;auc;directly optimize the;classification;classification;real-life datasets;classification methods;gradient descent;linear classifier;large datasets;high accuracy
classification;molecular biology;database;automatic methods;database;keyword-based;machine learning;cross-validation;high precision and recall;maximum-entropy;databases;relevant information;perform poorly;classifier
structural patterns;classification;web documents;classification accuracy;graph-based;web documents;structural patterns;classifier;model complexity;emerging patterns;vector-based;graph-based;expert knowledge;document representation;classification algorithm
context-specific;clustering performance;dirichlet mixture;clustering procedure;context-specific
clustering algorithms;hierarchical clustering;randomly generated;real-world networks;community structure;overlapping communities;community structure
privacy preserving;data analysis;market basket;randomized data;association rule mining;market basket data;randomized response;data privacy;data analysis;theoretical analysis;data analysis tasks;market basket;privacy preserving
sensor data streams;feature extraction;gps data;unsupervised learning;data mining techniques;privacy concerns;computing environments;human motion;data stream;sensor data streams
classification;classification;social network analysis;increasing number of;social network;social network data;link-based;data mining;link-based;network data;network clustering;link mining
nearest;nearest neighbour;nearest neighbour;data structures
web pages;keyword extraction;require expensive;content extraction;prior knowledge;performance gain;information retrieval
real-life problems;maximum likelihood estimation;classification;classification;interesting patterns;regression problem;decision rule;rough sets;rough set theory;multi-class;statistical model;decision theory;rough set approach
cross-validation;classification;classification problem;synthetic data;classification;synthetic datasets;classification accuracy;linear transformation;naive bayes;multilayer perceptron;regularization;data set;instance;data sets;decision trees;learning theory;supervised learning algorithms;training set;classifier;nearest neighbors
entropy-based;ad-hoc;naïve bayes;classification;model selection criterion
partition function;markov logic networks;accurate models;discriminative learning;conjugate gradient;perceptron algorithm;markov logic networks;markov networks;gradient descent;rates;statistical relational learning;learning problem;sufficient statistics;increasingly popular
classification techniques;classification;training examples;high dimensional;high dimensional;time series;case study;bayesian framework
domain adaptation;probability models;unlabeled data;target domain;correlated features;probability models;domain adaptation;information extraction;real-life;labeled data;target distribution;conditional random fields;training sample
training data;false positive;detection performance;dynamic bayesian network;machine learning;manual tuning;labeled data;sensor network;databases;learning performance
benchmark data;input parameters;nearest neighbor;classification;past experience;boosting methods;pattern classification;decision rule;real-world data sets;distance metric;nearest neighbor
clustering;gene expression data;partial rankings;partial rankings;statistical significance;partial rankings
high-dimensional;forward search;mutual information;gene expression data sets;classification accuracy;relative entropy;feature selection;mutual information between;class labels;feature subset selection
nearest neighbour;generally applicable;classifier;classification;machine learning
clustering;knowledge discovery;data mining) algorithms;data mining
local classifiers;sequence labeling;sequence labeling;dynamically changing
training data;target distribution;machine learning methods;classification;error rates;vector machine;machine learning;data sets;naïve bayes classifier;training and test data;transfer learning;classifier;test data
clustering results;data analysis;high dimensional;data distributions;clustering process;knowledge discovery;knowledge acquisition;data mining;cluster analysis
clustering;clustering;massive datasets;grid-based;connected components;grid-based
classification;database
data collected by;data manipulation;biased sampling;data reduction;uniform sampling;original data;skewed;sampling technique
expectation propagation;filtering step;takes into account;data analysis;approximate inference;expectation propagation
extended abstract;graph mining;closed sets;pattern mining;real-world applications;problem setting;data mining;operator;special case
multi-relational;spatial data;real datasets;multi-relational;emerging patterns;great potential;emerging patterns;spatial databases;spatial data mining
statistical properties;market basket;synthetic data;databases;artificial data;real life;databases;market basket;association rule mining algorithms
multi-dimensional;shape space;multi-dimensional
euclidean distance between;classification;classification;features extracted from;time series;time series;nearest-neighbor;decision trees;feature selection;high dimensional feature space;predictive accuracy
mobile objects;clustering algorithm;clustering
multi-relational classification;real world datasets;multiple-features;classification process;inductive logic programming;multi-relational classification;aggregation functions
fall short;frequent itemset mining;frequent itemsets;transaction databases;computationally tractable;data mining
high-speed;high correlation;neural-network;large number of;predictive model;sensor network;data mining;clustering algorithm;data arrives;clustering model
web documents;hidden-web;hidden web;classification;classification;database;database;combined method;web pages;classification tree;link-based;databases;databases;hidden web;hidden-web;query terms;class-specific
high utility;target class;data mining methods;classification;database;classification tasks;multi-relational;multi-relational databases;predictive performance;discover patterns;relational schema;relational database;classification model;predictive accuracy
general case;link analysis;special case;computationally intractable;general form
local patterns;temporal information;data sets;data partitions
randomized response;binary data;heuristic rules;categorical data;randomized response;user privacy;privacy-preserving data mining
clustering;bayesian methods;pre-processing;synthetic data;binary data;uncertain data;data sets;data sets;modeling approach;pre-processing;data mining methods
collaborative filtering;large-scale;graph-based;real life datasets;recommendation algorithms;collaborative tagging;baseline methods
naïve;collaborative filtering;data owners;real data;bayesian classifier;bayesian classifier;data collected;naïve
game theoretic;game-theoretic;real-life applications;sensitive data;privacy-preserving distributed;privacy-preserving;data mining;multi-party;distributed data mining;multi-party
clustering;clustering algorithms;query execution;clustering method;data sets;fuzzy clustering;xml documents;xml documents;hierarchical structure;feature vectors;clustering
database;outlier detection;decision-making;machine learning;detection methods;databases
classification;graphics processing units;color space;neural network;body parts;rates;neural network
selection problem;nearest neighbor classifier;classification accuracies;computational requirements;game theoretic approach;primary goal;game theory
classification;classification;extracted features;classification accuracy;dynamic bayesian networks;graphical model;joint distribution;data streams;seismic data;dynamic bayesian network
density-based;synthetic data;mixture model;density-based clustering;visual presentation;latent space;exact inference;measurement errors
prediction accuracy;training-set;text categorization;text categorization;svm classifier;margins;classifier
virtual machine;training set;classification;language called;classification
real world data mining applications;extreme values;cost-sensitive learning;regression models;target variable;test cases;classification problems
multi-label classification;classification model;multiple labels;classification;classification accuracy;single label;large number of;text categorization;multi-label classification;instances;associative classifier;binary classifiers;single-label;associative classification;multi-label;multi-label;instance-based
dna sequences;spanning-tree;genomic data;visual exploration of
association mining;association patterns;large databases;large data sets;data mining;interestingness measures
classification;semantic level;real-world;semantic features;machine learning;text classification;problems require;text classification;information retrieval
medical applications;case-based reasoning;case base;application programs
ct images;classification;classification;greedy algorithm;8,23,36;selection method;selection methods;feature selection;feature selection methods;9;individual features;high-resolution;feature selection
genetic programming;numerical analysis;reference model;genetic programming;data types;temporal sequences;application called;time series;time series;helps users
neural networks;data mining;raw data;data mining;information technology;neural networks
clustering;knowledge discovery;knowledge discovery;clustering
clustering;image segmentation;spatial information;machine learning;clustering algorithm based on;clustering model;image segmentation
computational framework;classification techniques;knowledge discovery
clinical data;decision-making;integrated environment;multimodal data;knowledge embedded in;multimedia objects;biomedical research;multimedia data
application domain;knowledge management;application domains;case base;data types;similarity measures
ensemble classifiers;classification accuracy;gene expression;high quality;classification results;weak classifiers;ensemble classifier;boosting algorithms;benchmark datasets;gene expression;weak classifiers
naïve bayes;classification model;classifier;classification
real-world;classification tasks;classification problems;classification
data structure;training set;density-based;data generation;density-based clustering;artificial data;probability density function;machine learning;multidimensional datasets;classification accuracy;imbalanced datasets;training data;true positive;classifier performance;multidimensional space;classification algorithm;uci datasets;classification algorithms
clustering;additional knowledge;unsupervised learning;data clustering;piecewise linear;data set;feature selection process;feature vectors
statistical properties;data mining methods;confidence intervals;association rules;statistical tests;association rules;association rule;association rules
9;association rule mining algorithms;post processing;weighted association rule mining;quantitative attributes;user defined;databases;weighted association rule mining;association rules
feature selection techniques;naïve bayes classifier;internet users;naïve bayes;selection techniques;feature selection technique
case study;pre-processing;spam filtering;bayes classifier;spam filtering
filtering systems;similarity measures;similarity measures
information systems;large scale;browsing behavior;information systems;real world;helps users
web pages;classification;data mining techniques;case study;web content;web site;web page;data mining algorithms
access logs;web usage mining;web navigation;user defined;user experiences;decision makers;web site;sequential data;web usage;sequential pattern;user behaviors
data mining;graph matching;graph based;database;graph matching;data representation;subgraph isomorphism;information retrieval;graph based;data mining;graph based data mining;relational data
united states;database;contrast-set mining;databases
data mining tool;graph-based;case study;data mining;software components;data mining algorithms;graph representation
markov process;petri nets;multi-agent system;social systems;physical resources
sequence labeling;dynamic programming;taking into account;training examples;active learning;sequence data;active learning;sequence labeling;labeling problem;labeling effort;query processing;maximum margin;supervised learning algorithm;natural language
clustering;clustering;distance function;clustering algorithm;index structure;real datasets;range query;time series;indexing scheme;time series;pruning method;filtering methods;evaluation techniques;databases;similarity searching;pruning power;search efficiency
data structure;protein sequences;natural language;data mining
case-based reasoning;pattern recognition;intelligent agents;rule-based;trading agents;human supervision
formal concept analysis;classification;unstructured text;exploratory data analysis;data processing;databases;discovered knowledge
scale-invariant feature transform;image recognition
formally defined;privacy-preserving;information source
14;database;information flow;fundamental problem;database privacy;data utility
data sharing;optimization problem;sensitive data
detection techniques;access control;database;database;databases;anomaly based;implementation issues
domain knowledge;database systems;database;user queries;update operations;dynamic databases;dynamic databases
data integrity;data provenance;data providers;takes into account;data sources;trust model;data provenance;information technology
distributed environments;business processes
instances;sensitive information
context information;perform inference;sensitive attribute;related information;sensitive data;case study;location-privacy;location based service;data-privacy;privacy requirements;location privacy;context-based;data-privacy
security policies;query rewriting;ontology-based;access control;access control;query rewriting;semantic web;web resources;web data
fine-grained;related data;fine-grained;service-oriented;access control;role-based;healthcare data;coarse-grained
fine-grained
graph model;ad hoc;role-based access control
model called;database
business intelligence applications;data warehouse;privacy requirements;business intelligence applications;privacy requirements;data owner
real-life applications;data mining;provide answers;theoretical results;large data volumes;business intelligence;data mining;data mining process;predictive analytics
years ago;machine learning methods to;processing algorithms;mathematical models;highly variable;image data;image processing
discriminant analysis;unsupervised learning;classification;cluster analysis;data clustering;category information;class labels;supervised learning;cluster analysis
human-generated;training data;automatically constructed;natural language
large space of;very large datasets;data mining tasks;data sparsity;data mining
state space;training data;high-quality;training set;classification;sample complexity;reinforcement learning;high confidence;2;statistical test;inverted pendulum;major limitation;classifier;2,3
partition function;partition function;graphical models;prohibitively expensive;parameter estimation;variational methods;upper bounds;structure learning;optimization problem;belief propagation;graph nodes;large graphs;lower bounds;loopy belief propagation;upper bound;lower bound;applications requiring;model selection;problem (solving
large volume;transductive learning;equivalence classes;hypothesis space;equivalence class;learning algorithm;large margin;transductive learning;training sample
classification-accuracy;instances;fault detection;entropy based;conditional entropy;ranking scheme;information content;classification problem;memory-usage;training dataset;instance;3;data streams;auc;training data;condition-monitoring;training datasets;training set;high-quality;classification;user-defined;classifier
graph theory;collaborative filtering;collaborative filtering;discriminative information;information retrieval;user similarity;information overload;user similarity;collaborative filtering algorithms;collaborative filtering;local similarity
benchmark data;auc;synthetic data;theoretical foundation;labeled instances;class label;sheds light on;performance metric;instances;ranking performance;theoretical analysis;margin;model selection
training data;collaborative filtering;regularizer;kernel approach;data sets;matrix factorization;3;2;squared error;predictive performance;maximum margin;algorithm to compute
search terms;probabilistic graphs;large graph;instance;biological information;search engine
database;databases;2;intermediate data;databases
data matrix;matrix decompositions;matrix decompositions;input data;data mining
mining task;graph mining;graph-based data;microarray data;database;mined patterns;social networks;support threshold;pattern discovery;numerous applications;community discovery;web data
theoretical framework;equivalence classes;condensed representation;sequential patterns;sequential patterns;frequent itemset mining;frequent sequences;association rules;negative result;sequential pattern mining
frequency information;condensed representation;real-world applications;user-defined;equivalence classes
2;1;extended abstract;multi-aspect;data mining and knowledge discovery;real datasets;high-order;tensor analysis;3;data stream;5;4;compression ratio;pattern discovery;multiple aspects
computational methods;prediction methods;biological processes;rule-based;structural characteristics;hidden markov models;prediction method;svm classifier;support vector machines;high-resolution
posterior probabilities;classification;regression tasks;information retrieval;gaussian processes;real world;loss functions;regression tasks
communication overhead;classification;classification accuracy;learning models;data privacy;upper bound;svm classifiers
image data sets;classification tasks;classification performance;low dimensional representation;transfer learning;learning method
association rules;association rules
benchmark data;majority voting;classifier;instances;comparative analysis;classifier
medical diagnosis;regularization;discriminative features;multi-task;global convergence;probabilistic model;real-world;learning algorithms;motion analysis;multi-task learning;medical images
high correlation;regularization;canonical correlation analysis;data manifold;regularization;dimensionality reduction technique;model selection criterion;highly correlated;canonical correlation analysis;manifold structure of;operator;semi-supervised;semi-supervised;dimensionality reduction
structured output;training examples;accuracies;inference procedure;exact inference;support vector machines;higher order
15;large graphs;real-life;2;5;random walks;databases;semi-supervised;classification problems;semi-supervised classification;random walks
similarity function;vector space;collaborative filtering;collaborative filtering;correlation coefficient;user-defined;similarity metrics;automatically learn;theoretical analysis;similar items;benchmark datasets;similar users;matrix factorization
data values;web pages;human effort;target domain;semi-structured;web sites;data types;web pages;web site;extraction accuracy;semi-structured;information extraction from;human supervision
state space;learned knowledge;algorithm called;markov decision process;markov decision process;multiagent learning
multi-class problems;growing number of;redundant features;classification tasks;feature selection algorithms;feature selection;very large datasets;multi-class;filtering methods
decision trees;decision tree construction;decision tree;learning algorithms;objective functions;decision trees;wide range;perform poorly;comparative analysis;information gain
multiple classes;model averaging;prior knowledge;model averaging;instances;bayesian networks;algorithm to compute
training samples;fast algorithm;vector machine;linear svm
decomposition methods;svm training;svm training;nearest point
training data;monotonicity constraints;classification;error-rate;prior knowledge;data set;training sample;monotonicity constraints;nearest neighbour;machine learning;nearest neighbour;response variable;classifier
graph-based;automatically determines;learning tasks;real-world domains;knowledge transfer;source tasks;learning task
graph mining;weighted graphs;feature selection
clustering;conceptual clustering;distance-based;conceptual clustering;operator;metric space;distance-based
real world;databases;mining frequent;mining process;mining frequent
clustering;clustering;low-dimensional representation;probability density functions;probability density functions;segmentation problem;null space;dimensionality reduction
data arrives;large volume;naïve;random projection;semi-supervised learning;convex programming;stochastic gradient descent;learning tasks;online learning;kernel space;real-world;manifold regularization;online algorithms;theoretical guarantees;manifold regularization
large networks;fast algorithm;overlapping communities;overlapping communities;community structure
risk management;measure called;case study;data mining tasks;sequential patterns;case study;sequential pattern mining
effective pruning;pruning techniques;evaluation functions;multi-class
multi-modal;natural scenes;visual cues;partially labeled;human actions;unlabeled data;labeled examples;semi-supervised;learning method
probabilistic databases;parameter learning;naturally leads to;training examples;approximation error;probabilistic database;optimization problem;probabilistic databases;real world
support vector machines;distance functions;classification;distance function;distance metrics;classifier;classification performance;instances;binary classifiers;nearest neighbour;classification problems;nearest neighbour
probability estimates;artificial data;classification;classification;support vector machines;decision boundary;density estimator;probability estimation;uci datasets;target class;recognition problem;novelty detection;density function;learning problem;estimation techniques;probability estimation;classification algorithm
bounded treewidth;subgraph mining;computationally hard;database;np-complete;bounded treewidth;subgraph isomorphism;subgraph mining;general case;pattern matching;frequent pattern mining
auc;classification model;evaluation metric;machine learning;classification performance;classification models;model selection;common practice;model selection
high-dimensional space;multiple views;multiclass classification;classifier performance;distance-preserving;projection-based;classifier performance;dimensional space;classifier
linear model;loss function;dimensionality reduction method;high-dimensional space;machine learning;laplacian matrix;dimensional data;local information;real-world applications;low-dimensional;dimensionality reduction
sample complexity;relevant features;parameter estimation;irrelevant features;large number of;logistic regression;dimensional data;supervised learning
benchmark data;face detection;problems require;vector machine;support vector machines;margin-based;theoretical guarantees
dynamic programming;exact inference;np-complete problem;structural svms;loopy belief propagation;structural svms;approximate inference;sampling algorithms;approximate inference;prediction models
semantic networks;semantic networks;large volumes of;learning approaches;clustering approaches;large quantities of;extracting knowledge from;supervision;1;relational clustering;markov logic;machine learning approaches;clustering
class label;weighted average;classification algorithms
large-scale;traffic light;reinforcement learning;reinforcement learning;traffic flow;tree-structured;locally optimal;traffic control
transaction data;stream mining;data streams;market basket data;probability distributions;sensor networks;data streams;data storage
classification models;database;single target;databases;quality measures;association analysis
united states;statistical model;inference algorithm;synthetic data;user-generated
real-world datasets;biological networks;pruning techniques;common properties;algorithm called;social networks;pruning techniques
large-scale;base classifiers;document collection;multilabel classification;database;large-scale;concept hierarchy;predictive performance;perceptron algorithm;classification problems;classifier;classification algorithms
gradient-based;importance sampling;function approximation;reinforcement learning problems;1;regression methods;state-action;state-action
parameter space;joint distribution;numerical experiments;linear model;policy gradient;large number of;reinforcement learning;state-action;objective function;1;policy gradient;stationary distribution;2;state-action;reward function;statistical learning
programming languages;programming languages;machine learning;context free;incremental learning;machine learning;programming language
learning framework;discriminative learning;classification;classification;pairwise constraints;pairwise constraints;data sets;labeled data;margin-based;learning problem;semi-supervised;margin-based
nearest neighbor;margin;learning algorithms;vector machine;local neighborhood;distance metric;data sets;metric learning;learning problem;support vector;margin-based;semi-definite programming;classification task
support vector machines;kernel matrix;large-scale;kernel matrices;learning algorithms;data reduction;data reduction;classification performance;data sets;upper bounds;wide range;support vector machines;classification error
clustering;clustering;underlying structure;approximation guarantees;approximation guarantees;clustering algorithm;theoretical guarantees
large graphs;parameter-free;community discovery;user intervention;document-term;local patterns;context-specific;bipartite graph;allowing users to;information theoretic
classification rule;genetic algorithm;fitness function;rule-based;data collections;naive bayes;text classifiers;test sets;classification algorithms
gradient-based;optimization techniques;monte carlo;gaussian process regression;covariance functions;real-world;gaussian processes;markov chain
kernel-based;convex optimization;kernel-based;learning tasks;text data;supervised learning;meta level
reinforcement learning algorithms;real-world;model-free;policy gradient;faster convergence;likelihood ratio
computational complexity;classification models;error bounds;classification;communication costs;theoretical results;feature representation;feature selection;dimensional data;margin based;hash-based;individual classifiers
clustering;spectral clustering;large-scale;ad-hoc;large-scale;stochastic gradient descent;data clustering;optimization problem;prior knowledge;dimensionality reduction techniques;optimization approach;main idea;real-world datasets
clustering;data point;cluster structure;data points;sensor data streams;data stream;sensor networks;central server;clustering algorithm
real data sets;classification;random variables;learning algorithm;instances;data sets;feature subset;selection algorithm
high-dimensional;domain experts;feature selection techniques;great promise for;small sample sizes;classification performance;feature subsets;feature selection;feature selection methods;model selection;giving rise to;feature selection technique;feature selection techniques
complex network;diffusion process;real networks;minimization problem;complex networks;directed graph;embedding methods;label assignment;numerical experiments;conditional probability;diffusion process;network topology
domain knowledge;domain experts;inductive learning;synthetic datasets;active learning;real world datasets;class label;active learning;domain knowledge;active learning methods;classification accuracy;remote sensing;labeled examples;transfer learning;text classification problems
matrix factorization;matrix factorization;optimization algorithms;special structure;clustering
clustering;spectral clustering;parallel algorithm;spectral clustering;data instances;finding clusters;large datasets;clustering algorithm
scene analysis;classification;multiple labels;medical diagnosis;higher classification accuracy;generative models;parameter estimates;multi-label classification;data sets;labeled data;data item;multi-label data;real world;label sets;synthetic data
active learning;active learning methods;linear regression;target function;benchmark datasets;learning method
bayesian network;bayesian network structure;continuous domains;real-world;conditional independence;learning algorithm;small sample sizes;probability distribution;graphical models;structure learning of
feature space;kernel-based;real-world;time series;time series;model-free;prediction error;operator;kernel framework
clustering;clustering;absolute error;kernel regression;data point;cluster labels;data sets;squared error;clustering algorithm;local learning;sparse matrix
mining results;probabilistic model;high quality;markov random fields;tree structure
reinforcement learning algorithm;state space;sample complexity;reinforcement learning;complex tasks;model-free;instances
efficient inference;artificial intelligence;probability distribution over;probabilistic logic;statistical relational learning;relational domains
binary classification;multi-class problems;multi-class classification;multi-class;algorithm performs;classification;boosting framework;semi-supervised learning;multi-class classification;semi-supervised learning algorithms;boosting methods;unlabeled examples;binary classifiers;boosting algorithms;learning problem;semi-supervised;multi-class;uci datasets;semi-supervised
error propagation;multi-level;entity recognition;unified framework;finite state;word segmentation
clustering;clustering results;low-dimensional representation;unlabeled data;dimensionality reduction;face recognition;prior knowledge;data analysis;dimensionality reduction;class labels;algorithm named;labeled data;dimensionality reduction
latent variable model;lower-dimensional;artificial data;lower dimensional;learning approaches;manifold learning;data sets;hierarchical model;compact representation;original data;low dimensional;learning framework;learning framework
predictive models;predictive accuracy;similarity-based;external knowledge;parallel computation;similarity-based
set cover;additive noise;correctly identify;reinforcement learning;regression tree;finite mixture;probability distributions;action models;context-specific;dynamic bayesian network;small samples;tree learning;algorithms rely on
temporal dependencies;unsupervised learning;network intrusion detection;event sequences;require expensive;explicitly modeling;bayesian networks;bayesian networks;network traffic;normal behavior
clustering;intrusion detection;real-world;data items;artificial datasets;data streaming;clustering algorithm;data distribution
data set;discriminant analysis;unlabeled data;data points;confidence measure;linear discriminant analysis;discriminant analysis;optimization problem;real-world applications;high confidence;algorithm called;labeled data;optimization procedure;class labels;semi-supervised;semi-supervised;dimensionality reduction;benchmark data sets
multiple domains;multiple domains;classifier;classifier performance
quality assessment;clustering approaches;data sets;subspace clustering;subspace clustering;quality measures;subspace clustering
edit distance;automatically learn;1,2,3,4
data mining techniques;integrated environment;data mining
web-based;open source;knowledge-based;significant patterns;data clustering;knowledge discovery;decision making
inductive database;instance-based learning;query language;rule-based;inductive database;data sets;clustering;classification models;relational model;software architecture;sql extension;activity relationships;pattern mining;gene expression analysis;relational data;database;relational model;multi-relational;query language;data model;distance measures;feature selection;knowledge discovery process
clustering;local optimization;sampling approach;classification;user-defined;distributed nature of;rates;large datasets;databases;data-mining tasks
sparse data;linear algebra;directed graphs;latent semantic;semantic graphs;additional information
collaborative filtering;user-item;user preferences;recommender systems;collaborative filtering;prediction accuracy;large scale;optimization problem;low dimensional;similar items;nearest neighbors;user-oriented
association rules;data analysis;case study;real-world
pattern sets;information contained in;pattern mining;machine learning;data sets;selection techniques;classification results;pattern set;heuristic approach
citation network;heterogeneous network;social network;heterogeneous network;additional information;graph-theoretic;random walks
graph partitioning;community discovery;synthetic datasets;real-world;social network;social network;prior information
local search;completeness;frequent patterns;mining frequent;efficient algorithms to;sequential patterns;global search;sequential patterns;efficient discovery of;frequent pattern
instance;accurate classifier;active learning;active learning framework;real-world;data streams;instances;stream data;data volumes;data streams;active learning;labeling process;classifier
training set;imbalanced data;imbalanced data;test instances;valuable information;test instance;instance;classification error;base learners;decision boundaries;error reduction;nearest neighbors
pattern recognition;classification;subspace learning;regularizer;principal component analysis;linear discriminant analysis;information processing;information retrieval;subspace learning;data mining;clustering problems;real world;dimensionality reduction
incremental algorithm;mining frequent itemsets;data distributions;frequent itemsets;space requirements;theoretical analysis
margin-based;entity types;high computational cost;text documents;named entities;computational cost;named entity;maximum-entropy;entity recognition;extraction task;classification task
nonnegative matrix factorization;background information;pairwise constraints;supervision;prior knowledge;clustering documents;document clusters;data sets;semi-supervised clustering;document clustering;iterative algorithm;document clustering;semi-supervised;document clustering;similarity matrix
computational efficiency;upper bound;false alarm;novelty detection;data set;positive definite;novelty detection;local structure;multidimensional data
randomized algorithm;synthetic datasets;representative set;high quality;graph patterns;user defined;0, 1;graph mining;graph patterns
classifier performance;sample selection bias;performance predictors;comprehensive evaluation;stationary distribution;classifier performance;real world applications;data distribution;classifier;classification algorithms
clustering;clustering algorithms;multi-view clustering;data analysis;data points;feature space;clustering solution;multi-view clustering;dimensional data;real world applications;high-dimensional;benchmark data sets
training data;stream mining;increasing number of;expected error;observed data;streaming data;model averaging;incoming data;implicit assumption;baseline models;data streams;data streams;class label
scientific data;large graphs;minimum spanning tree;protein interaction networks;approximation algorithm;exploratory mining;quality guarantees;approximation ratio
data mining;clustering algorithms;predictive models;customer segmentation;segmentation methods;index terms;statistics-based;customer data;customer behavior;customer segmentation;fitness;individual preferences;transactional data
attribute values;statistical measures;dynamic programming algorithm;discretization methods;data sets;classification errors;continuous data
text mining;association mining;link analysis techniques;extracted features;link-analysis techniques;text documents;algorithm generates;information extraction;knowledge discovery;text representation;document collections;text retrieval;document collections;link analysis techniques;special case;information retrieval
clustering algorithm;predictive models;share common
classification;classification;sampling method;reconstruction error;strong evidence;compression ratio;classifier
real-world datasets;discriminant analysis;analysis reveals;dimension reduction;linear discriminant analysis;discriminant analysis;machine learning;pattern classification;dimension reduction;dimension reduction
graph partitioning;community structures;model generalizes;pattern based;great potential;prior knowledge;theoretical analysis
itemset mining;parallel algorithm;closed patterns;multi-core;frequent closed;multi-threaded
training set;supervised learning;nearest neighbor;neural networks;classification;support vector machines;formal description;data mining problem;supervised learning
sample selection;2, 14;selection problem;exhaustive search;np-complete problem;real datasets;set cover;customer reviews
statistical information;frequent patterns;frequent itemsets;mining process;transactional databases;interesting patterns;statistical information;fault-tolerant;integer linear programming;transactional databases;data mining;fault-tolerant
predictive accuracy;digital content;graph-based;computational cost;mobile devices;data set;mobile phone;real-world
unsupervised learning;classification;ad-hoc;probabilistic model;social network;social network;classifier;expert finding;baseline methods;rule learning;conditional random fields
feature space;synthetic data;gaussian distributions;large number of;linear discriminant analysis;multimedia retrieval;data mining;kullback leibler
kullback-leibler divergence;real datasets;statistical test;maximum entropy;entropy based
link formation;semantic features;social network analysis;probabilistic models;semantic similarity;probabilistic graphical model;social network;large graphs;link prediction;link prediction;real datasets
general purpose;text documents;classification methods;classification results;text classification;classification algorithms
named entities;average precision;named entities;set expansion;markup language;semi-structured documents;language-independent;set expansion;set expansion
clustering;clustering;high-dimensional;feature extraction;pattern recognition;dimension-reduction;fixed-length;hierarchical clustering;time series;time series;case study;global structure;motion sequences;clustering algorithm;human motion
clustering;distance functions;distance) functions;clustering method;skewed;real-world data sets;cluster sizes;validation measures;clustering quality
large scale;graph-based;learning tasks;belief propagation;markov random fields;computational cost;belief propagation
real-world;main memory;time series;video surveillance;time series
matrix factorization;nonnegative matrix factorization;factor matrices;binary matrix;real world datasets
naive bayes classifier;text documents;classification methods;text categorization;machine learning;statistical information;semi-structured documents;natural language processing;svm classifier
clustering;dimensional space;aggregation techniques;clustering approaches;data resources;subspace clustering;clustering methods;data mining;real world;subspace clustering algorithms;subspace clustering
binary classification problems;5;time series;7, 3;6
data point;special case;sparse data;conditional distributions;variational inference;naive-bayes models;exponential family;em algorithm;latent structure;naive-bayes;mixture models
kernel discriminant analysis;discriminant analysis;regression problems;data points;computational cost;large number of;kernel matrix;linear discriminant analysis;input space;regularization;kernel discriminant analysis;training samples;reproducing kernel hilbert space;graph analysis;incremental version;efficient computation;face recognition;highly nonlinear;separability;times faster than
user preferences;feature types;candidate patterns;spatial datasets;mining process;indexing structure;pattern discovery
profile-based;recommender systems;social network;event detection;social networks;behavior model;neural network
synthetic data sets;biological networks;graph mining;data mining methods;large number of;mining frequent;mining frequent;patterns mined;knowledge discovery;social networks;mining process;complex structures;protein-protein interaction network
document transformation;entropy-based;label assignment;text categorization;text categorization;feature selection algorithms;feature selection;text collections;document transformation;single-label;multi-label;feature selection problem;multi-label;svm classifier;multi-class;feature selection
clustering;real data sets;personalized recommendation;diverse set of;random walk;random walk on;random walk on;highly correlated;recommending items
medical applications;learning algorithm;selection method;machine learning
case study;entity extraction;scientific articles
clustering;case study;data mining techniques;application domain;real-world;user behavior;sequential patterns;case study;data mining;problem-solving;patterns discovered;high-level;sequential pattern;discovery problem
clustering;local patterns;itemset mining;binary attributes;interesting itemsets;approximation algorithm;interesting patterns;data set;numerical attributes;numerical attributes
mining frequent sequences;sequence mining;10;sequence mining;data mining tasks;2;real datasets;6;support counting;sequence mining
clustering;clustering;vector space;takes into account;class labels;data-mining techniques;online news
em) algorithm;text data;latent topics;kernel density;em-algorithm;prior distribution;expectation maximization;latent semantic indexing;latent topic
relevant objects;score based;learning algorithm;inference procedure;relevant objects;data-set;identification problem;labeled dataset;unlabeled data
text mining;memory footprint;training data set;step size;training examples;large-scale;large scale;stochastic gradient descent;rates;step size;conditional random fields;mining tasks
clustering;active learning;pairwise constraints;pairwise constraints;selection method;error model;objective function;text document;document clustering;document pairs;active learning approach;semi-supervised;clustering approach;semi-supervised;language modeling
change analysis;neighborhood graph;multi-sensor;nearest neighbors
training set;test set;rule learning;meta-learning;large number of;rule learning
collaborative filtering;recommender systems;web sites;web site;web site;nearest neighbors
complex network;mining algorithms;graph mining;complex networks;stock market;complex networks;time series;complex networks;real world applications;network topology
detection techniques;web page;classification
latent structure;computational approach;text analysis
time-series;time-series;real datasets;input-output;state-space
partial matching;real datasets;time series;real numbers;stereo correspondence;recognition problem
mining frequent itemsets;frequent itemsets;database;generative models;generative models;frequent itemsets;generative model;statistical significance;pattern discovery;benchmark data sets
nonnegative matrix factorization;clustering;semi-supervised clustering;consensus clustering;clustering criteria;heterogeneous data sources;semi-supervised clustering;wide range;clustering problems;semi-supervised;nonnegative matrix factorization
event logs;classification techniques;nearest neighbor;high-end;rule-based;data set;event logs;fault-tolerant;nearest neighbor;support vector machines;classifier
ranking scheme;classification problem;ranking method;classification framework;feature ranking;social network;text data;training data;text classifier;social networks;ranking methods;local feature;social networks
obtained by applying;data mining applications;database;memory requirements;fp tree;frequency) counting;support counting;sparse datasets;distance-based;data mining;data mining algorithms
data collected by;motion capture;synthetic data;theoretical properties;temporal patterns;categorical data;time series;real-valued;data sets;multivariate data;fundamental problem;pattern discovery;pattern discovery;temporal data mining
clustering;consensus clustering;clustering performance;consensus clustering;clustering ensembles;multiple clusterings;data sets;iterative algorithm;clustering methods;clustering aggregation
high-speed;function approximation;predictive model;generic framework;learning problem;data mining algorithms
sample size;dimension reduction;linear discriminant analysis;small sample size;recognition tasks;efficiently computing;dimension reduction;semi-definite programming
text categorization;machine learning methods;similarity measurement;real world datasets;great success;tf-idf;text categorization;information retrieval;partial matching;local information;text processing;clustering procedure
database size;static databases;main-memory;upper bound;sequential pattern mining;mining results;static databases;data stream model;data streams;sequential pattern mining;sequence mining;real-world;data streams;pre-processing;sequential pattern mining;sampling methods;mining algorithms;potentially infinite;database;knowledge discovery;databases;error rate
traditional models;stock market;stock market
community structures;discovery algorithms;measure called;community discovery;complex networks;large number of;optimization problem;computationally expensive;real-world networks;social networks;high accuracy;low accuracy;np-hard;social science
network analysis;link structure;community-based;community-based;link structures
ranking approaches;survival analysis;training data is;convex programming;support vector;globally optimal solution;support vector
data point;classification;case study;case study;discrete data;data mining;classifier
training data;tree edit distance;edit operations;learning algorithm;expectation maximization;distance measures;parameter learning;tree edit distance;learning problem;probabilistic model
human movement;case study;information theory;human movement;wavelet-based
classification quality;classification;significant rules;association rule mining;rule-based;medical diagnosis;statistical techniques;associative classifiers;imbalanced datasets;rule base;imbalanced data sets;associative classification;rule-based
12;data generation;sensitive information;databases;data distributions;prior knowledge;original data;data perturbation;perturbed data;preserving privacy
mining frequent patterns;frequent pattern mining algorithms;frequent patterns;real-world;databases;large databases;data mining research;discover patterns;transaction databases;dynamic behavior;mining framework;frequent pattern;transaction database
latent dirichlet allocation;probabilistic model;topic model;topic models;topic discovery;topic-specific;information retrieval performance;trec collection;information retrieval;mining tasks
clustering;distributed data mining;clustering aggregation;clustering aggregation;mechanism design
frequent itemsets;data streams;data set;frequent itemsets;data stream;prefix tree
clustering;clustering;decision function;large number of;support vector;support vector;support vector;cluster boundaries;high dimensional feature space
ensemble approach;linear combination;data sets
clustering;real data sets;multiple data streams;pruning techniques;dynamically changing;multiple streams;search space;data sets;stream data;incremental clustering;subspace clustering;data streams;clustering algorithm;sliding windows;subspace clustering algorithms;subspace clustering
real-world;attribute values;supervised learning;active learning approach;data quality
text mining;text classification;classification techniques;classification;text analytics;data cleaning;rule learning;noisy datasets;information extraction;unstructured text;real-life;real life;data processing;text classifiers;text classification;benchmark datasets;pre-processing
clustering;clustering;text classification;information bottleneck;outlier detection;machine learning;outlier detection;information theoretic;cost function;information theoretic
domain knowledge;user)}$$ feedback;candidate matches;user feedback;conflict-resolution;data structures and algorithms;ficsr (pronounced as "fixer;conflict-resolution;query processing
community-based;large-scale;semi-structured;information entropy;information processing;schema matching;instance;personal information;social communities;schema mappings;large-scale distributed systems;computationally expensive
storage devices;database;semantic associations;increasingly large;space usage;personal information;information integration
query optimization;streaming algorithms;main memory;synopsis construction algorithms;approximate query answering;broader range;real-life and synthetic data;algorithms rely on;space complexity
schema translation;database;user interfaces;source schema;object-oriented;automatic generation of;operator;database schemas;xml schema
adaptive strategy;communication cost;takes into account;streaming data;solution space;distributed processing;cost-based;distributed stream processing system;tree construction;dynamic environment;cost model;cost-effective;data characteristics;processing cost
clustering;sql/xml;historical data;database systems;database;temporal information;indexing techniques;databases;query optimization techniques;data model;temporal queries;xml-based;xml views;relational databases;relational database;query conditions
pattern matching;position information;query length;database size;index structure;text data;query performance;text databases;databases;string matching;information retrieval;query processing
years ago;parallel programming;provide answers;database;design choices;3, 4;database transactions;1;case study;2
database;databases;data sets;databases;data-centric;medical knowledge;biomedical research;database researchers
high-dimensional;search methods;lessons learned;high-dimensional spaces;data types;image data;similarity search;search problem;nearest-neighbour;similarity search;vector spaces;databases;sequential scan;quantitative analysis
physical design;constraint language;real-world situations;database systems;search space
multi-query optimization;distributed settings;large volumes of data;exploratory queries;optimization problem;distributed queries;communication cost;exploratory queries;scientific databases;user-friendly;np-hard
sql queries;relational database systems;long-running;application servers;relational tables;management systems;computationally-intensive;rates;computing systems;data management system
program analysis;high level;optimization framework;high level language;real-world;declarative queries;query optimization techniques;monitoring applications;database;database;graph analysis;control flow;traditional database;answer queries;application server
web applications;search engine;monte carlo;search engine's;user types;databases;database;search engines;query logs;query log
intrusion detection;dynamic programming algorithm;multi-step;plans;heuristic algorithms;event detection;monitoring applications;complex events;distributed settings;event detection;real-world data sets;distributed sources;network monitoring
data-driven;supply chain;mining algorithms;sequence patterns;sequence data;sequence mining;rfid-based;input sequences;real data;sequence data;sensor networks;frequent sequences;desirable properties;support threshold;large database
clustering;ground truth;conventional methods;data points;data clustering;clustering accuracy;theoretical analysis;instance-level;algorithm takes;multidimensional data;locally weighted
networked data;data sets;social networks;network structure;network data;privacy risks
greedy heuristics;privacy-preserving;database;partial knowledge;efficient algorithms to;sensitive data;set-valued data;data dimensionality;common practice;high cost;real datasets;transactional data
text search;document retrieval;search result;search engines;similarity-based;query results;threshold-based
data sharing;tree structures;data integrity;sensitive information;data publishing;tree structure;tree-structured data;signature scheme;xml documents;data structures;signature schemes;content distribution;lower cost;data organization
real-world;dynamic nature of;high quality;rates;novelty detection
web portals;web applications;database;user study;database schema;application logic;data driven;automatically generated;web browser;application developers;social networking;application development
web pages;language identification;classifier;random sample;search engine results;web crawl;web page;web page;machine learning algorithms;web search engines
query optimization;search algorithms;plans;data dependencies;multi-core;data structure called;query execution plan;cost-based query optimization;dynamic programming;commercial systems;query plans;join queries
set similarity;low accuracy;space overhead;selection queries;cosine similarity;tf/idf;similarity queries;random sampling;query sensitive;estimation techniques;similarity measures
real data sets;computational methods;22;approximate query processing;distributed databases;original data;data streams;massive data sets
instance;minimum set of;instances;information integration;fundamental problem;mapping systems
schema mapping;community-based;community members;information integration;structured information;data integration
data exchange problem;mapping language;data exchange;visual interface;instance;data exchange
ordering constraints;query evaluation;order-preserving;stream-processing systems;query operators;data streams;input streams;stream systems;memory consumption
running times;optimal matching;naïve;query engines;xml data;real-world;streaming xml data;incoming data;pattern queries;pruning techniques;relational data
high-speed;window queries;streaming applications;window size;data model;sliding-window;query processing;sensor network;unified framework;real data;uncertain data streams;sliding-window;data generated from
approximation algorithms;approximation techniques;confidence values;database;search techniques;probabilistic database;closely related;davis-putnam;probabilistic databases;constraint satisfaction;decomposition methods;answering queries;query processing;np-hard problem;application scenarios;additional information
data objects;query evaluation;nearest neighbor;synthetic data;real-life applications;efficient search;uncertain databases;threshold queries;feature extraction;query semantics;trade-offs;marginal probability;nearest neighbors;query type;query point;data management
database systems;probabilistic models;large amounts of data;probabilistic inference;operator;human behavior;probabilistic data;sensory data;machine-learning;complex data;relational models;spatio/temporal;real-world applications;instance;data repositories;statistical model;probabilistic graphical models;management architecture;database;uncertain data;probabilistic information;query processing;correlation patterns;statistical models
type information;type inference;type checking;semi-structured data;np-complete;directed acyclic graphs;business processes;input/output;query languages;type inference;execution traces
synthetic data;computation costs;feature-based;subgraph isomorphism;np-hard
integrity constraint;np-complete;real data;static analysis;large data sets;greedy algorithm;desirable properties;functional dependencies;functional dependency
complexity bounds;relational algebra;upper bounds;data sources;views defined;algorithms for computing;functional dependencies;data integration
query rewriting;sponsored search;user queries;7;link analysis;click graph
computational complexity;complete set of;information retrieval;iterative algorithm;desired accuracy;optimization techniques;graph-theoretic;worst case
join results;publish/subscribe systems;large-scale;large number of;selection conditions;network traffic;processing cost
25,35;real-estate;performance degradation;space overhead;index structures;instance;user profiles
cardinality estimates;query rewriting;15;xquery expressions;estimation process;cost-based;cardinality estimation;data flow;estimation techniques;estimation errors
join algorithm;graph-structured xml;hash-based;reachability queries;tree-structured;xml documents;query processing;subgraph queries;query processing techniques
xml documents;xml structure;xml document;automatically generating
query workloads;materialized views;join operations;databases
storage layer;theoretical properties;database systems;database
databases;decision support;replication;log structured;replication;atomicity;database;conventional techniques;analytical models;storage engine;tree based;log-structured;random access;database replicas;high-throughput;processing queries;data layout;index scans;transaction processing
world-wide web;classification techniques;high-quality;schema elements;database;schema matching;general-purpose;graph traversal;search-engine;improving search;keyword search over;web crawl;databases;automatically-generated;html tables;search engine;structured data;huge number of;relational data;relational table
web applications;database;query result caching;performance gains;publish/subscribe system;query result;highly scalable;hit rate
user-oriented;application integration;systems support;return results;search" services;database;low-cost;cross-domain;general-purpose;multi-domain;web services;query plans;search services;plans
open-source;22;distributed stream processing;18;stream processing;9;processing nodes;2;fault-tolerance;fault-tolerant;memory resources;resource utilization
false dismissal;synthetic data;knn queries;multi-resolution;wide range;level-wise;level-wise;nearest neighbors;distributed streams
data sharing;high throughput;huge amounts of data;tree nodes;1;data structures;load-balancing;open-source;4;scalable distributed;fault-tolerant;internet applications;distributed transactions;fault tolerance;additional information
memory bandwidth;multi-core;main-memory;increasing number of;base-table;business intelligence;working-set;upper bound;concurrent queries;sampling technique
ad-hoc queries;table scan;table scans;evaluation strategy;multi-core
service provider;access pattern;database;database applications;database server;data centers;sample points;database engine;resource allocation
life sciences;schema-free;query optimizer;query language;query processor;data representation;semantic-web;cost model;large-scale datasets;data structures;complex queries;structured information;salient points;pattern matching;excellent performance;pattern-matching;physical-design
digital libraries;document scores;large amounts of;link structure;information retrieval;management systems;search result;enterprise data;keyword search;traditional olap;unstructured data
minimal cost;approximation algorithms;multi-dimensional;local search;text search;baseline algorithm;text queries;search problem;query processing in;retrieval model;query processing;real data;result quality;np-hard
classifier ensemble;automatically extracting;extraction techniques;extraction rules;web crawlers;extraction accuracy;higher accuracy;classifier;web forms
database administrator;database schema;query workloads;query interface;database;query interfaces;human experts;support queries;user queries;databases;query sets
clustering;upper-bounds;similar behavior;collaborative tagging;upper-bound;keyword search;aware search;query results
data values;data collection;sensor monitoring;synthetic datasets;database;data cleaning;uncertain data;probabilistic database;high efficiency;query classes;probabilistic databases;imprecise data;quality metric;location-based services;uncertain objects;query answers;range queries;quality guarantees
query result;provenance information;extracted data;information extraction
service quality;databases;database;probabilistic model;decision tree;large databases;case-based reasoning;distinctive features;automatic generation of
error-prone;historical queries;information systems;rewriting queries;data provenance;database schema;data migration;database;web information systems;mapping composition;scientific databases;schema evolution
data values;query interface;real-world domains;user queries;data sources;data integration systems;semi-automatic;source schemas;data integration
pre-defined;span multiple;life sciences;context-sensitive;ranked queries;specific information;data warehouses;related information;query templates;databases;keyword queries;data provenance;data resources;domain experts;database;specific queries;pose queries;typically rely on;multiple sources;schema mappings;web forms
data set;gene ontology;database;large databases;keeping track of;probabilistic databases;query processing;error bounds
data structure;query evaluation;database management systems;probabilistic graphical model;probabilistic databases;data sources;sensor data;speed-ups;random variable;data characteristics;inference problem
context information;partial information;access control;uncertain data;motivating application;access control;private data;provably optimal;data management
graph structure;private data;graph properties;anonymization techniques;meaningful results;tabular data;graph data;bipartite graph;ad hoc
medical applications;data publishing;real data;theoretical results;privacy preserving;dynamic databases;sensitive values;privacy preserving
database;query relaxation;aggregate queries;real data;individual privacy;limited number of;query result;personal information
historical data;access method;index terms;database;storage space;database engine;index pages;query performance;immortal db;concurrency control;indexing technique;microsoft sql server
error-prone;schema evolution;database;real-world;constraint language;web information systems;historical queries;databases;relational database;synthetic data
false positives;distance functions;synthetic datasets;database;answer set;spatio-temporal;feature extraction;time series;estimation error;general case;similarity queries;time-series;distance computations
real-life data sets;keyword queries;high quality;database;basic algorithm;database queries;dynamic programming algorithm;natural languages;keyword query;search queries;query processing;databases;keyword query;quality metric;query words
xml data;scientific applications;xml keyword search;search strategies;keyword search;user-friendly;query results;query keywords
pattern recognition;large-scale;similarity join;real datasets;parameter settings;edit distance;lower bounds;wide range;filtering methods;distance constraints;similarity joins;similarity join;data integration
data analysis tasks;ad-hoc;extraction task;real datasets;extraction techniques;large document collections;text collections;entity extraction
scheduling policies;compared with conventional;sharing opportunities;map-reduce;performance gains;data processing;data files;scheduling problem
large sample;memory footprint;sensor readings;biased samples;data updates;random samples;random sample;computing devices;data stream;flash storage;fundamental problem;unique characteristics;perform poorly;energy-efficient;data management;data storage
data structure;query processing strategies;randomized algorithm;data points;maintenance cost;query workload;large number of;time series;time series;data set;real world datasets;statistical tests;join queries;model building
load distribution;business process;service-oriented;business processes;composite service;fault tolerance
rdf data;synthetic data sets;storage scheme;relational-database;real-world;semantic web;indexing scheme;data model;instance;semantic web;general-purpose;worst-case;data management;query processing
distance computation;indexing schemes;query results;nearest neighbor;computation costs;data structure called;index structure;spatial queries;road network;search region;databases;query result;knn) queries;real world data sets
storage requirements;pre-processing;user preferences;evaluation techniques;tree search
parameter values;taking into account;query workload;data items;compressed representation;query-dependent;query results
search space;rknn queries;algorithm named;nearest-neighbor;algorithm called;nearest-neighbor queries;location data;query result;nearest neighbors;search region
result sets;density-based;real-life applications;real-world;mobile devices;trajectory data;moving objects;databases;refinement step;data management
clustering;lower-level;discriminative features;high-quality;classification;trajectory data;higher-level;movement patterns;region-based;real-world applications;model construction;feature generation;moving objects;effective classification;class labels;high classification accuracy
high potential;data objects;search space;nearest neighbor;query answer;database;computation costs;knn queries;region-based;region based;query processing cost;query type;query-dependent;query point;nearest neighbors
naive implementation;database operations;poor performance;rewritten query;real-world applications;multiple times;database access;stored procedures;error prone;user-defined functions;special case
parameter space;query execution;base relations;optimizer;plans;response times;query templates;database;visualization tool;plan choices;query optimizers;query processing
query execution;plans;query optimizer;execution plan;plan quality;operator;real world;microsoft sql server;estimation errors
query optimizations;sensor networks;data model;data management;overlay networks
data-driven;data values;discovery algorithm;database;real datasets;large number of;context-dependent;data quality;instance;domain constraints;data consistency;business rules;functional dependencies;data quality;application code;decision making
pair-wise;pruning strategy;search space;mutual information;binary data;real-life datasets;correlated features;lower bounds;strongly correlated;feature subsets;feature subset;feature subsets;high order
real data sets;keyword queries;search algorithms;external memory;keyword search on;graph structured data;keyword search on;graph representation
hierarchical data;external memory;applications including;database;scientific data;disk accesses;hierarchical structure;scalable solution;theoretical bounds
search space;attack detection;web sites;data mining problem;multidimensional data;data management;market segments
database server;data centers;power consumption;energy efficiency;computing power;transaction processing;working group;transaction processing
search space;web search;database;text search;large number of;deep web;html pages;web content;web crawl;search engine;structured data;highly scalable
xml data;customer relationship management;data source;duplicate detection;database;large scale;real-world;multiple representations;large volumes of;comprehensive evaluation;relational database;data mining;personal information management;duplicate detection
data analysis;programming model;parallel processing;parallel execution;click streams;allowing users to;massive data sets;parallel execution;search logs;plans
large numbers of;web applications;database;massively parallel;geographically distributed;replication;load-balancing;data storage
answer queries;microsoft sql server;schema evolution
atomicity;database management system;data feed;durability;semi-structured;highly scalable;rates;data volumes;storage systems;databases;management architecture;relational database management system;data management systems;unstructured data;relational data;database management systems
excellent scalability;algorithm performs;memory bandwidth;multi-core;database applications;analytical models;high-throughput;compares favorably with;multi-threaded
parameter space;high-dimensional;approximation techniques;optimizer;brute-force;query optimizer;nearest-neighbor;sql query;query templates;optimal plan;visualization tool;fine-grained;parametric query optimization;plan choices;representative set;inference techniques;plans
query optimization;data compression;data warehouse;ad-hoc queries;decision support;databases;column-oriented;data storage
execution plans for;optimizer;database systems;query optimizer;optimal plan;tightly integrated with;optimizer;optimal performance;resource utilization;plans
sql/xml;theoretical framework;relational algebra;xquery engine;xml data;xml view;xquery engines;cost based;xml storage;object relational;relational data;plans
optimizer;query optimizer;database;intermediate results;space utilization;valuable information;execution plan;cost model;join order;access path;query processing;execution engine;execution plans
partial orders;operator;execution model
xml search;ranking schemes;search engine;keyword search;query results;web data
data independence;database;sql server;object-relational;xml data;object-based;data model;entity framework;object-oriented;vice-versa;query translation;xml schema
xquery expressions;graph model;rule-based;database;plan generation;processing algorithms;wide range;execution plans;native xml;configuration parameters
query execution;biological networks;query operations;database;indexing techniques;emerging applications;large graph;graph datasets;real life;nodes represent;social networks;graph data;graph matching;life science;social networking;application domains
search results;graph-structured xml;query results;data cube;data exploration;document structure;xml data;xml repositories;algorithm to compute;data structures;information embedded in;xml data;keyword search;user feedback;xml search;user interaction;query terms
event logs;workflow management systems;business processes;management systems;business process;business processes
large number of;data management system;theoretical properties
total order;hash table;dynamic behavior
keyword queries;efficient retrieval;document collection;global search;overlay network;multimedia) documents;computing resources;text retrieval;bandwidth consumption
semantic annotation;service calls;optimizer;web services;large-scale;text search;xml query;xml documents;web data;data processing;8;web data
data services;large scale;heterogeneous data sources;distributed applications;general-purpose;multiple data sources;query processing;distributed data;key constraints
recommendation engine;service provider;real-world;history data;social network;management systems
false positives;high degree of;query processor;mobile devices;duplicate elimination;search engines;search engine;application logic;automatically identifying
clustering;singular value decomposition;comparative analysis;summarization methods;document-summarization
user queries;data collection;online communities;text queries;relevant documents
keyword queries;keyword-based search;semi-structured;data sources;search effectiveness;search engine;ranking mechanism;keyword search;structured data
database;link structure;search result;keyword search over;keyword search;traditional olap
data exploration;data quality;easy access;functional dependencies;human interaction;relational database system;efficient sql;user-friendly;relational data
community-based;information integration;structured information;data integration;community members
schema mappings;data exchange;schema mappings are;information integration;mapping systems
evolving data;stream processor;large volumes of;ad-hoc;large collections of;highly distributed;data processing;data flow;unstructured data
data analysis;large-scale;data extraction;large-scale;computing environment;data processing;web data
search results;real-world datasets;search space;semantically meaningful;search process;social network;share similar;online communities
data quality;sensitive data;data protection;database
learning curve;sql queries;database applications;automatically generated;decision support systems;visualization methods;database tuning;structured queries
analytical queries;response times;selection problem;view selection;materialized aggregate;optimal set of
10;database;main memory;stored procedures;databases;optimal performance;transaction processing
computational geometry;federated database;data analysis;motivating application;region based;data sets;query processing techniques
algorithm performs;protein structure;high throughput;database;databases;drug design;similarity search;high precision and recall;protein structures;algorithm takes;translation invariant;exhaustive search
multi-modal;proximity measures;random walk;query response time;database;databases;principal component analysis;online databases;gene expression;feature extraction methods;biological data
business activities;business process;business process;business processes;xml-based;application server;distributed environment
semantic mappings;semantic relationships;data management;semantic integration;integration systems
database technologies;information systems;database technology;database instances;real-world;ontology-based;source database;logic-based;conceptual model;information access;databases;data intensive applications;database researchers;data intensive
monte carlo;sql queries;event processing;probabilistic databases;probabilistic data;1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26;probability theory;error bounds;data management
data cleaning;data dependencies;constraint-based
xml retrieval;database;structural summaries;information retrieval;distributed environments;query processing;search tasks
data stream management system;multiple continuous queries;performance goals;data stream management systems;continuous queries
naïve;correlation clustering;subspace clustering;pattern-based clustering;data mining;dimensional data;ad hoc
frequent items;data stream mining;large scale;rates;high accuracy;data streams;modern hardware
time series;application domains;distance measures;data sets;dimensionality reduction;similarity measures
rdf data;open source;1;semantic web;data management;vertically-partitioned;data management;code base
xml based;knowledge representation;network routing;labeling schemes;classification
query efficiency;indexing techniques;database;moving object;moving objects;storage requirements;concurrency control;moving-object
response times;multi-dimensional;query processing strategies;business data;index structure;26;query response times;large data sets;rates;index compression;online-analytical processing
video databases;video search;large databases;content features;similarity search;similarity queries;visual information;efficient similarity search;video retrieval
retrieval model;video retrieval;user profiles;implicit feedback
software development;data mining techniques;pattern mining;rule mining;case studies;sequence database;mining patterns;execution traces;high cost
database applications;main-memory;database research;frequent updates;tree index;dimensional data;real world;index structures;short-term
xml database;xml storage;database technology;14;relational database system;database;semi-structured;query interface;query capabilities;18, 5, 6, 16;data stored in;4;data processing;5;data type;query results;operating systems;data storage
workflow management systems;scheduling algorithm;multi-stage;application domains;instances;business processes;wireless networks;resource allocation
search results;small groups;retrieval algorithms;unstructured information;access control;index structures;relevant documents;index structure;information retrieval;efficient indexing;privacy preserving;user access;distributed environment
network security;large-scale;network monitoring;data stream;monitoring systems;log data;data integration
data values;database;information integration;information integration;query answering;information integration
web-based;web applications;partial information;web-based;web application;theoretical foundations;execution traces
xml-document;document-filtering;database;xml documents;preprocessing phase;filtering algorithm;publish-subscribe system;pattern-matching;protein-sequence
query patterns;data partitioning;data skew;replication;domain-specific;data volumes;long-term;data characteristics;short-term;data management;load balancing
copyright © 2008 john wiley & sons;database;replication;data access;digital libraries;distributed data
copyright © 2008 john wiley & sons;security requirements;replication;sensitive data;replication;control mechanisms
data grid;semi-structured data;copyright © 2008 john wiley & sons;data sources;information integration;dynamic environment;metadata management
copyright © 2008 john wiley & sons;database management systems;database;dynamic environments;statistical analysis;data processing;database systems;load balancing
query execution;database;database replication;intra-query;grid services;olap queries;query processing;web services;database cluster;copyright © 2008 john wiley & sons
copyright © 2008 john wiley & sons;object-based;file systems;storage systems;grid environment;data management systems
copyright © 2008 john wiley & sons;data transfer
copyright © 2008 john wiley & sons;high-quality;database management systems;distributed applications;high-cost;data management
computing infrastructure;data management techniques;database performance;data management;international workshop on
real-world datasets;attribute values;data source;synthetic data;sample size;database size;high quality;streaming data;high-quality;sliding windows
interaction data;real data sets;customer segmentation;expression patterns;gene expression data;biological processes;complete set of;mining frequent;multiple datasets;mining frequent;social network analysis;web mining;data mining problem;gene expression;data mining methods
clustering;high-dimensional;clustering results;cluster ensembles;real datasets;cluster ensembles;input space;text data;ensemble methods;combining multiple
expected number of;multidimensional space;game theory;game theory;data mining
data services;programming languages;database research;usage scenarios;virtual worlds;database engine;unstructured data;database researchers
acm sigmod;8;3;correct answer;query evaluation
data sharing;data sharing;life sciences;structured data;instance;frequently updated

database
ir-style;database systems;information retrieval
search engines;world wide web;cognitive science;search engine
online users;information retrieval systems
information retrieval;web site;information retrieval
tensor product;document retrieval;natural language processing;data analysis;information retrieval
web graph;temporal information;highly compressed;compression techniques;link structure;link-based;temporal evolution;ad hoc;online communities
open-source;web graph;highly compressed;text-based;machine learning;data set;3;information retrieval techniques
information retrieval
sigir 2008 workshop on;sigir workshop on;focused retrieval;sigir 2008 workshop on;focused retrieval
speech recognition;spoken content;user interfaces;search applications;user interface;application domains;information retrieval;large scale;speech retrieval;wide range;video content;spoken content;content providers;retrieval systems
search results;emerging area;prototype systems;aggregated search;aggregated search;user studies
ir-style;database systems;information retrieval
information access;information access;cultural heritage;information access;wide range;poster session;digital library;cultural heritage
personal information management;personal information management;personal information management;information access
emerging area;document retrieval;information extraction;retrieval tasks;information retrieval
human effort;low-cost;expected cost;relevance judgments;human judgments;large corpora;ranking performance;retrieval engine;information retrieval;information retrieval systems
customer relationship management;opinion mining;information retrieval;opinion mining;automatic generation of
search systems;digital information;large sets of;information retrieval;information seeking;takes place;retrieve information
search services;search interface;web data;databases
search engine;time series;linear regression;query frequency;query log;commercial products
mining algorithm;spatial data;object-relational;spatial objects;evaluation scheme;spatial database;nearest neighbor;pre-processing
clustering algorithms;distance measure;spatio-temporal;data elements;distance measures;data sets;temporal data;spatio-temporal
geographic regions;automatically learns;ground truth;labeled samples;classification;semi-supervised learning;learning algorithm;large number of;unlabeled samples;supervised classification;learning scheme;image classification;semi-supervised
high-speed;transliteration;unstructured text;commercial applications;unstructured text;data streams
algorithm generates;decision trees;pattern recognition;classification;distributed databases;learning algorithm;tree-based;data sets;training data;counterpart;distributed settings;tree induction;chi-square;decision trees;data mining;completeness;continuous data;stored data;distributed environment
ensemble learning;clustering algorithms;mining algorithms;grid services;data mining models;data sources;learning models;data mining models
typical patterns;service oriented;data mining applications;data mining;data mining tasks;grid services;concurrent execution;data mining
main-memory;data mining;business intelligence;case studies;highly distributed;main features;data mining;data mining algorithms
high-speed;high speed data streams;large-volume;closed frequent itemsets;stream mining;streaming data;closed frequent itemsets;high speed data streams;sliding windows
clustering;clustering;data mining technique;data partitioning;hierarchical clustering;data clustering;market basket data;market-basket data;clustering approach
distance computation;distance computation;main memory;classification;sql queries;sql queries;data set size;computationally intensive;data set;large data sets;data mining techniques;distance computation;query optimizations;user-defined functions;clustering algorithm;data mining algorithms;clustering
tree) structure;pattern tree;maintenance algorithm;sequential patterns;maintenance algorithm
meaningful clusters;feature space;microarray data;data points;high dimensional;clustering problems;noisy datasets;large number of;microarray data analysis;noisy datasets;desirable properties;clustering problems;clustering algorithm;real life applications;clustering
text mining;classification;inductive inference;data mining;data mining;text mining techniques
false positives;data mining problems;false positive;false positive;high dimensional;data mining applications;data mining tasks;rates;true positive;rates;error rate;higher dimensional
pre-filtering;active learning;active learning;filtering technique;error-reduction;filtering techniques;computationally expensive;filtering technique;error-reduction
feature space;parameter-free;classification techniques;classification;based reasoning;instance;associative classifiers;fitness;associative classifiers;class association rules;rule based;mining algorithm;instances;association rule
distributed algorithm for;resource management;data fusion;large scale;linear programming;access control;resource management;large volumes of data;data management;distributed environments;simulated data;compute nodes;data mining;optimization problems;distributed environments;dynamic nature of;distributed systems
matrix factorization;recommendation systems;recommender systems;prediction accuracy;test instances;real-life;matrix factorization;matrix factorization methods
learning algorithms;unlabeled data;data mining applications;learning algorithm;application domains;real-world;single-view;semi-supervised;supervised learning;learning framework;semi-supervised
rule extraction from;ordered information;ordered information;ordered information
database;sequential pattern mining;support threshold;sequential patterns;algorithm called;databases;association rules;mining algorithm;sequential pattern mining;sequential pattern
apriori based;association rule;common knowledge;transaction database;data mining
multi-step;real-world;data mining;learning systems;classifier;data mining;machine learning techniques;benchmark datasets;classification problems;data mining problems;classifier
graph-based;clustering documents;document clustering;related algorithms;document collections;similarity graph
classification problem;classification;class label;prior knowledge;traditional classification;instances;classification problem;unlabeled set;classification algorithms
clustering;local patterns;inductive database;data mining methods;classification;case study;constraint-based mining;constraint-based;constraint-based mining;local optimization;model construction;knowledge discovery in databases;clustering tasks;data mining;pattern discovery;sequential pattern mining;constraint-based mining
multiple views;text classification;learning algorithms;active learning;semantic features;multi-view;active learning;semantic features;multi-view;multi-view learning;integrating information from;semi-supervised learning;canonical correlation analysis;basis vectors;text classification;semi-supervised;classifier training
ontology-based;relation extraction;graphical model;annotated data;named entity;local features;local information;information extraction;statistical learning;natural language understanding
web pages;extraction algorithm;keyword extraction;web pages;domain-specific;semantic content;web page;natural language processing;statistical model;related words
distance function;data mining;data mining;data mining task;constraint-based;mining algorithm
association rules;association rules
clustering;support levels;minimum support threshold;binary data;frequent itemsets;association rule mining;efficient computation;large number of;data set size;itemset support;data sets;association rules;machine learning techniques;cluster model;association rule;sufficient statistics;clustering model;association rule
clustering;complete set of;classification rules;real-world data mining applications;knowledge discovery;association rules
association rule mining;data model;data model;instance;data tables;rough set theory;data mining problems
real-world;detection method;time series;lower level;detection algorithm;multi-resolution;wavelet transformation;wavelet transformation

semantic annotation;semantic annotation;software tools;web service;service-oriented;web services
data sources;syntactic structure;web sources
contextual information;contextual information;predictive accuracy;user profiling;customer behavior
textual data;semantic analysis;unstructured data;classification;unstructured data
segmentation results;contextual information;wide range;contextual information;customer behavior
clustering;independent variables;predictive models;predictive modeling;large scale;predictive modeling;input space;data matrix;instance;making predictions;response variable

data mining;data visualization;dynamic environment
data describing;knowledge-discovery;stream data;stream data;prediction accuracy
user-profile;social network;social network data;rates;online advertising;ensemble classifier;social networks
training data;graph model;concept learning;concept learning;concept detection;video retrieval;rich information
desired properties;search algorithm
database;linear discriminant analysis;classification methods;human action recognition;low dimensional;feature vectors
clustering;clustering;video sequence;classification;clustering algorithms;multi-view;face image;face recognition system;clustering result;multi-view
quality assessment;summarization techniques;evaluation techniques;quality assessment;similarity based;data management
relevance feedback;search algorithm for;database;probabilistic model;real data;algorithm maintains;user feedback;relevance feedback;simulated data;takes into account;content-based image retrieval
real data sets;false positive;anomaly detection;probabilistic representation;true positive;rates;detect anomalies
long-term;high-speed
public data;temporal information;information contained in;large scale;web graphs;statistical analysis;temporal evolution
social media;network motifs;blog posts
compression-based;biological networks;frequent subgraph mining;graph-based;temporal patterns;rewriting rules;subgraph mining;compression-based;structural properties;dynamic networks;graph-based data mining
real-world networks;social interactions
high-dimensional;machine-learning applications;pattern recognition;data points;real datasets;noisy data;neighborhood graph;graph construction;dimensional data;dimensionality reduction
response times;basic operations;text search;text queries;web application;text search;index size;search engine;semantic information
approximate matching;database size;query pattern;visual query;social network;large graphs;fast algorithms;high speed
rule evaluation models;rule evaluation models;valuable knowledge;classification;accuracies;mined patterns;learning algorithms;post-processing;human experts;rule sets;accuracies;objective rule;class distribution;class distributions
discovered patterns;continuous domains;statistical tests;on-line analytical processing;association rules;association rules;association rules;statistical tests
domain ontologies;interactive exploration;textual data;named entities;extracted data;data layout;unstructured data;higher level
genetic programming;data mining tool;decision trees;mining framework;general purpose;classification;optimization methods;data mining;fuzzy-rules;regression models;data mining
data manipulation;test cases;information extraction;large data sets;remote sensing data;spatial data mining;spatial data mining
clustering;directed graph;clustering approach;news items
service provider;service provider;data visualization;data transformation;web services;data mining;pre-processing;workflow engine;knowledge discovery process
mining algorithms;online mining;data stream mining;data structures;definition language;data stream mining;data streams
classification;genetic algorithm;optimal combination of;raw data;data mining;wavelet transformation
privacy-preserving;data perturbation;real-life datasets;data mining applications;original data;statistical properties;homeland security;data mining technologies;data privacy;wavelet-based;privacy-preserving;data mining;preserving privacy
learning systems;sensitivity analysis;decision support;data streams;data streams
cost sensitive learning;cost sensitive learning;classification;data mining techniques;data mining and machine learning;sampling techniques;application domains;lower cost
decision trees;classification;decision-tree;decision trees;classification task;classifier
factors affecting;case-based reasoning;case-based reasoning;case-based reasoning;problem solving
classification;case study;low quality;case study;3;noisy data;classification approach;learning strategy;classifier;discovered knowledge
case studies;social security;user preferences;data mining techniques;real-world;domain-driven;decision-making;pattern mining;real-world;domain driven data mining;concept map;data mining;real world;data mining algorithms;actionable knowledge
parameter space;parameter values;constraint-based mining;domain driven;parameter settings;parameter tuning;pattern discovery;mining tasks
pattern analysis;behavior patterns;business intelligence;behavior analysis;problem-solving;online communities;transactional data
knapsack problem;historical data;automatically discover;feature selection methods
utility function;target domain;rank learning;labeled training data;ranking svm;cross-domain;ranking function;learning” problem;source domain to;algorithm called;labeled data;real-world;labeled data from;benchmark datasets;source domain
large volume;classification;classification;text streams;streaming data;user interests;window-based;labelled data;data distribution;ensemble approach;high speed
domain knowledge;domain ontologies;database;domain ontology;post-processing;post-processing;discovered association rules;data mining;association rules;discovered rules
ensemble learning;data collected by;feature set;predictive features;sales prediction;dynamic integration;sales prediction;sales prediction;classifier
large number of;redundant information;sequential patterns;false alarms
multi-dimensional;taking into account;knowledge discovery;clustering technique;graph-based;domain-driven;data mining;multi-dimensional;knowledge discovery;domain-specific;application domain;main components;data mining;data mining process;actionable knowledge;real-world
domain knowledge;decision boundary;linear discriminant analysis;real dataset;fault detection;identification problem;real-world;identification method;semi-supervised;semi-supervised
clustering;spectral clustering;clustering algorithm based on;spectral embedding
transductive learning;classification decisions;real world datasets;text categorization;text categorization;transductive learning;learning framework
graph structures;graph mining;feature construction;kernel based;pattern mining;graph kernels;classification performance;instances;main idea;structured data;prediction tasks
real-world datasets;multi-dimensional;euclidean distance between;instance-based learning;learning systems;representation language;machine learning;nearest neighbor classification;distance-based;multiple instance;nearest neighbor classification
clustering;labeled samples;synthetic datasets;semi-supervised clustering;clustering algorithm;semi-supervised;semi-supervised
synthetic data sets;density estimation;database;temporal sequence;temporal sequences;discovering frequent;temporal information;temporal patterns;extracted knowledge;temporal patterns
amino acids;partition-based;partition-based;selected features;associative classifier;classification method;protein sequences
clustering;context information;monitoring applications;general purpose;classification;clustering algorithms;takes into account;stream processing;user-defined;distance measure;data model;monitoring applications;clustering algorithm;stream processing;context information;data management systems;synthetic data
domain knowledge;mining task;original data;data mining approach
clustering;distance measure;density-based;clustering methods;categorical attributes;clustering methods;data mining;numerical attributes;measure---called
concise representations;frequent patterns;web information retrieval;word senses;information retrieval;related algorithms;search quality
structured databases;real world datasets;structured data;correlation mining;application domains;structured databases;data mining problems
action rules;action rules;classification rules;action rules;rule discovery;action sets
real-valued;regression algorithm;regression model;multiple-instance;remote sensing data;multiple-instance;multiple-instance;structured data;internal structure
meaningful patterns;synthetic datasets;frequent patterns;algorithm named;highly correlated;structured data;graph databases
clustering;speech recognition;automatic indexing;database;automatic indexing;features including;classification tree;source separation;training datasets;audio features;clustering
data collection;software tools;data generation;simulation model;real-world;spatio-temporal;movement patterns;simulation framework;modern hardware;spatio-temporal
inductive database;spatial objects;database technology;knowledge discovery;inductive) inference;inductive database;inductive database;spatial database;knowledge discovery;database;answer queries;prior knowledge
decision support systems;spatial data;irrelevant information;data warehouse
clustering;clustering algorithm;machine learning algorithms;statistical analysis
detecting anomalies;graph-based;kernel matrices;medical diagnosis;graph-based;traffic monitoring;time series;case study;data mining task;time series
support vector machines;real-life;support vector machines;simulated data
sensor readings;data exploration;sensor data streams;context sensitive;fine-grained;environmental monitoring;sensor networks;sensor data streams;real world
case study;long-range;data mining;data miners;spatiotemporal data;knowledge discovery;data mining
regular expressions;regular expressions;sequential pattern mining;data model;mining process;frequent sequences;databases;sequential pattern mining
incremental maintenance;spatial data;incremental maintenance;discrete data
query nodes;query evaluation;optimization method;query language;query processing;database functionality;large number of
discriminative features;feature set;classification;discriminative patterns
bayesian network;training data;bayesian network learning;learning algorithm;large number of;spatial context;instance;parameter settings;bayesian networks;random walks
regression analysis;united states;remote sensing;remote sensing
success rate;simulation experiments
segmentation algorithm
clustering;meaningful clusters;historical data;data points;clustering algorithms;database;categorical attributes;generation process;databases;domain knowledge;similarity measure;data items;database;data records;association rules;traditional clustering algorithms;making predictions;distance metric;future events;clustering algorithms;clustering
classification;unsupervised learning;classification;large-scale;dimension reduction;svm-based;vector machine;classifiers trained on;original data;principal component analysis;dimensional data;support vector machines;dimension reduction
time-series;number of segment-types is;bayesian approach to a segmentation model;object classification;fundamental problem
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
desired information;flow fields
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
neural networks
classification;decision function;machine learning methods;microarray data;vector machine;large number of;parameter optimization;feature selection;sparse set of;classification;gene selection;gene selection
number of segment-types is;neural networks;classification;bayesian approach to a segmentation model;fundamental problem;time-series
number of segment-types is;computationally intractable;state-space;bayesian approach to a segmentation model;time-series;state-space;variational approximation;bayesian approach;fundamental problem;time-series;inference algorithms
number of segment-types is;bayesian network;plans;bayesian approach to a segmentation model;fundamental problem;time-series;classifier
database;exemplar-based;temporal behavior;facial expression recognition;probability estimation;prior knowledge;facial expression recognition;bayesian networks;exemplar-based;exemplar-based
interval based;optimization problem;learning speed;bayesian learning;existing knowledge;random sampling
time-series;number of segment-types is;bayesian approach to a segmentation model;reproducing kernel hilbert space;fundamental problem
time-series;number of segment-types is;bayesian approach to a segmentation model;classifier;fundamental problem
learning process;search space;loss function;loss function;decision rule;regularization;decision rule;multiclass problems;general problem;artificial data sets;benchmark data sets;learning method
feature space;pre-processing;active learning;feature space;real-world;manifold learning;learning tasks;machine learning;text classification;high dimensionality;support vector machines;learning strategy
number of segment-types is;classification;bayesian approach to a segmentation model;vector machine;fundamental problem;time-series
domain knowledge;match scores;rule-based;information fusion;manual tuning;data records;high accuracy;real-world data sets;rule-based
action) space;input data;basis functions;reinforcement learning;reinforcement learning;common practice;basis function
number of segment-types is;partially observable;bayesian approach to a segmentation model;time-series;predictive model for;fundamental problem;imitation learning
classification;multi-class;training examples;inverted pendulum;learning problems;reinforcement-learning;reinforcement learning;control problem;classification problem;learning problem;internal structure;classification problems;optimal policies;classifier
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
local search;random variables;markov network;markov networks;joint probability distribution;algorithms produce;greedy algorithm;selection algorithm
greedy approach;classification;classification;ad-hoc;large number of;greedy approach;high accuracy
classification trees;accurate classifiers;calibration method;expected utility;squared error;poor quality;probability estimation;class probabilities
selection criterion;multi-criteria;test set;diversity measures;diversity measures
learning algorithms;learning algorithms;predictive accuracy;classifier
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
number of segment-types is;relevant features;high dimensional;bayesian approach to a segmentation model;noisy data;fundamental problem;time-series
number of segment-types is;sampling techniques;imbalanced data;low quality;fundamental problem;time-series;bayesian approach to a segmentation model
error-rate;kernel density;loss function;learning algorithm;classifier;data sizes;gradient descent;mixture model;kernel density estimation;machine learning;classifier;multi-class classification problems
database;classification error;cost function;3;automatic speech recognition;statistical pattern recognition;recognition tasks;8;cost function;automatic speech recognition;decision theory;4
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
mutual information;classification;generalization performance;learning paradigm;learning algorithm;learning paradigm;target variables;cost function
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
text mining;face recognition;test data;dimensionality reduction method;graph-based;reduction techniques;information retrieval;data set;nearest-neighbor;algorithm exploits;latent semantic indexing;dimension reduction;dimensionality reduction
time-series;number of segment-types is;bayesian approach to a segmentation model;unsupervised learning;fundamental problem
clustering;clustering algorithms;similarity measure;similarity measure;clustering task;data matrix;clustering task;clustering methods
consensus clustering;computational complexity;consensus clustering;unsupervised clustering;optimization problem;data set;data sets;real data sets;clustering result;combining multiple;clustering algorithms
time-series;number of segment-types is;bayesian approach to a segmentation model;continuous data;fundamental problem
clustering;approximate algorithm;clustering method;euclidean space;computational cost;clustering quality;integer programming;np-hard

learning problem;image segmentation;image segmentation
tracking algorithm;scale invariant feature transform
spatial neighborhood;image sequence;dynamic bayesian network;optical flow;local structure;inference techniques
video sequences;color histogram;color information
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
false positives;decision trees;exhaustive search;genetic algorithm;naive bayes;cost sensitive;classification methods;cost-sensitive;credit card fraud;neural nets;cost sensitive;credit card fraud;training phase
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
large networks;minimum set of;multi-objective;genetic algorithm
forecasting accuracy;multi-layer;real data;linear regression
number of segment-types is;improving accuracy;bayesian approach to a segmentation model;case-based reasoning;fundamental problem;time-series
time-series;number of segment-types is;bayesian approach to a segmentation model;unstructured data;fundamental problem
'orgchart';transfer learning
wikipedia articles;matrix factorization;prediction accuracy;hyperlink structure;nearest
ubiquitous computing;video data;supervision;loopy belief propagation;loopy belief propagation;sensor networks;local information;semantic representation;sensor networks
optimization strategies;knowledge discovery;case-study;large amounts of data;data mining;mining algorithm;classification task;distributed systems
simulation results;control algorithm;adaptive control;operating conditions;neural network;neural networks
human intervention;fully automatic;natural language processing
time-series;object-oriented;bayesian approach to a segmentation model;fundamental problem;number of segment-types is
classification problem;privacy concerns;recognition accuracy;sensor nodes;error rate;classifier;detection task
simulation data;data analysis;classification;high quality;fuzzy clustering;minimum spanning tree;multivariate data;high density;key features;times higher
clustering;clustering;data analysis;exploratory analysis;high dimensional;principal component analysis;machine learning;case study;statistical analysis;multivariate data;databases;maximum variance
time-series;number of segment-types is;stochastic games;bayesian approach to a segmentation model;fundamental problem
algorithm parameters;multi-dimensional;application area;recommender systems;multi-stage;recommendation process;linear algebra;linear systems
probabilistic approach;knapsack problem;greedy approach;learning strategy;knapsack problem
factor analysis;supervised learning;model assumes;partial knowledge;prior knowledge;estimation accuracy;semi-supervised;prior knowledge;independent components;semi-supervised
gaussian mixture model;private data;data set;direct access to;large amounts of data;model estimation
number of segment-types is;classification;bayesian approach to a segmentation model;fundamental problem;time-series;feature spaces
hybrid approach;classification;number of segment-types is;bayesian approach to a segmentation model;fundamental problem;time-series
ct images;segmentation methods;classification;feature extraction;segmentation method;artificial neural network;artificial neural network
margins;plans;neural networks;neural networks
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem;machine learning techniques
time-series;number of segment-types is;bayesian approach to a segmentation model;kernel-based;fundamental problem
machine learning models;decision trees;neural networks;fold cross-validation;great promise for;data set;complementary information;machine learning;machine learning models;support vector machines
classification problem;verification problem;classification accuracy;learning algorithm;training samples;training samples;high precision;principal component analysis;images acquired;artificial neural network
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
number of segment-types is;decision trees;genetic algorithm;bayesian approach to a segmentation model;fundamental problem;time-series
memory usage;database;suffix tree;markov model
multi-class;large-scale;multi-kernel;training samples;feature spaces;sparse kernel;classification problems;low-dimensional;expectation-maximization
training data;microarray data;classification;rank-based;binary classifier;classification;gene expression;classifier

clustering;number of segment-types is;bayesian approach to a segmentation model;microarray data analysis;graph cut;fundamental problem;time-series
number of segment-types is;auc;bayesian approach to a segmentation model;gene interactions;fundamental problem;time-series
survival analysis;test data;sample size;neural network model;neural network model;regression model;takes into account
number of segment-types is;classification;database;bayesian approach to a segmentation model;fundamental problem;time-series;dimensionality reduction
classification;classification;breast cancer;naive bayes;multilayer perceptron;naive bayes classifier;machine learning techniques;competitive performance;breast cancer;classifier
feature space;fusion method;extensible framework;information fusion;support vectors;distance based;support vector machines;classifier;decision making
number of segment-types is;classification;distance metric learning;bayesian approach to a segmentation model;fundamental problem;time-series;support vector machines
edge detection;neural networks;classification;hidden nodes;classification performance;neural networks
data set;linear model;missing values;medical datasets;missing data;model averaging;breast cancer;artificial neural network
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
time-series;number of segment-types is;bayesian approach to a segmentation model;highly scalable;fundamental problem
intrusion detection;decision trees;anomaly detection;decision trees;minimum description length;decision trees
11;1;joint distribution;markov-process;joint distribution;discrete cosine transform
prediction problem;number of segment-types is;bayesian approach to a segmentation model;fundamental problem;time-series;data mining algorithms
number of segment-types is;knowledge management;bayesian approach to a segmentation model;fuzzy clustering;fundamental problem;time-series
web based;modeling approach;modeling approach;domain model
clustering;protein structure;secondary structure
time-series;number of segment-types is;bayesian approach to a segmentation model;fundamental problem
computational efficiency;false positives;mutual information;dirichlet prior;higher accuracy;dynamic bayesian network;expression data;gene interactions;time-series;simulated data;dynamic bayesian network;network structure;great promise
time-series;number of segment-types is;bayesian approach to a segmentation model;clustering approach;fundamental problem
ranking documents;related documents;mathematical framework
classification problem;multiple images;rule-based;classification problem;fuzzy rules;fuzzy rules
pattern recognition;classification tasks;multiple tasks;incremental learning;training samples;knowledge acquisition;long-term;pattern recognition;multi-class
time-series;number of segment-types is;bayesian approach to a segmentation model;missing data;fundamental problem
classification;function approximation;test set;neural networks;input data;missing data;missing data;basis function;neural network
learning process;svm algorithm;classification;discriminative power;text classification;tree kernels;linguistic information;syntactic structure;target function;text classification;semantic information;classifier;machine learning approaches
data set;discriminant analysis;kernel matrix;large-scale;large-scale;data set;kernel space;space complexity;iterative algorithm
clustering;learning process;public data;classification problem;classification;classification tasks;learning algorithms;labeled documents;supervised learning algorithm;supervised learning;information theoretic
classification;classification;evaluation metric;imbalanced datasets;simulation results;imbalanced datasets;classifier;model selection
time-series;number of segment-types is;bayesian approach to a segmentation model;text categorization;fundamental problem
number of segment-types is;classification accuracy;bayesian approach to a segmentation model;user profiling;fundamental problem;time-series
face detection;pose estimation;concept learning;learning mechanism;speech recognition
variable selection;time-series;bayesian approach to a segmentation model;fundamental problem;number of segment-types is
time-series;distance measure;bayesian approach to a segmentation model;fundamental problem;number of segment-types is
computational complexity;meta-learning;predictive accuracy;classification task;learning algorithms;meta-level;uci datasets;visual analysis;model selection
data analysis;video stream;memory capacity;machine learning techniques;textual content;mobile devices;bayesian inference;user's preferences;machine learning techniques;multimedia content;analyzing data;multilayer perceptron
time-series;number of segment-types is;bayesian approach to a segmentation model;latent dirichlet allocation;fundamental problem
classification;classification model;maximum entropy;classification;classification accuracy;naïve bayes;gaussian kernels;numerical features;discrete data;unsupervised discretization;comparative analysis;continuous data;classifier
computational efficiency;artificial neural networks;artificial neural network;takes into account
number of segment-types is;bayesian approach to a segmentation model;feature extraction;fundamental problem;time-series;semi-supervised classification
number of segment-types is;large volumes of;bayesian approach to a segmentation model;fundamental problem;time-series;distributed systems
extended kalman filter;neural networks;high dimensional;time series;observed data;gradient descent;low computational cost;kalman filter;recurrent neural network;recurrent neural networks
entropy-based;data set;decision-making;input parameters;decision makers;decision making
time-series;number of segment-types is;bayesian approach to a segmentation model;neural network;fundamental problem
speech recognition;feature set;database;accuracies;classification method;feature selection;canonical correlation analysis;emotion recognition;neural network;irrelevant information;emotion recognition
combination weights;recommendation systems;recommendation systems;data integration;large-scale
classification methods;recognition rates;character recognition;recognition accuracy;document management;svm classifier;feature descriptors;character recognition;inter-class
graph patterns;graph structured data;graph patterns;chemical compounds;mining algorithm
intrusion detection;number of segment-types is;classification framework;bayesian approach to a segmentation model;fundamental problem;time-series
amino acids;training data is;data points;single class;decision boundary;single class;true positive;maximum entropy;classifier performance;9;classifier
number of segment-types is;face recognition;bayesian approach to a segmentation model;iterative algorithm;fundamental problem;time-series
control algorithm;rates;control law;fuzzy rules;simulation results;control parameters
number of segment-types is;bayesian approach to a segmentation model;knowledge acquisition;data driven;fundamental problem;time-series
time-series;number of segment-types is;bayesian approach to a segmentation model;classification;fundamental problem
naïve;clustering high-dimensional data;correlation clustering;subspace clustering;pattern-based clustering;data mining;ad hoc
naive bayes classifier;generalization error;cross-validation;probability distribution;model selection;classification algorithm
closed patterns;real-life;attribute domains;algorithm called;frequent closed;pattern discovery;special case
detection algorithms;search techniques;memory usage;pruning rules;main memory;detection algorithm;similarity search;distance-based;very large datasets;disk-resident
prediction accuracy;databases;database queries;data-mining problem;large number of;accurately predict;learning algorithms;large databases;predictive modeling;great potential;massive datasets;data preparation;database query processing;large datasets;real-world;main memory;cost-effective;sales data;machine learning algorithms;data-collection
web service;user-supplied
search results;semantic indexing;domain ontologies;heuristic techniques;information retrieval tasks;user requests;information retrieval
semantic annotation;human efforts;decision support;product family;case study;semantic tags;digital camera;product information;retrieval tasks;information processing;efficient indexing;semantic web;product design;faceted search;retrieval framework;faceted search;information management;tedious task
recommendation quality;probabilistic latent semantic analysis;collaborative filtering;hybrid approach;collaborative tagging;item recommendation;higher quality;interesting items
entity recognition;named entities;sentence-level;combined method;detection methods;novelty detection
query-independent;social interactions;free-text;ranking documents;query independent
case retrieval;information retrieval
context-aware;context aware;general purpose;web content;mobile devices
semantic annotation;valuable information
multi-modal;bayesian model;lda model;latent dirichlet allocation;mixture model;hierarchical dirichlet process;multiple objects;variational inference;image content;individual objects;hierarchical dirichlet process;model parameters
semantic concept;unsupervised classification;classification;free-text;image analysis;classifier;image classification;image-content;visual analysis;classification algorithm
search results;user experience;object class;search applications;probabilistic graphical model;expectation maximization;machine learning;image search;search engines;visual concepts;search engine results;user intervention;semi-supervised
clustering;video content;semantic video;low-level features;multimedia data mining;level features;information entropy;content analysis;hidden markov models;membership functions;machine learning;multiple modalities;video content;video annotation;resource allocation
world wide web;computation cost;temporal patterns;content-based video;multimedia content;indexing technique;excellent performance;information technology
low-level features;semantic content;multiple queries;rough set theory;retrieval method
computational complexity;video coding;search applications;mobile devices;machine learning;resource constrained;machine learning;resource constrained;computationally intensive;video coding
data exploration;decision-making;data representation;time series;databases;mining tasks
emerging topics;latent dirichlet allocation;modeling framework;text streams;lda) model;topic model;topic models;topic detection;topic model;benchmark datasets;empirical bayes;text streams;interesting patterns
clustering;real-world data mining applications;information filtering;local minimum;cross-domain;multiple datasets;feature sets;clustering framework;novelty detection;objective function;iterative algorithm
real-world data sets;active learner;malware detection
evolving data;decision trees;decision trees;predicting future
real-world datasets;monte carlo;counting algorithm;frequent sets;frequent (item)sets;desired accuracy;markov chain
regression problems;cost-sensitive;linear regression;prediction error;data mining;linear regression
text mining;text mining;free text;image feature;information contained in;feature extraction;image retrieval;medical images;rich information
data objects;clustering;density-based;clustering uncertain data;linkage-based;clustering algorithms;information-theoretic;hierarchical algorithm;probability density function;data representation;information-theoretic measure;hierarchical clustering;uncertain objects;clustering uncertain data;hierarchical algorithm
multi-stream;event sequences;significant patterns;information retrieval;stream data;significant patterns;multi-attribute;multi-stream;monitoring systems
ranking algorithm;real data sets;detection methods;network intrusion detection;graph-based;upper bound;machine learning;data set;similarity matrix;graph data;graph-based;rare classes;probability density
text mining;clustering;active learning;semantic knowledge;active learning;pair-wise;supervision;document representation;clustering documents;text document;clustering performance;document datasets;document clustering;instance-level;concept-based representation;semantic relatedness;knowledge base;supervised clustering
predictive model;supervised approach;semi-supervised;classification methods;semi-supervised
data matrix;original matrix;pattern recognition;data representation;information retrieval;matrix factorization;geometric structure;graph based;graph structure;real world;matrix factorization
graph mining;support measures
mining closed;search space;data stream environment;closed patterns;frequent patterns;mining closed;complete set of;pattern mining;sequential patterns;sequential patterns;data stream;closed sequential patterns;sliding windows;synthetic data sets;sliding windows
clustering;clustering;density-based;mixture model;data mining tasks;linear space;finding clusters;large datasets
microarray data;order-preserving;order-preserving;data items;data item;mining algorithm
optimization technique;microarray data;vector machine;quadratic programming;expectation maximization;feature selection;optimization problem;feature selection
pre-defined;sensor readings;window size;potentially infinite;naıve;search space;real dataset;computationally expensive;detection methods;water quality;pruning techniques
gradient-based;sequence alignment;dynamic programming;potentially infinite;selection problem;search space;learning problem;abstraction levels
high-dimensional;survival analysis;predictive modeling;support vector regression;high-risk;survival analysis;learning algorithm;censored) data;censored data;censored data;breast cancer
streaming data;data sets;streaming data;nearest neighbour;unlabelled data;classifier;nearest neighbour
mined results;frequent itemsets;mining algorithms;plays an essential role in;real-life applications;frequent itemset;frequent itemset mining;frequent itemsets;visual representation
multi-level;multi-dimensional;data analysis;measure called;real datasets;graph properties;on-line analytical processing;data sources;graph analysis;olap operations;data cubes;online analytical processing;traditional olap
temporal dependencies;search space;information systems;learned models;large scale;valuable information;correlated attributes;regression model;high order;computing systems;response variable
detection algorithms;web search engine;unique features;clustering technique;temporal dimension;real-life;web resources;real world;web search engines
multi-modal;training data;sampling methods;imbalanced data;wide range;supervised learning
high utility;high utility;data streams;data structure;tree-based;data mining tasks;sliding window;broad applications;data streams
clustering;clustering algorithms;computational complexity;search tree;clustering approaches;learning procedure;clustering algorithm;clustering algorithm
heuristic approaches;temporal dimension;ranking algorithms;probability distribution;link-based ranking;link-based;web search;markov chain
classification problem;classification accuracy;vector machine;training dataset;instances;optimization problem;privacy-preserving;svm classifier;svm classifier;radial basis function;classifier;support vectors
query processing;jim gray;data cube;data warehouse;ir techniques;text data;text database;online analytical processing;traditional olap;text database
feature space;multi-class classification;classification;classification accuracy;kernel function;uci data sets;kernel functions;feature spaces
data objects;text mining;support vector machines;mining framework;structured data;learning algorithm;tree structure;data mining;relation extraction;kernel methods;learning framework;relational data
hybrid model;finding task;large-scale;formal models;query topic;language model;multiple topics;expert finding;expert finding;benchmark dataset;prior probability;real world;human relevance judgments;weighted sum of
real-life datasets;synthetic datasets;cluster structures;cluster structures;distance matrix
nonnegative matrix factorization;nonnegative matrix factorization;spectral clustering;combinatorial optimization;spectral clustering;graph matching;data clustering;directed graphs;graph matching;key features;data mining problems
database;lower bounds;discriminative patterns;emerging patterns;space requirement;databases
search space;statistical dependence;significant rules;efficient discovery of;interesting rules;association rules;association rules
high quality;structural relationships;real datasets;closely related;application domains;sensor networks;social networks
high-dimensional;pattern-based;minimum support threshold;classification;classification accuracy;real-life;training phase;instance;power law;training instances;times faster than;parameter tuning;classification algorithms;classifier;web data;classification algorithm
nearest neighbor algorithm;training set;training data;clustering technique;classification models;streaming environment;unlabeled data;labeled instances;classification;synthetic data;evolving data streams;labeled data;supervised learning algorithms;manual labeling;evolving data streams;huge volumes of data;classification algorithms;labeled data;classification model;high speed
data set;real-world data sets;probability estimation;relational data
pre-defined;false positives;sequential pattern mining;memory usage;sequential pattern mining;data streams;static databases;sequential patterns;emerging applications;real-world applications;multiple times;data stream;pruning strategies;additional constraints;mining sequential patterns;data mining problem;sequential pattern mining;error bounds
clustering;data analysis;cluster structure;data points;linear projection;time series;data sets;customer segmentation;analysis tasks;dimensional data;high dimensional space
clustering;medical images;data collection;medical research;demographic information;instance;data mining techniques;cluster membership;patient data;image data;case study;target variable;similar features;data mining;relational database;data mining research
high-dimensional;tuning parameters;training set;density estimation;test set;massive datasets;outlier detection;cross-validation;outlier detection;closed-form solution;semi-parametric;training and test data;regularization parameter;real-world datasets
learning process;training algorithm;inductive learning;kernel functions;classification algorithm
language-model;language-model;mining framework;query language model;retrieval accuracy;object databases;local patches;visual words;pseudo-relevance feedback
clustering;clustering algorithms;computational complexity;multi-class;cutting-plane algorithm;unsupervised learning;pairwise constraints;vector machine;optimization problem;maximum margin clustering;pairwise constraints;maximum margin clustering;loss functions;objective function
total number of;graph mining;frequent patterns;minimum support;database;frequent subgraph;graph databases;chemical compounds;frequent subgraph;graph databases
information entropy;detection task;unsupervised algorithm;anomaly detection
svm algorithm;vector machine;time series;time series;support vector machines;objective function;learning method
clustering;domain knowledge;search-space;tree-based;target variable;association analysis;business intelligence;target variables;association analysis;association analysis
classification;logistic regression;loss function;vector machine;low computational cost;logistic regression;maximum margin;interpretability;maximum margin
cosine measure;classification;similarity matrices;similarity learning;similarity functions;nearest neighbor classification;perceptron algorithm;primary goal;similarity measures
user preferences;collaborative filtering;recommender systems;user behavior;implicit feedback;optimization procedure;compares favorably with;common task;personalized recommendations;explicit feedback;implicit feedback;confidence levels
labeled samples;unlabeled data;unlabeled data;semi-supervised learning;real data;relevant data;automatically detected;learning framework;semi-definite programming
data analysis;graph properties;large graph;large graphs;higher quality;graph data;data mining;systems biology
predictive models;large-scale;gene expression;prior knowledge;algorithm called;protein-protein interaction network;genomic data;gene expressions;breast cancer;gene expressions;data integration
sequence data;frequent subgraphs;complete set of;real world datasets;artificial datasets;graph data
naive bayes;classifier;naive bayes classifier;domain-specific;data mining based;biological sequences
multi-label classification;e;multi-label classification;classification process;label sets;complex data;multi-label
training data;lower bound;selection method;active learning;equivalence relation;additional constraints;active learning;large number of;similarity measure;theoretical results;record linkage;equivalence relations;selection techniques
high correlation;graph mining;frequent subgraphs;principal component analysis;makes sense;subgraph mining;machine learning;principal components;principal component analysis;mining algorithm
clustering;clustering;gaussian distributions;clustering problem;real-world;multiple attributes;markov random fields;clustering framework;markov random fields
real-world datasets;segmentation methods;takes into account;segmentation techniques;spatial features;moving object;spatial outliers
regularization;unlabeled data;unlabeled examples;sentiment analysis;text categorization;prior knowledge;sentiment analysis;labeled data;prediction problems;prior information;semi-supervised;semi-supervised;graph representation
data matrix;existing graph;data mining problem;local minimum;iterative algorithm
feature space;training set;classification method;classification;applications ranging from;application domains;text classification;data sets;produce accurate;text classification
clustering;uncertain objects;pruning techniques;probability density functions;clustering uncertain data
matching scheme;structural features;similarity measure;data mining applications;significant patterns;natural language texts;biological sequences
nonnegative matrix factorization;text mining;pattern analysis;mathematical formulation;coordinate descent;dimension reduction;real life applications;convex optimization problem;clustering
pair-wise;document corpus;gaussian process;topic models;discrete data;document corpora;latent variable models;model fitting;dimension reduction
kullback-leibler divergence;iterative algorithms
class information;feature extraction;canonical correlation analysis;recognition performance;feature extraction;feature fusion;canonical correlation analysis
nearest-neighbour;gaussian processes;machine learning methods;gaussian process regression
random walk;modeling approach;language model;random walk;topic model;topic models;baseline methods;modeling approach
mining frequent sequences;search space;mining algorithms;candidate patterns;regular expressions;mining frequent sequences;sequence mining;input sequences;application domains;regular expression;input data;regular expressions
global model;data analysis;database;original data;highly accurate;local patterns;data sets;missing data;missing data
high-dimensional;sparse data;multi-aspect;data mining;data mining;large-scale social networks;massive amounts of data;internet traffic;case-study;multiple aspects
social networks;computational complexity;subgraph mining;social networks;social interactions
correlation-based;ranked list;retrieving relevant;text-based;retrieval performance;search engines;classification problem;search engine;classifier;weak classifiers
sample selection;supervised learning;monte carlo;sampling techniques;learning algorithm;stationary distribution;training sets;large datasets;markov chain
complexity bound;random projections;random projections
data mining techniques;dna sequences;heuristic rules;post-processing;hierarchical clustering algorithm;clustering techniques
maximum entropy;private information;privacy concerns;static data;privacy-preserving;privacy-preserving data publishing;modeling method
classification;target domain;real data;probability distribution;latent semantic;clustering;learning task;cross-domain;labeled data;accurate classifiers;text classification;classification task;training data;document classification;text classification;semantic concepts;auxiliary information;transfer learning;classification algorithm;cross-domain;auxiliary data;learning strategies
named entities;performs poorly;set expansion;random walk;set expansion;ranking algorithms;ranking method;language independent;set expansion
additional information;sequence labeling;information extraction;computational cost;natural language processing
transaction data;data structure;multi-dimensional;data mining applications;scalable solution;data privacy;high dimensionality;data utility
auc;instances;anomaly detection;large number of;training set;large data sets;irrelevant attributes;distance-based;memory requirement;high dimensional
high-speed;feature extraction;classification accuracy;feature extraction;information processing;internet applications;support vector machines
real-world datasets;linear dimensionality reduction;component analysis;labeled instances;supervised methods;generalization performance;basis vectors;unlabeled instances;semi-supervised;graph-theoretic;dimensionality reduction
online reviews;low quality;online reviews;regression model;sheer volume of;large variations
clustering;data point;time series;time series;data set;clustering techniques
data mining;machine learning algorithms;multiple models;data management;trade-offs
competing methods;real-world;partially labeled;relational learning;statistical relational learning;unlabeled instances;relational learning;collective inference;labeled examples;higher accuracy than
subspace learning;training data;ranking performance;subspace learning;ranking tasks;text retrieval;semantic space;latent semantic indexing;text analysis;objective function
real-world datasets;segmentation methods;takes into account;segmentation techniques;spatial features;moving object;spatial outliers
cutting plane algorithm;data samples;data points;real world datasets;decision boundary;dimensionality reduction method;maximum margin;maximum margin;integer programming
unlabeled data;feature selection techniques;semi-supervised feature selection;graph-based;supervised methods;feature selection algorithms;statistical information;unlabeled examples;sample selection bias;labeled data;feature selection;graph models;semi-supervised;labeled examples;feature set
erroneous data;accurately identify;real-world;data streams;models built;maximum variance;data cleansing;labeled training examples;data streams;margin;data chunks
domain knowledge;similarity measure;theoretical properties;supervision;supervision;laplacian matrix;similarity measure between;semi-supervised;data mining algorithms;feature selection
closely related;instances;real datasets;data mining;real data;frequent itemset mining;instance;worst case;concept class;concept class;general problem;fast algorithms;low complexity;real world
learning process;parameter-free;semi-supervised learning;feature extraction;face recognition;semi-supervised learning algorithms;similarity-graph;feature representation;label propagation;semi-supervised;learning framework;unlike conventional
training data;collaborative filtering;collaborative filtering;low rank;negative examples;binary data;item recommendation;classification task;positive examples
clustering;data mining tool;original matrix;cluster structure;recommendation systems;parameter estimation;sparse matrices;real data;data matrix;exponential family;clustering techniques;dyadic data;low dimensional;market basket analysis;missing values
clustering;mining task;graph mining;collaborative filtering;database;map-reduce;mining process;raw data;real-world applications;case study;distributed processing;large datasets;execution engine;text mining;pre-processing;open source;distributed data;data storage
bayesian network;learning bayesian networks;closely related;regularization;instance;em algorithm;common practice;maximum likelihood;incomplete data;test data
real-world datasets;focused primarily on;inference techniques;temporal information;attribute dependencies;relational models;temporal dimension;relational information;social networks;relational learning;relational data;relational domains;efficient learning
web documents;web pages;high-recall;extraction process;query generation;extract information from;web mining;web sources;relevant information;service descriptions;application scenarios
clustering;order statistics;target class;higher-order;clustering performance;data stream;data stream clustering;purity;data streams;data stream clustering;evaluation measures
information spaces;local patterns;case study;real-life;relational information;global models;blog posts;web mining
parameter space;spectral clustering;multi-agent systems;multi-agent system;spectral clustering;spatio-temporal
decision trees;decision tree;high quality;compression technique;greedy approach;candidate itemsets;small groups
computational complexity;fast algorithm;graph structure;random walk;graph structures;large graphs;case studies
real graphs;total number of;real networks;database;clustering coefficient;social networks;real world
collaborative filtering;collaborative filtering;aggregate information;statistical model;large datasets;external data;individual users
em algorithm;classification performance;multi-label classification;parameter estimation;random variables;probabilistic model;multi-label classification;data sets;empirical bayes;classification method;probabilistic model;posterior distribution
complex structure;cluster structure;visual analysis of;laplacian matrix;real-world data sets;major limitation;cluster analysis
social network analysis;evolutionary clustering;dirichlet process;dirichlet processes;data mining;evolutionary clustering;automatically learning
transition matrix;hierarchical dirichlet process;evolutionary clustering;statistical model;social network analysis;hidden markov model;evolutionary clustering;automatically learning
world wide web;optimization framework;large-scale;dimension reduction;feature extraction;text categorization;text data;algorithm called;feature selection;problem remains;unified framework;feature analysis;principal component analysis;maximum margin;semi-supervised;dimension reduction;objective function
clustering;compact representation;synthetic data;data points;sensor data is;false alarm;energy-efficient;critical task;time series;event detection;time series;monitoring applications;hierarchical structure;sensor networks;event detection;sensor nodes;real data;sensor networks;energy-efficiency;clustering model;incremental clustering
training examples;instances;multi-label learning;quadratic programming;text categorization;multi-instance;multi-label learning;scene classification;class labels;maximum margin;multi-instance;learning task;maximum margin
link prediction;detecting anomalies;weighted graphs
real-world datasets;instance;instances;related entities;collective classification;statistical relational learning;performance gains;class labels;relational data
singular value decomposition;information content;latent dirichlet allocation;mixture model;multi-document summarization;multi-document summarization;multi-document summarization
synthetic data;subspace clusters;subspace clustering;high dimensional space;subspace clusters;subspace clustering
em) algorithm;feature subset;markov blanket;worst-case;selection method;expectation maximization;data sets;missing data;feature subset;missing data;incomplete data;selection algorithm
training data;classification;classification;data sets;algorithm called;prediction error;class labels;classification problems;classifier;training sample
clustering;mining frequent patterns;complex networks;data-stream;large networks;social networks;biological databases;topological properties;high precision
detection techniques;sequence data;large number of;detection techniques;sequence data;data sets
clustering algorithms;classification problem;classification;classification;locally linear;locally linear;linear classifier;clustering algorithm;single classifier
dimensional data;clustering high dimensional data;projective clustering;fuzzy clustering;algorithm named;projective clustering;document clustering;model parameters;data mining;dimensional data;traditional clustering;projected clusters;clustering model;probability model
market basket data;selection predicates;summarization techniques;data collected
matrix factorization;video sequences;factorization method;data set;time series
clustering;clustering algorithms;compares favorably with;general purpose;distance function;data mining;algorithms typically
gene expression data;synthetic data;predictive features;real data;predictive accuracy;feature selection;feature selection;feature selection methods;entity extraction;information theoretic;word sense disambiguation
classification techniques;synthetic data;relational model;class label;joint inference;data set;collective classification;class labels
overlapping clusters;gene expression;mixture models;overlapping clustering;exponential family;general setting;clustering algorithm based on;overlapping clustering;mixture models;benchmark datasets;clustering quality
test data;classification;anomaly detection;anomaly detection;vector machine;detection algorithm;benchmark datasets;classification problems;classification algorithm
theoretical analysis;multi-stage;linear systems
analytic hierarchy process;knowledge management;evaluation model;multi-criteria decision-making;knowledge management
game theoretic;data mining;knowledge discovery;data mining;social sciences;game theory
clustering;pattern recognition;fuzzy clustering;real dataset;fuzzy clustering;key features;dimensionality reduction
traffic flow;traffic light;traffic light;simulation results
distributed environments;programming interface
ad hoc networks;nodes represent;edges represent;ad hoc networks
detection algorithm;sensor node;specific properties;statistical model;detection algorithm;sensor networks;low-complexity;sensor nodes;sensor networks
web browser;user interface
knowledge management;information technology
agent architecture;agent model;multi-agent system;agent architecture

test-sample;phase space reconstruction;support vector regression;time series;machine learning techniques;evolutionary algorithm;decision making;power grid
customer relationship management;data warehouse;information management;business process;service-oriented;data analysis
web-based;high quality;web source;web sources;web source;web source
mathematical model
finite element

extended kalman filter;prediction model;neural networks;detection method;time series;rbf neural network;prediction error;detection performance;parameter estimates;radial basis function;short-term
databases;high accuracy;database
power demand;time series;power demand;maximum likelihood;absolute error;error rate
partial information
semantic similarity between;information retrieval;similarity computation;semantic information;semantic similarity;artificial intelligence
classification accuracy;feature space;multi-class;separability;kernel method
information management
uncertain information;relational operators;relational operators;relational database;relational database;relational databases;real world applications
algorithm combines;web applications;file content;file content;data security
singular value decomposition;recommender systems;collaborative filtering;singular value decomposition;collaborative filtering algorithm;data sparsity;recommender systems;collaborative filtering
mobile ad hoc network;intrusion detection;trust model;distributed nature of;network model;mobile ad hoc networks;mobile ad hoc networks
data acquisition
control algorithm
algorithm takes;field programmable gate
problem solving
high speed;traffic analysis;radio frequency identification;data mining;querying capabilities;traffic analysis
simple algorithm
power consumption
information resources;general-purpose;information extraction;search engines;text classification;information resources;huge number of
accurately identify;classifier;database management system;online analytical processing;resource allocation;online transaction processing
network model;control scheme;modeling techniques;database management
web services;classification;web services;semantic web services;web services;formal semantics
clustering;clustering algorithm;clustering method;clustering accuracy;synthetic datasets
vector machine;convergence rate;vector machine;global optimization;iterative algorithm;network structure;forecasting accuracy
retrieving information from;xml documents;xml information retrieval;retrieval model;xml information retrieval;structured data;unstructured data;vector space model
aggregation techniques;sensor networks;data aggregation;sensor nodes;prior works;sensor networks;data aggregation
multi-agent;multi agent
active power;regression method;programming model;genetic algorithm;data collected
agent model;small-world;simulation model;adaptive algorithm
knowledge management;competence;knowledge management
stock market;rates;data mining
clustering;factors affecting;quality assessment;water quality;water quality;clustering model
classification;classification;21s+t,3;21s+t,3,d;21c,s,t)+t,k,d;complete classification
memory usage;profile based;high-reliability
optimal combination of
vehicle detection;vehicle detection;moving objects;moving object
search performance;pso) algorithm;high speed
problem-solving;depth-first search;problem solving
spatial clustering;spatial data mining;tree-structured
statistical technique
regression analysis;regression analysis;logistic regression

factor analysis;resource management

learning examples;vector machine;input/output;data mining;test examples;decision making
clustering;svm algorithm;feature extraction;recognition rate;regression analysis;learning algorithm;vector machine;probability distribution;generalization ability;reliability analysis;reliability analysis
problem-solving;breadth-first search
large numbers of;multi-objective;simulation model;parallel processing;dynamically changing;temporal logic;complex systems;multi agent;agent-based
service quality;critical task;instance;critical task;selection method;grey relational;parallel structure;optimization technique;composite services;parallel structure;composite service;weighted sum of
convergence rate;sample data;learning algorithm;detection method;neural network;gradient descent;detection method;neural network
nearest neighbor algorithm;modeling method
mathematical model;regression analysis;linear regression;regression analysis;regression model;statistical test;dependent variable;quantitative analysis
supervision

evolutionary process;model fitting;model parameters;social sciences
instance;reference model;workflow management system;mobile agent
analytic hierarchy process;simulated annealing;simulated annealing
network model;network model;overlay network
genetic algorithm
dynamic model;simulation experiments;parametric model;regression model;semi-parametric;observation model
problem-solving;video game
information sharing;web services
data organization
enterprise application;application development;information technology;enterprise applications
information service;information service;data base;multi-level
long-term;short-term
long-term;positive effect;short-term

electronic commerce;knowledge-based;rapid rate
diffusion model;diffusion model
neural network model
hybrid model;complex structure;generalization performance;vector machine;maintenance cost;rough set theory
prediction model;water quality;vector machine;bp algorithm;time series;high efficiency;water quality;multilayer perceptron
scheduling strategies;multi-layer;multi-layer
generalization performance;rates;vector machine
classification;classification;vector machine;classifier;wide range;high accuracy;multi-class;classifier
vehicle motion;mathematical models;motion model;simulation result
hybrid approach;rough set theory;rough set;attribute reduction;np-hard problem;knowledge acquisition;feature selection;rough set theory;data mining;feature selection
text mining;clustering;clustering results;text documents;text understanding;text clustering;text clustering;clustering algorithm;clustering algorithm;clustering
petri nets;modeling tool
parameter estimation
genetic algorithm;bp neural network;genetic algorithm;neural network model;global convergence;forecasting accuracy;neural network;search efficiency
decision-making;information technology

long-term;chinese coal mining;short term
cluster analysis
network resources;user interface;multiple types of

case study;case studies;conceptual model;case study
neural network;neural networks;state detection;state detection;neural network
classification rules;information loss;rough sets;rough set approach
network resources;network resources;adaptive filtering;data mining
business logic;genetic algorithm;information extraction

closed-loop
multi relational;data warehouse;data warehouse;data mining research;relational information;multi relational;relational database;mining algorithm
image retrieval;image retrieval;explicitly model;lda model;latent dirichlet allocation;similarity measure;large scale;real-world;database;topic model;large-scale;1
clustering;clustering result;high efficiency;clustering method;data set;hierarchical clustering algorithm;data sets;similarity matrix;network model;clustering model
construction cost;classification;rbf neural network;regression analysis;decision-making;time series;rbf neural network;rbf) neural network;forecasting model;artificial neural network;construction cost;radial basis function;hidden layer
face images;singular value decomposition;discriminant analysis;discriminative features;kernel matrix;face databases;face recognition;data matrix;theoretical justification;recognition tasks;computational complexity
bp neural network;evaluation model;evaluation index system;bp neural network
world-wide web;web search;relevant pages;network resources;general-purpose;classifier;search tools;domain-specific;search engines;web resources;classifier
hidden nodes;radial basis function;neural network;pre-processing;radial basis function;neural network
clustering;clustering;customer segmentation;clustering scheme;segmentation method;basic properties;clustering methods;customer behavior;clustering analysis;cluster analysis;standard datasets
modeling method;product development;process management;multi-view;process management;information flow;development process;product development
utility function;data collection;time series;data analysis;dynamic analysis

regression analysis;sequence data;dynamic analysis;regression analysis;static analysis
clustering results;history data;product family
financial data;knowledge management;evaluation index system;principal component analysis;knowledge management
factor analysis;cluster analysis;statistical analysis
case study;configuration management
clustering;clustering problem;simulated annealing;data structure;optimization algorithm;optimization problem;clustering algorithm;low efficiency
feature extraction;recognition tasks
cold start;collaborative filtering;recommendation systems;collaborative filtering;preference based;user preference;recommendation quality;user preference
artificial neural network
rough set approach;classification rules;rough sets;short-term;neural networks;support vector machines;short-term;information loss
classification;information theory;naive bayes;classifier;tree structure;classifier;real world domains;naive bayesian
detection method;detection algorithm;translation invariant
convergence speed


multi-criteria;instance
feature space;feature set;optimization algorithm;optimization problem;pso algorithm;test data
wireless networks
rare classes;associative classification;classifier;rule set
surveillance systems;video surveillance
detection algorithm;detection accuracy;vector machine;classifier;uniformly distributed
semantic information;ontology mapping;ontology mapping;ontology mapping
information fusion;wavelet analysis;feature extraction;quantitative analysis;neural networks
machine translation;named entities;cross-language information retrieval;wide range;parallel corpora;natural language processing;statistical model;question answering;named entities
recommendation quality;web-based;recommendation systems;collaborative filtering;prediction accuracy;personalized recommendation;data sparsity;recommendation process;personalized recommendation
numerical experiments;genetic algorithm;genetic algorithm;common sense;convergence speed;operator
prediction model;prediction model;regression analysis;closed-loop;prediction errors;linear regression;data mining technology;prediction error;production data;regression model;artificial neural network
pattern matching;pattern matching;network intrusion;security problems;intrusion detection system
network security;content filtering;content filtering;network security;filtering methods
risk management;case-based reasoning;case-based reasoning;risk management
extracting knowledge from;secure multi-party computation;privacy-preserving data mining;privacy-preserving;private data
boolean functions;boolean functions
operator;risk management;evaluation index system
service-oriented;agent technology;classification algorithm
signature scheme;signature scheme
global convergence;bp neural network;conjugate gradient;simulation results;line search;convergence speed;neural network
workflow execution;data mining method;workflow execution;data warehouse;data mining;main idea;data processing;data mining
genetic algorithm
closely related;spatial data mining
clustering;multiple dimensions;processing algorithms;large number of;information fusion;complex data;neural network
clustering;fuzzy neural network;fuzzy neural network;neural network model;network model;data processing;complex data;clustering algorithm
wavelet analysis;detection algorithm;image processing
face images;distance measure;face recognition;euclidean distance between;similarity measure;recognition performance;feature extraction;linear discriminant analysis;face image;face databases;feature vectors
signature scheme;signature scheme
digital images;algorithm performs well;human visual system
evidence theory;evidence theory
fold cross-validation;genetic algorithm;classification accuracy;selected features;global optimization;classification performance;fitness function;feature selection;feature subset;svm classifier;feature subset selection;support vector machines;image recognition
correlation coefficient;regression analysis;principal component;spectral analysis
multiple classes;instance;neural networks;classification;label set;learning algorithms;data set;neural network;multi-label classification;instances;binary classifiers;multi-label data;multi-label;multi-label;classification task;classifier;web-mining
genetic algorithms;genetic algorithm;database;database;mobile computing;wireless network;additional information
high rate;tree model;decision tree
spatial data mining;geographic information;spatial data mining
attribute values;case retrieval
vector machine;real estate;vector machine;risk minimization;modeling approach;real estate;support vector machines;risk factors
prediction model;vector machine;vector machine;machine learning;learning theory;higher accuracy;prediction model;learning performance;neural network
complex network;degree distribution;position information;scale-free;simulation results;network model;evolving networks;power-law

geometric relationship between;semi-supervised;semi-supervised learning;multi-instance learning;multi-instance;graph based semi-supervised learning;instances;region-based;learning problem;image annotation;learning framework;image annotation
pso algorithm;global search;social behavior;population-based
case study;case study
intrusion detection;intrusion detection system;intrusion detection systems;hardware architecture;hardware architecture;pattern matching;field programmable gate

manufacturing process;retrieval method
mining framework;data mining systems;data mining
data mining;evaluation model;entire process;evaluation model;data mining
database;storage space;genetic algorithm;mining association rules;association rule;mining algorithm;encoding methods
communication protocol
regression model;data mining
video frames;compressed-domain;video content;rough sets;discrete cosine transform;mining tasks;compressed-domain
background model;edge detection;image segmentation;block-based;vision-based;shadow detection
color space;segmentation method;segmentation methods;color information
takes into account;access controls;data protection;data protection;data security
generated automatically
role-based;fine-grained access control;access control model
clustering;high-dimensional;databases;density-based;large-scale;density-based clustering;outlier detection;algorithm named;outlier detection;detection results;density-based clustering;local density;density-based clustering;data mining;concept called
fixed length;context-based
simulation experiments;hash table;trust management
attribute reduction;rough set;rough set;set intersection;attribute reduction;vertically partitioned data;attribute reduction;privacy preserving;data mining;distributed data;privacy preserving
model parameters;reliability analysis;data analysis;power law;statistical model
web pages;web usage mining;low cost;electronic commerce;web server;web site;web mining;artificial intelligence;data mining;data mining technology;mining algorithm;web data;web usage mining
database management;decision support system;data warehouse;data warehouse;data warehouse;decision support system
scale-invariant;knowledge-based;degree distribution
frequent item set;database;item set;high complexity;association rule;association rule
statistical analysis;data mining;social science;social science;data mining
low precision;optimization algorithm;global search;local search;fitness;operator;optimization algorithm
wavelet-based;compression based;natural images
smoothing method;data points;human visual system
evaluation index system;resource utilization;factor analysis
singular value decomposition;singular value decomposition

petri nets;access control;confidential information;electronic documents;petri nets;information flow;information flow
knowledge sharing;knowledge creation;game theory;knowledge sharing
knowledge base;database
process mining;model checking;process mining;event logs;process models;mining algorithm
classification;minimum support;classification accuracy;minimum confidence;classification rules;apriori algorithm;classification method;association rules;associative classification;classifier;high classification accuracy
knowledge management;management systems;knowledge management
data mining;historical data;data mining;stream data;data stream;data rates;data streams;high volume;comparative analysis;applications involving;traffic analysis
web server
search efficiency;mathematical model;selection algorithm;selection algorithm;large-scale
detection method;false positive;detection methods
test data;high dimensional data sets;high-dimensional data sets;image sequences;latent structure;dimensional data;local affine
frequency domain
human visual system;human visual system
continuous functions;local search;optimizer;continuous functions;search space
social network;real world networks;seed set
community structures;online social networks;synthetic datasets;networked data;real datasets;prior distribution;citation networks;social network data;map) estimation;noisy data;social networks;iterative algorithm;human interaction;temporal smoothness;community structure
greedy strategy;information diffusion;minimization problem;influential nodes;large social networks;social network;limited number of
prediction accuracy;collaborative question answering;prediction model;large-scale;query suggestion;question answering;community question answering;answer ranking;user ratings;user intent;information seeking;question answering;interface design
database technology;international conference on;database theory;databases;conjunctive queries;program committee
database technology;international conference on;high-quality;acm sigmod;large number of;review process;common practice;review process
multi-dimensional;rnn queries;data owners;knn queries;data outsourcing;query answers;low overhead;databases;query results
multiple times;data dissemination;data collection;static data;privacy-preserving
multiple queries;database;specific set of;theoretical results;general form;relational views;database table
information disclosure;xpath queries;xml fragments;user query;user queries;answering queries;xml database;query result;information disclosure
data mining;web pages;workshop on data mining;information organization;large volumes of data;information sources;social network;international workshop on;sponsored search;rates;online advertising;user intent;data mining;information retrieval;data mining problems;search logs;natural language
regular expressions;database;unstructured text;declarative queries;information extraction;large data sets;structured information;optimization techniques;unstructured data
information extraction;database;user feedback;user interface;information extraction;structured data;unstructured data;user interaction
web scale;large number of;application domains
query execution;query processing;optimization approach;text documents;output quality;retrieving relevant;information extraction systems;execution strategies;information extraction;cost-based;text databases;text documents;query optimizers;multiple relations;queries involving;structured data;structured queries
domain adaptation;unlabeled data;large number of;information extraction;model trained;labeled data from
natural-language text;large-scale;highly accurate;information extraction;knowledge discovery;instances;knowledge base;query engine
web mining;search applications;text segmentation;product search;entity relationship
domain-independent;natural language text;deep-web;hidden" databases;web-scale;extracted data;information extraction techniques;structured data;knowledge base
probabilistic databases;probabilistic databases;query languages;database systems
10, 6, 12, 7, 2
knowledge representation;ontology languages;domain specific;international conference on;international workshop on
electronic commerce;information processing;international workshop on;autonomous agents;multiagent systems;artificial intelligence
probabilistic approach;data sets;information retrieval;real data sets;multi-attribute
enterprise search;large parts;open-source;virtual view;semi-structured;information retrieval techniques;emerging applications;data set;search queries;base data;xml views;xml database;integration systems;keyword search over;query results
low-overhead;data processing;programming model;multi-core;high-end;join operators;operator;high scalability;data stream;high-throughput;data stream management systems;data transfers;column-oriented;rates;times higher;join operator;stream joins
computational efficiency;sensitive information;data publishing;high confidence;data privacy;social networks;privacy requirements;personal data
erroneous data;query complexity;data exchange;efficiently computing;data-integration systems;data sources;answering queries;data integration;source data;information extraction techniques;semantic mappings;schema mappings;data integration
online aggregation;data management problems;extreme values;outlier detection;large number of;data set;query processing;distance join;data management;bayesian approach
query optimization;query evaluation;input tuples;query optimizer;plans;estimation method;join operators;join processing;execution plan;joint distribution;estimation algorithms;data sets;depth estimation;scoring function;relevant answers;base tables;operator;depth estimation;principled framework;speed-ups
rdf data;web-based;rdf databases;vertically partitioned;query times;large-scale;scalability issues;database;real-world applications;semantic web;scale poorly;column-oriented;prior art;data management
short queries;probabilistic models;retrieval models;collaborative question answering;retrieval model;query processing;long queries;improving search;query expansion;context-based;search engine queries
highly relevant;data sets;user generated content;event detection;user generated content
enterprise search;search engines;enterprise search;web searching
ranking algorithm;portfolio theory;document ranking;collaborative filtering;ranked list;information retrieval;performance gain;retrieved documents;probability ranking principle;information retrieval;rank order;information retrieval systems;document ranking
document ranking;0,1;language models;document retrieval;rank documents;language modelling;information retrieval;retrieval models;normal distribution;posterior distribution;cost function;ranking documents;probabilistic retrieval model;language model
latent dirichlet allocation;relevant documents;cluster-based;coarse-grained;topic models;topic models;similar documents;feedback documents;information retrieval;query expansion;relevance models
search task;multiple users;trec data;shared information;search space;information retrieval;relevance feedback;search logs
collaborative filtering;recommender systems;information gathered;relevance model;rating prediction;movie database
clustering;ir) techniques;search strategies;ir techniques;information retrieval;relevance feedback;search strategies
entropy-based;active-learning framework;recommendation systems;collaborative filtering;rank learning;sampling strategy;sampling method;user feedback;rank-learning;instances;document retrieval;ranking functions;active sampling;user's preferences;sampling methods
past queries;term weights;document retrieval;focused retrieval;context-sensitive;document collections;retrieval accuracy;question answering
active learning strategies;text classification;classification;training examples;active learning;manually labeled;active learning;ranking function;multi-label classification;unlabeled examples;binary classifiers;single-label;text classification;multi-label;classifier
relevance scores;web search;relevant documents;average precision;rank documents;probabilistic graphical model;ranking function;information retrieval;model parameters;retrieved documents;evaluation measures;hidden layer;ranking model
search results;11;search tasks;video retrieval;low level features;similarity based;interactive search;diversity measures
density estimation;image annotation;generative models;image collection;image annotation;mixture models;bayesian hierarchical;positive effect
image retrieval;xml fragments;textual information;document retrieval;multimedia retrieval;structural information;xml tree
ranking algorithm;web documents;false positives;automatically extracted;vector-machine;keyword queries;classifier;digital library;search engine;document-level;retrieval results;manual effort
relevant expertise;expert finding;additional information;retrieval performance;expert finding
search task;entity ranking;expert search;ir techniques;information retrieval;vector space model;formal model;search task;query terms
world wide web;web-based;effectively identify;sentiment analysis;online advertising;web page
web pages;similarity metrics;online advertising;provide evidence;web page;information retrieval systems
keyword queries;xml element;language models;xml) data;test collections;retrieval model;keyword query;structural information;semistructured data;probabilistic retrieval model;typically requires
knowledge bases;language model;concept detection;detection methods;language modeling approach;average precision;language modeling
related terms;database;positive results;information retrieval;wide range;fuzzy logic;information retrieval systems;fuzzy sets;query terms
multiple criteria;document relevance;relevance assessment;information retrieval;aggregation operator;information retrieval
query aspects;human relevance judgments;ir systems;relevance judgments;information retrieval;test collections;query aspects;test collections;documents retrieved;human relevance judgments;test collection
document collections;relevance judgments;test collection
evaluation methodology;pre-retrieval;prediction methods;cross validation;performance predictors;combination methods;correlation coefficients;query performance;trec collections
term weights;relevance scores;mutual information;classification;ranked list;rank learning;opinion retrieval;feature selection methods;opinion retrieval;performance gains;learning approaches;classifier;likelihood ratio
detection techniques;topic-relevance;takes into account;automatically generated;opinion finding;opinion finding;trec blog track;retrieval task;opinion retrieval;retrieved documents;document collection;query terms
domain adaptation;supervised learning techniques;transfer-learning;sentiment analysis;naive bayes;naïve bayes;classifier;naive bayes classifier;sentiment analysis;base classifier;occur frequently;classifier
web page;web search;underlying assumption;multi-step;web page;path-based
query expansion;search results;document collection;trec collections;document retrieval;automatic query expansion;search result;query log;parameter tuning;search effectiveness;search engine;pseudo-relevance feedback;retrieved documents;query expansion;web search engines
web information retrieval;retrieval effectiveness;prediction methods;query-independent;prediction accuracy;query-independent;search tasks;query type;feature based;test collection
web search;search effectiveness;real-world;collection size;web crawl;web-pages;retrieval effectiveness
retrieval effectiveness;query-independent;user-generated content;contextual information;contextual information;retrieval model
term weights;information content;trec collections;language model;tf-idf;information retrieval;term frequencies;retrieval model;information retrieval
retrieval performance;semantic information;retrieval systems;information retrieval
transliteration;relevant documents;retrieval performance;query terms;document pairs;transliteration;cross-language information retrieval;query terms
binary classification;classification problem;classification;classification;classification approaches;classification process;data sets;classification results;detection methods;classification approach
online product reviews;feature selection;textual content;sentiment analysis
user preference;data analysis;prediction tasks;classification;user generated content
small sample;latent dirichlet allocation;relevant documents;resource selection;relative entropy;topic models;text) collection;text collection;distributed information retrieval;sampling algorithms
distributed information retrieval;information sources;data fusion;source selection;document collections;source selection;source selection;data fusion
current web;search engines;query biased;result set;large collections;query-biased;hit rate
search task;user studies;search tasks
weakly supervised;algorithm iteratively;named entity;entity types;classification
web pages;svm based;user query;ranking algorithms;search engine;web search engines
large-scale;large-scale;discovering association rules;discovering association rules;data mining;association rules;interestingness measures
context aware;web pages;search services;scoring function
search results;test collections;retrieval systems
order statistics;density estimation;image analysis;machine learning;1;additional knowledge;image annotation;rich semantics
search results;query intent;specific information;query intent;users' queries;query type;search engine;result pages
user groups;search engine;search engine
document collection;query processing;web search;distributed query processing;data centers;distributed query processing;data center;query processing in;search engines;remote sites;web search engines;information propagation
search results;result diversification;image retrieval;context based;image) search;search results
query expansion;ad-hoc retrieval;query expansion;retrieval performance;feedback documents;query expansion
web pages;strong correlation;tf-idf;document frequency;document frequency;term frequency;trec web track;high similarity
query execution;cost-aware;large scale;simulation results;query result caching;search engines;web crawl;ir systems;query results;web search engines;query log
low-quality;web sites;search engine
clustering algorithms;memory space;document clustering;clustering algorithms
user study
keyword queries;structural heterogeneity;xml retrieval;xml retrieval;query formulation;xml queries;xml repositories;keyword-based;search problem;query generation;xml data;keyword query;query refinement
language independent;multi-document summarization;multi-document summarization
document-centric;scoring functions
instance;retrieval models;large collections;term matching;large collections
simple linear;ranking algorithm;linear classifier;ranking measures;machine learning;query dependent;information retrieval
query execution;query logs;document-frequency;query terms;computational costs
text fragments;source text;low level
decision trees;text-based image retrieval;text-based;text-based image retrieval;visual concepts;retrieval task;visual concepts;detection task
language pairs;query translation
entropy-based;entropy-based;hybrid method;desired level of;1;los angeles;average precision
user navigation;xml retrieval;xml retrieval;user navigation;structural summaries;collection statistics
computational complexity;query-specific;multi-document summarization;query-specific
sentence level;document level;semantic similarity;detection approach;data collection;semantic relations;blog 2006 data;query words;text processing
domain-independent;opinion retrieval;query-specific;opinion retrieval;retrieved documents;query-specific;test sets
domain knowledge;domain model;user study;formal concept analysis;user feedback
question-answering systems;intermediate results
image retrieval;similarity metric;image collection;image content;similarity functions;content similarity;distance) metric;retrieval performance;image similarity;similarity based
speech recognition;spoken content;semantically similar;speech recognition;query-independent;speech retrieval;spoken content
semantic indexing;latent semantic indexing;document pairs
video data;classifier;visual features;semantically related
text classification;frequency information;boosting methods;supervised learning algorithms;text classification;binary features
document collection;latent dirichlet allocation;generative process;trend detection;topic model;topic discovery;text collections;document collections;generative model;latent dirichlet allocation
geographic regions;geographic regions;textual descriptions;geographic information retrieval;database;domain-specific;parameter learning;visual similarity;geographic information retrieval
data set;real-world;vector-based;search engine
extraction process;blog posts;relevant content;search engines;trec blog track;blog posts;average precision
automatically discovering;text descriptions;domain independent
information access;ir researchers;retrieval evaluation;information access
information diffusion;social network;online communities;social network;information retrieval;social network data;test collections;social networks;privacy issues;information retrieval techniques;ir models;information retrieval;multimedia data
specific applications;geographic information;relevant documents;related information;large scale;data sources;digital library;long term;web search engines;geographic information
search systems;document collection;search topic;relevant documents;documents retrieved;information retrieval evaluation;user studies
extracted information;extraction techniques;ranking models;information extraction;relevance feedback;instance;semantic search;information extraction
potentially) infinite;query log analysis;query logs;log mining;extracted knowledge;query logs;web search engines
low power;kdd community;data transfer;sensor network;sensor networks;context aware;sensor fusion;trend analysis
database;structural information;directed graph;query language;large graph;relational database management systems;edges represent;hidden structures;nodes represent;relational database;relational databases;relational databases
online content;social networks;search engine;online content;web search engines;page content
prediction accuracy;classification technique;tree based;probability distribution function;decision tree;data uncertainty;uncertain data;moving object databases;decision tree algorithms;emerging applications;network latency;sensor networks;classification method;uncertain data;information gain
intrusion detection;privacy-preserving;applications including;sensitive information;link discovery;link discovery;transitive closure;link discovery;privacy-preserving;network structure;social network analysis
high similarity;similarity measures;social networks;subgraph queries;social networks
sentence-level;novelty detection;sentence-level;text processing;novelty detection
distributional clustering;distributional clustering;linear support vector machines;text documents;linear svm;text categorization;classification performance;linear svms;text corpus;feature selection;application area
training set;unlabeled data;unlabeled examples;majority voting;negative examples;blog classification;test instance;unlabeled examples;blog classification;weak classifiers;positive examples
decision tree;decision tree;dictionary based;dictionary-based;hidden markov model;hidden markov model;word segmentation
text mining;text mining;redundant features;data structure;conditional distributions;correlated features;log-linear models;large number of;feature space;manual construction of;memory usage;large-scale;automatically generating;log-linear models;graph-based;mining tasks
gradient-based;optimization technique;unlabeled data;generalization error;labeled data;prediction error;graph based;graph-based semi-supervised learning;graph structure;semi-supervised classification
clustering;data point;algorithm performs well;regularization term;reconstruction error;clustering result;clustering techniques;low dimensional;clustering algorithm;dimensional data;objective function;clustering
clustering;clustering;lower bound;synthetic datasets;similarity-based;algorithm produces;lower bound;clustering quality
clustering;spectral clustering;spectral clustering;large data sets;data sets;sampling strategies;computational cost;real-world data sets;great promise
meaningful clusters;frequent itemset;clustering results;frequent itemsets;association rule mining;text documents;cluster labels;knowledge embedded in;document clustering;document clustering;high dimensionality;fuzzy association rules
clustering;pre-defined;data analysis;clustering algorithms;data set;data sets;noisy data;clustering algorithm;clustering algorithm called
polynomial-space;event sequences;polynomial-space;mining frequent;event sequence;event sequences
clustering;clustering results;training data;feature selection algorithms;finite mixture;feature selection;statistical model
real-world datasets;loss function;classification;regression problems;data mining and machine learning;great success;multi-resolution;machine learning;boosting algorithms;multi-resolution;ensemble technique;modeling techniques;classification tasks
real-world datasets;partial information;training examples;interval data;constraint programming;optimization problem;maximum-margin;classification;high probability;margins
interestingness measure;conventional methods;classification;artificial data;data set;interestingness measure;classification rules;rule discovery;heuristic search;minimum description length
search space;mining frequent patterns;fp-tree;frequent patterns;database;pattern tree;mining process;memory space;mining frequent patterns;mining frequent patterns;fp-growth;frequent pattern;pattern tree
data structure;database contents;mining frequent patterns;real-world scenarios;frequent patterns;database;pattern growth;pattern tree;complete set of;tree-based;transactional databases;highly scalable;efficient discovery of;transactional databases;pattern set;frequent patterns;mining algorithm;frequent pattern;huge number of
semantic networks;commercial search engines;search engine;semantic relations between;large scale;related queries;query logs;query logs;resource allocation;semantic relations;semantic relations;query-url;resource allocation
hybrid approach;semantic relations;web resources;coarse-grained;semantic relations
video sequence;pattern recognition;hierarchical dirichlet process;real-world;learning algorithm;large number of;event detection;real-world applications;activity models;hidden markov model;dirichlet processes;video sequences;false positive;weak classifiers
automated reasoning;bayesian network structure;active learning;real-life applications;active learning;active learning;data set;instances;active learning approach;selection criterion;bayesian networks
classification;classification;naive bayes;machine learning;classification performance;kernel density estimation;kernel density estimation;kernel density estimation;benchmark datasets;naive bayesian
matrix factorization;original matrix;matrix factorization;probabilistic matrix factorization;variational bayesian;operator;variational bayesian;factorization method
bayesian approach;collaborative filtering;prediction accuracy;learning algorithm;relevance feedback;long-term;relevance feedback;image collections;user models;long-term;bayesian approach
web archive;large scale;link structure;ranking algorithms;search engines;link-based;ranking scores;link analysis
bayesian network;markov blanket;classification;data mining techniques;mining association rules;ensemble classifier;vector machine;neural network;data driven;ensemble classifier;classifier;feature selection
ensemble techniques;data mining technique;synthetic data;stream classification techniques;data streams;data chunks;data streams;ensemble classifier;ensemble technique;ensemble technique;classifier;classification error
base classifiers;loss function;decision trees;classification;parameter estimation;decision tree;closely related;streaming data;predictive accuracy;training instances;streaming data;data structure;generation process;data streams;main idea
similarity computation;link graph;random walk on;real-world domains;algorithm called;link graph
numerical experiments;feature selection algorithm;regularization;local learning;variational methods;irrelevant features;online-learning;learning paradigm;large margin;regularization;feature selection algorithm;locally linear;online learning;training data;batch learning;regularization parameter;supervised learning;benchmark data sets
clustering;clustering algorithms;regularization;irrelevant features;data representation;1;feature selection;clustering;data representations;local learning;benchmark datasets;feature selection
rank learning;ranking svm;ranking svm;large data sets;search engines;data mining task;support vectors;retrieval quality;accuracies;information retrieval;learning method
fixed length;support vector machines;general purpose;classification;prediction methods;kernel functions;regression problem;window-based;encoding scheme;prediction problems;local information;kernel framework;feature vectors;prediction models
real-world datasets;matrix factorization;exponential family;low-rank;matrix factorization;exponential family;bayesian framework;relational data
mixture components;mixture model;image categorization;mixture models;posterior distribution;nonparametric bayesian;image categorization;applications involving;bayesian learning
nearest;sampling techniques;instances;instance;nearest neighbour;classifier;sampling technique
classification;sparse data;classification;contrast patterns;pattern based;contrast patterns;emerging patterns;data mining;building classifiers;contrast patterns;classifier
feature space;classification problem;discrimination power;synthetic data;classification accuracy;decision boundary;prior knowledge;pattern classification;classification accuracy;strongly correlated;classification problem;discrimination power;classification problems;classifier
large number of;algorithm selection;decision trees;parameter) selection
data collected by;synthetic data;attribute dependencies;personal information;real-world data sets;privacy-preserving) data mining;personal information
vertically partitioned;association rule mining;computation cost;privacy-preserving;association rules;efficient approximate;vertically partitioned data;privacy-preserving data mining
text documents;learning algorithm;extraction rules;high recall;filtering techniques;information extraction from;sliding-window
ensemble learning;ensemble learning;unknown word;word recognition;prediction accuracy;real-world;naïve bayes;data set;instances;corpus-based;corpus-based;word recognition
hybrid approach;extraction process;multiple sequence alignment;hybrid approach
text mining;semantically similar;natural language;accurately identify;semantic level;semantic similarity between;sentence-level;semantic structure;semantic structure;similarity measures;natural language
clustering;web pages;clustering algorithm;document clustering;instances;web mining;data mining;web mining;entity extraction
text classifier;unlabeled documents;labeled documents;unlabeled documents;documents retrieved;text classifiers;text classifier;query terms
topic distributions;baseline methods
related documents;statistical models;personal preferences
semi-supervised learning;unlabeled data;limited memory;semi-supervised learning algorithms;semi-supervised learning
semi-supervised learning;labeled examples;supervised learning;single view;real data
classification scheme;feature space;kernel-based;classification;distance measure;probability density function;neural network;data sets;feature vectors;classification;input data;kernel method;higher dimensional;classifier;kernel-based
kernel learning;relevance vector machine;1,2,3,4;generalization performance;vector machine;model called;sparse kernel;bayesian framework;support vectors
high dimensional feature spaces;cluster membership;clustering high dimensional data;constrained clustering;data items;prior knowledge;clustering structure;metric learning;clustering methods;dimensional data;semi-supervised;benchmark data sets;high dimensional space
wikipedia-based;concept-based;semantic knowledge;similarity measure;clustering documents;document datasets;document clustering;concept-based representation;semantic relatedness;document representation
clustering;conceptual clustering;data mining;hierarchical clustering;data types;distance-based;conceptual clustering;clustering problems;real numbers;distance-based
sequence alignment;algorithm iteratively;information theory;biological sequence;real data;knowledge discovery;rates;statistical analysis;high accuracy;comparative analysis;data generated from
synthetic data;sequential patterns;real-life;formally defined;sequential pattern mining;sequential pattern;transactional data
pattern sets;data mining;post-processing;selection techniques;large amounts of data;machine learning techniques;result set;classification task
attribute values;density-based;data mining techniques;uncertain attributes;uncertain data;uncertain database;database;sufficiently large;traffic management;sensor databases;application domains;recognition systems
clustering;clustering;tree based;statistical dependence;clustering algorithms;complex networks;computational savings;discovery algorithm;spanning tree;clustering algorithm
regression methods;feature selection;support vector machines;planning tasks
planning domains;learning task;ai planning;shared information;formal language;action models;transfer learning;action models
pruning strategy;rule mining;frequent itemset mining;candidate set;limited number of;interesting rules;class association rules;association rules;formal framework;interestingness measures
breadth-first search;action rules;massive data;negative examples;disk-resident;discovery algorithm;action rules;naive bayes classifiers;real-world data sets;rule discovery;discovered rules;unlike conventional;positive examples
phase space reconstruction;input data;input features;rates;data base;membership functions;fuzzy rules;neural network
association mining;hierarchical structures;databases;data items;pruning method;databases;association rules
sentence level;classification;sentiment analysis;product feature;feature-based;product review;sentiment analysis;sentiment classification;opinion mining;corpus-based
high utility;high utility;pruning technique;pruning technique;tree-based;pattern mining;pattern mining;transaction databases;pattern mining algorithms;pattern growth;database scans;level-wise;candidate generation;high utility;frequent pattern mining
prediction accuracy;class information;access latency;user behavior;highly accurate;prediction method;multi-label;web page;conditional random fields
pair-wise;ranked list;commercial search engines;query suggestion;real-world;effective search;query suggestion;candidate set;graph analysis;log data;query-url
tracking method;navigation patterns;navigation patterns;web usage
tree structures;classification problem;link structure;class models;tree-based;naïve-bayes;classification process;hidden markov models;web pages;web mining;hidden markov model;fast algorithms;sampling technique
corpus based;maximum entropy;classification;language model;semantic features;supervised learning methods;vector machine;naïve bayes;feature extraction methods;classification performance;emotion recognition;maximum entropy;text classification;semantic information;music retrieval;recognition accuracy;emotion recognition
real dataset;tracking method
search results;web-based;web search results;similarity measure;semantic similarity between;semantically related;fine-grained;search engines;web search results;coarse-grained;clustering
real-world datasets;parameter values;outlier detection;real-world;detection approach;theoretical bounds;distance-based;detection methods;real-world;detecting outliers;outlier factor;distance-based
data objects;hybrid approach;high-dimensional;space utilization;distance function;real data sets;space utilization;distance-based
high dimensional;outlier detection;data set;outlier detection;databases;dimensional data;high dimensional feature space
data objects;high-dimensional;skyline queries;synthetic datasets;database;query returns;skyline computation;skyline queries;data mining;preference queries;multi-criteria decision making;skyline objects
base classifiers;naïve;naïve bayesian;training examples;decision tree;accuracies;bayesian classifiers;boosting methods;bayesian classifiers;bayesian classifier;base classifier;naïve;error rate
logical reasoning;context information;music recommendation;based reasoning;domain specific;music recommendation;domain-specific;low-level;case-study;enabling users to;high-level;context-based
visual-words;content based image retrieval;cosine similarity;gaussian mixture model;retrieve images;image features;feature space;semantic level;word vectors;image representation;pair-wise;data set;visual words;color information;weighting scheme;image retrieval;content-based image retrieval;local descriptors;content-based image retrieval
real valued;lower bound;computationally intractable;rating data;probability models;em algorithm
unique features;special characteristics;vertical search;mobile search;query recommendation;mobile search;search engine;query recommendation
intrusion detection;intrusion detection;unsupervised clustering;data mining;unsupervised clustering;data mining
clustering;end-user;valuable knowledge;data streams;isolation level;multi-resolution;1;4;6;credit card;clustering algorithm;formal framework;automatically discover
clustering;change analysis;clustering algorithm called;spatial data;case study;change analysis;spatial datasets;change analysis;density function;density functions;estimation techniques
mr) images;image segmentation;spatial constraints;magnetic resonance;spatial constraints;mr images;neural network
real data sets;ensemble techniques;multi-classifier;theoretical analysis;classifier;diversity measures;classification algorithms
nearest neighbor
web pages;parallel algorithm;link structures;search engine;web page;hyperlink structure
citation graph;linkage-based;retrieval results;information extracted from;biomedical information retrieval;ranking algorithms;retrieval performance;information retrieval performance;information retrieval;citation graph;probability model
evaluation criterion;image sets;classification;feature values;real-world;search strategy;originally designed;feature selection algorithm;feature selection;similarity-based;real world;machine learning and data mining;benchmark data sets;feature selection
mining task;training data;classification;feature construction;training classifiers;class" attribute;feature construction;fault-tolerant
mutual information;feature subset;classification accuracy;rough sets;feature subsets;user defined;feature selection methods;feature subsets;rough sets
high computational cost;similarity measure;data types;similarity measurement;similarity search;time series;indexing method;similarity search;time series;low computational cost;distance metric;data type;time series
data mining tasks;time series;time series;dimensionality reduction techniques;large margin;dimensionality reduction;classification problems;dimensionality reduction
clustering problem;graph structure;data points;data stream;data streams;clustering algorithm;clustering data streams
data generation;decision tree;classification accuracy;real-world;data stream mining;random sample;data stream mining;data streams;concept based;relevant information;memory consumption
data stream;matching algorithm;large margin;global constraint;data stream;algorithms produce;computational cost;matching problem;space complexity
prediction accuracy;learning algorithms;base classifiers;real-world;data streams;model averaging;classifier ensemble;noisy data;data streams;ensemble methods;mining data streams;data chunks
pairwise classification;applications including;collaborative filtering;kernel matrices;predictive performance;adjacency matrix
parameter space;genetic programming;svm-based;search methods;optimal kernel;text classification;mapping function;classifier;benchmark datasets
training data;input data;recognition performance;classification tasks;prior knowledge;incremental learning;learning vector quantization;threshold-based;classifier;learning vector quantization
web applications;taking into account;applications including;strong correlation;information processing;evaluation criteria;real-life;user-generated content
attribute values;automatically extracted;classification;class values;manually labeled;equivalence relations;regression model;classification quality
stream mining;data distribution;incremental mining;data mining
clustering;anonymization techniques;social network data;privacy preserving;data utility;privacy preserving
data mining;web mining;usage data;usage data;data mining
streaming applications;data formats;window-based;anomaly detection;communication network;data model;high-order;tensor analysis;2;5;4;7;time-series;pattern discovery;matrix decomposition;online algorithms;data arrive
machine learning and data mining;medical data;breast cancer;medical data
breast cancer;breast cancer;linear svm;evaluation criteria;post-processing
classification;data points;medical data;ensemble method;location-based;breast cancer
data mining;workshop on data mining;international workshop on
workshop report;kdd08-workshop;data mining;acm sigkdd;data mining
massive data sets;data analysis;acm sigkdd;future directions for
workshop report;multiple information sources;acm sigkdd international conference on;knowledge discovery;data mining;multiple information sources
workshop report;acm sigkdd international conference on;data mining;data protection;international workshop on;knowledge discovery;data mining
knowledge discovery from sensor data;knowledge discovery;knowledge discovery from sensor data;problems require;data mining;acm-sigkdd;international workshop on;sensor data is;emerging patterns;acm sigkdd;knowledge discovery;common interests;data streams;high end;sensor data
acm sigkdd international conference on;large number of;social network;social network;knowledge discovery;poster session;data mining
workshop report;knowledge discovery;acm sigkdd international conference on;data mining;knowledge discovery
clustering;singular value decomposition;product recommendation;real-world applications;data set;sales prediction
recommender systems;collaborative filtering;23;preference elicitation;real users;simulation framework;personalized recommendations;recommender systems;low quality;information theoretic
search results;higher precision;ontology-based;keyword-based search;search paradigm;diverse set of;query-independent;gene ontology;topic areas;context-based;digital libraries;query results;context-based
real-world
image retrieval;search space;total number of;step forward;real life data sets;linear regression;user feedback;relevance feedback;multimedia retrieval;random accesses;sequential scan;total cost
synthetic data sets;search space;low cost;communication cost;query response time;multi-dimensional;single-dimensional;query processing in;query processing;skyline queries;query load;tree structure;load balancing
time-series;temporal dynamics;high accuracy;modeling approach
tree structures;heuristic approaches;sufficient conditions for;tree-structured data;tree patterns;tree pattern;query language;tree-pattern queries;keyword-based;multiple data sources;data sources;tree-structured;query containment;structural information;huge volumes of data;query languages;querying capabilities
completeness;database;real data;spatial queries;user queries;dynamic databases;outsourced databases;data owner;margin;processing queries
network lifetime;power supply;knn queries;query processing in;sensor networks;query processing;energy efficiency;adaptive algorithm;sensor nodes;object tracking;query point;nearest neighbors
distributed computing;cws;early stage;keeping track of;case study;service-oriented;xml-based;web services;high level;composite web services;composite web services
sql query;squared error;sampling-based
adaptive query processing;online algorithms
mining frequent sequences;mining frequent sequences;sequence mining;large databases;mining process;parameter settings;frequent sequences;processing cost
window size;real-world scenarios;sample size;memory requirements;sliding window;data stream;data streams;nearest neighbor search
approximate answers;real data sets;wavelet decomposition;data values;synopsis construction algorithms;tree structure;error metric;large data sets;squared error;compression scheme;greedy algorithms;real-world data sets;space budget;optimal set of
probabilistic latent semantic analysis;query times;efficient storage;storage space;information retrieval;term frequency;latent semantic;semantic information;high precision;document set
data structure;disk space;large quantities of;general-purpose;wide range;sort order;large sets of;disk-based;disk-resident
stream processing;continuous queries over;high-level;spatial data;financial data;real-world;performance gains;query types;real data;query operators;computational overhead;query processing;data streams;query results;error bounds
pre-defined;event-condition-action;query evaluation;optimization opportunities;large volumes of;event processing;semantic query optimization;pre-computed;long running;stream processing;complex patterns;event streams;performance gains;future events;highly scalable;complex event processing
data streams;high computational complexity;similarity measure;data streams;high speed data streams;stream data;data streams;time-series;real numbers;pattern matching
database;image data;range data
randomized response;multi-objective;optimization method;categorical data;randomized response;data mining;privacy-preserving data mining
network models;location-based queries;distance matrix;real-life;network connectivity;spatial features;shortest path;wide range;location-based services;dynamic environment;network data;relational dbms
domain knowledge;domain knowledge;schema mapping;real-world;data migration;real-life;data migration;data migration;schema mapping
rdf data;database;rdf graphs;rdf graphs;arbitrarily large;oracle database
schema mapping;vice versa;multiple target;instances;data transformation;operator;data flow
instance;web pages;web-site
fully automatic;instance;data integration;high-level language;high-level;data integration;programming language
clustering;transaction data;dimensional data;real-life datasets;anonymization algorithm;computational overhead;privacy-preserving;privacy-preserving data publishing;dimensional data;data utility;relational data;information loss
privacy guarantees;data publication;real data;privacy preserving;sensitive values;privacy threat
document detection;existing protocols;information disclosure;document detection;similar documents
high utility;public data;synthetic datasets
query optimization;plans;planning problem;query optimization;web services;query processing;join query;query plans;operator;optimal set of;join queries;distributed environment
estimation method;large-scale;large-scale;overlay network;distributed data;high efficiency;sampling algorithms;power-law;size estimation
tag tree;data records;data analysis;pattern recognition;data records
image search;image recognition;processing power;human computation
relational) database;large parts;open-source;design parameters;database;database architecture;design space;high-risk
complex queries;decision-making;remote sources;select-project-join;query processing;query plans;data management;query processing
web portal;data collection;sampling algorithm;sql server;spatio-temporal;web portal;users' queries;designed to support;data management;sensor data;query performance;spatio-temporal queries;sensor data
pre-defined;approximation error;adaptive sampling;sensor networks;sensor network;real-world data sets
data arrives;stream processing;stream processing;replication;semantic issues;operator;data flow;stream-processing
tree matching;join algorithm;tree edit distance;xml data;data-centric;unordered tree;data sources;approximate join;data-centric;real world;data integration
probabilistic approach;hidden markov models;hidden markov model;time series;detection algorithm
real datasets;social network;user-defined;minimum description length;network evolution;data description
structural heterogeneity;database schemas;database
data-driven;data cube;data warehouses;olap queries;real-world;data warehouse;hierarchical structure;dimension hierarchies;olap queries;trend analysis
clustering;clustering algorithms;main memory;recommendation systems;sampling- based;clustering methods;large number of;large datasets;data matrix;data matrices;decomposition methods;microarray analysis;implicit assumption;large datasets;computational cost;matrix decomposition
multimedia databases;similarity measure;feature representations;user feedback;similarity estimation;similarity estimation
hidden patterns;mining algorithms;frequent patterns;pattern mining;emerging applications;interesting patterns;algorithm called;tree based;approximate match;sequence databases;existing database
schema mapping;database;information integration;fundamental problem;data examples;design process;schema mappings
instance-based;schema-based;extracted features;schema matching;schema matching;information extracted from;scoring functions;genetic algorithm;query logs;aggregate functions
xml schemas;schema mapping;schema mapping;data instances;data-sets;instances;mapping generation;high-level;schema mappings;data integration;schema elements
real data;string transformations;record matching;user-defined;record matching
optimization technique;web interfaces;remote sources;conjunctive queries;large number of;data sources;query plan;query plans;data sources;answering queries;queries posed;query processing;pose queries;relational tables
highly relevant;monte carlo;random sample;markov random fields;product descriptions;markov chain
clustering;similarity search;classification
privacy preserving;memory requirements;1;2;information leakage;secure multi-party computation;privacy preserving
social networks;privacy threat;social network
database;large number of;databases
completeness;content distribution;conventional techniques;xml content;xml documents;information leakage;incremental updates
data record;multi-objective;optimization problem;data set;individual privacy;data privacy;human subjects;evolutionary algorithm;information loss
skewed data;response times;fixed-length;hash-based;table scan;database;query processors;sql queries;query performance;materialized views;query processing;compression scheme;ad hoc queries
trajectory patterns;prediction model;prediction methods;ad-hoc;moving objects;query processing;query processing techniques;access method;moving objects databases
motion pattern;nearest neighbor;temporal constraints;query evaluation;computation cost;motion pattern;moving object;spatiotemporal data;generic framework;moving objects;indexing scheme;query type;generic framework;wide range
motion capture;temporal data;motion sequences;high dimensional;temporal patterns;databases;time series;human motion;distance measures;feature vectors;query processing;databases;query processing techniques;high dimensionality;human motion;temporal sequences;content-based retrieval;times faster than
parameter-free;spatial datasets;lower bound;decision support;multi-dimensional;join operators;join result;join operator
high speed data streams;data arrive;streaming data;data streams
database;data confidentiality;databases
sensitive data;personal information;sensitive data
streaming applications;cluster-based;streaming data;adaptive clustering;1;incoming data;data sets;privacy preserving;streaming data;data streams;anonymized data
sensor readings;1;data stream;data streams;2;heavy hitters
similar queries;streaming applications;real-world;stream data;query plan;network management;aggregate queries over;data streams;extensive simulations
prediction accuracy;operator;data stream processing;stream processing;fault-tolerant;online learning;low overhead;fault-tolerant;data stream processing
synthetic data;integrating data from;real-world;community members;data sources;integration systems;multiple sources;online communities;online communities
information-theoretic;multi-column;schema matching;data sets;databases;database integration;database schemas;multi-column
feature space;search space;prohibitively expensive;data points;typically performed;strongly correlated;local linear;projected clustering;local linear;feature subsets;feature selection;dimensional data;dimensional data;biomedical applications;numerous applications
density-based;high quality;detection algorithm;outlier detection;outlier detection;trajectory data;high efficiency;outlier detection;data mining task;database;line segments;partitioning strategy;distance-based
clustering;clustering;synthetic data sets;statistical measures;large number of;mining process;data stream;uncertain data streams;uncertain data streams;data management applications
complex event;event streams;event streams
taking into account;multi-objective;ad-hoc;aggregation function;user interests;ranking function;decision support systems;aggregated data;skyline queries;result set;data records;aggregation functions;algorithm to compute
query results;skyline query;skyline objects;skyline queries
random samples;indexing scheme;random sample;wide range;large datasets;random sampling
uncertain data;probabilistic threshold;probabilistic threshold;4;uncertain data
uncertain databases;uncertain databases;memory usage;query processing in
clustering;multi-dimensional;database;data warehouse;query performance;fact table;product features;query performance
database;performance gains;data layout;real-life;scientific databases;future queries;query results;segmentation algorithm
dimensional data;expression patterns;high throughput;model called;gene expression analysis;real data;subspace clustering;order preserving;order preserving;order preserving;subspace clusters;cluster models;gene expression data
rule) mining;classification methods;fp-tree;frequent patterns;classification;classification accuracy;pattern mining;pattern mining;mining process;discriminative patterns;computationally expensive;instance;minimum support;training instances;effective classification;problem size;pattern set;frequent pattern mining;frequent pattern;huge number of;feature selection
mining frequent itemsets from;computational complexity;mining frequent patterns;window size;data streams;static data;frequent itemset mining;data streams;association rules;sliding windows;mining tasks
fundamental properties;formal framework for;signature schemes;web forums;real data;case studies;network traffic;ad hoc;communication networks
plans;sampling techniques;aggregate query;prior knowledge;random sample;bayesian framework;selection predicate
black box;query types;individual queries;multiple queries;database
bitmap index;false positives;auxiliary;real-world;space constraints;data store;disk storage;query load;data management systems;excellent performance
data generation;large-scale;query processing in;distributed environments;sensor networks;query processing;data streams
query-answer;similar results;social network;real data;social networks;similarity searching;social networking;similarity query processing
query evaluation;markov chain;monte carlo;sampling algorithm;database;aggregate queries;aggregate query;databases;aggregate queries;real data;linear regression;databases;query answering;efficient query evaluation;avoid redundant;network topology;random sampling
handle complex;complex queries;join queries;join predicates
data collection;wireless sensor network;monitoring queries;monitoring applications;sensor networks;monitoring queries
discriminant analysis;pattern recognition;efficient computation;linear discriminant analysis;information processing;information retrieval;discriminant analysis;graph analysis;computationally expensive;data mining;theoretical analysis;average number of;machine learning;separability;regularization;real world data sets
query results;score based;efficient computation;query processing techniques;online shopping;web applications;efficiently computing;selection conditions;query answering;query results;representative set
search results;high volume;search result;sponsored search;search result pages
data sets;high-quality;approximation quality;reduction techniques;data reduction;error metric;large data sets;approximation guarantees;error metrics
filtering algorithms;real data sets;data cleaning;query relaxation;inverted lists;similarity functions;edit distance;query string;filtering techniques;cosine similarity;approximate string
join algorithm;query evaluation;xpath queries over;large-scale;space overhead;twig queries;xml documents;join algorithms;lower bounds;space complexity;incur high;pattern matching;space complexity
algorithm finds;complex queries;optimizer;query optimizer;highly structured;search algorithms;xml data;xml databases;increasingly large;xml database systems;ibm® db;optimizer
motion pattern;spatial data;mobile computing;mobile devices;multi-resolution;retrieval methods;buffer management;wireless network
video sequence;hash-based;multimedia applications;copy detection;multiple continuous queries;large number of;bit vector;data stream;monitoring queries;memory requirement;efficient computation;index structure;continuous queries;cpu cost;video streams
plans;join results;join operators;disk-resident;data sources;rates;join query;join query;query plans;operator;join queries;join operator;plans
query optimization;computing infrastructure;optimizer;query optimization;distributed query processing;distributed computing;query language;sensor networks;query processing;sensor network;sensor networks
join operations;intermediate results;tree-based;complex event processing;data streams;high volume;deterministic finite automata;optimization techniques;memory consumption;complex event processing
rfid technology;supply-chain;incomplete data;data volume;inference techniques
sensor networks;minimum support;data aggregation
weighted) graph;query processing in;shortest path;nearest neighbor queries;refinement step;graph embedding
decision making;synthetic datasets;finding optimal;nearest-neighbor;spatial region;nearest-neighbor queries;nearest neighbors
set similarity;selection queries;data collections;data cleaning;index structures;selection queries;threshold algorithm;similarity functions;set similarity;semantic properties;similarity queries;tf/idf;queries efficiently;relational database;similarity measures
privacy guarantees;data generation;united states;original data;sparse data;source data;data anonymization;synthetic data
access control;database;specific set of;sql queries;conjunctive queries;sql queries
text mining;databases;original matrix;binary matrix;application domains;knowledge discovery;role based access control;basis vectors;unified framework;matrix decomposition;matrix decomposition;integer programming
computational complexity;multimedia databases;lower bound;large databases;high efficiency;similarity search;query processing;efficient similarity search;real world;completeness;small-scale
naive algorithm;join query;join algorithms;computation cost;query processing in;moving objects;computationally expensive;query type;moving objects
view selection;materialized view;query rewriting;encoding scheme;xpath queries;heuristic method;multiple views;multiple view;materialized views;view selection;multiple-view;single view;answering queries
authority flow;user's preferences;databases;query reformulation;authority flow;answering queries;rates;authority flow;ranking mechanism;biological databases;query results
transitive closure;directed graphs;directed graph;reachability queries;database
mining algorithms;frequent subgraph;graph mining algorithms;graph patterns;closed frequent;support threshold;graph databases;graph patterns
query optimization;large numbers of;scalability issues;rule-based;database research;blog data;information extraction;regular expression;large data sets;data sets;specific characteristics;optimization strategies;information extraction;rule-based;real-world
spatio-temporal databases;database
dimensional data;search methods;nearest neighbor;distance function;high dimensional;uncertain data;index structures;index structure;distance measures;similarity queries;query-processing techniques;uncertain data;distance functions;high dimensionality;synthetic data sets;large number of;range queries;high dimensional space
similarity query;data analysis;classification;query pattern;private information;sequence matching;search problem;data mining tasks;time series;time series;original data;privacy preserving;location-based services;pattern matching;individual users
completeness;pull-based;data sources;pull based;data sources;emerging applications;complex data;pull-based
desired behavior;clustering;web sites;monitoring data;classification;database;declarative queries
sensor networks;completeness;data delivery;optimization problem;data delivery
data objects;similarity query;data analysis;pruning technique;similarity search;cost model;information retrieval;data set;similarity search;similarity search;time-series;search problem;pruning power
domain-independent;nearest neighbor;hash functions;retrieval accuracy;sample data;index structures;trade-offs;indexing method;distance measures;vector spaces;distance-based;locality sensitive hashing;real-world data sets;nearest neighbor;efficient approximate;similarity indexing;distance-based
emerging area;location-based queries;location-aware
join algorithm;information systems;database size;data structures;anomaly detection;compact representation;large data sets;search effort;join result;similarity joins;similarity joins;point-sets
approximation algorithms;frequent itemset;greedy heuristics;mining algorithms;synthetic data;ad-hoc search;np-complete;retrieval algorithms;databases;ranking functions;integer programming
cluster-based;optimization approach;graph pattern matching;real-world applications;graph data;graph pattern matching;join) algorithm;data management
event logs;high quality;high-order;classifier;data stream;evolving data;high order;high accuracy;classification error;prediction models;benchmark datasets;execution traces;evolving data;web traffic;limited number of
completeness;extracted information;real-world;text data;information extraction;text corpora;cost-based;real-world data sets;information extraction
web-based;web pages;language models;graph-based;query language;keyword-based;search engines;question answering systems;search engine;semantic information;query results;knowledge base;result quality
large graphs;indexing method;graph structure;approximate matching;large-scale;database applications;database size;scientific applications;index structure;graph matching;graph datasets;structural information;index size;real datasets;graph matching;graph queries;pruning power;indexing technique;matching algorithm
theoretical analysis;application code;database
concurrency control;distributed transactions;concurrency control
high-level;rfid data;event extraction;rfid data;event extraction
data collection;data updates;sensor node;collected data;data collection;sensor networks;error-bounded;sensor networks;error bounds
unified framework;web pages;information retrieval
query execution;structured databases;optimizer;search paradigm;query workload;page rank;index size;index size;random walks
high-quality;web pages;efficient discovery of
fall short;nearest neighbor;trade-offs;intermediate result;large quantities of;query performance;communication cost;search technique;location privacy;query processing techniques;query performance;knn) queries;location privacy;nearest neighbors;query processing
dimensional space;data publishing;spatio-temporal;wide range;greedy algorithm;inherent uncertainty;moving objects databases;clustering;anonymity preserving;data quality;ad hoc;high quality;data quality;individual privacy;relative error;pre-processing;range queries;database;moving object;anonymized data;increasing attention;moving objects databases;np-hard
uncertain data management;uncertain data;privacy-preserving data mining;data perturbation;uncertain data;anonymized data;personal information
data stream processing;source nodes;data stream processing;rates;result quality;high-volume;space partitioning;query results;data item;efficient construction
nearest-neighbor query;nearest neighbor;sensor monitoring;database;uncertain data;monte-carlo;upper bounds;probability density functions;high confidence;nearest-neighbor queries;location-based services;uncertain data;uncertain objects;biological databases;probability values;query point;computationally expensive
attribute-level;relational algebra;uncertain databases;uncertain data;query evaluation;logical level;relational database management systems;large amounts of data;processing queries
data sharing;service provider;store data;digital data;data sharing;access control;key management;data confidentiality;cost-effective;storage management;metadata management;stored data;storing data;high speed
replication
query execution;optimizer;plans;access methods;databases;plan quality;low overhead;cardinality estimation;database administrators;real world;poor quality;microsoft sql server;estimation errors
clustering;high accuracy;automatically extracting;schema matching;extraction rules;web crawlers;machine-learning;online databases
keyword queries;materialized views;xml data;query evaluation;materialized views;answering queries;keyword search over;keyword queries;structured queries
fixed point;xquery expressions;transitive closure;user-defined;operator;wide range;operator;relational databases;fixed point
sql/xml;query evaluation;query execution plan;data source;xpath expression;cost-based optimization;query optimization;data streaming;query plan;performance degradation;concurrent execution;operator
indexing approach;index entries;xpath queries;xpath queries;indexing approach
data warehouse;data warehouse;ad-hoc;xml data;database;data model;join processing;tree structure;olap queries;data warehousing;query processing;xml query processing;stack-based;query processing
streaming applications;security constraints;access control;access control;streaming data;query plan;data stream management systems;data streams;aware query
query execution;continuous query;result-set;data streams;data stream applications;data stream;data stream management system;data streams;network traffic;update cost
high dimensional data streams;data stream environment;classification;classification;high dimensional;classification process;data structures;data streams;instance;data sets;data stream;classification method;data streams;high dimensional;online analytical processing;classification approach;data mining algorithms;traditional classifiers
materialized view;auxiliary;database;real-world;maintenance costs;data warehouse;solution space;genetic algorithm;base table
open source;data warehouses;data availability;main-memory;user requirements;data warehousing;source data;concurrency control
probabilistic databases;avoid redundant;relational queries;database;confidence values;data items;probabilistic databases;query processing;query plans;relational databases;query results
depends crucially on;conjunctive queries
optimization techniques;data item;data items;continuous queries
probabilistic data;tuple level;efficient representation;database;uncertain data;database;relational operators;information retrieval;imprecise data;sensor databases;discrete data;inherent uncertainty;numerous applications
classification;training data;polynomial space;gene expression data;classification;association rule mining;cross validation;rule-based;classifier;computationally expensive;classification accuracy;parameter tuning;worst case;large datasets;association rules;data mining problems;gene expression;rule-based;gene expression data
data structure;xpath processing;construction cost;path expressions;xml data;approximate query processing;edit distance;communication cost;structural information;xpath processing;processing cost;query languages;distributed systems
regular expression;web collections;xml collections
correlation-based;semi-structured data;semantically meaningful;outlier detection;outlier detection;detection approach;data cleaning;hierarchical structure;correlation-based;relational data
medical records;information discovery;domain ontology;xml documents;xml elements;xml-based;aware search;keyword search on
query-independent;high speed data streams;large volumes of data;communication costs;query node;data stream;instance;distributed processing;query execution plans;data stream management systems;data streams;query-aware;query-aware
rewriting algorithm;large number of;analytic processing;optimization opportunities;aggregation queries
large data streams;data stream processing;data stream;space requirement;data streams;sliding windows;sliding windows
large distributed;sliding window;exploratory analysis;query response time;aggregate queries;optimization algorithm;decision support;aggregate queries;partitioning scheme;main idea;takes into account;optimizer
mining results;data mining techniques;sensitive information;privacy risks;data anonymization;privacy-preserving data publishing;negative association rules;data utility;algorithm to compute;data anonymization
automatic extraction of;database;text documents;databases;amazon mechanical turk;annotated data;high precision and recall;text databases;databases;automatic extraction of;user studies;interesting items;text databases
importance sampling;database;sampling-based;rfid data;support queries;prior knowledge;incomplete data;application scenarios
viral marketing;user interactions;specific domains;network resources;np-complete;spam filtering;graph traversal;data set;communication network;real-life;social networks;social) network;social networking;set-expansion;communication networks
hybrid approach;real data sets;record linkage;real-world;computation costs;privacy concerns;cryptographic techniques;sensitive data;private information;data sets;real-world entities;matching accuracy;high accuracy
relevant information;provenance information;workflow systems;view based;increasingly popular
multi-dimensional;multi-dimensional data;additional information;mobile devices;location-based service;index size;location-based services;location-based;construction cost;data management;data owner
multi-dimensional;dimensional space;data cube;database;unified framework;incremental maintenance;query processing;skyline queries;preference queries;preference queries;decision making
variable size;database;preference queries;databases;preference queries;performance gains;digital libraries;preference queries
text mining;join algorithm;high-dimensional datasets;multimedia databases;graphics processing units;graphics processing;algorithm called;similarity joins;search problem;similarity join;similarity joins;clustering
instance;test queries;query language;query language;instances;test query;query processing;test database;test cases;query result;database instance
search results;business activities;information access;business process;database;document search;information extraction;business process;information access;concept based;relevant information;semantic search;information retrieval;unstructured data
xml data;underlying structure;regular expressions;xml collections
domain knowledge;human effort;data integration;probabilistic xml
web applications;data formats;data services;database;databases;programming interface;programming model
mobile user;data collection;location-aware;large-scale;data distributions;mobile devices;mobile devices;sensor measurements
preserving privacy;social network;anonymization techniques;social network data;social networks;high accuracy;preserving privacy;relational data;social networks
sql queries;maximum number of;semantically related;1;database updates;detection methods;np - complete;functional dependencies
synthetic data sets;communication overhead;data distributions;local constraints;network monitoring;real-life;queries efficiently;probability estimation;large-scale distributed systems
score function;search space;optimization technique;query efficiency;basic algorithm;indexing structure;indexing structure;dominant relationship;skyline points;high dimension;query returns;graph traversal;search efficiency
multiple criteria;skyline queries;large-scale;parallel processing;skyline queries;algorithm named;data set;distributed processing;distributed environments;relevant data;query processing;skyline queries;skyline query processing
low-cost;minimal overhead;location-based;query load;query results;limited resources
clustering;clustering;high speed data streams;data volume;synthetic datasets;storage space;raw data;original data;stream data;distributed data streams;algorithms for computing;computing power;data transmission;distributed data streams;distributed clustering
database systems;evaluation strategies;uncertain data;real world datasets;upper bound;probability theory;data management;database systems
continuous query;intermediate results;execution plan;data stream management system;operator;memory resources;memory consumption;continuous queries
probabilistic modeling;data arrives;probabilistic models;probabilistic database;particle filters;sequential monte carlo;streaming data;probabilistic data;streaming data;databases;allowing users to;relational database system;real datasets;query processing
resource sharing;web-based;web service;active xml;long running;data-centric;data-intensive
management systems;scientific data;scientific applications;relational data;additional information
supply chain;large) number of;distributed monitoring;xml data;fault management;highly dynamic;xml document;data streams;distributed monitoring
streaming algorithms;large volumes of;sliding window;user-defined;traffic monitoring;network traffic;disk-based;processing speed;monitoring systems
plans;query plans;rates;data stream management systems;data streams;optimization process;key features;statistical models;data stream management systems
data processing;business processes;data stores;business processes;business processes;data processing
spatial data;multimedia database;physical environment;database
skyline algorithms;search space;interesting items;skyline queries;incomplete data;personalized services;large numbers of;data items;optimization techniques;multi-dimensional;skyline query processing;skyline queries;incomplete data;missing values;decision making;designed specifically for
special structure;spatial join;spatial datasets;spatial joins;probabilistic data;spatial joins;databases;data management systems;speed-ups
concurrency control mechanism;open-source;application programs;serializability;serializable
database;trade-offs;join processing;database;computational overhead;databases;join queries;resource usage
database;block size;database queries;distributed applications;data transfer;web services;databases;data transfer
pattern-based;real-life datasets;stream mining;data mining;raw data;stream mining;data streams;semantic constraints;frequent pattern mining
medical applications;data rates;signal processing;stream processing;data stream management system;data type
cost-based optimization;query optimization;query processors;optimizer;plans;database;data warehouse;data warehouses;execution strategies;semi-join;query plan;query plans;query patterns;data warehousing;decision support queries;pattern matching;join queries;microsoft sql server
query workload;analyzing data;database operations;processing power;database
inter-transaction;inter-transaction;data sharing;false positive;database management systems;database;fully operational;commercial dbms;fine-grained;database management system
search results;multi-dimensional;information systems;semi-structured data;query interface;heterogeneous data;search tools;multi-dimensional;ir-style;allowing users to;text content
integration systems;heterogeneous data sources;integration systems
database technology;database;information management;data items;case study;databases;biological databases;database engine;biological data
data mining;microarray data;database;gene expression;database;clinical information;sample sizes;gene expression
mining views;decision trees;database;data mining;mining views;association rules;relational database;data mining;relational databases
clustering;web information retrieval;discriminant analysis;web pages;image retrieval;search result;image search;semantic analysis;post-processing;search results;search result;knowledge acquisition;web search results;search engine;document summarization;web page;huge number of
data processing;xml processing;xml data;efficient algorithms to;real-life;query processing;data transfers
memory footprint;query evaluation;xml streams;query answers;update streams;query results
matching algorithms;efficient search;query engines;main memory;xml data management;tree structured data;xml documents;low-complexity;matching problem;speed-ups
large-scale;query execution;query processing strategies;text-based;sql queries;real data sets;text documents;sql queries;information extraction systems;information extraction;query processing;select-project-join;cost-based;text databases;text database;sql query;execution strategies;structured data;result quality;text databases
database;real-world;web communities;data sources;community members;user effort;automatic methods;integrate data from
keyword queries;spatial data;keyword search on;keyword search on;excellent scalability;text data;information retrieval;indexing structure;nearest neighbor search;spatial databases
web-based;medical research;web browser;web browsers;medical images;data model;medical images;web browser;image annotation;image annotations;image annotation;distributed environment
data services;xquery language;heterogeneous sources;aqualogic data services platform
data model;inference engine;database application;inference engine;user-defined;user-defined;data stores;data store;rule based;oracle database
declarative queries;optimizer;data structures;declarative queries;execution strategies;data structures;execution plans;specific characteristics;query processing techniques;internal structure;programming languages
index maintenance;microsoft sql server;data management systems;heterogeneous data;data storage
building blocks for;service-oriented;data management
structured data from;business intelligence;unstructured data;relational database
domain-specific knowledge;distance functions;threshold values;distance function;data mining techniques;threshold-based;data mining tasks;time series;time series;data mining;threshold-based;huge number of
document collection;intermediate results;search applications;ground truth;semantic search;1;3;2
detection problem;training data;high-dimensional data streams;multi-objective;genetic algorithm;effective search;high-dimensional data streams;data stream;window-based;data streams;detecting outliers;supervised learning;unsupervised learning;plans
data sharing;pattern recognition;storage devices;scientific data;search engines;data mining tools;information technology;information access;data management;plans
computing infrastructure;higher-level;complex systems;service providers;data management systems;data management
data publishing;data anonymization;inference techniques;negative association rules;privacy requirements;estimation methods;data anonymization
mining results;private information;data mining results;aggregate information;private information;privacy concerns;original data;privacy risk;association rule mining;data publishing;data publishing;maximum entropy;association rules;association rule
distributed computing;database systems;access control;database server;minimum number of;database;high confidence;tree data structure;databases;query result;hash functions
information systems;knowledge bases;database;np-hard problem;large margin;semantic graph;large graphs;biological networks;analysis tasks;building block for;approximation algorithm;entity-relationship;online communities;main-memory;relational data;semantic relations between
latent dirichlet allocation;topic model;great potential;topic model;model parameters;global information
nearest;dynamic programming algorithm;access method;skyline points;distance-based;algorithms for computing;np-hard;distance-based
clustering algorithms;database;queries efficiently;decision support systems;similarity-based;operator;clustering algorithm;optimization techniques;instances;application scenarios;execution times
set similarity join;pattern recognition;similarity join;large-scale;similarity threshold;real datasets;similarity joins;web page;similarity join;similarity joins;data integration
data cube;real-life applications;large number of;typically involves;actionable knowledge;data mining system;data mining;data mining algorithms;actionable knowledge
relational data sources;virtual machine;data services;update processing;aqualogic data services platform;data sources;automatic generation of
context information;pair-wise;large-scale;query recommendation;prediction accuracy;search engine;search engine logs;search engines;query sequence;query recommendation;prediction algorithms;search intent;search logs
business intelligence;commercial systems;data warehouses
xml documents;xpath queries;xml documents;internet-scale;relevant data;hash table;internet-scale;xml indexing;pattern matching
search systems;databases;large databases;information stored in;domain independent;databases;faceted search
attribute values;data owners;database;data model;data quality;database;query answering
personalized search;web graph;link structure;transition matrix;search engines;query answering
hybrid approach;search algorithms;link information;materialized views;keyword query;information needed;query execution;related terms;storage requirements;search systems;search results;corpus based;random walk;starting points;high quality;pre-computed;link graph;high recall;algorithms require;large graphs;query processing;databases
web pages;web sites;deep web;duplicate elimination;web application;search result quality;search engines;search engine;internet applications
scheduling algorithm;budget constraints;query processing;return results;higher quality;query processing
multiple databases;real-world;aggregate queries;schema mappings;simulated data;databases;answer queries;query answering;schema mappings;data integration;large number of
high-quality;large-scale;data cleaning;real-world;efficient algorithms to;ad-hoc;domain-specific;declarative framework;very large datasets;theoretical guarantees
human users;attribute values;database;join queries;query log analysis;enterprise-wide;input-output;selection conditions;query log;join query;databases;data analysts;quality measures;ad hoc;join queries
attribute values;traditional olap;business decisions;sql queries;olap queries;wide range;business intelligence;probability distributions;data records;aggregation queries over;query answer;databases;query answering;multiple sources;query results;aggregation functions;lower bounds
recommendation methods;relational data
mining framework;pattern mining;influential users;databases;pattern mining;mining process;1;social networks;real-world;pattern discovery;graphical interface
highly-interactive;high-quality;data items;olap operations
ranking algorithm;share common;user requirements;web application
social network analysis;business intelligence;helps users;social networking
entity search;web search engine;search engine
search results;graph structures;search engine;multi-resolution;business processes;web services;search engine;relational data;keyword search on
data structure;sponsored search;vice versa;document retrieval;index structures;inverted files;sponsored search;query processing algorithms;retrieval techniques
clustering;clustering;large numbers of;similar results;sketch-based;credit-card;data streams;clustering method;theoretical results;domain values;high probability;data streams;clustering algorithm;sales data
worst-case;hierarchical algorithm;data streams;network monitoring;multi-attribute queries;aggregate queries over;data streams;monitoring systems
filtering step;general-purpose;general-purpose;sensor networks;join processing;sensor networks
data analysis;data warehouses;efficient algorithms to;query language;data stream management system;fixed point
query evaluation;minimum support;aggregate query;bandwidth consumption;monitoring applications;sensory data;sensor networks;query processing;sensor nodes;poor quality;normal behavior
data analysis;low overhead;markov models;distributed stream processing;large-scale;classification methods;real-world applications;case study;data stream;high accuracy;mining algorithm
clustering;production systems;high-confidence;monitoring data;data-mining;database;anomaly-based;synthetic data;enterprise systems;problems arising
skyline queries;i: j;large number of;time series;time series;linear space;incremental maintenance;compact data structure;skyline queries;theoretical analysis;query answering;search trees;query returns
access control;query rewriting;specifically designed to;access control;data streams;streaming data;user query;1;2;role-based;access control model;data streams
clustering;production systems;high-confidence;monitoring data;database;anomaly-based;enterprise systems;synthetic data
wireless sensor network;ad-hoc;user interface;sensor network;graphical user interface;query processing algorithms;user-friendly
temporal dependencies;data sharing;real-world;data acquisition;environmental monitoring;sensor networks;high resolution;sensor network;data gathering
web-based;similarity measure;video data;video search;online video;video streams
communication overhead;moving object;spatial queries;road network;location-based services;graphical user interface;location-based
security policies;information systems;security analysis;access control;query language;information systems;management systems;logic-based;declarative framework;data analysis;query processing techniques;network routing;distributed systems;network provenance
data residing;semi-structured data;database management;privacy concerns;relational databases;transaction processing
query rewriting;intermediate data;sql queries;rewritten query;data model;data items;data model;attribute values;data item;provenance information;data provenance;relational database;relational databases;query rewriting;result tuples
information extraction;join algorithm;retrieval strategies;optimizer;join results;large-scale;real-world;output quality;analytical models;information extraction;text collections;quality-aware;execution plans;optimization process;real-world applications;text databases
web search;threshold values;real datasets;content filtering;text filtering;hash table;query logs;false dismissal;unlike conventional;distributed environment
mining closed;mining algorithms;case study;step forward;closed patterns;database;sequence data;sequential pattern mining;result set;complete set of;instance;sequence database;support threshold;protein sequences;execution traces;ordered list
distance computation;motion capture;summarization method;filtering step;database;time series;block-based;time series;motion sequences;refinement step;motion capture data;filtering algorithm
data objects;high-speed;skyline query;applications including;skyline computation;data elements;candidate set;data stream;skyline queries;skyline operator;sliding windows;multi-criteria decision making;sliding windows
stream processing systems;local optima;real-world;optimization model;area network;stream processing;data stream;cost models;performance goals;flexible architecture;diverse applications;processing queries
data objects;search space;computation costs;dynamically changing;incrementally maintaining;nearest neighbor queries;concept called;current location;nearest neighbor queries;query results;query point
query rewriting;information systems;schema evolution;database;database;theoretical results;database content;web information systems;mapping composition;database-centric;schema evolution;high cost
query load;information systems;query allocation;query allocation
keyword queries;keyword queries;salient features;keyword query;keyword query;relational databases;relational views
query evaluation;path expressions;query formulation;query interfaces;xml query languages;query performance;query processing;query formulation;xml queries;query results;life sciences;query processing
database systems;database;database;physical design;configuration parameters;statistical learning
multi-dimensional;query efficiency;numerical values;storage structures;real datasets;attribute values;query processing strategy;management systems;similarity search;sparse datasets;random accesses;indexing structures;low-dimensional
rdf data;efficient storage;evaluation strategies;sparql queries;database;query language;data set;access patterns;operator;arbitrarily large
naive algorithm;query term;scoring functions;information retrieval;match scores;scoring function;efficiently computing;information retrieval;information extraction
synthetic data;access methods;access methods;efficiently extract;markov-chain;real data;query performance;hidden markov model;sensor data;storage manager;query processing
large numbers of;sponsored search;sponsored search;sponsored search;high volume;user clicks;search result pages
stream processing;large numbers of;spatial indexing;large-scale;data cleaning;probabilistic model;sampling-based;observed data;supply chain management;particle filtering;location information;high-volume;key parameters;event streams;probabilistic inference;rfid technology;cost-effective;error reduction;incomplete data
shortest paths;provenance information;network routing;communication overhead;correct answers;view maintenance;domain-specific;stream data;sensor networks;data access;data provenance;distributed sources;stream systems;data management;distributed streams
clustering;user location;algorithms require;user privacy;distributed algorithms;mobile users
privacy preserving data publishing;data set;multiple users;privacy preserving;information loss
privacy-preserving data publishing;sensitive values;sensitive attribute;data anonymization;sensitive information
physical design;unique characteristics;data warehouses;data warehouses
query processing capabilities;semantic caching;efficient storage;database;dynamic environments;query operators;database;science applications;data-intensive applications;distributed database
data sharing;data sharing;data sources;data exchange;database
web-based;information systems;database;data owners;generalization hierarchies;personal data;databases
similarity query;set similarity;database;data cleaning;index structures;query answers;semantic properties;similarity queries;answering queries;real datasets;approximate matches
multi-dimensional;ranking results;nearest neighbor;database;dynamic environments;index structure;ranking problem;nearest neighbor;nearest neighbor search
high rate;similar results;sampling based;meta-data;brute force approach;data items;user query;user queries;multiple categories;data item;high accuracy;keyword search;keyword query;keyword search over;real world;processing power
data processing;network applications;individual nodes;sensor networks;sensor network;sensor networks;distributed database;high-level;low-level
fast approximate;probabilistic data;uncertain information;dynamic programming;database systems;query processing in;database management;uncertain data;data exploration;reduction techniques;error metric;probabilistic data;wavelet-based;error metrics;query planning;data synopses;probability distributions;data distribution
ranking queries;fundamental properties;pruning techniques;probabilistic data;ranking queries;uncertain data;attribute-level;probabilistic databases;probabilistic data;scoring function;traditional database;tuple-level;high cost;processing cost
service provider;spatial data;data points;query efficiency;attack model;cryptographic techniques;spatial queries;service providers;user-generated content;data security;location data;search services;social networking;data owner
nearest;query processing;data clustering;nearest point;point set
service provider;query execution;processing cost;communication overhead;query execution;query response time;data structures;minimal cost;outsourced databases;database outsourcing;query results;data owner
clustering problem;data points;high dimensional;high dimensional;projected clustering;projected clustering;data stream;dimensional data;uncertain data streams;uncertain data streams
large amounts of;query evaluation;matching algorithm;query execution;query language;spatio-temporal queries;location-based services;binary decision diagrams;processing requirements;location-based;location-based;continuous queries
large-scale;dynamic environments;schema matching;overlay network;data integration systems;database management system;network topologies;answer queries;inherent uncertainty;schema mappings;query processing
data objects;query retrieves the;spatial relations;skyline computation;skyline points;search region;query point
human activity
database;search paradigm;databases;ranked list;databases;relational database;query result
user navigation;xml retrieval;xml retrieval;structured documents;processing algorithms;search engine;evaluation measure;xml elements;search engines;logical structure;xml structure
information discovery;information exploration;community-based;relevant content;user behavior;social network;data model;3;information exploration;relevant information;user communities;community structure
partial orders;uncertain information;web search;synthetic data;applications including;efficient query evaluation;uncertain attributes;probabilistic model;query types;large databases;query answers;partial order;rank aggregation;total order;data integration
database;large number of;search queries;hidden databases;online databases;selection conditions
open-source;database;lock;sql server;serializable;concurrency control mechanism;concurrency control;serializable
persistent database
parameter-free;web pages;database systems;highly interactive;worst-case;database queries;scheduling algorithm;web page
graph-structured data;keyword queries;search algorithms;database;search paradigm;indexing techniques;keyword queries;data elements;data model;rdf) data;query processing;data structure;optimization techniques;database engine;keyword search on
search space;return results;query processing;graph based;keyword search;operator;keyword search;increasingly popular
join tree;plans;query engines;xquery processing;join operators;provide evidence;query optimizers;relational database;relational database system;xquery language
estimation accuracy;database;counting algorithm;estimation error;common practice;wide range;memory requirement;fundamental problem;sampling process;memory resources;rates;estimation errors
web applications;end users;database;database queries;database servers;web applications;network latency;application code
clustering;concise representation;interactive exploration;concise representation;communication costs;wireless communication;range query;spatial database;communication bandwidth;range queries;query results;range queries;range queries;information loss
social networks;data types;query language;social networks;implementation issues;social networks
private information;private information;user privacy;social networks;online social networks
blog data;large volume;high level;online social networks;real-world
providing users;common interests;recommender systems;social networks;recommender systems
social media;web content;highly scalable;data quality
prediction accuracy;user profile;user profile;private information;learning algorithm;social network;user profile;competing methods;network data
data structure;buffer management;multi-core;high-end;lock;database;data processing;lock;common practice
online aggregation;sample size;small sample;data streams;data stream;sampling process;data streams;approximation method
intrusion detection;query patterns;completeness;multiple target;graph streams;computational cost;subgraph isomorphism;traffic data;vector space;chemical compounds;dominant relationship;real time monitoring;graph streams;pattern matching;graph databases
data manipulation;database;intermediate results;query interface;sql queries;visual query;data manipulation;query modification;long) sequences;query interface;user studies
database;result quality;queries efficiently;concept called;relational databases;keyword search over;query processing;keyword search;ranking mechanism;relational databases;search efficiency;np-hard
sliding window;incoming documents;search queries;monitoring applications;user queries;search queries;rates;text filtering;queries efficiently;user interests;threshold-based;large number of
matching techniques;measure called;video databases;video data;edit distance;content-based video;optimal parameters;content-based search;data distribution
business activities;query evaluation;business process;business processes;algorithms for computing;query results
keyword queries;large data streams;database;intermediate results;high quality;information retrieval;high efficiency;query processing in;data stream;keyword query;high speed;database schema;demand-driven;keyword search on
query retrieves the;search space;rnn) queries;nearest neighbor;rnn queries;decision support;query point;data set;nearest neighbor queries;query processing;data mining;profile-based;real world;pre-processing;resource allocation
clustering;uncertain data management;performance degradation;clustering uncertain data;uncertain data;management systems;query processing;data mining;clustering uncertain data;data management;data storage
mining results;classification;data uncertainty;uncertain data;rule-based;categorical data;real-world applications;rule-based classification;algorithm called;probability distribution function;uncertain data;network latency;excellent performance;classification algorithm
clustering;synthetic data sets;streaming applications;high quality;data model;probability distributions;data mining;clustering algorithm;uncertain data streams;uncertain data streams;data generated from
traditional databases;twig patterns;database;uncertain data;probabilistic xml;xml documents;query processing;xml document;databases
uncertain data;feature selection;data mining;class labels;pre-processing;feature selection
mining frequent itemsets from;mining algorithms;plays an essential role in;real-life applications;frequent itemsets;uncertain data;data streams;tree-based;frequent itemset mining;frequent itemsets;real-life;transaction databases;uncertain data
heuristic approaches;classification;uncertain data;data mining;probability distribution over;anonymization techniques;data sets;individual privacy;anonymized data;anonymized data;building classifiers;distributed data mining
decision tree classifier;uncertain information;decision trees;pruning techniques;decision tree;uncertain data;decision trees;probability distribution function
clustering algorithms;expression patterns;gene expression data;real-life datasets;cluster model;cluster model;expression data;time series;temporal coherence;expression levels;clustering algorithm
computational efficiency;feature space;automatically extracting;click data;query logs;automatic text summarization;large scale;data obtained from;learned model;similar features;noise reduction;search engine;user-centric;error rate
commercial database systems;query optimization;optimizer;plans
join algorithm;input tuples;join results;synthetic data sets;emerging applications;join algorithms;result tuples;join result;autonomous data sources;heterogeneous network;disk-resident
external sources;data streams;materialized views;data arrive;data warehouse
attribute-based;attribute-based;human movement;physical environment;information dissemination;relevant information;short-term;physical objects
complex data;completeness;multiple information sources
domain knowledge;clustering;biclustering algorithms;gene ontology;microarray data;biological processes;gene clusters;high quality;biological datasets;clustering methods;gene regulatory networks;increasingly complex
index structure;query optimization techniques;general-purpose;data items;data provenance;optimization techniques
data volume;usage scenarios;database design;relational database management systems;minimal overhead;performance tuning;long-term
response times;selection problem;open source;10;cost estimation;aggregation queries;optimal set of
query execution;high-quality;takes into account;scientific data;cost model;large amounts of data
database;sql tuning;sql tuning;performance tuning;database administrators;commercial database;oracle database
physical design;index selection;database;index selection
multi-version;genetic algorithm;data movement;schema evolution;performance gain;multi-version
cardinality estimates;statistical summaries;database;random samples;join operators;source data
selection predicates;estimation technique;database systems;fine-grained;access controls;sampling-based;fine-grained;queries efficiently;database;cardinality estimation;access controls
search results;tf*idf;xml document;xml data;great success;xml keyword search;scoring function;information retrieval;xml keyword search;ir) style;text database;xml database;search engine;ir-style;result quality;keyword search on
structural heterogeneity;xpath queries;selectivity estimation;query relaxation;xml repositories;communication cost;compact data structure;queries efficiently;communication costs;construction cost;query relaxation;structural similarity
synthetic data sets;xml streams;limited memory;sketch-based;xml data;aggregate query;streaming xml data;xml queries;sketch-based;join query;real-life;event streams;xml stream;xml query
error-prone;object classes;fully automatic;joint probability;probabilistic model;semantic knowledge;maximum entropy;multimedia content;digital cameras;recognition systems;fully automatic;human input;image annotation;poor quality;increasingly popular
search space;itemset mining;semantically related;data cleaning;closed itemsets;effectively identify;functional dependencies;functional dependencies;manual effort;relational data
query evaluation;protein function;real-world;scientific data;data elements;knowledge discovery;exploratory queries;data integration systems;query results;motivating application;data integration
query evaluation;optimize queries;distributed databases;query processing in;database management system;join query;network traffic;join queries;query processing strategy
service provider;database operations;low cost;database;database management;data sets;data outsourcing;computationally expensive;private data;service providers;privacy preserving;data management techniques;data outsourcing;encrypted data;fault-tolerant;data management;data storage
multi-level;service calls;service calls;naïve;query execution;web service;database queries;query execution plan;query execution plans;execution plans;web services;operator;incur high
error-prone;domain-specific;language called;service-oriented;web services;composite services;composite service;semi-automated
data formats;web users
data values;query evaluation;xml query;query language;scoring functions;user preference;relational database;relational data
query distribution;structural properties;data sources;decomposition techniques
xml stream;data collections;real-world;distributed settings;xml schema;data-centric;scientific workflow;type inference;coarse-grained;programming paradigm
search performance;database;storage utilization;competitive performance;immortal db;commercial database;database engine
confidence values;input data;confidence values;probabilistic databases;join algorithms;data item;query results;theoretical guarantees
sufficient conditions;set semantics;database;conjunctive queries
dimensional space;uncertain data;location based;location based;filtering techniques;uncertain data;dimensional space;verification framework
multiple users;iterative process;database;real data;preference queries;preference queries;single object
query patterns;matching algorithm;publish/subscribe systems;ontology based;publish/subscribe systems;user profiles;ontology based
web databases;web query interfaces;web query interfaces;database;semantic similarity;semantic similarity;short texts;formal representation;query conditions;query interface;data integration
graph partitioning;rdf data;large scaled;open data;exact matching;indexing scheme;wikipedia articles;data stores;semantic web;query processing;tree index;structured data from;instances;query engine
access control;cost model;fine-grained;service-oriented;business operations;cost models
service provider;large-scale;huge amounts of;business model
data set;internet-scale;replication;data centers;data center;high scalability;internet-scale;high availability;programming models
service-oriented;business model;business logic;business model
service oriented;database;storage space;virtual machines;grid computing;data stores;service providers;high cost
query execution;execution times;database queries;long-running queries;capacity planning;database;test queries;workload management;accurately predict;data warehouse;long-running;machine learning;real customer;correctly identify;machine learning;resource usage
real data sets;index size;indexing structure;data cleaning;query relaxation;compression techniques;query performance;user queries;query processing;indexing structures;approximate string;approximate queries;approximate string
database;confidence scores;text documents;prior knowledge;large number of;relational tables;query processing algorithms;text databases;extraction task;information extraction from;text database;information extraction;text processing;text databases
probabilistic databases;database;conjunctive queries;probabilistic databases;case study;query plans;exact computation;query plans;operator;functional dependencies;tuple-independent;query results;tuple-independent;query engine
singular value decomposition;high accuracy;privacy-preserving;building blocks for;privacy-preserving;vertically partitioned data
major components;privacy concerns;service providers;instance;shortest path;road network;user privacy;query processor;search services;path queries;query processing
functional dependencies;data sets;verification problem;functional dependencies
data publication;sensitive data;high confidence;theoretical analysis;increasing attention;privacy threat
multiple databases;false positives;record linkage;data exchange;record linkage;databases;record linkage is;record matching
query results;concept hierarchy;databases;large number of;cost model;search queries;databases;information overload;query results;search interface
shortest paths;network distance;hash table;average error;nearest neighbor search;sufficiently large;spatial queries;road networks;spatial networks;location-based services;spatial joins;relational database system;spatial network;nearest neighbors;spatial networks
real data sets;tree based;database;maximum number of;disk-resident;data set;instance;nearest neighbor queries;brute-force search;spatial databases;query point;spatial databases
monte carlo;probability density function;range query;query performance;gaussian distribution;query processing;spatial databases
search space;query response time;database;spatial objects;search strategy;effective pruning;search strategies;queries efficiently;keyword query;keyword search;query keywords;spatial databases
skyline computation;database operations;parallel programming;skyline computation;case study;skyline algorithms;parallel algorithm
ad hoc queries;optimization criteria;real dataset;query answering;ad hoc;sensitive attributes
simulation results;sensor readings;sampling approach;aggregate query;hardware technology;huge number of;monitoring applications;sensor networks;sensor networks;query results;energy-efficient;fixed-size
result sets;recommender systems;attribute-based;data set;limited number of;result set;real world
intensive workloads;search performance;lower level;tree index
data records;data allocation;sensitive data
query specification;instance;generic framework;input query;partially ordered;skyline queries;partially ordered
high-quality;user experience;question answering;huge amounts of;final ranking;user-generated content;cluster-based;profile-based;probabilistic model;online communities
expected number of;approximation algorithms;total number of;online social networks;approximation quality;probabilistic model;social network;local search;np-complete;network size
keyword queries;partial information;database;structural information;real datasets;efficient algorithms to;relational database;relational databases;keyword query;relational databases;keyword search on
users' interests;location-aware;high quality;propagation model;computational resources;temporal constraints
virtual environments;computational complexity;scalability problems;instance;massively multiplayer online games;computationally expensive;virtual worlds;action based
probabilistic databases;query evaluation;graphical models;highly structured;real-world;query evaluation;streaming data;perform inference;monitoring applications;highly correlated;sensor networks;query processing;query operators;operator;prior works;real world applications;query results;stream processing
data points;outlier detection;outlier detection;traffic data;outlier detection;real world;traffic data
synthetic data sets;data streams;wide range;frequency counting;frequency counting;operator;data streams;frequency counting;stream queries
clustering;clustering;evolving data;web search;instance;search log;execution traces;similar sequences;instances;dynamic programming;benchmark datasets;internet traffic
estimation accuracy;parallel algorithm;low frequency;sketch-based;multi-core;parallel processing;counting algorithm;high-frequency;limited space;vector data;query processing;hash functions
approximation algorithms;plans;distributed monitoring;synthetic datasets;large-scale;finding an optimal;np-hard;communication network;query processing and optimization;computing environments;communication cost;data movement;sensor networks;query processing;query plans;general problem;communication cost;communication networks;query processing
intrusion detection;persistent storage;event streams;event processing;supply chain management;partial order;complex event processing;event streams;pattern queries;stream processing;complex event processing;query processing
simulation data;high accuracy;database;spatial distance;algorithm to compute;query processing;tree index;data structure called;dimensional data;high-level;building block;mathematical analysis;scientific databases
dynamic environment;mobile devices;small-scale
privacy-aware;context-aware;spatio-temporal;database server;context-awareness;case studies;database engines
rewrite rules;sql/xml;database management systems;materialized views;materialized view;xml data;query language;queries over xml;ibm db;xml database systems;storage systems;xml queries;sql") queries;optimization techniques;relational queries
xml storage;xml processing;database;xml applications;real-world;databases;xml data management;xml data management;xml databases;document-centric;data-centric
replication;commercial database;database;source database;replication
similar queries;search systems;search services;specific queries
scientific workflow;np-hard;increasingly complex;management systems
medical records;information discovery;domain ontology;xml documents;xml elements;xml-based;keyword search;aware search
service provider;database;database applications;increasing number of;indexing scheme;performance degradation;databases;data management
domain knowledge;feature space;search space;frequent subgraph mining;feature set;frequent patterns;feature vectors;computation cost;scientific data;databases;large graph;highly scalable;patterns mined;large graph;wide range;significant patterns;databases;statistical significance;mining patterns;graph databases;classifier
context-aware;large graph;individual objects;large graphs;information flow;query answer
programming model;high rate;application domains;stream processing;environmental monitoring;data streams;code generation;high-rate
search results;multi-level;multiple queries;web search engine;internet users;query interface;search engine;search result quality;search result;wide range;search engine;medical knowledge;web search engines
unstructured information;unstructured information;database;real-life;business intelligence;business intelligence;structured information;structured data;unstructured data
program analysis;security problems;database;database applications;sql queries;static analysis;database application;data integrity;data access;analysis tasks;application development
duplicate records;end-user;detection techniques;past experience;domain specific knowledge;false positives;data set;knowledge acquisition;duplicate records;knowledge acquisition
user-profile;recommender systems;recommender systems;user-profile;personalized recommendation;large number of;1;filtering methods;similar users;collaborative filtering;collaborative filtering;4
traditional collaborative filtering;recommender systems;collaborative filtering;context-aware;rule-based;mobile applications;mobile applications
domain experts;relation extraction;domain ontology;knowledge management;multi-type;concept hierarchy;intelligent systems;web mining;theoretical analysis;relation extraction
data mining;query language;language called;data mining systems;data mining;oracle© database
web-based;information systems;dimensional space;data-mining;data cube;aggregate queries;data mining;multiple attributes;data-set;data-mining tasks;log-data;data cube
risk analysis;hash-based;classification;real-world;risk analysis;data mining tools;data mining;data warehousing
software development;machine learning methods;data mining techniques;quality assessment;machine learning applications;data mining;software quality
clustering;supply chain;ontology-driven;customer service;large number of;search space;large data sets;data sets;ontology-driven;association rule;radio frequency identification
mobile phone;service-oriented;plans;agent-based;service-oriented
prediction accuracy;access logs;recommendation systems;recommender systems;user sessions;pattern discovery;feature vectors;user sessions
network analysis;agent based;graph theory;web services;web services composition;user requirements;intelligent systems;dynamic systems;service discovery

information dissemination;end-user
sensitive information;social network;social networks;cryptographic techniques;2;personal data;access control model;personal information;social networks
point based;data values;data point;privacy guarantees;data compression;detection accuracy;time series;burst detection;detection results;data sets;data perturbation
genetic algorithm;data set;large data sets;data sets;fixed-size;location-based services;databases;protect privacy;database privacy;fixed-size;information loss
database;private information;sql query;sql queries;sql queries;2;np-hard
privacy policy;privacy policy;personal information;mathematical model
data point;nearest neighbor algorithm;training data;nearest neighbor;data points;nearest;centroid-based;similarity measure;cosine similarity;similarity metrics;text classification;data sets;text classification;classification algorithm;centroid-based;classification algorithm;classification algorithms
search space;concept discovery;search strategies;multi-relational;learning systems;user-defined;interesting patterns;instances;business intelligence;multi-relational data mining;business intelligence applications;relational databases;operator;multiple relations;concept discovery;decision making
generalization hierarchies;privacy-preserving;real-world scenarios;database design;databases;large databases;personal data;privacy-preserving;databases;preserving privacy
data mining processes;privacy preserving;time series;time series;privacy preserving;privacy issues;pattern discovery;frequent pattern;discovery problem;multi-party
network distance;spatial networks;road networks;private information;user privacy;spatial networks;mobile devices;euclidean space;location information;nearest neighbor queries;query processing;location-based services;real world;simulation results;range queries;query processing
privacy-aware;application domain
preference relations;partial orders;user preferences;large data sets;transitive closure;index structures;personalized ranking;user-defined;large data sets;partial order;preference queries;query results
specific applications;domain experts;image features;distance function;scientific domains;image data;domain-specific;training samples;image data
query evaluation;systems support;xml-retrieval;xml retrieval;structural constraints;evaluation strategies;xml data;queries efficiently;structural summaries;retrieval methods;management systems;answering queries;additional constraints;xml retrieval;keyword search;threshold algorithm
multi-dimensional;data analysis;database systems;data exploration;massive data;on-line analytical processing;computation model;implementation issues
database;post processing;rank/order;query answers;information integration;query processing;answering queries;information retrieval systems;information integration
pattern-based;ranking) functions;ontology-based;score functions;search paradigm;context based;context-based;text-based;ranking functions;semantic properties;gene ontology;context-based;digital library;ranking functions;test case;separability;context-based
context information;naive implementation;database;context-aware;information retrieval;probabilistic model;inherent uncertainty;query results;context-aware
similarity measures;12;xml processing;similarity measures
ranking queries;skyline queries;data collections;cost-aware;cost model;query processing capabilities;cost-based query optimization;skyline queries;skyline operator
web services;web service;rule-based;web services;markup language;low-level
core components;data interchange;core components;web services;core components;xml schemas
web service;case study;web services;formal specification;high degree of
web service;web service;automatically generate;quality control
service provider;cost functions;load distribution;web service;web service;web services;greedy algorithms
selection algorithm;users' preferences;multimedia applications;data distribution;network connectivity
domain knowledge;ranking techniques;semantic web;query terms;cost-effective
web service;service discovery;service oriented;application scenarios;web services
takes into account;business transactions;business process
pre-defined;service provider;database systems;fault-tolerance;xml repositories;xml documents;web services;atomicity;atomicity
medical records;access control model;information access;access control;access control;role-based access control;control mechanism;markup language;xml technologies
data services;semi-structured data;database queries;instances;wide range;web services
physical design;database systems;query optimizer;database;database tuning;database research;physical design;virtual machines;long-term;database workloads;resource allocation;total cost
control theory;databases;database management systems;database;formal models;databases;control theory;control theory;database researchers
load balancing;virtual machine;fine-grained;database;outlier detection;database applications;detection algorithm;database;data centers;database servers;coarse-grained;fine-grained;performance tuning;resource usage;database engine;database replicas;load balancing
database;relational database system;data management;database
database-centric;multi-tier
matching algorithms;highly structured;indexing scheme;automatically identifying;database
large space of;query optimizer;index selection;database design;cost estimation;integer linear programming;index selection;commercial database systems;optimization techniques;quality guarantees
selection problem;product development;query optimizer;database design;query plan;query processing;index tuning
physical design;index selection;query load;database
plan selection;query execution plans;black-box;plans;access methods;database;access methods;database;query optimizer;cost models;query optimizers;index scans;cost estimation;estimation errors
db2® universal database;poster session;database management system
semantically related;semantic mappings;large scale;large-scale;index structure;schema matching;schema matching;indexing structure;data sources;poster session;semi-automatically;human intervention
access patterns;database tuning;workload management;poster session;user's preference
database systems;predict future;making decisions;poster session;access patterns;database workloads;buffer size
discovered patterns;explicitly represented;spatial analysis;geo-referenced;large number of;great potential;classification rules;data mining technology;data mining;association rules;spatial data mining
gps data;location-aware;data mining techniques;discrete data;pose queries;location data;continuous data;obtain information
domain knowledge;frequent patterns;pattern mining;real data;spatial reasoning;spatial patterns;spatial patterns;spatial reasoning
aggregate information;data cube;data warehouses;spatio-temporal;data warehouse;aggregate functions;data warehousing;olap operations;aggregate function
spatial aggregation;database;on line analytical processing;aggregate queries;data model;moving object;data model for;moving objects;moving objects;increasing attention;moving objects databases;formal model;geographic information systems
spatial regions;spatio-temporal;temporal data;movement patterns;location-based services
spatio-temporal;mining algorithm;naïve;spatio-temporal;user-defined
clustering;distance metrics;classification;data types;spatio-temporal;database research;mobile devices;knowledge discovery;extracting knowledge from;clustering tasks;similarity query processing;databases;mining tasks
compact representation;mobile objects;clustering algorithm;wireless communication;similarity measure between;data allocation;memory space;mobile objects;web site;moving objects;data mining;incremental clustering;temporal data;execution times
query optimization;implicit assumption;continuous query;input data;instance;stream systems;distributed systems;query plan;query processing in;data partitions;rates;query processing;load balancing;instances;distributed systems;continuous queries
stream processing;stream processing;replication;semantic issues;operator;stream-processing
factors affecting;main memory;parallel processing;main memory;query processing;operator;decision making
data arrives;hybrid approach;real-world applications;continuous queries over data streams;data stream management systems;operator;multi-threaded;continuous queries
relational algebra;temporal sequence;execution model;time series;execution strategies;graph based;continuous queries
stream processing systems;long-running queries;stream processing systems;continuous queries;computational overhead;metadata management;adaptive query processing;metadata management
stream processing systems;synthetic data;data summarization;stream processing;data stream;network traffic;heavy hitters;size estimation
high-speed;sampling methods;data rates;general purpose;query-aware;large volumes of data;query sets;data feeds;large scale;data stream;data stream management systems;data streams;query-dependent;network traffic;query-aware;random sampling
pair-wise;multiple data streams;stream mining;multiple data streams;real-life;continuous queries;random sampling
security policies;information flow;business processes;xml-based;business processes
semantic web;security constraints;web service;designed to support;web service composition;semantic web;web service composition;composite web services
context-sensitive;access control;context-sensitive;service-oriented;web services;control architecture;contextual information;external sources
service oriented;formal specification;business process;information flow;xml-based;privacy requirements
desired properties;model checking;trust management;trust management;model checking;tool called;trust management;language called;security analysis;model checking;problem domain;access control;case study
speech recognition;features extracted;feature extraction;combined method;input parameters;speech recognition;neural network
span multiple;multiple users;information systems;multiple tasks;user sessions;access controls;access control;mutually exclusive;business processes;role based
question answering;information access
process models;mining patterns;process mining;document clustering;xml database
reduction techniques;independent component analysis;document frequency;high dimensionality;dimension reduction;document representation;dimensionality reduction;clustering problem;clustering results;text clustering;increasingly large;vector space model;clustering algorithm;benchmark datasets;natural language;text mining;clustering performance;feature selection technique;latent semantic indexing;dimension reduction;document representation;document representation
genetic programming;sequence patterns;document classification;document classification;temporal information;feature maps;data set;classification;competitive performance;feature selection techniques
product space;dimensional space;recommender systems
recommendation process;recommender systems;recommender systems
network structure;music recommendation;web application;recommendation systems;music recommendation
web pages;user interface;user interface;web browser;web browser;web page;quantitative analysis;positive effect
users' interests;web pages;users' behavior
bayesian network;recommender systems;recommender systems;group members;bayesian networks;rates;recommending items;bayesian networks
real-world;recommender systems;decision making;recommender systems;real-world applications
collaborative filtering;recommendation systems;mixture model;performance guarantees;extra information;information overload;additional information
security analysis;join queries
singular value decomposition;motion capture;feature space;medical data;motion capture;fuzzy clustering;motion capture data;extracted information;multi-dimensional;feature vectors;dimensionality reduction
surveillance video;relevance feedback;multiple instance learning;high level;database;learning algorithm;multiple instance;data mining;real-life;information retrieval;surveillance video;learning process;databases;dimensional data;video retrieval;learning framework;content-based image retrieval
data analysis;database schema;related information;data mining;image data;image analysis;great potential;information retrieval;expression levels;databases;information retrieval;image analysis;image processing
basic operations;data structures;indexing structures;multimedia retrieval;data sets;similarity computation;indexing methods;retrieval applications
edge detection;extraction methods;detection method;video indexing;video frames
music information retrieval;retrieval performance;rates;correlation coefficients;music retrieval;b1/b2j
search performance;index structures;artificial data;index structure;similarity search;multi-modality
illumination conditions;large scale;databases;news video;knowledge discovery;news video;databases;statistical machine translation;appearance variations;clustering analysis;correlation based;clustering model
domain knowledge;association mining;video sequence;content analysis;multimedia applications;association mining;video databases;temporal patterns;skewed;event detection;plays an essential role in;association rule mining;temporal information;event detection;video content;high-level;multimedia data;video indexing;data distribution;audio-visual
20;database;real-world;database security;frequent itemsets;problem statement;extracted knowledge;building block for;association rules
signal processing;ubiquitous computing;human-centered;human centered;machine learning;human activities;computing systems
static data;application area;streaming environment;database;static data;streaming data;data quality;data stream;sensor data;relational database;data quality;persistent database;persistent storage;business decisions
surveillance systems;context-aware;personalized services;multi-sensor;task-specific;multi-sensor;surveillance systems
great flexibility;relational schema;stream data;data stream;sql extension;data streams;continuous queries;increasing importance;sql extension;continuous queries
communication bandwidth;cost model;monitoring applications;communication cost;sensor networks;sensor data;storage requirements;cost model;sensor nodes;storing data;data storage
network bandwidth;data availability;cache management;continuous media;mobile devices;rates;cache management;mobile devices;hit rate
multimedia content;multimedia content;computing devices;context-sensitive;multimedia content
metric space;similarity measure;large number of;similarity measures
database administrator;database management systems;database;database server;prohibitively expensive;database management system
tuning parameters;database administrator;database management system;database
database administrator;database management system;load balancing
physical design;database administrator;database management system;database
virtual machine;database systems;database;virtual machines;virtual machine;database systems
multiple databases;database;databases;heterogeneous databases;performance tuning;database vendors;total cost
database administrator;poster session;database management system
physical design;database;database design;physical design;management systems;poster session;commercial database
systems require;hybrid approach;database systems;data warehouse;complex query;database servers;large data sets;business intelligence;data warehousing;poster session;business intelligence;building block;plan selection
database administrator;database management system
data analysis;ad-hoc;valuable information;data stored in;information services;data repositories
query evaluation;plans;data providers;spatio-temporal;dynamic environments;data sources;streaming data;service-oriented;multi-scale;traditional database;diverse range of;data integration;query processing
database administrator;database management system
agent technology;mobile devices;distributed environments;mobile agents;mobile agents;mobile agent
query results;information systems;information systems;mobile users;location-aware
database administrator;database management system;web services
mobile ad-hoc networks;intelligent agents;knowledge-based;ontology language;verbose queries;semantic web;description logic;artificial intelligence
relational databases;database administrator;knowledge bases;database management system
database administrator;database management system;data integration
information integration
information fusion;database administrator;data privacy;database management system
data integration;global schema
multi-domain;query answering;database administrator;database management system
search results;position paper;query rewriting;web search;web search engine;1;information integration;structured information;search engine;web search engines
database administrator;database management system
large-scale;query interface;information sources;data sources;data integration;information source
similarity measure;schema matching;case study;database management system;sufficient condition for;fully-automatic;data integration;similarity measures
multi dimensional;high dimensional;data-set;similarity search;vector spaces;range queries;real data-sets;nearest neighbors
high-dimensional;database administrator;similarity retrieval;database management system
search accuracy;large-scale;content-based retrieval;similarity search;efficient indexing;similarity search;sequence databases;biologically relevant;databases;bioinformatics research
visual similarity;visual similarity;partial information;sign language;visually similar;sign language
edit distance;database administrator;database management system
high-dimensional;database;storage space;theoretical results;vector spaces;metric space
data structure;similarity join;multidimensional databases;databases;similarity joins;metric spaces;metric space;nearest neighbor search
image retrieval;high-dimensional;nearest neighbor;distance function;answer set;database applications;search applications;search strategy;complex objects;nearest-neighbor;vector data;similarity-based;biological data;nearest neighbor search
database administrator;similarity search;database management system
database administrator;semantic web;database management system
virtual organizations;business opportunities;application scenarios
semantic web;application domains
database administrator;role-based;database management system;access control
domain experts;data formats;ad-hoc;ontology language;homeland security;semantic web;dynamic content;resource description framework;security concerns;development process;ontology based
database administrator;spatial domain;ontology based;database management system
document summaries;large scale;xml data;data representation;xml documents;real-life datasets;original document;xml document;user studies;large collections
database administrator;database management system
probabilistic databases;database administrator;database management system
relational databases;database;semantically meaningful;keyword search;relational databases
selection conditions;result set;database systems;relational algebra
uncertain data;database administrator;database management system
ranking functions;database administrator;database management system;user preference
search results;search terms;search tool;domain-specific;digital library;digital libraries;content-based search
relevance feedback;multimedia databases;database;content-based retrieval;database objects;feature-based;ranking function;relevance feedback;user preferences;databases;mathematical framework;similarity measures
high dimensional;outlier detection;decision boundary;scoring functions;ranking algorithms;data mining task;outlier ranking;dimensional data;decision making;categorical attributes
control mechanisms;poster session
database;data mining;social network;information retrieval;training sets;databases;prior probability;maximum likelihood;data integration
clustering;clustering;data stream clustering;density-based;data record;clustering algorithms;stream data;data stream;stream data;finding clusters;data streams;cluster structure;cluster boundaries;clustering quality;high-speed
link spam;web pages;link structures;target pages;real dataset;link-based;detection methods;ranking methods;link spam;web search engines;target detection
clustering;clustering algorithms;arbitrary shape;memory constraints;density-based clustering;high quality;streaming data;data stream;density-based clustering;stream data;data stream clustering;purity;data structure;data streams
instance;data accesses;skyline queries;multi-source;data points;query processing in;network datasets;instance;network distance;spatial networks;nn queries;lower bound;range queries
data objects;multi-dimensional;multi-dimensional data;real datasets;decision support;query evaluation;real data;ranking functions;multi-dimensional;skyline queries;data analysts;query returns
distance measure;longest common subsequence;similarity measure;indexing technique;time series;distance measures;shape matching;real world;data mining algorithms;shape matching;partial occlusion
attribute values;plans;maximum number of;distributed databases;join query;join queries;join predicates
data structure;query processing;computationally intensive;location-based service;nearest neighbors;tree construction;database outsourcing;spatial databases;data management;data owner
real data sets;quality guarantee;approximation methods;database;tree data structure;large databases;exact answer;theoretical guarantee;large data sets;cognitive science;linear complexity;query answering;synthetic data sets;instances;answer questions;quality guarantees
information systems;large-scale;query allocation;high efficiency;baseline methods;load balancing
large volumes of;data migration;highly skewed;clustering algorithm;cost model;query response times;access patterns;minimal overhead;rates;data structure called;high frequency;storage manager;multidimensional data;update-intensive;indexing methods
data objects;search space;nearest neighbor;database;uncertain databases;uncertain data;pruning method;data obtained from;nearest neighbor queries;query processing;moving objects;real world;nearest neighbors
query evaluation;sufficient conditions for;sql queries;sql query;bag-set semantics;query equivalence;sufficient condition for;set-semantics
multi-dimensional;frequent updates;disk accesses;location-dependent;theoretical analysis;concurrency control
inference method;bayesian methods;markov chain;unlabeled data;monte carlo;classification;semi-supervised learning;real-world;dirichlet process;multi-class;gaussian processes;generative model;semi-supervised learning;probability density;nonparametric bayesian;mixture models
perform inference;markov chain;monte carlo;gaussian process;generative model for;poisson process;bayesian inference;prior distribution;competing methods;real-world data sets;nonparametric bayesian;synthetic data
tree kernels;taking into account
domain knowledge;latent dirichlet allocation;real datasets;domain knowledge into;topic modeling;collapsed gibbs sampling;modeling methods
vector space;target distribution;theoretical properties;probabilistic models;principal component analysis;functions defined;real-valued;probability distribution;principal component analysis;inference algorithm;inference process
training process;global optimization;local minima;neural networks;machine learning
learning process;complexity bounds;binary classifiers;loss functions;active learning
variational inference;special case
policy gradient;partially observable markov decision processes;global optimal solution;finite-state;local optima
utility function;concept learning;decision support;user preferences
clustering;spectral clustering;graph laplacian;spectral clustering
parameter space;active learning strategies;predictive models;supervised learning techniques;active learning;physics-based;active learning framework;complex systems;information-theoretic
bayesian model;selection problem;posterior probabilities;dynamical systems;gaussian noise;systems biology;optimal design;kullback-leibler divergence;model selection criterion;model selection;information gain
data manifold;probability density functions;information retrieval;operator;modeling techniques;probabilistic latent semantic analysis;probabilistic framework;latent dirichlet allocation;hidden structure;graph laplacian;manifold structure;data analysis;social network analysis;dyadic data;real world applications;real data sets;takes into account;global consistency;dyadic data;geometrical structure;laplace-beltrami;euclidean space
score function;hill-climbing;bayesian network structure;global solution;structural constraints;global optimality;large data sets;knowledge based;dynamic programming;bayesian networks;score functions;structure learning of
binary classification;simple linear;classification;fully supervised;margin;sampling strategy;desired accuracy;classifier;instances;rates;upper bound;base classifier;semi-supervised;margin-based;noise model
clustering;clustering algorithms;multi-view clustering;canonical correlation analysis;principal components analysis;multiple views;link structure;lower-dimensional;random projections;document clustering;canonical correlation analysis;audio-visual;clustering
multiple tasks;generalization performance;alternating optimization;large data sets;feature representation;multi-task learning;globally optimal solution;theoretical analysis;multi-task learning;convex formulation;benchmark data sets
real data sets;kernel-based;classification;similarity matrices;optimization problems;reproducing kernel hilbert space;similarity-based;kernel methods;classifier;convex optimization problem;similarity measures
discriminative training;parameter space;gaussian distributions;multivariate gaussian;error rates;accuracies;learning algorithm;hidden markov models;convergence rates;automatic speech recognition
tree induction;instance-based learning;classification;decision tree;label ranking;probability models;ranking problem;instances
stable models;large number of;convex optimization;linear combination;prior knowledge;scene analysis;eigenvalue problem;linear regression
inference algorithm;tree-structured;graphical model;tree-structured;hidden variables
precision-recall;classification;statistical estimation;precision-recall;scoring functions;counterpart
transfer learning;unified framework;cross-domain;learning tasks;auxiliary data;learning problems;intrinsic structure;learning task
dimensional space;data sets;vector data;classification;clustering problems
close connection;structured prediction;unsupervised learning;expectation maximization;learning problems;high-quality;semi-supervised;supervised learning
discovered patterns;inductive learning;molecular biology;target domain;source domain;test instances;learning systems;social network;learned knowledge;markov logic;transfer learning
finite-sample;gaussian processes;dynamic systems;gaussian process;covariance matrix
theoretical analysis;low-quality;supervised machine learning;prior information;learning process
meaningful clusters;feature space;data points;noisy datasets;large number of;microarray data analysis;noisy data;clustering algorithm;desirable properties;clustering problems;clustering;real life applications;clustering
lower bound;sample-complexity;reinforcement learning;structure learning;feature selection;upper bound;benchmark domains;reinforcement-learning algorithm
regularization;designed specifically for;large-scale;performance guarantees;learning problem;objective functions;learning tasks;fast convergence;logistic regression;learning models;batch learning;regularization parameter;algorithms rely on;prediction models
convex optimization;learning algorithm;large margin;optimization problem;hidden markov models;labeled dataset
real-world datasets;scale poorly;inference techniques
latent dirichlet allocation;dcm;topic models;topic model
domain adaptation;decision function;auxiliary;domain adaptation;target domain;regularizer;domain adaptation;source domains;pre-computed;concept detection;multiple sources;classifier
gradient-based;parameter space;coordinate descent;regularization
unlabeled images;real-world datasets;active learner;large number of;magnetic resonance images;training images;real images
taking into account;continuous variables;classifier;bayesian classifiers;numeric attributes;error rate
gradient-based;bayesian framework;parameter estimation;convex optimization problem;learning models
inference procedure;evolving networks;gene regulatory networks;state space
gradient descent;gradient descent;iterative algorithm;equation
covariance functions;predictive distribution;posterior distribution;gaussian process;making predictions
learning algorithms;linear classifiers;bayesian learning
clustering problem;finding an optimal;vector machine;optimization problem;maximum margin clustering;stochastic search;large data sets;clustering accuracy;maximum margin;support vector machines;margin;clustering approach
learning agents;update rule;dynamic analysis
large scale;partial rankings;inference algorithm;ranking models;bayesian inference;data sets;maximum likelihood;expectation propagation;ranking model
clustering;case study;binary attributes;spam filtering;input space;optimization problem;instances;service providers;problem setting;feature vectors
convex optimization;portfolio selection;learning algorithms;adaptive algorithms;performance metric;online learning;algorithms produce;computational overhead;data-streaming
statistics based;reinforcement learning;learning speed;policy search
regularization;linear models;classification;feature selection method;standard svm;scaling factors;supervision;prior knowledge;data sets;feature selection;classification performances;microarray experiments;dimensional data;partially supervised;feature selection techniques
structured sparsity;greedy algorithm;statistical learning;feature set
synthetic data sets;data points;learning algorithm;linear dynamical systems;basic assumption;dynamic systems;data generated from
breast cancer;regularization;risk minimization;gene expression data;theoretical properties
prediction accuracy;artificial data;weighted graph;semi-supervised learning;matching method;learning systems;graph based semi-supervised learning;global consistency;supervised methods;wide range;graph based;graph construction;graph construction;benchmark datasets;inference algorithms;nearest neighbors
optimizer;feature selection;fundamental problem
multi-task;black-box;loss function;classification;special structure;convergence rate;learning tasks;multi-task learning;computationally expensive;learning problems;gradient method;semidefinite programming
learning framework
equivalence class;equivalence classes;adjacency matrix;belief propagation
protein structure;loss function;quality assessment;hyper-parameters;training data;protein structure;bayesian approach;approximate inference
language modeling
base classifiers;lower computational cost;collaborative filtering;computational efficiency;maximum margin;matrix factorization;domain-knowledge;semi-definite-programming;base learners;benchmark dataset;benchmark datasets;deep belief
clustering;real-world datasets;markov logic networks;local optima;markov networks;network structure;relational database;markov logic
simple algorithm;complexity bounds;high probability;reinforcement learning;bayesian approach
temporal difference;regularization;function approximation;temporal difference learning;irrelevant features;reinforcement learning;regularization;temporal difference;regularization framework;feature selection;convex optimization problem;computationally expensive
node labels;graph kernels
training set;monotonicity constraints;classification;rule learning;statistical analysis;class label;theoretical analysis
kernel learning;regularization term;regularization;numerical simulation;kernel functions;instance;gradient descent;positive definite;learning problem
singular value decomposition;original matrix;sampling methods;sampling-based;decomposition techniques;machine learning applications;sampling technique
regression problem;collaborative filtering;citation networks;spectral graph;graph kernels;large networks;dimensionality reduction methods;edge weight;social networks;unified framework;link prediction
level-wise;features including;inductive logic programming;classification learning
ranking algorithm;theoretical framework;ranking models;ranking algorithms;ranking performance;loss functions
action sequences;probabilistic relational;dynamic bayesian network;approximate inference;approximate inference;stochastic domains
high-dimensional;video data;motion capture;long range
collaborative filtering;gaussian process;stochastic gradient descent;matrix factorization;probabilistic matrix factorization;data sets;gaussian processes;latent variable models
high-dimensional;unlabeled images;object parts;natural scenes;unsupervised learning;probabilistic inference;generative models;belief networks;algorithm learns;generative model;recognition tasks;translation-invariant;deep belief;high-level;excellent performance;visual features
user-item;multiple tasks;collaborative filtering;rating-matrix;rating data;rating matrix;cross-domain;cross-domain;data sets;mixture model;generative model;real-world;cluster-level;multiple domains;transfer learning
multi-class classification
multiple kernel learning;unlabeled data;semi-supervised learning;highly competitive;closely related;alternating optimization;unlabeled instances;class labels;semi-supervised;support vector machines;margin;semi-supervised
sequence labeling;exponential family;inference algorithm;unlabeled examples;decision-theoretic framework;cost-effective
operator;case study;coordinate descent;multi-task;large-scale
worst-case;auxiliary;special structure;building block;learning problems
document collection;community discovery;large-scale;hierarchical approach;unified framework;social network;data sets;topic modeling;blog posts;high-level
kernel learning;pair-wise;metric learning methods;local geometry;kernel matrix;learning tasks;semi-supervised classification;generic framework;metric learning;low dimensional;manifold structure;semi-supervised;learning task;semi-definite programming;dimensionality reduction;model parameters
training data is;large-scale;learning approaches;web sites;classification accuracies;data set;online learning;online algorithms
signal processing;classification;dictionary learning;optimization algorithm;basis set;training samples;large datasets;machine learning;image processing;natural images;sparse linear
recurrent neural networks;td) networks;temporal-difference;partially observable;predict future;simulation experiments;temporal-difference;td networks;learning strategy;neural network
motion capture;graphical models;test set;optimization methods;sparse graphs;variational bayes;graphical models;map estimation;maximum likelihood;gene expression
computational cost;learning algorithms;learned model;max-margin;approximate inference;natural language
pair-wise;human perception;semidefinite programming;partial order;partial order;euclidean space;global structure;graph-theoretic;multiple kernels
monte-carlo;local maximum;objective function;optimization problem;performance tuning
unlabeled data;deep learning;temporal coherence;sequential data;recognition tasks;learning method
regression method;additive noise;statistical dependence;optimization problem;observational data;causal models
high-quality;machine learning methods;learning algorithms;reinforcement learning;algorithms typically;assignment problem
classification models;active learning;large scale;bayesian inference;generalized linear models;linear systems
graph partitioning;clustering;unsupervised learning;clustering problem;correlation clustering;combinatorial optimization;linear programming;optimization problem;machine learning;clustering aggregation;problems arising
factor analysis;linear combination;dirichlet process;underlying structure;inference algorithm;sparse set of
clustering;unsupervised learning;random variables;hierarchical model;motion capture data;algorithm learns;missing data;specific characteristics;unsupervised fashion;posterior distribution
state space;inverted pendulum;stochastic processes;real-world;reinforcement learning;reinforcement learning algorithm;action space;reinforcement learning algorithms;state-action
modeling assumptions;data sets;real world;time series;moving average
theoretical properties;performs poorly;linear programming;reinforcement learning;linear programs;poor quality
random variables;multi-class;image segmentation;classification;classifier;data set;global features;local image features;classification method;semantic segmentation;multi-class;conditional random fields;multi scale;conditional random field
real-world;optimizer;detection task
competitive performance;latent dirichlet allocation;conditional independence;topic models;topic models;topic model;prior model;text document;topic models;real data;latent variable models;inference algorithm;objective function;times faster than
regularization;learning algorithm;high-dimensional space;distance matrix;estimation problem;learning algorithms;semi-definite programming;coordinate descent;high dimension;algorithm exploits;distance metric;sparse metric learning;high dimensional feature space;high dimensional space
conditional random fields;sequence labeling;local features;feature set;optical character recognition;higher order;inference algorithm;potential functions;higher order;conditional random fields;data sparseness
image annotation;regularization;gradient method;multi-task
high-dimensional;clustering;vector space;classification;skewed;data set;vector spaces;nearest neighbors;dimensional data;high dimensionality;information retrieval;nearest neighbors
large-scale;unlabeled data;training examples;unsupervised learning;large-scale;massively parallel;learning algorithms;learning tasks;belief networks;machine learning applications;times faster than;learning models;parallel algorithm;highly nonlinear;vast amounts of
breast cancer;artificial datasets;ground-truth
probabilistic approach;majority voting;supervised learning;supervised learning
margin-based
partition function;markov random field;graphical models;real datasets;learning algorithm;higher-quality;structure learning;real-world domains;markov random fields;structure learning of;dependency structure;probabilistic graphical models
statistical significance;interesting patterns;support vector machines;database
lower-dimensional;high-dimensional space;monte carlo;gaussian process regression;linear regression;data set;predictive performance;gaussian processes;bayesian approach;markov chain
weight vector;data sets
dimensionality reduction;structure preserving;spectral embedding;structure preserving;kernel matrix;low-rank;structure preserving;euclidean space;dimensional data;topological properties;graph embedding;low-dimensional;nearest neighbors
monte-carlo search;reinforcement learning;policy gradient;gradient descent;supervised learning algorithms;main idea;monte-carlo simulation;squared error
domain knowledge;high-quality;labeled features;labeled features;active learning;general setting;supervision;uncertainty sampling;cost effective;text classification;labeled examples;labeled data;classifier;kernel-based
conditional distributions;belief state;dynamical systems
clustering;parameter estimation;access control;real-world;performance gains;observed data;data item;clustering methods;higher accuracy;noise levels
regularization;large-scale;conjugate gradient;spectral learning;linear discriminant analysis;generalized eigenvalue problem;machine learning;equivalence relationship;generalization ability;computationally expensive;canonical correlation analysis;major limitation;machine learning algorithms
convex optimization problems;instances;regularization;learning algorithm;support vectors
temporal difference;function approximation;gradient-descent;policy learning;learning rate;learning algorithm;temporal-difference learning;temporal-difference;convergence rates;update rule;related algorithms;computational requirements;objective function;algorithm converges;test problems
markov decision processes;fixed point;reinforcement learning
pattern classification;supervised learning;classification task
regularization;function approximation;gaussian process;temporal difference learning;reinforcement learning;reinforcement learning;gaussian processes;machine learning techniques;transition model
human motion;computational properties;exact inference;time series;hidden state
data point;auxiliary;data points;low variance;learning algorithm;markov chain;sufficient statistics
statistical methods;graphical models;constraint based;structure learning;distributed data
probabilistic model;local linear;inference methods;approximate inference;expectation propagation;maximum likelihood;optimal control
pairwise classification;classification;ranked list;margin-based;weighted average;optimization problem;information retrieval;benchmark dataset;classification approach;loss functions;high precision
kernel learning;regularization;multiple kernel learning;large scale;linear combination;learning tasks;optimization algorithms;gradient descent;feature selection;databases;real world applications;multiple kernel learning
mutual information;data points;data set;information theoretic;information theoretic;similarity measures
model-free reinforcement learning;model-free reinforcement learning;mixture model;em algorithm
energy function;pairwise constraints;probability distribution over;information retrieval;data set;ranking performance;scoring function;learning approaches;ranking functions;retrieved documents
data collection;clustering;data analysis;data-set;data structures
exact computation;topic modeling;topic models;evaluation metric
large scale;multitask learning;random subspaces;dimensionality reduction;high probability
joint model
sequence data;predictive performance;language model;pitman-yor;perform inference
clustering;principal components analysis;classification;unsupervised learning;semi-supervised learning;graph-cut;semi-supervised learning algorithms;instances;model parameters;semi-supervised;supervised learning
multiple kernel learning;global optimal solution;feature selection;feature selection methods;feature selection problem;informative features;combinatorial optimization problem;benchmark data sets
learning algorithms;classification;training examples;learning algorithm;convex optimization;online learning;originally designed;uci data-sets;online learning;positive definite;single classifier
stochastic search;search methods;fitness
structured output;latent variables;applications including;convex programming;optimization problem;information retrieval;structural svms;large-margin
decision problem;limited number of;stationary distribution;prior knowledge
prediction problem;prediction accuracy;regression tasks;large-scale;large scale;learning models;em algorithm;task-specific
information-theoretic;feature extraction;real-world data sets;class labels;quadratic optimization;objective function;information theoretic;manifold regularization
simulation results;user behavior;information retrieval systems;search engines;implicit feedback;learning framework;theoretical guarantees
training examples;probability distribution;coordinate descent;knowledge extraction;classifier;weak classifiers;standard datasets
learning process;reinforcement learning
image retrieval;unlabeled data;optimization framework;real-world applications;test instance;instance;unlabeled instances;labeled examples;effectively learn
complete model;graph-based;kernel matrix;unlabeled data;large-scale;large scale;semi-supervised learning;efficient approximation;vector machine;low-rank;benchmark data sets;machine learning;regularizer;data mining;supervised learning;graph regularization;highly scalable;information loss
clustering;object categorization;document classification;classification;classification tasks;fixed-length;application domains;complex objects;low-level features;discriminative information
instances;multi-instance learning;multi-instance learning;graph kernel
variational methods;maximum entropy;classification;topic model;topic models;data sets;low dimensional;max-margin;maximum margin;latent dirichlet allocation
real data sets;regularization;high dimensional;markov network;markov networks;desirable property;max-margin
kernel learning;computation cost;pairwise constraints;large sets of;data sets;closed-form solution;iterative algorithm;semi-definite programming
world wide web;content analysis;social science;web sites;information science;dynamic environment
data mining and knowledge discovery;knowledge discovery;acm sigkdd international conference on;data mining;highly competitive;knowledge discovery;data mining;program committee;acm sigkdd international conference on
score function;classification rule;auc;classification;data mining techniques;face recognition;data set;data mining tools;score functions;data mining;classification problems
web pages;user experience;web logs;search engines;privacy issues;web logs
data mining results;data mining approaches;computationally intensive;analysis tasks;data mining;data mining research
data mining;data sets;case study;data sources;discover patterns;data mining
statistical approaches;theoretical basis for;social network;statistical) methods
emerging trends;predictive models;data mining applications;predictive model;data mining;business users;markup language;predictive analytics
monte carlo;web search;unified framework;recommender systems;large scale;accurately predict;latent factor models;stochastic process;dyadic data;latent factor model;latent factors;benchmark datasets;model fitting;online advertising
synthetic data sets;uncertain data;tree-based;pattern growth;frequent pattern mining;frequent pattern mining
gene ontology;topic model;major source of;data mining tasks;topic models;biological information;life science;knowledge extraction;collapsed gibbs sampling;life science;inference algorithm;information retrieval
decision trees;classification;training data is;large-scale;hidden markov models;biomedical research;classifier
regularization term;high precision and recall;data analysis;compression rate;data compression;anomaly detection;stochastic processes;information-theoretic;generative process;meta-clustering;meta-clustering;baseline methods;information theoretic;model-based clustering;detect anomalies
anonymity-preserving;data mining application;data collection;communication overhead;data availability;data collected
dynamic analysis;interaction graphs;static analysis;graph analysis;social networks;dynamic graphs
click data;data obtained from;rates;server logs;scheduling problem;web traffic
clustering;clustering;clustering results;large-scale;markov random field;text collections;optimization procedure;parameter tuning;clustering methods;consensus clustering;clustering quality
data analysis;interactive exploration;input data;temporal information;information contained in;sequential patterns;extracted knowledge;condensed representation
frequent itemset mining;uncertain databases;item(set;frequent itemset mining;frequent itemsets;transaction databases;addressing this problem;item(sets
binary classification;fully supervised;binary classifier;partial information;classification algorithm
real-world datasets;online mining;ensemble methods;data mining research;data stream;data streams;evolving data streams;single classifier
minimum description length;parameter-free;data compression;question arises;source code;outlier detection;data model;data set;parameter settings;data sets;real world;outlier factor;independent components;distributed data;data distribution
pruning strategies;index structure;limited space;moving object;distance-based;data processing;object tracking;high speed
variational methods;large corpora;real-world;topic model;text corpora;social networks;network data;approximate inference
text mining;multiple classes;domain adaptation;training data;unlabeled data;document classification;linear transformation;target domain;predictive modeling;information extraction;concept space;feature representation;source domain;statistical learning;labeled data
conditional likelihood;prediction accuracy;graphical models;conditional random field;sequence data;optimization algorithm;constrained optimization;crf model;optimization problem;cross-validation;generalization ability;prediction tasks;constrained optimization;maximum likelihood;conditional random fields
database;greedy algorithms;cascade model;social network;heuristic algorithms;5;social networks;7;greedy algorithm;approximation algorithm
large-scale;generation algorithm;learning algorithm;parallel algorithm;feature vector;ctr;large-scale;probabilistic model;data structures;hadoop mapreduce;user behavior;training dataset;skewed;sparse representations;fine-grained;caching scheme;regression model;machine learning problems;highly scalable
web graph;structural properties;data structures;web graphs;social networks;np-hard
search results;world wide web;ranking algorithm;ranking algorithms;digital library;long term
bipartite graph;regularization;query suggestion;data types;search applications;query logs;scientific literature;content information;regularization framework;data mining;cost function;log data;link information;hits algorithm
clustering;predictive accuracy;classification;large-scale;independent variables;real life datasets;heterogeneous data;modeling technique;dyadic data;making predictions;applications involving
labeling effort;active learning;labeled instances;amazon's mechanical turk;data mining tasks;prior knowledge;queries issued;wide range;upper-bound;labeling process;noise levels;learning task
real networks;real-world;surprising patterns;million records;social networks;large datasets;evolving networks;communication networks
large number of;training dataset;case study;digital library;learning problem;maximum likelihood;classifier
user preferences;user study;optimization problem;information overload;user's preferences;learning framework;blog data;user-interaction
text classification tasks;learning algorithm;target variable;linear classifiers;svm classifiers;pre-processing;uci datasets;classifier
spatial data;spatial aggregation;classification;spatial relationships;rule learning;multi-relational;spatial features;mining framework;learning models;class labels;real world;relational data
high-dimensional;computational efficiency;high-dimensional datasets;accurate models;markov blanket;random fields;probability distributions;bayesian networks;prediction tasks;theoretical analysis;statistical learning;probabilistic graphical models
learning algorithms;hypothesis testing;learning algorithm;sliding window;change detection;streaming applications;data streams;sliding windows;increasing importance
clustering;instances;heuristic method;information sources;unsupervised clustering;large scale;np-hard problem;textual content;classification accuracy;data sets;enormous amounts of;internal structure;supervised learning;quality measure;decision making
training data;prediction model;case study;customer service;learning problem;instance;accuracies;real-world;learning algorithms;support vector machines
clustering;clustering algorithms;data points;data manifold;clustering method;cluster labels;geometric structure;clustering methods;clustering;nonnegative matrix tri-factorization;benchmark data sets
high-quality;online social networks;knowledge-sharing;social network;activity patterns;question answering systems;question answering;power-law;information propagation
clustering;discovered patterns;cluster structure;data mining methods;sampling based;random samples;data sets;real data;data set;data matrices;exploratory data analysis;data mining;data mining process;pattern discovery
wikipedia categories;category information;text documents;similarity metric;external knowledge;clustering method;clustering performance;instance;content information;document clustering;exact match;clustering methods;clustering framework;semantic information;additional features;document representation;information loss
cold start;additional knowledge;collaborative filtering;recommender systems;random walk model;recommendation methods;filtering methods;collaborative filtering;user item
sparse learning;representation scheme;multiple views;text descriptions;temporal dynamics;optimization problem;text-based;visual recognition;feature representation;regularization framework;gene expression
set cover;concise representation;heuristic techniques;frequent itemset mining;frequent sets;frequent itemsets;frequent pattern
detection techniques;class noise;label noise;data cleaning;spam filtering;spam-filtering;noise model;classification problems;instances;classifier
attribute values;data analysis;pattern language;database;database size;global models;global models;data mining;relational database
recommendation approaches;collaborative filtering;recommender systems;temporal dynamics;data instances;instance;long term;collaborative filtering
clustering;hill-climbing;web pages;keyword-based search;related topics;web text;local optimization;linear programs;real-world entities;manually-annotated;np-hard
cost functions;social network;communication cost;social networks;combinatorial problems;np-hard
parameter-free;aware search;search process;event detection
training set size;learning curve;training examples;data mining processes;sampling strategy;machine learning;sampling strategies;data mining;data mining process;data points;benchmark datasets;pre-processing;cpu cost;sampling methods
clustering;mathematical model;named entities;quantitative analysis;temporal patterns
motion capture;missing values;latent variables;synthetic datasets;hidden variables
evaluation methodology;portfolio theory;data-mining;data publishing;machine learning;anonymization techniques;data utility;data anonymization
social media;multi-dimensional;community structures;user actions;community discovery;social media;real-world;multi-relational;prediction tasks;data collected from;social networks;baseline methods;tensor analysis;relational data;aspect model;factorization method;community structure;social context;community structure
instances;log data;query-url;modeling technique;exact inference
regularization;convex optimization;black-box;logistic regression;large-scale;classification framework;convergence rate;large-scale;step size;logistic regression;machine learning;line search;feature selection;classification method;data mining;dimensional data;applications involving
pattern-based;classification technique;discriminative features;classification;classification;pattern mining;software systems;real-life;case studies;feature selection;execution traces;classifier
high-dimensional;synthetic data sets;sample size;source code;generalization performance;sample data;feature selection algorithms;training samples;feature selection;real-world data sets;small sample size
microarray data;causal modeling;variable selection;gene regulatory network;large number of;time series;boosting methods;group structure;regression methods;real world;modeling techniques;modeling methods
data centric;real world;causal modeling;temporal data;spatial-temporal
hybrid approach;learning problem;graph structure;active learning;graph-based;real-world;learning algorithm;networked data;partially labeled;social network;instance;candidate set;labeled data;risk minimization;semi-supervised learning;semi-supervised;instances;classifier
clustering;communication patterns;observed data;classification;outlier detection;poisson process;inference algorithm;data sets;hidden markov model;model parameters;latent structure;network data
large-scale datasets;running times;complete set of;graph mining;feature set;large-scale;classification accuracy;tree mining;search space;large-scale;cross-validation;training sets;upper bound
privacy guarantees;recommender systems;learning phase;user behavior;differentially private;post-processing;data set;personalized recommendations;unlike prior
concise representation;trajectory patterns;temporal properties;decision tree;predictive power;pattern tree;matching methods;pattern mining;mobile devices;moving object;movement patterns;real dataset;case study;real life;moving objects;location based services
itemset mining;constraint programming;pattern mining;pruning algorithm;memory requirements;information gain
real data sets;parameter-free;fast algorithms;main idea;recommender systems
probabilistic latent semantic analysis;collaborative filtering;data points;large-scale;matrix factorization;data sets;maximum-margin;low-rank matrix approximation;collaborative filtering;positive examples
source code;biclustering algorithms;microarray data;categorical variables;real-valued;data set;data sets;pattern mining algorithms;major limitation;patterns discovered;association analysis
concise representation;false positives;frequent itemsets
search space;real datasets;error guarantees;frequent itemsets;fault-tolerant;general problem;fault-tolerant;support counting
provide evidence;social-network;browsing behavior;social network;user-generated content;social networks;predictive-modeling;data mining;social networking
clustering;optimization problem;negative feedback;data mining;algorithms typically;clustering quality
singular value decomposition;recommendation methods;personalized ranking;observed data;optimization problem;theoretical analysis;gradient descent;prediction method;optimization criterion;missing values;higher order
clustering;multi-level;synthetic datasets;community discovery;graph clustering
network analysis;modeling assumptions;social networks;dynamic networks;network data;pre-processing
clustering;linear program;regularization;binary data;approximation error;minimum set of;binary matrix;large number of;maximum flow;small-size;matrix factorization;instance;computationally expensive;theoretical bounds
monte carlo;27;single dimensional;data points;21;scan statistics;brute force;real world;scan statistics;hypothesis testing;likelihood ratio
social media;markov random field;human behavior;prediction performance;structural relationships;level features;user similarity;information seeking
causal relationships;time series;time series;data sets;wide range;numerical data;real world;modeling methods
clustering;clustering;clustering results;heterogeneous information network;topic model;heterogeneous networks;real-world;cluster membership;rich semantics;multi-typed;hidden structures;multiple types of;high-quality;information network;heterogeneous information networks
real data sets;map-reduce;large-scale;large social networks;learning algorithms;large networks;large data sets;topic modeling;social influence;influence propagation;network structure;social influence
social media;multi-dimensional;discriminative learning;network connectivity;real-world;social media;data instances;relational learning;labeled data;class labels;social media data;collective inference;relational learning;collective inference
approximation algorithms;social interactions;dynamic networks;social network;general case;instances;real data sets;social networks;maximum weight;approximation algorithm;special case;community structure
network analysis;black box;graph mining;synthetic datasets;massive graphs;counting algorithm;clustering coefficient;real-world networks;high accuracy
data point;total number of;data cleaning;machine learning;data set;emerging area;real-world data sets;scientific discovery
indexing structures;event data;learning algorithm;network management;temporal evolution;information networks;communication networks
total number of;similar" queries;real-world;search sessions;search queries;search engine logs;query aspects;objective function;search session
clustering;clustering algorithms;clustering validation;class distributions;clustering performance;inconsistent information;validation measures;application scenarios
user-supplied;anomaly detection;data set;anomalous behavior;likelihood function;likelihood ratio
unlabeled data;class values;labeled data is;semi-supervised learning;classification methods;benchmark data sets;hybrid method;unlabeled examples;supervised methods;semi-supervised;class distribution;classifier
fast approximate;spectral clustering;computational complexity;fast approximate;data points;spectral clustering;large-scale;large margin;clustering accuracy;memory footprint;theoretical analysis;data sets;random projection;produce high-quality;times faster than;instances;clustering
data point;classification;unlabeled data;label information;multiple labels;active learning;classification accuracy;takes into account;prediction method;text data;version space;real-world data sets;labeled data;active learning approach;single-label;multi-label data;text classifiers;text classification;multi-label;support vector machines;active learning algorithms
explicitly model;efficient algorithms to;hidden variables;optimization problems;community detection;content analysis;generative models;citation networks;content information;generative model;networked data;link analysis;benchmark data sets
inference method;data structure;training documents;classification;large text collections;variational inference;sampling methods;large-scale;topic model;topic models;text classification;inference methods;topic distributions;document collections;computationally expensive;low dimensional;gibbs sampling;dimensional data;inference techniques;times faster than
nearest neighbor algorithm;nearest neighbor;classification;classification accuracy;time series;time series;algorithm requires;data mining;diverse domains;space complexity
web objects;classification problem;training examples;classification;social tags;semantic features;classification methods;social tagging;semantic categories;optimization problem;web object;semantic information
error reduction;multi-user;training examples;learning algorithm;vector representation;data collection;social networks;privacy issues;user groups;svm classifiers;semi-supervised;test examples;statistical learning
rewrite rules;structural patterns;biological networks;graph-based;data mining approaches;rewriting rules;machine learning;transformation rules;dynamic networks
computational complexity;data items;change detection;data streaming;clustering algorithm;data distribution
community structures;complex systems;community detection;mutual reinforcement;large networks;detection algorithm;network data;network topology;community structure
statistical properties;link formation;synthetic datasets;complex networks;social network;generative models;real-world networks;network evolution;generative model;controlled experiments
regularization;training data;unlabeled data;information-theoretic;boosting algorithm;optimization algorithms;gradient descent;labeled data;boosting algorithms;information theoretic;regularization framework;semi-supervised;loss functions;real world;weak classifiers
sample selection;source-domain;higher accuracy than;target-domain;source code;target domain;conditional probabilities;text categorization;cross domain;linear transformations;kernel space;source domain to;labeled examples;web page;transfer learning;kernel approach;prediction error
user browsing;web search;user sessions;theoretical foundation;search experience;highly competitive;computational costs;retrieval performance;web site;ranking tasks;contextual information;web page;web users
real data sets;structured prediction;prediction accuracy;markov network;max-margin;markov networks;competing methods;desirable property;em-algorithm;max-margin;relational learning;great promise
distance measure;data mining;image processing;large collections of;information retrieval
search results;sponsored search;1, 2, 3, 13, 25;search engine's;browsing behavior;user behavior;user feedback;instances;search engines;generative model;classifier;user activity
case study;real-life;data analytics;business data
clustering;specially designed;window size;similarity metric;information management;large collections of;content similarity;highly sensitive;content management;similar" documents
predictive models;user preferences;demographic information;parameter estimation;large-scale;commercial applications;real-world;monte carlo simulation;segmentation results;case study;data sets;content management;optimal design
controlled experiments;web sites;lessons learned;controlled experiments;controlled experiments;wide range
clustering;production systems;4, 7;collaborative filtering;large-scale;data mining applications;predictive modeling;domain knowledge;training dataset;1;5;clustering framework;data mining;memory management
opinion mining;large number of;opinion mining;wide range;pattern discovery
pattern matching;market segments;graph theoretical;stock market;spatial constraints;pattern mining;spatial - temporal;data mining approach;data mining;mining algorithm
training data;automatically extracted;low cost;classification;text corpora;classification accuracy;click logs;manually labeled;high quality;large quantities of;data sources;knowledge discovery;data sets;classification models;data mining;lower cost;simple algorithm;specific task;natural language
customer relationship management;large-scale;target domain;rule-based;reference data;data cleansing;business intelligence;training data;latent semantic;annotated corpus;informative samples;supervised learning methods;labeled training data;concept space;term space;supervised methods;human efforts;data distribution;labeled training;free-text;domain-specific;data set;data integration
high-quality;large-scale;clickthrough data;learning algorithm;user behavior;similarity functions;implicit feedback;online advertising
ranking techniques;data mining techniques;growing rapidly;software systems;knowledge discovery;rates;text analysis
taking into account;anomaly detection;detection method;data set;probability distribution;benchmark data sets;detect anomalies
linguistic features;rule-based;statistical techniques;machine learning;main components;online product reviews;integrates multiple;opinion mining;popular items;customer reviews
clustering;query results;specific features;real-world;algorithm called;feature-based;object-level;search result;web pages;subspace clustering;user experiences;object-level;query result;product search
collaborative filtering;recommender systems;similarity measure;random walk;random walk model;real-world data sets;neighborhood information
temporal dependencies;graph structures;graphical models;synthetic datasets;time series;complex systems;hidden markov model;accurate predictions
web pages;high level;classification;classification;web information extraction;content analysis;web information extraction;web information extraction;case study;web applications;conditional random fields
statistical methods;predictive models;web sites;classification;web sites;false positives;large numbers of;web site;end user
clustering;event logs;instances;log mining;network management;apriori algorithm;log data
domain knowledge;propagation algorithm;social network analysis;algorithm called;wide range;high accuracy;rates;true positive;high cost;link analysis;highly scalable
classification problem;training examples;sentiment analysis;labeled training data;specific domains;sentiment analysis;training data;user-generated content;unified framework;text classification;diverse domains;automatically identifying
data analysis;information-sharing;privacy concerns;anonymization algorithm;case study;real-life;essential information;model called;large datasets;healthcare data;hong kong;data anonymization
service provider;growing number of;simulated annealing;supervised learning techniques;multi-label classification;instance;service providers;clustering approach;expectation-maximization
data-driven;symbolic representation;time series;data centers;episode mining;data center;data centers;run-length;data streams;time-series;temporal data mining;higher level
desirable properties;power-law;analysis tool
sponsored search;large-scale;quantitative analysis;search engines;rates;quality measure;search engine users
domain knowledge;closely related;estimation technique;user feedback;estimation process;coordinate descent;alzheimer's disease;covariance matrix;biomedical research;alzheimer's disease
web applications;web news;user friendly;learning problem;user-friendly;extraction methods
web search;web search engine;relevance ranking;clickthrough data;user study;search quality;ambiguous queries;server logs;search engine;maximum likelihood estimation;user studies;search result pages;ranking quality
weakly supervised;web search;latent dirichlet allocation;applications including;named entity;large scale;data source;named entities;mining process;supervision;topic model;weakly supervised;latent dirichlet allocation;named entity;partially labeled;data collected;named entity;human supervision;query-url;web search engine
regression model;web forums;web applications;taking into account
case studies;ensemble methods;feature representations;training sets;malware detection
data-driven;data source;query suggestion;online mining;real data;search engines;search logs;search logs;query sessions

xml schemas;xml applications;web data;relevant information;database schemas;xml schema
database systems;optimization problem;database
data integration;programming languages;designed to support;application development;development process;data integration
instance;usage scenarios;information discovery
schema-mapping;completeness;schema-mapping;source database;data-complexity;databases;schema-mappings;source databases;high-level;data-exchange
information loss;naturally leads to;schema mapping;data exchange;tuple-generating dependencies;instances;query answering;target instances;source instances
data values;xml schemas;classification;source schema;static analysis;schema mappings;data integration
lower bound;database;worst-case;conjunctive query;conjunctive queries;general setting;structural properties;functional dependencies;join queries
combined complexity;data complexity
database theory;finite model;database
query answering over;data exchange;description logics;semantic web;databases;ontology languages;data integration
38;22;autonomous systems;instance;6;input streams
complete information;completeness;database;upper bounds;master data;conjunctive queries;closed-world
query answering over;prior distribution;worst-case;social network;query answers;edges represent;nodes represent;social networks;privacy-preserving
discovered patterns;data mining applications;frequent itemset mining;frequent itemsets;approximation method;takes into account;support threshold;vast amounts of data
metric space;metric spaces;distance function;buffer management;approximate nearest-neighbor
indexing schemes;range queries over;external memory;uncertain data;probability density function;disk accesses;linear space;measurement data;probability threshold;data management
streaming algorithms;lower bound;sampling-based;streaming model;sampling method;fixed size;fixed-size;data streams;sliding windows;sampled data;random sampling
worst-case;real data;skewed;optimal) algorithms;approximation guarantees;data stream;heavy hitters;error bounds
tracking algorithms;total number of;communication cost;streaming model;communication costs;remote sites;worst-case;lower bounds;approximation error;heavy hitters;data distribution
search tree;indexing structures;2, 8, 17;lower bounds;8, 21;multi-version;7, 18, 15;higher-dimensional;1;3, 14, 23;5;4;9;8;10, 12;data points;worst-case;index structure;6, 11, 13, 22;disk accesses;linear space;17;19, 20;16;worst-case;range search;index structures
data structure;data analysis;false positive;indexing problem;trade-offs;αl,αr;space usage;worst case;relational databases;internal memory;range queries
lower bound;update cost;database;block size;main memory;index structure;query cost;lower bounds;range query
satisfiability problem;path expressions;operator
normal form;nested queries;conjunctive queries;query equivalence;complex objects
query answers;query evaluation;sql queries;aggregate queries;1;instances;select-project-join;bag-set semantics;query-reformulation;set semantics;10
labeled trees;xml schemas;uncertain data;data model;xml documents;probabilistic xml;tree automata;tree automata
xml documents;query evaluation;classification;computational properties;incomplete information;schema information;structural information;query answering
complexity bounds;polynomial-space;polynomial space;xml tree;upper bounds;xml document;decision problems;web services
clustering;ranking queries;distance metrics;probabilistic database;aggregate queries;probabilistic databases;inconsistent information;query answer;rank aggregation;algorithms for computing;tree model
clustering;clustering;biological networks;database technology;large-scale;instance;approximation algorithms;random variable;clustering uncertain data;np-hard
uncertain databases;uncertain data;counting algorithm;skyline computation;data items;worst-case;skyline queries;space partitioning;increasing attention;multi-criteria decision making
high-speed;main memory;years ago;relational algebra;database;enterprise applications;data volumes;data warehouses;compression rate;data model;real customer;stored procedures;application development;data processing;application logic;data storage;enterprise systems;data management;transaction processing
web-based;data analysis;web site;data access;data visualization
data structure;base relations;complex queries;join processing;join algorithms;outsourced databases;database outsourcing;range queries
data analysis;privacy-preserving;programming interface
diverse set of;service providers;data collection;aggregate functions
query optimization;search space;dynamic programming;optimizer;plans;query optimizer;cost-based optimization;optimization algorithm;multi-core;22;21;plan generation;optimization process;query execution plan;graph-traversal;query optimizers;optimization techniques;14
join algorithm;data analysis;join operations;database applications;data structures and algorithms;data centers;relational query processing;power consumption;database query processing;solid state;solid state;query processing techniques;query plan;disk-resident;relational data
database systems;high-end;unique characteristics;solid-state;disk-based;online transaction processing
error-prone;optimization techniques;user interfaces;real-world;information extraction;provide feedback;user feedback;unstructured data
large numbers of;probabilistic framework;text based;probability estimates;extraction task;rule-based;learning algorithms;information extraction systems;decomposition techniques;information extraction;probabilistic databases;maximum-entropy;structured objects;real-world;rule-based;user-defined
context-aware;complex structure;web pages;database;document retrieval;context-sensitive;inverted files;scoring function;keyword-based;scoring functions;user's search;ranked retrieval;data collection;data access;relational databases;keyword search;user context;query processing
tuple-independent;sensitive attribute;machine learning
security requirements;nearest neighbor;database queries;encrypted database;case study;service providers;encrypted data;databases;general problem;encrypted database;cloud-computing environment
theoretical analysis;hidden databases;search queries;aggregate information;database
open source;database management systems;17;trade-offs;8, 17;large-scale data analysis;control flow
access paths;web applications;large scale;maintenance costs;view maintenance;aggregate views;complex queries;storage systems;databases;design space;application developers;distributed database;query results
dynamic programming algorithm;query patterns;plans;optimal plan;tree-based;computation costs;complex) event processing;query language;sequential patterns;cost model;query plan;composite events;query plans;cost-based;data rates;query processor;operator;deterministic finite automata;join operator
clustering;ensemble framework;local context;data analysis tasks;data mining;real world;supervised learning;combining multiple
real-world;higher accuracy;matching records;large datasets;database
database;data cleaning;data normalization;real-world;data normalization;data representations;pre-processing;formal framework;programming language
intermediate data;input data;real data;data sets;biased samples;data tables;data-centric
query optimizers;rule based;transformation rules;query optimizer
security policies;web applications;programming model;labeled data is;database;fine-grained;database;multi-tier;user-defined functions;query processing;object-level;class labels;programming language
access paths;decision support;database systems;query operations;memory intensive;main idea;vice versa;main memory;order-preserving;data structures;dictionary-based;query processing;domain size;compression schemes;business intelligence applications;indexing approach;column-oriented;compression scheme;1, 24;19, 23
base table;query evaluation;auxiliary;frequent updates;multi-attribute queries;physical design;data structures;data access;future queries;open-source
performance gains;query response time;decision support;open-source;database;ad-hoc;real-world;intermediate) results;query plan;materialized views;databases;operator
extracted information;real-world;rule-based;execution plan;text data;information extraction;text corpora;plan space;real-world data sets
tree structures;web pages;training examples;edit operations;web sites;tree structure;share common;probabilistic model;high cost;web-page
query forms;database systems;database;query language;search queries;keyword search over;databases;ad hoc querying;keyword search
large amounts of;instances;scientific data;ranking queries;probabilistic data;communication cost;large amounts of data;fundamental problem;multiple sources;real data sets
binary decision diagrams;query evaluation;efficiently computing;query answer;conjunctive queries
query optimization;greedy heuristics;dynamic programming;query complexity;query execution;large number of;graph structures;optimization problem;join order;highly relevant;randomized algorithms;cost functions;join-order;np-hard
input tuples;sql queries;join operators;instance;desirable property;computational cost;operator;rank join
incremental updates;false positives;document retrieval;indexing techniques;update propagation;tf/idf;information retrieval;query answers;incremental maintenance;search engines;specifically tailored;related algorithms;short strings;string matching;real datasets;indexing methods;similarity measures
business-intelligence;monte carlo;uncertain data;monte-carlo;map-reduce;computing environment;data stored in;query plans;enterprise data;distributed algorithms;prediction models
large amounts of;random variables;large-scale;query processing in;aggregation queries;data structures;decision support queries;index structure;application domains;information extraction;probabilistic databases;probabilistic data;wide range;event detection;data structure;databases;sensor networks;disk-resident;probabilistic database
synthetic data;valuable information;sketching techniques;data quality;random sample;functional dependencies
multi-dimensional;object-based;multi-objective;database;skyline computation;large number of;skyline algorithms;skyline points;large datasets;skyline operator;theoretical analysis;space partitioning;indexing technique;high dimensional spaces
skyline query;communication overhead;sliding window;communication cost;processing cost;streaming environments
database systems;kernel-based;real datasets;cardinality estimation;theoretical results;high accuracy;skyline operator;dimensional data;estimation errors;kernel-based
end users;sql queries;database;data item;traditional database;database access;result set;user study
data-driven;real data sets;database systems;classification;database schema;database;instance;optimization techniques;rank order
np-hard;algorithm produces;workflow views;high level;workflow views
high-dimensional;linear space;query results;nearest neighbor;high dimensional;query cost;access method;quality guarantee;nn) search;quality control;nn search;theoretical properties;locality sensitive hashing;relational database;linear-space;high dimensional space;result quality;nearest neighbor search
line segment;nearest neighbor;synthetic datasets;data points;query retrieves the;control points;data-partitioning;spatial queries;data set;shortest path;nearest neighbor queries;query processing;dimensional space;nearest neighbors;spatial databases
nearest;network distance;nearest neighbor;nearest neighbor query;dynamically changing;costly process;candidate set;shortest path;heuristic algorithm;road networks;current location;query returns;nearest neighbors
query optimization;xpath queries;plan selection;access methods;cost-based optimization;rewriting rules;data model;query plan;cost-based;cost models;cost-based query optimizer;operator;cost based;selection algorithm
execution environment;query optimization;search space;optimizer;query optimizer;sample size;query execution;sampling based;query plans;cost model;join algorithms;cost models;cardinality estimation;estimation techniques;resource usage;estimation errors
rdf data;rdf graphs;join processing;large graphs;fine-grained;scalability problems;query processing;performance gains;join-order;input streams;join queries;index scans;design issues
ranking algorithm;schema integration;schema based;schema integration;algorithm runs in;design choices;user interaction;relies heavily on;matching process;automatic generation of;source schemas;additional information
tuple generating dependencies;target instance;data exchange;data exchange problem;post-processing;mapping generation;data sources;schema mappings;mapping systems
domain knowledge;ontology matching;data exchange;semantic correspondences;ontology matching;multiple domains;data integration;metadata management
keyword queries;relational algebra;graph-based;real datasets;databases;large number of;keyword search;main idea;keyword search;keyword query;relational databases;current commercial
search systems;type-ahead search;highly relevant;relevant answers;database;user types;index structures;partition-based;query performance;search paradigm;real data sets;large amounts of data;relational databases;keyword search;query keywords;relational data
edit distance;performs poorly;matching algorithm;addressing this problem
labeling scheme;xml documents;labeling schemes;query performance;labeling schemes;management systems;query processing;xml database;label quality;frequently updated
modeling task;regular expressions;instance;formal language;xml schema;xml schema
recommendation engine;recommendation systems;relational operators;recommendation process;recommendation methods;high-level;structured data
domain knowledge;pruning techniques;poor performance;unstructured text;named entities;document processing;token-based;entity recognition;distance constraints;efficient approximate;entity extraction;approximate matches
real-world;reconstruction error;sensor data;large amounts of data;data mining;compressed data
high-level;interactive applications;scheduling policies;database transactions;database
real data sets;search performance;index structures;data distributions;data sets;wide range;dimensional data;processing queries
reachability queries;transitive closure;sparse graphs;directed acyclic graphs;indexing scheme;directed graphs;reachability query;index size;path-tree
memory footprint;data structure;data collection;human genome;suffix tree;sequence data;construction algorithm;main memory;time series;massively parallel;tree construction;disk-based;applications involving;working set
main features;data warehouse
historical data;data warehouse;design choices;streaming data;materialized views;data stream management systems;network-traffic;temporal data;data feeds
massively parallel;data warehousing
transaction throughput;database applications;years ago;storage devices
query optimization;commercial database systems;dynamic programming;optimizer;database management systems;data dependencies;query optimization;query optimizers;memory size;query plans;query optimizers
query execution;complex queries;disk space;data warehouses;low-cost;von neumann;data warehouse;data warehouse;large number of;operator;database;traditional database systems;fine-grained;lower cost;parallel database systems;resource utilization;database researchers
microsoft sql server;schema evolution;database
internet-scale;software architecture;application development
microsoft sql server;optimizer;plans;cardinality estimation
large numbers of;dynamic programming;optimizer;query optimizer;database;database;object-oriented;join order;cost-based;relational database system;instances;instance level
parameter values;space utilization;plans;materialized view;maintenance cost;query plans;plan generation;typically involves;query processing in;data access;database objects;data skew;cost based
computational complexity;optimizer;plans;query optimizer;equivalence classes;parallel processing;join processing;cost-based;logical properties
high potential;sql server;database;database servers;sql server;database;relational database
update semantics;data values;fundamental properties;durability;data types;database;1;domain-specific;traditional database;transaction management;atomicity;applications involving;execution times
heterogeneous information sources;data-driven;information content;data services;aqualogic data services platform;aqualogic data services platform;query optimization;access control;instance;access control;security policies;xml schema;fine-grained access control
information discovery;information management;3;4;information exploration;social networking
ad-hoc;business intelligence;quality metrics;design process
execution plans for;query optimization;complex queries;optimizer;query optimization;execution plans;query optimizers;usage data
database systems;database;text search;information retrieval;ranking problem;ranked retrieval;information access;user interaction
data collection;data exploration;applications ranging from;business intelligence;data analysis;online services;analyzing data;data management;social interaction
social web;large scale;social tagging;user-generated content;topic areas;social web
potential impact;application level;solid-state;storage systems
programming model;large scale;distributed databases;data model;general-purpose;execution plan;parallel computing;execution engine;high-level;distributed data;programming language
probability estimates;information theory;database systems;database;uncertain data;large-scale;data acquisition;convex optimization;probabilistic databases;management systems;inherent uncertainty;stream systems;statistical learning;database researchers
network processing;field-programmable gate arrays;database systems;database;increasing number of;hardware technology;high-level
clustering;graph-structured data;evolving data;learning curve;semi-structured data;xml data;query generation;data streams;query language;search quality;keyword search;database selection;query processing;analytical processing;databases;ranking functions;query result;keyword search on;keyword based;relational data;keyword search on
data-driven;traditional databases;database technology;database research;data structures;massively multiplayer online games;large amounts of data;databases;commercial databases;data management
query evaluation;anonymized data;sensitive information;uncertain databases;uncertain data;anonymization techniques;item set;original data;privacy guarantees;uncertain data;unified framework;anonymized data;structured data;data anonymization;ad hoc queries
historical data;query interface;temporal queries;real-life;schema evolution;data management
virtual world;data analysis;event processing;query language;interactive visual;pattern matching;application scenarios
security policies;security policies;access control;streaming data;data stream;data stream management systems;data security
stream processing;event processing;complex event processing
database access;data transmission;database-centric;mobile devices;computing platform
software development;program analysis;database;real-world;sql queries;relational database management systems;database;data access;database applications;data intensive applications;static analysis
commercial database systems;query optimization;optimizer;plans;query optimizer
social media;large scale data analysis;news sources;temporal evolution;news items;online video;blog posts;interactive exploration;user generated content;allowing users to
data publishing;high risk;sensitive data
query rewriting;data model;provenance information;data provenance;relational database;relational data
data manipulation;database;mathematical programming;query language;data access;traditional database;databases;application developers;query languages;high-level
execution environment;relational database;programming languages;efficient sql;database
search space;optimizer;database instances;query optimizer;extra information;cost-based;search spaces;optimization process;access plan;database instance
related information;web-based;database research;database management system
real-world;sensor data streams;sensor networks;application domains;sensor measurements;sensor networks;visual) exploration of
user navigation;large number of;concept hierarchy;databases;query results;search interface
data analysis;data mining techniques;users' behavior;mobile communication;data set;case study;customer segmentation;user profiles;mobile communication;data mining;real world;data mining problems;data mining system
sql queries;multiple sets of;database
keyword queries;log records;web search;ad-hoc;database applications;ad-hoc queries;query language;query term;query logs
search results;matching techniques;text search;information integration;search engine;search algorithm;query terms
associative memory;ranking scheme;desktop search;personal preferences;search systems;keyword based
complex structures;edges represent;complex queries;xml documents;querying xml documents

user preferences;data points;database;keyword) queries;database queries;large number of;information retrieval;similarity queries;result set;similar items;query results;query conditions;standard database
unstructured information;unstructured text;data sources;information extraction;domain-specific;structured information;information extraction system
web pages;web search;user queries;user types;web queries;structured data sources;digital camera;data stores;automatically extract
small sample;valuable information;hidden web;database;aggregate queries;databases;large number of;instance;hidden databases;search engines;data stored in;data analytics;online databases;web forms
web search;user oriented;data sources;keyword search;information space;structured queries
web-based;semi-structured data;data exploration;semi-structured;semantic graph;keyword-query;data sets;interactive visualization;data integration
data mining;data mining applications;privacy preserving data mining;security applications;data mining
data analysis;privacy-preserving;real-world scenarios;record linkage;geo-spatial;real-world applications;privacy issues;databases
service provider;information systems;human activities;human activity;mobile devices;mobile communication;user's location;computing devices;context-based;mobile phone;data mining;location-based;wireless networks;mobile users
attribute values;social network;social network data;data privacy;social networks;greedy algorithm;data gathering;information loss
intrusion detection;security problems;computing resources;intrusion detection;detection process;science applications;content analysis;security applications;knowledge discovery process
active-learning framework;file content;active learning;text categorization;real-life;evaluation measures;classification algorithms;classifier;test collection
optimization problems;parameter space;data privacy;desired level of
filtering systems;recommender systems;recommender systems
problem statement;huge amounts of data;relational) data;data acquisition;relational learning;kdd-09 workshop;relational learning;machine learning and data mining;statistical learning
data mining tool;learning mechanism;classification;high dimensional;sample data;search strategy;machine learning;data set;data sets;discovering patterns;ensemble method;error rate
protein structure;computational biology;graph model;complex networks;free energy;amino acids;protein structures;amino acid;graph-theoretic;correlation analysis
high dimensional;strongly correlated;data sets;effective pruning;dependency analysis;association rules
distance measure;training data is;classification;parameter values;np-hard problem;biological sequence;finding optimal;scientific literature;distance based;optimal parameters;sequence classification;np-hard
multi-class;kernel-based;classification tasks;solving complex;large margin;logic based;protein fold;inductive logic programming;multi-class;classifier;learning method;multi-class classification problems
sequence alignment;amino acid;amino acids;1;3;2;rates;4;provide evidence;5;protein sequences
incomplete information;missing information;censored data;missing values;principal component analysis
graph theory;naïve;graph-based;mining process;protein structure;data mining tools
data sets
clustering;visual analytics;spatio-temporal;interactive visual;temporal data;cluster analysis
data analysis;future directions for;data mining techniques;knowledge discovery;information visualization;data mining
exploratory analysis;similarity measure;categorical data;16;visual representations;categorical variables;interesting patterns;data sets;numerical data;visual exploration of
data analysis;frequent pattern mining algorithms;plays an essential role in;frequent patterns;mined results;knowledge discovery;real-life applications;visual representation;data mining;input data;frequently occurring;frequent pattern mining
visual analytics;auxiliary;visualization tool;case study;visual representation;visualization techniques
pair-wise;interactive exploration;database;statistical tests;on-line analytical processing;image data;data sets;statistical test;relational dbms;risk factors
multiple views;on-line analytical processing;survey data;application domains;typically involves;hierarchy levels;visual analysis of;data cubes;data cubes;visualization methods
automatically extracted from;graph structure;data description;semantic graphs;directed graph;data mining tasks;semantic graph;machine learning;exploratory data analysis;visual analysis of;named entity;automatically generating;visual analysis of;natural language processing;visual analytics;semantic representation;named entities;visual analysis
data set;case study;visual analysis;graph data
overlapping clusters;dimensional space;color image;data visualization;high dimensional;high dimensional data sets;data set;data sets;nearest neighbor
information retrieval;wide range;large collections of;international conference on;information retrieval
human behavior;social systems;world wide web;human activity;temporal patterns;emerging patterns;web browsing
context information;web pages;web search;conditional random field;classification;individual queries;web queries are;search sessions;search queries;aware query;real world;search logs
error propagation;training data;classification;large-scale;web pages;text classification;increasingly complex
clustering;11;clustering quality;interesting patterns;retrieval performance;4;clustering algorithm;clustering algorithm called;user study;search tasks;times faster than
web pages;web search;ir techniques;daily activities;high accuracy;web page;web searching
search results;data collected from;sequential nature;user interactions;relevance information;click data;sliding window;ranking problem;relational information;baseline models;search engine;supervised learning algorithms;result set;user clicks;ranking model;search session;conditional random field
search results;internet search;mobile search;search queries;search engines;improving search;search engine users
clustering results;query expansion;index structure;block-based;pre-computation;user queries;service providers;online advertising;random accesses;improving search;clustering algorithm;sequential scans;search result pages;spreading activation;limited number of
document collection;wikipedia based;expansion terms;retrieval effectiveness;relevance information;web collection;query-expansion;query dependent;term selection;information retrieval;pseudo-relevance feedback;query dependent;retrieval performance;relevance model;trec test collections;ambiguous queries;pseudo-relevance feedback;retrieved documents;language modeling
web search engine;implicit feedback;relevance feedback;pseudo relevance feedback;search engine;query expansion
rule-based;data sets;query terms
web pages;ad-hoc;named entities;human experts;broadcast news;automatic speech recognition;wide range;automated techniques;query terms;retrieval applications
speech recognition;short queries;retrieval effectiveness;topical relevance;relevance judgments;ranked retrieval;spoken content;average precision;query terms
retrieval strategies;trec collections;loss function;language model;smoothing method;information retrieval;collection statistics;retrieval models;scoring function;smoothing methods;language modeling framework;retrieval engine
mixture model;relevant documents;large scale;irrelevant documents;difficult queries;pseudo relevance feedback;data sets;relevance model;automatically generate
ranking algorithm;portfolio theory;document ranking;relevant documents;ranked list;information retrieval;portfolio theory;text retrieval;retrieved documents;probability ranking principle;rank order
search results;relative entropy;term frequency;query logs;tag prediction;query terms
web communities;content analysis;linear combination;dynamic nature of;online discussion
statistical methods;cluster labeling;test collection
clustering;query throughput;web pages;web search engine;position information;position data;ranking functions;text collections;search engines;query processing;access patterns;index compression
life sciences;large-scale;brute force;document similarity;brute force;document collections;brute force algorithm;ad hoc retrieval;pairwise similarity
search systems;web search;trade-offs;real-world;search engines;return results
ranking scheme;plans;classification;share information;multiple datasets;content features;classification approach;question answering;classification tasks
redundant features;high-quality;user-generated;question-answer;user-generated content;question answering systems;machine learning techniques;relational data
tree matching;average precision;ground-truth;community-based;semantic features;matching problem;kernel based;qa) systems;tree structure;qa systems;retrieval framework;real world;finding similar;question answering
collaborative recommendation;large amounts of;taking into account;collaborative filtering;social tags;graph model;random walk;social network;social network;generic framework;pearson correlation;social networks;personal preferences;information embedded in;social annotation;personal information;social graph
recommendation quality;user-item;sparse data;recommender systems;large-scale;information filtering;factor analysis;very large datasets;real world;social trust;social trust
data-driven;singular value decomposition;user ratings;data dependencies;collaborative filtering;large-scale;learning algorithms;low-rank;matrix factorization;optimization algorithms;matrix factorization methods;share similar;principal component analysis;latent factors;accurate predictions
enterprise search;web search;relevant pages;anchor text;retrieval effectiveness;difficult queries;document representations;common practice;search tasks;test collection
commercial search engines;web search;anchor text;web sites;web site;hyperlink structure;page content
web documents;link analysis;link structure;link information;business transactions;spectral analysis;knowledge discovery;privacy-preserving;real world;link analysis;weighted graphs;link-analysis
entity ranking;ground truth;web search;aggregate information;entity ranking;query words;named entities;entity search;question answering
proximity measures;ad hoc;information retrieval;query-terms;retrieval models;retrieval model;document scores;term proximity
regularization;directly optimize the;baseline methods;loss function;rank learning;ranking models;algorithm named;gradient descent;pairwise classification;discounted cumulative gain;learning models;ranking measures;increasing attention;theoretical guarantee
web search;entity recognition;classification;topic model;partially labeled;unsupervised learning;weakly supervised;probabilistic approach;baseline methods;named entity;entity recognition;log data;supervised learning;learning method;latent dirichlet allocation
trec blog track;named entities;text retrieval;retrieval methods;poisson model;named entity;term frequency;text retrieval;text collection;trec 2008 blog track;poisson model
domain adaptation;unsupervised learning;classification;large number of;cross-domain;information extraction;text content
language model;ranking models;information retrieval;query terms;average precision;language modeling;term proximity
black box;trec test collections;document ranking;language model;language models;passage retrieval;information retrieval;proximity-based;retrieval model;language modeling approach;density function;density functions;query terms
information retrieval;data sets;retrieval performance;bayesian learning;ranking model;biomedical information retrieval;posterior probability;bayesian learning
web search;retrieval effectiveness;classification;search engine's;federated search;query traffic;analysis reveals;queries issued;domain-specific;query string;resource selection;web search;vertical selection;search services
search systems;query traffic;user feedback;aggregated search;web search results;vertical selection
search results;highly relevant;web search;gaussian process;search techniques;queries submitted to;information retrieval;search result quality;search engines;document collections;location-aware;information retrieval techniques;ranking documents;ranking quality
similarity analysis;frequency information;query suggestion;query log analysis;information retrieval;query frequency;weighting scheme;log data;query-url;click graph
search results;commercial search engines;highly relevant;rates;web search results;news events
clustering;sparseness problem;web search;document ranking;clickthrough data;smoothing methods;search applications;random walk on;ranking models;features extracted from;real-world;web search;models trained on
search results;users' interests;recommendation systems;information sources;user interests;search engine;contextual information;information-gathering;user interaction
user-generated;query suggestion;query formulation;query suggestions;interactive information retrieval;relevance feedback;automatically creating;information seeking
search systems;pre-defined;user study;real-world;designed to support;decision making;increasing importance;search interface;search tasks;multiple aspects
web documents;information sources;social tags;multiple sources;support vector machines;music retrieval;social context
clustering;classification;feature representations;content-based video;user evaluation;data organization;additional information
music information retrieval;data collections;similarity measure;application domains;efficient retrieval;locality sensitive hashing;supervised learning;music collections;data storage
geographically distributed;document ranking;distributed query processing;query traffic;performance gains;web crawling;replication;distributed nature of;search engines;cost models;query processing;search engine;search quality;real-life datasets;web search engines
unlike earlier;training data;server selection;distributed information retrieval;selection algorithm;network traffic;high precision
search systems;federated search;specific queries;retrieval performance;query expansion;user interaction
evaluation" measure;information retrieval;simulated data;rank correlation;statistical significance;evaluation measures;rank correlation
test collections;information retrieval systems;test sets;relevance assessment
analysis reveals;considerable effort;relevance judgments;retrieval systems;quality control;review process;video content
training data;training set size;evaluation measure;sufficiently large;information retrieval;locally optimal;evaluation measures;machine learning
document selection;document selection;active-learning;relevant documents;feature extraction;data sets
markov random field;markov random field model;retrieval accuracy;verbose queries;retrieval models;document collections;weighting scheme;supervised learning;model estimation;term-based;natural language
database;tag-based;language model
cross-language retrieval;test set;baseline methods
search results;search performance;visual search;ranked list;real-world;visual patterns;optimization problem;visual words;data sources;search engines;benchmark dataset;image patches;multiple image
search systems;trec data;ir metrics;search process;retrieval systems;test collections;trec web track;user study
search engine
ranked list;ranked retrieval;document scores;score distributions;optimization problem
traditional collaborative filtering;web portal;rating data;collaborative filtering;nearest neighbor algorithm;prediction accuracy;data sparsity;cold-start;data set;nearest-neighbor;user-study;recommending items;web users
ranking algorithm;users' interests;web pages;large-scale;user experience;text documents;multi-type;social tagging;graph-based ranking;data set;graph-based;ranking" problem;increasingly popular
domain experts;classification
information retrieval evaluation;relevant information;evaluation techniques;retrieval methods
long queries;relevant documents;query quality;long queries;average precision;classifier;test sets
pre-defined;data structure;conditional random field;structured documents;query term;crf) model;click data;database;crf model;extract information from;user queries;data sources;search queries;structured information;accuracies;label information;semi-supervised;statistical models;conditional random fields;learning method;manually-labeled
search results;retrieval effectiveness;relevance judgments;search engines;selection methods;search effectiveness
search results;sponsored search;ranking list;high quality;optimization problem;machine learning;ranking function;search engine;log data;ranking model
web information retrieval;web documents;unstructured text;instances;higher accuracy than;query logs;weakly-supervised
ground truth;training data;error rates;automatic methods;user feedback;error rate
web documents;statistical measures;graph-based;high quality;collaborative tagging;data sets;hits algorithm
social interactions;large number of;correctly identify;instance;online video;social networks;classification algorithm;test collection
spatio-temporal;visual saliency;content analysis
statistical significance;information retrieval evaluation;ad-hoc retrieval;sample sizes
classification;query terms;search engine
type-ahead search;keyword queries;type-ahead search;user types;information-access;query keywords
search task;search intent;user interactions;query suggestion;user behavior;search sessions;behavior patterns;common task;search evaluation;search result pages;search session
random walk on;query topic;extra information;user query;data set;language modeling;random walks
search systems;evaluation methodology;correlation coefficient;query difficulty;query difficulty;information retrieval;average precision;retrieval systems
web page;trec blog track
string kernel;classifier;classification
cluster-based;query expansion;relevance model;document clusters
commercial search engines;web pages;taking into account;ranking function;score distribution;ranking functions
search task;specific features;clustering approaches;competitive performance;retrieved documents;clustering approach;indexing methods
language modeling;retrieval models;compression-based;document length;language models
indexing approach;labeled samples;video indexing;labeled data;concept-based
search advertising;text classification;web search
ranking algorithm;counting algorithm;0.1, 0.9;web graph
cross language;information retrieval methods;translation model;transliteration;training data
training set;training data;training sets;relevance judgments;cost effective
filtering systems;computational power;large volumes of;massive amounts of;energy efficient;information processing;test collections;field programmable gate arrays;times faster than
generic model;training data;preference learning;search engine;labeled data;model trained
clustering solution;probabilistic latent semantic analysis;local minima
document collection;close connection;semantic concepts;retrieval models;retrieval performance;retrieval model
language-model;class information;document content;language models;query performance;search result quality;quality measures;query difficulty
machine translation;cross-lingual;search topics;patent retrieval;retrieval accuracy
search algorithms;variance reduction;search algorithm;web search;search tasks
type information
document streams;document summarization
cross language information retrieval;text search;language pairs;string matching;edit distance
syntactic features;feature selection;abstraction levels
keyword extraction
high quality;score distributions;dictionary-based;score distribution;opinion retrieval;topic relevance;test collection
wikipedia articles;graph-based;cross language information retrieval;data sets
search systems;information retrieval;information retrieval techniques
text mining;heuristic approaches;automatic methods;real data;statistical model;language modeling
link discovery;ground truth;machine learning approaches
search systems;user study;cultural heritage;search performance
ranking algorithm;document collection;ranking function;prior knowledge;prior model;multi-document summarization;ranking functions
retrieved documents;document retrieval;document clusters;query-specific;cluster-based
high potential;ir model;phrase-based;retrieval effectiveness
retrieval systems;trec blog track;opinion-finding;finding task;relevant documents
accuracy compared to;collaborative question answering;classification;sensitive information;data- mining methods;collaborative question answering;question-answering systems;automatically identifying
naïve;information exchange
instance;high-quality;user-generated;target domain;text categorization;cross-domain;sentiment analysis;labeled data;sentiment classification;manual effort;automatically identifying
main memory;disk based;tree structure;large scale;efficient approximation;information retrieval;sparse representations;large document collections;document clustering;document collections;clustering algorithm;space requirements
generative process;link information;topic model;clustering task;topic distributions;generative model;digital libraries;related documents
weighting function;test collections;accurately predict;poor performance;information retrieval
cross-media;retrieval tasks;high similarity
mining task;product features
taking into account;topical relevance;classification methods;implicit feedback;sensory data;facial expressions;information retrieval systems
response times;training data;training data is;text search;source-specific;probability distribution;search engines;resource selection;search engine;query-specific
ranking scores;search engines;retrieved images;visual features;ranking method
search log;user queries;query log analysis;query types;reformulated queries
clustering;multi-view clustering;multi-view;multi-view clustering;information retrieval;clustering model;correlation analysis
search systems;retrieval effectiveness;relevant documents;expert search;document search;upper-bound;retrieval performance;search engine;relevant expertise;document rankings
hadoop mapreduce;large-scale;document corpora;programming model;information retrieval
training samples;rates
query refinement;search engines;web search engines;applications including;human computation
generally applicable;retrieval methods;retrieval model;emotion recognition;music retrieval;information retrieval;information retrieval tasks
search sessions;search session
benchmark datasets;information retrieval;information retrieval
ranking model;web search;web search;training set;greedy algorithm
automatically extracting;baseline methods;naive bayes;ranking svm;classification
rank learning;document rankings;ir systems
amazon mechanical turk;relevance criteria;relevance criteria
error-prone;sponsored search;collaborative filtering;ctr;click data;search engine logs;relevance model
event tracking;temporal patterns;event tracking;topic model;news events;text streams
trec test collections;relevant documents;information retrieval;comprehensive evaluation;baseline models;information retrieval
high complexity;retrieval systems
application domains;upper bound;worst case;poor performance;hits algorithm
high level;programming languages;data exploration;information retrieval;case study;search engines;search engine;high-level;programming language;low-level;higher level
online news;web search engines;search engine
precision/recall;visual features
clustering;hierarchical clustering;similarity threshold;hierarchical clustering;result-set
link structures;query log analysis;exploratory search;query log;query type;search engine
broadcast news;textual data;retrieval systems;broadcast news;rates;automatic speech recognition;information retrieval;word segmentation
large-scale;ir evaluation;sentence-level;relevance assessment;document length;sentiment analysis;retrieval performance;opinion retrieval
link structures;specific topics;graph-theoretic
complete information;retrieval effectiveness;query sets;prior information
web objects;vector space;similarity measure;term space;computational cost;cosine measure;similarity computation;tag-based;web object;latent semantic indexing;dimension reduction
nearest-neighbor classification;image descriptors;scale invariant feature transform;training examples;multiple views;accuracies;classification tasks;visual words;visual vocabulary;image classification;classifier
social network analysis;web forums;large number of;web forums;opinion retrieval;rich information;extracting data from
prediction problem;prediction accuracy;users' preferences;collaborative filtering;collaborative filtering;prediction methods;dynamic nature of;parameter selection;user ratings
search task;extended abstract;frequency distribution;similarity measurement;correlation coefficient;similarity measure;information retrieval;query frequency;search queries;temporal features;similarity measurements;high-level;temporal characteristics of
probabilistic model;rates;query terms;term-based
selection criterion;large amounts of;selection problem;ir evaluation;query difficulty
user preferences;mining framework;specifically designed for;semantic knowledge;textual data;statistical model;relevance feedback;data collection;feature selection;video retrieval
high confidence;statistical significance;test collections;confidence levels;test collection
prior-art;effective search
information sources;query segmentation;information retrieval;term dependencies;retrieval performance;long queries;query logs;syntactic structure
search topics
million images;image features;data set;low level;text retrieval;high recall;metric spaces;query processing
retrieval effectiveness;relevant documents;expert search;expert finding;search result;search engines;search engine;relevant expertise;log data;search engine users
multiple criteria;information retrieval;information retrieval models;multi-criteria;user-centric;user-centric
relevant documents;recall-oriented;search tasks;interactive information retrieval

entity ranking;wikipedia categories;manually assigned;category information;ad hoc retrieval;ad hoc;ad hoc search
retrieval effectiveness;test collection
web search;improving web search;query frequency;query log;real-world;web search
retrieval effectiveness;large-scale;prediction methods;query performance prediction;retrieval performance;correlation coefficients;query performance;query expansion;query performance prediction
document genre;semantic annotations;narrative structure;language model
unique features;extraction algorithm;similarity measure;share information;semantic search;1;3;2;noisy data;news events;dense clusters;text messages
social annotation
search techniques;image search;search results;color information
image retrieval;similarity search;large-scale;search engine
information gathering;semantic properties;information processing
search systems;search process;search engine
document collection;information retrieval;related topics;information retrieval
temporal information;long-term;enterprise search;text retrieval
topic structure;information retrieval
social media;users' interests;dynamic nature of;text data;ranking performance;documents retrieved;ranking method;ranking documents;user-oriented
document corpus;relevance judgments;information retrieval;performance metric;ir) systems;common practice;retrieval systems;average precision;test collection
search performance;information retrieval;relevance feedback;task type;2, 4;document usefulness;1;user queries;3;2;5;individual users;users' interests;search topics;user-generated;threshold based;query expansion;contextual factors;search terms;user preferences;background information;user generated;user behaviors;additional information
associative memory;search results;digital data;data types;information retrieval;1;data sets;real life;5;4;mobile phone;information seeking;2, 3;personal information;limited number of
high-dimensional;benchmark data;data mining and machine learning;data mining;distributed-memory;information theoretic;clustering;singular value decomposition;regularization;workshop on data mining;scientific computing;matrix factorization;high order;graph theory;dimension reduction;spectral clustering;pattern recognition;large scale;tensor analysis;topic areas;principal component analysis;structured data
world wide web;candidate patterns;marketing strategies;mining association rules;web sites;hidden information;database;web mining;traversal patterns;data mining;web page;increasing importance;traversal patterns
performance guarantees;multiagent learning;reinforcement learning;learning agents
automated design;mechanism design;constraint-satisfaction;positive results;additional constraints;negative results;search problems
multi-robot;formal model
np-hard;search algorithm for;application domains;minimum number of
task allocation;task allocation;moving targets;sensor network
instances;high probability
case study;autonomous agents;artificial agents
instance;computational resources;addressing this problem;scheduling problem;classification approach;prediction models
theoretical properties;database
logic program;polynomial space;answer set programming;decision procedure for;propositional satisfiability;sat-based;answer sets
search space;simple temporal;temporal reasoning;lb(e, ub(e;temporal constraints
optimization problems;multiple features;theoretical basis for;real-world domains
np complete;np-complete;stable models;stable model;general case;logic program
logic programming;operator;stable models
answer set;logic program;answer sets
model checking;temporal logics;temporal logic;distributed systems;model checking
causal effects;bayesian networks;observational data
local consistency;computational complexity;global constraints
instances;search algorithms;problem hardness
local models;computational complexity;complexity bounds;multi-context systems
instances;sat instances
pruning power
search space;local search;genetic algorithm;problems require;scheduling problem;times faster than
theoretical results;constraint satisfaction problems;context dependent;constraint networks
constraint satisfaction;constraint satisfaction problem
local search algorithms;local search;worst-case;tree-based;objective function;satisfiability problem;propositional satisfiability;local minima;constraint satisfaction problems;space complexity
constrained problems;user preferences;constraint programming
search algorithms;worst-case;search strategy;tabu search;satisfiability problem;local search;tabu search
approximation algorithms;discrete optimization;real-life;cost-based;knapsack problem;filtering algorithm
lower bounds;lower bound
key features;local search;weighting schemes;weighting scheme;satisfiability testing
labeling problems;constraint networks;path consistency
approximately) optimal;utility function
bounded treewidth;connected component;special structure;np-complete;problems arise;instance
concise representation;computational complexity;np-complete
human behavior;data collection;machine-learning
parameter space;optimal parameters;linear transformations;computationally hard;search problem

complete information;database technology;database;situation calculus;incomplete information;relational database;relational databases;action theories;relational dbms
belief change;belief state;minimal change
gene ontology;description logics;formal specification
pattern matching;temporal structure;multi-agent;event detection;natural languages;low-level;video sequences;high-level;object classification;natural language
formal representation;model-theoretic;logic-based;argumentation based
formal representation;classical logic
special case;answer set semantics
state representation;classification models;formal framework for;instance-based;network connectivity;partially observable;reinforcement-learning;cost-sensitive
belief change;preference-based
hidden markov models;generative model for
action language;causal model
search techniques;domain-knowledge;knowledge-based;link analysis
special case
character recognition;tree structure;hidden markov models;hidden markov model;text retrieval;hidden states;video indexing
ensemble techniques;training data;ensemble techniques;diverse set of;naïve bayes;model averaging;instance;decision trees;models built;bayes classifier;density function;ensemble technique
decision trees;classification;decision tree;loss functions;highly accurate;multiple times;cost-sensitive;high accuracy;probability estimation;decision making;posterior probability
neural networks;artificial intelligence;neural network
efficient inference;high level;accurately predict;particle filters;prior model;low level;sensor measurements;manual labeling;gps data;markov model
decision-making;action sequences
bayesian network;classification;memory requirements;bayesian network classifiers;classifier;data sets;bayesian networks;feature selection;feature selection methods;classification rate
algorithm called;boosting algorithms;online learning;neural network
inference method;selection problem;dimensional space;monte carlo;simulation experiments;hierarchical model;bayesian inference;principal components;principal component;principal component analysis;probabilistic pca;markov chain
instances;class label;instance;information-gain;noisy datasets;classifier
growing number of;virtual environments;plans;narrative structure;human users;virtual world;automatically generated;conceptual graphs;question-answering;computational models
domain-independent;training examples;rule learning;large collections of;extraction rules;instances;domain-specific;information extraction from;automatically identifies;hand-labeled;high precision
question-answering systems;wide range;knowledge-based;knowledge base
text analysis;index terms;vast amounts of
web documents;information extraction system;language modelling;database;error rates;takes into account;linear-chain;conditional random fields;user interface;extraction methods;information extraction;unstructured data;markov model
appearance model;structural features;entity types;model assumes;joint distribution;supervised approach;generative model;general form;machine learning approaches;natural language
clustering;training set;unlabeled documents;ranked list;semi-supervised learning;text classifiers;human experts;labeled training examples;feature selection;manual labeling;text classification;em algorithm;classifier;unlabeled set
additional knowledge;context-free;context-free;natural languages;long-term;positive examples;natural language
tensor product;artificial neural network;natural language processing;tensor product;human beings;computing power;syntactic structure;semantic information;learning method;natural language
local appearance;object recognition;decision trees;vision systems;surveillance systems;information content;discriminative power;image noise;object identification;object recognition;local patterns;entropy measure;object representation;partial occlusion;object models;local information;robot vision;background clutter
sketch recognition;automatically created;domain specific;domain descriptions;domain independent;recognition systems;multiple domains
pose estimation;large-scale
spatial resolution;multi-resolution;large-scale
unknown environment;feature-based;images acquired;instance;spatial distribution;pose estimation;automatically learning;visual features
markov random field;spatial relationships;intensity images;indoor environment;probabilistic inference;range data
user study;hand-drawn;sketch recognition;cognitive science;classification
spatio-temporal;physical environment;machine learning;multimodal data;image sequences;natural language processing;automatically infer
planning problems;taking into account;plans;partially observable;plan generation;counterpart;assumption-based;partially observable domains;plan execution
optimization problems;stochastic optimization;salient features
recognition algorithm;plan recognition;computationally expensive
desirable properties;heuristic techniques;deterministic domains;problem domains
genetic algorithm
belief states;state space;belief space;partial observability;belief state;special case
linear constraints;sat-based;plans;real-valued
path planning;theoretical framework;prior distribution;probability distribution;hidden markov model;bayesian framework
scheduling problems;resource-constrained;conflict resolution;heuristic algorithm;temporal constraints;scheduling problem
total number of;shortest path;common properties;real-world;shortest path;shortest path;recognition process;weighted graph
completeness;complete information;planning domains;sensing actions;regression function
plans;state-space;plan quality;generate plans;real world;heuristic search;integer programming
search space;domain-independent;large parts;temporal planning;constraint programming;constraint programming;partial order;wide range;pruning rules;special case
logical reasoning;plan-recognition;indoor environment;goal recognition;real data;application domains;high-level;dynamic bayesian network;wireless network;high-level;plan recognition;low-level
data-driven;error propagation;sampled data;high level;spatial aggregation;domain knowledge;spatial reasoning;application area;model selection;spatial aggregation
plans;input data;decision making;qualitative simulation;simulation results;artificial intelligence;decision process
multi-robot;low-level;decision making;human-robot
probability estimates;common sense;database;data acquisition;common sense knowledge;common sense;generated dynamically;user input;collected data;knowledge base;common sense
learning environment;machine learning;machine learning algorithms;control problem;machine learning
generate plans;plans;temporal planning;dynamic environments;autonomous systems;partial order;temporal constraints;dynamic environment;resource constraints;plan execution
learning process;robot control;recurrent neural networks;learning algorithm;reinforcement learning;reinforcement learning;instance;gradient method;learning scheme;control mechanism
markov chain;agent performance;markov decision process;markov decision process
pattern databases;instance;pattern database;instances;heuristic function;highly correlated
state space;search algorithms;directed graph;large margin;search effort;game tree
disk accesses;random access;duplicate detection;hash table
game tree search
search methods;normal-form
random walk;close connection;reasoning tasks
problem instances;pattern databases;state space;problem instance;multiple sequence alignment;heuristic function
state space;duplicate detection;external-memory graph search;state-space;graph based;disk storage;heuristic search;external memory
gradient-based;sequential nature;large-scale;decision problem;stochastic local search;finite-state;partially observable markov decision processes;dynamic programming;low computational cost;higher quality;stochastic local search
social welfare;preference aggregation;majority voting;social welfare;voting rule;preference relation;np-hard
accuracy compared to;running times;amino acid;source code;database;heuristic techniques;protein sequences;objective function
dynamic programming algorithm;dynamic programming;partially observable;stochastic games;algorithm iteratively;partially observable markov decision processes;dynamic programming;normal form;special case
markov decision processes;pruning rules;real world;decision problems;sampling-based
low-cost;disjunctive temporal;finding optimal;simple temporal;soft constraints;constraint satisfaction problems
individual agents;multiple agents
compact representation;total order;sufficient conditions for
higher quality;semi-markov;decision process;decision processes
collaborative filtering;collaborative filtering;decision-theory;filtering methods;recommending items;evaluation measures
text summarization;specific features;summarization task;product features;common practice;customer reviews
syntactic features;support vector regression;wide range;squared error;fold cross-validation
constraint satisfaction;optimization problem
clustering problem;knowledge sources;clustering algorithms;instances
argumentation-based;bayesian network;bayesian networks;decision making;learning process
incremental algorithms;simple temporal;constraint satisfaction problem;incremental version
cost function;reasoning capabilities;cognitive model;constrained optimization
network routing;adaptive algorithms
decision theoretic;relevant documents;information retrieval;fine-grained;retrieval performance;bayesian networks;user intent
stochastic processes;discrete event systems
mobile ad hoc network;sensor monitoring;intelligent systems;agent systems;mobile agents;software agents;ad hoc;agent-based
optical flow;open source;high-accuracy;closed-loop;motion control
multi-agent systems
involving multiple;decision support;agent-based
java-based;social network analysis;social behavior;computation model;agent-based;social networks;agent-based
data collection;hospital information;decision support system;mobile clients;computing devices;data entry
semantic similarity;word senses;database
sql queries;user interfaces;web browsers;data set;databases;access information;natural language;natural language
web based;high level;semantic web;web services
clustering algorithms;large corpora;word senses;instances;finding clusters;feature selection;clustering approach
human activity;agent-based

multi-modal;domain specific;domain independent;problem solving;spoken language;knowledge base;natural language
highly dynamic;web queries;semantic web services;intelligent agents;knowledge-based
decision-making
syntactic structures;classification;generated dynamically;training sets;syntactic structure;natural language processing;neural network;artificial intelligence;natural language
branches visited by chronological backtracking;texas hold'em;search tree
problem instance;search trees;branches visited by chronological backtracking;decision procedures;search tree
game-tree search;branches visited by chronological backtracking;search tree
pruning method;domain independent;branches visited by chronological backtracking;search tree
path planning;search algorithms;large state spaces;search algorithm for;search techniques;planning problem;simulation results;search efficiency;search problems
parallel algorithms;parallel algorithm;duplicate detection;multiple sequence alignment;instances;mapping function;duplicate detection
game trees;model assumes;game tree search;perfect-information;game tree;expected utility;search algorithm
constrained problems;local search;variable selection;numerical results;complete search;search algorithm;coarse-grained
probabilistic approach;game tree search;highly optimized
greedy algorithm;labeling problem
sample sizes;strong evidence;monte carlo;trade-offs
search algorithms;state spaces;search efficiency;search algorithm
planning problems;search algorithms;duplicate detection;wide range;external-memory graph search;domain-independent;domain-independent;local structure;search problems;external memory;search nodes
markov decision processes;multi-agent systems;markov decision processes;iterative algorithm;temporal constraints;higher quality
instances;upper bounds;bayesian networks
instance;preference elicitation
search tree;approximation quality;branches visited by chronological backtracking;belief propagation
intrusion detection systems;false positive;network size;anomaly-based;probabilistic models;communication bandwidth;rates;probabilistic inference;network traffic;high sensitivity;probabilistic graphical models
probabilistic models;potential functions;random variables;probabilistic inference
partially observable;stochastic games;decision making;solution concept
graph nodes;graphical representation of;multiple attributes;utility functions
join-tree;local structure;inference methods;bayesian networks
latent) variables;bayesian network;causal effects;bayesian networks;causal relationships
bayesian network;early stage;outbreak detection;expected utility;bayesian networks;time-series;cost-effective;artificial intelligence
basis functions;branches visited by chronological backtracking;search tree
branches visited by chronological backtracking;search tree
bayesian network;model counting;davis-putnam;bayesian inference;instances;bayesian networks;exact inference;high density
action sets;synthetic data;real-world;decision-making;information from multiple sources;real data;disparate sources;decision making;approximation algorithm;mathematical framework;selection algorithm
utility function;dynamic programming;plans;decision-theoretic;approximation guarantees;utility functions;piecewise linear;decision making
state space;control policy;inverted pendulum;basis functions;spectral analysis;markov decision processes;graph laplacian;radial basis function
search algorithms;search tree;caching scheme;graphical models;search spaces;search algorithm;memory intensive;context-based
probabilistic model;dynamic nature of;bayesian approach;random variables;evaluation functions
branches visited by chronological backtracking;search tree
branches visited by chronological backtracking;causal models;search tree
real-time dynamic programming;lower bound;search algorithm for;real-time dynamic programming;algorithm called;upper bound;long-term;reward function
multi-agent systems;point-based;multi-agent;partially observable;stochastic games;belief states;test examples;partially observable markov decision processes;point-based;dynamic programming;single-agent;linear programming
bayesian network;causal effects;causal models
bounded size;semidefinite programming;approximation quality;optimal value function;partially observable markov decision processes;upper bound;piecewise linear;decision making
multi-agent;semantic web;case study;semantic web;web resources;semantic web technologies;service discovery
web objects;linguistic information;dynamic environments;natural language;web object
document ranking;multi- document summarization;trade-offs;text fragments;sentiment classification;data mining;online product reviews;machine learning algorithms
reasoning tasks;description logics;instance level;instances;computationally tractable;description logic;knowledge base;description logic
search tree;branches visited by chronological backtracking;expectation-maximization
branches visited by chronological backtracking;search tree
branches visited by chronological backtracking;search tree
description logics;semantic integration;real-world applications;semantic web;semantic web applications;ontology languages
text categorization;branches visited by chronological backtracking;search tree
content-based filtering;proximity measures;recommender systems;collaborative filtering;hybrid approach;semantic features;similarity measures
web pages;high level;table extraction;table extraction;information extraction;recognition algorithm;knowledge acquisition;spatial reasoning;semantic information;visual appearance
semantic matching;takes into account;high precision and recall;information processing
related concepts;digital library;semantic network;domain ontology;document retrieval;domain ontology;semantic web;search engines;spreading activation;information access;search engine;ontological knowledge;search performance;ontology based;keyword based
text mining;common features
semantic web technologies;access control;web-based;management systems;semantic web technologies
web-based;social network;social networks
svm models;learned models;machine learning;search engines;link-based;web search engines
branches visited by chronological backtracking;search tree
web services;web service;data types;sample data;large number of;semantic model;user queries;service providers;correct classification;web services;integration systems;classification method;classification methods;automatically learn;data samples
prediction accuracy;low-cost;automatically created;individual users;semi-automatically;information overload;task-specific;web users
collaborative recommendation;search results;web pages;human subjects;collaborative filtering;similarity measure;ranking measures;search engine;ranking algorithms;hierarchical structure;information retrieval techniques;individual users
semantic web;social networks;branches visited by chronological backtracking;search tree
clustering;probabilistic latent semantic analysis;nearest neighbor;collaborative filtering;recommender systems;recommendation algorithms;collaborative filtering algorithms;active user;scalability problems;recommendation accuracy;similar users
semantic web;branches visited by chronological backtracking;search tree
search results;world wide web;web search;web documents;large scale;named entities;named entity;human knowledge;textual documents;extraction patterns;text processing
search results;training data;clickthrough data;relevance judgments;relevance feedback;search engines;result set;real world
data set;prediction accuracy;user's preferences;recommender systems;user's preference
wikipedia based;semantic relatedness;knowledge base;search engine
branches visited by chronological backtracking;search tree
web search;branches visited by chronological backtracking;search tree
convergence properties;multi-agent;domain specific;temporal difference;requires solving;multiple agents;reward functions;assignment problem
robotic systems;human learning;learning tasks;human-robot
mathematical framework;supervised learning;4;clustering framework;linguistic knowledge
search heuristics;automatically constructing

cognitive architecture
free-form;large-scale;success rate;natural language
object recognition;reference model;human-robot;spatial knowledge;allowing users to;computational model;object classification
supply chain management;branches visited by chronological backtracking;search tree
intelligent tutoring systems;natural language processing
branches visited by chronological backtracking;search tree
multi-modal;cognitive science;cognitive architecture;problem solving;intelligent behavior
lessons learned;vector machine;ai research;event detection
artificial intelligence
machine learning applications;ai technologies
knowledge representation and reasoning;human-robot
branches visited by chronological backtracking;search tree

branches visited by chronological backtracking;search tree
game theory;game-theoretic
database schema;sharing data;semantic relationships between;semantic integration;disparate sources;data sources;databases;semantic mappings;assists users;semi-automatic
multi-agent;large number of;vice versa
domain-specific knowledge;constraint networks;constraint programming;version space;addressing this problem;sat-based;combinatorial problems;solving complex
predictive model;artificial intelligence
preference elicitation;branches visited by chronological backtracking;search tree
text classification;case-based reasoning;text representation;case-based reasoning
imbalanced data sets;branches visited by chronological backtracking;search tree
minimal cost;private information;high-level;decision-making
branches visited by chronological backtracking;search tree
autonomous agents;artificial intelligence;multiagent learning
algorithms require;decision trees;decision tree learner;locally optimal;globally optimal
clustering;semi-structured data;unsupervised learning;learning approaches;web sites;generation process;extraction rules;fully-automatic;probabilistic model;clustering approach;automatically extracts;web data
convex optimization;heterogeneous data;euclidean distances;euclidean space;low dimensional;complex data;statistical models;low-dimensional
case-based reasoning
branches visited by chronological backtracking;search tree
large scale;semantic web;performance tradeoffs;knowledge base;application area
branches visited by chronological backtracking;search tree
fault tolerant;single-item
user preferences;recommender systems;specific information;information-seeking;machine learning techniques;increasingly popular
planning domains;branches visited by chronological backtracking;search tree

domain knowledge;user preferences;simple models;information retrieval;markov random fields;artificial intelligence
branches visited by chronological backtracking;search tree
automatically learns;web databases;categorical attributes;domain independent;query relaxation;functional dependencies
web services composition;branches visited by chronological backtracking;search tree
user interface;ai techniques;modeling techniques
similarity assessment;case-based reasoning;classification;classification tasks;case-based reasoning;application domains;feature weights;similarity measures
prior knowledge;video game;neural networks;machine learning
cognitive science;natural language processing
domain knowledge;explanation-based learning;learning mechanism;training examples;explanation-based learning;image understanding;higher quality
statistical methods;large amounts of;unlabeled data;convex optimization;maximum variance;low dimensional;algorithm relies on;dimensional data;machine learning;dimensionality reduction
tree matching;web pages;regular expressions;database;tree-based;data extraction;algorithm called;data items;data records;extracted data;web data extraction;similar patterns;wrapper generation
information seeking;context-sensitive;branches visited by chronological backtracking;search tree
search algorithms;breadth-first search;solution path;memory requirements;heuristic search;search problems
multi-robot;robotic systems
supply chain management;autonomous agents;trading agents
reinforcement learning;learning problems
matching techniques;branching factor;vector machine;heuristic algorithms;human players;evaluation function
search tree;kernel methods;branches visited by chronological backtracking;word sense disambiguation
clustering

partial plans;inductive logic programming;limited resources;resource-constrained;past experience
feature space;data points;fundamental problem;order-preserving;character recognition;natural language processing;feature vectors
meta-learning;recommender systems;learning algorithms;information retrieval;instance;general problem;game theory
transfer learning;multi-resolution;knowledge transfer
plans;objective function
branches visited by chronological backtracking;search tree
support vector machines;branches visited by chronological backtracking;search tree
function approximation;belief-state;action selection;reinforcement learning;markov decision processes;action selection;selection techniques;bayesian learning
domain-independent;semantic matching;semantic similarity;web service composition;planning algorithms;domain-specific;web services;semi-automated
behavior patterns;higher level
branches visited by chronological backtracking;intelligent systems;search tree
search tree;branches visited by chronological backtracking;word sense disambiguation
branches visited by chronological backtracking;search tree
integrating data from;branches visited by chronological backtracking;search tree
web documents;sparql queries;semantic web;web resources;web services;environmental data
branches visited by chronological backtracking;search tree
artificial neural networks;video game;increasingly complex;interactive learning
intelligent tutoring system;video game
branches visited by chronological backtracking;search tree
human-robot
cost-effective;branches visited by chronological backtracking;search tree
pattern-recognition;neural networks;complex objects;1;classification accuracies;lighting conditions;neural networks
speech recognition;human-robot;audio/visual;autonomous navigation;higher level
complex structures;theraulaz and bonabeau, camazine et al. 2002;desirable properties;high level
artificial intelligence
vision based;multi agent
open-source;intelligent agents;learning algorithms;knowledge-based;markov networks;information extraction;model construction;inductive logic programming;inference algorithms for;link prediction;markov logic;markov chain;real world;monte carlo
boolean satisfiability;highly structured;constraint satisfaction;problem hardness;real-world applications;worst-case;parameter settings;problem domain;instances
decision procedure for;search tree
complexity bounds;graphical models;constraint networks;theoretical properties;information processing;local consistency;lower bounds;tree decomposition;bayesian networks;theoretical bounds;information propagation
search space;search algorithms;instances;constraint satisfaction;randomly generated;soft constraints
belief states;increasingly complex;probabilistic data;hidden markov models;state estimation;belief state;state estimation;normal form;embedded systems
knowledge compilation;normal form;partially ordered;partially) ordered
problems arise;bound-consistency;finite-domain;computational cost;inference rules
problem instances;high-confidence;monte carlo;solution space;model counting;additional constraints;highly structured;model counting;np-complete problem;high probability;propositional satisfiability;markov chain;high precision
constraint satisfaction problem;decision support systems;binary decision diagrams;user-friendly;user interaction;total cost
lower bounds;inference rules
distance metric;lower bounds;path-consistency;randomized algorithms
soft) constraint;efficient representation;constraint satisfaction;local consistency;cardinality constraints;constraint satisfaction problems
instance;lower bounds;lower bound;instances
search space;answer set;model generation;propositional satisfiability;sat-based;answer sets
search methods;data structure;concise representation;local-search techniques;performance gains;search problems;local-search algorithms
boolean satisfiability;pruning techniques;boolean satisfiability;large number of;optimization problem;instances;wide range;iterative algorithm;sat-based;programming models
constraint satisfaction;disjunctive temporal;sat-based;search space
constraint programming;global constraint
scheduling problems;bound consistency
operator;answer set;answer sets;answer sets;answer set semantics

normal form;normal form;search space
intelligent tutoring systems;action sequences;finite-state;problem solving;multiple data sources
user study
intelligent tutoring system;mixture model;intelligent tutoring systems
reinforcement learning algorithm;human behavior;sealed-bid;reinforcement learning;decision making

user activity;desktop search;social network;knowledge base;clustering approach
virtual environments;goal recognition;bayesian networks;predictive power
general concept;normal distributions;normal-distribution;gaussian distribution;normal distribution;quantitative analysis;natural images
relational learning;instance;exemplar-based;cognitive model;similarity-based
world-wide web;preference-based;users' preferences;user study;search tools
planning problem;memory requirements;reasoning capabilities;inter-related;resource-bounded;model-checking
qualitative decision;end user;decision process;decision making
maximally-contained rewritings;model counting;large number of;general case;materialized views;semantic web;information integration;query-rewriting;normal form
temporal logics;temporal logics
logic programming;desirable properties;computational complexity;preference elicitation;knowledge bases
logic program;graph-theoretic;answer sets
closed world;bounded treewidth;bounded treewidth;knowledge representation and reasoning;logic programming;worst case
belief change;belief change;belief update;ranking functions
model-theoretic;nonmonotonic reasoning
description logic;description logic
upper bound;19;conjunctive queries;description logics;description logics;semantic web;data repositories;query answering;data complexity;ontology languages;finite model;inference techniques
logical properties;model-theoretic;knowledge bases;computational complexity;merging operators
situation calculus;situation calculus;domain descriptions;programming languages
decision tree classifier;classification problem;classification;private information;databases;classifier
np-complete;computational complexity;completeness
knowledge based;semantic role labeling
clustering;real data sets;density-based;clustering algorithms;local density;meaningful clusters;finding clusters;algorithm exploits;dimensional data;density-based clustering algorithm;clustering quality
real-world;social choice;reinforcement learning;state abstraction
data sets;classification problems;classifier;multiple classifiers;theoretical properties
high-dimensional;topological properties;face recognition;classification tasks;feature extraction;embedding methods;original data;machine learning;tensor space;higher-order;video retrieval;data representations;dimensionality reduction
clustering;clustering;graph theory;sufficient conditions;np-complete;constraint sets;problem instances;worst case;artificial intelligence
nonnegative matrix factorization;matrix factorization;theoretical basis for;real-life datasets;objective function;hybrid method;chi-square;document clustering;latent semantic indexing;local minima;latent semantic indexing
decision trees;real-life applications;training examples;decision tree;learning tasks;greedy approach;decision tree learners
temporal difference;function approximation;reinforcement learning;policy evaluation;temporal difference learning
feature space;range images;active learning;active learning;negative examples;instances;instance;automatic generation of;visual concepts;positive examples
planning algorithms;state representation;theoretical framework;learning algorithms;hidden state
point-based;partially-observable;dynamical systems;structural properties;latent-variable models;decision processes
clustering;bayesian model;related concepts;relational model;semantic knowledge;learning systems
feature space;regression tasks;classification;vector machine;inductive logic programming;kernel methods
induction algorithms;learning algorithm;algorithm selection;classification performance;data sets;parameter tuning
convex optimization problems;algorithm iteratively;large datasets;regression) algorithm;theoretical results;optimization problem;machine learning;regression problem;specifically designed to;logistic regression;requires solving;classification problems;objective function;convex optimization problem
minimum description length;mining algorithms;closed patterns;classification;inductive inference;equivalence class;minimum description length;algorithm called;frequent closed
target task;transfer learning;reinforcement learning;knowledge learned;algorithm takes;heuristic search
matrix factorization;training data;unlabeled data;training data is;multi-label learning;large number of;text categorization;optimization problem;matrix factorization;high similarity;classification techniques;semi-supervised;small size
training set;regression method;unlabeled data;classification;support-vector machines;classification methods;knowledge-based;prior knowledge;learning problem;support vector;support-vector;semi-supervised;kernel methods
discriminative training;clustering;regularization;latent variables;classification;topic models;exponential family;logistic regression;text data sets;latent structure;conditional probability;conditional random fields;classification error;latent dirichlet allocation
detailed comparison;record linkage;candidate matches;machine learning;record pairs;ad-hoc;data sets;matching records;record linkage is;matching problem;automatically learn;record linkage is
problem solving;cascade model;instance;provide evidence;problem-solving;qualitative reasoning
music information retrieval;sequence alignment;function approximation;synthetic data;database;real-world;negative examples;sequence alignment;gradient boosting;gradient boosting;dynamic programming approach;music retrieval
efficient inference;auxiliary;real-world;inference algorithm;markov networks;belief propagation;inference methods;statistical relational learning;gibbs sampling;markov logic;classification problems;satisfiability testing;combines ideas from
synthetic data;learning tasks;classifier
community structures;social science;large networks;clustering methods;fundamental problem;neighborhood information;weighted graphs
probability estimates;meta-learning;misclassification costs;learning algorithms;cost-sensitive;cost sensitive;training instances
decision tree learning algorithm;misclassification costs;test examples;medical diagnosis;cost-sensitive;machine learning;test strategies;case study;cost-sensitive;machine learning algorithms;total cost
planning problems;memory usage;efficient inference;satisfiability testing;relational domains
state space;reinforcement learning;state variables;reinforcement learning;state-action;markov decision process
algorithm performs;decision tree learning algorithm;naive bayes;conditional independence;learning algorithms;large number of;decision-tree;decision-tree learning algorithm;large data sets;data sets;training data;text classification;benchmark data sets;space complexity
multiple tasks;neural networks;cross-domain;machine learning;knowledge transfer;learning method
multiple users;genetic algorithm;evolutionary process;conflict resolution;salient features;evolutionary computation;conflict resolution
function approximation;real-world;reinforcement learning;reinforcement learning problems;optimal value function;temporal difference;evolutionary computation;state-action
perform inference;parameter estimation;dynamical systems;real-world;linear dynamical systems;generative model
state space;decision tree;sample data;state abstraction;transition probabilities;state abstraction;reward functions
outlier detection;vector machine;large margin;training process;training methods;support vector machines;margin
accuracy compared to;probabilistic framework;training data;data points;labeled data is;classification;distance metric learning;data distributions;information retrieval;distance metric;distance metrics;separability;kernel-based
multiple criteria;expected cost;semi-markov;reinforcement learning;markov decision processes;sensor network;np-hard;decision processes
evolutionary algorithms;convergence rate;theoretical analyses
search space;genetic algorithms;extraction algorithm;classification;genetic algorithm;large scale;feature extraction;linear combination;face recognition;classification performance;training samples;generalization ability;face databases;basis vectors;dimensional data;excellent performance;space complexity
multi-class problems;class-imbalance learning;misclassification costs;cost-sensitive learning;unified framework;multi-class;binary-class
expected utility;black-box;complete set of;agent performance;low variance
distributed algorithms;multi-robot;robotic systems
search engines;multi-agent systems;ranking systems;basic properties
social welfare
arbitrarily large;theoretical results
computational complexity;multi-player;social choice;game-theoretic;normal-form
search algorithms;general case;rank aggregation;multiple agents;voting rule;np-hard
linear programs;final ranking;rank aggregation;multiple agents;voting rule;np-hard
instances;multiagent systems;voting rules;computationally hard;np-hard
generic model;application area;data fusion;coalition structure;instance;sensor networks;sensor network
np-complete;bounded-size
maheswaran et al. 2005;design decisions;multi agent
problems arise;multi-agent systems;distributed environment;large-scale
constraint satisfaction;combinatorial optimization problem;assignment problem;optimization problems;np-hard
multi-agent systems;inductive learning;interactive learning;multi-agent;instances;inductive learning;multi-agent;single-agent
utility function;optimization criterion;utility functions;preference elicitation;multi-attribute
algorithm runs in;utility functions;context-specific
formal semantics;mental models
multi-agent
compact representation;representation scheme;solution concept;concept called
distributed algorithm for;scheduling problems;search space;combinatorial optimization;dynamic programming algorithm;worst case;information exchange
desired behavior;control mechanism
single agent;exhaustive search;temporal patterns;action sequences;activity recognition;spatio-temporal;low-level;recognition problem
virtual organizations;multiagent systems;information resources;large-scale
computational properties;computational model;logic-based;inference problem;game-theoretic
computationally hard;partial information;probabilistic information;relevant information;optimal design;mechanism design
private information;convex programming;real-life;scoring rules;implementation issues;scoring rules
human users;sensor data
linguistic information;question answering;text processing
data collections;information theory;text categorization;classification performance;supervised methods;term weighting scheme
document images;document image;language identification
similarity metric;semantic similarity;knowledge-based;information retrieval;data set;vector-based;corpus-based;product descriptions;short texts;text classification;short text;error rate;corpus-based
higher precision;decision trees;instance-based learning;fold cross-validation;annotated data;high precision;semantic relations;support vector machines;learning task;corpus-based
search space;domain-independent;planning problem;temporal logic;benchmark domains;heuristic search
exact inference;graphical model;hidden markov model;corpus-based;machine learning techniques
single agent;parallel algorithm;plans;ad-hoc;randomly generated;autonomous agents;operator;multiple agents;plan execution
theoretical analysis;planning algorithms
clustering;state space;closed-loop;control policy;large-scale;sampling based;real-world;related) data;reinforcement learning;feature-based;optimization problems;shortest path;action space;support vector regression;resource allocation
planning problems;hill climbing;planning domains;breadth-first search;optimal planning;planning graphs;upper bound;external memory
real-world;hierarchical algorithm;model-based diagnosis;computational complexity
monte carlo;execution model;hypothesis testing;plans;domain model
large number of;dynamic model;causal relationships
randomized) algorithms;a, b;real-life;c, d;g, h;reasoning problems;simple temporal;e, f;execution times
description language;common features;blocks world;database;action language;general-purpose
real-world;actual values;plans;probabilistic planning;missing information
event sequences;plans;np-complete
lower bounds;contract algorithms;ijcai
temporal planning;multi-modal;real-world domains
multi-agent;partial-knowledge;ai planning;blocks-world;partially observable;belief state;partial observations;decision making
response times;completeness;based reasoning;compact representation;space requirements;embedded systems
planning problems;plans
large-scale;database;large quantities of;web-scale;information retrieval;probabilistic representation;common sense;precision/recall;common sense
unique features;partially observable;algorithm maintains;relational structure;algorithm takes;partial observations;relational domains
partially observable;approximation guarantees;action models;partially observable domains;algorithms rely on
planning algorithm;optimal plan;plans
internal state;natural images;direct access to;artificial agents
search techniques;search effort;grid-based;search algorithms
high-dimensional;plan execution;plans;low dimensional;search algorithm;control parameters
visual similarity;segmentation problem;image segmentation;vice versa;semantic similarity;instances;semantic content;manual labeling;boundary detection;semantic similarity
error-prone;monte carlo;probabilistic models;particle filters;manual tuning;model parameters;posterior distribution;motion models
involving multiple;computational requirements;multi-robot;stochastic local search
monte carlo;meaningful results;instances;probability distribution function;sensor networks;sensor data;sensor network;range data;maximum likelihood;markov chain;local minima
high-speed;software architecture;probabilistic reasoning;machine learning;ai technologies;human intervention
ground truth;unlabeled data;wireless sensor network;context-aware;estimation problem;labeled data;sensor network;higher accuracy;accurately detect;mapping function;sensor-network;artificial intelligence;manifold regularization
search space;taking into account
human users;learning performance;reinforcement learning;reinforcement learning;provide feedback;implicit assumption;user study
complex structure;security problems;real-world;knowledge discovery;service providers;acm sigkdd;data mining;kdd workshop
content-based filtering;benchmark data;training set;private information;structural properties;confidential information;real-life;low-dimensional;1;3;2;learning approaches;real world;statistical models;classifier
intrusion detection;hidden markov model;false positive;normal behavior;detection rate
extracted knowledge;large-scale;real-world;real-world;decision-making;traffic monitoring;knowledge discovery;high degree of;fuzzy inference system;multi-criteria decision-making;long-term;multi-criteria;global analysis
detection techniques;file content;information-theoretic;detection accuracy;malware detection;wide range;statistical analysis;classification results;data mining based;data mining algorithms;malware detection
game theoretic;classification problem;mining framework;classification;extracted features;highly competitive;feature extraction technique;prior knowledge;incomplete information;filtering techniques;data mining;machine learning techniques;support vector machines;margin;classifier;classification algorithms
data mining;security policies;security problems;information sharing;data security;data warehouses;data security;privacy preserving data mining;semantic web;data confidentiality;databases;data management systems
share information;information sharing
shortest paths;communication patterns;information sharing;information sources;social network analysis;social network;sensitive data;social networks;total number of;social networks;random graphs;missing information;obtain information
service oriented;information management;united states;semantic web;answer questions;semantic web
data mining applications;data mining application;security applications;data mining
analysis tool;security problems;information processing;web servers;web server
high-quality;data mining and knowledge discovery;human computation;amazon mechanical turk;conference on knowledge discovery and data;human computation;user generated content;relies heavily on;acm sigkdd;online content;human computation;pattern discovery;program committee;knowledge extraction;game theory;optical character recognition
multi-player;user-centered;user-centered;machine learning models;online social network;design process;social context
amazon mechanical turk;diverse set of;image annotation;image annotations;human computation
data collection;community-based;community-based;data collected;goal-oriented;human computation;data quality;online communities;web users
human computation
data collected from;image search;1;relevance judgments
web pages;web search;applications including;clickthrough data;human computation;human relevance judgments;web search engines;query refinement
mechanical turk;web service;human computation;mechanical turk;programming paradigm;increasingly popular
improving web search;web page;improving web search
machine learning algorithms
high throughput;amazon's mechanical turk;visual representations;web application;control flow;high-level;end user;low-level
search results;human effort;search engine;human computation;search result;human computation;rank aggregation
cost-effective;human computation
labeling effort;unlabeled data;17;interactive learning;active learning;labeled instances;active learning framework;decision-making;active learning methods;human experts;instances;22, 23;5;labeled dataset;learning approaches;hand-written
image retrieval;image retrieval;automatic methods;low-level features;fundamental problem;image annotations;2, 3
image labeling;image data sets
public domain;design principles
artificial intelligence;human computation
multi-player;human computation;production") systems;game-theoretic;human computation;ai technologies;game theory
formal models;amazon mechanical turk;game-theoretic approach;correct" answers;human computation
upper bound;data sets;labeling problem;data instances;human computation
web-based;real-world;amazon's mechanical turk
clustering;social science;classification;spatial regions;time series;classification;supervised learning;unsupervised learning
machine learning algorithms;detection method;sensor data;data processing;machine learning algorithms;classification rate
multi-dimensional;data mining techniques;data mining;regression models;large amounts of data;data mining;data mining problem;small-scale
clustering;classification;classification;data mining techniques;principal components;principal components;principal component analysis
item pairs;data mining method;everyday life;correlation coefficients;data mining;data mining method
data mining technique;classification;classification;database;association rule mining;key parameters;data mining
medical imaging;computed tomography;machine learning algorithms;artificial intelligence;medical images
medical diagnosis;medical diagnosis;case base;knowledge representation;problem solving;medical knowledge;standard sql
real data;neural networks;phase transition;data mining system
distance matrix;online auctions;online auctions;online auction;structure information;mobile phone;simple algorithm;high volume
customer segmentation;pattern recognition;data mining methods;classification;database;data clustering;real customer;problem solving;association rules
web pages;multi-layer;decision tree;machine learning;knowledge extraction;web mining;nearest neighbour;support vector machines;machine learning algorithms
linear model;data mining methods;vector machine;time series;time series;multiple linear regression;feature selection
test set;classification;data mining techniques;variable selection;search process;classification accuracy;data mining;input variables;classifier;low dimensional space
artificial neural networks;data-mining techniques;data-mining techniques;fuzzy logic;fuzzy inference system
detection problem;anomaly detection;interesting patterns;detect anomalies;extracted knowledge;normal behavior
domain experts;control systems;closed-loop;time series;regression models;simulation experiments;data collected from;data mining technology
agent based;data mining
neural networks;computational framework;data reduction;case study;building classifiers;support vector machines;effective tools
classification model;case study;unstructured text;missing values;human-centered;highly accurate;knowledge discovery;domain-specific;real life;formal concept analysis;classification errors;expert knowledge
case study;missing values;data mining
text search;similarity searching;ad hoc;feature extraction methods;retrieval performance
ground truth;training data;unlabeled data;word recognition;training examples;semi-supervised learning;training set;industrial applications;recognition accuracy;depends crucially on;word recognition;unlabeled set
similarity analysis;test image;frequency domain;feature extraction technique;sliding window;similarity analysis;classifier;frequency domain;sliding windows;neural network
clustering;data objects;categorical data;real datasets;hidden information;distribution information;traditional clustering algorithms;clustering algorithm;clustering
distance measure;medical data;entity identification;heterogeneous data sources;entity identification;compression based;databases;data representations
rule mining;higher degree of;partially labeled;missing data;sensor data;large amounts of data;general problem;constraint based;sequential pattern;partially labeled

similarity analysis;similarity analysis;combination methods;pattern recognition;classification decisions;classification;features extracted;feature extraction;training process;higher order;classification performance;instances;higher order;hidden layer;neural network;classifier
multi agent systems;genetic algorithms;ant colony optimization;simulation model;large scale;wireless communication;multi agent;optimization scheme;multi-agent;decision making;performance prediction;multi agent systems;data mining;decision making;ant colony optimization
synthetic data sets;data anonymization
decision support systems;applications including;homeland security;knowledge discovery from sensor data;data fusion;raw data;international workshop on;massive volumes;high-priority;sensor networks;distributed data;knowledge discovery
modeling assumptions;increasing attention;real data sets;uncertain databases;uncertain data;uncertain database;data mining;mining uncertain data;mining uncertain data;knowledge discovery;data sets;acm sigkdd;mining tasks;probabilistic reasoning;uncertain data;data analysis tasks;object tracking;data integration
digital camera;recommendation service;learning algorithm;decision support;online stores;collect data;sample selection bias;online auction;cost-sensitive;online auction;business opportunities;prediction models
plays an essential role in;frequent patterns;uncertain data;knowledge discovery;real-life;data mining;association rules
data mining algorithms;document classification;classification;classification algorithms
predictive models;biological networks;data describing;graph identification;inter-dependencies;explicitly models;social networks;missing information;incomplete data;communication networks
uncertain information;classification;naive bayes;instances;set-valued;classifier
classification problem;taking into account;partial knowledge;training data;supervised learning;identify patterns;classifier
interaction data;uncertain data;closely related
predictive performance;feature values;addressing this problem;expected) values
data structure;spatial data;large-scale;design choices;spatial datasets;knn queries;scalable distributed;data storage
xpath queries over;xpath queries;sql queries;xml data;schema-based;query optimization techniques;xml views;operator;query answering;optimization techniques;query translation
error-prone;window queries;search algorithms;location-based queries;wireless communication;knn queries;data access;location-based;spatial index;continuous query
service-oriented;service providers;web services
virtual world;data collection;radio frequency identification;event processing;rfid) technology;rfid data;event detection;complex events;business applications;data processing;application logic;high volume;event-based;data integration;physical objects
large distributed;privacy-preserving;access control;access-control;real-life;privacy guarantees;privacy-preserving;content providers;trade offs
multiple labels;energy functions;graph cuts;optimization algorithms;graph cut;multi-label;multi-label;excellent performance;stereo correspondence
error propagation;scene flow;moving objects;image domain;optical flow;moving objects;reference image;stereo camera;scene flow
energy function;image segmentation;graph cuts;level set;gradient descent;intensity values;local minima;image segmentation
graph matching;graph theory;graphics processing units;graph matching;bipartite graph;matching problem;processing power
face images;facial images;face verification;database;block size;energy minimization;test images;matching method
multi-scale;modeling approach;multi-scale
heuristic functions;search algorithms;energy minimization;globally optimal;iterative algorithms
variational formulation;vector valued;image processing;color images;image data
regularization;low-resolution;motion model;motion estimation;variational formulation;high-resolution;motion models
optical flow;optical flow;decomposition methods;image sequence;multi-channel
distance functions;image segmentation;distance function;fourier transform;image analysis;image analysis;shortest path;surface reconstruction;closed-form solutions;closed form solution
signal processing;numerical results;image analysis;optical flow;flow field;optical flow;edge detection
region features;local features;natural images;region-based;local region
12;high level;database;feature-based;clustering techniques;9;image-segmentation
regularization term;regularization;image feature;globally optimal;spatial-temporal;regularization;variational approach;large variations;appearance model;temporal smoothness
high quality;higher order
remote sensing images;remote sensing images;geometrical constraints;parameter estimation;prior information
video data;state space;motion analysis;motion estimation;particle filter
long-range;spectral graph;image segmentation

point sets;convergence rate;objective functions;computational costs;industrial applications;numerical experiments;point set
shape space;step size;motion field;shape space;level set;variational approach;multi---scale;finite element;optical flow
fixed-point;image registration;variational formulation;image registration;optical-flow
shape space;classification;decomposition approach;image analysis;multi---resolution;complex objects;finite element;shape matching
image patches;exemplar-based;variational formulation;exemplar-based
synthetic and real images;variational framework;geometric structure;image denoising;local image;local image;variational framework
kernel density estimation;window size;filtering methods;color images
color image;local neighborhood;image deblurring;image super-resolution;image denoising;image restoration;image information;local information;image processing;image restoration
flow fields;motion field;flow fields;variational approach;image sequences;image sequence;takes into account;image information;real world;local information;additional information
color image;image segmentation;mixture model;basis functions;distance function;color images;closed form solution;spatially varying;local orientation;image segmentation
color image;color image;local geometry;basis functions;spatially varying;main idea;spatially varying;local orientation;color images
decomposition method;numerical results;local orientation
global optimal;color information;multiple objects;variational approach;energy functional;object detection;convex formulation;user interaction;global optimal
face images;gaussian process regression;database;regression problems;regression problems;linear transformation;learning algorithms;optimization problem;real-world applications;age estimation;learning scheme;metric learning;regression methods;distance metric;test data;prior knowledge;learned metric
clustering;data point;unlabeled data;label information;classification;generative process;generative models;sample set;hidden markov models;generative model;separability;clustering
confidence measure;user input
support vector machines;regression problems;training examples;large-scale;large-scale;support vector machines
instance;cutting plane algorithm;local search;svm-based;optimization methods;instance-level;benchmark data sets;instances;margin;multi-instance learning;local minima;multi-instance learning;content-based image retrieval
monte carlo;active learning;inverse reinforcement learning;active learning;inverse reinforcement learning;higher dimensional;inverse reinforcement learning;general problem;reward function
classification;classification;iterative process;learning procedure;collective classification;low complexity;classification algorithm
clustering;training set;graph theory;machine learning problems;instances;graph-based;feature ranking;graph-based;classification performance;instance;real-life;label noise;semi-supervised learning;operator;classification problems;competence
underlying data distribution;total number of;classification techniques;classification;streaming environment;labeled instances;test instances;learning algorithm;instances;data stream;training instances;data streams;classification task;stream classification techniques
neural networks;game tree search;evaluation functions;agent systems;human intervention;neural networks
privacy concerns;web searches;general-purpose;data collected from;search queries;statistical relational learning;search engine;log data;markov logic;search session
gradient-based;partition function;hidden variables;inference procedure;time series;state variables;time series;motion capture data
feature set;irrelevant features;learning algorithm;feature selection;variance reduction;supervised learning;relevant features;feature selection
search space;lower bound;distance-based outlier detection;outlier detection;pruning rules;real datasets;detection process;distance-based;detection methods
retrieval precision;dirichlet prior;document set;topic model;topic models;information retrieval;storage requirements;maximum likelihood;information retrieval;latent dirichlet allocation
multiclass classification;multiclass classification;error-correcting output codes;classifier;performance gain
domain knowledge;data analysis;frequent patterns;closed patterns;knowledge extraction;scoring function;graph databases
conditional likelihood;baum-welch;parameter optimization;parameter learning;posterior probabilities;classification;bayesian network classifiers;learning algorithms;bayesian network classifiers;data sets;parameter estimates;conjugate gradient;parameter learning;1;classification results;naive bayes;network structures;lower-bound
mining task;pattern mining;optimization problem;data set;making decisions;greedy algorithm;real world;algorithms rely on;spatial data mining
multi-label classification;computational complexity;multi-label classification;predictive performance;large datasets;multi-label;classifier
classification problem;automatic extraction of;natural language text;dependency trees;tree kernels;kernel-based;text understanding;relation extraction;structural features;data set;tree kernels;relation extraction;automatically generated;relation extraction;semantic retrieval;support vector machines;structured data;syntactic structure;natural language text
multi-relational;prior knowledge;predictive performance;formal representation;social network data;learned model;description logic;statistical inference;real world;graphical model;learning task;statistical relational learning
training set;active learner;generalization error;active learning;monte-carlo;sampling strategy;application domains;limited number of;tree structured;learning problem
capacity control;large datasets;classification;instance;classification methods;data instances;control problem;instances;partial order;partially ordered;feature sets;worst case;linear classifiers;graph datasets;partially ordered;hypothesis class;capacity control;partially ordered;feature sets
data mining;data set;27;independent component analysis;random projection;principle component analysis;sensitive information;high efficiency;random projections;original data;gaussian distribution;perturbed data;rates;privacy-preserving data mining
text mining;heuristic approaches;synthetic data;automatic methods;real data;statistical model;text corpora;language modeling
clustering;source code;image datasets;knowledge transfer;source data;intrinsic structure;class labels;transfer learning
frequent item sets;database;databases;inductive databases;databases;query result;classifier
high correlation;clustering problem;high-dimensional;classification;predictive power;multi-attribute;multiple attributes;data clustering;real-world applications;pattern mining;highly correlated;high correlation;multi-attribute;mutual information between;attribute sets
graph structure;conditional random field;joint inference;monte carlo sampling;joint inference;markov chain;programming language
auc;lda model;latent dirichlet allocation;classification;classification accuracy;similar topics;selection method;document categorization;latent dirichlet allocation;topic distribution;web document;specific topics;labeled training
real-world datasets;transductive learning;propagation algorithm;graph-based;learning algorithm;mutually exclusive;transductive learning;prior information;multi-label;convex optimization problem
training data;binary classification problems;error-correcting output codes;text data;classifier;text classifier;svm classifier;multi-class;classifier
1;statistical properties;policy evaluation;learning algorithm;policy evaluation;reinforcement learning;model-free;online learning;online learning;statistical inference;estimation error
classification;theoretical properties;features extracted from;generally applicable;time series;cross-correlation;time series;similarity function;data sets;similarity functions;kernel methods;machine learning algorithms
clustering;real-world datasets;wide range;subspace clustering;subspace clusters;model selection
clustering;clustering;parameter estimation;clustering algorithms;variational inference;probability distribution;clustering model;data matrix;data matrices;real data sets;variational bayesian;collapsed gibbs sampling;clustering approach
face recognition;mutual information;graph embedding;feature extraction;spectral analysis;geometric information;data sets;high-quality;error rate;graph embedding;feature extraction methods
ir) techniques;data analysis;confidence scores;high-recall;databases;information retrieval;vector space;identification problem;language modeling approach;ir models;query sets;language modeling
nonlinear regression;equivalence class;conditional independence;causal models;independent component analysis;causal discovery;brute-force search
regularization;decision function;data points;large scale;semi-supervised learning;computational complexity;semi-supervised learning methods;geometric information;data sets;low dimensional;high density;dimensional data;semi-supervised;learning method
low-dimensional space;linear discriminant analysis;object databases;linear discriminant analysis;gaussian distribution;small sample size;missing data;semi-supervised;principled framework;semi-supervised;dimensionality reduction
regression method;unlabeled data;multi-task;semi-supervised learning;kernel function;learning performance;machine learning applications;data sets;labeled data;multi-task learning;gaussian processes;semi-supervised;kernel parameters;semi-supervised;multi-task
discriminant analysis;classification;linear discriminant analysis;kernel discriminant analysis;squared error;fisher discriminant analysis;benchmark data sets;dimensionality reduction
detection problem;social security;sequence classification;classification methods;social security;sequential patterns;case study;sequence classification;classifier;transactional data
dynamical systems;partially observable;dynamical systems;reinforcement learning;partially observable;internal structure;markov decision process
source code;auc;training data;unlabeled data;target domain;1;sample selection bias;labeled data;training and test data;2;maximum margin;transfer learning
feature weighting;desirable properties;relevant features;linear models;application domain;decision rules;predictive power;training data is;variable selection;predictive performance;learning machines;accurate predictions;feature spaces;interpretability
knowledge sources;great potential;natural language
text classification;text documents;classification problem;data set;text documents
natural language processing;semantic graph;machine learning techniques;web page;page content;graph representation
multi-dimensional;sensitive data;highly interactive
semantic annotation;machine learning methods;web service;semi-automatic;fully automatic;web services;graphical user interface;machine learning algorithms;semi-automatic
classification accuracy;learning task;information extracted from;semi-automatic;learning framework;semi-automatic
frequent patterns;sequential patterns;data collected from;privacy preserving;discovering patterns;sequential pattern
web documents;language models;web pages;temporal dimension;language models

machine translation;statistical machine translation;web technologies;news items;statistical information;statistical machine learning;support vector machines;data gathering
community-based;community-based;data mining techniques;data mining;machine learning;machine learning;scientific discovery;querying capabilities
statistical properties;social interactions;real-world;huge amounts of;business intelligence;social communities
semi-supervised learning;theoretical analysis;machine learning;statistical machine learning;learning theory;multi-task learning;privacy-preserving;theoretical analysis;machine learning
statistical approaches;recommendation systems;machine translation;optical character recognition;speech recognition;search engines;vast amounts of data;artificial intelligence
ecml/pkdd;web-based;semantic web;semantic web;linked data
web search;valuable information;provide answers;log mining;search queries;search engines;query logs;web search engines
clustering;text mining;document classification;historical information;topic detection;social networks;entity recognition;multi-label;text analysis;online news
multilabel classification;multiple labels;classification;instance-based learning;learning algorithms;logistic regression;evaluation criteria;predictive accuracy;instance;instance-based learning;logistic regression;similarity-based;multilabel classification;multilabel classification;nearest neighbor;instance-based
structured output;feature space;learning algorithms;inference algorithm;real world;prediction algorithms
great flexibility;prediction performance;linear svms;data set size;1;sparse kernel;support vector machines;\sum^{\#sv}_{i=1} \alpha_ik(x_i, x)\right;support vectors
optimization criterion;fixed point;policy evaluation;policy evaluation
probability estimates;active learning;misclassification costs;active learning methods;cost-sensitive;cost sensitive;uncertainty sampling;uncertainty sampling;uncertainty sampling
bounded-size;graphical models;probability distributions over;linear representations;efficient inference;probabilistic inference
posterior probabilities;cost-sensitive learning;cost-sensitive;multiclass problems;decision boundaries;probability values;margin
real graphs;edge/weight;real-world graphs;model parameters
compact representation;clustering;sequence mining;human activities;application domains
search space;large parts;performance gain;databases;knowledge discovery;discovery algorithm;numerical attributes;discovery algorithms
data mining;nearest neighbor;classification;classification accuracy;stream mining;performance gain;data streams;data streams;quality measure;benchmark data sets
database;data distribution;databases
high-dimensional;bayesian model;high-dimensional;small sample-size;application area;sample size;hierarchical model;highly correlated;input variables;high dimensionality;dimensionality reduction;latent variables
forward selection;base classifiers;algorithms require;pattern mining;prediction accuracy;ensemble pruning;pattern mining;mining process;ensemble pruning;pruning algorithm;ensemble pruning;tree structure;transaction database
auc;multi-class;theoretical properties;decision tree learner;decision tree;naive bayes;discovery algorithm;weighting scheme;evaluation measures;multi-class;class distribution;evaluation measures
phase transition;phase transition;np-complete;learning algorithms;multi-relational databases;instances;general- purpose;relational learning;real-world applications
topic distribution;latent dirichlet allocation;generative models;topic models;text corpora;topic modeling;automatically generate;benchmark datasets
computational power;classification;storage space;classification task;computation costs;distributed classification;data sizes;communication cost;synthetic datasets;class distribution;fault-tolerance;high availability
probabilistic model;instance;structured prediction;structured prediction
local patterns;minimum support threshold;frequent patterns;graph-evolution;graph evolution;minimum confidence;large networks;real-world networks;social networks;association rules
state space;dynamic bayesian networks;particle filters;probability distribution over;hidden states;particle filtering;dynamic systems;dynamic bayesian network;real world;hidden variables
xml tree;mining algorithms;classification;frequent patterns;classification framework;closed patterns;classification methods;streaming data;evolving data streams;classification method;classification;evolving data streams;data streams;stream classification;xml tree
closed itemsets;condensed representation;temporal dimension;condensed representation;theoretical results
real-world datasets;search space;equivalence classes;equivalence class;database;quality function;additional cost;pattern discovery;np-hard
parameter space;document classification;similarity measure;document similarity;information retrieval evaluation;latent semantic indexing;information retrieval
text mining;clustering;distance metric learning;background information;high quality;prior knowledge;matrix factorization;data sets;text representation;document clustering;iterative algorithm;document clustering;semi-supervised;word clusters;semi-supervised
event logs;event logs;efficient search;complex systems;hidden structures;pattern discovery;clustering algorithm;real world
latent dirichlet allocation;document level;network connectivity;topic modeling;modeling approach;intrinsic structure
classification;network classification;network classification;local structure;classification method;learning problem;local structure;semi-supervised;image processing;random walks
multiple tasks;regression tasks;classification;predictive features;feature selection;biological datasets;multi-task learning;feature selection;select features;multi-task
machine learning problems;kernel-based;17;16;takes into account;greedy algorithm;regression algorithm;7;4,10;benchmark datasets;basis function
medical applications;margin;hierarchical clustering;margin;multi-class;learning task;classifier
feature space;svm algorithm;feature weighting;real world datasets;margin;standard svm;vector machine;predictive performance;feature selection;standard svm;original formulation;feature weighting;margin
multiple kernel learning;margin;kernel function;linear combination;kernel methods;compares favorably with;training instances;support vector machines;margin;objective function
ground truth;linear models;predictive power;machine learning;inference methods;inference algorithms;similarity measures
decomposition techniques;training set;ordinal classification;auc;ranking function;instances;decomposition methods;evaluation criteria;total order;ranking functions;unlabeled instances;labeled examples;multi-class
data-driven;machine learning methods;classification;text corpora;classification accuracy;naive bayes;data instances;classifier;instances;higher-order;vector spaces;feature values;text classification;bayesian framework;supervised learning;higher order
support vector machines;database systems;sql queries;syntactic information;databases;data mining;natural language;machine learning algorithms;natural language
gradient-based;baseline methods;unlabeled data;instance;domain description;learning strategy;network intrusion detection;optimization problem;domain description;labeled examples;recognition tasks;support vector;relevant information;semi-supervised;learning task;semi-supervised
matrix factorization;multiple views;multi-view;matrix factorization;exploratory data analysis;integrating multiple;unsupervised algorithm;clustering problems;model selection
clustering;regularization;data points;classification;regularizer;classification framework;semi-supervised learning;classification methods;regularization;semi-supervised learning methods;data sets;geometric structure;semi-supervised learning;data manifold;common assumption;takes into account
data set;mutual information;accuracies;selected features;evaluation criteria;feature selection algorithm;data sets;feature selection;selection criterion
high-dimensional;policy search;large number of;policy search;policy search;robot learning;learning framework
clustering;clustering;accuracy compared to;distance measure;clustering performance;graph datasets;shortest paths;vector data;graph data;clustering algorithm;fully connected
prediction accuracy;regularization;dimensional space;globally optimal;rule set;optimization problem;regularization framework;locally optimal;relational learning;real world;interpretability
generic model;sampling algorithm;topic models;topic models;domain-specific;bayesian networks
feature level;classification;feature selection method;transfer learning;scaling factors;model estimation;supervision;linear svm;microarray datasets;feature selection;classification performances;databases;dimensional data;transfer learning
logical reasoning;automated reasoning;bayesian network;relational models;markov networks;knowledge representation;bayesian networks;probabilistic logic;probabilistic graphical models
markov logic networks;graphical models;predictive accuracy;training examples;linear programming;max-margin;markov logic networks;statistical relational learning;max-margin;approximation algorithm;support vector machines;learning method
clustering;parameter-free;clustering approaches;high-dimensional datasets;clustering high-dimensional data;automatically determined;huge number of
fast algorithm;pruning techniques;suffix tree;dna sequences;probabilistic models;text data;estimation methods
search methods;simulation study;kullback---leibler divergence;learning bayesian networks;sample size;causal discovery;conditional independence;constraint-based;free energy;free energy;learning performance;maximum entropy;constraint-based;small samples;hypothesis testing;learning bayesian networks
kernel-based;kernel-based;long-range;risk management;gaussian processes;time-series;kernel methods;efficient learning
unified framework;function approximation;learned models;reinforcement learning;reinforcement learning
state space;function approximation;gaussian process;prediction performance;policy evaluation;basis functions;reinforcement learning;real-world applications;feature selection;feature selection;model selection;state variables;learning task;computational savings;model selection;feature selection
prior distribution;graph kernels;preference learning;real-world applications;probabilistic relational;relational information;gaussian processes;probabilistic inference;nonparametric bayesian;model selection
concise representations;level-set;feature sets;feature selection method;network intrusion detection;observed data;competitive performance;optimal kernel;feature selection;recognition tasks;linear program;support vector machines;level-sets;model parameters
local search algorithms;local search algorithms;learning algorithms;instance;convergence rate;global optimization;instances;computational resources;search algorithm
random variables;reinforcement learning;decision problems;giving rise to;reward functions;markov decision process
computational efficiency;intelligent agents;probabilistic model;relational models;relevant objects;large sets of;typically involves;relational domains;large state spaces
query optimization;relational databases;privacy concerns;sensitive data;query processing;databases;relational databases;benchmark datasets;sensitive attributes;design issues
multi-user;database;space overhead;database;scientific data management;provenance information;unique characteristics
intrusion detection;security problems;false positive;database systems;database;anomaly-based;database;user profiling;intrusion detection system;specifically tailored;role-based access control;detect anomalies;database management system
confidence values;data integrity;database;high quality;decision makers;high confidence;data item;additional cost;data integrity;query processing techniques;confidence level;query results;data-intensive;decision-making
fine-grained;personal preferences;access control;data security;access control
source code;access control model;access control model;access control
relational model;instances;user interface;access control;access control
access control;xml data;static analysis;xml documents;entire document;xpath query;native xml;databases;relational databases;times faster than
privacy-aware;range queries over;location based service;communication costs;user-defined;spatial queries;service provider;location information;moving objects;privacy-preserving;privacy requirements
data utility;data tables;sensitive data
pre-defined;indoor environment;sensor readings;increasing attention;real-world;goal recognition;dynamic model;real data;recognition algorithm;low-level;common assumption;wireless network;high-level;artificial intelligence;recognition problem
parameter values;fast algorithm;variational inference;activity recognition;learning algorithm;hidden markov model;exact inference;variational approximation;fixed point;context specific
training data;web search;network applications;large-scale;mobile devices;edges represent;location information;higher accuracy than;location-based services;bayesian framework;high-level
text corpora;activity models;sensor technology;activity recognition;learned models;learn models;common sense;labeled data;human activities;human intervention;human activity;natural language
error-prone;motion pattern;human effort;human activities;activity models;probabilistic models;activity recognition;learning algorithm;dynamic model;motion-pattern;sensor network;data collected;motion patterns;activity recognition;low-level;markov process
weighted graph
high probability;stochastic sampling;multiagent learning
search algorithm;constraint satisfaction problems;local optima
highly dynamic;single agent;autonomous agents;multi agent
mobile agents
multiple agents;service-oriented;service providers
plans;formal semantics
multi-agent systems;large-scale;multi-agent;dynamic nature of;social network;distributed information retrieval;social networks;network structures;agent-based
group members;social behavior
clustering;multiple agents;temporal order;highly correlated;event detection
np-complete;preference aggregation;preference aggregation
multi-agent;model-based diagnosis;distributed systems;np-hard
multi-agent;lessons learned;real-world applications
autonomous systems;state variables;plans;optimal control;plan execution
user community
domain knowledge;human behavior;modeling method;agent behaviors
generation algorithm;local search;global optimal;real-world;policy search;network structure
optimization technique;distributed computing;computing systems;resource allocation
ad hoc networks;mobile agents;network size
software agents;total number of;memory requirements
search cost;efficient algorithms to;search strategy;search strategies
task allocation
sealed bid;social welfare;auxiliary;multi-party
probabilistic modeling;multi-agent systems;formal framework for;multi-agent;task allocation;formal framework
autonomous agents;competence;ai research;artificial intelligence
computational model;agent model;model checking;multiple sources
hill climbing;network connectivity;stationary distribution;mobile agents;wireless networks;agent-based
provide evidence
common sense;reasoning problems;comparative analysis;qualitative reasoning
public domain;case based reasoning;case base;classification;case based reasoning
case-based reasoning;classification;formal concept analysis;association rules;knowledge base;artificial intelligence;query refinement
case-based reasoning;high-quality;input data;theoretical foundation;data sets;raw data;competence;data set;case base;case-base;additional features;competence
domain knowledge;task decomposition;domain-independent;planning domains;domain-independent;domain theory;retrieval precision;decomposition techniques;task decomposition;adaptation knowledge;case retrieval
private information;approximation algorithm;positive results;mechanism design
communication complexity;np-hard
social welfare;np-complete;linear programming;utility functions;takes place;special case
protocol called;computational overhead;combinatorial optimization problem;strategy-proof
approximately optimal;parameter space;computational complexity;worst-case;performance guarantees;linear transformations;average-case;general problem;mechanism design
multi-agent;boolean satisfiability;search space;single-agent;problem solving
search methods;greedy search;stochastic search;particle filtering;state estimation;dynamic systems;exact inference
knowledge compilation;normal form;completeness;binary decision diagrams
potentially infinite;knowledge base
knowledge compilation;decision diagram;normal form;model-based diagnosis
virtual environments;virtual worlds;camera parameters;decision making
belief change;belief base;computationally expensive;optimization process
robotic systems;belief state;hidden markov models;approximation technique;belief state;increasingly complex
optimization problem;complex systems;constraint-based;soft constraint;model-based diagnosis;vision-based
domain knowledge;recommender systems;recommender systems
real-world;graphical structure;completeness;probabilistic reasoning;decision problems
identification method;causal effects;causal models;linear models;identification problem
local search;highly structured;real-world;stochastic local search;learning problems;local minima

decision diagram;compact data structure;constraint propagation;constraint reasoning
computational complexity;constraint satisfaction problem;diverse set of;constraint programming;wide range;global constraints
probabilistic approach;bounded number of;limited number of;application domains
instances;search algorithms;constraint satisfaction problems;random 3-sat
clause learning;global constraints
constraint satisfaction problem;efficiently computing
general purpose
real life problems;constraint programming;low overhead;search tree
instance;space constraints
accurately identify;extracted information;constraint satisfaction;constraint programming;large number of;data sources;data sources;vector data;real-world;satellite images
answer set programming;constraint programming;general-purpose;formal framework;decision problems;propositional satisfiability;constraint satisfaction problems;search problems
satisfiability problem;phase transition
algorithm produces;simple temporal;high-quality;greedy algorithm
scheduling problems;fast response;dynamic programming;combinatorial optimization;dynamic environments;optimization problem;distributed systems
scheduling problem;sat-based;constraint satisfaction problem
constraint solving;approximation method;constraint-based
propositional satisfiability;low-overhead;model checking;application domains;multi-class
constraint satisfaction problem;model counting;np-complete;model-counting;bayesian inference;reasoning problems;bayesian networks;real-world;satisfiability testing
eliminee subsets of the players' strategies;eliminable relative to given dominator;computational methods;eliminability criterion for bimatrix games that
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
normal form;search algorithms;integer programming
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
concept called
reasoning tasks;natural language
classification problem;classification;accuracies;feature extraction;linear svms;data collection;accurate classifiers;design decisions
decision theoretic model;eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
recommender systems;large-scale;product feature;users interact with;multiple features;automatically generating;user study
user preferences;recommendation systems;decision-theoretic;np-complete;real-world;utility theory;multi-attribute;operator
eliminee subsets of the players' strategies;goal-directed;eliminability criterion for bimatrix games that;eliminable relative to given dominator
context-dependent;plan execution;planning tasks
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
instance;situation calculus;reasoning tasks;description logics;description logic
logic programming;planning problems;deterministic domains;plans;problem solving
propagation algorithm;inference algorithm;knowledge bases;inference process
key points;formal representation;argumentation based
answer-set programming;answer-set;combines ideas from;problem domain
eliminee subsets of the players' strategies;eliminable relative to given dominator;description logics;eliminability criterion for bimatrix games that
knowledge base

multiple tasks;blocks world;target values;representation language;feature values;individual preferences;data set;application domains;greedy algorithm;high-quality;user's preferences;objective function
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
close connection;nonmonotonic reasoning
eliminee subsets of the players' strategies;eliminable relative to given dominator;knowledge bases;eliminability criterion for bimatrix games that
eliminee subsets of the players' strategies;eliminable relative to given dominator;description logics;eliminability criterion for bimatrix games that;knowledge integration
complete set of;swartz, 2003;newell, 1982;general-purpose;problem solving;knowledge base
description logics;transitive closure;decision procedure for;ontology language;description logics;semantic web;operator
handle complex;process models;modeling language;inference algorithm;dynamic bayesian network
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
interaction networks;action language;additional information
logic programming;desirable properties;logic program;answer sets;extended logic programs
logic programming;action sequences;answer sets;high level language
answer-set programming;'orgchart';uniform equivalence
logic programming;uniform equivalence
computational properties;logic program;unified framework
training data;test data;medical diagnosis;process control;supervised learning algorithms;machine learning techniques;supervised learning
binary classification;classification problem;multiclass classification;multi-label classification;binary classifiers;multiclass problems
positive results;multi-agent;learning algorithms;incomplete information;solution concept;efficient learning;efficient learning
eliminee subsets of the players' strategies;domain-specific;eliminability criterion for bimatrix games that;eliminable relative to given dominator
eliminee subsets of the players' strategies;bayesian network;learning algorithms;eliminability criterion for bimatrix games that;eliminable relative to given dominator
instances;labeling effort;active learning;active learning framework;labeled training data;learning paradigm;instance;total number of;prediction tasks;machine learning algorithms;information extraction system
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
incremental algorithms;baum-welch;hidden markov models;long sequences
hybrid approach;generative/discriminative;classification performances;hybrid model;combination weights;generative models;unlabeled samples;classifier;training data;naive bayes models;labeled samples;maximum entropy;model trained;text data sets;text classification problems;semi-supervised;machine learning;semi-supervised
conditional likelihood;parameter estimation;classification accuracy;class label;classification tasks;graphical structure;model selection criterion;cross-validation;instance;classification error;real-world;performs poorly;bayesian belief;objective function;model selection;training sample
bayesian network;training examples;probabilistic models;machine learning;knowledge acquisition;high order
eliminee subsets of the players' strategies;eliminable relative to given dominator;eliminability criterion for bimatrix games that;short-term
learning environment;learning algorithm;pruning technique;training examples;policy learning
base line;post-processing;naïve bayes;rule sets;inductive logic programming;naïve bayes;learning scheme
regulatory networks;clustering;probabilistic framework;graph structure;microarray data;learning approaches;map estimation
conditional entropy;low cost;mixture model;global convergence;theoretical analysis;missing data;mutual information between;em algorithm;incomplete data
feature space;data sets;unlabeled data;conditional random field;classification;input features;semi-supervised learning;manifold-learning;linear-chain;topic models;labeled training data;error reduction;semi-supervised;semi-supervised;word clusters;low-dimensional;word segmentation
kernel regression;knowledge-based;reinforcement learning problems;knowledge-based;kernel regression;reinforcement learning;support-vector regression;learning task
statistical approaches;bayesian network;bayesian network structure;continuous domains;continuous variables;real-world;conditional independence;learning algorithm;structure learning;probability distribution;graphical models;bayesian networks;desirable properties;compares favorably with;conditional probabilities
domain knowledge;query relaxation;causal structure;training dataset;databases
redescription mining;redescription mining;conceptual clustering;machine learning;intrinsic structure;data mining problem
clustering;spectral clustering;spectral clustering algorithms;classification;sequence data;hierarchical algorithm;data base;biological sequences
feature extraction;linear discriminant analysis;efficiently extract;class labels;mutual information between;dimensional data;classification error
eliminee subsets of the players' strategies;maximum likelihood;eliminability criterion for bimatrix games that;eliminable relative to given dominator
training data;takes into account;real-world;generation process;real-world domains;entity matching;large data sets;generative model;constraint-based;entity matching;matching accuracy
discriminative training;eliminee subsets of the players' strategies;markov logic networks;eliminability criterion for bimatrix games that;eliminable relative to given dominator
eliminee subsets of the players' strategies;conditional independence;decision trees;eliminability criterion for bimatrix games that;eliminable relative to given dominator
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
application domain;computing environment;reinforcement learning;computing systems;data center;expected utility;resource allocation
time-series;process models
eliminee subsets of the players' strategies;eliminable relative to given dominator;eliminability criterion for bimatrix games that;active learning
eliminee subsets of the players' strategies;support vector machines;eliminability criterion for bimatrix games that;eliminable relative to given dominator;semi-supervised;multi-class
learning algorithm;blocks world
bayesian model;bayesian network;mutual information;classification accuracy;attribute dependencies;conditional independence;uci data sets;naive bayes;structure learning
finite sample;finite sample;density estimation;nearest neighbor;real data sets;learning theory;theoretical analysis;classification problems;nearest neighbors
clustering;share information;perceptual information
graphical models;graphical-model;machine-learning techniques;learning algorithms;lessons learned;computational model
data-driven;eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;dynamic systems;eliminable relative to given dominator
multi-level;object classes;object instances;classification;instance;data describing;object parts;classification process;database;high accuracy;fitness
eliminee subsets of the players' strategies;eliminable relative to given dominator;eliminability criterion for bimatrix games that;object segmentation;motion segmentation
classification problem;multiple labels;scene-understanding;instance;concept learning;scene understanding;scene understanding;learning task;visual features
particle filtering;single agent;principled framework;space complexity;multi agent
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
stochastic processes;real-world domains;censored data;modeling language;bayesian networks;expectation-maximization
traditional models;partially observable markov decision processes;state representation;dynamical systems
utility function;planning problem;decision-theoretic;decision problem;decision makers;blocks-world;utility functions;expected utility
state space;function approximation;parameter estimation;basis functions;task-specific;graph laplacian;operator;spectral analysis;laplace-beltrami
operator;decision problem;error bounds for;supervised learning
markov chains;monte carlo;policy gradient;optimization method;policy evaluation;markov decision processes;operator;transition probabilities;variance reduction;markov chains
eliminee subsets of the players' strategies;eliminability criterion for bimatrix games that;eliminable relative to given dominator
action selection;reinforcement learning;temporal-difference;knowledge transfer;theoretical analysis;source tasks;action sets
phase transitions;symbolic representation;partially observable;markov decision processes;real world;markov chains
training data;training examples;fine-grained;high-accuracy;data acquisition;manually annotated;word sense disambiguation
large number of;knowledge representation;text collection;fundamental problem;natural language;natural language understanding
classification approach;generative probabilistic;text categorization;naïve bayes;unsupervised fashion;compares favorably with
clustering;clustering;classification technique;transliteration;classification framework;classification accuracy;language-independent;bayesian classifier;natural language processing;error rate;question answering
natural-language;database;representation language;query language;formal language;transformation rules
graph structure;svm based;semantic graphs;classifier performance;semantic graph;svm classifier;document summarization;document content;support vector machines;linguistic analysis;learning method
language-independent;training data is;word sense disambiguation
knowledge sources;machine learning
high-precision;semantic analysis;cross-lingual;parallel corpora;fully automatic
training data;unlabeled data;semi-supervised learning;word sense disambiguation;sufficient training data;supervised learning;word sense disambiguation
logical reasoning;low cost;semantic features;semantic representation;inference algorithm;statistical machine learning;minimum "cost
classification;information extraction systems;information extraction;data set;information extraction;classifier
dynamic bayesian network;classification methods;labeled data;dynamic bayesian network;dependency structure;finite state
dynamic programming;basic operations;worst case;decision problem
prohibitively expensive;linguistic information;context-free;computational cost;linguistic features
classical planning;planning graphs;planning graphs;shortest path;planning graph;belief-space
description language
approximation algorithms;real-world;regression analysis;time series;state variables
planning domain;planning domains;plans;process models;implementation issues;domain models
planning domain;local search;search space
automatically selecting;pattern database;lower bounds;domain-independent planning;planning graph
planning algorithms;control mechanisms;state spaces;probabilistic planning
markov decision processes;state spaces;continuous-state;planning problem;real-world applications
planning problems;data structure;search space;temporal planning;planning graph;heuristic search
disjunctive temporal;finite-domain;instances;domain constraints;temporal constraints;spatial constraints;special case;disjunctive temporal

hybrid approach;search procedure;linear programming;high quality;optimization problem;partial order;higher quality;resource constraints
classical planning;planning problems;stream processing systems;language constructs;grid computing;web services;stream processing;programming models
domain description;high degree of;action language;large number of;state constraints;benchmark domains
rule-based;learning algorithm;planning domains
planning domains;plans;real-world;simple temporal;temporal constraints;completeness;temporal reasoning;constraint propagation
search space;mutual information;computationally intractable;information-theoretic;information fusion;information gathering;cost-effective;graph-theoretic;information gain
landmark-based;wide range;planning domains;heuristic search;parameter values
decision-making;plans
sensor networks

markov random field;black box;sensor networks;large-scale
state space;numerical simulation;learning algorithm;policy gradient;online learning;gradient method;real world;learning framework
moving object;sensory information;supervised learning;motion model;multi-sensor
desired behavior;probability estimates;statistical models;great potential
large scale;multi-scale;trade-offs
planning problems;greedy heuristics;integer linear programming;task allocation;declarative framework;temporal constraints;spatial constraints;error bounds;formal description
great promise
classification;human-robot;features extracted from;semantic categories;hidden markov model;range data;path-planning;semantic information;supervised learning;classifier
supervised learning techniques;margin;classifier
training phase;high degree of;labeled training data;segmentation accuracy;computationally expensive;memory resources;color segmentation
real world;dynamic environments;particle filter
accurate models;data points;mobile robotics;highly accurate;range data;dimensional data;global constraints
simulation results;information spaces;unknown environment;belief) space;state estimation;theoretical analysis;complex tasks;information space;moving targets
planning problems;heuristic functions;search algorithms;instances;worst case;depth-first search
heuristic function;search algorithm;algorithm converges;state abstraction;times faster than
search algorithms;rates;optimal strategy;search problems
search algorithms;heuristic search;optimal path;optimal-path;shortest path;search algorithm;heuristic search;weighted graphs
davis putnam;propositional satisfiability;problem hardness;problem hardness
knowledge engineering;ai techniques;search engine;large-scale
parallel processing;search algorithms;breadth-first search;large-scale
parameter settings;parameter selection;problem domains;domain-dependent
learning process;dynamic nature of;search algorithms;plans;heuristic function
pattern databases;state space;duplicate detection;external-memory;external-memory graph search;search problem;heuristic search;pattern database;heuristic function;data-base;external-memory;evaluation function;external memory
large-scale;query language;dynamically changing;large number of;web service composition;index structure;randomly generated;internal structure
web-based;knowledge base;artificial intelligence
core components;index terms;document-term;retrieval performance;weighting scheme;statistical inference;computational approach;information retrieval;vector space model
graph partitioning;connected component;weighted graph;query translation;translation model;query translation;cross-language information retrieval;query words;translation probabilities
world wide web;world knowledge;common sense knowledge;common sense;knowledge acquisition;long-term;machine learning;knowledge base
single document summarization;document understanding;independent variables;document understanding;automatic text summarization;lessons learned;multi-document summarization;dependent variable;automatic text summarization;test sets
real-life;semantic relationships between;theoretical foundation
ad-hoc;software agents;user interface;cognitive load
web documents;relevant documents;classification accuracy;document retrieval;web documents;genre classification;web searches;search engines;rates;high accuracy;search engine;genre classification;readability
distributed data sources;svm) classifiers;vector machine;counterpart;support vector machines;sufficient statistics;distributed data
assists users;6
database technology;large-scale;reasoning tasks;general-purpose;knowledge integration;logic-based;description logic;knowledge sources;knowledge base
question answering
pair-wise;bayesian network;theoretical framework;semantically similar;inference methods;bayesian networks
recurrent neural network;recurrent neural network
kullback-leibler;bayesian networks;spanning tree;approximate inference;approximate inference;times faster than
learning problems
fine-grained;design decisions;cognitive load
supply chain management
evolutionary process;gene expression;search process;problem solving;complex data;gene expression;mining tasks;higher level
decision-making;probabilistic planning
specific information;language models;probability distributions over;input text;natural language generation;generation process
search methods;supervised learning methods;automatically learns;dynamic programming;neural networks;real-world;design choices;reinforcement learning;reinforcement learning problems;temporal difference;evolutionary computation;function approximation;policy search;long-term;neural network

texas hold'em;ai research
action selection;human beings;selection process
language independent;graph-based;ranking algorithms;training data

similarity measure;natural language;word sense disambiguation
clustering;clustering algorithms;natural language text;large corpora;diverse range of;document clustering;feature selection;language independent;natural language;related words
web based;string matching;database
web-based;gene ontology;high-throughput;high accuracy;machine learning
software architecture;spoken language
error-prone;operator;higher-level;artificial intelligence;human operators
agent architecture;intelligent agents

microarray datasets;high-quality;gene expression data
knowledge workers;state university;user interface;knowledge workers;machine learning
object recognition
cognitive science;robotic systems
high-volume



human-robot
computationally hard;preference elicitation;voting rule;computational complexity;preference aggregation
social choice;incomplete information;model checking;temporal logics;mechanism design
incomplete information;cost functions;resource selection;upper bound
action-based;multi-agent;taking into account
approximation algorithms;optimal algorithms;private data
large volume;selection criteria
multi-agent systems;network topology
linear program;dimensional space;similarity metric;convex optimization;indexing scheme;texas hold'em;statistical significance;game theory
approximately optimal;dynamic programming;real-world applications;online auctions;online auction;worst case;mechanism design
formal representation;logic-based;common knowledge;common assumption
automated design;social welfare;trade-offs;worst-case;general-purpose;instances;objective function;mechanism design
np-complete;bounded treewidth;dynamic-programming approach;game-theoretic
convex optimization problems;efficient computation;optimization problem;markov decision processes;multi-stage;action sets
multi-agent system;private information;stochastic optimization;resource allocation;decision making;mechanism design
multi-agent;solution concept;parameter values;reasoning patterns
automated design;training set;learning algorithm;voting rules
formal framework for

multi-dimensional;probabilistic model;single-dimensional;trust model;covariance matrix;kalman filter

agent model;intelligent agents;decision making;specification language
sealed-bid
counterpart;game-theoretic;logical structure
graph theory;graph model;constraint language;instances;graph models;csp instances
high-order;high order;inference rules
temporal dependencies;large-scale;optimization problems;optimization algorithms;instances;produce high-quality
variable ordering;search space;poor performance;search strategy;search space;optimization problem;cost function;search algorithm;objective function
clause learning;real-world;search space;cost effective;search tree
data structures;constraint programming;combinatorial problems;data structures
satisfiability problem;sampling framework;search space;lower bound
high precision;boolean satisfiability;filtering algorithm;constraint based;constraint satisfaction problems;integer programming
decision diagram;efficient computation;instances;data structure;data structures
data structure;regular-expression
probabilistic framework;efficient search;expectation maximization;belief propagation;expectation maximization;constraint satisfaction problems
worst-case;incremental version;algorithm's performance;instance;cost-based;filtering algorithm
constraint networks;worst-case;instances;csp instances;path consistency;space complexity
constraint satisfaction;constraint satisfaction problem
multi-objective;multi-objective;instances;combinatorial optimization;instance;multi-objective optimization
problem instances;classification methods;classifier
expected number of;optimization problems;logic-based;constraint programming;control problem
instances;simulated annealing;population-based;parallel implementation;population-based
local search algorithms;local search;search algorithm for;constraint language;constraint programming;constraint-based;high-level;resource allocation
human knowledge;knowledge bases;text understanding;knowledge representation and reasoning;lessons learned;knowledge representation;natural language texts;natural language processing;artificial intelligence;natural language

solution space;space requirements;dynamic systems
description logic;instance;instances;efficient querying;query processing;semantic retrieval;worst case
satisfiability problem;discrete-event systems
semantic interoperability;ontology mapping;ontology mapping
ranking algorithm;search results;web pages;valuable information;ranking results;ranking schemes;search engines;search engine;term weighting scheme;keyword based;vector space model
constrained problems;representative set;user's preferences;decision process;decision making
data model
compact representation;model-based diagnosis;space requirements;discrete-event systems;binary decision diagrams
web pages;clickthrough data;query suggestions;data sets;search engines;search engine
feature set;highly correlated;posterior probabilities;feature subset selection;posterior probability;monitoring systems;posterior probability
biological entities;bayesian network learning;expression data;causal models;knowledge discovery;high-throughput;data mining;automatically identifies
computational complexity;ontology-based;specifically designed to;upper bound;application domains;data complexity;semantic web;data management;large amounts of data;conjunctive queries;data integration
operator
propagation algorithm;counterpart;operator
recognition tasks
information flow;belief states;multi-context systems
complex queries;taking into account;regular expressions;regular path queries;upper bound;query language;description logics;path queries;description logics;instance;conjunctive queries;query answering;tree-automata;knowledge base
databases;database;approximate query answering;instance;databases;query answering;efficient approximate;closed-world;standard database
computational properties;description logics;description logics;instance level;instances;description logic;algorithms for computing;knowledge base
classical logic;domain description;action language;domain descriptions;logic programming;reasoning problems;query answering
large graphs;relational structures
single-agent;situation calculus;incomplete knowledge;plans;game theory
belief change;belief change;operator
equivalence relations;inductive logic programming;multi-agent systems;domain theory;answer set programming
single agent;multi-agent;semantic model;analysis reveals;computational model;logical properties
everyday life;cognitive architecture;modeling assumptions;transfer learning;knowledge learned;common sense;problem solving;transfer learning;problem-solver
worst-case;combined complexity;description logics
computational complexity;answer-set;auxiliary;uniform equivalence;fine-grained;answer-set programming;answer sets
approximation techniques;large scale;learning problem;observed data;everyday life;common sense;learned model;sensor data;databases;context-based;semi-supervised;model called
operator;counterpart;logical properties;knowledge bases;computational complexity
private information;description logics;description logics;incomplete information;general setting;description logic
stable models;logic program;formal semantics
artificial intelligence
distinguishing features;computational model;semantic model
knowledge compilation;instances;knowledge compilation;worst case;tree width
uniform equivalence
hidden information;particle filtering;texas hold'em;particle filtering;estimation techniques;artificial intelligence
redundant features;mathematical programming;optimization algorithm;medical images
data processing;linear subspace;high dimensional;linear dimensionality reduction;data manifold;learning algorithms;locally linear embedding;information processing;geometrical structure;training samples;real life data sets;euclidean space;low dimensional;principal component analysis;complex data;dimensionality reduction
training set;classification model;loss function;active learning;combination weights;learning algorithm;unlabeled examples;algorithm selection;performance prediction;efficiently identify;worst case;maximum entropy;model selection
kullback-leibler;prediction model;naive bayes classifiers;learning algorithm;semi-supervised learning algorithms;machine learning;data set;labeled data;naive bayes classifiers;training and test data;text classification;em algorithm;basic assumption;test sets;test data
context-dependent;social network;social relationship
reinforcement learning algorithm
function approximation;basis functions;random walk;metropolis-hastings;markov decision processes;theoretical analysis;operator
convex optimization problems;regularization;numerical experiments;logistic regression;regression problems;large-scale;specifically designed for;data set;gradient method;feature selection;logistic regression;classification problems
upper bounds;learning problems;modeling framework;reinforcement learning;action models
graph partitioning;graph partitioning;exponential family
domain knowledge;accurate models;classification;real-world;linear programming;knowledge-based;knowledge-based;data set;regression methods;support vector;support-vector;artificial data sets;prior knowledge
learning algorithm;classification;semi-supervised setting;network (nodes;networked data;information sources;classifier;instances;predictive performance;semi-supervised learning;measure called;multiple types of;relational learning;multiple types of;benchmark data sets;relational data;similarity measures
base classifiers;intermediate data;instances;collective classification;data sets;collective classification;algorithm exploits;collective inference;relational data;classification algorithms
approximation algorithms;path planning;dynamic programming;gather information;wireless sensor network;spatio-temporal;black box;step procedure;optimization problem;query plan;plans;answer quality;issue queries;real world;data collection;theoretical guarantees;np-hard
markov logic networks;training data;target domain;transfer learning;real-world domains;source domain to;transfer learning;relational domains
density estimation;time series;intelligent systems;real-valued;data sets;high density;automatically discover
accurate predictions;transition matrix;markov chain;monte carlo;markov chain
vector space;support vector regression;prediction performance;error rates;numerical features;high dimensionality;string kernel;biological data
clustering;outbreak detection;false positive;unsupervised learning;data set;detection task
complete model;instance;exact answers;pixel level
learning process;dynamic bayesian networks;learning algorithm;structure learning;reinforcement learning;conditional probability
energy function;collaborative filtering;energy functions;semi-supervised learning;class labels;unlabeled examples;major limitation;semi-supervised learning;graph-based;label propagation;pairwise similarity
clustering;regularization;global information;data mining and machine learning;clustering method;optimization problem;global consistency;clustering methods;cost function
community structures;gaussian mixture model;latent dirichlet allocation;community discovery;large-scale;lda model;complex networks;hierarchical bayesian model;real-world networks;social networks;diverse domains;latent variables
multi-label learning;classification;instances;multi-label learning;learning algorithms;input space;instance;scene classification;concept classes;class labels;web page;learning framework;multi-label
labeled training;semi-supervised learning;component analysis;real-world applications;unlabeled examples;labeled training examples;semi-supervised learning;labeled examples;content-based image retrieval
linear program;regularization;kernel regression;unlabeled data;target values;takes into account;optimization problem;regression algorithm;sentiment analysis;risk minimization;semi-supervised learning;benchmark datasets
problem-solving;dynamic programming;general-purpose;decision problems
von neumann;multi agent systems;game theory;np-hard
multi-agent;computational complexity;decision problem;resource allocation;fully connected
fitness
state space;state spaces;large state spaces;state estimation;particle filter;state estimation
approximation algorithms;computational complexity;computational complexity;np-hard
np-hard
cognitive model
multi-agent systems;share information;information sharing
social welfare;global solution;constraint satisfaction;combinatorial optimization problem;computation costs;assignment problem;assignment problem;theoretical guarantee
multi-agent systems;decision problems;error bounds for
imitation learning;imitation learning;active learning;learning tasks;learning agent
constraint reasoning;total number of;spanning trees;depth first search;distributed algorithms
statistical methods;supply chain management;real-world;simulation experiments;incomplete information;high complexity;agent performance;autonomous agents
structural properties;voting rule;voting rules
clustering;representative set;decision-making;approximation technique;candidate models
algorithms for computing;algorithm to compute
execution strategies;plans;programming language
constrained problems;human-generated;intelligent behavior;agent behaviors;objective functions;autonomous agents;artificial neural networks;artificial intelligence
face recognition;image sets;generalization performance;training set;face space;learning framework;recognition rates
human behavior;predictive power;game theoretic
learning algorithm;optimal strategy;separability;databases
neural networks;learning algorithms;cognitive model;temporal logic;agent technology;computational models;artificial intelligence
domain knowledge;user interactions;natural language interface;knowledge learned;domain-specific;knowledge extraction;benchmark datasets
music information retrieval;hybrid approach;human subjects;neural networks;genetic-programming;fitness;artificial neural networks;corpus-based
domain knowledge;hand drawn;sketch-based;image-based;feature-based;spatial context;recognition accuracy;hand-drawn;allowing users to
markov decision processes;desired behavior;reward functions;reward function;target distribution
evaluation functions;large database;evaluation function
human behavior;human subjects;human learning;semi-supervised learning;machine learning;unlabeled examples;decision boundaries;gaussian mixture model;labeled examples;semi-supervised classification
automatic methods;relation extraction;natural language understanding
hidden variable;visual cues
sensory data;existing knowledge;learning process
semantic networks;natural language text;semantic network;large scale;semantic knowledge;knowledge acquisition;spreading activation;million nodes;automatically generate;semantic information;million edges
training data;learning systems;supervision;natural language
supervised methods;word sense disambiguation
learning mechanism;document retrieval;meta-learning;information-gathering;corpus-based
semi-structured;database records;joint inference;information extraction;joint inference;markov logic;inference process
content analysis;modeling task;unstructured text;content analysis;great potential;case study;obtained by combining;natural language processing;accurate predictions
discovered patterns;classification models;application area;rule-based;tree patterns;sequential patterns;classification model;provide feedback;mining sequential patterns;classification results
single document summarization;single document summarization;information contained in;document expansion;document set
information extraction;automatically extracted from;knowledge bases;kernel-based;question answering;kernel method;data set;machine learning techniques;test data;inference rules;dependency trees;classifier;structural features
utility function;plans;decision-theoretic;plan recognition;general case;recognition process;recognition tasks;expected utility;plan recognition;plan recognition
planning domain;plan execution;sufficient conditions for;decision problem;np complete;scheduling algorithm;definition language;concurrent execution;resource allocation;moving targets
planning problems;planning domain;direct comparison;situation calculus;situation-calculus;definition language;computationally expensive
description language;existing protocols;knowledge representation
common patterns;learned model;action models;plans
situation calculus;knowledge base
space partitioning;equivalence classes;action space;decision-makers
classical planning;planning problems;total number of;planning domains;plans
search space;plans;constraint satisfaction;action sequences;planning graph;search strategies;action sets
robot control;sampling-based;real-world;sampling techniques;expected error;sequential monte carlo;autonomous agents;natural language processing;action sequences;partial observations;filtering algorithm
planning problems;instances;search space;domain-independent;optimal planning;depends crucially on;pattern database;domain-independent planning;planning problem;pattern database;domain-independent;problem domain;heuristic search;search problems
ai planning;web service composition;instances;web service composition;special case;effective tools
search methods;scheduling problems;search procedure;real-world;problem instances;performance tradeoffs;scheduling problem;increasing attention
description language;instance
computational complexity;partially observable;instances;simple temporal;partial observability;constraint-based;temporal reasoning;filtering algorithm;inference rules
clustering;mental models;decision-making;incomplete information;wide range;high accuracy;high cost;user modeling;plan recognition
logic-based;optimal number of;planning problem;plans
situation calculus;high level;low-level;plan generation
database;np-complete;preference elicitation;markov decision processes;partially observable markov decision processes;decision processes
computational complexity
regression algorithm;np-complete;decision procedures;common knowledge;multiple agents
camera-views;vision-based;finite-state-machine;main components
sensory data;heuristic function;occam's razor;search process;additional information
monte carlo;extended kalman filter;estimation method;inference procedure;gaussian noise;highly accurate;sensor network;markov chain;inference problem
learning algorithm
training phase;wireless networks;manifold-learning;mobile devices;counterpart;stream data;high accuracy;dynamic environment;labeled and unlabeled data;dimension reduction;semi-supervised
regularization;multi-view learning;activity recognition;machine learning;physical location;mapping function;location-estimation
bounded memory
hand-held;human perception;document images;image analysis;digital cameras;surface shape
deformable objects;scale invariant feature transform;multiple objects;video data;voting scheme;instances;partial occlusion
description language;evaluation functions
upper bounds;heuristic functions;lower bounds;search algorithm;stochastic model
search space;solution path;constraint satisfaction;probabilistic reasoning;memory requirement;heuristic search
local search algorithms;algorithm parameters;local search;globally optimal;tree search;neighborhood structure;parameter settings;decision problems;automatically determined;tedious task
continuous domains;optimal plan;minimum-cost;lower bound;heuristic function;desired accuracy;search algorithm;continuous state;search problems
brute-force search;pattern database;pattern databases;quantitative analysis;branching factor
search algorithms;search tree;real-world;graphical model;graphical models;search spaces;control strategy
shortest paths;path planning
coalition structure;dynamic programming algorithm;autonomous agents;coalition structure generation
wide range;information processing;automatically constructing;evaluation function
complexity bounds;training data;black-box;problem instance;instances;instance;algorithms for computing;average-case;combining multiple
problem instances;training data;online algorithms for;instances;approximately) optimal;run length;instance;algorithms for computing
search effort;heuristic search
domain-independent;duplicate detection;search algorithm for;external-memory graph search
decision theoretic;computationally intractable;classification;real world datasets;feature subsets;data structure called
search methods;attribute values;decision support;general case;decision making;np-hard
planning problems;plan execution;decision-theoretic;shortest-path;partially observable;action-based
point-based;belief states;point-based;approximate algorithm
markov decision processes;real-world;planning domains;large state spaces
clustering;expected number of;bayesian network;bayesian networks;inference algorithms
dynamic programming;sampling algorithms
probability distributions over;multiple objects;generative models;probabilistic reasoning
regularization;graphical models;markov blanket;variable selection;graphical model;regularization parameter;search algorithm
causal effects;identification problem;linear models
clustering;approximation techniques;point-based;finding an optimal;partial knowledge;clustering method;belief states;soft clustering;belief space;partially observable markov decision processes
probability distribution;importance sampling;bayesian networks;sampling algorithm;probability distributions
context-specific;ontology language
search results;probabilistic models;user click;search advertising;rates;search engine;contextual factors
large corpora;evaluation measures;learning algorithm;relation extraction
security requirements;knowledge acquisition;web users
business processes;web services
text summarization;similarity measure;topic segmentation;passage retrieval;context-based;segmentation algorithms
classification;accuracies;naive bayes;classifiers trained on;data sets;web logs;topic-specific;sentiment classification;web log
large number of;video stream
information diffusion;log data;social networks;social relations
semantic annotation;multi-player;design principles;human computation;search engines;game-theoretic;manual labeling;multimedia content;image annotation

greedy strategy;expected number of;information diffusion;graph theory;large-scale;influential nodes;real-world;computational cost;social network;optimization problem;greedy algorithm;combinatorial optimization problem
information sources;inference algorithm;high confidence;social networks;computing systems;decision making;sampling technique
web service;service descriptions;web service composition;high efficiency;semantic web;web service composition;web services
planning algorithm;planning problem;web service;rdf graphs;web service composition;semantic model;graph patterns;web services
classifier;social network;robust estimation;semantic web;search engine;web page;robust estimation;integrates multiple
statistical methods;filtering systems;low precision;social systems;collaborative filtering;detection algorithms;large number of;training data;user profiles;high accuracy;attack detection
logical reasoning;semantic relations between;automatically discovering;semantic web;human intervention;matching systems;semantic heterogeneity;ontology alignment
data source;classification;features extracted from;heuristic rules;intelligent systems;wikipedia articles;annotated data;named entity;relation extraction;relation extraction;key features;syntactic structure
real-world datasets;semantic structure;web page;multiple communities;multiple topics
markov chain;hyperlink structure;semantically related
query answering over;response times;completeness;knowledge compilation;semantic web applications;data complexity;query answering
large scale;manually annotated;semantic similarity between;semantic relations between
web-based;large-scale;large scale;query language;semantic web;public-domain;semantic web
software agents;service-oriented;autonomous agents;wide range;np-hard
logical reasoning;description logics;ontology language;information integration;structured objects;service discovery;formal semantics
rdf data;world wide web;large volumes of;graph-based;database;answering queries;resource description framework;graph based
case-base;competence
web based;theoretical foundation;web environment;formal model;temporal reasoning;event calculus
data structure;web pages;graph structure;ranking process;online databases;efficient querying;search engines;dominant relationship;structured data;relational data
similarity measure;query suggestion;tf×idf;machine learning;similarity function;similarity measures
electronic commerce;human users;web-based
clustering;text based;text streams;information embedded in;event detection;graph cuts;detection approach;information flow;social networks;real world;web forums
machine learning;web pages;feature set;visual features;information sources
collaborative learning;learning agent;knowledge representation and reasoning;machine learning;great promise;ai technologies;agent-based;natural language understanding
cognitive architecture;computational methods;human-robot;processing algorithms;intelligent systems;information retrieval;applications involve;data structures and algorithms;inference algorithms;natural language understanding
clustering
real-world;human-centered;key features;knowledge intensive
ai research;large-scale;knowledge representation and reasoning;natural language processing;natural language understanding;knowledge base;natural language
reasoning capabilities;information processing
spatial information;moving target;higher-level;spatial reasoning;human-robot
design space;case-based reasoning;cognitive architecture
case-based reasoning;intelligent agent;intelligent tutoring systems
cognitive architecture;intelligent behavior;spatial information;reasoning problems;spatial reasoning;spatial reasoning
based reasoning;extraction process;user interface;automatically generated;supervised learning;ai techniques;semi-automatic
ontology-based;artificial intelligence;natural language
natural language text;great potential;machine learning;natural language processing;user studies;integrates multiple
computing power;learning problem;semi-supervised;object-recognition;large-scale
domain theory;domain model;completeness;plans;domain models
attribute-based;real-world entities;database;real-world;adaptive approach;answering queries;query processing;databases;data integration;processing queries
training samples;learning algorithm;learning paradigm
user interfaces;high-level;user modeling
knowledge engineering;case-based reasoning;classification systems;case base;decision boundaries;error reduction;competence
computationally hard;answer set programming;constraint logic programming;answer set programming;constraint logic programming;applications involving
functional dependencies;bounded treewidth;logic-based;database
local features;kernel-based;approximate matching;learning tasks;text processing;computationally expensive;feature vectors;efficient learning
excellent performance;kernel approach;density estimation;reproducing kernel hilbert space;databases
simpler models;exact inference;optimization framework;search problems
high-dimensional;feature space;high-dimensional feature space;learning algorithm;high-dimensional feature spaces;manifold structure;semi-supervised;diffusion process;low-dimensional
activity recognition;np-hard;sensor networks;water distribution;minimum cost
agent systems;information space;artificial intelligence;basic properties
automatically extracting;normal distributions;reasoning problems;temporal reasoning
accuracies;test set
clause learning;instance;algorithm called;local search;bounded memory
action selection;case-based reasoning;case-based reasoning
search methods;reinforcement learning;genetic algorithms;reinforcement learning;complex tasks;temporal difference;temporal difference;learning method
protocol called;instance;high-level
clustering;data objects;generative model for;data clustering;relational data;relational data
multi-level;domain-independent;time series;real-valued;change detection;segmentation algorithms
case-based reasoning;similarity measures;case-based reasoning;similarity-based
data set;greedy algorithms
image classification;classification tasks
fusion methods;classification;genetic algorithm;features extracted from;classification performance;voting scheme;kernel functions;combination methods;svm classifiers;single classifier;classification task;classifier;support vectors
dependency relations;natural languages
natural language text;semantic network;semantic knowledge;spreading activation;semantic information;automatically generate
human-robot
additional features;feature representation;named entity;training data;instances
interval based;individual objects;ontological knowledge;semantic web;rough set theory;natural language processing;ontological knowledge;real world;artificial intelligence
machine learning;knowledge base;knowledge-driven
user profile;web search;aggregation techniques;user-centered;ranking list;history data;data set;user interests;rank aggregation;primary goal
ontology-based;communication protocol
neural networks;independent component analysis;machine learning;independent component analysis;generative model;time-series;neural networks
knowledge engineering;cognitive architecture;raw data;classification
sampling method;bayesian network;numerical experiments;continuous variables;inference algorithm;probabilistic reasoning;monte carlo simulation;probability distributions;algorithm called;bayesian networks;approximate inference;inference algorithms;artificial intelligence
ontology-based;ritchie, 2004;binsted, 2006;formal methods;domain size;2004;real world;natural language
partially observable;partially observable markov decision process
clustering;belief state;point-based
distance measure;network models;social networks;match-making
uci datasets;confidence intervals;prior knowledge
missing data;missing values;cost-sensitive;cost-sensitive
sensory information;partially observable markov decision processes;continuous state;continuous state
complete information;completeness
natural language text;semantic knowledge;automatically generating;million nodes;spreading activation;million edges
multi-modal;spatial relationships;reasoning tasks
learning environments;intelligent tutoring systems
decision-making;computational models
service selection;ontology-based;dynamic environments
autonomous agents;predict future;data analysis;information gain
long-term;answer questions;knowledge bases;application called
structured data;semantic web;human knowledge;database
intelligent systems
semantic annotation;game theoretic;multi-player;design principles;face detector;semi-automatic
behavior patterns
speech recognition;dynamically generated;knowledge management;autonomous agents;multi-party;natural language understanding
data providers;data sets;highly distributed;data access;production data;data integration
constraint satisfaction;global constraints
motivating application
classification
long-term;human-robot
learning process;training data;object category;training examples;object categorization;image collection;real-world scenes
traffic control
optimization problems;population-based;optimization algorithms;information flow;agent performance;multiple agents;optimization techniques
single agent;multi-agent;np-complete;expected cost;physical environment;success probability;applications involving;search problems
domain-independent;private information;upper bound;electronic) commerce;learning theory;multi-attribute;arbitrarily large
graph coloring;multi agent;single-agent;multiple agents
optimization method;black box;combinatorial optimization problem;stochastic optimization
solution concept;multiagent systems;naive algorithm;np-hard
approximation algorithms
multi-agent;resource allocation
randomly generated;normal form;particle filter;stochastic games;uniformly distributed
point based
weighted voting;weighted voting;computationally hard
complexity bounds;smoothing method;memory requirements;complexity bound;desired accuracy;interior-point methods
total number of;scale free;social network;sensor networks;network topologies;token based
object-level;everyday life;logic-based
counterpart;solution concept;special case
single-item
relevant attributes
social choice
variable ordering;multi-agent systems;graph structure;constraint networks;resource constraints;objective functions;resource constrained;problem solving;resource constrained;resource constraints;search space
approximately optimal;classification framework;data points;classification;machine learning
logic programming;belief change;logic programming;minimal change;operator;logic program
hierarchical structures;multi-agent system;multi-agent systems;wide range;hierarchical structure
multiple tasks;control algorithm;multi-agent;action space;multiple models;single-agent;simulation results
preference relations;individual agents;taking into account;artificial intelligence
social welfare;worst-case;coalition structure generation;algorithm called;dynamic programming;dynamic programming
multi-agent system;decision making;mechanism design
theoretical analysis;shortest path
multi agent systems;formal semantics
taking into account;sealed-bid;budget constraints;autonomous agents;multi-unit;real world
voting rule;voting rules
partial orders;np-complete;voting rules;linear order;voting rule;scoring rules
voting rules;multiple attributes;vice versa;decision making
markov decision processes;reward function;np-hard
weighted voting;computationally hard;weighted voting;lower bounds;multiagent systems;decision making
sat instances;space complexity;instances
local search;exhaustive search;randomly generated;np-complete problem;solution space;csp instances;problem size
instance;bounded size;global constraints;constraint programming;constraint propagation
protein structure;instances;fitness function;maximum number of;data structures;protein structure;local search;algorithm relies on;search algorithm;excellent performance
instances;minimum number of;markov random fields;phase transition;random 3-sat
local consistency;instances;cost functions;assignment problem
monte carlo;action selection;game-tree search;heuristic function;control knowledge;human intervention;automatically learning;automatically learn;intelligent agents
phase transitions;constraint satisfaction problem;phase transition;global structure;np complete;random instances;lower bounds
satisfiability problem;uniform distribution;biased samples;metropolis-hastings
game trees;texas hold'em;game tree
clause learning;clause learning;real world;real-world
linear program;dynamic programming;piecewise linear;piecewise linear;partially observable markov decision processes;belief state;objective function
dynamic programming;ad-hoc;resource-constrained;worst-case;shortest path;special case
clause learning
scheduling problem;context-free
breadth-first search
constraint optimization problems;constrained problems;search space;dynamic programming
real-world;online learning;online learning;decision making
bayesian network;random variables;large-scale;model counting;randomly generated;bayesian networks;exact inference;search algorithm
planning problems;planning problem;local minima;local minimum;probabilistic guarantees;state-space;heuristic function;heuristic search
wide range;lower bounds;lower bound;search-tree;instances
heuristic functions;single-agent search;single-agent;search applications;evaluation functions;search effort;problem domain;combining multiple
computational complexity;taking into account;real-world;hidden structure;theoretical results;instances;sat instances;random 3sat
instances;search tree;constraint programming;search heuristics;global constraint
bound consistency;bound-consistency;cardinality constraints;bound consistency;constraint satisfaction problems
conditional distributions;random sample;conditional distribution
image retrieval;constraint networks;spatial objects;information science;formal model;artificial intelligence
constraint optimization problems;local search algorithms;local search;search algorithms;higher level
description logics;semantic web applications
temporal logics;temporal logics;temporal logic
knowledge compilation;normal forms;description logics;description logic;desirable properties;normal form
answer set programming;completeness;query-answering;goal directed;answer set programming
numerical experiments;similarity matrices;manifold learning;linear combination;random walk on;sensory information;transition probabilities;similarity matrix;low-dimensional;random walks
basic assumption
problem instances;specific information;answer set programming;optimization methods;answer set programming;large number of;integer linear programming;automatically generated;sat-based;representation language
knowledge compilation
answer-set programming;answer-set semantics;answer-set;answer sets;problem solving


operator;knowledge acquisition;knowledge base;efficient construction;knowledge bases
answer set;programming language;stable model;answer set programming
logic programming;logic programming;completeness
software development;situation calculus;theoretical framework;model-based diagnosis;theoretical foundation
preference relations;partial orders;real life;query optimization techniques;algorithms for computing
scene analysis
worst-case optimal;description logics;data complexity;description logic;worst-case optimal;conjunctive queries;query answering;satisfiability testing;knowledge bases
moving objects;temporal logics;knowledge base;l,u
boolean functions
operator;logical properties
description logic;ontology language;binary decision diagrams
distributed nature of;computational resources;finite state machine;worst case
state space;approximate algorithm;conditional independence;real-world applications;bayesian networks;confidence level;decision making
knowledge bases;semantic network;common sense knowledge;common sense;knowledge base;dimensionality reduction
model offers
logic programming;model-theoretic;closely related;stable-model semantics
modeling framework;real-world;model generation;complex systems;graph models;generation process;model-based diagnosis;gene expression
sentence level
automatically creating;machine learning
answer set;global constraints
logic programming;probability distribution;logic program;knowledge base
algorithm performs;computational complexity;fisher discriminant analysis;semidefinite programming;distance metric learning;distance metric;closed-form solution;computationally expensive;fisher discriminant analysis;class labels
model-free;transition model;reinforcement learning;reinforcement-learning algorithms
relevant features;regularizer;linear discriminant analysis;basis functions;linear combination;geometrical structure;graph based;principal component analysis;graph embedding;dimensionality reduction;document databases
clustering;distribution information;clustering methods;global information;local density;directed graph;random walk;random walk on;directed graphs;transition matrix;structure information;graph based;clustering algorithm;similarity based;markov random walk;similarity measures
inference engine;learning algorithms;hand-written;integrating multiple;integrating multiple;marginal probability;markov logic;markov logic;integrates multiple
learning algorithm;decision-making;problem domains;machine learning;geometric information;case study;visual recognition
ranking approaches;ranking algorithm;supervised learning techniques;web search;information retrieval applications;query independent;document retrieval;optimization techniques;ranking algorithms;retrieval performance;meta-search;query-dependent;semi-supervised;quadratic program;combining multiple;semi-supervised
multiple instance learning;optimization framework;semi-supervised learning;text categorization;instance-level;real-world applications;instances;multiple instance learning;instance-level;graph-based semi-supervised learning;real-world data sets;machine learning;multiple instance;content-based image retrieval
training data;machine learning;ranking problem;learning problem;classification problems;character recognition
kernel machines;random projection;object recognition;local features;high dimensional;approximation error;feature-set;feature-sets;image database;low dimensional;upper bound
clustering;clustering;statistical properties;graph partitioning;clustering algorithms;multi-type;information retrieval;web mining
clustering;greedy strategy;simulated annealing;local optima;single label;ground truth;multiple clusterings;real-life;objective functions;computational cost;clustering algorithm;computationally feasible;optimization scheme
directly optimize the;global optimal;feature-level;graph-based;feature selection algorithms;prohibitively expensive;brute force;feature subsets;feature selection;feature subset;computational cost
feature space;text classification;dimensionality reduction method;classification;target domain;transfer learning;learning algorithms;regression models;latent space;labeled data;source domain;source domain to;real world applications;transfer learning;low-dimensional;dimensionality reduction
active learning strategies;sample complexity;relation extraction;active learning;active learning;machine learning;adaptive strategy;complex applications
relevant information;data exchange;learning framework;reinforcement learning;requires minimal
bayesian network;learning bayesian networks;markov blanket;classification accuracy;high degree of;information theory;structure learning;high-dimensional data sets;feature selection;promising candidates;support vector machines;search space
search spaces;plans;artificial intelligence
data structure;sufficient statistics
service descriptions;learning algorithm;planning) problems;web-service composition;bounded number of;web services;efficient learning;efficient learning
semi-supervised classification;regularization;specific algorithms;classification;data manifold;semi-supervised learning methods;semi-parametric;semi-supervised;semi-supervised classification
data point;regularization;regularizer;semi-supervised learning;classifier;local classifiers;semi-supervised classification
probabilistic framework;synthetic data;hidden variables;interaction networks;speech data;exponential family;global) features;graph models;network structure;latent structure;real world
sequence labeling;discriminative models;sequence data;probabilistic models;state variables;entity recognition;class labels;compares favorably with;conditional random fields;natural language
regularization;classification;classification;information contained in;benchmark data sets;machine learning;data set;classifier
regularization;local learning;multi-view;multiple sets of;multi-view;real-world applications;multi-view;clustering problems;single-view;semi-supervised;local learning
ensemble learning;base classifiers;data representation;ensemble methods;pairwise constraints;instances;boosting algorithms
multi-agent system;free text;free-text;personal information management;natural language processing
intrusion detection;intrusion detection system;synthetic data;game theoretic;learning systems;classifier
speech recognition;web-based;recognition performance;united states;generic framework;error rate;user study
end-user;end-users;algorithm parameters;learning algorithms;dynamic model;machine learning;case study;hidden markov model;learning parameters;user-oriented
quality metric;desired properties;similar results;reinforcement learning;markov decision processes;quality function;search algorithm
inference method;detection techniques;classification;real-world;feature selection;control flow;conditional random fields;pattern-matching;identification problem
human input;interactive applications;change detection;sequential data;time-series;support vector machines;semi-automatic
micro-array data;algorithm performs;gene ontology;gene expression data;high computational cost;knowledge driven;large number of;structure learning;bayesian inference;matrix factorization;addressing this problem;prior knowledge;gene regulatory network;optimization algorithm
domain knowledge;classical logic;answer set programming;model theoretic;answer set programming;representation language;knowledge representation;knowledge acquisition;natural language
relation extraction;semantic relations between;classification;training examples;large number of;semantic relations;semantic information
data sets;labeled examples;unlabeled data;classification;classification;world knowledge;text categorization;semantic concepts;labeled data;competitive performance;supervised learning algorithm;semantic representation;classifier
spatio-temporal;topic model;topic-specific;visual features;automatically-extracted;low-level
knowledge-transfer;mutual information;classification;world knowledge;performs poorly;customer service;heterogeneous data sources;human performance;text categorization;correct classification;common task;knowledge transfer;operator;multi-task learning;error reduction;transfer learning
target language;bayesian model;performance gains;parallel corpus;text corpora;cross-lingual;knowledge transfer;cross-lingual;linguistic knowledge
ranking algorithm;nearest neighbor;global information;graph-based;information contained in;local information;document set
vector based;wikipedia articles;semantic relatedness
contract algorithms;general setting
planning problems;plans;state action;cost model;model checking;algorithms for computing;finite state
planning algorithms;space overhead;search space;optimal path;search history
classical planning;rich information;heuristic search
temporal planning;forward search;simple temporal;planning graph;temporal constraints
dynamic programming;external memory;main memory;markov decision processes;dynamic programming;external-memory
domain descriptions;ad hoc;heuristic approach;classification
markov chain;completeness;stochastic search;algorithm called;high-cardinality;algorithms for computing;model-based diagnosis
cardinality estimates;model-based diagnosis;low cost;high-cardinality
relevant entities;computational savings;action sequences;heuristic function
operator;situation calculus;action theories
heuristic functions;domain-independent;planning domains;pattern-database;domain-independent planning;planning algorithms;pattern database;heuristic function;arbitrarily large;benchmark domains;heuristic search
search algorithms;planning domains;heuristic search;theoretical analyses;planning tasks;search algorithm;benchmark domains;heuristic search;search nodes
knowledge engineering;planning problems;planning domain;planning domains;plans;planning problem;theoretical results;decomposition methods;semantic information;hierarchical task;classical planning
heuristic functions;search space;plans;algorithm called;concurrent execution;resource constraints;optimal set of
web-based;large numbers of;human computation;real data;human computation;control problem;web users
shortest paths;approximation algorithms;worst-case;expected cost;directed acyclic graphs;markov decision processes
multi-valued;extraction algorithm;rates;planning tasks
plans;temporal logic
planning problems;problem instances;plans;classification;instances;problem instance
temporal dependencies;plans;multi-agent;rank-order;classification errors;pre-processing;plan recognition;low-level
situation calculus;negative result;model-theoretic;action theories;knowledge base
action selection;planning problems;heuristic search;probabilistic planning;probabilistic planning
prediction accuracy;sequence labeling;high-quality;conditional random field;dna sequences;crf model;graphical model;prediction task;conditional random field
belief propagation;graphical model;inference algorithm;approximation quality;belief propagation
bayesian network;mutual information;large state spaces;belief propagation;bayesian networks;mutual information between;belief propagation;focus primarily on
ranking algorithm;graphical models;preference aggregation;numerical experiments;utility functions;decision problems;utility theory;decision making
partially observable markov decision processes;point-based
state space;forward search;plans;poor performance;markov decision processes;probability distributions;algorithm called;stochastic domains
labeled samples;decision-making;policy evaluation;partially observable markov decision processes;observation model;decision making
large numbers of;random variables;probabilistic models;probabilistic inference;inference algorithm;queries efficiently;inference algorithms
decision-theoretic
real-world;real-world domains;probabilistic relational;inference methods;relational structure;inference algorithms;relational domains
probability distributions;graph structure;induction algorithms
lower bound;partially observable markov decision processes;heuristic search;optimal value function;upper bound
inference methods;inference algorithm;belief propagation
finite sample;algorithm parameters;bayesian network;decision support;bayesian network learning;variable selection;causal discovery;learning algorithms;theoretical results;produce high-quality;learning task;bayesian network learning;theoretical bounds
clustering;markov logic networks;continuous domains;classification;real-world;markov networks;real-world applications;exponential family;bayesian networks;modeling language;inference algorithms for;inference algorithms
sample data;tree model;tree-structured;bayesian networks;computational cost;approximate inference;tree models
target variables;approximate algorithm;bayesian networks;simulated annealing;instance
decision procedure for;sufficient conditions for
wikipedia-based;world knowledge;high quality;relevant documents;labeled training data;concept-based;retrieval tasks;feature generation;information retrieval;trec data;retrieval performance;feature selection;feature selection method;supervised learning;query words;information retrieval systems;high precision;documents retrieved
web-based;network models;store data;online social networks;social networking;semantic web;social network;case study;semantic web;social networks;network structure;social networking
query translation;user clicks;machine translation;web queries;language models;cross-lingual information retrieval;real dataset;search engine;user click;increasing attention;query translation;high precision
similar queries;classification model;similarity measure;search result;location information;log data;query expansion
supervised learning techniques;unlabeled data;classification accuracy;semi-supervised learning;blog classification;labeled dataset;semi-supervised;blog classification;learning method
viral marketing;user interactions;classification;user behavior;social network;collaborative tagging;link-based;social networks;web services;social networking;prediction tasks
extraction algorithm;automatic extraction of;data points
greedy strategy;information diffusion;real networks;minimization problem;influential nodes;dual problem;social network;limited number of
users' preferences;web search;high-quality;large-scale;objective functions;user's location;web search queries;personal data;search efficiency;online services;personal information
access logs;related queries;recommendation process;query recommendation;search engines;hierarchical agglomerative clustering;ranking algorithm;recommendation systems;bipartite graph;query-url;graph clustering;naïve;linkage based;query recommendation;ranking method;unique characteristics;distance) measure;query-url;web search;similarity measure;query recommendation;term based;vector based;keyword based
language-model;relevant document;web navigation;language model;context-aware;segmentation techniques;passage retrieval;search engines;hidden markov model
search results;multi-level;web search engine;search result;search engine;medical knowledge;desired information
global optimal solution;constraint satisfaction problem;constraint satisfaction;heuristic rules;semantic correspondences;neural network;ontology mapping;semantic interoperability;neural network;ontology mapping
logical reasoning;automatically created;matching systems;human experts;semantic correspondences;automatically generated;semantic web technologies
wikipedia categories;classification;category structure;instances;wikipedia categories;knowledge acquisition;semantic relations
web documents;class attribute;web text;information extraction;weakly supervised;instances;search queries;query logs;automatically-extracted
relevance score;linear model;language model
application domain;semantic web technologies;ontology-based
semantic classes;large numbers of;labeled instances;information extraction;instances;high precision
attribute values;web sites;real-world;web sites;graphical model;text fragments;web pages;learning framework
ranking algorithm;similarity analysis;ranking results;document content;pagerank algorithm;user-oriented
data structure;ranking process;dominant relationship;aggregation methods;search engines;efficient querying;dominant relationship;online databases;rank aggregation;structured data;relational data
occurring human teaching behavior;spatially structure the learning environment to;learning systems;robotic systems;machine learning;spatial scaffolding is a naturally;robot learning;highly reliable;human learning
human performance;user-generated;web service;representation language;semantic representations;learning systems;machine learning;web services;hierarchical task;learning task
ai techniques;decision-making;computational model;contextual factors;qualitative reasoning;natural language
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
decision theoretic;plans;domain model;learning scheme;action models;relational learning
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
model-based diagnosis;heuristic search;plans;additional information
domain knowledge;high-level;human players;competence;integrates multiple
plans;computational framework;general-purpose;goal-directed;state estimation;constraint-based;adaptive control
cognitive architecture;past experience
occurring human teaching behavior;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;spatial reasoning;highly reliable
occurring human teaching behavior;human-robot;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable
ordering constraints;gradient-based;parameter learning;monte carlo;noisy data;activity recognition;metropolis-hastings;real-world applications;naive bayes classifier;exponential family;partially ordered;baseline models;real data;location-based;maximum likelihood;markov chain;classifier
cross-validation;importance sampling;function approximation;data samples;sampling techniques;reinforcement learning;prohibitively expensive;model selection;sampling technique
cognitive architecture;human-robot
real-world datasets;probabilistic framework;accuracies;activity recognition;goal recognition;crf model;conditional random fields;sensor networks;high-level;activity recognition;artificial intelligence;recognition problem
occurring human teaching behavior;spatially structure the learning environment to;information-theoretic;robotic systems;spatial scaffolding is a naturally;highly reliable;efficient optimization
planning algorithm;planning problem;optimization approach;dynamic model;performance metric;computation cost;convex optimization problem;dynamic systems;upper bound;optimal control;iterative algorithm
manifold learning;location estimation;data collected;optimization problem;labeled data;model trained;mapping function;low-dimensional;machine learning approaches
graphical models;data mining techniques;large scale;probabilistic models;large number of;structure learning;structure learning;prior model;common sense;statistical model;labeled data;sensor data;machine learning techniques;statistical models
filtering algorithms;problem remains;motion capture;high computational cost;motion tracking;dynamic bayesian networks;high dimensional;state space;state variables;probability distributions;particle filtering;vision problems;filtering algorithm;dynamic systems;dynamic bayesian network;growing number of
object recognition;data collection;probabilistic model;mobile robotics;learning algorithm;training set;emerging applications;object class recognition;training sets;collect data;recognition systems
multi-modal;data points;svm based;graph-based;real-world;voting scheme;data association;shape model;spatial distribution;hand-labeled;laser range;motion patterns;tracking method;vision-based;motion models;classifier;sensor fusion
regularization;activity recognition;forward-selection;feature selection algorithms;conditional random fields;sensory data;feature selection;features selected;feature selection methods;multi-robot;conditional random fields;small size;feature selection
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
feature space;learning algorithm;multi-task;optimization approach;real-world;data distributions;learning tasks;original data;multi-task learning;regression models
utility function;maximum entropy;inverse reinforcement learning;real-world;collected data;probabilistic approach;decision problems;maximum entropy;performance guarantees;imitation learning
instances;multi-agent;markov decision processes;worst-case;high complexity;problem solving;automatically generated;general problem;dimensionality reduction
sat instances;instances;instance;high resolution;csp instances
occurring human teaching behavior;classification;spatially structure the learning environment to;robotic systems;short text;spatial scaffolding is a naturally;highly reliable
scheduling problems;occurring human teaching behavior;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable
occurring human teaching behavior;spatially structure the learning environment to;robotic systems;state space search;spatial scaffolding is a naturally;highly reliable
similarity measure;probability distributions over;accuracies;distinguishing feature;optimal strategy;game theory
auxiliary;accurately identify;classifier
linear classifiers;covariance matrix
constraint reasoning;global solution;distributed systems;real-world
squared error;outbreak detection;linear combination;time series
computational complexity;spectral learning;multi-resolution;wavelet analysis;graph laplacian;operator
cost function;local search;core components;highly structured
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
learning scheme
occurring human teaching behavior;spatially structure the learning environment to;robotic systems;incomplete information;spatial scaffolding is a naturally;relational learning;highly reliable
bayesian model;probabilistic model;matrix factorization;missing data;low dimensional;latent structure
shape information;training set
incremental algorithms;perform inference;completeness;solution-space;boolean functions
feature space;lower-dimensional;dimensionality reduction method;multiple labels;multi-label learning;data mining tasks;machine learning;multi-label learning;original data;closed-form solution;class labels;multi-label;multi-label;dimensionality reduction
online learning;learning algorithms;learning systems;online learning
classical planning;occurring human teaching behavior;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable;control knowledge
probabilistic modeling;unlabeled data;probabilistic model;machine learning;probabilistic models;real world;dependency structure
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
user interfaces;large quantity of;machine learning;machine learning techniques;artificial intelligence;user-oriented
completeness;stable model semantics;rewriting algorithm;query equivalence;databases;query answering;data integration
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
bounded treewidth;occurring human teaching behavior;spatially structure the learning environment to;efficient computation;robotic systems;spatial scaffolding is a naturally;highly reliable
heuristic functions;state space;finding an optimal;state abstraction;state variables;pattern database;heuristic function;model checking;cost function;special case
semantic concepts;activity recognition;higher-level;real world;dynamic bayesian networks
domain-independent planning;wide range
efficient algorithms to;optimal policies;los angeles;security applications
statistical machine learning;statistical machine learning;increasing number of;machine learning algorithms
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
training set;vision systems;ground-truth;image features;large-scale;single images;supervised learning
occurring human teaching behavior;spatially structure the learning environment to;robotic systems;single-document summarization;spatial scaffolding is a naturally;highly reliable
real world domains;traffic flow;limited resources;reinforcement learning;monte carlo
instance;constraint satisfaction problems;np-hard
nonmonotonic reasoning;answer set programming;answer set programming;knowledge representation;knowledge-intensive;search problems
machine learning;vision systems;natural language processing
game theory;multi agent systems;game theory
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
reasoning patterns;decision-making;redundant information;computational savings;reasoning patterns
higher-level
decision-support system;medical applications;decision support;data model;ontology driven;argumentation-based;decision-support
occurring human teaching behavior;information sources;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable
occurring human teaching behavior;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable;web personalization
sketch recognition;manifold learning;feature-based;selected features;sketch recognition;recognition systems;machine learning techniques;learning method
sketch recognition;recognition systems
instance
multiple agents;stochastic games;partially observable

object recognition;training data;robot vision;large quantities of;image databases;image search;image understanding;search engines;image categorization
data structure;transaction logic;trust management;logic programming;semantic web;semantic web services;transaction logic;artificial intelligence
transaction logic;transaction execution;logic programming;transaction logic;knowledge base;artificial intelligence
context information;data structure;naïve bayes classifier;semantically related;database;semantic web;semantic web;search engine;classifier;related words
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems

spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
medical image;classification;random fields;learning algorithms;vector machine;conditional random fields;logistic regression;structured data;distributed) data
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
low-level
natural language texts;machine learning
sketch recognition;occurring human teaching behavior;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable
occurring human teaching behavior;answer set programming;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable
false positives;false positive
optimization process;genetic algorithm;global optimization;large-scale
occurring human teaching behavior;classification;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable

occurring human teaching behavior;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable;transaction logic
algorithm called;plans;plan generation
free text;desired information;extract information from;relevance information;information extraction
decision-support system;group members;decision support;data model;incomplete information;argumentation-based;designed to support;test case;decision-support;decision making
hybrid approach;occurring human teaching behavior;spatially structure the learning environment to;robotic systems;spatial scaffolding is a naturally;highly reliable
high volume;machine learning;relevant information
spatially structure the learning environment to;highly reliable;spatial scaffolding is a naturally;occurring human teaching behavior;robotic systems
fine-grained
agent systems
multi-agent;key features;local constraints;los angeles
multiagent systems;human-robot
semantic network;artificial-intelligence;constraint satisfaction;text descriptions;knowledge-based;spatial knowledge;wide range;spatial reasoning;knowledge base
search applications;instances;agent architecture;domain-specific;case-based reasoning
test set;learning algorithms have domain-specific parameters;learning algorithms;error rate;machine learning algorithms;benchmark data sets
learning examples;learning algorithm;intelligent agents;variable selection;high dimensional data sets;learning algorithms;action sequences;data sets;learning problems;input variables;dimensional data;high dimensional spaces
complex systems;measurement data;search efficiency
selection method;dynamic environments;reinforcement learning;reinforcement learning;computational resources;autonomous agents;learning method
reinforcement learning algorithm;state space;neural networks;continuous variables;finite difference;finite difference;model-free reinforcement learning;reinforcement learning;reinforcement learning algorithm;optimal control
network size;reinforcement learning;case study;rates;dynamic networks;markov chains;data delivery
training set;training examples;classification;feature values;classification performance;concept learning;real world applications
decision trees;predictive accuracy;decision tree;decision tree;learning tasks;local information
learning algorithms;decision tree learning algorithm
ordered list;classifier;competence
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
predictive accuracy;classification;real-world;problem domain;search strategies;computational cost;resource-bounded
type information;negative examples;real data;comprehensibility;programming language;learning performance;readability;positive examples
inductive logic programming;learning framework;complementary information
learning algorithms have domain-specific parameters;machine learning;knowledge acquisition;dynamic systems;machine learning algorithms;benchmark data sets
operator;control strategies;regression tree;rule set;machine learning
baum-welch;hidden markov models;faster convergence;local information;special case;local minima
data point;dimensional space;database;machine learning;instance;databases
evolutionary algorithms;mathematical analysis
additional constraints;decision trees
plans
natural language generation
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
target language;machine translation;weighting function;machine translation;indexing method;word frequency;corpus-based;rates;corpus-based;segmentation algorithm
hybrid approach;learning algorithms have domain-specific parameters;machine translation;rule-based;machine learning algorithms;benchmark data sets;corpus-based
machine translation;linguistic information;spoken-language;machine translation
takes into account;decision process;user-centered
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
compact representation;billot and lang, 1989; lang, 1994;context-free;maxwell and kaplan, 1996;free form

higher-order;higher-order
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
multi-modal;domain knowledge;domain model;probabilistic information;database;integrating information from;database access
machine translation;user-friendly;natural language
regularization;energy minimization;control parameters
building blocks for;generation process
classification;neural networks;learning algorithms have domain-specific parameters;data structures;machine learning algorithms;benchmark data sets
machine learning algorithms;neural network;learning algorithms have domain-specific parameters;benchmark data sets
minimum description length;neural networks
sensor network;event-driven
semantic networks;collins and loftus, 1975;semantically similar;semantic similarity;network model;semantic representation
computational complexity;multi-layer;recurrent neural networks;boolean functions;training methods;neural network;neural networks
associative memory;fundamental properties;numerical simulation;randomly generated;natural language processing;word sense disambiguation
probability distributions over;computationally tractable;spatial location;low dimensional;competence
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
scene analysis;processing algorithms;recognition accuracy;inverse problem
learning algorithms have domain-specific parameters;machine learning algorithms;neural networks;learning method;benchmark data sets
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
local learning;neural networks;genetic algorithm;network topology;hidden layer;simulation results;digital signal;mobile communication;learning speed;neural network;applications requiring;building block
markov decision processes;decision theoretic;machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
planning problems;np-complete;model checking;exact computation;problem-solving;finite state
planning problems;search techniques;propositional satisfiability;plans
autonomous systems;average case
intelligent agent;specifically designed to;goal-oriented
simulation results;computational resources;plan quality;plans
probabilistic representation;progressive processing;high level;meta-level;resource-bounded;progressive processing;data transmission
action sequences;learning algorithm;goal recognition;vast number of
machine learning algorithms;plans;learning algorithms have domain-specific parameters;benchmark data sets
situation calculus;user-defined;concurrent execution;programming language
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
closed-loop;plans;multi-stage;concurrent execution;long-term;resource constraints
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters;domain model
domain ontology;knowledge acquisition;knowledge-base;knowledge engineers;knowledge acquisition;problem-solving;planning systems
local search;lower bound;optimization problems;local search techniques;real-life problems;objective function;local search;search effectiveness;cost function;constraint satisfaction problems
automatically generates;scheduling problems;problem instance;obtained by applying;instances;problem instances;automatic generation of;real world;search heuristics
control architecture;planning systems;np-hard;decision problem;scheduling problem
large numbers of;probability models;object identification;high levels of;online learning;object identity;operator;probability theory
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters;probabilistic inference
state space;efficient inference;stochastic processes;real-world;state variables;learning problems;posterior distribution
automated reasoning;probabilistic inference;reasoning tasks;combinatorial optimization
causal relationships;causal discovery;causal models;observational data;sample sizes;small samples;causal model;small sample size
machine learning algorithms;bayesian approach;learning algorithms have domain-specific parameters;benchmark data sets
real-world;rule-based;belief networks;instances;knowledge representation;probabilistic logic;causal structure;bayesian networks;knowledge base;wide range
utility function;probability distribution;state space;von-neumann;bayesian networks
machine learning algorithms;benchmark data sets;statistical modeling;learning algorithms have domain-specific parameters
unknown environment;multi-robot;complexity bounds
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
rates;fuzzy logic;belief networks
path planning;dynamical systems;vector field;partial knowledge;spatial resolution;dynamically changing;heuristic approach
control architecture;reinforcement learning;learning problem

bounded number of;depth-first search;tree search
discrepancy search;search trees;completeness
approximation algorithms;pruning rules;case study;np-complete;approximation algorithm;rule based
beam search;scheduling problems;heuristic search;constraint propagation
operator;case study;situation calculus;stable model semantics
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
incomplete information;situation calculus;object level;action sequences;situation calculus
action theories;machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
computational complexity;completeness;reasoning problems;temporal logic
general case;np-complete;decision problem;temporal reasoning
temporal reasoning;constraint satisfaction problems;temporal constraints
satisfiability problem;temporal information;complete classification
line segment;object recognition;local search;optimal matching;object model;key features;search algorithm;feature matching
shape descriptor;dynamic programming;computational complexity;digital library
rich information;image sequences;image processing;news video;natural language processing
machine learning algorithms;neural network;learning algorithms have domain-specific parameters;benchmark data sets
linear algebra
artificial intelligence
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters;face tracking
plans
software agents;agent systems
classical logic
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters
real-world;real world;high-level;ai techniques;large-scale

business rules;nonmonotonic reasoning;nonmonotonic reasoning
machine learning algorithms;benchmark data sets;learning algorithms have domain-specific parameters;machine learning techniques
syntactic structures
statistical information;natural language processing
activity relationships;ai program contribute to scientific discovery;knowledge discovery;inductive logic programming;scientific discovery;large database
activity relationships;ai program contribute to scientific discovery;bayesian networks
interface design;access logs;user interactions;ai research;ai techniques;web sites;web servers;machine learning;web site;user access;knowledge representation;user modeling;plan recognition
individual agents;machine learning;multi-agent;plan-execution;intelligent agent;key constraints
scene analysis;automatic speech recognition;automatic speech recognition;noise reduction;artificial intelligence
dynamically changing;perceptual information;perceptual information

stochastic search;satisfiability testing
string matching;selection process;knowledge-based
belief change;closely related
activity relationships;ai program contribute to scientific discovery;belief update
decision procedure for;minimal change;intelligent behavior
minimal change;knowledge bases;rule-based;gelfond and lifschitz, 1991;marek and truszczriski, 1994; baral, 1994; przymusinski and turner, 1995;answer set semantics;formal semantics;extended logic programs
belief change;fuhrmann and hansson;nonmonotonic reasoning;zhang
activity relationships;ai program contribute to scientific discovery;nonmonotonic reasoning
activity relationships;ai program contribute to scientific discovery
rewrite rules;sufficient conditions for;rules generated;large number of;consistency constraints;worst case

query answering;knowledge bases;knowledge base
knowledge based systems;computational properties
query answering;decision procedures;description logics;description logics
knowledge bases;description logics;inference algorithm;meta-level;instance;higher-order;object-oriented databases;knowledge-based systems
counterpart;inconsistent information
activity relationships;ai program contribute to scientific discovery
junker, 1992;brewka, 1989;mccarthy, 1986; lifschitz, 1985; grosof, 1991
activity relationships;ai program contribute to scientific discovery
incomplete information;negative examples;positive examples;inductive logic programming;extended logic programs
logic programming;logic program;extended logic programs;answer sets;extended logic programs
problem remains;≠;--r;decision procedures;operator;artificial intelligence
activity relationships;ai program contribute to scientific discovery
multi-modal;randomly generated;decision procedures;davis-putnam
artificial intelligence

instance;real world;case-based reasoning;similarity scores
np-hard problem;optimal performance;heuristic approaches;competence
class noise;classification accuracy;nearest neighbor classifier;training instances;average-case;noise levels;monte carlo
case based reasoning;case-based reasoning;similarity information;case study;adaptation knowledge;integrating multiple;knowledge sources
activity relationships;ai program contribute to scientific discovery
learning process;case-based reasoning;inductive learning;concept hierarchies;unlabelled data;learning method
machine learning techniques
conceptual clustering;learning phase;supervision;similarity-based
activity relationships;ai program contribute to scientific discovery
databases;human performance;knowledge-based;knowledge-based systems
activity relationships;ai program contribute to scientific discovery
visual tracking;computational model;spatial knowledge;target object
generic model;evolutionary computation;agent based
hybrid approach;high-level
decision-making;plan execution;plans
domain knowledge;problem-solving;intelligent tutoring systems;learning process
activity relationships;ai program contribute to scientific discovery
activity relationships;ai program contribute to scientific discovery
constraint logic programming;model-theoretic;constraint solving;constraint logic programming
computational complexity;computational complexity;theoretical results;interactive applications;specific problem;np-hard
compares favorably with;davis-putnam
activity relationships;instances;ai program contribute to scientific discovery
adaptive control
demand-driven;communication overhead
problem-solving;knowledge-intensive;problem-solving;knowledge-based systems
closed world;knowledge compilation;knowledge bases
gardenfors and makinson, 1994;semantic representation
logic programming;worst-case;computational complexity;nonmonotonic reasoning
logic programming;view update;extended logic programs;nonmonotonic reasoning;answer set semantics
goldszmidt et al., 1993;maximum entropy;maximum entropy
linear programming;upper bounds;jaumard et al., 1991;anrig et al., 1998;probabilistic logic;assumption-based
model checking
np-complete;knowledge representation and reasoning;stable model semantics;logic programming;model checking;stable model
model checking;model checking
information systems;description logics;description logics;information integration;general form;databases
tight bound;spatial regions;optimal algorithms;description logics;real-world applications;description logic
description logics;knowledge bases
description language;multi-dimensional;description logics;application domains;operator;common knowledge
description logic;counterpart;description logics;knowledge bases
mccain and turner, 1995;shoham, 1988;minimal change
query evaluation;query evaluation;knowledge bases;situation calculus;knowledge base
decision problems;belief states;decision model;years ago;decision problem
event calculus
logic-based;robot control;logic-based;formal semantics
situation calculus
correct answers;action theories;general setting
np-complete
operator;inference mechanism;formal specification;instances
belief change;total order;sheds light on;fine-grained;similarity-based
operator;state representation;special case;darwiche and pearl, 1997
answer queries;query answering;computationally tractable;classical logic;belief base
resource-bounded;reasoning capabilities;high-level;agent-based;programming languages
general case;case study;decision problem;specific problem
contract algorithms;taking into account
theoretical analysis;adaptation knowledge;demand-driven
problem-solving;case-based reasoning;learning paradigm;case base;feature weights
case-based reasoning;lower bound;case base;case-base;maintenance algorithm;case-base;competence
conditional probabilities;case based reasoning;bayesian network;exemplar based;bayesian networks
case-based reasoning;formal framework for;stochastic model;probabilistic reasoning;constraint-based;statistical inference;inference process
knowledge engineering;case-based reasoning;data bases;machine learning methods;real-world;databases
statistical methods;interface design;learning algorithm;index pages;web sites;web site;web site;clustering algorithm;clustering problem;description language
knowledge discovery;structural information;machine learning
multi-agent;lessons learned
answer queries;normal form;bounded treewidth;reasoning tasks
search techniques;real world;search algorithms;data encryption;search problem
problem instances;search space;search performance;constraint satisfaction;stochastic local search;instances;powerful tools for;search spaces;satisfiability testing;combinatorial problems
planning problems;planning domain;linear programming;programming models;ai planning;integer programming
planning problems;boolean satisfiability
problem instances;graph-based;sat-based;propagation algorithm
temporal planning;temporal planning;planning domains;graph representation
np-complete;high quality;machine learning;knowledge acquisition;decision process;process control;operator;decision making
autonomous agents
fine-grained;computational model;natural language
cognitive processes;cognitive states;cognitive architecture;cognitive model;high quality
steels, 1997, 1998
long term;knowledge representation
jamnik, et al., 97;cognitive processes;instance
computational model;spatial reasoning;semantic model
database theory;decomposition methods;constraint satisfaction problems
variable selection;real-world;depth-first search;constraint satisfaction problems;instances
incremental algorithm
high levels of;inverse consistency;search algorithm
real-world;filtering algorithm
medical diagnosis;bayesian networks
large collections of;search performance;constraint satisfaction problems;temporal databases;improving search
path-consistency;dimensional euclidean space
counterpart;base relations;complete classification;path-consistency
local consistency;sparse graphs;path consistency;path consistency;constraint satisfaction problems
data structure;data bases;compact representation;production rules;set-valued attributes;query sets;artificial intelligence
object recognition;constraint satisfaction problems;constraint satisfaction problem;constraint propagation;randomly generated;domain values;constraint propagation
decision theoretic;state space;multi-agent;decision problems;long term;multiagent systems;optimal policies;decision processes
agent communication;agent communication
learning algorithms;decision-theoretic;action rules;multi-agent;decision-making;computational resources;autonomous agents;resource-bounded;pre-processing;machine learning algorithms;decision making
utility function;multi-agent;autonomous agents;decision making;control mechanism;game theory
software agents;autonomous agents;agent technology;obtain information
multiagent systems;autonomous agents
task allocation
decision problems;resource allocation;programming model
electronic commerce
np complete;search space;search algorithm for
computational complexity;synthetic data;np-complete;search space;optimization problem;general case;depth-first search
provably optimal;sealed-bid;resource allocation
domain-specific knowledge;learning rate;temporal coherence;temporal difference;temporal coherence;learning task;learning method;control parameters
domain-specific knowledge;search space;ai research;single-agent search;search applications;search tree;state-space;domain-dependent;single-agent;domain-dependent
game tree search;search algorithms;game theory

np-complete;optimization algorithm;tabu search;greedy algorithm
world wide web;case-based reasoning;distributed architecture;retrieval techniques
reusability;classification;description logics;knowledge representation and reasoning;meta-level;knowledge representation;description logic;knowledge based systems
web sites;verification problem;data-intensive;web sites;web site's;data-intensive;knowledge base
relevant information;knowledge acquisition;frequently occurring;dynamic systems
domain knowledge;problem-solving;large-scale;knowledge base;problem-solving
reasoning capabilities;design principles;automatically constructing;visual presentation


text classification;search engine;reinforcement learning;machine learning;domain-specific search engines;search engines;machine learning techniques;domain-specific search engines;information extraction;topic hierarchies;increasingly popular
clustering;document collection;document summarization;specific information;extraction process;naive bayes;domain-specific;learning scheme
training data;learning rules;decision tree learning algorithm;content words;machine learning techniques;word sense disambiguation;word sense disambiguation
text mining;em) algorithm;model parameters;maximum likelihood estimation;unsupervised learning;text data;data driven;information access;topic hierarchies;expectation-maximization
aspect model;collaborative filtering;latent space;class models;clustering model;data set;em algorithm;individual preferences
document collection;user query;large number of;text database;information retrieval
training data;decision tree;decision tree;prediction error;variance reduction;error reduction
training set;version space;version space;problem domain;learning strategy;user-supplied
error estimation;domingos, 1998b;search process;generalization error
learning algorithm;gammerman et al., 1998;confidence values;vector machine;support vector machines;margin
feature space;support vector machines;pattern recognition;learning algorithm;jaakkola and haussler, 1999;vapnik, 1995; 1998;sparse linear;upper bound;benchmark datasets;support vector machines;classifier;convex optimization problem
markov decision processes;1998;stochastic games;reinforcement learning;1997
domain-specific search engines;search engines;reinforcement learning;increasingly popular
basis functions;learning algorithm;linear combination;reinforcement learning;temporal-difference learning;temporal-difference;hypothesis class;algorithm converges
shortest path;network routing;learning framework;network topologies;communication networks
problem instances;scheduling problems;relevant features;domain experts;reinforcement learning;finding optimal;global optimization;neural network
data set;data sets;observed data;neural network
graph theory;linear regression;times faster than

algorithm finds;search space;oates and cohen, 1996c;categorical data;time series;search process;worst case;addressing this problem;upper bound;multi-stream;wickens, 1989
np-hard;algorithms for computing;real-world applications;learning rules;database
attribute values;data bases;data mining techniques;simple queries;data mining;sql queries;large data sets;decision tree;data mining;optimization criterion;efficient sql
target function;large-scale
input sequence;neural network;natural language
sensor devices;condition monitoring;problem domain;classification;data fusion
associative memory;large database
knowledge sources;fuzzy set
cognitive architecture;multi-agent system;semantic network;cognitive model;power supply;computational resources;promising candidates
semantic relations between
human intervention
semantic interpretation
features including;natural language generation;natural language
domain-specific search engines;knowledge sources;search engines;increasingly popular
ai technologies;classification;textual data;knowledge management;classification systems;pattern matching;information extraction;fine-grained;pattern matching;existing knowledge;large number of
statistical approaches;statistical queries;training data;markov models;naive bayes;statistics-based;generative model for;generative models;learning theory;maximum entropy;learning approaches;classifier;natural language
clustering;data set;hierarchical clustering;human subjects;search engine
relational learning;inductive logic programming;theoretical basis for;natural language processing
hand-crafted;general-purpose;information retrieval;information retrieval performance;automatically constructed;test collection
information derived from
domain knowledge;singular value decomposition;intelligent tutoring system;information retrieval;latent semantic analysis;statistical technique;latent semantic analysis;semantic information;corpus-based
domain-specific search engines;search engines;game-theoretic;geometric constraints;increasingly popular
classical planning;description language;computational complexity;complete information;plans;planning problem;np-complete;computational complexity
planning problems;high degree of;search space;problem-solver;giving rise to
interaction data;action sequences;recognition systems;clustering algorithm;similar" sequences;plan recognition;clustering approach
planning problems;planning algorithm;domain-independent;plans;times faster than
higher-order;encoding methods;relevance information;encoding schemes
explanation-based learning;search algorithm
planning problems;propositional satisfiability;special structure;stochastic local search;plans
domain-specific search engines;search engines;increasingly popular
decision tree;real-time monitoring;general case;decision process;algorithm to compute;decision making
contract algorithms;problem-solving;ai techniques;contract algorithms
constraint reasoning
multi-pass;search space;profile-based;model generalizes;optimization problem;conflict resolution;resource constrained;scheduling problem;global analysis
convergence properties;comparative analysis;dynamical systems
black-box;rule-base;inference procedure;input-output;automatically generated;qualitative simulation;fuzzy rules;qualitative reasoning;fuzzy logic
domain knowledge;search space;dynamical systems;knowledge representation and reasoning;quantitative analysis;model building
moving objects;qualitative spatial
local information;merging operators
human beings
test cases;logic-based
identification method;dvorak and kuipers, 1991;incomplete knowledge;dynamic systems
concise representation;search space;assignment problem;domain theory;large number of;heuristic search;case study;real-world;relevant information;assignment problem
domain-specific search engines;search engines;increasingly popular
semantic level;wide range;learning environments;graph representation
relational queries;query language;real-life;causal models;instances;formal language;modeling language;modeling approach;relational database;causal model

human robot;spoken language
state spaces;control mechanism;state space;state space;sensor data
accurately predict;motion sequences;estimation algorithm
eye movements;context-dependent;search behavior;visual attention
correlation-based;range images;error model;video-rate;mobile agents;storage requirements;range scans
large numbers of;monte carlo simulation;multiple objects;probabilistic model;object identification;data association;real-world domains;keeping track of;long-range;sensor data;threshold algorithm;approximation algorithm;markov chain;threshold-based;probability model
small world;path length;search problems
search algorithms
domain-specific search engines;search engines;increasingly popular
automated reasoning;basic algorithm;davis-putnam
phase transitions;problem instances;matching problem;phase transition
completeness;plans;information sources;greedy algorithm;query plans;information gathering;network traffic;optimization techniques
genetic algorithms
search space;genetic algorithms;genetic algorithm;fitness
human users;meta-level;agent architecture;comprehensibility
continuous domains;action selection;dynamic environments;autonomous agents;domain independent;additional information
update semantics;preference-based;multi agent
fine-grained;temporal relations;temporal reasoning
propagation algorithm;algorithm runs in;vidal and fargier, 1997;simple temporal;applications involve;vidal and ghallab, 1996;np-hard
temporal information;path consistency;global consistency
progressive processing;output quality;decision problem;meta-level;computational resources;information retrieval techniques;progressive processing
prediction model;decision-theoretic

context-specific;bayesian networks;context-specific;probabilistic inference;inference algorithms for;inference algorithms
key features;bayesian networks;natural language
statistical methods;learning bayesian networks;parameter estimation;database;relational databases;real-world;relational models;relational database systems;learning procedure;structure learning;retrieval techniques;relational structure;large datasets;databases;bayesian networks;data representations;efficient learning;dependency structure;statistical learning;standard database
planning problems;state space;dynamic programming;structural properties;monte-carlo;markov decision processes;action space;estimation techniques
expected cost;decision problems;increasingly large;minimum cost
reinforcement learning algorithms;state spaces;optimal planning;sampling algorithm;partially observable;state space;markov decision processes;generative model;kmn;arbitrarily large;optimal policies
compact representation;dynamic programming;algorithm maintains;state variables;markov decision processes;dynamic bayesian network
selection problem;tight bound;state space;markov decision processes;learning speed;theoretical result;optimal policies
high-accuracy;state abstraction;markov decision processes;state abstraction;optimal policies;optimal control;continuous-state
search technique;learning tasks
learning method;approximation methods;learning algorithms;reinforcement learning;finite state;continuous state
utility function;decision theory
incremental algorithm;training data;learning algorithm;incremental learning;fuzzy rules;fuzzy rule
natural language processing;wide range;common task;application domains;human-robot
lessons learned;real world;model-based diagnosis;model-based diagnosis
learning algorithm;boosting algorithm
machine translation;high-quality;natural language generation;information extraction from;source-language;high cost;multilingual documents
multiple modalities;generated automatically;information space;information access;goal-oriented
agent-based;social interactions;real-world;conceptual model;software systems;complex systems;autonomous agents;artificial intelligence;high-level;agent-based;solving complex
applications involving;causal modeling
meta-level;formal methods;provide evidence
dynamically changing;application domains;machine learning;autonomous agents;share common;real world;theoretical results;artificial intelligence
csp instances
search space;global constraints;constraint satisfaction problems
global constraints;constraint programming;constraint network;cardinality constraints
query-driven;constraint networks;constraint network
scheduling problems;real-life problems;relevant features;classification
real-world applications;constraint programming;constraint networks;constraint network
cost functions;lower bound;optimization framework;cooper and schiex, 2004;wide range;np-hard
constraint satisfaction;constraint satisfaction problems;constraint satisfaction problem
constraint language;high level;formal language;combinatorial problems;natural language
computational properties;completeness;constraint satisfaction;reasoning tasks;decomposition methods;constraint satisfaction problems
constraint programming;1,0
operator;instances;domain specific;binary decision diagrams
problem instances;constraint satisfaction;real-world;global constraint;randomly generated;distance constraints;distance constraints
tree-decomposition;complexity bound;constraint networks;tree-width
constraint optimization problems;search space;search performance
worst-case;domain size;basic algorithm;lower bound;constraint network
csp instances;search algorithm;wide range;total number of;search tree
search procedure;worst-case;constraint programming;data structures;constraint satisfaction problems;space complexity
constraint propagation;local consistency;probabilistic approach;search effort;high probability;search algorithm;constraint satisfaction problems;artificial intelligence
variable ordering;hierarchical structure;wide range;decision diagram
domain knowledge;domain knowledge;bayesian network;learning algorithms;learning bayesian networks;bayesian network structure;real-world;conditional independence;domain knowledge into;training data;parameter estimates;theoretical framework;bayesian networks;feature selection;learning models;optimization problem;maximum likelihood;expert knowledge;learning bayesian networks
algorithm performs;classical logic;constraint programming;search problem;instance;2005;long-term;high-level;search problems
upper bounds;memory requirements;problem domains;petcu and faltings, 2005;computational requirements;mailler and lesser, 2004
model counting;maximum likelihood;probabilistic inference
decomposition method;instances
state space;traffic load;reinforcement learning;simulation results;action space;rates;learning scheme;long term;control scheme;control scheme
fast algorithm;local consistency;worst case;randomized algorithms;constraint satisfaction problems;space complexity
search space;constraint optimization problems;wallace and freuder, 1993; larrosa and meseguer, 1996; larrosa et al., 1999; larrosa and schiex, 2003; 2004;larrosa and schiex, 2003; 2004;algorithm maintains;local consistency
temporal reasoning;increasing attention;qualitative reasoning;inference mechanism
numerical experiments;communication overhead;learning agents;information-theoretic;memory size;takes place;resource allocation
multi-dimensional;description logic
knowledge representation;medical image;spatial relations;spatial reasoning;graph based
knowledge bases;formal concept analysis;description logic;formal concept analysis;knowledge base;description logic
temporal logics
logic programming;knowledge representation;probabilistic logic;probabilistic reasoning;programming language
fixed length;high computational cost;markov models;partially observable;partially observable markov decision processes;hidden markov models;transition probabilities;artificial intelligence;hidden state;context-based;decision making
1990;answer set programming;brewka and eiter;rule-based;reiter;fixed-point;baral;nute
higher-order;cognitive science
counterpart;logic programming
computational complexity;knowledge bases;distinguishing feature;query language;description logics;language called;incomplete information;query processing in;description logic;database technologies;databases;operator;query answering;knowledge base
efficient computation;search space
complex systems;model-based diagnosis
complex systems
black-box;logic-based;ontology languages;real-world
stable-model semantics;semantic web;knowledge-base;classical logic
computationally tractable;post-processing;special-case
multi-agent systems;completeness;temporal reasoning;temporal logic;model checking;temporal logics;combining multiple;temporal reasoning
complex systems
logic programming;answer-set semantics;uniform equivalence;lower bounds;knowledge representation and reasoning
computational complexity;plan execution;planning problem;knowledge compilation;action sequences;generic framework;logic-based;plans
generation algorithm;relational model;finite state machine;software systems;case study;physical systems;finite state
matching algorithms;low precision;completeness;ontology alignment;information retrieval
operator;operator;model-based diagnosis;modeling approach
normal form;normal form;boolean functions;giving rise to
logic programming;logic program;answer set semantics;answer set programming
stable models;logic program;stable model;answer set programming
probability distribution;model-based diagnosis;bayesian networks
answer set;constraint-based;answer sets;answer set programming
instance
query answering over;complexity bounds;completeness;knowledge bases;conjunctive queries over;query language;description logics;data complexity;combined complexity;description logic;conjunctive queries;query answering;knowledge base
model offers;computational model;neural network model
training data;rule learning;learning algorithms;exemplar-based;artificial intelligence;positive effect
decision problems;decision problems;related topics
svm based;svm classifier;high-quality;online discussion
graph partitioning;low cost;web queries;resource bounded;information gathering;resource-bounded;additional features
constraint network;qualitative spatial;constraint language;satisfiability problem;spatial reasoning;real world applications
answer set;logic programming;classical logic
systems require;sketch recognition;classification;incremental learning;incremental learning;inherent uncertainty;computational model
description logics;description logic
computational model

program analysis;based reasoning
logic programming;complexity bounds;lifschitz, 1991;description logics;knowledge bases
problem-solving;selection process;problem solving;decision-theoretic
domain knowledge;hypothesis generation;plans;anomaly detection;multi-agent;plans;action models;software agents;agent-based;plan execution
np-complete;formal model for;spatial reasoning
yaman et al., 2004;plans
clustering;real world applications;autonomous agents
real-world-graphs;np-complete;automatically generated;complex systems;real-world;mathematical framework;model-based diagnosis;topological properties
bayesian network;features extracted from;numerical results;learning algorithm;left ventricle;clinical practice;bayesian networks;bayesian networks;structural information;classifier
automatically identifies;reasoning problems;temporal reasoning;qualitative spatial;np-hard
satisfiability problem;ai planning;closely related;sat-based
state space;decision problem;worst case;upper bound
real-world;sufficiently large;sufficient condition for
similarity function;information contained in
set semantics;medical applications;description logics;based reasoning;description logics
common sense;spatial domain;knowledge representation;spatial reasoning;spatial context
real-world;temporal reasoning;reasoning tasks
distributed nature of;computational resources;event-driven
large number of;search spaces
cognitive states;classification techniques;high dimensional;naive bayes;fmri data;inverse problem;human brain;case study;feature selection;machine learning techniques;magnetic resonance imaging;support vector machines;correlation based;nearest neighbour;select features
algorithms for computing;generated automatically;physical systems;theoretical foundations
preference relation
recognition performance;sign language;large number of;appearance based;pca based;appearance based;pre-processing
resource constraints;linear programming;limited resources;resource constraints;resource utilization


classification;real-world;he et al., 2005;convergence speed;tensor-based;subspace analysis;iterative algorithms;databases;subspace learning;ye et al., 2004;objective function;yan et al., 2005
dynamic programming algorithm;classification;common properties;similarity measure;information contained in;brute force approach;time series;similarity functions;time series;real world;real-valued;metric spaces;longest common subsequence
action selection;resource-bounded;selective attention;symbolic representation;general-purpose
handle complex;incomplete information;computational complexity;answer set programming;np-complete;logic programming
density estimator;kernel-based;decision tree;decision boundary;decision tree;class membership;class probabilities;training sample
forward selection;regularization;classification;training algorithm
operator;noisy data;training examples
state space;state spaces;game-tree;learning agent;reinforcement learning;feature-based;knowledge learned;knowledge transfer
classification rule;fast algorithm;classification techniques;measure called;learning method;instance-based
clustering;multi-modal;document representation;latent dirichlet allocation;clustering method;user's preference;clustering documents;text collections;labeled data;document clustering;document collections;clustering process;task-specific
reinforcement learning algorithm;reinforcement learning algorithm;heuristic function;simple heuristics;reinforcement learning
feature space;data analysis;unsupervised discretization;unsupervised discretization;density estimator;machine learning;discretization methods;algorithms require;class information;continuous attributes;kernel density estimation;pre-processing;real-world applications
multilabel classification;loss function;label set;utility) function;total order;special case;rank correlation
discriminant analysis;data points;data manifold;face databases;linear discriminant analysis;discriminant analysis;global structure;training samples;geometrical structure;manifold structure;local structure;margin
large numbers of;real-world;classification methods;prior knowledge;multi-label;classification problems
transfer learning
reinforcement learning algorithms;relational models;online learning;reinforcement learning;online learning;reward function
database;plans;positive definite;distance matrix
ramon et al., 2001;state space;function approximation;1995;mcgovern et al., 2003;blocks world;learning agent;finney et al., 2002; driessens et al., 2001;utgoff et al., 1997;relational reinforcement learning;tree-based;search space;srinivasan, 1999;tree induction;jensen and neville, 2002;relational learning;stochastic sampling
kernel discriminant analysis;null space;gene expression data;classification;classification accuracy;feature extraction;text data;real-world applications;kernel discriminant analysis;wide range;small sample size;operator;dimensional data;gene expressions;kernel methods;applications involving;dimensionality reduction
case-based reasoning;data-mining;decision support;case base;knowledge discovery;case base;adaptation knowledge;knowledge acquisition;databases;breast cancer;domain-dependent
training data is;real life problems;theoretical results;real-life;training samples;large datasets;classifier
model complexity;model parameters;linear models;computational efficiency;generalization performance;linear regression;variational bayesian;data driven;bayesian formulation;local information;space complexity;locally weighted
tree induction;sampling techniques;observed data;real-world domains;version space;occam's razor;machine learning algorithms
structured domains;recurrent neural networks;hidden markov models;sequential data;probabilistic graphical models;classification algorithm
multi-agent;learning algorithms;learning systems;learning algorithm
data point;computational efficiency;linear programming;optimization problem;excellent performance;similarity function;feature selection;positive definite;kernel learning;machine learning algorithms;kernel function;feature selection
computational resources;prior knowledge;problem solver;randomized algorithms;computational cost
molecular biology;learning algorithm;hidden markov models;user profiling;hidden markov models;hidden markov model;artificial datasets
closed sets;multi-relational;databases;relational data;multi-relational databases
linear transformation;machine learning;salient features;limited number of;distance metric;dimensional data;metric space;low-dimensional;dimensionality reduction
action sequences;learning performance;similarity function;similarity based;optimal policies
unlabeled data;active learner;mutual information;instance based;classification;active learning;information contained in;instance;labeled data;selection criterion;unlabeled instances;classifier
learning problem
structured output;input data;applications including;active learning;large margin;maximum margin;maximum margin;multi-class classification problems
decision trees;kearns and mansour, 1999;induction algorithm;classification;decision tree;higher-level;boosting algorithm;machine learning;decision trees;boosting algorithms;building block;freund and schapire, 1997
recurrent neural network
domain theory;qualitative model;machine learning
greedy search;prediction performance;learning algorithms;machine learning;predictive performance;artificial datasets;learning models;greedy-search;evaluation measures;artificial neural networks;evaluation measures
high-quality;approximation techniques;point-based;partially-observable;dynamical systems
classification scheme;csa a;classification;hvfac;attribute based;high volume;radial basis function;sensitive attributes
training data;data points;unsupervised learning;decision-theoretic;active learning;gaussian process;supervision;large data sets;supervised learning techniques;increasing amounts of;high cost;supervised learning;human effort
desired behavior;sensory data;radial basis function;neural network;database
feature space;feature representations;svm models;linear support vector machines;svm-based;learning algorithms;scaling factors;text categorization;massive datasets;feature selection;feature subset;text classification;computational efficiency
problem instances;state space;state spaces;feature set;learning agents;reinforcement learning;high-level
machine learning problems;instance labels;classification;instances;data sets;multi-instance;instance;desirable property;standard svm;support vector machines;positive class
markov chain;user modeling;object identity;equivalence classes;generative models;protein fold;protein structure;mobile phone;markov chains
gradient-based;special structure;naive bayes;bayesian network classifiers;structure learning;benchmark data sets;machine learning;learning problem;classifier
classification tasks;support vector machines;kernel functions;standard svm
case-based reasoning;maintenance algorithm;facial expressions;case base;case-base;video sequences;human faces;object tracking
domain knowledge;discriminative features;training examples;classification;input features;feature construction;feature-construction;automatically constructed;level features;machine learning algorithms;classification task
domain knowledge;model complexity;graphical models;machine learning methods;long-range;structural properties;protein fold;informative features;inference algorithms
explicitly model;local optima;gaussian process regression;search strategy;large number of;bayesian approach;optimization techniques
markov logic networks;perform inference;knowledge bases;random fields;structure learning;richardson and domingos, 2006;databases;markov logic;knowledge base
auc;total number of;classification tasks;data collected;support vector;error rate;real world;support vectors
worst case
process model;partial observations;concept classes;missing information;learning framework;optimal strategy;information loss
algorithm parameters;test cases;measurement noise;highly variable;objective functions;common sense;evolutionary algorithm;semi-automated
clustering;local search;language models;objective functions;8;occur frequently;objective function
classification task;kernel matrix;kernel matrix;data distributions;feature space
discriminative features;linear transformation;class distributions;linear discriminant analysis;optimization problem;data sets;supervised dimensionality reduction
case base;agent performance;communication protocol
neural network;complex tasks
content based image retrieval;decision trees;decision tree;sample complexity;benchmark data sets;machine learning;data set;decision tree learning;training samples;error bounds for;error rate;error bounds
kernel machines;conjugate gradient;learning problems;conjugate gradient;optimization problem;data-structure;reproducing kernel hilbert space;upper bound;loss functions
selective attention;learning framework
case base
association rules;databases;quantitative association rules;data mining tool;genetic algorithm
case-based reasoning;target task;transfer learning;reinforcement learning;performance gains;source tasks;transfer learning
feature space;classification;data analysis;discriminative learning;kernel-based;input data;sequence data;vector machine;similarity metric;object recognition;time series;sensor readings;problem domain;recognition tasks;character recognition
domain knowledge;local shape;large numbers of;temporal difference learning;evaluation functions;reinforcement learning;reinforcement learning;low level;translation-invariant;evaluation function;binary features
labeled and unlabeled data;unlabeled data;gaussian process;graph-based;kernel functions;cross-validation;labeled examples;semi-supervised;expectation propagation;test examples;semi-supervised;model selection
knowledge learned;transfer learning;markov decision process
neural network;temporal difference;iterative process;neural networks;reinforcement learning
training data;state representation;high-level;control policy;low-level
probabilistic modeling;density-based;data types;data sets;visualization methods;controlled experiments;structured data
cristianini et al., 1999;poor performance;learning rate;quadratic programming problem;update rule;noisy data;support vector machines;margin
markov decision processes;dynamic programming;transition probabilities
meaningful clusters;data analysis;neural networks;knowledge discovery;incremental learning;gaussian distribution;data mining;pattern discovery;real world;statistical models;similarity matching;gene expression data
learning process;rates;temporal difference learning
high-end;dynamically changing;machine learning;low-level;distributed systems;operating systems;online transaction processing
compact representation;hidden markov model;generative model;low level
data mining tasks;classification;feature extraction;low dimensional;input data;feature space;kernel based;feature extraction;pattern classification;parameter learning;data analysis;kernel based;positive definite;learning problem;high dimensionality;information needed;subspace kernel;kernel function;dimensionality reduction
graphical representation of
kernel-based;real-world;kernel approach;kernel matrix;low-rank;data set size;data sets;metric learning;pairwise similarity;semi-supervised;learning method
domain knowledge;cross validation;multiple models;model selection;learning models
sampling algorithm;large-scale;expected error;sampling method;machine learning;domain-specific;target function;optimization process;real-world applications;decision making
classification performance;strongly correlated;feature selection;classification;computationally intractable
past queries;web-based;web search;probabilistic models;machine learning;user activity;web search;conditional probability;user clicks;click prediction
inductive learning;training examples;noise handling;classification accuracy;cost-sensitive learning;cost-sensitive;data cleansing;cost-sensitive classification;theoretical analysis
model checking;representation language;temporal logic;temporal logic
coalition logic;multi-agent systems;social choice;coalition logic;majority voting;model checking
multi-agent;ranking systems
multi-agent systems;complete information;incomplete information;theoretical analysis;autonomous agents;trading agents;expected utility;resource allocation
markov decision processes;reduced model;real world;real time dynamic programming;real time dynamic programming
common assumption;common knowledge;sealed-bid
multi-dimensional;optimization techniques;multi-dimensional;globally optimal;locally optimal
multi-unit;production process
heuristic algorithm
case study;greedy heuristics;game-theoretic
autonomous agents
natural language processing;personal data;personal information;artificial agents
strategy proof;naïve;computationally hard;optimization problems;instances;voting rule;mechanism design;mechanism design
desired behavior;human behavior;motion capture;continuous state;machine learning techniques
real world;artificial intelligence
multi-agent systems;network routing;performance bottlenecks;share information;resource-constrained;partial observability;communication protocol;adaptive approach;adaptive strategy;resource constraints
physics-based;complex systems;multi-agent systems;computational resources
multi-agent;token-based;control method;large-scale
similar items;online auctions
incomplete information;np-complete;normal form
normal form;common knowledge;knowledge-based
computational complexity;1992
social-welfare;social welfare
multi-agent systems;modeling framework
path planning;planning algorithm;planning problem;communication costs;instances;inductive logic programming;distributed systems;agent-based
automated design;social welfare;worst-case;computationally tractable;multi-attribute;mechanism design
human subjects;large scale;machine learning;human activities;real world;design process;embedded systems
social-welfare;real world;utility functions
diffusion model
multi-agent;large number of;single-item
preference relations;real-world;aggregation function;decision problems;structural properties;linear order;voting rule
majority voting;computational complexity;voting rules
approximation method;high level;trading agents;optimal set of
multi-dimensional;model checking;temporal logic
low power;wireless sensor network;multi-agent;energy-efficient;sensor network;target tracking
knowledge sharing;multi-agent system;multi-agent;decision support;user interfaces;software agents;agent-based
compact representation;np-hard problem
trading agents
search techniques;computational complexity;search strategy;search strategies
directed graph;cost functions
multi-agent system;multi-agent system;communication protocol
logic based;real world domains;argumentation based;partially observable domains
multi-agent;model checking;distributed systems
constraint optimization problems;graph structure;optimization problem;quality guarantees;locally optimal
dechter, 2003;dynamic programming;distributed search;optimization problems;memory requirements;problem domains;bounded memory;darwiche, 2001;modi et al., 2005;petcu and faltings, 2005
action selection;case study
multiple agents;preference elicitation;multi-criteria;aggregation function;preference aggregation
salient features;data management
worst-case;social choice;np-hard
decision-making;partially observable markov decision process
multi-agent
solution space;autonomous agents;proposed protocol;common knowledge;distributed search
automated design;dynamic programming;negative results;preference aggregation;general-purpose;optimization techniques;mechanism design
individual agents;agent behaviors;multiagent systems;multiple agents;instances
agent architecture
web browser;multiagent systems
weighted average;mobile agents;simulation experiments
state space;processing algorithms;optimization problem;optimization problem;distributed algorithms;constraint based;constraint propagation
action selection;instance
trading agents
multiple sources;probability distribution;trust model;multiagent systems
long-term
learning process;hypothesis space;learning algorithms;human-machine;machine learning algorithms;classification rate
source language
similarity information;probability estimates;smoothing methods;regression problem;relevant information;conditional probability;similarity graph
instance-based;classification tasks;training documents;svm classifier;class labels;latent semantic indexing;latent semantic indexing;retrieval applications
classification;classification approaches;maximum entropy;error reduction;classification approach;classifier
sparse data;maximum entropy model;word sense disambiguation;knowledge base;base-line;conditional random fields;word sense disambiguation
case base;natural language understanding;rates;case-based reasoning;human-robot
wikipedia-based;human users;semantic analysis;high-dimensional space;world knowledge;semantic analysis;human judgments;domain-specific;natural language texts;machine learning techniques;common-sense;semantic relatedness;vast amounts of
common features;context sensitive;recognition systems;natural language processing;plan recognition;natural language
text processing
biber, 1998;large document collections;anomaly detection
transliteration;maximum entropy model;large margin;named entity;web mining;named entity
kohomban and lee, 2005;machine learning;fine-grained;clustering techniques;classifier performance;word sense disambiguation
collecting data;specific features;end-users;learned models;supervision;predictive model;parameter tuning;user activity
text segmentation;genetic algorithm
clustering;classification;highly complex;cluster-based;rule-based;question answering;real-life;question answering systems;document retrieval;question answering
automatically selecting
automatic evaluation;evaluation measures;corpus-based
classifier
training data;test data;world knowledge;data sparsity;data sparsity;graph based;semantic information;word sense disambiguation;training corpus;word sense disambiguation
graph structures;word sense disambiguation;large scale;graph-based;word senses;graph nodes;natural language processing;standard datasets;graph connectivity;word sense disambiguation
semantic features;semantic knowledge;knowledge sources;data sets;feature set
classification problem;question classification;kernel methods;training data;maximum entropy;semantic categories;question classification;question answering
recommendation quality;user input;query recommendation;query generation;user query;natural language generation;user request;natural language;natural language
constraints imposed by;error propagation;baseline methods;conditional random field;word segmentation
multilabel classification;database;human performance;database;text-database;information extraction systems;database entries;global features;natural language generation;locally optimal;compares favorably with;classification task;classifier
internet applications;automatically generate;web pages
training data;large quantities of;manually annotated;spreading activation;spreading activation;semantic relations;word sense disambiguation;word sense disambiguation
training corpus;high accuracy;active learning
recognition task;pre-computed;invariant features;computationally intensive;automatic speech recognition;brute-force search;length normalization
detection problem;linear program;link detection;correlation clustering;graph-based;clustering approaches;constrained clustering;news event;correlation clustering;clustering framework;clustering algorithm;larger datasets
semantic similarity;information retrieval;interactive applications;designed to support;machine learning;problem solving;topic hierarchy;selection algorithm
prediction accuracy;structured prediction;classification;word pairs;markov networks;training methods;logistic regression;base classifier;maximum margin;support vector machines;conditional random fields;natural language
test set;named entities;named entity;error reduction;entity recognition
dynamic bayesian network
document cluster;content-words;summarization task;position information;multi-document summarization;multi-document summarization
improving accuracy;probability values;domain adaptation
machine translation;contextual constraints;information extraction;web mining;context-specific;natural language processing;question answering
data source;log data;user clicks;question answering;natural language processing
image understanding;vu et al., 2003;increasing importance;high quality
planning problems;heuristic functions;planning algorithm;plans;search space;heuristic search
current location;customer service;specific features;highly dynamic
coalition logic
search applications;search algorithms;heuristic search;low-level
search techniques;replication;gene regulatory network;ai planning;ai research
search space;long-distance;long-distance
planning problems;situation calculus;action language;closed-world;databases
classical planning;planning algorithm;temporal planning;plans;temporal planning;state-space
markov decision processes;state space;algorithm named
multi-agent;web-service composition;computational complexity
real-world;benchmark domains;heuristic search;integer programming
decision-theoretic model;human subjects;knowledge workers;user effort;goal-directed;hidden state;decision-theoretic framework;domain models
plans;learned knowledge;ai planning;machine learning;learning method;problem solving;problem-solving;control knowledge;control-knowledge
operator;situation calculus;semantic web services;knowledge base;context-dependent
planning problems;instances;general problem;domain-independent
state representation;forward search;planning domains;closely related;petri nets;state space search;partial-order;automatic generation of;analysis tool
clustering;detection problem;unlabeled data;data points;multiple views;detection method;sliding window;data stream;high-dimensional data streams;data streams;clustering algorithm;compares favorably with;labeled data
state-space;model checking;boolean satisfiability
planning problems;solution space;global constraints;plans;planning problem
real world domains;state-action;plans;partial observability
planning algorithm;planning domains;plans
planning algorithms;theoretical analysis;planning algorithm;high-level
automated reasoning;situation calculus;reasoning tasks;partial observability
incremental algorithms;linear programming;incremental) algorithms;partial) plans;objective function;simple temporal;piecewise linear
scheduling problems;resource-constrained;resource-constrained;decision support systems;formal description;scheduling problem
classical planning;planning problems;plan execution;temporal logic;domain-specific;partially ordered;model-checking;operator;propositional satisfiability;control knowledge;plans
markov decision processes;high scalability;plans;stochastic domains
optimization problems;expected utility;sufficient conditions;optimal policies;large-scale
real-world;simple temporal;temporal reasoning;constraint-based
logic programming;son et al., 2005a;planning problems
np complete;multi-agent;optimal algorithms;markov decision processes;optimal policies;decision making
planning problems;path planning;search space;plans;planning tasks;multi-robot
dynamic programming algorithm;dynamic programming;partial information;decision-making;planning algorithms;belief space;space complexity;decision making
domain-independent;distance functions;plans;domain independent planning;diverse set of;local search;domain independent;domain independent
model checking;probabilistic planning
database;situation calculus;incomplete information;incomplete knowledge;action theories;sensing actions
state space;irrelevant attributes;blocks world
beam search;structured output;planning domains;classification;ai planning;heuristic functions;discriminative learning;state-space;discriminative learning;benchmark domains;search heuristics;search problems
forward search;planning domains;heuristic-search;plans;rates;heuristic search;automated techniques
state space;reinforcement learning;learning tasks;control knowledge;knowledge transfer
parameter space;efficient inference;hidden variables;belief propagation;image labeling;object detection;model parameters;tree structured;real world;conditional random fields
visual tracking;view based;estimation problem;real data
spatial relationships;real world;spatial constraints;human-robot
mutual information;application domain;image alignment;similarity measures;image data;reference image;image alignment
optimization framework;motion capture;dynamic model;low dimensional;prior knowledge
expected utility;multi-robot
belief states;partial observability;decision-theoretic;programming language
database;indoor environment;stereo matching;graph matching;matching problem;stereo images
conditional random field;random fields;unknown environments;graphical model;laser range;topological structure
object recognition;vision systems;tracking objects;low resolution;data stream;high resolution
unsupervised learning;data association;probabilistic reasoning;temporal context;sensory data;model selection;real world;multi-sensor;audio-visual
data set;large-scale
detection problem;theoretical foundation;optimum solution;multi-sensor;multi-target;target detection
sketch-based;patch-based;image content;bayesian approach;statistical inference;human faces
expectation maximization algorithm;network connectivity;probabilistic model;search strategy;occam's razor;sensor network;higher level
knowledge representation;distributed architecture;data acquisition;multi-sensor;data fusion
information retrieval research;text categorization;machine learning;instances;long-term;location data
radio frequency identification;learning algorithm;mobile devices;latent semantic indexing;graph laplacian;high accuracy;sensor networks;semi-supervised;labeled and unlabeled data
bayesian framework;feature based;feature based
high-resolution;dynamic environments;state estimation;probabilistic model
detection algorithms;gaussian process;classification;particle filters;estimation process;particle filter
filtering algorithms;markov random field;smoothing method;data association;real data;loopy belief propagation;spanning tree
stereo vision;markov random field;learning algorithm;depth estimation;visual cues
planning algorithm;mutual information;gaussian process;path length;approximation guarantees;region-based;environmental monitoring;mutual information between;real world
vision-based;lighting conditions;real world;environmental conditions;image segmentation
high-speed;learning process;mobile robotics;supervised learning;human intervention;control problem
object recognition;nearest-neighbor classifier;instance-based;classification;feature extraction;markov networks;classifier;classification method;range data;classification rate;feature vectors;instance-based
object recognition;monte carlo;multiple hypotheses;databases;real world;takes place
activity models;common sense;human activity;common sense;labeled data;precision/recall;sensor data;feature selection

search methods;window size;search tree;sliding window;algorithm called;knapsack problem;search algorithm;heuristic search;tree model
local search algorithms;search algorithms;years ago;search algorithm for;large number of;high-level;satisfiability testing
additional constraints;auxiliary;combinatorial problems;instances
computational complexity;dynamical systems;formal model for;social network;social networks;computational models;finite state
long-range;real-world;data sparsity;machine learning;adaptive sampling;control flow;high probability
clustering;search results;clustering results;web pages;breadth-first search;web graph;multi-agent;web graph;information retrieval;clustering technique;beam search;web search results;retrieved documents;ambiguous queries;web page;heuristic search
information-theoretic;real-world;search algorithms;instances;information-theoretic
model counting;boolean satisfiability;local search;solution space
local search
satisfiability problem;np-complete
theoretical properties
clause learning;search algorithms;strong evidence
case study;pattern database;disk-based;heuristic search;parallel processing
parameter-free;genetic algorithms;genetic algorithm;knapsack problem;selection process;search process;solution space;fitness;szeto and zhang, 2005
instances;lower bounds;lower bound;heras and larrosa, 2006;inference rules
search algorithms;search algorithm
graphical models;tree decomposition;trade-offs
ranking algorithm;state space search;finding optimal;state-space;optimization procedure;search algorithm;search problems
search methods;effectively exploit;local search;independent variables;real-world;local search techniques;problem instances;local search;search technique
multi-agent systems;dynamic programming;tight bound;coalition structure generation;worst case;heuristic search
search space;pre-processing;prior knowledge;algorithms require;search algorithms;learning speed;path-planning;search algorithm;heuristic search
utility function;sequence alignment;training data;shortest-path;temporal planning;problem domains;algorithms require;provably optimal;user's preferences
search space;bayesian networks;excellent performance;search algorithm for
shortest paths;search tree;search algorithm;search problem;incremental version
neighborhood structure;neighborhood graph;dimensionality reduction;benchmark data sets
search space;search algorithms;combines ideas from
search algorithms;external memory;search algorithm for;external-memory graph search;search problem;domain-independent;disk storage;local structure;external-memory
linear program;optimization method;memory requirements;finite-state;linear programs;partially observable markov decision processes;wide range;optimization techniques
belief base;linear order;efficient computation
approximate algorithm;conditional independence;probability distribution over;large data sets;particle filtering;network structure;network structures;posterior probability;distributed data;information gain
markov models;linear programming;bellman and kalaba, 1959;reinforcement learning;optimization problem;markov decision processes;computationally tractable;linear programming;existing knowledge;markov decision process
network structure;local structure;bayesian networks;multiple queries
incomplete information;perfect information;game theory
conceptual graphs
belief change;belief change;spatial locations;problem instance
link discovery;approximation algorithm;success probability;biological networks;binary decision diagrams
partially observable;instance;general setting;dynamic systems;fundamental problem;belief state;markov decision process
latent variable model;training set;gaussian process;gaussian process;latent-space;ground truth;location-aware;building block for;physical location;latent variable models
markov models;joint inference;label sets;training methods;computational cost;computational overhead;perform poorly;supervised learning;benchmark data sets
discriminative models;graph-based;linear regression;generative model;graph-based semi-supervised learning;semi-supervised;semi-supervised learning methods;conditional probabilities;learning method
description language;belief change;action language;extended logic programs
training set;classification problem;high cardinality;information bottleneck;naive bayes;bayesian network;hierarchical bayesian model;decision trees;bayesian networks;preprocessing phase;conditional probability;class probabilities
graph model;program analysis;automatically infer;highly accurate
belief update;belief update
transition probabilities;hidden markov model;huge number of;high accuracy
synthetic data;parameter estimation;random fields;feature selection;unified framework;maximum likelihood;classification problems;conditional random fields;training algorithm
planning problems;plans;probability distributions over;real-valued;markov decision processes;probability distributions;quality guarantees;desired accuracy;continuous state
search algorithms;hierarchical structure;forward search;performance gains;heuristic search;stochastic domains;search nodes
probabilistic modeling;data structure;computational complexity;graph-based;normal forms;item sets;bayesian networks;binary decision diagrams
markov decision processes;reduced model;real world;real time dynamic programming;real time dynamic programming
hierarchical organization;markov models;sign language;multi-channel;hierarchical structure;time series;hidden markov models;hidden markov model;efficient learning;human actions;markov chain;markov model
belief change
approximation error;function approximation;linear systems
particle filtering;inference algorithm;dynamic systems;state variables
apprenticeship learning;inverse reinforcement learning;preference elicitation;probability distribution over;learning tasks;prior knowledge;inverse reinforcement learning;reward functions;reward function;markov decision process
partially observable markov decision processes;search algorithm;state space;search algorithm for
efficient inference;mixture model;naive bayes;dirichlet process;naive bayes classifier;data sets;learned model;generative model;naive bayes classifiers;approximate inference;transfer learning
hidden markov models;belief propagation;probabilistic logic;probability computation
belief states;planning domains;partially observable;competing methods;domain size;filtering algorithm;belief state;deterministic domains;stochastic domains
point-based;belief space;forward search
cohen and levesque, 1990; rao and georgeff, 1991;common assumption;situation calculus;2005; 2006
planning algorithm;markov models;black box;goal-directed;current location;motion models;short-term
efficient computation;partially observable;belief space;decision problems;computational cost;computational model;error bounded;error bounds
classification rules;large distributed;distributed data mining;databases
clustering;overlapping clusters;gene expression data;classification;clustering approaches;meta-clustering;computationally tractable;data mining;noise levels;huge number of
expert search;information retrieval;expert finding;multiple models;graph-based;specific problem
high-dimensional;data set;high-dimensional space;hash functions;data points;large scale;real-world;hash function;similarity function;large data sets;locality-sensitive-hashing;similar items;video retrieval;positive examples
data-driven;training examples;user queries;extraction rules;information extraction;human input;information extraction from;error reduction;web page;highly scalable
recommender systems;dynamic nature of;semantic web;case study;semantic web;knowledge representation;databases;fuzzy sets
conditional random fields;information extraction systems;answer questions
nonnegative matrix factorization;nonnegative matrix factorization;dynamic nature of;static data;real data;topic detection;data streams;latent factors;temporal data
information sources;real-world;semantic model;web-forms;automatically generating;web services;web sources;error prone
clustering;clustering algorithms;distributed databases;distributed databases;data items;user queries;convergence speed;clustering algorithm;distributed database;evaluation measures
real-world;vector space;synthetic data;graph embedding;directed graph;stationary distribution;transition probability;link structure;classification problems;web page;graph embedding;random walks
case study;partially-ordered;rich information
database;classification tasks;relational tables;statistical model;statistical relational learning;relational database;statistical models;relational data;statistical relational learning
machine translation;lower-dimensional space;language-independent;cross-lingual;linguistic knowledge
pre-defined;web corpus;web text;markov models;named entities;web text;training data;entity recognition;semi-supervised;conditional random fields
predictive models;web search;browsing behavior;search sessions;user actions;user activity;information seeking;search behavior
information retrieval applications;machine learning;linguistic information;question answering;information extraction;question answering;context-based;natural language
hybrid approach;web pages;classification;generalization performance;semi-supervised learning;link information;test collections;labeled samples;maximum entropy;model trained;classifier
human judgments;sentence-level;search engine
data collected from;user preferences;random-walk;recommender systems;ranking techniques;memory usage;fouss et al., 2005; sarwar et al., 2002;data set;fouss et al., 2005;extracting knowledge from;computational cost;standard database
clustering;score based;frequently occurring;high-frequency;cluster-based;strongly correlated;semantic web;blog data
simulation study;special attention;rank documents;ranking algorithms;search engines;social networks;personalized recommendations;document rankings
data objects;algorithm performs;clustering results;data points;large number of;categorical data;multiple clusterings;clustering accuracy;clustering process;data sets;diverse set of;dimensional data;clustering algorithm;clustering
ksantini et al., 2006;color image;probability distributions;irrelevant features;retrieval accuracy;retrieval method;regression models;logistic regression model;distance measures;retrieval performance;feature selection;logistic regression;databases;content-based image retrieval;deselaers et al., 2004;feature vectors;similarity measures
computational efficiency;density estimation;dirichlet prior;mixture model;great promise for;dirichlet process;data clustering;cluster assignment;variational bayesian;real world;nonparametric bayesian;mixture models
test set;test data;unlabeled examples;test instances;traditional classification;algorithms require;instances;labeled training examples;bayesian classifier;text classification;naïve;classifier;unlabeled set
fuzzy inference system;detection accuracy;vector machine;high complexity;feature selection;fuzzy inference system;statistical significance
parameter estimation;higher-level;data sets;discovery algorithm;sensor data;fundamental problem;real world;artificial intelligence
clustering;web documents;high precision and recall;automatic extraction of;social network;semantic web;extraction methods;social networks;automatically extracts;social networks
online auctions;online auction;13;feature-based
relevant attributes;large-scale;information extraction;web documents;query logs
classification problem;unlabeled data;semi-supervised learning;naïve bayes;training data;supervision;product descriptions;databases;semi-supervised;automatically extract;classification algorithms
regression problems;dynamic integration;learning algorithm;large number of;data sets;dynamic integration;computational overhead;feature selection
context-free;higher-level;representation scheme;human activities;explicit feedback;high-level;long-term
learning process;user profile;classification;database;classification accuracy;text documents;keyword-based;user profiling;user interests;user profiles;naïve bayes;text classifiers;linguistic knowledge;automatically infer;word sense disambiguation
classification problem;document summarization;summarization task;heuristic rules;conditional random fields;data set;supervised methods;document summarization;labeling problem;conditional random fields
probability estimates;multiple tasks;low-cost;cost model;combination methods;features extracted from;general problem;special case;classifier;likelihood ratio
higher precision;web page;poisson process;web crawlers;web sources;web page
ai technologies;vector space model;machine learning
tree-structured
face recognition;recognition performance;training dataset;selection methods;recognition problem;face database
knowledge base;semantic indexing;community members;competence;semantic search
user profile;ranking process;summarization task;manifold-ranking;multi-document summarization;greedy algorithm;manifold-ranking
real-world datasets;latent dirichlet allocation;hidden variables;mixture models;time series;data records;data streams;pattern discovery;mixture models;svd-based
hidden markov models;hidden markov model;state transitions
semantic smoothing;cosine measure;similarity measure;performs poorly;smoothing method;context-sensitive;information retrieval;agglomerative clustering;automatically identifies;kullback-leibler divergence;class labels;clustering quality;nearest neighbors;class-specific
feature space;web pages;web search;link information;keyword-based;search result;multimedia information;search performance;web search;image-based
times faster than;apriori-based;complex patterns;pruning technique;sequential patterns
data values;classification;input data;plans;speculative execution of information gathering plans;data sources;speculative execution of information gathering plans;multiple sources;closely tied to how accurately
logic programming;queries posed;data integration systems;instances;stable model semantics
data values;query rewriting;closely tied to how accurately;data integration systems;speculative execution of information gathering plans
web pages;instance-based learning;naive bayes;statistical method;integrating multiple;integrating multiple;desired information;text analysis
low bandwidth;source documents;temporal reasoning;temporal dimension;multimedia documents
multi-agent systems;ontology-based;data extraction;design requirements
clustering;mining results;web pages;web page classification;classification;feature weighting;web mining;mining tasks;web mining;web page;unlike conventional;irrelevant information
hidden markov model;human players
electronic commerce;selective attention
generation algorithm;database;similarity metric;case retrieval;general case;evaluation criteria;user request;helps users
data values;closely tied to how accurately;speculative execution of information gathering plans
logical properties;database;knowledge bases;query evaluation;databases
query answering over;knowledge bases;based reasoning;partition-based;graph-based;hand-crafted;answer queries;query answering
data values;closely tied to how accurately;speculative execution of information gathering plans
data values;closely tied to how accurately;speculative execution of information gathering plans
semantic properties;logic program;minimal change;minimal change;knowledge bases
case-based reasoning;sufficient conditions for;recommender systems;attribute-selection
preference-based;preference-based;recommendation process;recommender systems;user feedback
closely tied to how accurately;data values;support vector machines;speculative execution of information gathering plans;information gain
mccain and turner, 1997;nonmonotonic reasoning
takes into account
causal models
data values;closely tied to how accurately;speculative execution of information gathering plans
simulation data;conventional methods;dynamic bayesian networks;large-scale;information processing;ruff et al, 2001;human brain;modeling approach
closely tied to how accurately;speculative execution of information gathering plans;data values;human-robot
object recognition;simple heuristics;learning method
constraint solving;search procedure;constraint satisfaction problems;search tree
general case;decomposition methods;cohen et al., 1997; 2000b; 2000a;constraint satisfaction problems
parallel implementation;habbas et al, 2000;decomposition method;instance;jegou, 1993;constraint satisfaction problems;search efficiency
data values;closely tied to how accurately;soft constraints;speculative execution of information gathering plans
optimization problems;approximation techniques;real life;soft constraints;computational complexity
ordering constraints;constraint satisfaction;applications including
decomposition method;graph-based;hidden-variable;tree decomposition;constraint satisfaction problems
constraint satisfaction problems
local consistency;data values;closely tied to how accurately;speculative execution of information gathering plans
local consistency;simple algorithm;constraint programming;constraint propagation;worst case
constraint optimization problems;discrepancy search;resource allocation;hybrid method;competing methods;real-life;optimization problems;constraint propagation
constraint programming;hentenryck et al., 1999;decision problems;wide range;walsh, 2002;decision making
local consistency;global consistency;constraint network;set intersection
boolean satisfiability;efficient construction

utility function;intelligent agents;preference elicitation;incomplete information;utility functions;expected utility;decision theory
expected utility;utility functions
expected utility;decision rule;closely tied to how accurately;data values;speculative execution of information gathering plans
expected utility;qualitative decision;decision procedures;artificial intelligence;dubois and prade, 1995
data values;closely tied to how accurately;speculative execution of information gathering plans
description logics;description logic;knowledge bases
description logic;description logics
transitive closure;operator;sattler and vardi, 1999;description logics
data values;closely tied to how accurately;description logics;speculative execution of information gathering plans

data values;closely tied to how accurately;speculative execution of information gathering plans
knowledge bases;description logic;description logic
closely tied to how accurately;model checking;data values;speculative execution of information gathering plans
multi-agent;design space;design choices;distributed algorithms
model-based diagnosis;embedded systems
data values;closely tied to how accurately;speculative execution of information gathering plans
algorithm to compute;decomposition methods;constraint satisfaction problems;decomposition methods;tree-structured
model based diagnosis
information extraction from;web documents;semi-structured documents;specific information;information extraction
multi-source;meta-data;information extraction;multiple modalities;semantic categories;video content;multimedia content;multiple sources
algorithm requires;active learning;multi-view;case study;labeled data;multi-view;labeled examples
dynamic bayesian networks;extraction task;information extraction;entity recognition;natural language processing;information extraction system;language modeling
multi-level;accurate models;machine learning methods to;automatically extracting;information extraction;instances;hidden markov models;information extraction
clustering;manually assigned;semantically related;domain-specific;web mining;extraction algorithm;huge number of
logic programming;data values;closely tied to how accurately;speculative execution of information gathering plans
instance;constraint network;data points;computational properties
closely tied to how accurately;average-case;knowledge representation;data values;speculative execution of information gathering plans
closely tied to how accurately;sketch recognition;data values;speculative execution of information gathering plans
real-world;heterogeneous sources;classical logic
clustering;principal components;objective functions;data clustering;data clustering
clustering;distributed data sources;valuable knowledge;huge amounts of data;local density;takes into account;knowledge discovery;communication costs;distributed clustering;data mining;kernel density estimation;distributed sources;clustering algorithm based on;distributed environment
bayesian network;discriminative learning;classification;optimization methods;naive bayes;network models;regression models;prediction tasks;logistic regression;learning problem;network structure;naive bayes models
ensemble learning;error estimation;margin;monte carlo
tree induction;machine learning methods;generalization error;training examples;ensemble methods;training data is;multiple hypotheses;learning curve;higher accuracy than;classifier ensembles;base classifier;predictive accuracy
data values;closely tied to how accurately;test data;speculative execution of information gathering plans
auc;evaluation criterion;predictive accuracy;classification;learning algorithms;predictive performance;evaluation measures
learning algorithms;search spaces;object identity;theoretical results
general purpose;spatial relationships;human performance;probabilistic reasoning;np-complete problem;human players;automatically learning
kernel-based;linear models;vector machine;local minimum;local minima;objective function
data values;closely tied to how accurately;dimensionality reduction;speculative execution of information gathering plans
unlabeled data;labeled data is;classification;semi-supervised learning;expectation maximization;data-sets;error model;classification;semi-supervised;labeling process;maximum likelihood;classifier
spectral clustering algorithms;unlabeled data;high accuracy;spectral learning;classification accuracy;learning algorithm;data set;unlabeled documents;training documents;labeled examples
multiple classes;single-class;unlabeled data;classification;classification;support vector machines;target class;single-class
closely tied to how accurately;data values;learning algorithm;speculative execution of information gathering plans;web page
training phase;classification;text categorization;text categorization;single-label;supervised learning algorithm;machine learning;learning method;weighting scheme
unlabeled data;unlabeled documents;training documents;text classification;classification techniques;classifier;positive class
inductive learning;memory constraints;decision tree construction;data set;data structures;inductive learning;sequential data;control mechanisms;learning framework;wide range
tree induction;induction algorithms;decision tree learners;decision tree
problem remains;individual agents;np-complete;real-world;software agents;game theory
data values;closely tied to how accurately;speculative execution of information gathering plans
simulation experiments;large number of;physics based
control mechanisms;replication;location-based;programming language
application domain;rule based;learning mechanism
multi-agent systems;problem called
agent behaviors;problem domains;evolutionary computation;evolutionary computation;theoretical justification;multiagent systems
linear programming;closely related;human players;texas hold'em;game-theoretic
data values;closely tied to how accurately;speculative execution of information gathering plans
closely tied to how accurately;logic-based;data values;speculative execution of information gathering plans
decision theory
agent communication;natural) language
gradient-based;policy learning;learning rate;stochastic games;learning algorithm;reinforcement learning;learning tasks;general-purpose;robot learning;multiple agents;multi-robot;single-agent;multiagent learning
belief states;brute-force;globally optimal;analysis reveals;optimal algorithms;partially observable markov decision process;theoretical results;growing importance;dynamic programming approach;search algorithm;reward function
closely tied to how accurately;speculative execution of information gathering plans;data values;bayesian approach;reinforcement learning
data structures;intelligent agents;plans
human behavior;intelligent tutoring system;requires minimal;human effort
strategy-proof
multi-agent systems;individual agents;multi-agent systems;complex systems;agent systems;control mechanisms
data values;closely tied to how accurately;speculative execution of information gathering plans
data values;closely tied to how accurately;speculative execution of information gathering plans
game theory;np-hard
potential function;general case;potential functions;theoretical results
sufficient conditions;existing protocols;np-hard;preference aggregation
data values;closely tied to how accurately;speculative execution of information gathering plans
resource sharing;multi-stage
sample data;concept hierarchy;word sense disambiguation;semantic relatedness;human judgments
statistical approaches;statistical method;natural language generation
data values;classification;world knowledge;speculative execution of information gathering plans;word sense disambiguation;closely tied to how accurately
closely tied to how accurately;speculative execution of information gathering plans;data values;agent-based;natural language
outlier detection;detecting outliers;outlier detection;knowledge-base;detection problem
multi-valued;information content;information derived from
logic programming;computational complexity;knowledge-based;aggregate functions;aggregate functions;logic programming;finite structures;real-world applications
stable models;logic program;benchmark domains;stable model;stable model semantics
answer set;answer sets
user preferences;answer set programming;answer set;answer sets;optimization techniques;preference relation
counterpart;rule-based
data values;closely tied to how accurately;speculative execution of information gathering plans
artificial intelligence
case study;spatial regions;instance;designed to support;spatial reasoning;general problem;knowledge base
data values;closely tied to how accurately;speculative execution of information gathering plans
motion patterns;data values;closely tied to how accurately;speculative execution of information gathering plans
high-dimensional;optimization problems;5; 16; 9;2;object models;range data;mathematical framework
closely tied to how accurately;particle filters;data values;speculative execution of information gathering plans
planning domain;plans;black box;planning algorithm;general-purpose;tree structure
planning problems;instances;domain descriptions;semi-automatic;solving complex
temporal planning;planning graphs;search process;plan quality;planning graph;heuristic search
np-complete;constraint network
planning problems;graph based;planning graphs;planning problem
domain descriptions;plans;real-world domains
minimal cost;bayesian network;probabilistic inference;memory requirements;probabilistic reasoning;caching scheme;bayesian networks;exact inference
computational efficiency;state space;particle filters;dynamic systems;particle filter
instances;belief networks;probabilistic inference
data values;relational models;closely tied to how accurately;speculative execution of information gathering plans
markov decision processes;instances;linear programming;plans
learning agents;semi-markov;decision problem;generally applicable;decision process;decision processes
probability distributions over;policy gradient;reinforcement learning algorithms;policy search
point-based;point-based
optimization methods;closely tied to how accurately;data values;speculative execution of information gathering plans
reference point;real-valued
multi-level;data collection;multi-layer;spatial aggregation;gaussian process;data mining;process models;input data;mining algorithm;simulation results
training data;case study;qualitative model;regression function;machine learning
data values;situation calculus;closely tied to how accurately;control knowledge;speculative execution of information gathering plans
partially observable;closely tied to how accurately;data values;speculative execution of information gathering plans
computational complexity;reasoning tasks
numerical values;action language
high-level;constraint logic programming;programming languages
situation calculus;formal framework for;sensing actions
problem instances;decision-theoretic;real-world;theoretical results;limited number of;information gathering
markov decision processes;optimal policies;optimization problem
data values;closely tied to how accurately;speculative execution of information gathering plans
human subjects;classifier
unknown environments;decision-theoretic
data values;closely tied to how accurately;speculative execution of information gathering plans
data values;closely tied to how accurately;speculative execution of information gathering plans
data values;closely tied to how accurately;filtering algorithm;speculative execution of information gathering plans
estimation problem;mobile robotics;filtering technique;fundamental problem;belief state;maximum likelihood;kalman filter;linear-space;unknown environment
variable ordering;davis-putnam
problem instances;problem instance;worst-case;structural properties;model-checking;small sets of;real-world;combinatorial problems
local search algorithms;instances;local search;local minima;random sat
phase transitions;phase transitions
clause learning;data values;closely tied to how accurately;speculative execution of information gathering plans
phase transitions;data values;closely tied to how accurately;speculative execution of information gathering plans
scheduling problems;worst-case;search strategy;contract algorithms;search strategies;contract algorithms
scheduling problems;computational cost
agent based;scheduling problem;autonomous agents;multi-agent system;highly complex
search algorithms;heuristic search;heuristic function;dynamic programming;connected components;optimal policies
sequence alignment;dynamic programming;computational biology;higher-dimensional;space requirements;multiple sequence alignment
simpler models;finite-state;multiple sequence alignment;instances;exact inference;tree models
np-complete problem;problem size;running times;search tree;minimum number of
search space;search algorithms;korf and zhang, 2000;sparse representation;search effort;space complexity
data values;closely tied to how accurately;speculative execution of information gathering plans
computational properties;logic-based
data values;closely tied to how accurately;temporal constraints;speculative execution of information gathering plans
global optimality
spatio-temporal;recognition algorithm;simulated data;recognition process;temporal constraints
goal recognition;goal recognition;corpus-based;corpus-based;hand-crafted
data values;closely tied to how accurately;plan recognition;speculative execution of information gathering plans
face recognition;dynamic programming;real-world;large number of;reinforcement learning;prohibitively expensive;operator;training sample;information gain
filtering algorithms;state space;multiple objects;generally applicable;dynamic model;probability distributions;measurement data
approximation algorithms;gather information;distributed systems;np-hard
logic programming;domain theory;rounds and zhang, 2001;knowledge representation and reasoning
data values;case base;closely tied to how accurately;solution-space;speculative execution of information gathering plans
data values;closely tied to how accurately;speculative execution of information gathering plans
accurately reflect;user profiling;user ratings;web browsing;case-study
united states;salient features;simulated annealing;8; 4
data values;closely tied to how accurately;grid-based;speculative execution of information gathering plans
larsen et al, 2002;long-distance;plan execution;random variables;plans
local consistency;constraint satisfaction problems;finite domain;tree search
data values;closely tied to how accurately;constraint satisfaction problems;speculative execution of information gathering plans
data values;closely tied to how accurately;speculative execution of information gathering plans
instance;probabilistic reasoning;random walks
operator;learning process;bound-consistency;finite domain;finite domain
sensor networks;test problems
fitness function;genetic algorithms;genetic algorithm
data values;closely tied to how accurately;temporal reasoning;speculative execution of information gathering plans
belief change;belief state;minimal change
event calculus;sensing actions;partially observable domains;causal structure;sensing actions;event calculus
knowledge learned
kwok, et.al. 98
utility functions
closely tied to how accurately;data values;logic-based;image sequence;speculative execution of information gathering plans
semantic retrieval;learning framework;image content
stream processing;broadcast news;entity search;news video
web search;collaborative web search;search process;search engines;search engine;security concerns;web search engines;web users
closely tied to how accurately;speculative execution of information gathering plans;data values;string similarity;statistical model
association mining;video sequence;valuable knowledge;association patterns;database management;class label
ground truth;learning algorithm;piecewise linear;probabilistic approach;labeled and unlabeled data;motion models
data values;closely tied to how accurately;large-margin classifiers;speculative execution of information gathering plans
image classification;speculative execution of information gathering plans;data values;closely tied to how accurately;active learning
data values;closely tied to how accurately;speculative execution of information gathering plans
ensemble learning;ensemble learning;theoretical framework;multiple classifiers;machine learning;cross-validation;parameter settings;ensemble classification;single classifier;classifier
markov decision processes;reinforcement learning algorithms;optimal policies;reinforcement learning;reinforcement learning problems
text classification;unlabeled examples;training examples;text classifiers
feature space;euclidean distance between;pattern recognition;label information;distance metrics;distance metric learning;real-world;input space;machine learning;distance-based;benchmark data sets;metric learning
closely tied to how accurately;speculative execution of information gathering plans;data values;resource allocation;computing systems
agent architecture
incomplete information;data values;closely tied to how accurately;speculative execution of information gathering plans
imitation learning;imitation learning;hidden markov model
virtual world;multiple users;multi-agent system;human-robot
speech recognition;multi-agent system;multi-agent;distributed processing;agent architecture;task allocation
data values;closely tied to how accurately;speculative execution of information gathering plans
word sense disambiguation
early stage;intelligent tutoring system;latent semantic analysis;natural language understanding

automated reasoning;question answering;text processing
semantic features;syntactic features;special case;knowledge base;decision tree
action selection;control architecture
data values;closely tied to how accurately;speculative execution of information gathering plans
neural networks;motion control;action selection;action selection;multi-robot;continuous state
temporal sequence;artificial neural network;artificial neural network;banquet et al, 1998
data values;closely tied to how accurately;speculative execution of information gathering plans
temporal relations;planning domains;plans;multi-agent;planning algorithms;multi-agent;partially ordered;search algorithm;single-agent;formal framework
probabilistic model;plan recognition
markov decision processes;heuristic techniques;plans
data values;planning algorithm;closely tied to how accurately;speculative execution of information gathering plans
branching factor;state space;plans;partial plans;post-processing
planning problems;data values;closely tied to how accurately;speculative execution of information gathering plans
times faster than;state space;plans;search space;exhaustive search
heuristic functions;search methods;search techniques;single-agent;complete search;single agent search;heuristic function;path-planning;heuristic search
human experts;ai research
problem instances;search algorithms;application domain;moving target;moving target;multiple agents;multiple agents;single-agent
cache performance;extended abstract;duplicate detection;breadth-first search
np-complete problem;hill-climbing;tabu search;simulated annealing;minimization problem
closely tied to how accurately;local search;data values;speculative execution of information gathering plans
content based image retrieval;line segments;image retrieval;shape similarity;real images
data values;closely tied to how accurately;speculative execution of information gathering plans
speech recognition;real-world;mobile robotics;human robot;knowledge based;visual recognition;automatic speech recognition
data values;image-based;closely tied to how accurately;speculative execution of information gathering plans
data values;closely tied to how accurately;image processing;artificial intelligence;speculative execution of information gathering plans
knowledge representation;corpus-based;corpus-based;knowledge base
high-level;web search engines
information resources;plans;record linkage;web sources;relevant information;query planning;personal information
information technology;artificial intelligence
data values;closely tied to how accurately;speculative execution of information gathering plans
automated reasoning;graph theory
automated reasoning;automated reasoning
user interfaces;data values;closely tied to how accurately;speculative execution of information gathering plans
domain-specific;specific features;information technology;intelligent systems;application scenarios
data values;closely tied to how accurately;speculative execution of information gathering plans
cognitive science
clustering;broadcast news;ai technologies;news video;machine learning;broadcast news;entity search;user preference
cooke and hine, 2002;schreckenghost, et al, 2002;bonasso, et al, 1997;software agents;bonasso, et al., 2002;human intervention
data values;closely tied to how accurately;speculative execution of information gathering plans
computational complexity;search algorithms;preference elicitation;task allocation;game theory;privacy issues;real world;resource allocation;game theory;mechanism design
evolving networks;behavioral patterns;interaction graphs;interaction graphs;diffusion model;link prediction;semantic information;event-based
clustering;spectral clustering;cost functions;synthetic data sets;clustering problem;evolutionary clustering;clustering results;clustering result;clustering methods;long-term;clustering problems;clustering data streams;short-term;evolutionary clustering;temporal smoothness;clustering quality
classification;anomaly detection;data sequences;hidden markov models;traffic monitoring;hidden markov models;query sequence;high-probability;times faster than
brute-force;computational cost;large number of;chi-square;upper bound;error rate
real-world datasets;information diffusion;viral marketing;simple models;classification;heuristic techniques;collective classification;markov random fields;graph-based semi-supervised learning;label propagation;objective function;network clustering;optimal set of
local structures;large datasets;event types;event sequences;sequence mining;produce high-quality;event sequences;local patterns;dynamic-programming;user activity;data description;event sequence;optimization problem;patterns discovered;real datasets
logic program
plans;event-driven;execution model;control architecture;high-level;low-level
decision-theoretic


domain-specific knowledge;multi-level;data collection;qualitative analysis;domain knowledge;physical systems;sparse data;sampling strategies;case studies;distributed data
base relations;path-consistency;qualitative spatial;temporal intervals
region based;visual field
qualitative simulation;regulatory networks;qualitative simulation
real-life
np-complete;complete classification
interval-based;temporal reasoning

resource-bounded;resource-bounded;computational complexity
conflicting information;theoretical properties;information from multiple sources;conflict resolution;inconsistent information;minimal change;computational cost;knowledge integration;intelligent agent
belief base;concurrent updates
belief change;nonmonotonic reasoning;minimal change;minimal change
basic properties;logical structure
situation calculus
situation calculus;sufficient conditions;agent communication
computational complexity;worst-case;description logics;description logic;functional dependencies;database schemas;knowledge base;data management
case study;optimization techniques;knowledge bases;classification;description logic

knowledge compilation;large number of;directed acyclic graphs
phase transitions
decision procedures;description logics
ontology languages;semantic web;web resources;description logic
conceptual graphs
description logics
description logics;description logic
knowledge sharing;human interaction;natural language processing;formal concept analysis;instances
boolean satisfiability;local search;completeness;search tree;number partitioning;discrepancy search;tree search;cost model;search problem;information gathered;depth-first search
state spaces;heuristic search

blocks world;optimization problems;problem hardness;finding optimal;number partitioning;decision problems;problem size;graph coloring
constraint satisfaction problem;search tree
high degree;power law;random graphs;real world;small world;small size;search problems
propositional satisfiability;decision procedures;decision procedure for;artificial intelligence;speed-ups
boolean satisfiability;stochastic local search
search space;graph structure;constraint satisfaction problems;algorithm performs well
constraint satisfaction
constraint programming;space complexity
propagation algorithm;data structure;worst-case;constraint propagation
mackworth and freuder, 1985;theoretical results;instances;worst case;constraint satisfaction problems;path consistency;space complexity
constraint reasoning
hybrid approach;linear programming;instances;tabu search;knapsack problem
search procedure;ai research;specialized algorithms;general-purpose;generic algorithm;linear programs;resource constraints;satisfiability testing
test cases
problem instances;constraint satisfaction;uniform distribution;local search

visualization techniques;real-world;information spaces;data values
causal structure;knowledge representation;computational models;problem solving

global information
steels, 1999;constraint satisfaction;autonomous agents;spatial reasoning
partial matching;physical environment;plans;physical objects
planning problems;planning systems;control knowledge;specific information
planning algorithm;plans;problem domains;planning algorithms;algorithm called;partially ordered;total-order;generate plans;knowledge bases
search space;context dependent;temporal logic;plans
compact representation;benchmark domains;state space;specially designed;domain independent

sheds light on;local search;state spaces;heuristic function;local search;topological properties
partial order
model checking;heuristic search
planning problems;algorithm generates;search space;plans;model checking;partial observability;deterministic domains;partially observable domains
classical planning;planning algorithm;plans;temporal properties;model checking;deterministic domains
temporal planning;graph-based;autonomous agents;muscettola et al., 1998b;programming language
planning systems;simple temporal;problem solver;plans;applications involve
planning algorithms;state space;large state spaces;probabilistic planning;partially observable;ai planning;computational complexity;complexity classes;markov decision processes;decision problems;complex applications;decision making;plans
computational complexity
np-complete problem;planning problems;normal form;ai planning
search trees;related algorithms
temporal difference;learning algorithm;evaluation function;human effort;temporal difference learning
multi-agent systems
decision trees

distributed monitoring
ad hoc;high-level;event calculus;event-based;model-based diagnosis
model-based diagnosis;hierarchical representation;mozetic, 1992
hidden markov models;belief update;modeling language;monitoring systems;constraint-based
domain knowledge;lower-level;high-level;logic programming;problem solving;computational model;constraint solving
logic programming;answer set programming
stable models;software tools;nonmonotonic reasoning;stable model semantics

search space
query answering over;graph-based;completeness
answer set programming
polynomial space;rule-based;graph theoretical;graph coloring;graph theoretical;answer sets
generic framework;knowledge bases;knowledge base
logic programming
specific information;context-specific
transition model;real-world;basis functions;linear combination;markov decision processes;algorithms require;dynamic bayesian network;mathematical framework;error bounds

algorithm performs;state space;dynamic programming;decision-theoretic;situation calculus;optimal value function;dynamic programming approach
progressive processing;adaptive control
approximation algorithm;partially observable markov decision process;grid-based;approximation algorithms
qualitative decision;von neumann;order-preserving;utility functions;decision problems;utility theory;decision theory
fuzzy logic
programming language
approximate inference;monte carlo;relational structures;equivalence classes;probability models;probability distributions;approximate inference;markov chain;knowledge base
complete information;large-scale;probability distributions;knowledge acquisition;finite domain;bit;knowledge base

nonlinear functions or high input dimensionality;rule extraction;input space;algorithm called;knowledge extraction;rule extraction;radial basis function
optimization problems;learning algorithm;local minimum;optimization problem;time-series;faster convergence;prediction quality;neural-network;constrained optimization;time-series;artificial neural networks
unsupervised clustering;information processing;lower level;sensory data;event extraction;robot learning
pattern analysis;neural network model;graph matching;long-term;prediction model;hong kong;short-term
energy function;neural networks;instance;optimization problems;local maximum;instances;computational model;neural network
neural networks;genetic algorithm;genetic algorithm;learning paradigm;neural network;generalization ability;computational cost;ensemble approach;neural network
genetic programming;nonlinear functions or high input dimensionality
design parameters;stochastic model;sensitivity analysis;decision-making;multilayer perceptron;multilayer perceptron;network structure;neural network
nonlinear functions or high input dimensionality;reinforcement learning
algorithm performs;single-agent
multi-agent systems;multi-agent systems;learning algorithm;reinforcement learning;complex tasks
search algorithm;learning task;policy search
search spaces;completeness;object identity
theoretical framework;apriori algorithm;version space;data sets
underlying structure;active learner;active learning;learning problem;active learning;bayesian network;theoretical framework;bayesian networks;fundamental problem;causal structure;learning task
clustering;nonlinear functions or high input dimensionality;relational data;classification
predictive models;web interfaces;web navigation;markov models;link structures;naïve bayes;learned model;mixture models
'orgchart';classification;text classifiers
association rules;association rule;frequent item sets;multiple relations
feature selection techniques;feature selection method;selected features;selection criterion;text classification;document set;features selected;information gain
link analysis;markov chain
probability estimates;training data;active learning;classification accuracy;learning approaches;data items;learning tasks;class membership;data sets;labeled data;multiple models;class labels;probability estimation;uncertainty sampling;learning method
nonlinear functions or high input dimensionality;phase transition
training set;instance;weighting scheme;margins;margin;classifier
knowledge bases;end users;plan generation;helping users;process models;helps users
domain experts;type information;end users;planning domain;instance;knowledge acquisition;problem solving;problem-solving
reinforcement learning algorithm;stochastic games;reinforcement learning
optimal values;optimal value function;reinforcement learning algorithm;algorithm called;rates;learning parameters
domain knowledge;reinforcement learning algorithms;policy gradient;reinforcement learning;reinforcement learning;policy gradient;state space;algorithm converges
cost-sensitive learning;nonlinear functions or high input dimensionality
text mining;partial matching;instance based;textual data;rule-based;matching methods;frequency information;documents retrieved;instance-based learning
lessons learned;planning domain;evaluation measures;decision processes
case-based reasoning;decision trees;product recommendation;decision tree;path length;case retrieval;decision trees;problem solving;average number of;special case
task decomposition;nonlinear functions or high input dimensionality;case-based reasoning
query rewriting;rewriting queries;information sources
case-based reasoning;numerical values;adaptation knowledge;case-base;base data;general-purpose
nonlinear functions or high input dimensionality;stochastic games
graphical models;probability distributions;multi-agent;real-world;decision-making;representation language;decision rule;normal) form;computational cost;multi-agent
phase transition
decision-making;computational resources;decision-making;algorithm iteratively;autonomous agents

nonlinear functions or high input dimensionality
multi-agent systems;competitive analysis;decision making

positive results;negative results;mechanism design
multi-unit;electronic commerce;automatically determined;artificial intelligence;protocol called
electronic commerce;protocol called;ai technologies;heuristic method;optimization problem
np-complete;decomposition techniques;search algorithm for;specialized algorithms
software architecture;nonlinear functions or high input dimensionality;dynamically generated
intelligent agents
case-based reasoning;target tracking
sharing information;expected utility;information sharing;answer queries;information exchange;load balancing
nonlinear functions or high input dimensionality
nonlinear functions or high input dimensionality
propositional satisfiability
theoretical analysis;strategy-proof
human interactions;nonlinear functions or high input dimensionality
nonlinear functions or high input dimensionality;interactive search
bayesian network;bayesian networks;user models
multi-criteria;agent architecture;multi-attribute
traffic management;traffic control;computational costs;qualitative model;main features
nonlinear functions or high input dimensionality
algorithm finds;shim et al., 1994;large number of;finding optimal;small size;np-hard
naïve bayesian;base line;statistics-based;tf-idf;automatically generated;language independent;nearest neighbour;expectation-maximization
text content;generation process
nonlinear functions or high input dimensionality
user-defined;information extraction from;natural language processing;information extraction from;real world applications;training corpus
relational learning;information extraction;case study;relational information;relational learning;information extraction
multi-domain;end-user;text collection;information extraction system;machine learning
nonlinear functions or high input dimensionality;hidden markov models;information extraction
nonlinear functions or high input dimensionality;hidden markov models
statistical methods;computational complexity;ittner et al., 1995;yang, 1999;statistical model;linguistic information;support vector machines;feature selection;natural language processing;text classification;nearest neighbour;information retrieval;classification task;classifier;linguistic features
high quality
domain knowledge;natural language generation;effectively learn;problem solving
real users
nonlinear functions or high input dimensionality;context-free
automatically extracting;nonlinear functions or high input dimensionality
incomplete information;obtained by combining;robot control;formal framework for;database
control systems;high-level
agent-based;simulation experiments
nonlinear functions or high input dimensionality
shape descriptors
design process
aspect model;vision systems;perceptual information;view-based;visual recognition;real world
face recognition;real-world;expected cost;object-detectors;low-level;information gain
human perception;texture analysis;vision applications;texture features;similarity measurements;visual features
matching algorithm;input image;subgraph isomorphism;conceptual graphs;image matching;conceptual graphs;natural scenes;test results
motion patterns;classification;temporal properties
hybrid approach;classification;greedy search;pre-computed;invariant features;principal component;sensor data
nonlinear functions or high input dimensionality
correlation-based;face recognition;real-time tracking;multiple objects;real-world;real-world applications;visual tracking;multiple-object tracking;network latency;spatial location;ip network
web-based;past experience;expert knowledge;high quality;hong kong;decision making
nonlinear functions or high input dimensionality
preference-based;page content;nonlinear functions or high input dimensionality
web documents;learning algorithm;human knowledge;high quality;irrelevant documents;general-purpose;excellent performance;decision-tree learning algorithm;domain-specific;input query;search engine;domain-specific search engines;search performance;web search engines
user interfaces;potential impact;cognitive science;expert systems;ai research
nonlinear functions or high input dimensionality
case-based reasoning;pattern recognition;classification;information sources;learning paradigm;problem solving;nearest neighbor
nearest neighbor;case-based reasoning;retrieval strategies;product recommendation;rule-based
language independent;domain knowledge;case-based reasoning;case-base;semantic similarity between
domain knowledge;classification;classification accuracy;domain theory;machine learning;data sets;similarity-based;partial knowledge;similarity measures
data point;ordinal data;data points;similarity measure;classification tasks;distance measures;similarity measures;inference process
global constraints
domain knowledge;simulated annealing;search algorithms;search problem;heuristic algorithms;genetic algorithms;knowledge acquisition;search algorithm;heuristic search;combinatorial problems
compact representation
worst-case;lower bound;space complexity
propagation algorithm;global constraints;wide range;specification language;np-hard
boolean satisfiability;theoretical results;instances;random sat
constraint satisfaction;decomposition methods;constraint satisfaction problem
phase transitions;phase transition;graph evolution;decision problem;instances;high probability;search algorithm;margin
soft constraint;wide range
implementation issues;higher-order;answer-set semantics;answer-set programming;higher-order;declarative framework;answer-set programming;external sources;semantic web
computational complexity;answer set;uniform equivalence;answer-set semantics;answer-set programming;answer sets
pattern databases;heuristic function;search tree;state space;pattern database
constraint satisfaction;finite-domain;constraint programming;specification language;combinatorial problems
involving multiple;min-cost;search efficiency
beam search;beam search;memory capacity;problem instances;memory consumption;benchmark domains;search problems
perfect information;completeness;game theory
knowledge learned;constraint satisfaction problems;constraint satisfaction problem
computational complexity
structural properties;bounded treewidth;bounded number of;constraint satisfaction problems
knowledge compilation;model counting;search algorithm;computational power
variable ordering;search algorithms;variable ordering;constraint satisfaction;real-world;problem hardness;constraint satisfaction problems;search heuristics
graphical models;high degree;np-hard problem;stochastic local search;stochastic local search;fundamental problem;belief networks
finding optimal;approximation methods;optimization problem
scheduling problems;search space;search procedure;complete search;lower bounds;resource constrained;kolisch and sprecher, 1996;constraint propagation
tabu search
closely related;davis-putnam;implementation details;alsinet et al., 2003; xing and zhang, 2004; shen and zhang, 2004; de givry et al., 2003;local consistency;lower bounds;upper bound;constraint satisfaction problems
greedy search;greedy approach;constraint networks;relevant information;breadth-first search;inference process
constraint satisfaction;disjunctive temporal;randomly-generated;instances;problem size;constraint satisfaction problems
heuristic function;game trees;theoretical analyses;real numbers;game tree
search problems
search space;graphical models;constraint optimization problems;real-world;belief networks;optimization problems;search spaces
clustering) algorithms;graphical models;search spaces;reasoning tasks
coarse-grained;search space;coarse-grained;constraint satisfaction problems
instances;disjunctive temporal;local search;disjunctive temporal;high quality
taking into account;answer set;stable model semantics;stable models;knowledge representation;stable model;operator;logic program;programming paradigm
random sample;belief states;game-tree search;partial information
global constraints;total number of;search heuristics
optimization problems;dynamic programming;algorithm requires;constraint networks;arbitrarily large
partial orders;instance;roney-dougal et al., 2004
belief states;database;partially observable;belief-state;instances;belief state
inference algorithms;finite domain
search space;human players;game-theoretic
detection problem;search trees;constraint satisfaction problems;detection algorithm
statistical properties;real-world networks;small-world;decision making;scale-free
search algorithm;constraint satisfaction problem;search effort
perfect-information;game trees;pruning techniques;game tree;multi-player
minimal cost;general case;decision-making
decision tree;search tree;finding optimal;compact representation;instance;bayesian networks
search methods;xu and li, 2000; 2003;phase transition;random instances;instances;domain size;csp instances
instances;local search;search algorithm for;real-world
based reasoning
controlled experiments;data obtained from;pearl, 2001
counterpart;general concept;description logic
regulatory networks;complex network;dynamical systems;powerful tools for;qualitative simulation;simulation results;model-checking
ordered information
description logics
giunchiglia et al., 2004;assumption-based;special case;bondarenko et al., 1997

simulation data;functions defined;real-valued;sensor data;model-based diagnosis;simulation results;segmentation algorithm
operator;computational properties;answer set;answer sets
knowledge representation;knowledge bases;complexity classes;query processing
computational complexity;formal framework for;domain description;directed graph;domain descriptions;knowledge base;intelligent agent
operator;computational complexity;merging operators
answer sets;sufficient conditions for;answer set programming
constraint reasoning;situation calculus;constraint based;situation calculus;plans
domain description;knowledge engineering;design principles;domain descriptions
decision procedure for;goal-directed
pre-defined
belief change;operator;belief update
combined complexity;description logics;data complexity
logic programming;knowledge bases;knowledge base
intelligent agents
belief base
situation calculus;action theories
knowledge-based;fagin et al., 1995
counterpart;model-theoretic;logic program;answer sets
situation calculus;plans
logic programming;answer set semantics
normal form;context-dependent;knowledge base;dynamic systems
temporal information;knowledge engineering;cognitive model;spatial relations;problem-solving
computational complexity;nonmonotonic reasoning;application domain;np-complete;constraint language;medical diagnosis
gärdenfors and makinson, 1988;pagnucco et al., 1994; pagnucco, 1996;belief change;ourston and mooney, 1994;machine learning;belief change;muggleton, 1987; 1992;machine learning techniques;muggleton, 1995;machine learning approaches;nonmonotonic reasoning
semantic web;semantic web
belief base;knowledge bases;information source
information flow;multi-context systems
classification;formal concept analysis
semantic mappings;fixed-point;knowledge representation and reasoning;operator
mapping language;ontology languages
belief change;1998;situation calculus;mccarthy and hayes, 1969; levesque et al., 1998;2001;gärdenfors, 1988; spohn, 1988
belief states;partial-knowledge;semantic web;autonomous agents;databases;belief state
qualitative model;training data;learning algorithm
problem remains;implication problem;description logics;object-oriented;description logic;functional dependencies
description logic;description logic
moving objects;yaman et al., 2004;numerous applications;moving object
closed world;common sense;yaman et al., 2004;moving objects;np-complete
computational properties;logic program
decision boundary;decision trees;classifier;distance-based;decision tree
regression trees;statistical models;bayesian approach;classification;metropolis-hastings
data mining methods;mined patterns;case base;case study;process-model;knowledge-intensive
answer queries;plan recognition;decision-tree;machine-learning
text mining;semantic relations between;molecular biology;natural language processing techniques;unsupervised learning;named-entities
forward selection;selection algorithm;algorithm iteratively;classification results;feature selection;feature selection methods;game theory;feature selection
learning scheme;learning algorithm;designed specifically for;base learners;maximum-entropy
database schema;probabilistic models;database;statistical relational learning;database applications;relational databases;statistical relational learning;relational table
training set;web page;graph connectivity;ranking function;graph nodes;web pages;model parameters;score distribution;node labels;markov chain;learning task;training algorithm
building block;unknown environment;convergence rate;reinforcement learning;reinforcement learning
logical reasoning;learned models;cost model;hidden states;de raedt and dehaspe, 1997;minimum-cost;transition model;learning method
hybrid model;uci data sets;naive bayes
numerical experiments;label-propagation;classification;binary classification problems;instances;binary classifiers;class labels;classifier;synthetic data
base classifiers;classification accuracy;data sets;ensemble classification;base classifier;classifier
binary classification;computational properties;false positives;false positive;decision-theoretic;binary classifiers;classification error
local search algorithms;subset selection;hill climbing;subset selection;search algorithms;search algorithm
model called;dynamical systems
training examples;instance;labeled instances;uci datasets;real-world;learning problems;decision rule;formal description;session-based;unlabeled instances;supervised learning;instances
data point;ensemble learning;classification models;classification;ensemble methods;heterogeneous data;model generation;multiple times;image classification;ensemble approach;classification problems;benchmark datasets
domain knowledge;statistical methods;generalization error;learning agents;state abstraction;prior knowledge;state abstraction;state variables
graphical models
multi-modal;hybrid approach;data collected by;ubiquitous computing;activity recognition;hidden markov models;human activities;informative features;automatically identifying
data collected by;efficient inference;temporal information;activity recognition;markov networks;information extracted from;features including;markov networks;databases;location-based;location data;global constraints
reinforcement learning;prior knowledge
social network analysis;topic distributions;social networks;latent dirichlet allocation
domain knowledge;parameter values;neural networks;expert knowledge;fuzzy rules;radial basis function;fuzzy logic;natural language
manifold learning;larger datasets;competing methods;high-dimensional spaces;high dimensional spaces;information propagation
maximum likelihood estimation;neural networks;generalization error;generalization performance;computational costs;empirical bayes;empirical bayes;posterior distribution;generalization error
phase transitions;statistical properties;inductive learning;years ago;induction algorithms;phase transition;hypothesis space;search space;inductive logic programming;risk minimization;learning problems;phase transition;deterministic finite automata;search heuristics
multi-agent;bounded memory;learning algorithms;wide range;multiagent systems
auc;induction algorithms;rule set;classification rules;rule learning;rule sets;instance;algorithm called
discriminant analysis;nearest neighbor classification;nearest neighbor;feature extraction method;feature extraction technique;linear discriminant analysis;face recognition;statistical pattern recognition;face databases;covariance matrix;dimensional data;small sample size
state representation;reinforcement-learning;reinforcement learning;td networks
training set;human subjects;relevant features;active learning;text categorization;predictive) features;feature selection;classifier performance;feature relevance
probabilistic semantics;parameter learning;generation process;logic-based;modeling language;constraint-based
gradient-based;context-sensitive;recurrent neural networks;learning algorithms;quadratic programming;linear dynamical systems;linear regression;problem domains;hidden state;local minima;neural network;short-term
multi-modal;real-world datasets;local optima;multi-objective;optimization approach;temporal patterns;optimization problem;spatio-temporal
td) networks;td networks;world knowledge;common-sense knowledge;temporal-difference;partially observable markov decision processes;sutton and tanner, 2005
information extracted from;replication;instance-based learning
ensemble learning;ensemble feature selection;data sets;ensemble feature selection;feature subsets;higher accuracy;base classifier;machine learning and data mining
clustering;local search techniques;high rate;missing values;artificial data sets
domain knowledge;expert knowledge;robot-vision;image domain
algorithms for computing;map estimation;temporal logic;markov models
discovered patterns;domain knowledge;distance metrics;association patterns;features extracted from;scientific data;efficient algorithms to;scientific datasets;real datasets
effectively exploit;nearest neighbor;distance metrics;training examples;data mining applications;semi-supervised learning algorithms;machine learning;regression algorithm;unlabeled examples;semi-supervised;unlabeled data;semi-supervised;semi-supervised classification
search engines;multi-agent systems;ranking systems
multi-agent systems;general setting;uniformly distributed
optimization problems;preference elicitation;optimization problem;decision problems;model parameters;constraint-based;objective function
compact representation;computational complexity;nonmonotonic reasoning
utility functions
decision problems
local search algorithms;computational complexity;local search;multi-agent;constraint optimization problems;real life;high probability
learning theory;learning algorithm;learning framework;learning models
clustering;local optimization;clustering coefficient;simulation model

regression problem;sealed-bid;learning performance;real-valued
real numbers;belief states
low frequency;active learning;ensemble members;active learning;statistical model;manually annotated;uncertainty sampling;ensemble method
large amounts of;text analysis;finite-state;feature generation;machine learning;annotated corpus;information derived from;temporal reasoning;small size
semantic content;natural language generation;search problem
training examples;data set;parallel corpora;estimation algorithms;word sense disambiguation;word sense disambiguation
semantic annotation;learning method;instance;probability distribution over;context-free;semi-structured documents;xml schema;joint probability;classifier;xml schema

text classification tasks;database-backed;web site;entity recognition;natural language processing;semi-automatically;natural language
support vector machines;mutual information;sample size;training examples;extracted information;probabilistic model;extraction rules;information extraction;extracting knowledge from;logistic regression;fundamental problem;information extraction
information graphics;probabilistic framework;bayesian network
world knowledge;text categorization;feature generation;knowledge-based;common-sense knowledge;domain-specific;natural language processing;machine learning algorithms;web crawling;word sense disambiguation
learning strategies;learning strategies;relational learning;problem-solving;question answering;natural language
class information;tree kernel;kernel-based;relation extraction;semantic information;natural language
kingsbury et al., 2002;test set;classification;classification accuracy;error reduction;additional features
temporal relations;information extraction;key features;user study;scoring methods;natural language
highly relevant;search process
automatic evaluation;predictive power;human judgments;modeling approach;automatic evaluation;fully-automatic
semantic annotation;semantic annotation;free text;reference data;reference set;information extraction;semantic web;extraction methods;vast amounts of
temporal context;question answering;natural language text;temporal context
instances;machine learning
extracting knowledge from
semantic role labeling;joint inference
output variables;structured output;local classifiers;theoretical justification
text categorization;classification accuracy;vector space model;statistical estimation;text categorization;information retrieval;vector space model;information retrieval;machine learning approaches;feature selection
singular value decomposition;semantic relations between;high degree of;semantic similarity;word pairs;cognitive science;semantic relations;vector space model
search algorithms;solution space;statistical machine translation;statistical machine translation;local search;algorithms produce;search problems;np-hard

agent model
semantic role labeling;tree-bank;hand-crafted;machine-learning methods to;fully automatic;word segmentation
automatically extracting;frequency information;hierarchical structure;natural language processing;similarity measure
artificial agents
decision-theoretic model;social behavior;agent-based;decision-theoretic;social interaction
learning mechanism;high level
sensor measurements
filtering algorithms;scheduling problems;high quality;beck and wilson, 2004;constraint-based;scheduling problem;search technique
classical planning;plans;planning problem;graph based;benchmark domains;soft constraints
real-time dynamic programming;shortest path;decision problems;transition probabilities
planning problems;search space;forward search;problem domain
graph-based;temporal reasoning;constraint-based
path planning;higher-level;data structure
search algorithm;theoretical properties
state space;optimal planning;state variables;search algorithm;resource constraints;stochastic domains
data values;planning domain;theoretical framework;search space;web services
planning problems;plans;probabilistic planning;np-complete;belief space;partially observable markov decision processes;decision problems;wide range;partial observability
planning problems;continuous variables;random variables;probabilistic reasoning;probabilistic representation;resource usage
motion patterns;dynamically changing;computation model
knowledge bases;partial knowledge;counterpart;auxiliary;knowledge base;inference process
single agent;finite-state
computational complexity;decision-theoretic model;partially observable markov decision process;alzheimer's disease;decision-theoretic
parameter values;bayesian network;markov network;sensitivity analysis;markov networks;bayesian networks;sensitivity analysis;conditional probabilities
local structure;bayesian networks
graph-theoretical;decision problem;bayesian networks;set valued
graphical models;poole, 2003;exact inference;inference algorithm;perform inference;probabilistic inference;inference algorithms
finding an optimal;computationally feasible;objective function;integer programming
application domain;partially observable markov decision processes;decision problem;relevant features;decision making
real-world datasets;graphical models;real-world;optimal algorithms;hidden markov models;sensor network;decision making
structured domains;continuous variables;constraint satisfaction;linear programming;markov decision processes;space complexity;continuous state
probabilistic models;formal language;probability models;probability distribution over;inference algorithms
process model;continuous variables;particle filtering;online learning;dynamic systems;particle filter
low-cost;location estimation;labeled training data;vector-space model;labeled data;canonical correlation analysis;physical location;location-estimation;models trained on
markov decision processes;markov decision processes;finding an optimal;expected utility;decision rule
probabilistic semantics;conditional probability;probabilistic reasoning
worst-case;inference algorithms;context-specific;probabilistic inference
bayesian network;random variables;large number of;probability distribution over;inference algorithm;probabilistic reasoning;probability distribution;hierarchical structure;bayesian networks;probabilistic reasoning;domain size
statistical methods;visual tracking;video sequence;state estimation;particle filter
domain knowledge;shape information;sketch recognition;hierarchical approach;specific context;multi-domain;hand-drawn;recognition accuracy
cognitive states;recognition rates;human-robot
search results;past queries;web search;collaborative web search;collaborative web search;large-scale;similar queries;user evaluation
intelligent tutoring system;bayesian networks;modeling approach
virtual world;false positives;version spaces;partially observable;reinforcement learning;action models;deterministic domains;theoretical guarantees;decision making
human movement;bayesian network;prior information
classification decisions;classification;test images;labeled data;input images;data collected;image classification
image-based;scene recognition;view-based
multiple hypotheses;visual attention;human action;computational resources;goal-directed;action recognition;human actions
information content;context awareness
efficient inference;monte carlo;spatial relationships;markov networks;individual objects;laser range;sensor data;spatial constraints;line segments;markov chain
sensor data;sensor-data;perceptual information;gather information
classification;landmark points;shape descriptors;retrieval performance;generative model;ad hoc
graph-based;distributed systems
structural information;information processing;adaptive approach
artificial intelligence
mining algorithm;sequential data;automatically constructing
incomplete knowledge;scheduling problem;minimization problem;partially ordered;np-hard
search technique
np-complete problem;constraint reasoning;problem instances;phase transition;instances
problem instances;finite model;exhaustive search;general-purpose;search space
constraint satisfaction problem;random walk;real life;search strategies;scheduling problem;tabu search
greedy search;constraint satisfaction;real-world;constraint programming;problem-solving;constraint satisfaction problems
data structures;search space;constraint networks;instances
decomposition approach;cook and seymour, 2003
decomposition methods;tree decomposition;memory consumption
search methods;local search;wide range
description language;analysis tasks;automatic generation of
ontology-based;user modeling
search results;online users
cognitive model;typically involves;problem solving;problem-solving;computational model;solving complex
planning systems;information contained in
generic framework
instances
knowledge acquisition
belief change;state transition;belief update;unified framework;knowledge base;decision making
case-based reasoning;multiple models;hierarchical representation
knowledge based;knowledge-based
image retrieval;information overload;context information;addressing this problem
temporal reasoning
closed world;instances;structural properties
selection criterion;constraint satisfaction;computational models;retrieval method
browsing behavior;web pages;user study;relevant pages;current web
search results;collaborative web search;collaborative web search;search behaviour;collaborative web search
image retrieval;web search;web search;text-based;word senses
network security;induction algorithm;case study;complex event;learning algorithm;user profiling;complex events;hidden markov model;long sequences
mathematical model of;reinforcement learning;visual cues
labeled data;fmri data;neural networks;classification;machine learning techniques
incremental algorithm;data points;high dimensional;vector machine;sliding window;real dataset;data stream;data streams;multi-class;classifier
web searching;domain-specific;primary goal;information retrieval tasks
data clustering;code length;data set;maximum likelihood;maximum likelihood;model selection
sentiment analysis;classification results;sentiment analysis
learned knowledge;knowledge transfer
multiple classes;low-dimensional space;classification;feature extraction;class membership;data sets;dynamic environment
classification models;network intrusion;classification decisions;loss function;cost-sensitive learning;classification tasks;large number of;input space;instances;addressing this problem;problems require;labeling process;detection task;total cost
user models;domain model
structured domains
learning scheme;acoustic features;machine learning
operator;inductive logic programming;inductive-inference
web resources;search queries;web sites;web search;web search engines
precision/recall;unlabelled data;web pages;closely related;web information extraction
question classification;regular expressions;machine learning;generated automatically;machine learning techniques;structural information;question classification
domain knowledge;state variables
lower computational cost;global model;data analysis;gaussian mixture model;synthetic datasets;highly accurate;data privacy;model parameters;global models;distributed data;data generated from
multi-agent systems;decision-theoretic
np-complete problem;special case;strategy-proof;optimization problem;network flow
service quality;common knowledge;high-quality;fast convergence;formal framework
operator;multi-agent systems;temporal logic
mobile computing;mobile devices;bandwidth consumption
cost-benefit analysis;recommendation algorithms;collaborative filtering;recommender systems;prediction accuracy
communication costs;agent communication;rates
trust model
web search;mobile search;related queries;mobile devices;effective search;mobile internet
natural language understanding;fundamental problem;large number of;knowledge representation;natural language

compares favorably with;generative probabilistic;text categorization;naïve bayes;unsupervised fashion

maximum entropy;rank order;sentence extraction;naïve bayes;classifier
fault-tolerant;fault-tolerant;context-based
minimum description length;automatically learning;natural language processing
artificial agents
dynamic programming algorithm;parallel corpus;parallel corpora
text-mining;common knowledge

machine translation;natural language generation
decision making
natural language generation;readability
automated reasoning;human interaction;plans
takes into account;planning algorithm;planning problem;multi-valued
operator;control mechanism;task allocation
multi-agent;partial knowledge;assumption-based;planning problem
planning algorithm;open-world
temporal planning;planning problems;disjunctive temporal;simple temporal


path-planning;path-planning;path planning;lighting conditions
robot learning;competence;state variables
state space;general purpose;efficient search;information theory;search process;moving objects;computational cost;accurate tracking;video streams;heuristic search;high dimensional space;search problems
human perception;line segments;genetic algorithm;human visual system
illumination conditions;classifier;large database;svm-based
state representation;littman et al., 2002; singh et al., 2004;dynamical systems
point based;approximation algorithms;belief states;point-based;worst-case;instance;pineau et al., 2003
computational efficiency;random variables;jin, 2005;fitness function;linear algebra;probability distribution;valiant, 1984;boolean functions;machine learning;instance;evolutionary computation;general setting;target function;learning framework
network structure;real-world;local search;generation algorithm
inductive logic programming;multiclass problems;optimization problem
graphical models;multi-layer;latent variables;learning algorithms;graphical model;classification performance;generative model;handwritten digits
clickstream data;pattern discovery;data collection;web mining;pre-processing
instances;structural constraints;object-based;probabilistic xml
human effort;similarity measurement;probabilistic database;user feedback;semi-automatically;data integration
uncertain data;completeness;relational operations
random variables;probabilistic database;data complexity;probabilistic databases;conjunctive queries;aggregation function
web databases;query rewriting;query patterns;optimization framework;data dependencies;database;network resources;missing attribute values;selectivity estimates;reformulated queries;user query;naïve bayes;functional dependencies;query processing;databases;high recall;query processing techniques;web environment;high precision;database query processing
incomplete information;census data;probabilistic information;efficient representation;relational algebra
duplicate records;clustering algorithms;accurately reflect;string data;data cleaning;real-world;major source of;valuable information;query-answering;probabilistic databases;approximate join;databases;query results;probabilistic database
approximation algorithms;query evaluation;boolean queries;twig patterns;query evaluation;data complexity;probabilistic xml
incremental update;query evaluation;nearest-neighbor query;probability density functions;sensor monitoring;database;uncertain data;upper bounds;data items;cpu cost;query answers;nearest-neighbor queries;location-based services;biological databases;moving object;probability values;query point
compact representation;query evaluation;graphical models;inference algorithm;uncertain data;probabilistic database;graphical model;modeling technique;information extraction;probabilistic databases;machine learning;noisy data;sensor data;probabilistic inference;query processing algorithms;real-world applications;numerous applications;inference algorithms;query processing
prediction accuracy;collaborative filtering;recommender systems;large number of;implicit feedback;cost function;user similarity
computational efficiency;large datasets;sampling algorithm;prior knowledge;genomic data;relevant information;patterns discovered
statistical measures;discovery algorithm;post-processing
aggregated data;multidimensional databases;sequential patterns;decision makers;multidimensional databases;mining sequential patterns;transactional data
intrusion detection;explicitly model;classification;frequent patterns;accuracy compared to;explicitly models;sequence mining;pattern mining;sequence data;higher-order;sequential data;complex patterns;web usage;protein sequence;hidden markov model;open-source
video game;sentence-level
annotated corpus;sentiment analysis;topic-specific;extraction techniques;classifier
review text;classification;classification accuracy;sentiment analysis;feature selection;feature sets
data sources;wide range;social web;question answering systems
high-precision;automatically creating;user-generated content;text classification;opinion mining;high precision;problems arising
domain-specific;contextual-information;classifier;sentiment analysis;domain-specific;feature generation;automatically extract;sentiment classification
domain-independent;classification;weakly supervised;sentiment analysis;training data;sentiment classification;machine learning techniques;weakly-supervised;weakly-supervised
linguistic information;user-generated content;high precision;user-generated
data set;user preferences;opinion mining;collaborative filtering;free-text
segmentation methods
context information;sentence level;dirichlet prior;language models;smoothing method;sentence retrieval;local context
sentiment analysis;instance;domain specific;target object;sentiment analysis;multiple aspects
data mining models;open source;open source;data mining system
very large datasets;data warehouses;build models;data mining systems;databases
35;data mining
predictive model;data mining standards;markup language;data mining
data manipulation;visualization methods
emerging trends;predictive models;data miners;predictive model;business applications;data mining;statistical models;markup language;predictive analytics
open source;data mining projects;database;data cleaning;real-world;record linkage;real-world entities;user experiences;data mining;graphical user interface;record linkage;record linkage
text mining;database systems;database;knowledge management;knowledge management;data mining;early stage;data mining research;semantic web;databases;digital libraries;information retrieval;data management
correlation clustering;correlation clustering
mining closed;frequent patterns;tree mining;closed sets;ensemble methods;data stream;link-based;association rules;clustering;performance guarantees;data stream model;data streams;data mining algorithms;high speed;black-box;closed patterns;sliding window;data stream mining;decision trees;evolving data streams;frequent closed;data arrive;mining algorithms;closed frequent;classification methods;counterpart;window based;naíve bayes
local search algorithms;local search;naïve;brute-force;sparse graphs;brute-force search;brute force search
intelligent tutoring systems;problem solving;intelligent tutoring systems
collecting data;data cleaning;machine learning;network data;statistical models;optimal policies
human intervention
ai planning;human users;plans;activity recognition;lower level;sensor data;sensor network;statistical inference;high-level;activity recognition;low-level
game-theoretic
evaluation functions;decision making;reasoning patterns
planning algorithm;plans;monte-carlo;objective functions;highly dynamic;multiple agents;numeric attributes
margins;sponsored search;search technique
computational complexity;total number of;np-complete;candidate pairs;voting rules;special case;scoring rules
theoretical foundation;arbitrary-length

single-agent;game-theoretic
social-welfare;solution concept;explicitly model;social welfare;coalition structure
computational complexity;weighted voting
logic programming;domain knowledge;event calculus
voting rule;communication complexity
control problem;computationally hard
social choice;maximum likelihood estimation;scoring functions;multiple agents;maximum likelihood;noise model
local search algorithms;small sample;loss function;search tree;learning algorithms;local neighborhood;compact representation;lower bounds;undirected graphical
preference aggregation;preference aggregation
1992;unified framework
multi-unit;instances;computationally intractable
real-world;stochastic games;5;texas hold'em;algorithms for computing

solution concept;game-theoretic
multi-step;graph nodes;action space;integer programming;multi-step;multi-sensor;np-hard
real world scenarios;physical environment;resource consumption;multi agent
execution environment;scheduling problems;multi-agent;probabilistic reasoning;trade-offs
decision-theoretic;constraint optimization problems;real-world domains;sensor networks;sensor network;simulation results
computational efficiency;plans;real-world;real-life;open world;computational methods

computational complexity;test problems;state-space;multi-agent;formal models;worst-case;approximation algorithm;algorithm produces;multiple agents;approximation ratio;optimal policies;np-hard
instances;kernel function;quadratic program;kernel method
utility functions;highly nonlinear;constraint-based
approximately optimal;strategy proof;classification;decision-theoretic;expected error;meir et al., 2008;classifier
knowledge base;static analysis
large numbers of;mechanism design
lower bounds;lower bound;resource allocation
approximation algorithms;specifically designed to
strategy-proof;game-theoretic;mechanism design
coalition structure;partition function;multi-agent systems;coalition structure generation
optimization problems;cognitive model
simulation experiments;autonomous agents;agent systems;yu and singh, 2000;distributed systems;negative feedback
probability distribution over;similarity metric;large number of;texas hold'em;action sets
real-world;computational complexity;autonomous agents;multiple tasks
service providers;finding optimal;complex tasks;service-oriented;heuristic algorithm;software agents;application scenarios;execution times
greedy strategy;path planning;pruning techniques;gaussian process;application domain;squared error;information gain
edit distance;instance;highly dynamic;organizational structure;problem solving
game theory
modeling framework;event data;models built;model generation;learn models;rates;generation process;development process;agent-based;model building
phase transition;computationally hard;worst-case;voting rules;theoretical results;highly correlated;np-hard
web service;real-world;hidden markov models
social choice;computationally intractable;minimum number of;voting rules;local consistency;scoring rules;xia and conitzer, 2008
special case;voting rules
np-complete;computational complexity;social choice;voting rules
search algorithms;relative error;error bounds;np-hard
multi-agent;local information;multiagent learning;resource allocation;resource allocation

distributed algorithms;task allocation;task-allocation;multiple agents
problem instances;contract algorithms;problem instance;problem solving
computational properties;phase transition;randomly generated;real-world;random sat;instances;random instances;sat instances;probability distributions
optimization problems;problem instances;optimization problem
learning scheme;learning scheme;learning algorithms;industrial applications
optimization algorithms;stochastic optimization
propagation algorithm;lower bounds;instance;lower bounds;normal form;global constraints
global constraints
algorithms for computing;bound consistency
pattern databases;search algorithms;completeness;pre-computed;state-space;problem size
remote sensing;theoretical results;special case;optimal policies;sensing actions;total cost
growing number of;parallel algorithms;massively parallel;small scale;computing power;constraint solving
shared-memory;state space;heuristic search;temporal logic;multi-core
state space;monte-carlo search;monte-carlo search
np-complete;np-hard;instance;euclidean space;constraint network
search space;search strategy;search strategies;search tree
domain-specific knowledge;monte carlo;search techniques;risk management;monte carlo tree search;decision making
graph structure;duplicate detection;combinatorial optimization problem;depth-first search;memory requirement;depth-first search;combinatorial problems
local search algorithms;local search;naïve;brute-force;sparse graphs;brute-force search;brute force search
game trees;structural properties;game tree;performance gains;branching factor;search algorithm;search heuristics;np-hard
clause learning;processing units
search space
variable ordering;pruning algorithm;instance;times faster than;inference rules
local search;computationally hard;sat instances;stochastic local search;satisfiability problem;manual effort
constraint optimization problems;tree-width;search space
constraint optimization problems;branching factor;search space;performance gains;clustering algorithm
number partitioning;linear-space;running times
hybrid approach;local search;communication overhead;multi-core;search strategy;shared memory;instances;information exchange;combinatorial problems
search space;inference rules;constraint propagation;constraint satisfaction problems
lower bound;constraint satisfaction;graph-based;inverse consistency;constraint optimization problems;global constraints;soft constraints;constraint satisfaction problems
np-complete;selected features;application domains;global constraint;problem domains;search effort;pruning rules
encoding scheme;completeness;constraint networks;qualitative spatial;instances;fundamental problem;temporal reasoning
global constraints;constraint programming;specific algorithms;problem solving;global constraint
perfect information;hill climbing;moving target;computational complexity
tree based;upper bounds;problem called
structural properties
tree-decomposition;instance;graphical models;tree decomposition
state space;shortest-path;state spaces;search problem
shortest paths;search algorithms;moving target;search problem;incremental version;search problems
perform poorly;search performance;constraint satisfaction problems
based reasoning;qualitative spatial;temporal information;temporal reasoning;performance gain;csp instances;instances;sat instances;constraint-based;qualitative reasoning;constraint propagation
worst-case;heuristic search
search algorithms;depth-first search;search strategies
multi-player;search algorithm;search strategies
bayesian framework;software systems;bayesian approach;probability computation
resource-bounded;coalition logic;temporal logic
preference-based;knowledge bases;takes into account
satisfiability problem
databases;obtained by combining;reasoning tasks;tuple-generating dependencies
computational properties;decision problems;decision processes;np-complete;computational properties
argumentation-based;search process
low-complexity
closely related;description logics
classification
complex queries;regular expressions;regular path queries;upper bounds;description logics;application domains;regular path queries
reduced model;multi-agent systems;false positives;theoretical results;counterpart;model checking
answer questions;ontology language
expected cost;model-based diagnosis
inductive learning;knowledge bases;description logics;normal forms;description logics;information retrieval
completeness;argumentation based;unified framework
logic program;answer set semantics;performance gains;structural information;external sources;knowledge base;knowledge bases
description logic;query answering;computational complexity;description logics;query answering
computational complexity;answer set programming;answer set;reasoning tasks;computational costs;logic programming;data structures
knowledge compilation;subbarayan et al., 2007;target language;knowledge compilation;data structures
approximation algorithms;greedy approach;low-cost;information entropy;large number of;model-based diagnosis
efficiently computing;model-based diagnosis;np-hard
interesting items;knowledge-based;problem solving
stable models;logic program
logic programming;logic programming;real-world applications
hypothesis generation;scientific discovery;multiple hypotheses;binary decision diagrams;systems biology;em algorithm
bounded treewidth;dynamic programming;answer-set;answer-set programming;algorithm relies on;answer sets
answer set programming;answer set;reasoning tasks;stable model semantics;stable model;sat-based;event calculus
instances;conjunctive queries;description logic;large-scale
automatically extracted from;extraction algorithm;large-scale;instances;logic-based;instance;formal framework for
situation calculus;closely related;action theories;knowledge base
preference relations;separability;preference relation;multi-attribute
spatial information;spatial objects;spatial reasoning;qualitative spatial;np-complete
action theories;situation calculus;knowledge bases
coalition logic;multi-agent systems;belnap et al., 2001; horty, 2001
meta-level;object level
approximation algorithms;spatio-temporal;user preferences;real-world
context-dependent;deterministic domains;event calculus
computational complexity;closely related;description logics;description logics;fine-grained;operator;logical properties

situation calculus;high-level;action theories
human intervention;answer set programming
upper bounds;probabilistic logic;description logics;relational structures;upper bound
query answering;query rewriting;database technology;description logic;database
logic programming;incomplete information;answer set;balduccini and gelfond, 2003;knowledge base
constraints imposed by;web service composition
finite structures;secondary structure;search problems
conjunctive queries;description logics;description logic
instance;completeness
efficient inference
temporal information;case study;test instances;temporal reasoning;qualitative spatial
exponential family;data sets;exponential family;discriminative models;semi-supervised learning
function approximation;active learning;design method;active learning methods;linear regression;reinforcement learning
randomly generated;computational complexity;relational data;instance
clustering;consensus clustering;cluster ensembles;ensemble members;diverse set of;cluster ensembles;large number of;ensemble selection;data set;clustering performance;performance gain;margin;benchmark data sets
associative memory;high-resolution;incoming data;long-term;high-level;high-resolution
learning algorithm;finite-state
nonnegative matrix factorization;human brain;high-dimensional;data points;high dimensional;compact representation;databases;information processing;feature values;matrix factorization;matrix factorization;geometric structure;low dimensional;data manifold
semi-supervised learning;semi-supervised;learning task;selecting informative
statistical method;optimal combination of;small sample sizes;data items
inverse reinforcement learning;reward function;partially observable domains;markov decision process;partially observable
clustering;end users;dimension reduction;knowledge driven;linear projection;objective functions;general-purpose;domain expertise;generalized eigenvalue problem;desirable properties;dimension reduction;high dimensional data sets
search techniques;search algorithms;algorithms rely on;search technique;real-valued
nonnegative matrix factorization;nonnegative matrix factorization;local learning;data matrix;geometric structure;clustering methods;discriminative information;local learning;machine learning and data mining;benchmark data sets;clustering
pre-defined;face image;user preferences;real-world;vector machine;selection problem;problem domain;learning problem;learning approaches
domain knowledge;structured documents;information retrieval methods;general-purpose;margin based;performance guarantees;prior art;margin;high dimensional feature space;training algorithm
graph embedding;graph structure;label information;data points;data manifold;information processing;real life data sets;graph based;additional constraints;graph embedding;dimensionality reduction
minimum description length;optimal parameters;word segmentation
high-dimensional;general case;data analysis;input data;classification;classification problems;linear dimensionality reduction;loss functions;multi-label classification;data preprocessing;dimensionality reduction;class labels;multi-label;multi-class;learning framework;benchmark data sets;dimensionality reduction;classification algorithms
classification;classification accuracy;real-world;semi-supervised classification;learning algorithm;semi-supervised learning algorithms;semi-supervised classification;closed-form solution;long-term;classifier;temporal smoothness;short-term
regression models;labeled graphs;complete set of;subgraph features;extracting features from
mining task;database;pattern mining;inductive logic programming;statistical relational learning;provide evidence;data mining;biological network;correlated patterns
domain specific;fundamental problem;supervision;information retrieval;domain-specific;natural language processing;rank aggregation;multiple domains;learning framework
sample data;learning performance;reinforcement learning;real-valued
raina et al., 2007;unlabeled data;loss function;exponential family;performs poorly;data types;learning algorithm;higher-level;optimization problem;learning performance;generalized linear models;exponential family;text classification;noise model;learning task
multi-modal;text-based image retrieval;unified framework;image annotation;similarity estimation;query expansion;content-based image retrieval
matrix factorization;lower-dimensional;web pages;real data sets;latent space;learning algorithm;matrix factorization;content information;data analysis;link) structure;latent semantic indexing;principled framework
ensemble learning;object recognition;object category;video sequence;classification accuracy;image-set;object recognition;classification performance;generalization ability;recognition tasks;real-life data sets;classification task
inference method;recognition accuracy;social interactions;probabilistic models;activity recognition;random fields;hidden markov models;loopy belief propagation;social networks;probabilistic models;conditional random fields;classification algorithm
kernel-based;classification;kernel matrix;classification performance;gradient descent;kernel methods;benchmark data sets
kernel learning;spectral clustering;regularization;kernel machines;labeled instances;linear programming;kernel matrix;data point;graph-theoretic;classification tasks;graph laplacian;unlabeled instances;semi-supervised classification;classification error;classifier
partition function;large numbers of;markov logic networks;graphical models;random variables;large-scale;large-margin;closely related;vector machine;markov random field;learning tasks;large margin;sample complexity;large margin;variational approximation;structural svms;learning algorithm;classification problems;artificial neural networks;conditional random fields
training data;target task;social interactions;target domain;transfer learning;increasing number of;transfer learning;existing knowledge;relational domains
training images;training data;classification;learning algorithm;semi-supervised learning;training classifiers;scene recognition;supervision;image data;search engines;rough sets;web data;databases;unified framework;semi-supervised;probabilistic model;web images
state representation;continuous domains;markov decision;process planning;reinforcement learning;dynamic bayesian network
clustering;clustering;regularization;clustering methods;spectral clustering;clustering method;cluster assignment;low dimensional;real-world data sets;dimensional data
cross-domain;domain adaptation;training data;machine learning methods to;target domain;component analysis;data distributions;domain adaptation;regression models;feature extraction methods;feature representation;learning problem;text classification;real-world applications;source domain;reproducing kernel hilbert space;learning method
bayesian methods;gaussian process regression;support vector regression;classification;regression problems;semi-supervised learning;closely related;learning tasks;maximum margin clustering;cross-validation;classification problem;gaussian processes;real-world data sets;semi-supervised;semi-supervised classification
corpus based;sentiment words;large number of;extraction rules;sentiment analysis;application domains;product features;dependency trees
reasoning capabilities;learning strategies
computational geometry;algorithm performs;classification;large-scale;streaming model;accuracies;tsang et al., 2005;streaming data;instance;weight vector
metric learning methods;data points;linear transformations;linear transformation;pairwise similarity;pairwise constraints;topological structure;real-world data sets;semi-supervised clustering;metric learning;distance metric;semi-supervised;machine learning algorithms;semi-supervised;learning method;kernel-based
reinforcement learning algorithms;nearest neighbor;high dimensional;linear dimensionality reduction;accurate predictions;state spaces
real data sets;multi-dimensional;canonical correlation analysis;feature extraction;regularization;equivalence relationship;canonical correlation analysis
discriminative training;latent variable model;latent variables;classification;classification accuracy;probabilistic models;convergence properties;traditional models;perceptron algorithm;learning task
skewed data;data analysis;1978;counting algorithm;data set;instance;power law;space usage;relative error;natural language
search methods;predictive features;partially observable;future events;conditional probabilities;control problem
benchmark data;multiple labels;kernel-based;classification;microarray data;unified framework;multi-label;multiple kernel learning;classifier
multi-dimensional;single dimensional;time series;general case;multi dimensional;synthetic data
multi-class problems;multi-class;classification;binary classification problems;convex formulation;large margin;high accuracy;class labels;multi-class;classifier;feature weights
local geometry;lower dimensional space;data instances;information retrieval;knowledge transfer;semi-supervised;machine learning and data mining
clustering problem;clustering results;weighted) sum of;clustering solution;clustering result;data set;clustering aggregation;unified framework;clustering aggregation;real world data sets
selection problem;preference learning;gaussian process;partially ordered;bayesian framework;preference learning;general problem;semi-supervised
text mining;similarity information;machine learning problems;target domain;learning framework;labeled data;similarity matrix;graph based;knowledge transfer;labeled data from;unlabeled data;label propagation;transfer learning;real world
real data sets;nearest neighbor;classification;classification;time series;time series;data sets;classification method;classifier
labeled and unlabeled data;labeled samples;unlabeled data;relevant features;classification;feature selection method;semi-supervised feature selection;optimization method;convergence rate;probability distribution;optimization problem;benchmark data sets;unlabeled examples;labeled examples;margin;semi-supervised;manifold regularization
process model;machine learning problems;entity types;real-world datasets;multi-relational;gaussian processes;prediction performance;relational learning;process models
model parameters;approximation techniques;transfer learning;level features;information retrieval;hierarchical bayesian model;empirical bayes;limited number of;transfer learning;additional information
temporal patterns;conditional random field;synthetic data;spatio-temporal;real-world;spatio-temporal;critical task;real data;event detection;real-world applications;event detection;large-scale;sensor networks;event detection;spatio-temporal;conditional random fields;sensor data
regularization;metric learning methods;auxiliary;training examples;distance metric learning;recognition task;distance metrics;pairwise constraints;prior knowledge;unlabeled examples;data sets;labeled data;distance metric;labeled examples;label information;manifold regularization
image retrieval;image database;tabu search
clustering;clustering;regression problems;classification;multiple instance;optimization problem;machine learning;instances;clustering task;maximum margin;multiple instance;maximum margin;efficient optimization
nonnegative matrix factorization;feature space;face databases;computational cost;gaussian kernel;kernel space;basis vectors
domain knowledge;recognition performance;principal components;external knowledge;model estimation
human beings;unlabeled data;distance matrix;metric distance;semantic relations;label propagation
kernel discriminant analysis;gaussian process;binary class;probabilistic models;kernel discriminant analysis;multiclass problems;probabilistic model;real world;dimensionality reduction
auxiliary;data collections;semi-supervised learning;heterogeneous data sources;instance-level;instances;build models;multiple information sources;training algorithm
clustering;unique features;domain-specific;domain-independent
clustering;nonnegative matrix factorization;classification;real-life datasets;hierarchical clustering;clustering methods;gene expression
data analysis;geographic information;input features;real-world;location based;performance prediction;regression model;long-term;semi-supervised;semi-supervised;manifold regularization
test data;classification;training examples;critical task;hand-drawn;recognition systems;classification rate
classification accuracy;classifier;higher accuracy
perform inference;human players;human experts
ranking algorithms
human behavior;clustering techniques;predictive models;large amounts of data
prediction accuracy;topic tracking;topic model;memory requirement;computational cost
web-based;user experience;qualitative analysis;keyword-based;machine learning;rates;information extraction;natural language understanding

expression patterns;local regions;multi-label learning;learning framework;large number of;multi-instance;multi-label learning;vector machine;textual description;multi-instance;gene expression
description language;resource allocation;data centers;data-center
online learning;predictive features
visual appearance;recognition accuracy;hand-drawn;handwritten digits;multiple domains;classifier
context aware;context-sensitive;web mining
hidden variables
data points;linear algebra;latent structure;scientific discovery;matrix decomposition;learning task;normal form
machine learning methods;simulation model;physical environment;explicitly modeling;fold cross validation;learning method;natural language
parallel implementation;parallel algorithm;designed specifically for;general case;biological sequences;longest common subsequence
specific domains;graph-based;knowledge-based;knowledge-based;annotated corpus;domain-specific;news related;related words;word sense disambiguation
diverse range of;context-sensitive;web-scale
clustering;information retrieval research;latent dirichlet allocation;classification;concept-based;semantic analysis;information retrieval;wikipedia articles;instance;retrieval models;latent semantic indexing;semantic information;latent topics;cross-language information retrieval;cross-language
question-answer;question-answer;sentence-level;learned models
stochastic process;inference rules;machine learning
bayesian model;unsupervised learning;natural language processing
human performance
knowledge sources;semantic search;spreading activation;great potential;natural language
highly complex;question answering;learned models;information extraction;instance;domain-specific;machine learning;information services;natural language processing;question classification;question answering
machine translation;automatically generate;context-based;context-based;increasingly popular
syntactic structures;competitive performance;error reduction;dependency trees;latent variables
semantic space;information gathering;word vectors
limited space;2007;language modeling;language models
semantic smoothing;translation model;mixture model;language model;context-sensitive;dependency relations;semantic graph;context sensitive;test query;mixture models;related words
ranking algorithm;graph-based;learning algorithms;multiple documents;multi-modality;multi-document summarization;ranking methods;benchmark datasets
algorithm runs in;basis functions;lower level;instances;data sets;automatically constructed;document corpora;operator
common sense;latent semantic analysis;automatically infer;corpus-based
scale space;knowledge base;natural language processing
clustering;clustering algorithms;density estimation;data distributions;exponential family;historical data;clustering problems;evolutionary clustering;data distribution
word sense disambiguation;large-scale;automatic methods;human efforts;parallel corpora;high accuracy;supervised learning;word sense disambiguation
belief space;state space;planning problem;search problem
domain knowledge;randomly generated;planning domains;domain descriptions
user actions;goal recognition;markov models
point-based;point-based
margin;formal model for;generally applicable;online learning;real-life;search algorithm;heuristic search
partially observable markov decision processes;probability distributions over;action sequences;equivalence relations;markov decision processes;equivalence relations;givan et al., 2003;partial observability;partially observable markov decision processes
search methods;state space;search algorithms;completeness;search cost;directed graph;state-space;sufficient conditions;completeness;planning domains
planning problems;high-quality;planning problem;general-purpose;algorithms require;state-space search;completeness;heuristic search
temporal planning;linear program;linear programming
planning problems;algorithm generates;planning algorithm;domain-independent;probabilistic planning;memory constraints;heuristic search;dai et al., 2008;state space;partitioning scheme;benchmark domains;external-memory
belief space;real world;point-based;state space;topological structure
planning problems;optimal planning;database;finite-domain;soft constraints;objective function
answer set programming;activity recognition;formal language;activity recognition;recognition problem
plan recognition;plans
planning algorithm;hierarchical task;planning domains;learning algorithm;desired properties;hierarchical task
real-world datasets;sensor readings;false positive;hierarchical dirichlet process;activity recognition;vector machine;real-world applications;algorithm's performance;activity models;hidden markov model;model parameters;generative model
real world;planning domains;plans
search procedure;heuristic search;optimal planning;optimal) planning
shortest paths;shortest-paths;plans;closely related;plan quality;graph theory;benchmark domains;heuristic search
web service;service descriptions;web service composition;general case;web service composition;web services;partial observations
classical planning;state space;function approximation;high-quality;probabilistic planning;decision-theoretic;domain model;basis functions;expected-utility;additional knowledge;probabilistic-planning;decision-theoretic framework;typically requires;decision theory;little and thiebaux, 2007
generate plans;hierarchical task;plans;prior information;manual construction of;learning "method;user preferences;action models;error prone;hierarchical task;control knowledge
multi-agent;multi-agent;plans;partially observable
search methods;classical planning;action selection;monte-carlo;random walk;local neighborhood;stochastic local search;monte-carlo simulation;random walks
heuristic approaches;plans;real-world;optimal plan;user-interface;representative set;multiple objectives
planning algorithms;plan recognition;domain theory;optimal plan;plan recognition
state space;real-time dynamic programming;performance guarantees;real-time dynamic programming;perfect information;markov decision processes;lower bounds;dynamic programming
preference-based;planning domain;user preferences;plans;definition language;hierarchical task;control knowledge
service-oriented;real world;control flow;web services
planning problems;domain knowledge;real-world;knowledge-engineering;action models;unlike prior;hierarchical task;partial observations;formal framework
multi-robot;theoretical results
naïve;human-robot
planning algorithms;path planning;planning algorithm;search tree
instance;mapping problem;search strategies
automatically generates;multiple images;supervised approach;image analysis;data set;large data sets;learned model;road network;single image;road network;supervised learning algorithm;error prone;human intervention
path planning;plans;theoretical guarantee;multi robot;objective function;np-hard
real data;parameter-free;low-dimensional;gaussian process;articulated objects
computational model;semantically meaningful;human-robot
training examples;image features;representative set;novelty detection;image analysis;level features;distance metric;kernel density estimation
planning problems;path planning;search space;completeness;real-life applications;multi-agent;branching factor;upper bounds;memory requirements;general case;multi-agent;worst-case
distance measure;training data;classification;human activity;feature extraction;intelligent systems;compared with conventional;unsupervised clustering;human action recognition;low-level;human activities;human activity;svm classifier;human action;visual features
high computational complexity;markov network;markov networks;ad-hoc;ontology matching;semi-automatic
preference elicitation;user's preferences;accurately predict;learning problem
upper-bound;bayesian networks;decision-making;efficient representation;utility theory
closely related;graphical structure;random instances;utility functions;utility theory;search problems;individual preferences;worst case
decision problem;greedy algorithms;optimal policies;real-world situations;distributed databases;greedy approach;simulated data;greedy algorithms;partial observations
planning problems;dynamic programming;model checking;relational structures;markov decision processes;knowledge representation
data structure;probabilistic models;inference algorithms for;aggregation operator;exact inference;random variable;domain size;probabilistic graphical models
query complexity;attribute domains
lower computational cost;gaussian process;large-scale;stochastic processes;memory usage;gaussian processes;large datasets;covariance matrix
probabilistic models;data structures;programming languages;filtering algorithm;programming language
markov logic networks;sampling methods;markov logic networks;real-world domains;inference algorithm;approximation techniques;exact inference;complex relationships
causal discovery;conditional independence;latent variables
search algorithm;bayesian networks;search efficiency
data analysis;random variables;causal models;graphical models;identification problem;fundamental problem;social sciences;artificial intelligence
multi-player;monte carlo;low variance;agent performance;high cost;analysis tool
variable ordering;efficient computation;upper bounds;assignment) problem;2003;bayesian networks;upper bound
knowledge bases;probabilistic logic;minimal change;merging operators;probabilistic logic;multiple sources
partially observable;graphical models;decision making;candidate models
linear programming;specific problem;game-theoretic;2008;decision model;markov decision process
rewriting queries;data model;query reformulation;peer data management systems;description logic;data management
large-scale;filtering systems;multiple users;accurately reflect;collaborative filtering;recommender systems;strong correlation;share information;classification;sketching techniques;sketching techniques;data sets;target user;high probability;hash functions
theoretical framework;recommender systems;parameter estimation;real-world;user's interests;predictive accuracy
service provider;argumentation-based;search space
search results;agent based;large volumes of;social network;social network;search queries;social networks;improving search;social networking;agent-based
build models;closely related;classification
pruning strategy;computational complexity;diffusion model;social network;multiple times;social networks;real world networks;theoretical analysis
user-item;auxiliary;collaborative filtering;target domain;rating matrix;data sparsity;cross-domain;cluster-level
local features;video frames;scale invariant;spatial distribution;feature distribution;video frame;similar images
taking into account;digital content;similarity metric;linear combination;search engines;page content;social media sites
query execution;knowledge bases;large-scale;relational database systems;ibm db;description logic;conjunctive queries;query answering;relational database system
training data;information extraction;supervised approach;reference set;information extraction from;conditional random fields
wikipedia categories;fine-grained;classification;large-scale;large number of;instances;high accuracy;automatically generated;semantic information
effectively learn;user-defined;semantic web;semantic structure;ontology learning;real world data sets
human computation;web sites;labeled samples;machine learning algorithms;machine learning techniques
collaborative filtering;data set;domain-specific;retrieval task;detection methods;user behaviors;related words
multi-player;monte carlo;low variance;agent performance;high cost;analysis tool
xpath expression;update processing;xml publishing;view updates;view update;xml documents;xml views;xml document;compressed representation;relational database;optimization techniques
web-sites;matching algorithms;auxiliary;semi-structured data;storage structures;main memory;memory requirements;tree nodes;complex objects;xml documents;unordered tree;chemical compounds;real-life data sets;access patterns;internal structure;pattern matching
benchmark data;moving object databases;moving object;data sets;noisy data;moving objects;nearest neighbour;spatio-temporal;range queries
open-source;management systems;peer data management;statistical information;query processing in;base data;distributed systems;traditional database;internet-scale;data management
clustering;clustering algorithms;high-quality;clustering structure;tree structure;data stream;data streams;decision making
real-world
ir evaluation;sigir 2009 workshop on;ir evaluation;poster session;sigir workshop on
information access;statistical machine translation;real-world applications;entity recognition;information access;question answering;cross-language information retrieval
computational advertising;information retrieval;information retrieval;information retrieval
index compression;distributed computing;computational power;distributed query processing;collection selection;information retrieval;resource discovery;large scale;similarity search;distributed information retrieval;large-scale distributed systems;query processing;web search;information retrieval;data organization;large-scale distributed systems;load balancing
large scale;information retrieval evaluation;text retrieval;domain specific;information retrieval
query log analysis;query log analysis

information retrieval;information retrieval;poster session;international workshop on
domain knowledge;domain experts;search topics;mesh terms;relevance judgments;information retrieval;search effectiveness;text retrieval;information seeking;search topic
answer set;high recall;information retrieval
cost-benefit analysis;online discussion;large scale;data sets;test collections;prior art;test collection
entity ranking;structured documents;focused retrieval;xml mining;test collections;wide range;evaluation measures;ad hoc
web content;conflicting information
ranking techniques;query routing;data quality;computational power;distributed query processing;information retrieval;resource discovery;similarity search;distributed information retrieval;large-scale distributed systems;resource selection;information retrieval;data organization;large-scale distributed systems;load balancing
international workshop on;information access;information access;international workshop on
information retrieval;information retrieval;poster session;international workshop on
information retrieval;information retrieval;information retrieval
xml) documents;systems support;content-oriented xml retrieval;semi-structured;retrieving information from;xml repositories;xml documents;user queries;xml elements;xml document;logical structure;xml element;segmentation algorithm;content-oriented xml retrieval
relevant expertise;ranking documents;expert search;search tasks;search engine
document portions;large numbers of;xml retrieval;structured documents;document sets;document retrieval;automatic generation of;structured documents;retrieval systems;xml documents;xml elements;logical structure;xml element retrieval;query independent
state university;limited memory;international conference on;data mining;sliding window;error guarantees;data stream;linear space;lower bounds;data stream;relative error;statistical analysis;data streams;negative result
error rate;precision-recall;linear classifier;vector machine;rank-based;data mining;test statistic;learning theory;statistical analysis;multivariate data;data streams;svm classifier;test statistics;hypothesis testing;margins
prediction accuracy;regularization;learning algorithms;data mining;real data sets;error rates;large-margin;activity recognition;dynamic environments;large margin;training instances;feature selection;prediction problems;linear classifiers;machine learning algorithms;statistical analysis
real-world datasets;frequent sets;mining algorithm;low frequency;database;special case;data mining research;1;finding patterns;statistical analysis;data mining;operator;efficient discovery of;interesting patterns
training data;data mining;associative classifiers;competence;test instances;meta-learning;comprehensive evaluation;classifier;classification performance;associative classifier;accurate classifiers;statistical analysis;ensemble method;associative classification;evaluation measures;competence
em) algorithm;models learned;data cube;textual information;text documents;expectation-maximization;topic models;text data;topic hierarchy;data records;topic model;topic modeling;text databases;statistical analysis;data mining;topic modeling;online analytical processing;latent topic;structured data;model called;text database
computationally intractable;event types;probability models;sliding window;time series;fixed size;window sizes;statistical analysis;data mining;sliding windows;brute force search
classification learning;auc;numerical experiments;imbalanced data;class distributions;sampling method;skewed;data sets;statistical analysis;data mining;real-world data sets;real-world applications;sampling technique
high-dimensional datasets;synthetic datasets;gene expression;data mining;grid-based;subspace clustering;statistical analysis;similarity measurements;subspace clusters;sliding-window;distance-based
clustering;parameter estimation;real datasets;generative model for;data matrix;coordinate descent;subspace clustering;gibbs sampling;typically rely on;approximate inference;search heuristics;subspace clustering
clustering;nearest;nearest neighbor;target values;classification;learning procedure;feature space;multiclass classification;learning algorithms;algorithm performs;learning process;benchmark datasets;learning vector quantization
subgraph mining;synthetic datasets;frequent patterns;large number of;mining process;subgraph mining;multidimensional space;frequent pattern
learning examples;classification;training examples;partial knowledge;argumentation based;constraint network;knowledge acquisition;problem solving;constraint networks
12;synthetic datasets;sensitive information;group members;privacy preserving data publishing;privacy preserving;sensitive attribute;information loss
convergence rate;black-box;multi-task;large-scale;generalization performance;learning problem;learning problem;optimization problem;text classification;convergence speed;gradient method;learning problems;feature selection problem;gradient method;real world;training algorithm;loss functions;multi-task
clustering;clustering;fusion strategies;multi-view;real datasets;information retrieval;clustering process;multiple-view;clustering algorithm;data mining research;clustering approach
prediction accuracy;data collection;prediction model;ground-based;spatio-temporal;data-mining approach;large number of;strong evidence;distributed data;remote sensing
clustering;web pages;large-scale;large scale;large margin;large scale;linguistic patterns;semantic relations;high-level;instances;mining tasks
user visits;random sample;web content;web content;ctr
cost-effective;real-world;personalized services;business opportunities;contextual information
regression trees;decision trees;learning rules;regression problems;real life problems;decision rules;multi-target;problem domains;rule sets;optimization procedure;learned model;multi-target;ensemble approach
clustering;global models;expectation maximization algorithm;density estimation;anomaly detection;sample data;dynamic nature of;data mining tasks;large number of;data sources;gaussian mixture models;expectation maximization algorithm;sensor networks;scalable distributed;algorithm takes;learning parameters;distributed environments;target tracking
high-dimensional;high-correlation;efficient search;scientific domains;closely related;large databases;large datasets;desirable properties;likelihood ratio
clustering;learning process;clustering ensembles;relation extraction;text documents;information contained in;information extraction;learning approaches;machine learning techniques;automatically generate;human supervision;information extraction system;detection task;unsupervised techniques
clustering;clustering ensembles;object-based;data clustering;optimization problem;feature-based;projective clustering;clustering methods;clustering problems;clustering ensembles
latent topics;multi-level;latent topic;citation network;scientific articles;generative process;document level;topic models;knowledge discovery;knowledge discovery;hierarchical structure;topic distributions;variational approximation;digital library;databases;citation networks;modeling task
covariance matrix;synthetic data;modeling methods
clustering;clustering algorithms;similarity measure;target domain;cross-domain;supervision;clustering task;supervision;real-world datasets;clustering algorithm;source domain;clustering accuracy;clustering
support vector machines;classification;classification;probabilistic model;nearest-neighbor classifier;audio features;data set;kullback-leibler divergence;supervised learning;classifier
association mining;high-confidence;sampling-based;low-support;sampling method;real data sets;association rules;pairwise similarity;theoretical guarantees;similarity measures
prediction problem;sample size;learning algorithm;complexity bound;classification results;learning problem;internet applications;real world
data objects;similarity query;data dimension;data collection;data values;incomplete data;database;probabilistic framework;retrieval results;information retrieval;similarity query;upper bounds;real data sets;query processing;sensor network;data mining;incomplete data;probability threshold
minimum support;frequent set;frequent set;database
computation cost;knowledge extraction;speed-ups;multi-core;data mining
syntactic structures;syntactic structures;latent dirichlet allocation;hybrid model;evaluation metric;content words;hidden markov models;topic-specific;modeling method;text analysis
clustering;domain knowledge;simulated annealing;graph partitioning;plans;spatial clustering;heuristic functions;instance-level;clustering process;spatial clustering;cluster-level;clustering
irrelevant features;convergence rate;optimization problem
regression algorithm;computational complexity;fast algorithm;numerical results;cross validation;regression algorithm;correlated features
network analysis;clustering;biological networks;biological networks;complex networks;weighting schemes;graph-theoretical;biological networks;clustering methods;hidden knowledge
anomaly detection;generalization performance;data sets;novelty detection;vector based;support vector machines
ground truth;markov models;real world datasets;markov models;instances;knowledge based;stationary distribution;real world;temporal data;similarity based;traffic data;knowledge discovery
generalized queries;active learning;specific queries;real-world;active learning;human experts;accurate classifiers;active learning;labeled examples;active learning algorithm;generalized queries
support vector machines;web search;neural networks;sampling method;information retrieval;machine learning;optimizer;conditional probability;gradient based;loss functions;feature vectors;monte carlo
feature space;density-based;cluster structure;real-world datasets;density-based clustering;user intervention;semi-supervised clustering;labeled dataset;semi-supervised;semi-supervised;density-based clustering algorithm
decision function;regularizer;vector machine;decision rule;training samples;margin
clustering;contrast patterns;clustering validation;categorical data;pattern based;high quality;categorical data;distance function;pattern based;clustering quality
document cluster;evaluation criterion;document summarization;multi-document;multi-document summarization;document set
encoding scheme;specific information;association rule mining;association rule mining;data confidentiality;data mining;data owner
feature space;learned metric;real-world;text clustering;clustering process;clustering result;clustering task;parameter tuning;metric learning;learned “knowledge;clustering algorithm
detection algorithms;multivariate data;outlier detection;multiple subspaces;great success;classification methods;detection methods;dimensional data
log analysis;detection techniques;finite state;anomaly detection;large scale distributed systems;source code;learned models;distributed systems;free form;log analysis;distributed systems;text messages;detect anomalies
clustering;clustering;classification;web pages;multi-task;classification;cross-domain;classification approaches;clustering approaches;closely related;clustering method;clustering performance;input space;shared subspace;clustering tasks;shared subspace;text data sets;real world;classification method;clustering methods;multi-task
degree distribution;conventional techniques;theoretical analysis;million nodes;theoretical analysis;power-law;degree distribution
graph-structured data;graph structures;label set;kernel based;graph kernel;structural characteristics;benchmark data sets;graph kernels;large graphs;knowledge discovery;high scalability;computationally expensive;graph kernel;node labels;average number of
graph-based;clustering algorithm based on;clustering method;clustering methods
automatically extracting;extraction process;task-specific;user evaluation;natural language;knowledge base;real world data sets
ranking problem;prediction accuracy;real data;ranking models;svm classifiers
clustering;clustering;sliding window;time-series;moving average;data mining;time-series;pattern extraction;moving average
synthetic data;computational biology;classifier performance;classification problems;classifier;classification error
clustering;clustering;mathematical models;valuable information;time series;time series;real world
low-dimensional representation;nearest neighbor;optimization method;sparse metric learning;real-world;vector machine;large margin;dimension reduction;large datasets;unified framework;distance metric;benchmark datasets;sparse metric learning
search results;clustering;web pages;web search;graph-based;graph-based;search engines;unsupervised algorithm;graph-based;web search;web data
data mining;greedy approach;synthetic datasets;statistical tests;tree-based;spanning tree;scientific applications;tree-based;data sets;tree construction;statistical test;dynamic programming approach;real world
prediction accuracy;low computational cost;multiple tasks;target task;training examples;transfer learning;collaborative tagging;weak classifiers;learning parameters;negative transfer;transfer learning;learning method;prediction tasks
real graphs;spectral clustering;open source;highly optimized;web graphs;graph mining;connected components;graph mining;mining tasks
real datasets;interpretability;classifier;data analysis;database
cosine measure;classification;data sets;similarity matrix;batch learning;similarity measures
data structure;pagerank algorithm;dynamic social network;learning algorithm;social network;organizational structure;social network analysis;organizational structure;dynamic social network;real data;real world;random walks
gaussian process;probabilistic model;conditional distribution;gaussian processes;data mining;desirable properties;risk minimization;benchmark datasets;rates
naive bayes;taking into account;classification;data uncertainty;uncertain data;naive bayes;probability estimation;uci datasets;data item;probability distribution function;uncertain data;measurement errors;machine learning algorithms;classification algorithm
dynamic graphs;real-world;community structures;meaningful patterns;structured data;dynamic graphs;constraint-based mining;temporal evolution;pattern mining;time series;pruning strategies;social networks;constraint-based;dynamic graphs
increasing demand for;data analysis;frequent subgraph mining;theoretical result;frequent subgraphs;theoretical foundations;subgraph mining;lower bound;mining process;highly correlated;efficient discovery of;graph data;simple algorithm;graph mining;approximate algorithm
clustering;clustering;clustering results;index structure;streaming data;parameter free;incoming data;data stream
feature space;classification;data points;highly skewed;time series;imbalanced data sets;accurately detect;data mining;imbalanced data sets;imbalanced data sets;svm classification;support vector machines;classification rate;feature spaces
input data;cost-aware;computational cost;instances;wide range;classification;classifier;instance;computational cost;classification systems;computational cost;pre-processing;traditional classifiers;network flow;decision trees;computational power;classification accuracy;input space;computational costs;feature selection;building classifiers;classifier
ground-truth;classification;attribute dependencies;classifier learning;bayesian classifiers;higher-order;training samples;probability estimation;excellent performance
real-world;large number of;data set;spatial distribution;quality control;reference point;quality measures
training data;label space;text categorization;optimization problem;text categorization;classification problem;classification method;learning problem
dimensional data;real data sets;parameter-free;community structures;weighted graph;community discovery;complex networks;nearest-neighbor;community structures;fully automatic;simple algorithm;network structure;dimensional data;pre-processing;community structure;social sciences;high dimensional space
conditional entropy;probabilistic segmentation;web service;learning phase;learning algorithm;probabilistic approach;labeled data;maximum likelihood;supervised learning;discover meaningful
clustering;temporal information;topic model;real-world domains;temporal sequence;topic modeling;transition probability;latent topics
information sharing;mathematical models;privacy concerns;privacy risk;online social networks;social networks;real-world;individual users;online social networks
low-dimensional space;dimensional space;computation cost;linear discriminant analysis;linear discriminant analysis;instances;high accuracy;computational cost;algorithm produces;dimensional data;dimension reduction
nonnegative matrix factorization;face recognition;optimization problems;sample data;objective functions;multi-label;feature spaces;image annotation
total number of;input data;data summarization;observed data;real data;data sets;matrix decomposition;matrix decomposition;data management;np-hard
data points;active learning;ensemble members;active learning;naive bayes;data set;instances;uncertainty sampling;accurate classifiers;single classifier;classifier;random sampling
predictive models;accurate models;predictive accuracy;learned models;classification tasks;data representation;data sets;similar features;model parameters;sequence classification;sequence classification;feature selection
global model;classification models;global-model;hierarchical classification;naive bayes;naive bayes;classification model;global--model;protein function;classification algorithm
supervised methods;unsupervised feature selection;spatio-temporal;gait recognition;recognition rates
feature space;regularization;feature space;quadratic programming;tensor product;data sets;feature selection;feature sets;feature selection methods;tensor product;feature spaces;selecting features;feature selection
web pages;weighted graph;application domain;real datasets;large graph;topic distributions;web page;supervised learning;web users
clustering;clustering;graph-based;clustering accuracy;real-world applications;matrix factorization;clustering methods;learning models;multiple sources;semi-supervised;matrix factorization
clustering;real world datasets;clustering accuracy;objective function;laplacian matrix;low dimensional representation;pairwise similarity
multi-dimensional;naive implementation;multi-step;search cost;time-series;similarity search;discover patterns;multiple categories;data mining;query distribution;sequential scan;similar users
feature mapping;data mining methods;classification;irrelevant features;large-margin;feature mapping;supervised dimensionality reduction;large datasets;classification;distance metric;relevant information;large-margin;neural network
clustering;disk-resident;classification;image processing;main memory;higher-level;time series;time series;rule-discovery;databases;data mining algorithms;time series
subspace clustering;clustering quality;global optimization;multiple times;subspace clustering;subspace clustering;high accuracy;dimensional data;relevance model;np-hard;subspace clusters;real world;high dimensional space
service quality;classification;classification problem;high-quality;classification;distance metrics;active learning;case study;service providers;labeled data;metric learning
clustering;clustering;pattern recognition;maximum margin clustering;clustering technique;data manifold;vector machine;maximum margin clustering;maximum margin;maximum margin clustering;real world data sets
ranked list;special case;real data sets;outlier detection;temporal context;data set;probabilistic approach;stationary distribution;random walks;anomalous behavior;contextual information;real-world applications;normal behavior;random walks
clustering;large networks;pattern recognition;real networks;complex networks;large networks;agglomerative clustering;data mining;produce high-quality;clustering algorithm;image processing
matrix factorization;collaborative filtering;collaborative filtering;normal distributions;data set;matrix factorization;model parameters
object recognition;synthetic data;kernel matrices;linear combination;counterpart;fisher discriminant analysis;learning problem;fisher discriminant analysis;multiple kernel learning
regression tasks;gaussian process;prediction performance;covariance functions;real-world;gaussian process;regression model;learning scheme;gaussian processes
cross-validation;attribute dependencies;classification accuracy;link structure;data mining and machine learning;statistical tests;network classification;learn models;instances;statistical tests;network data;modeling techniques;relational data
network models;mutual information;event streams;mining temporal;dynamic bayesian network;episode mining;event streams;frequent episodes;mathematical models
clustering;clustering;mining process;vector representation;moving objects;databases;distance metric;clustering algorithm;real world
sequence labeling;target class;class label;closely related;sequence labeling;information extraction;supervised approach;entity recognition;semi-supervised;classifier;natural language
network analysis;network analysis;real world applications;random walk;real world networks
parameter-free;distance measures;data representation;similarity measure;large data sets;hierarchical algorithm;categorical data;real data;hierarchical clustering algorithm;chi-square;clustering categorical data;fully automatic;clustering categorical data
citation network;efficient inference;learning algorithms;topic models;topic model;topic discovery;topic distributions;multiple types of;gibbs sampling;multiple relations;modeling methods
domain experts;bayesian network;probabilistic information;compact representation;relevant attributes;bayesian networks;clustering algorithm;bayesian networks;bayesian learning
database;locality-sensitive hashing;performance gain;cost-effective;data-mining applications;cost-effective;upper-bound;cost-effective
clustering;clustering;clustering results;training data;markov models;clustering technique;semi-markov;activity recognition;semi-markov;time series;ordered list;sensor data;max-margin;activity recognition
clustering problem;graphical models;cluster structure;causal relationships;causal modeling;optimization approach;decomposition approach;real datasets;large number of;time series;graphical model;maximum weight;optimization procedure;graph theoretical;causal structure;graphical model;objective function
training set;random walk;supervised approach;database;graphical structure;clustering method;database entries;edges represent;nodes represent;database relations;relational databases;relational databases;support vector machines;random walks
multi-class;unsupervised learning;classification;feature vector;variational inference;great success;variational approximation;logistic regression;text classification;class labels;benchmark datasets;classification problems;naive bayes models;latent dirichlet allocation
instance;transfer learning;machine learning;instances;semi-supervised learning methods;semi-supervised learning;semi-supervised learning methods;transfer learning;learning method
nearest;face recognition;nearest point;kernel-based;line segment;classification algorithm;classification;quadratic programming;algorithm called;support vector machines;high dimensional space;kernel-based
web documents;markov random field;topic distribution;random variables;modeling framework;distribution function;real datasets;topic model;graphical model;joint probability;topic modeling;topic modeling;generative model;network structure;structure information;information network;increasingly popular
social media;multi-dimensional;structural features;large scale;group members;multiple dimensions;instance;hidden structures;real-world;social networking;community structure
vast number of;frequent episodes;pattern mining;frequent sets;level-wise;computationally feasible;quality measure
data point;matrix factorization;million images;data points;large-scale;distance preserving;data set;information retrieval;matrix factorization;large data sets;candidate set;social networks;data mining;real-world data sets;low-dimensional
clustering;high-dimensional datasets;cluster structure;data visualization;compression technique;original data;data privacy;compression scheme;storage requirements;time-series;clustering algorithm
data objects;classification models;classification;large scale;accurate classification;model generation;large scale;feature selection;learning models;model generation;dimension reduction
query forms;data source;mixture model;deep web;complete set of;deep web;data sources;data sources;input query;online databases;high recall;data dissemination;data integration
multitask learning;unlabeled data;multi-task learning;share similar;gradient descent;labeled data;multi-task learning;learning problem;multi-task learning;semi-supervised;real world data sets;semi-supervised;classifier
greedy approach;training examples;active learning;benchmark data sets;counterpart;incremental learning;greedy approach;computational cost;support vector machines;objective function
behavioral patterns;real datasets;great potential;individual objects;behavior analysis;data mining;mining algorithm
anomaly detection;principal component analysis;highly accurate;1;frequent sequences;data mining;mining patterns;estimation techniques;execution traces;statistical learning;frequent pattern mining
feature extraction;dimension reduction;optimization framework;feature selection algorithms;optimization framework
detection techniques;data analysis;data mining method;probability density function;algorithm called;peculiar data;theoretical basis for;distance based;bayes classifier;bayesian classifier;classification problems;conditional probabilities;benchmark data sets;nearest neighbors
anomaly detection;density-based;anomaly detection;anomaly detection;sampling-based;multiple criteria;instances;data mining task;high accuracy
data mining;learning framework;data collection;semi-supervised;training examples;limited resources;real-world;data collected;unlabeled samples;unlabeled examples;stream data;data streams;unlabeled instances;labeled examples;mining data streams;labeled and unlabeled data;prediction models
clustering;clustering algorithms;clustering results;loss function;loss function;linear combination;cutting plane algorithm;optimization problem;maximum margin clustering;clustering performance;error rate;maximum margin clustering;maximum margin
search space;real-world;effectively identify;data set;large data sets;transaction database;efficient discovery of;upper bound;efficiently identify;association analysis
instances;real-world;data streams;training process;instance level;instance;data streams;classifier
training set;training data;training set size;optimization algorithm;stochastic gradient descent;linear svm;common sense;logistic regression;learning problem;special case;loss functions
large scale;stochastic gradient descent;vector machine;optimization problem;gradient descent;training samples;svm classifier
human interactions;visual data mining;predictive power;classifier;distance metric;mathematical framework;classifier;benchmark datasets
outlier detection;negative examples;inductive logic programming;inductive logic programming;anomalous behavior;detecting outliers
text mining;text mining;latent dirichlet allocation;user-generated;topic model;effectively identify;topic modeling;latent topics;online news
distributed algorithm for;database;unified framework;belief propagation;ontology matching;data mining;systems biology;combinatorial optimization problem;np-hard
multiple classes;latent dirichlet allocation;large collections of;topic model;dirichlet mixture;dirichlet mixture;document collections;statistical analysis;dirichlet distribution
local search;combinatorial optimization problem;np-hard problem;chi-square;protein interaction networks;protein sequences;support measures;support measure;protein interaction networks
low bandwidth;detecting anomalies;anomaly detection;database;data mining applications;spectral methods;anomaly detection;large-scale;sensor networks;large datasets;data streams;sensor networks;computing power
latent dirichlet allocation;information extraction;retrieval tasks;user generated content;collaborative tagging;probabilistic approach;data sets;data mining;collaborative tagging;data complexity;common knowledge;collaborative tagging;similar users
approximate algorithm;real world graphs;collaborative filtering;similarity measure;real world datasets;similarity scores;large graph;information retrieval;prohibitively expensive;power-law distribution;synthetic datasets;link-based;real world networks;link-based
multiple dimensions;data analysis;time series;complex patterns
data mining;data sets;data mining techniques;knowledge acquisition;data mining
probability density functions;case study;anomaly detection;anomaly detection;data mining;high dimensional datasets;case study;joint probability distribution;data mining;hidden knowledge
massive amounts of;computing infrastructure;observational data;data analysis;observational data
large amounts of;data analysis;high level;scientific data;simulation results;data sets;dynamic systems;combining multiple;spatio-temporal
climate data;high computational cost
detection problem;climate data;visualization tools;obtained by applying;detection algorithm;change detection;climate data;statistical test;physical location;temporal data;geographical information;cluster structures
simulation data;climate data;software tools;multiple sites;knowledge discovery;data analysis;data management
singular value decomposition;evolving data;link structure;data mining tasks;link prediction;tensor-based;numerical experiments;link-based;social networks;temporal link prediction;link prediction;tensor decomposition
web services;real-world;data complexity;scientific literature;case studies;wide range;data processing;long term;user interaction;web interface
large data sets;modern hardware;highly scalable;memory requirement;real data sets
mining algorithm;very large datasets;decision tree;cloud computing environment;data mining
singular vectors;surprising patterns;large graphs;social graphs;social networks;community structure
distributed file system;data mining;storage model;user interface;distributed environments;data mining;data intensive applications;computational model;data mining based
candidate generation;semi-structured;web pages;annotated corpus
database;case study;markov models;real-world;markov models;large data sets;model parameters;estimation algorithms
high-dimensional;regularization;document classification;loss function;large-scale;parallel implementation;machine learning;machine learning applications;squared-error
decision trees;large scale;text documents;text categorization;feature-set;computational costs;tree induction;decision trees;multi-label data;data-fusion
greedy algorithm;domain experts;labeling effort;greedy algorithms;classification;training examples;batch mode active learning;classification accuracy;real-world;batch mode active learning;objective function;greedy algorithms;text classification problems;real world;supervised learning;active learning algorithms;user interaction;active learning algorithm
local optimization;connected component;graph mining;large scale datasets;massive graphs;community detection;map-reduce;structure mining;prior works;structure mining;structure mining
mining results;data structure;web usage mining;incremental mining;database;pattern mining;incremental mining;incremental mining;algorithm named;traversal patterns;databases;web data;traversal patterns
clustering;spectral clustering;signal processing;data samples;spectral clustering;theoretical results;clustering performance;data mining;compressed domain;compressed data;real world data sets;theoretical bounds
underlying structure;redundant information;data mining method;class label;attribute sets;data set;inter-dependencies;feature selection;learning problem;correlated attributes;information theoretic
classification problem;classification;classification;data mining and machine learning;kernel k-means;content based image retrieval;training samples;multi-class;kernel k-means;supervised learning
regularization term;regularization;svm models;classification;real-world;great success;vector machine;large margin;prior knowledge;structure information;feature selection;feature selection methods;microarray data analysis;dimensional data;real world;model selection;feature selection
complex networks;real world;real-world networks;heuristic algorithm;community structure
lower-level;support vector regression;fold cross-validation;sample size;optimization problems;learning models;data mining;support vector machines;loss functions
forward selection;individual features;feature selection;feature selection methods
multi label classification;large-scale;real-world;post-processing;regression problem;text classification;multi-label;retrieve information
multi-class;synthetic data;large scale;knowledge management;data mining;large number of;training data;information retrieval;real world datasets;coordinate descent;labeled data;semi-supervised learning;semi-supervised;increasing attention;labeled and unlabeled data
computational complexity;pre-processing;gene expression data sets;feature selection method;classification accuracy;real-world;learning algorithms;auc;correlation coefficient;feature subsets;feature selection;feature selection methods;classification problems;classifier;feature selection
multiple instance;instances;transfer learning;text categorization;real-world applications;content based image retrieval;instance;machine learning and data mining;multiple instance;learning problems;drug design;transfer learning
learning algorithm;classification;classification rules;accuracies;learning algorithms;attribute sets;classification rules;algorithm selection;objective rule;classifier
effectively exploit;prediction accuracy;unlabeled data;labeled training;instances;labeled training data;large number of;feature selection algorithms;real-world applications;algorithms require;labeled data;feature selection;feature subset;semi-supervised;data distribution;labeled and unlabeled data;feature selection
training data;target task;performs poorly;source task;instance-level;instances;source data;instance-based;source tasks;negative transfer;instance-based
heterogeneous information network;recommendation systems;information sources;prediction tasks;matrix factorization;link prediction problem;heterogeneous information networks;knowledge transfer;real world;information networks;data sparseness;auxiliary
text mining;text classifier;classification;document classification;retrieval tasks;document retrieval;real data;classifier;text classifier;accurate classifiers;text classification;transfer learning;labeled data
cross-domain;belief propagation;semi-supervised learning;partial knowledge;graphical model;cross-domain;learning tasks;information retrieval;data set;knowledge discovery;iterative algorithm;belief propagation;belief propagation;semi-supervised;relational data;real world data sets
classification;classification;real data;sequential data;small sample size;classifier
data stored;data warehouses;domain ontology;pattern mining;hidden information;domain driven;pattern mining;data tables;data stored in;apriori-based;storing data
clustering algorithms;semantic relations between;semantic similarity;benchmark data sets;document clustering;document clustering;document representation;vector space model
domain knowledge;web applications;mining framework;transition probability;markov models;domain ontology;context-aware;markov models;mining process;higher order;semantic web;web site;web usage;accurate predictions;semantic information;semantic information;space complexity;markov model
text mining;text mining;textual description;classification;text representation;short texts;word senses;knowledge extraction;knowledge extraction
database;semantic features;similarity measure;semantically related;semantic model;text clustering;clustering techniques;statistical analysis;clustering quality;text mining techniques;text clustering
ranking scheme;support vector machines;nearest neighbor;data mining method;classification;information management;graphical model;classification performance;text features;classification results;model trained;biomedical research;pre-processing;conditional random fields;neural networks
classification rule;link-based;store data;classification;class label;related entities;classification methods;target object;naive bayes classifier;accuracies;link-based;naive bayes classifiers;databases;link-based;benchmark datasets
large amounts of;classification;text documents;linear programming;text data;collaborative tagging;data mining;end users;distributed computation;unstructured text;classifier learning;data obtained from;knowledge extraction;text documents;end user;classification algorithm;classifier learning;large scale;applications involving;central server;digital libraries;classifier;web users
clustering;remote sensing;clustering approach;sensor data;sensor data
high-dimensional;negative examples;high dimensional datasets;filtering techniques;imbalanced data;classifier performance;feature selection;filtering techniques;feature selection;sampling techniques;data mining;feature selection
anomaly detection;performance guarantees;anomaly detection;counterpart;real-world;sufficient statistics;information loss
clustering;clustering;mutual information;heterogeneous information sources;database;large scale;database;clustering method;attribute based;textual information;vector space model;clustering methods;spectral clustering;graph clustering
itemset mining;graph mining;frequent patterns;social network;algorithm named;graph mining algorithms;quantitative attributes;pattern discovery
structured databases;closed sets;application domains;algorithm named;real world datasets;structured databases;obtained by combining;databases;efficient discovery of;data mining problem;structured data;correlated patterns
local patterns;global model;classification model;local patterns;data sets;data mining task;learning} framework;data repository
data values;knowledge bases;join algorithms;clustering technique;information from multiple sources;fuzzy clustering;join algorithms;databases;pre-processing;string similarity
software development;ranking techniques;data collection;fold cross-validation;real-world;large number of;cross-validation;case study;data collection;feature selection;development process;software quality
stock market;clustering algorithms;stock market;time series;clustering algorithm
pre-defined;multi-dimensional;data mining technique;relational model;spatio-temporal;multi-dimensional;continuous attributes;relational learning;real world;tree models
sensor networks
data point;text classification;nearest neighbor;high dimensional;real world data sets;semi supervised;cluster membership;text data;text document;fuzzy clustering;subspace clustering;semi-supervised;text classification;chi square;high dimensionality;classification approach;semi-supervised;subspace clustering algorithms;subspace clustering
domain experts;false positive;rule-based;high quality;data mining;true-positive
classification;input data;training data is;class label;classifier learning;decision process;building classifiers;classifier
domain specific;multi-step;concept hierarchies;user-generated;web sites;concept hierarchy;user-generated;collaborative tagging;domain specific
clustering;clustering results;density-based;spatial data;large-scale;clustering method;trajectory data;interesting patterns;multi-granularity;location-based services;multi-granularity
multi-agent systems;real-world;trajectory data;low-level;trajectory data;high-level;low-level
spatial datasets;additional constraints;hierarchical clustering;spatial objects;hierarchical clustering;large margin;application domains;objective functions;optimization method;cluster analysis;objective function
remote sensing images;classification scheme;classification;active learning;land cover;classification tasks;active learning;sampling based;classification accuracies;gaussian processes;gaussian processes;maximum likelihood;classifier;active learning algorithm;classification algorithms
feature space;spectral clustering;trajectory data;finding clusters;trajectory data;similarity measures
real dataset;clustering;raw data;trajectory data;matching method
data set;search algorithm;classifier;optimization problem;np-hard
temporal information;event sequences;temporal patterns;sequential patterns;application domains;real world datasets;clustering techniques;discover patterns;databases;numerical attributes;frequent pattern mining
databases;feature extraction;extraction algorithm;database;breast cancer;data mining;free text;information extraction;case study;missing data;addressing this problem;data mining;statistical significance;machine learning techniques;information retrieval;breast cancer
category-specific;training data;classification;classification;automatically extracting;machine learning;category-specific;supervised approach;customer reviews;classification task;learning method;customer reviews
domain knowledge;statistical properties;ontology-driven;domain-driven;interesting association rules;medical datasets;interesting rules;data mining;rule discovery;mining algorithm
rfid based;database;spatio-temporal;representation scheme;parameter tuning;graphical interface;user-friendly;spatio-temporal
similarity assessment;similarity assessment;data analysis;matching scheme;multi-source;road network;spatial data mining
united states;real-world;simulated data
real-world datasets;discovery algorithms;complete set of;moving object;discovery algorithm;moving object;false dismissal
time series;prediction problem;classification;classification;semi-supervised learning;time series;regression models;skewed;time series;regression model;predict future;semi-supervised;graph regularization;classifier
high-speed;clustering;classification;data streams;distributed data streams;data streams;interactive visualization;decision making;high speed
frequent subgraph mining;weighted graph;pattern mining;knowledge discovery;labeled graphs;databases;pattern discovery;weighted graphs
data describing;data sets;noisy data;models built
tree-based;pattern tree;tree-based;real data;regression tree;algorithm called;classification tree;missing data;data mining;missing data;missing values
ensemble learning;single-class;classification;class imbalance learning;medical diagnosis;classification performance;condition monitoring;real-world;single-class;class imbalance learning
real-world datasets;outlier detection;data distributions;detection method;detecting outliers;data description;support vector
knowledge sources;microarray data;database;probabilistic model;ranking list;external knowledge;gene ontology;biologically relevant;biologically relevant;additional information;gene selection;gene selection
heterogeneous information sources;highly heterogeneous;information services;information sources;land cover;domain specific;data mining approach;ontology mapping;information services;semantic interoperability
shortest paths;clustering algorithms;planning problem;clustering algorithms;clustering process;density-based clustering algorithm
data analysis;bayesian network learning;data warehouses;database;spatial correlations;trajectory data;user interface;visual analysis of;dependency analysis;mobile phone;network model;bayesian networks;ad hoc
factors affecting;structural properties;large number of;sales prediction;case study;moving average;accuracies;sales prediction;context aware;sales prediction;context aware
real data sets;classification;classification;database;decision tree;emerging patterns;privacy preserving;higher accuracy;privacy preserving;information loss
high utility;clustering;public data;private data;database;information-theoretic measure;privacy-preserving;data mining;primary goal
decision tree classifier;decision trees;data updates;privacy guarantee;prediction accuracy;decision tree;differentially private;sum queries;differentially private;data sets;privacy-preserving;decision tree classifier;existing database
probability distribution;graph model;differentially private;knowledge discovery;network structures;graph databases
data miners;database;data miners;perturbed data;sensitive information;additional information
random variables;sample size;theoretical results;statistical analysis;data mining;hypothesis testing;personal information
real-world;video data;multimedia data mining;video content
semantic annotation;image datasets;image data;similar images;search engine;image annotation;image data;image annotations;web images
image retrieval;image retrieval;sparseness problem;maximum likelihood estimation;language model;smoothing methods;image representation;language model;information retrieval;content based image retrieval;visual words;databases;web page;model estimation;retrieval applications
search engine;video search;million images;large-scale;quality assessment;multimedia information retrieval;image search;semantic concepts;million web pages;image annotation
computational efficiency;multiple instance learning;unlabeled data;level features;multiple-instance learning;optimization procedure;multi-instance;content based image retrieval;sparse representation;human effort;limited number of;learning problems;labeled examples;content-based image retrieval;multiple instance;labeled examples;data distribution
large-scale;web-based;concept learning;concept-based;video search;text-based;semantic concepts;concept detection;concept detection;video retrieval;statistical models;visual learning;information source
relevant features;rough set;negative examples;unlabeled examples;video retrieval;partially supervised;positive examples
local geometry;training set;real world;image annotation;data manifold;learning algorithms;data representation;cross-domain;cross-domain;intra-class;image datasets;training samples;subspace learning;discriminative information;image annotation;margin;labeled data
visual search;web service;visually similar;cross-media;textual information;web services
visual content;image collection;social networks;common interests;accurate predictions;social relationship
data-driven;regression trees;continuous data streams;dependent) variable;regression tree;algorithm produces;temporal data;benchmark data sets;prediction models
database systems;user specifies;database;microsoft sql server;ad hoc;microsoft sql server
special characteristics;systems support;database;intermediate results;database;query plan;cost-based;ibm® db;base tables;commercial database;basic concepts;key features;estimation errors
database administrator;dynamic programming;database management systems;solid state;tool called;storage capacity;materialized views;ibm db;random access;solid state;hard disk;database engine
xml processing;parallel processing;xml data;xml documents;data integration;databases;computational model;database engine;data integration
web applications;open-source;query language;xquery implementation;optimization techniques;application scenarios;programming language
xml storage;xml indexes;21;schema-based;object-relational;database;xml data management;xml documents;query processing in;data management;wide range;xml schema;schema evolution;data-centric;schema information;xml schema;query processing
relational database system;parallel execution;window functions
join algorithm;memory bandwidth;hash-based;database;multi-core;analytical models;highly optimized;data skew;data sizes;join algorithms;wide range
parallel database;complex queries;data volume;large enterprises;query response time;large number of;algorithm called;data skew;management systems;business intelligence;database systems;skewed;skewed;parallel dbms;load balancing
reusability;database systems;database;massively parallel;user-defined;query plan;database functionality;user-defined functions;highly scalable;programming paradigm
data manipulation;programming model;map-reduce;data sets;high-level;readability;low-level;open-source
regression trees;classification;training data;parallel computing;classification;algorithms require;computational advertising;massively parallel;large datasets;massive datasets;data mining task;scalable distributed;computing infrastructure;real world;tree learning;learning task;tree models;distributed computation
real-world datasets;real-world scenarios;data mining;search strategy;frequent-pattern mining;mining process;data sets;minimum-support;support threshold;user groups;association rules;frequent patterns;minimum support;real customer;data characteristics;performance bottlenecks;real-world;frequent-pattern;modern database;result set;user-defined;frequent-pattern mining algorithms;mining algorithm
required information;business data;optimization methods;heterogeneous data sources;business objects;business intelligence;source data;databases;database schemas
data storage;data residing;database systems;data warehousing;query processing;transaction processing;index compression;total cost
scientific discovery;querying capabilities;database
parallel database;data analysis;big data;data warehouses;database design;database;statistical techniques;data acquisition;business intelligence
lessons learned;design decisions;data bases;web server;xml-based
data bases;database management systems;international conference on;database;data volumes;storage systems;data storage;high-resolution;data management;multimedia applications
web data extraction;case study;application area;data warehouse;web sites;data extraction;extracted data;data cleansing;high scalability;web data extraction;massive amounts of data;data flow;highly scalable
information discovery;web search;federated search;deep web;deep web;general-purpose;web crawlers
statistical properties;machine learning;query processing;query plans;query processing techniques;data characteristics;classifier;main idea
key features;data model
data source;large-scale;decision-making;large number of;traffic monitoring;streaming data;real data;5;moving objects;network traffic;monitoring systems
data dependencies;database;database;minimal overhead;relational database management system;oracle database
domain-specific knowledge;low-overhead;monitoring data;vice versa;database;machine-learning techniques;databases;database administrators
error-prone;data independence;access control;database;relational database management systems;views defined;database schema;application programs
accessing data;spatial-temporal;storage layer;compression techniques;data sets;storage systems;real world;location based services;indexing technique
declarative query language;stream processing;stream query processing;complex event processing;query processor;operator;key features
workload management;database;online-transaction processing;business intelligence;databases;workload management
query processors;stream processor;database systems;data warehouse;main-memory;databases;aggregate queries;view maintenance;query plan;high-volume;database updates;data streams;wide range;commercial dbms
interactive exploration;user queries;web services;missing information;local database;knowledge base
parameter tuning
document retrieval;user interface;keyword-based;user queries;domain-specific;search engines;faceted search;relevant entities;aware search;knowledge base;natural language
pre-defined;rewriting algorithm;generation algorithm;generation process
domain experts;end users;extensible framework;evaluation strategies;real-world;performance tuning;matching methods;instances;user interface;wide range;user intervention;ontology alignment
data source;sql queries;link discovery;data items;data sources;relational data;assists users;high-quality;real world scenarios;web interface
domain ontology;enterprise-wide;master data;reasoning capabilities;enterprise applications;data management;semantic web technologies;data management
data objects;data structures;large enterprises;object-oriented;online auction;storage systems;databases;structured data;data integration
real-life;master data;web data;generated dynamically
modeling tool;database;data exchange;data transfer;computationally expensive;optimization criterion
human activity;probabilistic models;indexing techniques;stream processing;sensor data;uncertain data streams;text streams
np-hard;workflow views;workflow views
anonymization techniques;database engines;impact analysis;privacy threat;information loss
information-network;information network analysis;data analysis;database;data exploration;4;text data;1;3;2;text databases;text database;databases;information network;structured data;online news;heterogeneous information networks;scientific databases
map-reduce;open-source;3;growing rapidly;business intelligence;prohibitively expensive;large data sets;data sets;low level;programming model
multimodal data;event based;multiple queries;heterogeneous data;large database
low overhead;network applications;ad hoc networks;resource-constrained;mobile devices;specifically designed to;ad hoc;social networking;small-scale
workflow systems;edges represent;parameter settings;nodes represent;4;data flow
web-sites;query evaluation;vast number of;optimal algorithms;goal-oriented;web-site;navigation paths
web search;web-content;structured data;wide range;large amounts of
10;databases;web sites;semi-structured data;database;data warehouse;relational database systems;map-reduce;web-scale;large data sets;3;2;5;7;6;9;8;sequential scan
user community;database architecture;database architecture
web portals;data fusion;scientific data;data sources;11, 13;data integration;enterprise data;data management applications
rates;data analysis;human knowledge;social science;data visualization
web documents;web interfaces;information retrieval techniques;data sources;databases;schema information;increasing importance
search algorithms;efficient approximate;existing indexes;estimation techniques
computing infrastructure;information sources;information theory;data management;inter-related;computing power;query results;data management;information gain
attribute values;database systems;large-scale;database;data-intensive applications;traditional database systems;column-oriented;column-oriented;database table
conflicting information;relational queries;database;multi-agent;database applications;formal model for;query language;database content;base data;databases;databases;data repository;formal semantics;synthetic data
speech recognition;index construction;nearest neighbor;high dimensional datasets;distance functions;queries efficiently;tree-based;time series;distance measures;similarity search;indexing methods;6;distance functions;query answering;metric spaces;range queries
distance computation;pattern recognition;synthetic datasets;edit distance;lower bounds;graph data;graph databases;np-hard
high-dimensional;multi-valued;data structure;inverted lists;publish/subscribe systems;emerging applications;information retrieval;16;online advertising;publish/subscribe system;arbitrarily complex;normal form
stream processing;stream processing systems;open-source;end users;large volumes of;real datasets;continuous queries over data streams;user queries;query containment;publish/subscribe system;data dissemination;query result
xml updates;access control;xquery update;schema-based;xquery queries;static analysis;materialized views;schema information;concurrency control
continuous query processing;semantic issues;data exploration;stream data;sensor networks;data stream management system;location-based services;data streams;base data;query results;focus primarily on;continuous queries
multi-pass;skyline algorithms;lower bound;synthetic datasets;average case;streaming model;skyline computation;partially ordered;worst case;pre-processing
data collected from;statistical queries;storage space;frequency domain;scientific computing;data centers;time series;sparse representations;sensor networks;multi-scale;raw data;compressed data
real data sets;search space;dimensional space;efficient computation;business intelligence;highly ranked;optimization techniques
search terms;user ratings;synthetic data;data exploration;textual information;refinement process;textual data;user generated content;blog posts;domain-specific;query-driven;real data sets;storage requirements;faceted search;result set;query result;query expansion;convex optimization
clustering;database;tree-based;query refinement;large number of;data set;refinement step;computationally expensive;traditional clustering;result set;query results;user study;result quality;standard database
synthetic data sets;power consumption;large number of;data reduction;data set;real-life;filtering techniques;line segments;piecewise linear
sensor readings;data collection;query-processing;frequent updates;real-world;tree-based;error guarantees;answering queries;sensor networks;sensor data;communication costs;query results;quality guarantees
multi-version;real-world scenarios;approximation methods;energy-efficient;range query;data items;query processing in;sensor networks;query processing;energy efficiency;sensor network;error bounds;data access
keyword queries;matching methods;instance-based;data warehouse;data providers;search engine's;large number of;schema-based;instances;data sources;fall short;matching process;schema information;data feeds;schema elements
human intervention;propagation model;serializability;relational data
sequence alignment;fixed-length;similarity measure;retrieval accuracy;database;large databases;candidate matches;reference-based;edit distance;sequence databases;efficiently identify;reference-based;queries efficiently;pruning power;query length;query sequence
efficient parallel;multiple data streams;multi-stream;analysis reveals;real-world;counting algorithm;multiple streams;network monitoring;stream processing;performance degradation;data sets;data stream;click stream;frequency counting;rates;multiple sources
operator;continuous queries;data processing;data stream processing
hybrid approach;explicitly model;plans;event sequences;query plan;fixed-point;stream queries;query plans;operator;query result;stream systems;pattern-matching
high scalability;high cost;low cost
lock;database technology;database;lock;multi-core;traditional database;data type;concurrency control;data management
complex queries;plans;data warehouses;query engines;large-scale;concurrent queries;large data sets;data analysis;commercial systems;join queries;join operator;query engine;execution times
ranking techniques;human supervision;statistical model
xpath queries;14;context-free;web services;xml data sources;potentially infinite
search results;query results;web search;np-hard;exploratory search;search result;limited number of;information exploration;error prone;structured data;keyword search on
complex queries;automatic extraction of;web query interfaces;web query interfaces;database;hierarchical approach;query interface;web interfaces;query interfaces;web page;web source;tree structure;domain-independent;hierarchical representation;database integration;extraction algorithm;web crawling
web documents;spatial proximity;text retrieval;taking into account;location-aware;web information retrieval;takes into account;spatial web objects;geo-spatial;search space;efficient retrieval of;geo-referenced;text documents;excellent performance
involving multiple;web databases;theoretical framework;user interfaces;query interface;semantic relationships between;query interfaces;schema matching;real data;real estate;approximation algorithm;web interface
memory constraints;index structure;mobile devices;cost model;update operations;sensor nodes;key features;memory management
query execution;plans;query optimizer;multi-core;query execution times;query plans;database performance;join operator;data structures;data sets;hash table;databases;access patterns;concurrent queries
processing units;processing units;table scan;multi-core;highly compressed;main memory;database systems;operator;data structures;compressed data;power consumption;highly distributed;data warehouse systems;table scans;column-oriented;hardware architecture;database engines;multi-threaded;data volume
large number of;extraction techniques;similarity functions;document collections;exact match;entity matching;similarity measures
matching methods;similarity metrics;data sources;efficiently identify;matching records;functional dependencies;special case;record matching
query execution;query result;randomized algorithms;database;skewed;join query;disk-based;join queries
selection predicates;real data sets;data points;data streams;data set;desirable properties;data sets;sensor networks;fundamental problem;operator;web page;resource constraints;distributed data streams
approximate answers;online aggregation;sample size;confidence intervals;online aggregation;random samples;high quality;aggregate queries;processing nodes;computationally expensive;hash table;future queries;decision making
utility function;query workload;cluster membership;large-scale
multi-version;data integrity;probabilistic model;isolation level;predictive model;parameter settings;configuration parameters
database;lock;database records;lock;transaction management;storage manager;database workloads;database engine;transaction processing
large scale distributed systems;application-level;database workloads;finer-grained;distributed systems
user preferences;uncertain data;decision-making;application domains;probabilistic databases;query processing in;markov networks;optimization problem;large datasets;ranking functions;multi-criteria;ranking function
np-hard problem;optimization problem;real-life;string transformations;similarity functions;approximation algorithm
statistical properties;dynamic programming;management systems;uncertain data;probabilistic information;approximate query processing;approximate query answering;error metric;probabilistic data;squared error;probability distribution function;modern database;error metrics;query planning;query results
ranking algorithm;increasing number of;data model;user requirements;web application;share common
web portals;true values;conflicting information;synthetic data;integrating data from;scientific data;large number of;data sources;data integration systems;real-world;enterprise data;multiple sources;data management applications
erroneous data;true values;synthetic data;bayesian model;integrating data from;dynamically changing;information management;data sources;data integration systems;data item;high accuracy;real-world;hidden markov model
4, 5;attribute values;real data;real-world
web pages;completeness;digital content;web sites;scheduling strategies;great potential;data quality;business analysts;web site;rates;web archives;quality measures;data quality
duplicate records;detection algorithms;duplicate detection;input data;relational queries;data quality problems;duplicate elimination;instance;parameter settings;data processing;clustering
preference relations;high accuracy;np-complete;discovery algorithm;discovery problem;skyline queries;databases;query result;preference relation
high potential;density-based;density based;critical task;clustering method;high quality;clustering accuracy;data sets;clustering methods;dynamic networks;temporal smoothness;information theory
databases;distance function;database;random walk;information content;transition probabilities;higher accuracy than;relational database;relational databases;metric space
ip network;multiple sets of;database;data sets;data sources;sampling framework;numeric attributes;processing queries
set similarity join;set similarity join;frequent patterns;mining process;power-law;size estimation
greedy algorithm;approximation algorithms;data summarization;dynamic-programming;real-world;approximation quality;summarization techniques;general-purpose;histogram construction;hierarchical structure;space requirements;error metric;error-bounded;real-world data sets
11;10;database;query execution;minimum number of;long queries;adaptive approach;distributed database;concurrent queries;range queries
data-driven;data analysis;temporal locality;general purpose;database;large-scale;tree-structured data;labeled trees;frequent subtrees;real world applications;load balancing
large numbers of;multi-core;access latency;main memory;data freshness;query processing techniques;data-stream processing;relational table
clustering;distance measure;attribute values;summarization methods;automatically learn;large graph;topological structure;existing graph;clustering techniques;graph clustering;clustering methods;theoretical analysis;clustering algorithm;graph clustering;structural similarity
sampling approach;frequent patterns;sampling based;frequent subgraphs;pattern mining;metropolis-hastings;large graphs;graph mining algorithms;graph patterns;sampling strategies;databases;sampling framework
clustering;mining algorithms;classification;reduction techniques;great success;pattern mining;subgraph isomorphism;large graphs;existing graph;graph patterns;social networks;structured data;probabilistic guarantees;main idea
group members;group recommendation;data set;amazon mechanical turk;group recommendation;share similar;information exploration;efficiently computing;interesting items;user study;formal semantics
complex queries;graph data;large quantities of;social network data;social networks;high accuracy;anonymized data;social networking
individual users;optimization framework;efficient search;real-life;large scale;incrementally maintaining;social annotation;large data sets;annotated data;highly scalable;keyword query;social annotation
data publishing;data publishing;external sources;census data;high utility
query answers;completeness;data updates;ad-hoc;query workloads;query answers;dynamic databases
multi-level;privacy guarantees;theoretical foundation;privacy preserving data mining;anonymized data;random perturbation;random perturbation
distance measure;finding similar;high-quality;highly scalable;database;related information;time series;time series;algorithm called;refinement step;sensor data;databases;efficient similarity search;real world;fast computation;typically requires;lower bounds;computationally expensive
hash-based;synthetic data;uniform distribution;query processing;databases;modern hardware
query execution;theoretical properties;query workload;query feedback;prohibitively expensive;histogram construction
disk resident;query processing;massive graphs;sampling based;main memory;index structure;maximum-flow;large graphs;random access;social networks;disk-resident;communication networks
large numbers of;density-based;input data;moving object;applications ranging from;large number of;pattern mining;traffic monitoring;streaming data;clustering process;parameter settings;streaming applications;rates;shared execution;query workload;data streams;test cases;memory resources;memory space;times faster than
join processing;pruning strategies;distance-join;data management problems;synthetic datasets;shortest-path;database;search space;large graph;selection method;join order;reachability query;distance-based;vector space;graph embedding;graph databases
real datasets;multi-criteria decision making
processing units;data processing;field-programmable gate arrays;general-purpose;computation model;power consumption;data processing
data analysis;mapreduce-based;high-end;parallel databases;fault tolerance;databases;unstructured data;data management applications
quality metric;partition-based;medical databases;query logs;set-valued data;market basket data;anonymization techniques;search logs;databases;set-valued data;information-loss;query engine
social security;algorithm performs well;data management;privacy preserving;social networks;algorithm (called;data mining problems;personal information
attribute values;sensitive attribute;target distribution;query answers;data sets;ad hoc
query answering over;query optimization;sufficient conditions;rewriting queries;knowledge bases;database;data exchange;implication problem;general case;instances;database instance;data integration
plans;query optimizer;intermediate results;optimal plan;execution plans;query optimizers;estimation errors
cardinality estimates;query optimization;query execution plan;optimizer;plans;query optimizer;microsoft sql server;prohibitively expensive;cardinality estimation;subexpressions
instance;schema mapping;sql queries;data exchange;schema mappings are;source schema;target instance;instances;linear order;database;unlike prior;algorithms for computing;source instances;schema mappings;tuple-generating dependencies
10;14;mapping language;data exchange;mapping rules;2;tuple-generating dependencies;schema mappings;metadata management
xml fragments;object-oriented;mapping language;xml data;object-based;data model;object-oriented;vice-versa;query results;query translation;xml schema;object representation
privacy-aware;service provider;road networks;position information;mobile user;privacy-aware;analytical models;communication cost;service providers;location-based services;key features;query-processing cost;mobile users
algorithm iteratively;synthetic data;database;preference queries;personal preferences;single object
database replication;high cost;total order;database servers
large sample;web pages;semantically meaningful;multi-column;large number of;relational tables;domain-independent;high accuracy;html tables;missing information;multiple sources
attribute values;deep-web;data cleaning;integrating data from;structured data sources;provide feedback;keyword query;data sets;data integration systems;structured information;databases;operator;html tables;integration process;web page;structured data;source databases;data integration
rewrite rules;database schemas;schema mapping;schema mappings are;data exchange;database research;aggregate queries;query answering;algorithms for computing;high-level;schema mappings;data integration;tuple-generating dependencies
data objects;synthetic data sets;information systems;nearest neighbor;road networks;query efficiency;queries efficiently;knn queries;pre-computed;emerging applications;7;counterpart;shortest path;location-based services;dynamic environment;real world;knn) queries;nearest neighbors
nearest neighbor;spatial database;algorithm called;real life applications;problem called;times faster than
nearest neighbor;computation cost;safe regions;communication cost;location update;nearest neighbor queries
web archives;web directories;web pages;data mining;addressing this problem;web sites;web archive;algorithm called;real data;business analysts;web site;web site;web archives;association rules;discovered rules
service provider;mining results;database;association rule mining;theoretical foundation;mining process;probabilistic guarantees;mining results;frequent itemset mining;frequent itemsets;computational resources;data owner
training data;naïve;classification accuracy;real-world;original data;bayesian classifiers;sufficient conditions;bayesian classifier;computationally expensive;classifier
workload-aware;performance bottlenecks;query loads;learning phase;index structures;skewed;workload-aware;query cost;moving objects;similar characteristics;location-based services;frequently updated;update-intensive;data management;communication networks
inference method;nearest neighbor;moving-object;real-world;index structures;moving object;highly dynamic;moving objects;query processing;location-based services;query results
clustering;access paths;relational query processing;access method;maintenance costs;large number of;cost model;wide range;4;tree index;data structure called;functional dependencies;sequential scan;correlated attributes;small size
physical design;database administrator;ibm db;tuning tools;database
database systems;database;large number of;adaptive sampling;parameter settings;usage scenarios;query plans;database administrators;configuration parameters;database systems
massively multiplayer online games;database technology;simulation model;durability;high-end;main-memory;main memory;massively multiplayer online games;rates;virtual worlds
clustering;clustering algorithms;ground truth;similar objects;detailed comparison;synthetic data sets;clustering high dimensional data;data sets;clustering result;projected clustering;subspace clustering;dimensional data;real world;evaluation measures
duplicate records;clustering algorithms;duplicate detection;detection algorithms;data cleaning;real-world;large databases;data quality;join algorithms;approximate join;general purpose;highly scalable;record linkage is
data bases;international conference on;international conference on;database theory;international workshop on;edbt/icdt;data management;database technologies;database theory;data management;database researchers
user communities;users' interests;social networks;information exploration;xml technologies
large scale;database replication;mathematical models;distributed systems;replication
data management systems;access control
data objects;large-scale;large scale;large number of;hash table;distributed data;data management
maintenance cost;long-term;5
approximate answers;hash-based;join results;query workload;distributed processing;hash table;performance gains;network traffic;join queries
network resources;keyword search;data transfers;replication
database replicas;database;large-scale;replication;databases
query evaluation;structural join;main memory;pre-computed;data set size;fast computation
open-source;finite state machine;xml database;combination function;storage overhead;hash function
24;clustering problem;incremental algorithm;main memory;storage scheme;encoding schemes;xml data;query processor;data clustering;index entries;incremental clustering;storage structure;xpath expression;disk page
discovering frequent;taking into account;highly heterogeneous;extensible markup language;extracting knowledge from;xml documents;data sources;xml document;association rules;biological data
15;takes into account;xml data;xml streams;xml queries;relative accuracy;data streams;query results;stream systems;relational data
xquery engine;6, 7;web interface;database
bayesian network;database;static databases;3;2;bayesian approach;sensitive values;statistical databases;4
mining results;frequent itemset;fundamental properties;instance;np-hard;data quality;frequent itemset mining;frequent itemsets;association rules mining;association rules;frequent item-set;real world;special case;association rule;transactional data
attribute values;theoretical framework;mutual information;information-theoretic;information theory;anonymized data
ad hoc;conceptual model
cluster based;partition based;privacy-preserving;database management;local density;database;real dataset;encrypted data;false positive;clustering algorithm based on;data distribution
service provider;attack model;relational information;databases;query logs;personal information
13;retrieval techniques;efficient indexing;data exchange;query evaluation;mobile devices;query processing in;data access;service discovery;resource constraints;information exchange
business processes;tedious task
systems require;matching techniques;heterogeneous data;original data;real-world entities;aware query processing;query processing
association mining;collaborative filtering;commercial dbms;machine learning;knowledge discovery;data sets;data warehousing;discrete data;real world;information retrieval;market basket analysis;relational data
error-prone;database administrator;control policy;access control;database applications;data interchange;data integration;user access;xml data;relational data
required information;sensor readings;redundant information;data stream management;data fusion;query processing;real world;change frequently;sensor fusion
clustering;clustering;relational structures;web graphs;online users;large graphs;relational information;user communities;information technology
sentiment analysis;recommender systems;recommender systems;large-scale;large-scale datasets;user opinions;recommendation methods;kdd 2007 workshop;data mining;kdd workshop;opinion mining
recommender systems;rating data;user opinions;user-behavior;sentiment analysis;opinion mining
matrix factorization;data set;interpretability;clustering model;collaborative filtering
filtering systems;collaborative filtering;recommender systems;user / item;individual preferences;decision making
matrix factorization;recommender systems;similarity measure;prediction accuracy;large-scale;regression problem;neighborhood information
network analysis;music recommendation;collaborative filtering
matrix factorization;recommendation systems;recommender systems;prediction accuracy;test instances;real-life;matrix factorization;matrix factorization methods
real-world datasets;classification;knowledge discovery in databases;outlier detection;machine learning;outlier detection;detection methods;support vector machines;estimation methods;local-density
ranking algorithm;ranking algorithms;ranking list
regularization;machine learning;parameter tuning;benchmark datasets;supervised classification
cross-validation;data sets;breadth-first search;expected error;state space search;cross-validation;data sets;hidden markov model;model selection;computational complexity;learning problem;optimal parameters;computationally feasible;selection algorithm;exhaustive search
feature space;multi-stream;real data sets;optimization algorithm;linear combination;hidden markov models;multi-stream;classification error
discriminant analysis;classification;regularization;discriminant analysis;classifier;classification performance;local similarity;similarity-based;svm classifiers;classifier;local similarity
database;intermediate results;feature extraction;classifier;simple algorithm;support vector machines;character recognition
frequent itemset mining;association rules;formal framework;interesting rules
11;10;decision trees;classification;storage space;class association rules;significant rules;great flexibility;classification method;class association rules;associative classification;quality measure;unstructured data;class prediction
fusion method;nearest neighbor classification;classifier fusion;classification;dimensionality reduction method;feature fusion;classifier;fusion methods;dimensionality reduction methods;nearest neighbor classification;principal component analysis;dimensionality reduction methods;dimensional data;optimal number of;dimensionality reduction;information gain
clustering;learning process;search space;path planning;learning algorithm;large number of;problem domains;compact representation;action space;decision process;clustering framework;high probability;action sets;decision processes
virtual world;visual representations;plans;natural language
fusion method;weighted average;loss function;input data;main components;classification methods;data sources;context-dependent;context dependent;real world;individual classifiers;classification error;classification algorithms
frequent itemset;feature vector;fixed-length;data sets;structured data;classifier;relational data;machine learning methods
minimal cost;minimal cost;search algorithms;instance-based learning;problem instance;search tree;optimization problems;search strategy;instances;problem instances;limited-memory
tree induction;average precision;decision trees;large margin;decision trees;correct answer;question answering;ranking quality
clustering;clustering algorithms;high-quality;synthetic datasets;mixture model;real world datasets;algorithm called;selection algorithm;stable model;model selection
clustering;clustering;spectral clustering;generative model for;distance measures;semi-parametric
hard disk;decision support systems;false alarm;black box;highly accurate;performance degradation;rates;high accuracy;rule based;real world;interpretability;prediction task
input parameters;matlab/simulink;fuzzy logic;fuzzy logic
multi-strategy;selection problem;learning strategies;machine learning;learning strategies;learning strategy
data set;artificial neural networks;artificial neural networks;artificial neural network;principal component analysis
clustering;phase transitions;class information;local optima;real-world;automatically determines;clustering method;clustering methods;cost function;clustering algorithm;relational data
sampled data;classification models;classification;binary class;imbalanced data;complete set of;models built;data preprocessing;feature selection;imbalanced data;attribute selection;software quality;prediction models;feature selection
ranking techniques;auc;feature ranking;prediction problems;fold cross-validation;feature ranking;support vector machines;naıve bayes;logistic regression;classifier performance;multi layer;nearest neighbors;feature selection
feature extraction;specific features;feature vector;features extracted;domain specific;recognition accuracy;handwritten digits;domain-specific;feature sets;handwritten digits;character recognition;recognition problem
unsupervised) learning;correlation-based;classification;document classification;related data;learning task;prior knowledge;knowledge transfer;learning tasks;machine learning;information extracted from;knowledge extraction;feature generation;knowledge transfer;classification errors;document classification;transfer learning;supervised) learning
classification task;semantic information;generalization ability;semantic role labeling
rule sets;context free;satisfiability problem;context free;normal form;learning method
data objects;unlabeled data;label propagation;graph data;real-life datasets;multi-relational;labeled data;laplacian matrix;class labels;label propagation;label propagation
multi-class problems;structured prediction;learning problems;vector machine;large margin;complex objects;prediction problems;algorithm exploits;margin;higher order;structured prediction
comparative analysis;synthetic data;support vector machines;classification accuracy;linear programming;minimum number of;quadratic programming;classification performance;generalization performance;support vector machines;optimization criteria;margin;support vectors
decision function;classification;optimization problem;support vector;support vector machines;classifier
state space;real-world;reinforcement learning;reinforcement learning;pattern selection;real world
global solution;accuracies;classification tasks;learning algorithms;optimization problem;image data;algorithm learns;text data;large datasets;margin;classifier;automatically learn;synthetic data
sensitivity analysis;collect data;markov decision processes;build models;model parameters;partially observable;decision making;theoretical bounds
dynamic programming;feature set;specifically designed for;feature sets;reinforcement learning;learning tasks;machine learning;automatic feature selection;reinforcement learning;feature selection;dynamic bayesian network;markov decision process;feature selection
inverted pendulum;test case;reinforcement learning;continuous state;problem domain
state space;reinforcement learning;potential function;domain knowledge into;reinforcement learning;knowledge-based;scalability problems;reinforcement learning;assignment problem
multiple agents;action models;transfer learning;reinforcement learning;hybrid model
classification model;collaborative filtering;real-world;support vector machines;information retrieval;problems arise;lessons learned;wide range;ranking methods;real world applications;machine learning;power grid
clustering;clustering;naïve bayesian;mobile devices;mobile devices;supervised approach;purity;dimensional data;log data;clustering approach;naive bayesian
training data;conditional random field;spatial relationships;boosting algorithm;conditional random fields;data collected from;probability distribution;model parameters;local information;belief propagation;classifier
radio frequency identification;unsupervised learning;spatio-temporal;rfid-based;temporal behavior;temporal behavior;noisy data;data mining;data mining method
regularization;systems support;neural networks;neural network model;unknown environments;input-output;neural network
feature extraction;classification;classification;feature extraction;vector machine;low-power
approximation techniques;matching algorithm;human activity;activity recognition;time series;data sets;increasingly large;long-term;high-resolution;piecewise linear
selection problem;neural networks;user intervention;learning paradigm;grid computing;scheduling algorithm;resource selection;learning speed;resource selection;computing power;selection algorithm;neural networks
computational complexity;fast convergence;algorithm converges
intrusion detection;rfid based;intrusion detection;real-world;decision-making;decision making;decision- making;access control;classification problems;data generated from
classification;genetic algorithms;multi-layer;neural networks;classification;vector machine;accurate classification;high sensitivity;fuzzy inference system;support vector machines;classifier
control method;spectral clustering;feature selection algorithms;microarray data;microarray datasets;large number of;feature selection algorithms;feature selection;informative features;gene expression
microarray data analysis;training dataset;separability;features selected;nearest neighbour;classifier trained on;separability;optimal number of;classifier
clustering;gene expression;confidence level;gene expression
clustering;training data;feature vector;classification;gene expression;greedy approach;feature subsets;feature selection;individual features;gene expression;nearest neighbors
clustering;unseen data;classification;discriminative power;dimension reduction;specific set of;basis function;cross-validation;rbf) neural network;feature selection;vector machine;dimension reduction;classification accuracy;gene expression;objective function;fold cross validation
high-order;bayesian logistic regression;predictive power;high-order;logistic regression model;bayesian logistic regression
prediction accuracy;96.67%, 97.41%;plans;fold cross-validation;input features;flow rate;large number of;machine learning;0.41%, 0.43%;prediction error;machine learning
ct images;regularization;training images;principle component analysis;ct) images;prior knowledge;numerical experiments;prior information
database;learning algorithm;machine learning
training set;label information;takes into account;classification framework;linear discriminant analysis;high-dimensionality;linear discriminant analysis;dimensionality reduction techniques;classification accuracy;data dimensionality;principal component analysis;feature vectors
statistical methods;predictive models;high risk;machine learning techniques;machine learning;feature selection;machine learning techniques;classification algorithms
bayesian network;search algorithms;bayesian network learning;bayesian classifiers;planning systems;network structures
training images;learning algorithm;tracking algorithm;maximum variance;data set;image sequences;training data set;great promise
auc;support vector machines;bayesian networks;missing data;machine learning models;bayesian networks;missing data;missing data;support vector machines
probability estimates;svm models;vector machine;feature-ranking;statistical machine learning;support vector machines;prediction models
ct images;semi-automatic;image segmentation;semi-automatic;image segmentation
genetic algorithms;logistic regression;covariance matrix;data sets;rates;logistic regression;fitness;maximum likelihood;local minima;maximum likelihood;ant colony optimization
feature space;mutual information;naive bayes;document frequency;naive bayes;selection techniques;dimensionality reduction;high dimensionality;information gain;term space;decision theory;selection methods
binary classification;web server;classification;web server
clustering;consensus clustering;clustering results;gene ontology;mutual information;consensus clustering;database;clustering methods;optimization process;automatically determined;protein interaction networks
knowledge representations;semantic representations;knowledge representations;instance;correct answer;customer reviews
classification models;class imbalance learning;imbalanced datasets;bioinformatics datasets;real-world;real world;classification problems;class imbalance learning;classifier;model selection;positive class
medical applications;medical diagnosis;posterior probabilities;medical diagnosis;medical applications;medical datasets;cost sensitive;cost-sensitive classification;decision boundaries;classification problems;probability values
random projection;random projection;microarray data;database;gene expression;preprocessing phase;dimensionality reduction;machine learning algorithms;gene expression;dimensionality reduction;classification algorithms
clustering problems;hierarchical clustering;structured data;test data;similarity measure
linear models;online learning;scale poorly;concept detection;video retrieval;support vector machines;building block;support vectors
training set;high-level;classification method;machine learning techniques
acoustic features;classification;classification;content-based music;clustering method;classifier;low-level features;training data;bayesian classifier;audio features;low-level
hidden markov model;training set;machine learning methods;feature space;content-based retrieval;large margin;learning tasks;generative models;rates;structural information;prediction models;structured prediction
image retrieval;image database;image region;object-based;retrieval systems;relevance feedback;retrieval framework;clustering algorithm;image clustering;single object
classification problem;real-world;music retrieval;statistical models;audio features;low-level;retrieval applications
inference mechanism;rule-based;naive bayes;decision support system;classification algorithm
domain knowledge;classification;domain-independent;classification;training examples;graph-based;structure learning;instances;image understanding;subgraph-isomorphism;hand-crafted;probabilistic model;classification rate
object recognition;search algorithms;classification;optimization problems;data mining;uci data sets;factor analysis;nearest neighbor classifier;independent component analysis;data sets;search procedure;feature selection;principal component analysis;factor analysis;dimension reduction;search space
radial basis function;image analysis;empirical mode decomposition;radial basis function;empirical mode decomposition
expert systems
genetic algorithm;genetic algorithm
low cost;active power;inference mechanism;active power;rule-based;naive bayes classifier;decision support systems;decision support system;decision support system;active power
pattern recognition;latent variables;extracted features;data set;parametric model;regression model;extracting features from;latent variables
control method;multi-layer;neural network;neural network
power demand;large-scale;knowledge management;multi agent;knowledge extraction;multi agent;power demand
machine translation;training set;sparse data;text summarization;large number of;text understanding;information retrieval;instances;semi-supervised
unsupervised fashion;natural images;natural images;spatial relations;topic models
globally optimal;combinatorial optimization;graph-cut;target object;machine learning;kernel principal component analysis;object localization;graph cut;density function
face recognition;recognition algorithm;face recognition;face databases;original data;sparse representation;machine learning;sparse representation;sparse representation;distance based;classification algorithm;classification algorithms
ensemble learning;ensemble learning;gaussian mixture models
probabilistic framework;temporal structure;probabilistic models;discriminative information;probability distribution;correct classification;level features;rates;nearest neighbour;markov chain;classifier
handle complex;pattern recognition;neural networks;high dimensional;probabilistic models;high dimensional;input space;image classification
prediction problem;ranking problem;simulation study;induction algorithms;classification;decision tree;voting scheme;tree-based;1;2;rank aggregation
hybrid approach;hybrid approach;event logs;process model;process mining;process mining;data mining;process mining
naïve bayes classifier;rule-based;input parameters;inference mechanism;decision making;operating parameters;decision making
approximation methods;upper bound
geographic regions;geographic regions;discriminant analysis;ip network;classification;database;position information;classification methods;data sets;location information;user groups;location-based;location based services
genetic algorithms;classification method;classification;genetic algorithm;classification methods;classification errors;classification model;classification method;supervised learning algorithms;multi-criteria
clustering;clustering algorithms;taking into account;unsupervised learning;data structure;learning algorithm;multiple datasets;purity;learning strategy
video sequence;false positive;copy detection;information contained in;copy detection;video database;content-based video;true positive;spatial information
clustering;classification;classification;retrieval tasks;retrieval results;data set;hidden markov models;feature representation;nearest neighbors;data base;support vector machines;low-level
hybrid method;genetic algorithms;ant colony optimization;early stage;hybrid method;optimum solution
input variables;search space;classification problems;optimal set of
evolutionary algorithms;population based;global convergence;optimization methods;differential evolution;global optimization;convergence speed;rank based;cost function;rank based;real world;machine learning;dimension reduction;additional information
decision trees;decision tree;decision rules;dynamically changing;data instances;decision trees;generation process;data examples;rule based
gps data;control algorithm;multi-objective;success rate;evolutionary computation;fitness;evolutionary algorithm;human supervision;control algorithm
training data set;raw data;case study;user interface;real world;bayesian belief
text classification;semi-structured;information extraction;text categorization;feature selection
intelligent systems;artificial neural network
ranked list;predictive features;learning task;case studies;machine learning
evolutionary algorithms;parameter values;genetic algorithm;test cases;genetic algorithm;evolutionary computation;rates;evolutionary algorithm;statistical framework
inference method;graph structure;inference problem;combinatorial optimization;generative model for;post-processing;instances;loopy belief propagation;maximum weight;graph structure;combinatorial optimization problem
clustering;em) algorithm;data analysis;mixture model;normal distributions;real-world;random variable;maximum likelihood;ad hoc;mixture models;expectation-maximization
graph partitioning;link analysis;probabilistic model;real datasets;hierarchical clustering;generative models;probabilistic approach;network data;community structure;community structure
prediction intervals;confidence level;prediction intervals;regression model;linear regression
multi-level;markov logic networks;gradient-descent;probabilistic models;lower level;real-world domains;learning parameters;multiple domains
classification approach;machine learning techniques;density estimation;maximum entropy;machine learning
synthetic datasets;sliding window;sliding window;mining frequent;highly correlated;candidate-generation;data streams;rule based;correlated attributes
strong correlation;feature extraction technique;time series;multiple datasets;machine learning;time series;feature selection;discrete cosine transform;classification problems;time series
prediction accuracy;lower-dimensional;lower dimensional;classification;classification;data sparsity;machine learning;simulation results;high-dimensional feature spaces;data sets;ensemble classifier;high-dimensional feature spaces;feature spaces;classification problems;higher dimensional;high dimensional feature space;information loss
binary classification;ordinal data;classification;classification;ordinal data;real datasets;binary classifier;complex data;classifier

large sample;memory footprint;sensor readings;biased samples;data updates;random samples;random sample;computing devices;data stream;flash storage;fundamental problem;unique characteristics;perform poorly;energy-efficient;data management;data storage
frequent items;data stream mining;large scale;rates;high accuracy;data streams;modern hardware
search space;database systems;highly interactive;constraint language;real-world situations;physical design
graph structure;private data;graph properties;anonymization techniques;meaningful results;tabular data;graph data;bipartite graph;ad hoc
computational complexity;complete set of;information retrieval;iterative algorithm;desired accuracy;optimization techniques;graph-theoretic;worst case
database theory;program committee;database research;database technology;international conference on
intrusion detection systems;gaussian mixture model;decision tree;data sets;data set;data sets;detect anomalies
multi-dimensional;intelligent control;data processing
ontology-based;fuzzy set;ontology language;designed to support;event extraction;event extraction
risk analysis;factors affecting;supply chain;information systems
document classification;document classification;classification algorithms;document datasets;increasing amounts of;maximum margin;increasing attention;maximum margin;classification algorithm
conditional probability;bayesian learning;bayesian learning
domain knowledge;learning process;natural language processing;prior knowledge;graph based;semantic relationships between
text based;text categorization;text categorization;text classification;segmentation method;feature selection;feature selection;mutual information between;word segmentation;word segmentation
classification scheme;mixture models;classification
clustering;knowledge sharing;cluster based;factors affecting;knowledge sharing;knowledge creation;game theory;network structure;game theory
learning mechanism;knowledge creation;knowledge transfer;knowledge-based;knowledge transfer
private information
regression analysis;knowledge management;correlation analysis;knowledge management
knowledge management;factor analysis;inter-relationships;survey data;knowledge management
web-based;web log;development process;textual information;web site;web mining;web mining;web log;automatically identifying;web logs
1, 2;3-5;scientific datasets;visual representation;data mining;analysis tool;statistical analysis
fuzzy neural network
completeness;detection method
multi-agent systems;multi-agent system;genetic algorithm
hybrid method;hybrid method;comprehensive evaluation;comprehensive evaluation
grid computing;scheduling algorithm;grid computing;scheduling algorithm;scheduling strategy;workload management
working principle;operating conditions
theoretical analysis;1,2

numerical analysis;finite element;taking into account;numerical simulation;large-scale
text mining;starting points;case study;early stage;case study;product design;opinion mining
large-scale;semantic web;persistent storage;web data;persistent storage;relational database;relational database management system;persistent storage
operator;bayesian networks;optimization algorithm;bayesian networks;genetic algorithm
frequency domain;feature extraction;independent component analysis;original data;blind source separation;simulation results
collaborative filtering;web based;recommender systems;collaborative filtering;correlation coefficient;complex networks;absolute error;local similarity
computational complexity;collaborative filtering;recommender systems;large scale;collaborative-filtering;recommendation algorithms;computing platform
collaborative learning;knowledge management;virtual communities;virtual communities;knowledge management;virtual communities
multi-party;multi-party
subset selection;search algorithm;classification performance;vector machine;subset selection;data set;maximum margin;multi-class;classification algorithm
customer relationship management;classification;data mining;customer relationship management;data mining technology;data mining;clustering analysis
long term

document sets;matrix factorization;document set;document set
frequent itemset;mining algorithm;mining association rules;association rules;frequent itemset
hidden knowledge;case-based reasoning;data mining technique;discovered knowledge;information retrieval
wavelet analysis;time series;high accuracy
data flows;structural characteristics;meta-data;network data;diverse applications;similar characteristics
web pages;database;user interfaces;extraction techniques;extract information from;memory usage;key words
total number of;radio frequency identification;large-scale;large number of;multimedia information;personal preferences;wireless network;mobile agents
multi-relational data mining;database;data mining approaches;relational database systems;pattern mining;relational tables;inductive logic programming;computationally expensive;implementation issues;relational table
database;domain ontology;data model;domain ontology;transformation rules;conceptual model;relational database;domain ontology
hierarchical model
light source
unsupervised clustering;theoretical framework;classification;classification
clustering;clustering;svm algorithm;classification accuracy;differential evolution;vector machine;imbalanced datasets;training dataset;auc;data cleaning;imbalanced datasets;standard svm;standard datasets;differential evolution;positive class
machine translation;information extraction;context free;linguistic patterns;complex patterns;frequent subtrees;linguistic patterns;automatically discover
clustering;clustering;computation cost;probability density function;uncertain objects;clustering methods;data mining;uncertain objects;data management applications
network models;support vector regression;nonlinear regression;generalization error;training error;time series;time series;risk minimization;upper bound;complex data;svm models;forecasting accuracy;kernel function
image retrieval;aerial images;image retrieval;color space;detection method
human face;salient points
knowledge engineering;semantic networks;ontology-based;development environment;graph-based;directed graph;representation language;object-oriented;knowledge representation;logic-based
hybrid approach;sample data;kernel function;vector machine;kernel principal component analysis;principal components;efficiently extract;theoretical analysis;generalization ability;simulation results
hybrid model;genetic algorithms;support vector regression;neural networks;genetic algorithm;hyper-parameters;generalization ability;moving average;arima) model;support vector regression;predictive accuracy
content-based filtering;content-based filtering;recommender systems;taking into account;collaborative filtering;collaborative filtering;textual information;data sparsity;cold-start;demographic information;machine learning techniques;similar items;poor quality;recommender systems
simulation results
communication protocol;communication protocol
rbf neural network;genetic algorithm;genetic algorithm;neural network model;hidden layer;rbf neural network;neural network;artificial neural network
matching algorithms;data structure called;data mining task;prefix tree;real life and synthetic data;prefix tree;string matching
small sample;estimation method;data mining;high reliability;performance gains;data mining;large sample;high cost;estimation methods
knowledge workers
predictive models;information systems;information systems;data mining techniques;data mining techniques
poor quality;databases;neural network;classification;clustering approach
low quality;high quality;success probability
hybrid model;stock data;conventional techniques
long-running;filtering algorithm;filtering algorithm
intrusion detection;detection techniques;rules generated;outlier mining;data set;rule base;detection methods;simulated data;association rule;data mining based;outlier mining
data set;low frequency;neural network model;multilayer perceptron;wavelet analysis;forecasting model;high accuracy;high frequency
case study;original data;comparative analysis;impact factors
scheduling algorithm;video coding
scheduling problems;search algorithm
clustering;data-driven;hidden patterns;global consistency;data points;input data;real-world;manifold learning;data clustering;global consistency;data set;instance;basic assumption;manifold structure;image data;low-dimensional;clustering
user visits;web-based;web applications;web-based;web service;heterogeneous systems
great significance
unique features;bp) neural network;learning process;bp neural network;bp algorithm;neural network;predictive model;data stream;prediction model;neural network
prediction model;vector machine;vector machine
product design
regression analysis;business opportunities
grey relational;grey relational
service quality;quality assessment;grey relational
increasing number of;video-based;target detection;moving objects;video-based;noise reduction;real time monitoring;video-based;monitoring systems
comprehensive evaluation;web usage mining;web usage mining
trust management;related concepts;storage capacity;trust management
small-scale
counterpart;comprehensive evaluation;comprehensive evaluation;evaluation index system;principal component analysis
clustering;clustering;relevant data;dynamic analysis

utility function;utility function
analytic hierarchy process;evaluation model
lower computational cost;genetic algorithm;breast cancer;global optimization;data set;artificial neural network;breast cancer;artificial neural network
dimensional space;classification;decision tree;genetic algorithm;real data;tree structure;target variable;rates;upper bound;customer data
intrusion detection;network traffic;network security;security issues;classification
genetic algorithms;fuzzy logic;fuzzy model;fuzzy rules;short-term;fuzzy rules;short-term
clustering;web objects;clustering results;similar objects;synthetic data;web objects;noisy data;cosine similarity;clustering
error rate;machine learning;analysis tool;web data;machine learning
uniform distribution
ordering constraints;clustering;clustering algorithms;constrained clustering;instance-level;agglomerative clustering;clustering methods;hierarchical agglomerative clustering;instance level;ordering constraints;ordering constraints
clustering;color features;color space;clustering
simulation results;estimation method;image based;image based;estimation method

meta-search engine;agent based;general purpose;user's interests;large number of;search engines;agent architecture;dynamic content;meta-search engine;search results;web search engines
knowledge transfer;game theory;knowledge transfer
clustering;target class;data points;classification;line search;data sets;optimization technique;distance-based;clustering methods;supervised clustering
data analysis;formal concept analysis;equivalence relation;equivalence class;hierarchical structure;rough set theory
human-centered;goal-directed;sharing information;information sharing
required information;text summarization;web page;web sites;key words;knowledge discovery;search engines;visualization techniques;desired information;document type
document images;document processing;character segmentation;automatic selection
clustering;user profile;classification techniques;user profile;mobile internet;desired information;artificial neural networks
data structure;frequent itemsets;database;frequent itemset;tree structure;tree-based;pattern-growth;frequent itemsets
canonical correlation analysis;canonical correlation analysis;strong correlation
duplicate records;pair-wise;web service;duplicate detection;data stored;record linkage;databases;domain- specific;kernel method;web service;domain-specific;entity recognition;unified framework;record linkage;pre-processing;record linkage;web data
information content;similarity computation;semantic similarity;similarity computation;semantic information;feature vectors
clustering;unlabeled data;high dimensional;large quantity of;data clustering;data items;data clustering;time series;learning problem;dimensional data
supply chain management;supply chain;analytic hierarchy process;supply chain management;evaluation model;information flow
performance degradation;entire process;low overhead;low overhead;simulation results
aggregation function;classification problems;neural network;computational power
image retrieval;retrieve images;database;image database;computationally expensive;operator;visual features;content-based image retrieval
optimization algorithm;finite element;optimization algorithm;step-size;objective function
threshold selection;threshold selection;local minimum;image segmentation;threshold based
large quantity of;processing speed;image based

decision tree;decision tree;decision tree
classification;recognition accuracy;decision rule;parameter settings;web site;performance tuning
query-specific;natural language processing
data processing
1;decimal floating;2;image processing;decimal floating
location based;human face
quality guarantee;quality guarantee

united states
image retrieval;discrete optimization;wavelet transformation;fitness function;similarity retrieval;image databases;computation model;similarity search;color histogram;similar images;histogram-based;retrieval method
gradient-based;estimation technique;source separation
data points;ant-colony;clustering algorithm;theoretical analysis;ant-colony;clustering algorithm based on;clustering analysis
scheduling algorithm
analytic hierarchy process;analytic hierarchy process
expert finding;expert finding;expert finding;query terms;window-based;unified framework;language modeling
design method;design method
theoretical analysis;machine tool
text summarization;text summarization;automatic text summarization;matrix factorization;original document;information retrieval;document matrix
network model;neural network;surface reconstruction
database;optimize queries;execution plan;query processing in;database management system;databases;query processing
query processing;database;distributed databases;multi agent system;databases;long term;distributed database;join queries;database management system
description language;multi-agent system;web services;multi-agent;business processes;web services;semantic mappings;automatically generate
similar queries;case-based reasoning;ontology-based;semantic web;end users;traditional information retrieval;case-based reasoning;semantic search;case base;ontology-based;case-based reasoning;search process;ontology learning;semantic search;information retrieval;ontology learning
rdf data;linked data;huge amounts of data;related information;linked data;rewriting rules;query rewriting;semantic web;structured data;data integration
access control;access control;attribute based;xml documents;privacy policies;semantic relations;complex relationships
data centers;public data;computing infrastructure;parallel computing;map-reduce;cloud intelligence;sensitive data;cloud computing environment;cloud intelligence;data sources;business intelligence;long term;computing power;data management;programming model
business operations;main components;web data
conceptual modeling;web-based;spatial data;data warehouses;spatial information;web engineering;data sources;instance;decision makers;modeling language;increasingly complex
large amounts of;data warehouses;semi-structured;index structures;data warehouse;description logics;instance;semantic web;semi-automatic;theoretical foundations;data repositories;instances;semantic annotations
heterogeneous data sources;business information;semi-structured data;data warehouses;data source;data warehouses;transformation rules;xml structure;case study;business intelligence;specifically tailored;highly sensitive;automatically generate;development process;security issues;xml technologies
data mining techniques;decision support system;data mining;management systems;databases;learning environments;decision making
open source;business intelligence;business intelligence;data warehousing;data integration;business intelligence applications;data management
domain experts;ad-hoc;decision support;business intelligence;large data sets;business intelligence;high-volume;data sources;business users;highly scalable;decision making
automatically generates;database;query relaxation;data warehouse;textual content;information extraction;keyword search over;managing data;databases;online analytical processing;database schemas;unstructured data;high precision;natural language
structured databases;data warehouse;question answering;heterogeneous data sources;transactional databases;web pages;business intelligence;data warehouses;qa) systems;question answering;unstructured data;decision making
user profile;ontology-based;domain ontology;tf-idf;news items;ontology-based;term-based
xml updates;type checking;optimization problems;makes sense;data model;view maintenance;incremental maintenance;xml update;xml update;type inference
functional dependencies;functional dependency;tree patterns;xml document
object model;web applications;data structures;web environment;tree structure;xml update
special structure;view update;xml documents;source database;update operations;xml views;update propagation
xml schemas;information contained in;xml documents;instances;schema evolution;xml schemas;xquery update;xml schema
complexity bounds;xml data;probabilistic xml;xml documents;probability distributions;databases;probabilistic models
xml documents;real-world;xml databases;data interchange;xml update;queries efficiently;desirable properties
ontology-based;relational model;application level;relational databases;mapping language;update operations;semantic web technologies;relational databases;relational data
real-life;database;data mining algorithms;data owner;data mining
database;success rate;databases
information gain;sensitive attributes;mutual information;optimization problem;sensitive information
user community;web sites;privacy policy;service providers;web site;data privacy;personal data;wide range;user study
social networking;social networks;privacy policies;data storage
survey 2006 data;synthetic data;linear regression
privacy-aware;sensitive information;anomaly detection;sliding window;traffic monitoring;relevant information
sensitive information;data mining tasks;static databases;sequential patterns;privacy preserving data mining;databases;association rules
software development;case studies;conceptual model;programming interface
event-driven;application domain;event processing;knowledge-based;complex events;complex event processing;real-world;ontological knowledge
computational complexity;rewriting queries;maximally-contained rewritings;data integration systems;integration systems;data integration
data structure;xml schemas;xml data;data representation;heterogeneous systems;data model;data sources;conceptual model;web services
biological networks;database research;instance;systems biology;database engine;markup language
database;data collections;large number of;user requests;web services;produce high-quality;query processing techniques;highly scalable
network resources;small-world;location-aware;large-scale
world wide web;web pages;growing importance;web archive;web crawl;web page
discriminant analysis;decision trees;decision tree;algorithm called;cost-sensitive;decision tree learning;cost-sensitive
predictive models;clustering method
high-dimensional;problems arise;direct computation;document categorization;multiple labels;feature space;binary classification problems;information contained in;input space;generalized eigenvalue problem;multi-label classification;protein function;web page;binary classifier;multi-label;image annotation;shared-subspace;learning framework;gene expression
classification rule;data collection;census data;classification;classification rules;data mining;mining process;decision support systems;data mining;data analysts;rule based
statistical methods;document retrieval;statistical techniques;text retrieval
video databases;digital libraries;video stream
search systems;major search engines;web search;linkage-based;web documents
probabilistic latent semantic analysis;document collection;manually assigned;classification;link information;high quality;data sets;information retrieval;content information;information access;text classification;link analysis;sufficiently high
hierarchical classification;classification;document classification;web documents;feature extraction;training sets;html documents;feature selection process
natural language queries;structured documents;document structure;structured document retrieval;element retrieval;xml documents;hierarchical structure;relevant document;length normalization;structured queries
document collection;query complexity;structured documents;query difficulty;test collections;small-scale;test collection
interface design;starting points;structured documents;relevant objects;user behaviour;user behaviour;information seeking;small-scale
online stores;recommendation systems
collaborative recommendation;correlation method;collaborative filtering;similarity measure;information filtering;learning algorithms;prediction errors;recommendation accuracy;machine learning techniques;user similarity
text corpus;named entities;broadcast news;named entity;entity recognition;automatically generated;information extraction from
clustering;statistical properties;document classification;classification;classification accuracy;term selection;document frequency;document representations;document representation;term selection
test collection;main findings;language-independent;text retrieval
question answering;linguistic analysis;noisy data;question answering systems;random sampling;question answering;vector space model;natural language
relevance score;retrieval effectiveness;probabilistic model;keyword-based;retrieval systems;term-proximity;retrieval performance;documents retrieved;retrieval framework;trec10 test collections;term proximity
logic-based;structured documents;query formulation;large-scale;information retrieval;retrieval performance;structured queries
ad-hoc retrieval;resource selection;retrieval quality;databases;upper-bound;ranking documents
semantic classes;document representation;information organization;traditional information retrieval;spatio-temporal;event-based;topic detection;topic detection;online news
clustering;clustering;document clustering;multi-document summarization;multi-lingual
user browsing;relevance-feedback;rank documents;irrelevant documents;information retrieval;relevance feedback;query modification;text retrieval;vector machine
search systems;major search engines;web search;linkage-based;web documents
classification scheme;kullback-leibler;incoming documents;text categorization;text categorization;probability distribution;million documents
entropy-based;text categorization;information retrieval tasks;discretization methods;continuous attributes;text classification;classifier
efficient inference;smoothing techniques;classification;language model;naive bayes;language models;naive bayes;naive bayes classifiers;text classifiers;text classification;markov chain;language modeling;real world data sets
web pages;search process;visualization tool;helping users;search engines;web search results;web page;tedious task
relevance feedback;relevance information;performance gain;relevance feedback;retrieval performance;retrieval model;performance gains;low-level;content-based image retrieval
relevant information;document retrieval;ground truth;relevant pages;web browser
svm) classifiers;clustering structure;active learning;text classification tasks;learning performance;user opinions;vector machine;labeled documents;human efforts;text classifiers;text classification;support vector machines;active learning algorithms;unlabeled documents;random sampling
tf*idf;document collection;sparse data;text categorization;large-scale;weighting model;weighting schemes;text categorization;weighting model;weighting scheme;feature weighting;vector space model
improving accuracy;support vector machines;classification;automatic selection;text categorization;text classifier;accurate classifiers;feature selection;optimal parameters;parameter tuning;comparative analysis
web directories;web directories;document collection;partial information;classification;web documents;hybrid model;data structures;data structure;information retrieval systems
computational complexity;data collections;similarity join;similarity joins;metric spaces;nearest neighbors
matching algorithm;moura et al., acm tois;compression techniques;lower bounds;compression method;similar features;text databases;sequential pattern;natural language texts
context information;database;compression method;text databases;databases;semistructured documents;natural language
digital libraries;reference collection;retrieval performance;bayesian belief;vector space model
image retrieval;image collection;extraction methods;semantic relationships;query expansion;corpus-based
fitness function;genetic algorithms;extraction algorithm
content providers;spoken language;multimedia data;information retrieval
meta-search engine;information retrieval;java-based;general-purpose;information retrieval
search space;parallel computing;training set;term selection;retrieval results;relevance feedback;computationally intensive;parallel computing
image retrieval;index terms;weighting schemes;image content;weighting scheme;text retrieval;global analysis
clustering;search results;web documents;text documents;search engine;hierarchical clustering;ranked-list;readability;user interface;web search results;takes into account;phrase-based;clustering method
approximate matching;document retrieval;indexing techniques;indexing structure;low level;video retrieval;visual features;retrieval method
relevant documents;document retrieval;question answering systems;document retrieval;question answering;retrieval techniques
web search engine;web queries;everyday life;search engines;language independent;obtain information
digital libraries;hierarchical clustering algorithm;semantic similarity;composite events;event detection
naïve bayes classifier;classification;extracted features;data cleaning;feature extraction;large quantities of;highly accurate;machine learning;text classification;pre-processing
content-based music;information retrieval systems;database
search process;user profiling;web site;user profiles;relevant information;query refinement
domain knowledge;high quality;decision support;data resources;data mining;data mining;cost-effective;high-level
mining results;data mining techniques;temporal sequence;event sequences;temporal sequences;interesting patterns;instance;temporal constraints;data mining;sequence mining
frequent episodes;temporal patterns;databases
mining frequent patterns;frequent patterns;synthetic datasets;real data;data mining research;graph theory;real world
typical patterns;decision tree;predictive accuracy;classification;graph-structured data;graph-based;decision tree;machine learning;greedy search;graph-based;classification task;classifier;classifier
clustering algorithms;benchmark" datasets;low dimensional
hierarchical clustering algorithms;automatic extraction of;clustering structure;hierarchical clustering algorithms;automatically determines;hierarchical clustering;data set;hierarchical representation;pre-processing;user interaction
training data;unlabeled data;syntactic information;document classification;unlabeled documents;large scale;large number of;learning algorithm;classification systems;content words;large scale;labeled data;partially supervised;reuters-21578 corpus;training algorithm
cut algorithm;mutual reinforcement;multiple documents;textual content;graph based;similarity graph
text mining;data-driven;nearest neighbor;probabilistic latent semantic analysis;semantic knowledge;learning algorithm;latent semantic analysis;machine translation;knowledge acquisition;semantic similarity
automatically generates;training set;training data;regular expressions;textual data;information extraction;unseen data;information extraction from;pattern discovery;semi-supervised
association rule mining;real-world;data set;constraint-based;association rules;mining patterns;temporal data
accuracy compared to;protein structure;database;data mining techniques;databases;protein sequence;protein sequences;class prediction
automatic extraction of;learning rules;domain specific knowledge;heuristic rules;knowledge-based;extraction rules;machine learning;test set;extraction process
complete set of;detection methods;human genome;large-scale
data mining approaches;change frequently;data model
web pages;information service;graph-based;optimization algorithm;interesting association rules;association rules;access patterns;association rule
clustering;web documents;markov models;user profiling;web site;high-dimensionality;clustering;navigation paths;markov model
user preferences;behavioral patterns;user interests;relational information;bayesian networks;user behaviors
frequent patterns;business process;mining frequent;mining frequent;instances;mining algorithm
1;video data;video streams;clustering;data mining
window sizes;temporal relations;temporal characteristics of;input data
algorithm generates;linear classifier;algorithm's performance;algorithm produces;data stream;prediction error;batch learning;simple algorithm;maximum margin;large margin classifiers;margin
data set;hierarchical structures;classification performance;domain values;statistical technique;real-world data sets;learning framework;bayesian learning
bayesian network;classification model;bayesian-network;classification;bayesian network classifiers;naive bayes;classifier
clustering;high-dimensional datasets;density-based clustering;high scalability;clustering algorithm;high speed
clustering;multi-level;multi-level;clustering method;spatial clustering
clustering method;space-partitioning;index structure;retrieval performance;approximation technique;dimensional data
mining task;association mining;candidate 2-itemsets;frequent itemset;sliding window;incremental mining;candidate itemsets;frequent itemsets;transaction database
association rule mining algorithms;closed itemsets;frequent itemsets;databases;information overload;error bounds
transaction data;large volume;genetic algorithms;completeness;database;rules generated;association rules;mining association rules;data sets;transaction database;dynamic databases;data mining;evolutionary approach;association rule discovery;distributed transactions;large itemsets;distributed environment
web access;web access;pattern tree;prefix tree;frequent sequences;sequence database;sequential pattern mining;web log
document filtering;mining algorithm;html documents;user interests;structured documents
xml document;mined patterns;post-processing;xml documents;data mining approach;similarity computation;sequential pattern mining;structural similarity
generation algorithm;decision trees;test data;classification;decision tree;rules generated;genetic algorithm;classification methods;breast cancer;attribute-based;membership functions;comprehensibility;high accuracy;decision boundaries;fuzzy rules;fuzzy rule;fuzzy sets
svm training;decision boundary;classification accuracies;pattern set;support vector;pattern selection;computational burden;simulation results
label noise;ensemble method
nearest neighbor rule;decision tree algorithms
data mining technique;distance metrics;real-life applications;numeric data;outlier detection;large databases;outlier mining;categorical data;mining process;distance-based;database systems
data set;knowledge discovery in databases;global structure
involving multiple;multi-dimensional;on-line analytical processing;data cubes;efficiently computing;pruning power
erroneous data;tree structures;tree pattern;tree patterns;labeled tree;tree structured;information extraction from;semistructured data
multiple linear regression;modeling tool;potentially infinite;compression technique;data stream;multiple linear regression;management systems;stream data;data stream management systems;online analytical processing
database;transaction database;data mining;association rules;discovered rules;discovered knowledge;frequent pattern mining
singular value decomposition;factor analysis;open source software;open source software;graphical model;statistical technique;association rules;association rules
data structure;fp-tree;parallel execution;frequent patterns;fp-growth
feature selection algorithms;training data set;large data sets;data pre-processing;instances;class information;feature selection;data mining;active sampling;random sampling;benchmark data sets;feature selection
error-prone;attribute values;database;ad-hoc;categorical attributes;databases;categorical data;similarity functions;similarity function;distance measures;data mining tools;categorical attributes;nearest neighbour;classifier;random walks
training data;variance reduction;classification;naive-bayes classifiers;training instances;classification error;theoretical analysis;small size
clustering;equivalence relation;equivalence relations;clustering method;user-defined
11;involving multiple;environmental data;higher accuracy
correlation analysis;large number of;time series;correlation analysis;computational cost
stream data;sequential patterns
transaction data;clustering;synthetic data sets;clustering technique;large number of;data sets;clustering techniques;databases;clustering algorithm;high dimensionality;high-level
large databases;density-based;clustering method;random sampling
clustering;intrusion detection;optimal values;clustering algorithms;clustering method;data set;anomalous behavior;data objects;common knowledge;occurrence frequency
logic programming;frequent subgraphs;large graph;geometric information;apriori algorithm;knowledge representation;total order;graph structured data;structured data
bayesian network;extraction algorithm;computation cost;original data;inductive logic programming;bayesian networks;overfitting problem;data mining;conditional probability
clustering;meaningful clusters;clustering results;clustering algorithms;large number of;similarity measures;data set;density function;data sets;similar patterns;clustering algorithm;clustering validity;input parameters;clustering
web search;web ir;specific information;model assumes;query formulation;ir model;information from multiple sources;navigational queries;user intent;search engine;relevant information;link analysis;information retrieval;web search engines
information science;search engine;information retrieval systems;evaluation methodology;information retrieval
spam detection;sample complexity;function approximation;pattern recognition;classification;takes into account;functions defined;ranking function;ranking schemes;learning machines;vector spaces;web mining;learning machines;real-world;learning mechanism;link analysis;human supervision;page content;statistical relational learning
user queries;retrieval model;xml retrieval;search strategies;databases
growing number of;image retrieval;sponsored search;search log;query reformulation;information retrieval;language modeling;perform poorly;similarity measures;data sparseness;similarity measures
weighting model;probabilistic models;linear combination;term frequency;information theoretic;test collection
score distributions
language models;ad-hoc;query likelihood;term association;ir) systems;query reformulation;information retrieval;retrieval performance;window-based;language modeling framework;mixture models;modeling techniques;language modeling
reduction techniques;inverted files
control systems;information systems;management systems;text search;multiple sequence alignment;indexing method;efficient indexing
pruning techniques;specific information;pruning technique;low-cost;fixed-length;information retrieval;retrieval performance;5;trec test collections;low-frequency
web documents;document collection;main memory;million documents;retrieval performance;compression ratio;assignment problem;web search engines
memory requirement;large scale;fixed size;query performance;compression ratio;memory space;efficient construction
required information;network traffic;query independent;information retrieval systems;web search engines
highly heterogeneous;information retrieval;instance;relevant data;weighting scheme;digital libraries;ambiguous queries;information retrieval systems;test collection
decision-theoretic model;resource selection;selection" problem;retrieval quality;decision-theoretic framework;query routing
rank-based;selection method;distributed information retrieval;limited number of;collection selection;resource constraints;collection selection
results merging;results merging;distributed information retrieval;regression models;linear regression
fusion method;retrieval systems;search engine results;improving search;document scores;average precision;data-fusion
difficult" queries;trec collections;jensen-shannon divergence;scoring functions;query difficulty;query performance
clustering;initial query;natural language;context-sensitive;query reformulation;helps users;knowledge base;query refinement
query expansion;information retrieval systems;machine learning
question classification;syntactic information;classification;kernel function;kernel functions;tree structure;specific task;semantic information;support vector machines;question answering;data sparseness
relevance feedback;initial query;relevant documents;active learning;relevance feedback;retrieval performance;feedback documents;learning problem
relevance feedback;retrieval effectiveness;computational efficiency;information-theoretic;relevance feedback;large collections;query expansion;retrieval effectiveness
information retrieval evaluation;evaluation methodology;test collections;finer grained
query-independent;features extracted from;user study;visual features;data collected
classification scheme;classification;automatically extracting;information extraction;classification performance;text classifier;structured information;information extraction;classifier
syntactic structures;tree kernel;classification;similarity based;information management;text retrieval;text classification;semantic information;question classification;question answering;automatically learn
data objects;spectral clustering;spectral clustering algorithms;synthetic data;data points;clustering algorithms;large-scale;clustering
clustering;randomly generated;probabilistic model;generative models;clustering documents;scientific literature;text documents;clustering model
relevant information;information systems;interesting items;filtering algorithm;collaborative filtering
search results;collaborative web search;search tasks;collaborative web search
algorithm finds;user preferences;collaborative filtering;collaborative filtering algorithms
entropy-based;information theory;language models;relative entropy;information retrieval;collection size;large document collections;search engines;sufficient training data;large collections
filtering systems;information organization;traditional information retrieval;link detection;link detection;topic detection;document representations;document representation
domain-independent;query term;information retrieval;domain-independent;ad hoc retrieval;ad hoc retrieval
probabilistic framework;count-based;mixture model;language models;probabilistic models;generative models;expert finding;common task;retrieval accuracy;candidate generation
search task;final ranking;taking into account;expert search;query topic;pseudo-relevance feedback;relevance feedback;instance;retrieval performance;relevant expertise;assists users;query expansion;search tasks
xml element;retrieval effectiveness;xml retrieval;smoothing methods;language modelling;xml repositories;user query
document portions;xml retrieval;relevant content;retrieval systems;xml documents;logical structure;information seeking
search task;parameter values;anchor text;hyper-parameters;relevance assessment;parameter settings;test collections;term frequency;search tasks
query-independent;query-dependent;relevance criteria;related documents;web retrieval
semantic indexing;classifier fusion;kernel-based;classification;svm-based
query length;error rate;search engines;spoken document retrieval;speech retrieval
classification;web navigation;multimedia information retrieval;statistical language models;retrieval systems;web resources;virtual communities;statistical modeling;natural language processing;interesting items;collaborative filtering
search results;social web;user opinions;online content;information access;search technique;web search engine
cross-language;information retrieval systems;cross language information retrieval;cross-language information retrieval;cross-lingual;information access;effective tools
related concepts;external knowledge;representation scheme;text classification;machine learning algorithms;graph traversal
modal analysis;mutual information;learning process;textual information;information entropy;information theory;data set;document (image;select relevant;information theoretic
dynamic programming algorithm;integer linear programming;inference algorithms for;multi-document summarization;knapsack problem;dynamic programming approach;inference algorithms
graph model;information processing;document representation;traditional models;semantic relations;document representation;vector space model;classifier
sentiment analysis;dependency trees;sentence level;linguistic analysis;sentiment analysis
ranking algorithm;large variations;mathematical analysis
query term;question answering systems;machine learning techniques;tree learning;question answering;query terms
issue queries;search results;search engines;navigational queries;user search behavior
ranking algorithm;ranking results;impact factors;discounted cumulative gain;digital libraries;hits algorithm
retrieving information from;vector space model;vector space model;text collections
xml retrieval;retrieval systems;xml documents;document structure;retrieval performance;named entity;xml document;search engine;semantic structure;xml structure
classification;document retrieval;linear combination;document search;baseline methods;relevance model;document type
multi-modal;cross-media;textual descriptions;digital content;information retrieval;cross-media;semantic space;low-level;audio-visual
web pages;search performance;anchor text;valuable information;1;ranking functions;information embedded in;ranking results;image information;relevance score;web page;web search engines
pagerank algorithm;web pages;web search;web graph;trec web track;special case;random walk model;transition probability;search accuracy;web page;average precision

human users;case study;search experience;clustering
distributional clustering;distributional clustering;signal processing;information-theoretic;text categorization;naïve bayes;1;3;2;5;4;signal processing
statistical test;average precision;retrieval methods
desktop search;personal information management
video sequence;general purpose;video summarization;level features;visual attention;supervised classification;hierarchical representation;video content;neural network;low level
accuracy compared to;large numbers of;training data;classification;active learning;active learning;labelled data
spam detection;ranking process;spam pages;web spam;structure information;link spam
smoothing methods;training set;text classifier;naive bayes;parameter estimation
query operations;video search;term selection;term selection;text retrieval;video retrieval
prediction quality;threshold-based;recommender systems;collaborative filtering
data set;bayesian network;language models;multimedia documents;xml documents;logical structure;multimedia content;semantic structure;multiple sources;language model
classification;language models;naive bayes;language model;classification methods;question answering systems;bayes classifier
music information retrieval;textual data;multiple views;textual features;content words;genre classification;feature sets;multimedia content;audio features;low level
retrieval method;retrieval methods;video content;visual information;retrieval method
web information retrieval;retrieval effectiveness;web information retrieval;query independent;retrieval performance;ir) systems;conditional probability;query independent
enterprise search;retrieval precision;web search;finding task;language models;trec enterprise;expert search;expert finding;retrieval performance;common task;document retrieval;language modeling
clustering;clustering problem;consensus clustering;clustering technique;large number of;text clustering;text representation;document clustering;document representations;computational cost
context-dependent;user preferences;mobile devices;user-defined
retrieval accuracy;retrieval model;bayesian approach;term frequency;document type;scoring function
information network analysis;data cube;multimedia data mining;rfid data;data mining;pattern mining;moving object;data mining technologies;sensor networks;theoretical foundations;web mining;online analytical processing
pattern discovery;sample data;data mining
class noise;information sources;noise handling;data mining research;data records;noisy data;data mining algorithms
clustering algorithms;attribute values;cluster structure;density-based clustering;categorical datasets;categorical data;input parameters;subspace clustering algorithms
decision trees;nearest neighbor;classification;multiple representations;complex objects;support vector machines;classifier;class probabilities;class prediction
support vector regression;basis functions;regression algorithm;support vector;support vector machines;support vectors
mining frequent itemsets from;probabilistic framework;mining frequent itemsets;frequent patterns;mining frequent itemsets from;uncertain data;data model;cpu cost
clustering;clustering algorithms;overlapping clusters;test cases;algorithms produce;clustering tasks;clustering problems;real world;web page
image retrieval;semantic concept;remote-sensing;starting points;remote sensing;semantic features;high-resolution;complex objects;real-world datasets;feature selection;pattern discovery;individual objects;high-resolution;feature sets
private information;distance-preserving;distance-preserving;pca based;independent component analysis;original data;projection based;data mining;theoretical analysis;privacy-preserving data mining
attribute reduction;cross validation;selected features;rough sets;feature subset;feature subset selection;search algorithm
statistical methods;intrusion detection;genetic algorithm;detection method;anomaly detection;user behavior;detection rate
data-flow;process model;event logs;workflow management systems;process mining;control-flow;mining algorithm;workflow systems
traffic data;prediction model;recurrent neural network;wavelet-based;time-series;recurrent neural network;neural network
graph partitioning;multi-level;ant colony optimization;graph model;np-complete;data clustering;adjacency matrix;clustering;algorithm produces;algorithm relies on;data objects;weighted graph
data streams;prediction errors;change detection;data stream;score functions;data mining;detect outliers;simulation results;approximate results
parameter space;support vector machines;starting points;cross validation;optimization problem;gradient based;support vector machines;margin;benchmark data sets
image retrieval;segmentation results;regularization;image segmentation;mixture model;database;image database;learning rate;learning algorithm;selection problem;region-based;minimum description length;em) algorithm;competitive learning;computational cost;em algorithm;maximum likelihood;model selection
distributed environments;analysis task;distributed environment
prediction problem;prediction methods;vector machine;large number of;high-throughput;risk factors
classification techniques;plans;data mining techniques;decision tree;feature-based;classification rules;principal component analysis;benchmark dataset;high dimensionality;information gain;feature based;optimal set of
learning process;traffic data;multi-layer;learning algorithm;time series;recurrent neural network;neural network;long-term;network traffic;recurrent neural network
probabilistic latent semantic analysis;automatic query expansion;document sets;text documents;automatic query expansion;latent semantic analysis;latent semantic;query expansion
large amounts of;training data;unlabeled data;training examples;classification;semi-supervised learning;total number of;similarity matrix;text classification;text classification problems
cognitive states;high dimensional;training classifiers;fmri data;machine learning;human brain;case study;case studies;noisy data;machine learning techniques;magnetic resonance imaging
1;frequent items;memory space;log analysis;data stream;item pairs;data streams
data points;spatial clustering;algorithm combines;incremental clustering;clustering methods;data mining;incremental clustering
class membership;document classification;classification;smoothing methods;multiclass classification;class membership;training samples;moving average;naive bayes classifiers;support vector machines
hybrid approach;decision trees;decision tree;computation cost;data miners;privacy-preserving;data mining;secure multi-party computation;vertically partitioned data
duplicate detection;record linkage;clustering approach;instances;data records;semantic) relations between;database table;relational data
transitive closure;dynamic bayesian network;statistical analysis;temporal features;dynamic bayesian networks
data point;manifold learning;data points;learning algorithms;quadratic programming;iterative algorithm;dimensionality reduction
spectral clustering;discriminant analysis;null space;mixture model;highly complex;linear discriminant analysis;iterative process;linear discriminant analysis;implicit assumption;covariance matrix;null space;clustering method
regularization;pattern analysis;kernel-based;kernel function;kernel functions;simulation experiments;learning machines;machine learning techniques;support vector regression;support vector machines
clustering;clustering algorithms;clustering results;clustering;multiple objects;data objects;clustering algorithm;clustering algorithm based on
face recognition;theoretical foundation;classification accuracy;feature extraction;linear discriminant analysis;small-size;high efficiency;extraction methods;small sample size;face database;computationally expensive
mining results;search space;graph mining;classification;structural constraints;classification framework;pattern mining;mining process
bursty features;classification model;bursty features;event detection;feature selection method
outlier detection;clustering;density based clustering;clustering framework;high dimensional datasets
clustering;detection algorithm;region-based;static images
relevant features;irrelevant features;feature selection algorithms;real-life datasets;data preprocessing;feature selection;feature selection methods;irrelevant attributes;learning performance;classification algorithms
search space;sequential patterns;incremental mining;tree structure;prefix tree;mining algorithm
valuable knowledge;linear programming;decision-making;vector machine;support vector;rule-based classification;multiple kernels;feature selection;rule extraction;learning problem;feature selection problem;interpretability;uci datasets;support vectors
public domain;feature space;training set;training data;label information;spam filtering;high accuracy;produce accurate;semi-supervised;classifier
conditional entropy;heuristic algorithm;qualitative simulation;rough sets;qualitative reasoning;entropy based
clustering;clustering;vector machine;clustering method;vector machine;clustering algorithm
feature ranking;extracted features;probabilistic model;feature extraction;hierarchical structure;rule based;customer reviews
feature space;web page;rough set;negative examples;vector machine;unlabeled examples;web pages;margins;web page;classifier;positive class;rough set approach
frequent pattern;frequent patterns;equivalence classes
1;semantically similar;feature vector;content-based image retrieval systems;relevance feedback;database application;semantic structure;learning strategy;content-based image retrieval
clustering;genetic algorithm;combinatorial optimization;clustering method;data item;clustering algorithm;distance metric;manifold structure;evolutionary algorithm;artificial data sets;evolutionary clustering
amino acid;input parameters;tree based;tree model;ensemble-learning
temporal relations;data mining methods;temporal patterns;temporal relations;relational database;temporal sequences
supervised learning;question-answer;document retrieval;optimization problem;ranking function;fundamental problem;supervised learning;word segmentation
feature space;classification
attribute values;low support;sequential patterns;frequent sequential patterns;privacy-preserving;1, k;sequential pattern
text mining;profile based;graph mining;automated methods;link-analysis techniques;text documents;large volumes of;valuable information;information extraction;knowledge discovery;extracted features;graph-based;relevant information;link analysis techniques
probability density function;automatically determined;neural network;artificial neural network
entity) recognition;error-prone;training data;statistical method;similarity matrix;entity recognition;semi-automatically;semi-supervised;training corpus
test results;contrast set mining;risk factors;contrast set mining
sequence alignment;database size;large databases;clustering method;memory consumption;optimization techniques;sequential pattern mining
prediction model;time series;rbf neural network;analysis tool;prediction method;moving average;radial basis function;neural network
clustering;clustering algorithm;physical characteristics;classification;similarity measure
classification model;data clustering;kernel method;probability distributions;neural network;clustering algorithm;neural network
formal description
statistical methods;web documents;web pages;web collection;special properties;classification performance;text classification;text classification;word segmentation;classification algorithm;problem called;segmentation algorithm
sheds light on;fast algorithm;data points;randomly generated;clustering method;projected clustering;cluster quality;data set;correlation clusters;projected clustering;clustering methods;higher quality;clustering quality

pattern language;specifically designed for;spatial datasets;pattern mining;visual interface;language called;spatial patterns
clustering;clustering algorithms;clustering ensembles;ensemble methods;hierarchical clustering algorithm;similarity matrix
optimization algorithm;search efficiency
phase space reconstruction;classification;support vector machines;feature extraction;simulation results;training samples;phase space reconstruction;classification method;svm classifier;support vector machines;distinctive features
real data sets;access information;link analysis;impact factors;web forums
auc;extraction algorithm;rough set theory;rough set;attribute reduction;cost-sensitive learning;classification;rule extraction;class imbalance learning
clustering;learning process;algorithm iteratively;bayesian information criterion;learning algorithm;data set;competitive learning;times faster than
recognition rate;classification;high dimensional;feature extraction;classification algorithms;gene expression;gene expression data
genetic programming;parameter values;kernel function;vector machine;text categorization;text categorization;tree structure;gradient descent;gradient method;support vector machines
similarity query;fast algorithm;database;accuracies;databases;range query;input sequences;multi-point;similarity queries;spatial index;biological databases;protein sequences;biological sequences
intrusion detection;anomaly detection;anomaly-based;detection method;data set;user transactions;detection methods;clustering algorithm;normal behavior
mining closed;huge number of
clustering;multi-dimensional;main memory;multi-dimensional data;memory usage;high dimensional data streams;clustering method;data set;data stream;grid-based;data mining;high dimensionality;space complexity
boundary points;boundary points;noisy datasets
syntactic information;similarity measure;average precision;similarity measurement;retrieval models;qa) systems;qa systems;finding similar;question answering;similarity measures
text mining;data structure;conventional methods;frequent patterns;text corpus;tree structure;mining algorithm;dependency structure;structure mining
sample set;feature extraction;reconstruction error;extracted features
semantic annotation;local linear;kernel-based;large-scale;kernel based;local neighborhood;semi-supervised learning algorithms;data set;supervised approach;video database;training samples;labeled and unlabeled data;video annotation;local linear
computational complexity;mutual information;learning bayesian networks;scoring function;mutual information;incomplete data;machine learning and data mining;learning bayesian networks
evolutionary algorithms;classification;hidden nodes;uci datasets;learning algorithm;hidden layer;higher accuracy;network structure;radial basis function;neural network
clustering;algorithm performs;clustering technique;data mining applications;large databases;algorithm named;hybrid method;simulation results
clustering;object tracking;aggregation algorithm;suffix tree;resource-constrained;graph connectivity;tracking objects;communication cost;connected component;sensor network;clustering algorithm based on;mining algorithm
spam detection;social network;huge amounts of;memory space;real dataset;filtering technique
decision support;real data;time series;sequential data;mining process;integrates multiple
higher classification accuracy;vector machine;instances;training samples;clustering algorithm;clustering analysis;uci datasets;fold cross validation
text summarization;extraction algorithm;keyword extraction;document processing;text clustering;semantic graph;text retrieval;word sense disambiguation
bayesian network learning;conditional independence;feature representations;naive bayes;theoretical justification;text classification;information gain
feature extraction method;classification;feature extraction;microarray experiments;support vector machines;gene expression;classifier;gene expression data
ant colony optimization;genetic algorithm;plans;genetic algorithm;resource allocation;scheduling problem;resource allocation;ant colony optimization
building blocks for;image classification;image classification
mining temporal;spatio-temporal databases;pattern mining;mining temporal;video data;sequential pattern mining
training data;training data is;learning algorithm;vector machine;incremental learning;text categorization;knowledge learned;incremental learning;real world;benchmark datasets;support vector machines;classifier;posterior probability
clustering;clustering;gene ontology;clustering technique;text documents;data set;frequent-itemsets
query sequence;dynamic programming;post-processing;dna sequence;databases
language model;document retrieval;ir model;concept-based;tf-idf;retrieval model;conditional probability;average precision
attribute values;classification problem;classification;learning algorithms;statistical information;incremental learning;data mining;human beings;real world;memory utilization;learning method
hidden markov model;semantic analysis;navigation patterns
machine translation;clustering methods;translation model;web-accessible;domain-specific;document clustering;cross-lingual;document collections;cross-lingual
genetic programming;genetic programming;optimization algorithm;neural network;prediction problems;input variables;tree model;optimal design
clustering algorithms;large data sets;data sets;clustering categorical data;local minima;objective function
regression method;nonlinear regression;data mining;generally applicable;feature selection;regression methods;classifier
pattern recognition;principal components;rates;principal component analysis;artificial neural networks;classifier
clustering;clustering algorithms;density-based;input data;data points;data structure;disk access;density-based
em) algorithm;linear transformation;database;transformation matrix;model parameters;unified framework;em algorithm;maximum likelihood;model training;expectation-maximization
web pages;web search;social annotations;language models;smoothing methods;language model
clustering;clustering result;data set;clustering method;objective function
computational complexity;attribute reduction;optimization algorithm;search process;optimization problem;data sets;optimization techniques;np-hard
clustering;clustering problem;database;kernel matrix;kernel method;graph nodes;step procedure;adjacency matrix;similarity measure between;kernel k-means
regulatory networks;gene expression data;gene clusters;time series;coding scheme;microarray datasets;clustering algorithm
parallel algorithm;learning bayesian networks;large-scale;conditional independence;learning algorithm;problem domains;likelihood function;bayesian networks;em algorithm;algorithm to compute
domain knowledge;1;correlation-based;support vector regression;kernel based;domain knowledge into;domain knowledge;feature selection algorithm;algorithm combines;feature selection;canonical correlation analysis;mutual information between;kernel methods
clustering;distance measure;clustering results;geo-spatial;measure called;artificial data sets;spatial clustering;spatial distance;clustering approaches;attribute values;spatial knowledge;spatial attributes;problem solving;clustering methods;spatial objects
regression method;instance-based;missing values;missing data;classification performances;categorical attributes;machine learning and data mining
microarray data;feature selection method;classification accuracy;selection method;accurate classification;feature relevance
clustering;data points;data owners;private information;real-world;private data;data clustering;cluster labels;clustering scheme;privacy-preserving;original data;theoretical bounds
algorithm performs;spatial data;spatial clustering;real datasets;clustering method;obstacles constraints
real-world datasets;classification problem;window size;data volume;cost/benefit analysis;network intrusion detection
multiple databases;structural learning;database;structural learning;prior knowledge
spatial index;information extraction;spatial data;database;spatial database
clustering;attribute values;clustering results;clustering accuracy;machine-learning;databases;clustering algorithm
network traffic;accurate classification
generic model;random graph;real-world;hierarchical structure;network data;hypothesis testing
graph model;data collected by;strong evidence;social networks;social relations
emerging topics;probabilistic generative model;group structure;social network analysis;latent-variable models;joint inference;topic discovery;large data sets;multiple modalities;vice-versa;relationship data
network analysis;clustering;fast approximate;relational structures;variational inference;3;1;data sets;2;social networks;bayesian framework;relational data;clustering model;low dimensional space
statistical properties;generative model;real-life;social networks;evolving networks;social interaction
latent space;bayesian framework
clustering;degree distribution;model generalizes;stochastic model;complex networks;clustering coefficient;statistical model;exponential family;statistical inference;network data;markov chain;monte carlo
classification;network evolution;social networks;graph models;estimation algorithms;maximum likelihood;statistical models;hypothesis testing
state space;synthetic data;dynamic model;real-valued;1;euclidean space;observation model;kalman filter
discovery algorithm;community structure;information sharing;dynamical systems
dynamic social network;data set;social networks;social networks;social network
pattern recognition;machine learning methods;classification;networked data;machine learning;social networks;probabilistic inference;network data;benchmark data sets
social network;personal information;online social networks
case based reasoning;retrieval methods;case based reasoning
learning procedure;database
novelty detection;case-based reasoning;takes into account;medical diagnosis;detection method;intelligent systems;novelty detection;visual perception
instance-based learning;data mining methods;classification;memory constraints;stream mining;learning algorithm;machine learning;incoming data;data stream;data streams;desirable properties;data characteristics;instance-based
classification;application domain;optimization problem;learning theory;support vectors;benchmark datasets;support vector machines;margin;integer programming;theoretical bounds
shortest paths;feature vector;computational complexity;classifier;ant colony optimization;genetic algorithm;ant colony optimization;face recognition system;population-based;optimization algorithms;database;simulation results;recognition systems;feature selection;feature subset;classifier performance;feature selection method;social behavior;feature selection;feature selection
outlier detection;detecting outliers;artificial data sets;streaming data
clustering;multi-dimensional;classification;data mining;principal component analysis;breast cancer
clustering;clustering algorithms;image segmentation;synthetic data;random projection;density-based clustering;post-processing;randomly-generated;random projections;combines ideas from;real images;numerical data;clustering algorithm;data distribution
clustering;clustering algorithms;communication overhead;clustering technique;data mining applications;local models;clustering algorithm based on
set covering;markov models;rule-based;interesting patterns;statistical method;stream data;rule induction methods
link detection;similarity measure;information retrieval;documents retrieved;link detection;query expansion technique;retrieved documents;query expansion
world wide web;entity recognition;world wide web;web-based;highly accurate;wide range;entity recognition;domain independent;knowledge sources
case-based reasoning;statistical model;case base;starting points;knowledge-based systems
case-based reasoning;case-based reasoning;intelligent systems;knowledge-based;case base;knowledge representation
clustering;search space;large databases;data set;case study;databases;large database
multiple models
active learning strategies;training set;taking into account;active learning;active learning strategies;active learning;real data;intelligent systems;machine learning;predictive model;case study;human-machine;learning problem
case-based reasoning;data set;problem solving;principal component analysis;control mechanism;maximum likelihood
interestingness measure;customer segmentation;cluster analysis;rule mining
1;network models;prediction algorithms;collaborative filtering
complex queries;visualization tools;query formulation;visual query;data analysis;temporal databases;relational databases;relational database;query languages;query language;temporal databases
expert systems;classification techniques;success rate;human intervention;image-based;shape features;web services;image processing;shape matching
distributed computing;computational power;data mining;knowledge discovery in databases;inductive logic programming;data mining system;large amounts of data;data mining;relational data;million records
privacy preserving;frequent patterns;privacy-preserving;time series
time series;prediction method;time series;artificial neural network;neural networks
association mining;apriori algorithm;association rules mining;association rules;data mining
computational complexity;skyline algorithms;dimensional space;skyline queries;application requirements;decision-making;data set;data sets;skyline queries;desirable properties;skyline query;missing values
service provider;continuous query processing;completeness;data updates;data streams;indexing scheme;query processing;databases;database functionality;database outsourcing;query results;data distribution;data owner
xml schemas;schema mapping;real datasets;query reformulation;xml databases;mapping rules;query answering;query translation;data management system;global schema
query optimization;compression rate;incremental-maintenance;data cube;real-world and synthetic datasets;relational model;highly compressed;on-line analytical processing;typically involves;query-processing;incremental maintenance;aggregate queries over;query processing;large datasets;great promise;real-world applications
threshold-based;randomized algorithm;applications including;uncertain data;highly accurate;data set;data sets;exact computation;efficiently computing;multidimensional space;threshold-based;algorithm to compute;decision making
cost functions;large scale;maximum number of;instance;distance metric;assignment problem;mobile users
query execution plan;database;test queries;wide range;test database;databases;test case
unified framework;tree-structured;real data;conflict resolution
control systems;multi-layer;multiple layers;database;financial data;data representation;instance;personal information
control systems;access control
nearest point;data points;data owners;data publishing;metric space;space partitioning;reference point;query results;query point;nearest neighbors
xpath expression;query rewriting;query answer;access control;query formulation;rewriting algorithm;security views;xml data;query answering
linear transformations;trade-offs;data availability;query answers;instances;answering queries;query answer;databases
intrusion detection;data collection;audit data;intrusion detection system;database;database security;database server;data acquisition;intrusion detection systems;database administrators
group members;encrypted data;keyword-based;data confidentiality
large amounts of;attribute values;frequency distribution;data privacy;clustering algorithm;diversity measures
share information;end user
privacy policy;privacy-preserving;1,2;management architecture;privacy policies
medical records;digital information;security requirements;storage systems
intrusion detection system;private data;web browsers;web crawlers;external information;detection methods
context-dependent;database privacy;database privacy;database records;user privacy
database;application systems;hash-based;radio frequency identification;user privacy
graph partitioning;multi-level;simulated annealing;image segmentation;np-complete;fundamental problem;knowledge discovery;data mining;algorithm produces;algorithm relies on
clustering;clustering;graph theory;pattern recognition;auxiliary;classification accuracy;computational costs;benchmark datasets;machine learning;image segmentation
energy minimization
network flow;shortest path;graph cuts;shape matching;graph cut
digital images;energy function;edge detection;graph cuts;user interaction;decision making
pattern recognition;image segmentation
pose variations;face image;detection accuracy;face recognition system
document images;document structure;visual features;search algorithm;heuristic search;document image;optimal set of
image restoration
detection algorithm
active appearance models;statistical framework;active appearance models;estimation algorithm;statistical model
recognition systems;fusion strategies
pose variation;human body;training examples;image features;body parts;image understanding;high-level;learning framework;multiple image;shape matching
ground truth;general purpose;database;large scale;large-scale;object boundary;image database;object level;databases;video frames
automatically generate;face image;hierarchical structure;graph representation
markov random field;object category;sketch-based;graph model;category recognition;object categorization;context free;hierarchical structure;structure information;generative model;maximum entropy;matching method
markov random field;mrf model;26;inference algorithm;partial order;4;6;bayesian inference;posterior probability;graph representation
classification;input image;feature-based;specular reflections;single image;color segmentation
data point;segmentation problem;gaussian mixture model;image segmentation;data points;vector field;statistical method;statistical models;image processing
image segmentation;random fields;image sequences;moving objects;graph cut;shadow detection
blurred image;motion parameters;frequency domain;inverse problem;simulation results
video sequences;video sequence;object tracking;moving target;distance transform
vehicle tracking;point features;vehicle tracking;feature-based;video surveillance;kalman filter;image alignment
von mises;state space;global optimal;real-world;sequential monte carlo;particle filtering;posterior distribution;magnetic resonance imaging;noise model
object recognition;database;multiple objects;probabilistic graphical model;visual representations;camera motion;object models;object models;multi-class;model selection
context information;clustering problem;video segmentation;sampling approach;synthetic data;bayesian model;dirichlet process;time series;adaptive clustering;video segmentation;spatial structure;inference algorithm;mixture models;clustering
multiple object tracking;multiple objects;computational complexity
shape representation;pruning method;shape reconstruction
shortest paths;shape representation;classification;contour based;pruning method;bayesian classifier;classification results;similarity based
2;shape analysis;optimal matching
clustering;shape space;statistical summaries;mri data;shape analysis;shape analysis
gradient-based;multi-view;control points;quality control;energy minimization
data sets;classification results;texture features;classification;remote sensing
optimization technique;memory requirements;global optimization;counterpart;graph cut;input images;image information
markov random field;image segmentation;database;takes into account;segmentation results;real images
line segment;simulated annealing;automatic extraction of;monte carlo;gaussian filter;prior model;location information;markov chain
test data;parameter settings;ground-based;surface reconstruction;surface reconstruction
open source;open source software;data mining techniques;data source;industrial applications;data mining research;open source data mining;data mining systems;data mining;open source
cost-effective;content analysis;sheer volume of;accurately reflect;short-term
time-series;hong kong
probabilistic approach;information retrieval
automatically detects;semantic analysis
high potential;social-security;behavior patterns;business process;customer behavior;real world
privacy-preserving;data mining techniques;distributed) computation;data privacy;data mining systems;privacy-preserving;data mining;privacy-preserving data publishing;privacy-preserving data mining
association rule mining;apriori algorithm;quantitative association rules;knowledge discovery;case base;quantitative attributes;data mining approach;statistical analysis;data mining;association rules;fuzzy association rules;data-mining process
security level;power grid
redundant features;data collection;prediction model;automatically selecting;real-world;resource management;environmental data;lower cost;linear model;model selection
biomedical applications;workshop on data mining;data mining and knowledge discovery;data mining techniques;conference on knowledge discovery and data;biomedical applications
prediction accuracy;gene ontology;similarity measurement;gene ontology;extracting features from;data mining process;extracting features from
clustering;complex networks;social networks;increasing attention;combinatorial optimization problem;protein-protein interaction network;community structure
data mining tool;decision-making
protein function
classification models;microarray data;highly competitive;vector machine;data sets;wide range;dna microarray data;gene expression
text mining;entity recognition;poor performance;quality assessment;high quality;conditional random fields;machine learning models;entity recognition;online databases;conditional random fields
mining frequent;synthetic datasets;wavelet transformation
optimization problem
singular value decomposition;biclustering algorithms;expression patterns;microarray data;local structures;biological processes;microarray data analysis;data set;clustering problems;benchmark datasets;gene expression
classification performance;training set;classification;microarray data;high dimensional;generalization performance;gene expression;automatically determined;data set;data sets;dimension reduction;high dimensionality;dimension reduction;latent variables
bayesian classifiers;linear features;classification techniques
data mining;international workshop on;data mining methods;conference on knowledge discovery and data;data mining
transaction data;mining frequent itemsets from;redundant information;memory usage;sliding window;mining results;data stream;high accuracy;data streams;false dismissal
data structure;frequent items;large number of;frequent items;data streams;high precision;data distribution
classification techniques;classification;decision tree;mining data streams;search tree;tree nodes;continuous attributes;classification method;data streams;high-speed data streams;search trees;continuous data
skyline computation;skyline queries;data mining;complex data;dimensional data;helps users;threshold-based;skyline objects;high dimensional space
communication overhead;data mining applications;security analysis;privacy preserving;distributed clustering;information theoretic
process mining;process management;process mining;high efficiency;business processes;process models;theoretical analysis;long term;high cost
clustering;distribution information;web applications;edit-distance;similarity measure;real datasets;data representation;xml documents;edit-distance;xml document
ensemble learning;clustering;local sites;distributed data sources;similar results;data mining techniques;clustering results;data set;data sets;distributed clustering;ensemble learning;algorithm called;combinatorial optimization problem;clustering model;data distribution
distributed data sources;data services;data mining and knowledge discovery;data model;mobile agent;discover patterns;mobile agents;databases;database access;service discovery;distributed data mining;distributed data;programming interface
clustering;domain knowledge;detection algorithm;outlier detection;real datasets;feature space;outlier detection;counterpart;outlier detection;parameter tuning;distance-based;detection methods;clustering;high dimensional space
spatial data;ant colony optimization;arbitrary shape;spatial clustering;obstacles constraints;spatial clustering;ant colony;spatial clustering;obstacles constraints;spatial data mining
high-dimensional;data cube;data cube;data warehouses;data warehouse;pre-computation;fact table;data cubes;low dimensional;high dimensional;online analytical processing
data dimension;density estimation;estimation method;clustering algorithm;data set;clustering techniques;algorithm requires;grid-based;dimensional data;grid-based;clustering algorithm based on;high dimensionality
mining frequent patterns;frequent patterns;pattern tree;fp_growth;data mining research;tree structure;mining algorithm;mining frequent patterns;mining process;database scans;candidate generation;large database
large scale;monitoring data;time series;classification;distributed monitoring
supervised learning algorithms;labeled and unlabeled data;learning tasks;training algorithm
clustering;web documents;simulated annealing;classification;genetic algorithm;web documents;key words;clustering documents;classifier;vector space model;clustering
resource management;replication;large-scale;data mining applications;grid services;computing environments;data resources;data transfer;data management;data mining;data-intensive applications;metadata management;data management;data access
association rule mining;frequent itemsets;frequent itemset;minimum support threshold;database
mining frequent patterns;minimum support threshold;frequent patterns;mining frequent patterns;fp_growth;data mining research;tree structure;incremental maintenance;database scans;patterns discovered;frequent pattern;large database
mining closed;data stream environment;closed frequent itemsets;closed itemsets;mining frequent;data mining research;frequent itemsets;3;transaction database;data streams;association rules
efficient clustering;large quantity of;data clustering;distributed environments;distributed clustering;databases;biological data
pruning strategy;large networks;parallel algorithm;np complete;random graphs;load balancing
retrieval effectiveness;language models;information retrieval;retrieval models;retrieval model;language modeling approach;dependency structure
human interactions;ranking problem;trust model;svm models;information retrieval
ubiquitous computing;conference on knowledge discovery and data;international workshop on;ubiquitous computing;data management;data management
received increasing attention;low cost;individual agents;structural properties;small-world;high efficiency;simulation results
sensor network;wireless sensor network
multi-dimensional;communication overhead;wireless sensor network;wireless sensor network;grid-based;sensor nodes;grid-based;security level
ad hoc networks;network traffic;ad hoc;simulation results;detection rate
ubiquitous computing;grid computing;replication
graph model;traffic load;large-scale
hash based;low-cost;hash based;computing environment;security analysis;low-cost
identification method;automatically identifies;radio frequency identification
radio frequency identification;personal information;recognition rates
existing protocols;sensor networks;storage overhead;data security
network security;home network;home network;computing environment
location-based;sensor networks
wireless network;mobile ad hoc networks;simulation results
stream data;data stream mining;estimation error;original data;data stream;streaming data;compressed data;fixed size
web service;web services;access control;web service;computing environments;ubiquitous computing;formal description;access control model
trust model;content delivery;management architecture;access control
web pages;access logs;online algorithms for;considerable effort;access patterns;web searches;current location
recommendation engine;web pages;link structure;browsing behavior;web logs;usage information;distance metric;web page
nearest;users' preferences;partial matching;collaborative filtering;collaborative filtering;nearest;similarity measure;nearest-neighbor;real data sets;information overload
web documents;document categorization;vector representation;instance-based;classification accuracy;graph-based;nearest neighbor;text categorization;decision-tree;information retrieval;vector-based;vector space;document collections;naïve bayes;web document;structural information;vector-space model;document representation;classification algorithm
individual nodes;web content;web navigation
web searching;web search engines;web search engines;query reformulation;user sessions
domain knowledge;sequence alignment;concept hierarchies;similarity scores;concept hierarchy;structural characteristics;management systems;web-content
data collected from;user preferences;random-walk;recommender systems;ranking techniques;memory usage;1,2;data set;extracting knowledge from;computational cost;recommender systems;standard database
clustering;recommendation quality;specially designed;nearest neighbor;collaborative filtering;recommender systems;large-scale;highly scalable;large data sets;recommendation accuracy;clustering model
classification;recommender systems;large number of;information gain;collaborative filtering;classifier;attack detection
support vector machines;naïve bayes classifier;naïve bayes;web logs;feature selection;web log;machine learning techniques;sentiment classification;confidence level;supervised learning;1.5, 2.7;feature selection technique;information source
web search engine;user session;user types;click data;user behavior;query log;search engine;data mining process;queries submitted to;web search engine
web content;search engines;ranking function;web graph
text mining;source code;link analysis techniques;graph mining;feature vector;data mining techniques;domain ontology;feature vectors;instance;data-driven;web mining;data mining;instances;ontology learning;link analysis;mining task;textual) content;semi-automatic
clustering;similarity assessment;instance-based learning;data collections;semantic features;conceptual clustering;representation language;knowledge extraction;conceptual clustering;similar items;clustering algorithms
clustering;temporal sequences;data mining;time series;clinical practice;clustering methods;temporal evolution
genetic programming;knowledge bases;fitness function;conceptual clustering;clustering method;language-independent;distance-based;feature construction;evolutionary approach;clustering algorithm;ontology languages
information content;classification;feature selection method;feature selection;select features;feature selection
domain experts;rule induction methods;database;accuracies;stock market;rule mining;data pre-processing;mining process;post-processing;case study;algorithm selection;extraction methods;temporal data mining;databases;data mining process;temporal data;temporal data mining;pattern extraction
text mining;clustering;classification;data mining techniques;information retrieval;knowledge discovery;language-independent
classification
automatic indexing;classification;16;rule-based;temporal features;feature analysis;high volume;low-level;automatic indexing
document collection;additional knowledge;cluster structure;text documents;adaptive clustering;user defined
hybrid approach;aggregation methods;machine learning;convergence speed;error reduction;single classifier
amino acid;internal structure;mining frequent patterns;real datasets;algorithm named;item sets;graph patterns;complex structures;pruning techniques;real world applications;graph structured data;structured) data
mining frequent patterns;item sets;optimization problem;data mining research;item set;frequent pattern
base classifiers;multi-class classification;ordinal classification;decision rules;gradient descent;ordinal classification;dependent variable;class labels;theoretical framework
medical data;rule sets;data mining;data sets;rule generation;data mining;complex data
(ω) Λ (α → β);database;action rules;classification rules;action rules;6;action based;φ → ψ
training set;training data;document images;learning algorithm
distance measure;web pages;automatic indexing;textual data;domain ontology;domain specific;information retrieval;human judgments;context-based
clinical data;data mining methods;classification;decision tree;classification task;data mining and machine learning;instances;medical knowledge;prediction model;knowledge sources;expert knowledge
information technology;data analysis;multi-aspect;information processing;problem solving
closed patterns;frequent patterns;semi-structured data;polynomial-space;pattern mining;mining frequent;closed) patterns;large collections of;pattern mining algorithms
diverse domains;meaningful information;information access;information networks
misclassification costs;class distributions;cost-sensitive;machine learning;cost-sensitive;classifier performance;operating conditions;classifier
large scale;knowledge based;essential information;massive data;time series
real world datasets;classification;classification;classification accuracy;application domains;label information;relevant attributes;noisy data;subspace clustering;relevant information;irrelevant attributes;subspace clusters;classifier;subspace clustering
synthetic data sets;quality-aware;dense regions;data set;quality-aware;grid-based;subspace clustering;subspace clustering;efficiently identify;subspace clusters;subspace clustering algorithms;clustering quality
mining frequent itemsets from;statistical properties;probabilistic model;computational cost;uncertain data
multiple classes;learning process;tree patterns;training data;relation extraction;entity recognition;unsupervised learning;dependency trees;tree-based;extraction patterns;entity recognition;efficiently computing;multi-class;question answering
clustering problem;density-based;algorithm performs;arbitrary shape;clustering algorithms;fitness function;real-world;spatial datasets;feature-based;case study;agglomerative clustering;grid-based
data normalization;data mining tasks;time series;computationally intensive;efficient retrieval of;motion capture data;multimedia data;space complexity
data samples;equivalence classes;feature construction;constraint-based mining;frequent itemsets;data sets;feature construction;association rule;closed itemsets;classification tasks;mining tasks
market basket data;data mining results;randomized data;synthetic data sets;association rule mining;data miners;randomized response;privacy preserving;association rules
speech recognition;discriminant analysis;privacy-preserving;dimension reduction;face recognition;fisher discriminant analysis;cryptographic techniques;data privacy;privacy-preserving;data mining;fisher discriminant analysis;machine learning algorithms;vertically partitioned data;privacy-preserving data mining
change analysis;unsupervised learning;outlier) detection;data sources;real data;data sets;supervised learning;classifier
clustering;clustering algorithms;spatial data;posterior probabilities;real-world datasets;penalty term;clustering performance;expectation-maximization algorithm;em algorithm;objective function;expectation-maximization
numerical data;numerical attributes
kernel based;tree kernels;tree kernel;unordered tree;classification
positive effect;relevant features;classification;large scale;learning algorithms;informative features;low-level;relevant features
data set;large scale;real-world;link information;personalized services;users' behavior;answer questions;similarity function;instance;algorithm called;individual users
feature space;regularization;data points;generalization performance;vector machine;svm algorithm;training dataset;hidden layer;randomly generated;classifier
frequent itemset mining;data analysis;frequent patterns;data mining and knowledge discovery;large-scale;graph-based;main memory;data compression;frequent itemset mining;frequent itemsets;fast algorithm;data structure;data mining process;knowledge discovery
nearest neighbor algorithm;pattern detection;clustering algorithms;outlier detection;detection techniques;data set;real life;boundary points;nearest neighbors
search results;world wide web;web pages;mixture model;named entities;dirichlet process;social network;web pages;search engines;web document;latent topics
structured databases;graph mining;breadth-first search;highly-correlated;real world datasets;algorithm named;transaction databases;upper bound;pruning techniques;confidence measure;graph databases
finding an optimal;ad-hoc;model trees;learning tasks;numeric data
data stream environment;classification;tree-based;machine learning;data streams;numeric attributes;multi-class;memory intensive;takes place;classification algorithm;classification algorithms
search space;record linkage;record pairs;record linkage;data sets;record linkage is
clustering;large numbers of;large-scale;private information;fault-tolerance;computational complexity;privacy-preserving;data mining;user-centric;privacy-preserving data mining
labeled samples;dimensionality reduction method;unlabeled samples;benchmark data sets;dimensionality reduction methods;global structure;extensive simulations;globally optimal solution;semi-supervised;fisher discriminant analysis;semi-supervised;perform poorly;dimensionality reduction
web documents;fixed length;finding similar;database size;large scale;string data;huge amounts of;high degree;edit distance;databases;short strings;log data;finding similar
frequent itemset;polynomial space;database;frequent itemset mining;frequent itemsets;transaction databases;data mining
data sets;human movement;real-world;vector machine;time series;classifier;image sequences;subspace analysis;motion sequences;motion patterns;multi-class;low-dimensional
mining algorithm;association rule mining algorithms;association rule mining;privacy concerns;incrementally maintaining;incremental maintenance;discovered association rules;databases;association rules;preserving privacy;association rule;discovered rules
web pages;general purpose;spatial information;spatial distribution;algorithm called;search engine;keyword query
synthetic data sets;tree induction;induction algorithms;decision tree;classification accuracy;massive data;case study;large data sets;classification model;wide range;classification models;margin;induction algorithm
benchmark data sets;nearest-neighbor;feature selection;max-margin;feature weighting;class distribution;objective function;feature selection
domain knowledge;learned knowledge;real-world;data mining tasks;single snapshot;data set;data sets;accurate predictions;positive class
data set;learning approaches;network structures;learning algorithm;bayesian network
predictive models;rule set;multiple target;rule learning;learning algorithm;rule sets;classification rules;single target;learning approaches;clustering approach
probabilistic latent semantic analysis;mixture model;language model;real-world;semantic knowledge;language models;expert finding;query terms
multi-dimensional;blind source separation;signal processing;matching method;time series;real data;data sources;privacy issues;data mining;data flow;preserving privacy
label information;class attribute;semi-supervised learning;labeled data;semi-supervised;support vector machines;relational data;classification algorithms
clustering;applications involve;mixture models;data mining tasks;finite mixture;dirichlet mixture;discrete data;databases;continuous data;color image
clustering;linked data;training data;classification techniques;classification;real world situations;binary classifier;record linkage;training examples;require expensive;accurate classification;record pairs;supervised machine learning;data preparation;data mining projects;databases;record linkage is
parameter space;risk management;class distributions;1;machine learning applications;imbalanced datasets;classifier performance;probability estimation
local linear;probabilistic model;dimensional data;finding clusters;principal component analysis
real-life datasets;interactive visual;real life;high dimensional datasets
evaluation functions;learning scheme;decision tree learning;data stream;data streams;batch learning;numerical attributes
high utility;high utility;search space;sparse data;data structure;pattern growth;pattern tree;main memory;long patterns;larger datasets;frequent itemset mining;frequent itemsets;transaction database;large datasets;disk storage;frequent pattern mining
bayesian network;local search;markov blanket;learning approaches;feature subset selection;theoretical basis
clustering;hybrid approach;hybrid model;hidden markov model;fuzzy model;fuzzy logic;artificial neural network
9;relational database;pattern mining;instances;relational tables
uci data sets;test set;neural networks;classification;classifier training;accuracies;predictive modeling;training data;instances;highly correlated;base classifier;neural network

expected number of;support vector regression;designed specifically for;competing methods;generalization performance;automatically determines;vector machine;selection method;support vector;risk minimization;support vector regression;pattern selection;regression model;support vectors
considerable effort;decision makers;data sets;large numbers of
intrusion detection;false positive;data-mining;network intrusion detection;false alarm;matching methods;data set;rates;bayesian approach;network intrusion;anomaly detection;automatically detects
decision trees;taking into account;sensitive data;data mining research;data mining;imbalanced data sets;class probabilities
mining results;frequent itemset;real-life applications;raw data;frequent itemset mining;frequent itemsets;visual representation
frequent pattern mining algorithms;frequent patterns;uncertain data;tree-based;real-life;transaction databases;mining algorithm;frequent pattern mining
clustering;data collections;historical information;data acquisition;stream data;graph based;data streams;limited resources;real world data sets
prediction problem;search term;learn models;feature selection;search engine;user-centric;search behavior;clickstream data
probabilistic latent semantic analysis;language modelling;related terms;model assumes;query expansion;language modelling;naïve bayes;information retrieval;vector space;query expansion technique;related documents;query expansion;language model;query terms
data point;mixture model;data points;em-algorithm;1;probability distribution;joint probability distribution;data stream;bayesian networks;joint probability;naive-bayes;mixture models
training set;pattern recognition;real datasets;similarity matrix;nearest neighbor;classifier
named entities;closely related;topic model;topic models;statistical model;making predictions;statistical models;edge weight
supervised (classification;regression trees;unsupervised classification;decision trees;feature maps;unsupervised techniques
database;emerging patterns;transaction databases;relational data;rough set approach
frequent itemset mining;discovered patterns;latent semantic indexing;weighting schemes;database
clustering;hybrid approach;density-based;pattern recognition;algorithm performs;clustering technique;data mining applications;large databases;grid-based;clustering methods;data mining;clustering algorithm
clustering;clustering;clustering results;dimensional space;clustering process;clustering result;feature weighting
clustering;clustering algorithms;clustering results;clustering methods;similarity measure;clustering method;gene expression analysis;constrained clustering;clustering quality;distance-based;correlation coefficients;data mining;validation measures;gene expression;similarity measures
automatically discovering;amino acid;automatically generating;learning algorithm
computational efficiency;mining algorithm;completeness;association rule mining;complete set of;large databases;pruning strategies;association rule mining;knowledge discovery;negative association rules
optimizer;process parameters;fuzzy neural network;statistical techniques;fuzzy inference system;optimal parameters;artificial intelligence
clustering;clustering;query processing;wireless sensor network;collected data;adaptive clustering;wide range;multi-granularity;energy efficiency;sensor networks;query result;random access;data distribution
local density;locally linear embedding;learning algorithm;locally linear embedding;dimensionality reduction
computational complexity;feature extraction;kernel-based;predictive features;learning algorithms;subset selection;sparse kernel;feature weighting;sparse kernel;kernel-based
identification method;inter-related
data distributions;data structures;local models;online-learning;locally linear;dimensional data;low-dimensional
meaningful clusters;clustering results;traditional clustering algorithms;high dimensional data sets;data sets;clustering result;evidence theory;original data;high dimension;subspace clustering;clustering methods;clustering problems;clustering algorithm;subspace clustering
prediction accuracy;genetic algorithms;data mining methods;multi-objective;genetic algorithm;classification methods;databases;large number of;evaluation criteria;distance-based;databases;prediction models
network analysis;minimum number of;pattern mining;social network;large graph;large graphs;instance;graph patterns;databases;support measure
minimum support;database;cluster-based;main memory;membership functions;fuzzy association rules;cluster-based
classification;data set;instances;data sets;irrelevant attributes;feature selection;bayes classifier;incomplete data;classifier
fast algorithm;duplicate detection;large-scale;short text;optimization procedure;databases;term weighting scheme;ad hoc
survival analysis;customer relationship management
data values;fuzzy set;data summarization;medical diagnosis;rule extraction;imprecise data;rough set theory
clustering;on-line analytical processing;larger datasets;clustering task;dimensionality reduction;clustering algorithm;data management
pattern-based;classification;emerging patterns;transaction databases;image classification;data mining problems
minimum support threshold;minimum support;rules generated;user-defined;user defined;support threshold
clustering;instances;association rule mining;association rule mining algorithms;traditional clustering algorithms
ontology-based;web search;aggregation techniques;user-centered;rank-based;data set;user interests;user preferences;rank aggregation;web search;primary goal
neural networks;time series;data set;stock data;data mining;time-series;neural network;short-term
web pages;topic structure;feature extraction;text categorization;category structure;domain- specific;dictionary-based;web page;text categorization
clustering;clustering algorithms;local search;clustering problem;prior works;prior works
instance;labeled training;classification;active learning;active learning;instances;version space;target function;base classifier;unlabeled instances;strong classifier;learning method;sampling methods
taking into account;semi-supervised setting;multiple labels;test image;annotated corpus;limited number of;image annotation
clustering;clustering results;large number of;classification
accurate classifier;learning algorithm;semi-supervised;semi-supervised feature selection;medical diagnosis;training set;feature selection algorithms;unlabeled examples;5;feature selection methods;labeled examples;real world applications;optimal set of
statistical method;automatic extraction of;automatically extract
hybrid approach;low precision;pattern-based;learning process;low recall;ontology learning;ontology learning
clustering;clustering algorithms;data sets;data points;related data;categorical data;original data;data set;reference set;numerical data
human effort
analysis task;feature selection;evaluation criteria
network structure;real world;social network;social networks;visual exploration of
fp-tree;database;pattern tree;incremental mining;fp-growth;tree structure;database scans;data streams;candidate generation;frequent pattern mining
text mining;biological entities;entity recognition;existing knowledge;entity recognition;information extraction;information extraction system
statistics-based;high-level;video annotation;visual features;association patterns
high density;detection algorithm;detect outliers;large datasets;outlier factor
graph construction;semi-supervised;learning strategy;web spam;label propagation
share information;human activity;spatio-temporal;social network;social networks
graph datasets;graph patterns;graph databases;complete set of
social security;case study;association rule mining algorithms;class attribute;association rule mining;compared with conventional;rule sets;frequent itemsets;rule generation;actionable knowledge;association rule;association rules;interestingness measures
semi-automatically;highly sensitive
prediction accuracy;classification problem;classification;decision tree;sample data;decision tree;classification methods;classification rules;data sets;classification model;rough sets;classification method;data mining;automatically generate;classifier
ranking results;real-world;communication network;social network;latent semantic analysis;social networks;ranking measures;latent semantic indexing;text analysis;working group;concept evolution
case studies;long term;web archive;web archive
computing infrastructure;large scale;web-scale;advanced search;search engines;online advertising;data mining;computing platform
data mining;network management;high speed data streams;high speed;data streams
data mining models;multi-criteria;real world;data mining methods;data mining
dynamic nature of;web data;interesting patterns
community structures;high quality;social network;real-world networks;optimization process;protein-protein interaction network
high-confidence;high confidence;classification;instance;mining frequent;pattern-growth;rule mining;rule-generation;skewed;classification rules;high confidence;interesting rules;class distribution
conventional methods;collaborative filtering;information sources;selection process;web content;recommendation methods;web content;web usage data
search tool;small world;social networks
pattern-growth;frequent subtrees
web based;sufficient conditions;concept hierarchies;concept hierarchies;large corpora;low precision;low recall;natural language processing;high recall;high precision
detection algorithms;attribute values;data mining technique;real data sets;outlier detection;probabilistic approach;data sets;detection performance;databases;detecting outliers;increasing attention
agent based;multi-agent system;classification;wireless sensor network;energy-efficient;wireless sensor network;simulation results;low-power;clustering algorithm;wireless communication
graph model;scheduling algorithm;global optimization;genetic algorithm
grid computing;scheduling algorithm;graph construction;scheduling problem;scheduling strategy;grid environment
information dissemination;data grid;distributed environments;data-sharing;load balancing
low bandwidth;low cost;cluster-based;moving objects;video-based;stream data;sensor networks;video-based;sensor networks;mining algorithm
grid computing;virtual machine;computing environment
real-world;dynamic nature of;sensor data streams;sensor networks;multi-scale;window-based;data mining
grid environment
operator;query optimization;decision procedure for;xpath queries;xml tree
xquery language;data transformation;source schema;xml documents;data integration;global schema
clustering;xml element;sequence patterns;tree models;xml document;xml documents;hierarchical structure;finding clusters;similarity computation;xml element;semantic information;tree model
genetic programming;training set;relevance feedback;test query;retrieval performance;query expansion;information retrieval systems;standard datasets
multi-domain;ontology-based;information service;semantic knowledge;query language;multi-domain;information service;domain-specific;natural language queries;information services;natural language processing;natural language;linguistic knowledge;mobile users;obtain information
database;web queries;xml query languages;1;structured information;evaluating queries;web queries
sensitive attributes;sensitive attribute;relational table
web pages;user study;end users
graphical representation of;np-hard
multi-agent;computation cost;proposed protocol
service provider;proposed protocol;hash function
greedy algorithm;multi-dimensional;partition based;security constraints;data owners;np-hard problem;database;single-dimensional;sensitive data;encrypted data;information leakage;data management;data distribution
rfid based;radio frequency identification;web service;increasing number of;mobile communication;mobile phone;sensor network;rfid technology
pattern selection
security problems;computation cost;existing protocols;communication bandwidth;proposed protocol;desirable properties
sensitive information;prototype systems;xml databases;schema information;concept called;verification framework
pattern discovery;search queries;great flexibility;lower level
web pages;extracted information;web information extraction;web pages;relevant information;fully-automatic;semi-automatic
hand-held;interface design;database;ad-hoc;mobile devices;query results
pattern-based;web pages;pattern-based;information extraction;search engines;web resources;vision-based;human supervision;page content
clustering;high-quality;data extraction;deep web;data sources;complex data;structured data;large number of
fusion method;data fusion;linear combination;information retrieval
search space;sequence data;index structure;desirable property;emerging applications;similarity search;computational cost;efficient similarity search;long sequences;increasingly popular
large scale;operator;pre-processing
highly dynamic;design space
overlay network
information exchange;database technologies;recursive queries
domain experts;database;automatically extracting;tool called;domain specific;produce high quality;common knowledge;occurrence frequency
knowledge sharing;cluster-based;knowledge management;concept-based;semantic web;edit distance;instances;clustering techniques;formal concept analysis;conceptual clustering;ontological knowledge;knowledge sources;clustering model
distributed computing;web services;web service;large scale distributed systems;keyword-based;1;category-based;web services;service discovery
domain experts;data cleaning;domain specific;rule mining;data quality;ontology based;user interaction
pre-defined;web-based;web applications;end users;web browsers;user-defined;formal specification;service-oriented;generated automatically;behavior model;user-friendly;model-checking
feature selection algorithm;configuration parameters;key parameters;feature selection;information gain
query patterns;hash-based;mobile communications;query response time;distributed applications;query load;data distribution;customer data;load balancing
algorithm called;continuous attributes
real life;time series;vice versa;movement patterns
remote sensing images;inductive learning;spatial data;data mining techniques;image database;spatio-temporal;remote sensing;association rules mining;remote sensing data
approximate answers;data structure;frequently occurring;internet traffic;sliding window;aggregate queries over;sliding windows;limited-memory;internet traffic;limited memory
clustering algorithm;learning examples;usage patterns;web search results;takes into account
statistical properties;web pages;web graph;web graph;web graphs;web page
web pages;web search;result page;web pages;search engine;web search
semantic information;semantic analysis;simulation experiments;algorithm named
training set;classification problem;kernel-based;classification;3;large scale;real-world;svm-based;data acquisition;mixture components;data set;fisher discriminant analysis;web content;noisy data;fisher discriminant analysis;image annotation;statistical learning
data samples;database;query relaxation;deep web;databases;result set
interface design;query capabilities;user interface;user queries;data sources;operator
web services
instances
pre-defined;involving multiple;ir research;multiple queries;instance;user effort;test collections;real life;highly) relevant documents;session-based;common assumption;query sessions
preference judgments;search engines;information retrieval systems;provide evidence
search results;search topics;clickthrough data;click data;user click;user feedback;search result;information retrieval performance;user study;individual users
clustering;source code;training data;web documents;tree structure;template based;distance measures;clustering
search results;pre-retrieval;document level;data types;query performance prediction;retrieval methods;query performance prediction;search tasks;pre-retrieval
clustering;exhaustive search;retrieval accuracy;real-world;clustering method;overlay network;information retrieval;common interests;dynamic environment;high recall;network traffic
tf*idf;directed graph;social network;ranking algorithms;named entity;social networks;search engine;markov chain;named entities
simple models;trec 2007 blog track;retrieval model;ad hoc retrieval;automatically generate;document ranking
data structure;commercial search engines;search algorithms;graph-based;real-world;graph-based ranking;vector space;link-based;user activity;search engine;log data;web page
parallel corpus;parallel corpora;corpus-based
end user;web search;cross-language information retrieval;search effectiveness
rule based;matching methods;cross-lingual;cross-language information retrieval;result quality
2;text categorization;information retrieval
similarity information;specific algorithms;language models;document retrieval;passage-based;relevance models;relevance model;probabilistic model;language model;document ranking
retrieval effectiveness;document-centric;estimation technique;weighting schemes;retrieval models;information retrieval models;language modeling
semantic indexing;training examples;web-based;level features;active learning;active learning approach;active learning
user evaluation;community based;recommendation algorithms;implicit feedback;usage information;graph-based;video retrieval
scientific literature;retrieval performance;test collection
information content;training data;automatic extraction of;stopwords;labeled documents;domain-specific;document frequency;classifier performance;information loss;information gain
specific features;user interactions;special attention;large-scale;book search;information retrieval;retrieval model;book search;ir) research;digital libraries;unique characteristics;ir models;ir methods
search tool;retrieval performance
entity ranking;document collection;web pages;xml document collections;link structure;named entities;document structure;information retrieval;xml elements
ad hoc;local context;semantically related;link structure;focused retrieval;information sources;retrieval tasks;main findings;ad hoc retrieval;link evidence;ad hoc information retrieval;retrieval effectiveness;web data;test collection
clustering;search task;trec enterprise;expert search;high quality;expert finding;retrieval performance;relevant expertise
finding task
search) task;expert finding;documents retrieved;trec enterprise;language models
formal models;expert search;ranking performance;nearest-neighbors;1, 2, 3
trec collections;language model;association rule mining;takes into account;query likelihood;high-order;relevance model;information flow;computational overhead;language modeling;association rules;sliding windows;query terms;feedback documents
trec-11 collections;relevance score;probabilistic model;term frequency;average precision;query terms;term proximity
relevance information;data fusion;sliding window;fusion methods;probabilistic data;sliding windows
probabilistic latent semantic analysis;training set;document classification;valuable information;error model;text classification problems;9;unlabeled documents;multiclass problems;partially labeled;6;text classification;document collections;semi-supervised;semi-supervised;model parameters
keyword queries;term frequency;language modeling approaches;verbose queries;retrieval model;term frequency;smoothing methods;average precision
dirichlet prior;language models;document retrieval;query likelihood;language model;document length;information retrieval;retrieval performance;scoring function;language modeling
limited memory;finding task;retrieval tasks;information retrieval;maximum entropy;competitive performance;feature weights
scoring functions;information retrieval;information retrieval performance;discounted cumulative gain;result set;average precision
directed graph
semantic role labeling;semantic representation;text understanding;takes place;question answering;natural language
question answering;anchor text;question answering;retrieval task;link structure;relevant information;query expansion;query terms
graph model;graph-based;iterative algorithm;multi-document summarization;global information
probabilistic representation;clustering algorithms;language-modeling;similar documents;retrieval methods
passage retrieval;language-modeling framework;document ranking;language models;document retrieval;passage retrieval;relevance models;pseudo relevance feedback;relevance models;relevance model;ranking method;query expansion;query sets
10;vector space;heterogeneous systems;ontology matching;ontology alignment;ontology matching
ir systems;information retrieval;information retrieval
multi-modal;semantic features;semantic relationships;graph based;semantic relationships;contextual information;image annotation;similarity measures
similarity function;detection algorithm
content-based search;time series;computationally intensive;multimedia retrieval;multimedia information;real world;retrieval systems;multimedia data;space complexity
text classification;representation scheme;classification results;syntactic structure;efficient computation
wikipedia-based;document collection;cross-language;similarity measure;instance;language-independent;retrieval model;similar documents
singular value decomposition;vector space;semantic relations between;semantic analysis;principal component analysis;models built;semantic relations;local structure;high dimensionality
cross-media;appearance models;ranking function;database
mixture model;document selection;document retrieval;pseudo relevance feedback;retrieval performance;feedback documents;optimal" number of;query-specific;query expansion
ground truth;document ranking;expert search;assists users;relevant expertise;ranking documents;search evaluation
clustering;search results;web search;search result;user query;ranking algorithms;web search;document rankings
spatio-temporal;spatial context;design issues
text classification;structural information;classification results;classification;syntactic structure
web search;temporal information;temporal dimension;users' behavior;web search queries;information extraction techniques;specific topics
music information retrieval;web content;web pages;automatically generated
user clicks;clickthrough data;relevance judgments;relevance feedback;search effectiveness;search quality;user clicks
compression techniques;information retrieval;test collections;link graph;search engine;link analysis techniques
ad-hoc retrieval;relevance information;individual queries;statistical information;retrieval models;retrieval performance;term dependency;main components
evaluation measure;information retrieval systems
classification;language model;language models;text classification;opinion mining;natural language
image retrieval;high-quality;textual information;text-based;text-based image retrieval;search engines;web document;image annotation;retrieval models;image annotations;term-based
web search;web search;search interface;search engine
natural language queries;textual information;search engine;contextual information;natural language;document rankings;vector space model;web retrieval
relevant information;topic tracking;business users
user profile;user interactions;graph-based;information retrieval;user queries;user profiles;information retrieval systems
information retrieval evaluation
search results;personalized search;hybrid method;taking into account;specific information;hybrid method;digital libraries
language-model;active user;vector-space;implicit feedback;relevance feedback;retrieval models;provide feedback;user search behavior;information retrieval;search session
cluster-based retrieval;retrieval effectiveness;cluster-based retrieval
classification problem;rule-based;large number of;information retrieval;logistic regression;learning task
ranking algorithm;question answering;starting points;linguistic features
cluster-based;retrieval methods;nearest neighbor;retrieval effectiveness;cluster-based retrieval
named entity
inter-document similarity;inter-document similarity;estimation problem;evaluation measure;relevance model;cluster-based retrieval;term-based
training data;language model;query topic;query difficulty;average precision;semantic similarity;query terms;document set
web directories;web directories;query processing strategy
search engine;search efficiency;making decisions;information retrieval
document retrieval;passage retrieval;information retrieval;retrieval performance;named entity;information access;question answering
data analysis;ad hoc;statistical databases
information sharing;privacy policies;local patterns;privacy preserving;distributed clustering;clustering algorithm;distributed data
greedy heuristics;optimization algorithms;greedy algorithms;higher quality;increasing attention;health information;information loss
social security;database;databases
graph-based;search engine;anonymization techniques;query log;query logs;special case
large volume;predictive models;decision trees;models learned;decision tree;statistics-based;data owners;distributed environments;privacy-preserving data mining
data integrity;high accuracy;randomized data;large number of;multiple sites;data mining tasks;distance-preserving;privacy-preserving;classification errors;distributed data;accurate classifier;private data;distance-preserving;training dataset;supervised classification;accurate classifiers;standard datasets;worst case;privacy concerns;application domains;privacy-preserving;kernel density estimation;classifier
vertically partitioned;database;cross validation;candidate models;machine learning;model selection;privacy-preserving;data mining;distributed data;model selection
predictive model for;structural characteristics;graph data
data analysis;national security;data warehouse;rule learning;geographic information;information extraction;predictive model;entity recognition;data mining;predictive analytics;cluster analysis;markup language;ecml pkdd
edge detection;database;rates;pattern analysis;video-stream;human face
matlab/simulink
pattern matching;database;feature extraction;object detection;hand tracking;video camera;pattern matching
traffic control;multi-agent;real data;performance prediction;bayesian networks;multi agent;traffic control
low bandwidth;information systems;large-scale;web service;mobile computing;mobile devices;large repositories of;wireless networks
text categorization;linear support vector machines;text documents;increasing number of;text categorization;linear svm;linear svm;learning speed;simple algorithm
world wide web;profile based;classification techniques;high quality;textual content;text classification;context-based
amino acid;correlation coefficient;amino acid;vector machine;fold cross-validation
medical applications;open source
data set;hybrid approach;heuristic algorithm;ant colony;attack detection
computing devices;embedded systems;user-defined;human machine
decision trees;database;data set;link prediction;nodes represent;link prediction;semantic information
cost-sensitive classification;svm-based
evolutionary algorithms;optimization problems;differential evolution;convergence rate;differential evolution
clustering;local optima;data clustering;wide range;data mining;clustering;clustering algorithms;large datasets;clustering algorithm;benchmark datasets;clustering problem;optimization algorithm;data clustering;algorithm combines;fast convergence;pso algorithm;search technique;evolutionary algorithms;unsupervised classification;solution space;population-based;pso algorithm
hidden patterns;classification;classification;dna sequences;dna sequence;artificial neural network
analyzing data;data warehousing;data generation;forensic analysis;data mining
dynamic programming;large scale;large scale;control strategies;optimal control;simulation results;optimal control
low cost;position information;unknown environments;piecewise linear;neural network;training algorithm;neural network;hidden layer;low cost
network bandwidth;feature extraction;image feature;classification;classification;storage space;feature extraction;false positive;color histogram;classification method;image classification;artificial neural networks;artificial neural network
classification;classification accuracy;artificial neural networks;complete set of;missing values;accurate classification;machine learning;data set;pre-processing;artificial neural networks;classifier
statistical significance;machine learning algorithms;machine learning algorithms;statistical technique
agent based;data collection;data collection;sensor networks;mobile agents;sensor network;data collected;mobile agents
artificial neural networks;pre-processing;image compression;image compression;artificial neural networks
test data;multi layer;character recognition;neural network
bp neural network;bp algorithm;artificial intelligence;bp algorithm
fitness;genetic algorithm
sensor nodes;sensor networks;simulation results;sensor networks
state space;linear model;simulation studies;neural network;control strategies;behavior model;pattern recognition;fuzzy logic
character recognition;character recognition
clustering;spectral clustering;cut algorithm;manifold learning;pattern recognition;high dimensional;classification accuracy;learning algorithm;low-dimensional;local linear;data analysis;multivariate data;clustering algorithm;dimensional data;dimension reduction;simulation results;standard datasets
clustering;ant colony optimization;arbitrary shape;clustering method;clustering method;global search;clustering algorithm;additional information
web-based;expert systems;knowledge base;decision making;inference engine
clustering;data set;expectation- maximization;clustering approach;expectation-maximization
video sequence;data rates;signal processing;video coding;remote sensing;multimedia applications;compression techniques;block based;wavelet based;video coding;multimedia applications
pre-defined;syntactic structures;query forms;application domain;semantic model;semantic model;query processing;semantic representation;syntactic structure
fourier transform;edge-preserving;image restoration;image restoration;color image
group members;multiple users;resource utilization;computational complexity;storage requirements
support vector machines;vector machine;quadratic programming problem;classification
face recognition;pattern recognition;database;face recognition;training samples;limited number of;input images;principal component analysis;face database
computational approach;rough set;rough set;common properties;rule sets;databases
radial basis function;radial basis function;target image;image registration
prediction problem;genetic algorithm;genetic algorithm;cost-sensitive;risk analysis;real-life;predictive performance;cost-sensitive;learning vector quantization;classification task;learning vector quantization;classification algorithms
classification techniques;classification;classification;risk factors;evaluation scheme;machine learning;data set;data mining;neural network;alzheimer's disease;neural networks
security requirements;processing algorithms;vehicle detection;recognition systems;vision based;vehicle detection;vision-based
intrusion detection;data mining technique;intrusion detection;vector machine;data set;classification problems
classification problem;classification;svm-based;data mining applications;medical diagnosis;cost-sensitive;data set;svm classifier;support vector machines;cost based;positive class;classification algorithm
aspect ratio;fuzzy logic;fuzzy logic
aspect ratio;fuzzy logic;fuzzy logic
neural network;correct classification;artificial neural networks;neural network;neural networks

neural network;human brain;regression method;neural network;neural networks
increasing complexity;spatial data;feature extraction;semi-automatic;high resolution;fully automatic;semi-automatic;high resolution
genetic algorithms;genetic algorithm;np-complete problem;genetic algorithm;genetic algorithm;finding an optimal;heuristic algorithm;general problem;simulation results;scheduling algorithm
acoustic features;nearest neighbor;classification;classification;database;classification accuracy;retrieval tasks;feature selection algorithm;neural networks;feature selection;multi-layer;feature vectors;retrieval applications
training data;noisy data;neural network
benchmark data;compression techniques;web based;compression rate;database;dna sequences;compression techniques;compression technique;1;web page
ad-hoc networks;mobile user;mobility patterns;wireless communication;energy efficient;energy efficient;energy efficiency;network lifetime;wireless networks;network topology;small size
context free;theoretical basis for;tedious task
information content;semantically similar;query expansion;ontology based;semantic similarity;information retrieval;similarity function;similarity computation;information retrieval models;ontology based;retrieval effectiveness
data mining methods;based reasoning;machine learning;hybrid method;machine learning approaches;data mining;artificial neural network
autonomous navigation;artificial intelligence
prediction model;rough set;rough set;neural network;network model;neural network
behavior patterns;agent behaviors;learning algorithm;reinforcement learning;rough set theory
code generation;data dependencies;multi-core
simulation result;high speed
data mining application;classification;database;medical databases;medical diagnosis;machine learning;data set;instances;apriori algorithm;association rules;numeric data;discovered rules;data mining;association rule;numeric attributes;pre-processing;market basket analysis;knowledge discovery
prediction accuracy;support vector regression;physical characteristics;correlation coefficient;supervised machine learning;squared error;support vector regression;regression model
wavelet based;learning rate;uniform distribution;learning algorithm;input space;neural network;basis functions;wavelet based;learning rate;neural network
clustering;clustering;training data;classification;classification;supervised approach;svm based;classifier;data set;supervised approach;training data set;class labels;semi-supervised;labeled data;network traffic
pattern recognition;discrete optimization;international conference on;energy minimization;partial differential equations;markov random fields;image denoising
database technology;international conference on;database theory;data exchange;san diego;program committee
parallel programming;big data;database;database theory;general-purpose;wide range;large datasets;competitive performance;programming models;distributed systems;relational algebra
graph-structured data;sequence alignment;semantic associations;regular path queries;rdf graphs;combined complexity;conjunctive queries;graph structured data;pattern matching;path queries;query evaluation;problems arising
speech recognition;special case;computational biology;radio-frequency identification;probabilistic database;finite-state;statistical model;regular expression;sequential data;upper bound;information extraction;image processing
relational algebra;upper bounds;query language;higher-order;input query;conjunctive queries
query optimization;database;worst-case;data cleaning;data stream;success probability;data mining;network topology
query optimization;cardinality estimation;cardinality estimation;conjunctive queries;principled framework;data management
semantic classes;natural-language;knowledge-sharing;semantic level;named entities;web sources;automatically extracting;search engines;web sources;high recall;knowledge base;high precision
evolving data;selectivity estimation;approximate query answering;sliding window;data set;data sets;fundamental problem;query planning;data management;distributed streams
query evaluation;database;nested queries;aggregate queries;view maintenance;query operators;databases;normal form
clustering;data streams;network monitoring;databases;support vector machines;nearest neighbor search
query optimization;state space;close connection;data dependencies;database;semantic query optimization;type information;decision problems;logic-based;conjunctive queries;cost function
query answers;instances;related queries;data analysis tasks;similar behavior
13;loss-function;aggregate information;database;sensitive data;5;8;information consumers;query results;decision theory
query answers;synthetic data;differentially private;randomized algorithms;mathematical analysis
data set;massive data;information theory;lower bounds;data streams
instance;bounded size;complete information;completeness;open-world;database;databases;master data;upper bounds;instances;real life;databases;9;closed-world;missing values;21
database;uncertain databases;conjunctive queries over;data model;primary key;decision procedure for;conjunctive queries;low complexity
pattern-based;complexity bounds;data exchange;xml queries;xml query language;incomplete information;schema mappings;query answering;databases;missing information;static analysis;model-theoretic;data integration
completeness;algorithm runs in;probabilistic databases;conjunctive queries;tuple-independent;algorithm to compute
query evaluation;database instances;database;random walk;probabilistic databases;query languages;markov chain
redundant information;schema mapping;schema evolution;operator;schema mappings;metadata management
xml schemas;xml repositories;xml documents;formal language;schema design;deterministic finite automata;global schema
schema integration;basic operations;regular tree;decision problems;xml schema;xml schema
tuple generating dependencies;database schemas;schema mapping;real-life applications;schema mappings are;data exchange;negative examples;instance;databases;data examples;high-level;schema mappings;data integration
polynomial space;auxiliary;database;general case;search engine;search algorithm
sample set;learning algorithm;similar results;xml transformations;xml transformations
external memory;large databases;index structures;block size;disk accesses;hash table;databases;hash function
disk block;variable size;search tree;search cost;performance guarantees;expected cost;dynamic-programming algorithm;tree structure;performance guarantees;worst-case;arbitrary length
implication problem;data dependencies
decision problem;database;database theory;greedy algorithms;specific set of;global consistency;local consistency;instances;decomposition methods;conjunctive queries
acm symposium on;database;massive volumes;large scale distributed systems;acm sigmod;technical program;operating systems;program committee;operating systems
space usage;data obtained from;normal forms;online advertising;evaluation techniques
data manipulation;query results;database systems;database;query evaluation;automatically generating;data provenance;operator;query result
database;hierarchical clustering;hierarchical clustering algorithm;visualization techniques;preference queries;skyline objects
multi-dimensional;total number of;dimensional space;high dimensional;complex objects;query processing;databases;uncertain objects
bayesian inference;data collection;radio frequency identification;state detection;data cleaning;low quality;high efficiency;metropolis-hastings;data redundancy;prior knowledge;data cleansing;rates;raw data;data quality;spatio-temporal;extensive simulations;data distribution
integrating data from;instances;similarity computation;real-world data sets;relational databases;test results;similarity measures
missing information;bayesian network;exact inference;database;data cleaning;real-world;user defined functions;inference algorithm;belief propagation;statistical method;bayesian networks;databases;real-world data sets;statistical inference;efficient approximate;statistical framework
user study;temporal patterns;convex optimization;recommendation algorithms
growing number of;shortest paths;tree decomposition;breadth first search;index size;index construction;pre-computation;large graphs;keyword search;shortest path;social networks;databases;query answering;query processing
user-friendly;graph database;synthetic datasets;query formulation;visual query;indexing scheme;graph databases;query processing in;algorithm called;visual interfaces;efficient retrieval;result set;graph construction;subgraph matching;query results;query languages;query processing
biological networks;reachability queries;synthetic datasets;sampling techniques;transitive closure;tree-based;labeled graphs;huge amounts of;semantic web;reachability query;social networks;graph data;spanning tree;real-world graphs;graph databases
web graph;large-scale;large graphs;social networks;fault-tolerant;computational model
online aggregation;query times;join algorithm;ad-hoc queries;flash-based;join algorithms;rates;online aggregation;data streams;join operations
sampling method;stream processing;random variables;processing algorithms;uncertain data;relational operators;data model;efficient computation;monitoring applications;case study;detection results;evaluation techniques;data streams;uncertain data streams;stream systems;uncertain data streams
fast approximate;graph partitioning;false positives;approximation algorithms;real datasets;error guarantees;data center;environmental monitoring;correlation coefficients;time-series;data intensive applications;cpu cost
data summarization;event data;hidden markov model;generation process;hidden markov model;interpretability
relational algebra;database;programming languages;sql queries;data tables;database engine;data management;data storage
query optimization;database contents;replication;database schema;database;query interfaces;information management;database content;web information systems;clustering techniques;scalability problems;scientific databases;low overhead;databases;query optimizations;attribute-level
conflicting information;database;data items;conflict resolution;stable models;databases;logic program
database operations;database systems;database;energy-efficient;database server;data centers;power consumption;energy efficiency;query plans;configuration parameters
times faster than;multi-player;road networks;applications ranging from;baseline algorithm;large number of;moving object;partitioning scheme;real-world data sets;moving objects;query processing;location-based services;continuous spatio-temporal queries;road network;continuous queries;indexing technique;continuous query
lower bound;database;memory usage;spatial distance;spatial-temporal;general-purpose;similarity function;search effort;databases;spatial index;upper bound
real-world;join processing;sensor networks;sensor data;communication costs;join queries;continuous queries
communication overhead;data cleaning;outlier detection;similarity metrics;sensory data;sensor networks;locality sensitive hashing;load balancing;increasingly popular
query evaluation;graphical models;modeling framework;probabilistic database;graphical model;probabilistic databases;probabilistic data;evaluating queries;tuple-level;dependency structure
probabilistic databases;false positives;database;data uncertainty;query answers;probabilistic data;pruning methods;query answering;high cost;graph representation;range queries;data collected from
query optimization;ranking queries;uncertain databases;uncertain data;uncertain database;sql queries;optimization problem;query plan;query execution;probabilistic threshold;nearest-neighbor queries;operator;attribute sets;range queries
prohibitively expensive;pruning techniques;similarity measure;data cleaning;scientific computing;real-world applications;edit distance;probabilistic data;distance based;databases;operator;real data;similarity joins;similarity join;database engines;data integration
memory bandwidth;data accesses;database;tree search;compression techniques;integrating multiple;tree structured;computing power
low bandwidth;memory bandwidth;database operations;analytical models;competitive analysis;data sizes;databases
special case;synthetic data;poor performance;flash-based;update operations;computing devices;storage systems;disk-based;storing data
web search;data structure;data mining and machine learning;similarity search;high accuracy;data mining;locality sensitive hashing;brute force search;extensive simulations;nearest neighbor search;search methods;approximate nearest neighbor;streaming applications;computational biology;access control;database entries;instance;locality sensitive hashing;real world;high speed;clustering;applications including;bit vector;linear space;euclidean space;data base;database;large scale;lower bounds;similarity search;nearest neighbors;high dimensional spaces
11;data source;search techniques;32;scientific data;databases;data sets;data items;user feedback;data sources;2;provide feedback;automatically discover;source databases;keyword search;label propagation;machine learning;data integration
query execution;service calls;high-quality;knowledge bases;knowledge-sharing;data model;information extraction;real-life;web services;user query;knowledge base;web services
clustering;web scale;large numbers of;classification techniques;human intervention;structured data sources;clustering approach;multi-domain;keyword query;data integration systems;semi-automatic;fully automatic;probabilistic model;semi-automatically;structured data;source schemas;data integration;classifier
knowledge bases;rewritten query;text search;structured data from;query language;keyword-based;rich semantics;keyword search;extracted data;web sources;schema information;query languages;structured queries;query processing
social media;markov random field;social media data;multiple features;probabilistic model;multimedia objects;feature fusion;temporal information;real-life;efficient retrieval;similarity measure between;social interaction
graph theory;completeness;memory usage;social network analysis;large graph;memory space;large networks;algorithms require;essential information;random access;fundamental problem;external-memory;update cost
real datasets;2;social networks;graph data;data utility;privacy preserving;np-hard
privacy-aware;data collection;internal node;index structure;overlay network;data items;network nodes;user queries;privacy-preserving distributed;online communities;internal nodes
database operations;parallel programming;multi-core;large volumes of data;user-defined;data structures;generic framework;data-intensive
efficient parallel;set-similarity;fine-grained;real datasets;main memory;replication;similarity joins
declarative queries;directed acyclic graphs;data skew;query execution times;data processing;unlike prior;path-based;small-scale
multiple queries;database systems;data warehouses;push based;database;data access;data-centric;incur high;analytic processing
parameter values;query optimizer;database applications;decision support queries;execution plan;commercial dbms;multiple times;instances;real-world
transaction management;data structure;keeping track of;computational overhead;databases
archived data;aggregation algorithm;database;ranking problem;instances;web archive;shared execution;databases;keyword query;space partitioning;indexing technique
optimization techniques;database;ad-hoc;database research;view maintenance;optimization problems;error prone
business model;database applications;cloud services;comprehensive evaluation;data-intensive applications;transaction processing
indexing structure;multi-dimensional;tree based;database systems;large-scale;23;database;indexing scheme;tree nodes;multi-dimensional data;query processing in;query processing;cost model;data volume;compute nodes
low overhead;database;main memory;makes sense;database;low overhead;databases;concurrency control;distributed transactions
declarative query language;continuous queries over;optimization techniques;database;network management;meta-data;extensible framework;forensic analysis;efficient querying;issue queries;wide range;network provenance;data provenance;internet-scale;distributed streams;internet-scale;distributed environment
data objects;query optimization;highly skewed;real-world applications;data set;query result;histogram construction
order statistics;lower bound;query optimization;designed to support;databases;temporal databases
online aggregation;multiple queries;online aggregation;partitioning strategy;random samples;queries efficiently;multiple queries;data flow
commercial database systems;error bounds
data structure;monte carlo;database;conjunctive queries over;probabilistic databases;exact computation;tree structured;conjunctive queries;tuple independent;optimization techniques
uncertain data;statistical queries;database objects;threshold queries;database
nearest;search space;search algorithm;fuzzy objects;distance function;ad-hoc;pruning rules;refinement process;knn queries;image databases;optimization techniques;fuzzy objects;query processing;real datasets;nearest neighbor search;probability threshold;nearest neighbor search
labeling scheme;reachability query
execution environment;security requirements;security concerns;software systems;wide range;data processing;query processor;code generation;diverse applications;distributed systems
location traces;aggregation algorithm;central server;real-life data-sets;fourier transform;aggregate queries;differentially private;data-mining applications;differentially private;query answers;query answer;time-series;perform poorly;expected error;individual users;large number of
privacy preserving data publishing;privacy-preserving data publishing;anonymization algorithm;anonymized data
preserving privacy;peer data management systems;real data;security analysis;internet scale;query processing;query answering;query results;data integration
active learning;active learning;learning approaches;learning algorithm;labeled examples;classifier;quality guarantees;record matching
web corpus;million documents;text documents;information extraction systems;task-specific;structured information;information extraction;answer questions;information extraction system
information extraction;data values;attribute values;test data;unsupervised learning;semi-structured;information extraction;probabilistic approach;learning strategies;high degree of;information extraction;text segmentation
keyword queries;data accesses;applications including;increasing number of;data sources;data items;network latency;data obtained from;data freshness;keyword search;data quality;search interface;web crawling;web data;web pages
web applications;database;web sites;web-scale;rates;event streams;high-rate;web page
data sets;online shopping;large sets of;composite items;web users
hidden web;database;approximate query processing;hidden databases;search queries;databases;theoretical analysis;web interface
large networks;itemset mining;graph mining;large-scale;frequent subgraphs;pattern mining;interesting patterns;large graphs;frequent itemsets;real-life;graph patterns;fp-tree;malware detection
feature space;parallel computing;patterns mined;classification accuracy;large databases;large graph;graph classification;evolutionary computation;databases;association rules
minimum number of;adaptive algorithm;semi-join;instance;worst case;bipartite graph;arbitrarily complex;spatial join;join predicates
similarity threshold;np-complete;real datasets;efficient algorithms to;similarity search;filtering techniques;chemical compounds;distance-based;low" quality;verification framework;indexing technique
fast response;tree based;data updates;similarity measure;large number of;indexing techniques;database;index structure;biological sequence;information retrieval;edit distance;high scalability;similarity search;similarity queries;search queries;fundamental problem;search process;query types;join queries;memory constraints
set similarity;real life data sets;inverted lists;index structure;similarity functions;similarity function;indexing problem;string transformations;set containment;record matching
optimization technique;optimizer;database systems;database;linear programming;workload-aware;database objects;storage devices
indexing schemes;formal models;indexing techniques;query language;probabilistic databases;incremental maintenance;keyword search over;data provenance;databases;query answering;query results;schema mappings;data management
open-source;4, 3;large scale;query language;scientific applications;web log data;information management;application domains;data model;management systems;storage manager;remote sensing;data management system;array data
open source;data volume;data warehouses;large scale;parallel dbms;data warehouse;data warehouse;large number of;business applications;large volumes of data;data sets;web logs;data stored in;sensor data;optimization opportunities;parallel computing;large scale data analysis;parallel dbms;16;programming paradigm;business decisions
join algorithm;data analysis;large volumes of data;click-stream;reference data;join algorithms;implementation details
collecting data;data analysis;mapreduce-based;main memory;data-management;statistical analysis;data-analysis;data repositories;data management systems;data management system
real life;social networking;algorithm named
high-dimensional;user visits;high-dimensional space;simple models;real-world;motivating application;data sets;dimensional data
open source;cost effective;data warehouse;design choices;large data sets;business intelligence;data warehousing;ad hoc
relational database;internet scale;sql server
information-retrieval;sql/xml;document content;keyword-based search;semi-structured data;database;text search;context-aware;data migration;text data;potential impact;fine-grained;ir) style;adaptive approach;keyword search;structured data;unstructured data
clustering;large amounts of;schema-based;xml data;native xml;database management system;memory management
optimizer;query optimizer;database;database schema;query optimization techniques;log-structured;operator;relational databases;data entry
information integration;open source;open-source;large-scale
data sharing;database management systems;computing environment;data acquisition;business applications;database management system;data management
domain experts;data sets;database;large databases;large database;visual representations;visual interface;massive amounts of data;view-based;private data;formal language;query performance;visual interfaces;user experiences;databases;relational databases;statistical models;structured data;visual analysis;data management;visual interfaces
streaming data;stream processing;replication;query processing;data streams;building block for;high availability;data volumes
end-users;end-user;large volumes of;static data;stream processing;instance;user queries;databases;location-based;network data;multi-faceted;user interaction;continuous queries
real-world;data sources;information extraction;instance;business intelligence;business operations;unstructured data
mapreduce-based;parallel databases;web data;massive datasets;data warehousing;application scenarios;protein sequence;real world applications;real world;fault tolerance;flexible architecture
online aggregation;distributed computing;continuous query;hadoop mapreduce;user-defined;stream processing;online aggregation;batch processing;continuous queries;data-intensive;fault tolerance;programming model
massive datasets;mapreduce based;duplicate detection;keyword search
high level;graph-based;large graph;execution plan;large graph;social network;visualization tool;data structures;social graphs;graphical user interface;increasingly popular
classification;database;ad-hoc;sql queries;classification rules;5;relational schema;rule extraction;databases;oracle database
pattern-based;data analysis;exploratory analysis;sequence data;1;query processing;olap operations;query processing techniques;online analytical processing;traditional olap;graphical interface
visualization tools;decision support;decision support queries;multi-criteria;multi-criteria;key features;queries involving
extensible framework;structural constraints;specific characteristics;xml collections
step forward;database;application domain;pattern queries;stock market;rfid-based;xml queries;data stream applications;financial services;xml data;user-friendly;sql queries;sequence patterns
control systems;field programmable gate arrays;data stream management;large number of;data stream processing;monitoring applications;data stream management system;high accuracy;data streams;image processing;monitoring systems
pattern mining;visual interfaces;7;data streams;visual exploration of;abstraction levels
emerging topics;trend detection;social media;users interact with
stream processing;field-programmable gate arrays;database systems;query execution plan
ranking mechanism;takes into account;question answering;keyword search on
search results;data cube;point sets;aggregated data;search engine;keyword search
structured databases;web queries;named entities;entity-oriented;web search results;relevant entities;pre-processing;web search engines;additional information
search engines;web sites;web forms
fine-grained;ad-hoc;decision makers;integrated environment;data model;instance;data sources;data integration
greedy heuristics;open source;computationally hard;database;real-world;database administrator;scientific datasets;query workloads;tuning tools;optimal performance;combinatorial optimization problem;tuning tools;optimal set of
public data;financial data;multiple data sources;risk analysis;semi-structured documents;structured information;primary goal
object model;database;object-relational;entity framework;schema evolution;persistent database;application development
attribute values;database;meta-data;probabilistic database;management systems;relational database;additional information
search space;user preferences;navigation systems;skyline computation;shortest path;skyline queries;result set;personal preferences
mining results;real data sets;interactive exploration;pattern mining;moving object databases;web technologies;mobile devices;mobile communication;data collected from;broad applications;moving objects;moving object;data mining;increasing amounts of;data mining tools;user-friendly
complex queries;large-scale;query language;query language;data model;primary key;relational databases
document content;ad-hoc;query language;data model;relational model;search tasks
database systems;proximity-based;cardinality constraints;cardinality estimation;automatically generate;query refinement
visualization tool;web site;wide range
data quality;active learning;domain specific;machine learning;user feedback;decision theory
user-generated;scalability issues;index structures;main memory;large number of;data freshness;rates;table scans;specifically designed to;relational table
end-user;database systems;database;sensitivity-analysis;adaptive sampling;parameter settings;minimal overhead;configuration parameters
private data;database server;personal data;storage capacity;database servers
key management;privacy-preserving;access control
similarity-aware;cost-based query optimization;database management system;application scenarios;database
database operations;database;query processing;query processor;queries involving;database engine;code base
information network analysis;data quality;information networks;database;data cleaning;real datasets;multi-typed;data integration;case studies;inter-related;databases;data repository;heterogeneous information networks;data storage
database systems;large data sets;large scale;data mining;machine learning models;data structures;data mining;database systems
information content;information theory;database design;information-theoretic;data anonymization;information theory;data integration;analyzing data;data management
design requirements;database;unstructured text;prior knowledge;information extraction;3;structured information;information extraction
visual analytics;knowledge discovery;data mining;visual analytics;knowledge discovery
visual analytics;visual analytics;visual analysis
data analysis;visual analytics;future directions for;data mining techniques;knowledge discovery;information visualization;data mining
clustering;distance functions;exploratory analysis;spatiotemporal data;spatial distributions;analysis tasks;clustering algorithm;interactive visual;cluster analysis
13;exploratory analysis;17;similarity measure;visual representations;categorical variables;interesting patterns;data set;data sets;visual analysis of;numerical data
mining results;visual analytic;data analysis;plays an essential role in;frequent patterns;raw data;knowledge discovery;real-life applications;visual interfaces;visual representation;data mining;frequently occurring;frequent pattern mining
multiple views;on-line analytical processing;survey data;application domains;typically involves;hierarchy levels;visual analysis of;data cubes;data cubes;visualization methods
model complexity;computational complexity;real-world;learning algorithms;application domains;statistical relational learning;statistical models;relational data;statistical relational learning
program committee;review process;artificial intelligence;data mining
predictive models;customer relationship management;training examples;marketing strategies;data mining techniques;large number of;large numbers of;past experience;database;categorical variables;classification problem;noisy data;decision trees;web site;kdd conference;ensemble methods;missing values;large database;positive class
workshop report;data mining;acm sigkdd;kdd09-workshop;data mining
human computation;human computation
decision support systems;knowledge discovery from sensor data;applications including;homeland security;knowledge discovery from sensor data;data fusion;raw data;international workshop on;massive volumes;acm sigkdd;sensor networks;knowledge discovery;common interests;data mining;high-priority;acm-sigkdd;distributed data;sensor data;knowledge discovery
application domain;data acquisition;acm sigkdd;statistical relational learning;machine learning and data mining;statistical relational learning
game theoretic;classification problem;mining framework;classification;extracted features;highly competitive;feature extraction technique;prior knowledge;incomplete information;filtering techniques;data mining;machine learning techniques;support vector machines;margin;classifier;classification algorithms
web-based;real-world;amazon's mechanical turk
domain knowledge;control systems;closed-loop;noise handling;real datasets;ground truth;detection methods
multi-class;classification tasks;large margin;logic based;protein fold;inductive logic programming;multi-class;classifier;learning method;multi-class classification problems
plays an essential role in;frequent patterns;uncertain data;knowledge discovery;real-life;data mining;association rules
database;query optimization;optimizer;plans;large-scale;application level;database management;data centers;manual tuning;evaluation model;low-power;database management system;aware query;data management;plan selection
pattern-based;supply chain;complex event processing;decision making;data sharing;multi-dimensional;pattern analysis;event sequence;management systems;olap operations;online analytical processing;optimization techniques;multi-dimensional data;data streams;tag-based;pattern matching;abstraction levels;pattern queries;rfid-based;query language;stream processing;efficient computation
analytical queries;computational complexity;data analysis;redundant information;efficient storage;large scale;data locality;large databases;high levels of;spatial locality;query processing;scientific databases
association rules;minimum support;transactional databases;fp-growth
probabilistic logic;instances;domain independent;graph-based
incremental update;radio frequency identification;database;efficient storage;rfid data;7;lessons learned;business processes;moving objects;ad-hoc querying;business intelligence applications;databases;accurate tracking;real world
data integrity;web services;ad-hoc;business processes;web services;transaction management;web services
clustering;incremental algorithms;resource consumption;classification;database;outlier detection;modeling framework;handle arbitrary;similar characteristics;query processing;statistical modeling;databases;numerical data;model fitting;model estimation;data management;model parameters
data values;attribute values;test data;semi-structured;information extraction;information extraction;probabilistic approach;learning strategies;high degree of;information extraction;text segmentation
similarity query;set similarity;database;data cleaning;index structures;query answers;semantic properties;similarity queries;answering queries;real datasets;approximate matches
hybrid model;data warehouse;highly complex;data sources;decision makers;data warehouses;development process
relational databases;query processing;schema evolution
data analysis;data source;olap queries;data sources;aggregate functions;data sets;distributed processing;databases;massive data sets;query engine

real-world;efficient storage;data management;database
complex queries;indexing structures;spatio-temporal;temporal dimension;indexing scheme;general case
database;attack model;database;key management;encrypted data;data security
spatio-temporal;database research;location privacy;nearest neighbor queries;data management;data processing;spatio-temporal;distributed systems;continuous queries
1;database

relational data sources;xquery engines;20;xquery expressions;xml data;xquery engine;data stored in;xml data sources;relational data
database operations;database systems;access control;mobile computing;computing devices;storage systems;transaction management;concurrency control;semistructured data;standard sql;database management systems
rdf data;evaluation strategies;query processors;resource description framework;database
social networking
years ago;data stored;energy management;intensive workloads;replication;data replication;data-intensive;load balancing
discovered patterns;clustering;completeness;classification;high-quality;pattern mining;pattern mining;interesting patterns;original data;acm sigkdd;small sets of;data mining;local structure;mining algorithm;focus primarily on;huge number of
pattern analysis;pattern mining;pattern mining;broad applications;data mining;mining algorithm
interesting itemsets;frequent pattern;interesting rules;data mining
frequent itemset;computational methods;mixture models;significant patterns;complex systems;databases;high-resolution;biological data;information loss
frequent pattern mining algorithms;frequently occurring;frequent patterns;frequency information;data mining tasks;raw data;mined results;knowledge discovery;closed patterns;real-life applications;visual representation;data mining;visual exploration of;huge number of;frequent pattern mining
pattern sets;prior information;databases;pattern set;2, 3;interestingness measures
classification;mining algorithm;gene expression data;data points;item set;classification algorithms;pattern mining;mining process;time series;item sets;vector space;finding patterns;continuous attributes;algorithms for computing;kullback-leibler divergence;vector data;abstraction level;feature selection
closed patterns;closed frequent;real life;margin;databases;mining sequential patterns;margin;sequential pattern mining
set cover;frequent itemset;frequent patterns;generative model for;frequent itemsets;set-cover;frequent pattern mining;frequent pattern mining
syntactic structures;high computational complexity;feature set;classification;tree mining;classification accuracy;kernel based;syntactic features;subtree patterns;computational burden;real datasets
pattern language;structural properties;selection method;structural characteristics;pattern selection;target variable;sensor data;data mining;time-series;pattern selection;patterns discovered;high-resolution
data collection;classification;data mining techniques;decision tree;confidential information;data sets;aggregated data;data mining technique;personal data;real-life data sets;classification models
skyline query;data points;large number of;index structure;skyline points;skyline queries;multidimensional space;skyline query processing
high-speed;window queries;streaming applications;window size;data model;sliding-window;sensor networks;query processing;unified framework;real data;uncertain data streams;sliding-window;data generated from
data objects;search space;data accesses;nearest neighbor;computation costs;knn queries;region-based;safe regions;region based;cost models;spatial networks;query answer;spatial-network;query point;nearest neighbors
individual queries;ranked list;data distributions;scoring functions;wide range;query answer;query processor;threshold algorithm;disk-resident
service provider;spatial data;data points;real-estate;trade-offs;maintenance costs;query efficiency;instance;data confidentiality;search services;large database;data owner
inductive logic programming;biological networks;graph mining;semi-structured data;social network analysis;emerging applications;statistical relational learning;social networks;analyzing data;graph structured data;structured data;communication networks
probabilistic modeling;feature space;unsupervised learning;text streams;pattern mining;topic modeling;sensor data;clustering methods;clustering;frequent itemset;classification;anomaly detection;data stream mining;evolutionary clustering;health monitoring;document streams;clustering algorithms;data stream;incremental clustering;mining algorithms;international workshop on;stream data
clustering;social media;ir researchers;information retrieval research;classification;recommender systems;emerging topics;information retrieval;technical program;query logs;information retrieval
search results;user interactions;database;search problem;relevance feedback;constraints imposed by;result set;query refinement
clustering;clustering results;completeness;optimization problem;web collections;optimization criteria
clustering;clustering results;named entities;document similarity;low recall;web search results;clustering algorithm;key words;high precision
task type;document usefulness;user behavior;information retrieval;search behaviors;information seeking;retrieved documents;contextual factors
features extracted from;average precision;query logs;instance;search engines;search strategies;search engine;information seeking;user study;search engine users;query log
search results;web search;web search engine;visual attention;search engines;task type;result pages
desktop search;web pages;prediction performance;human computation;document types;prediction method;search effectiveness;learning models;distributed ir
automatically extracted;similarity scores;class attribute;instance;derived data;class labels;weakly-supervised;instances;random walks
information retrieval tasks;highly relevant;machine learned;features extracted from;community based;candidate matches;ranking function;information retrieval;match-making;user profiles;match-making;question answering;structured queries
search methods;clustering;network size;large scale;retrieval functions;high scalability;retrieval performance;network clustering;dynamic systems;network structure;relevant information;information space;distributed systems;information networks
search results;web search engine;poor performance;search engines;search engine results;search engine;query log
linear-programming;geographically distributed;query log;document collection;replication;real-life;search engines;result quality;search efficiency
classification model;classification;information sources;federated search;user query;resource selection;high probability;resource selection;information source
search results;sponsored search;user actions;click model;real datasets;users' behavior;click logs;search engine;higher quality;bayesian framework;click prediction
web corpus;ranking scheme;web page;years ago;web graph;link structures;search results;web pages;link-based;real-world;algorithms typically
search results;search task;commercial search engines;web search;retrieval effectiveness;anchor text;text search;link information;trec 2009 web track;web collections;collection size;link graph;text retrieval;ad hoc retrieval;link evidence;ad hoc;ad hoc search;web retrieval
interaction data;search results;click logs;web search;user interactions;data collected from;user behavior;real users;user's search;fine-grained;search session;search advertising;improving search;user studies;search intent;search behavior;result quality;search evaluation
directly optimize the;naturally leads to;models learned;learned models;unified framework;query execution times;search engines;learning models;ranking functions;load balancing;loss functions;retrieval effectiveness
ranking function;ranking method
search results;web documents;data collection;ranking methods;retrieval accuracy;real-world;search engine;query logs;ranking models;machine learning;fundamental problem;information retrieval;web retrieval
domain experts;training data;labeled data is;target domain;information retrieval;instance;ranking algorithms;27;weighting scheme;document pairs;model trained;retrieved documents;semi-supervised
search results;clustering;unique features;classification;clustering problem;optimization algorithm;relevance judgments;optimization problem;objective function;cluster labels;test collections;evaluation techniques;upper bound;document-level;information retrieval;meta clustering;clustering algorithms
label quality;structural relationships;structural properties;jensen-shannon divergence;chi square;hierarchical structures;hierarchical structure;document clustering;term frequency;labeling process;cluster labeling;high-level;topic hierarchies;information gain
high-dimensional;vector space;tf-idf;large number of;real data;information retrieval;search result;vector spaces;term frequencies;dimensionality reduction;intrinsic) dimensionality;takes into account;semantic structure;cosine similarity;vector space model;similarity measures
social media;tag-based;user study;social media;item recommendation
high-dimensional;user profile;vector space;information filtering;user's interests;vector representation;vector-based;user profiles;information filtering;vector space model;additional information
web-based;user ratings;recommender systems;collaborative filtering;evaluation techniques;temporal characteristics of
user/item;high degree of;target user
retrieval effectiveness;ir models;expected values;ranking problem;retrieval model;statistical analysis;joint probability;ir metrics;objective function;optimization framework
mixture models;ad hoc;ad hoc information retrieval
precision-recall;score distribution;relevant documents;information filtering;recall-oriented;score distributions;gaussian distribution;basic assumption;term frequencies;score distribution;meta-search;ranking functions;retrieved documents;mathematical framework;language models;distributed ir
theoretical foundation;multiple documents;information retrieval tasks;document clusters;feedback documents;pseudo-relevance feedback
document ranking;query-performance;relevance judgments;relevance models;query-performance prediction;relevance model;performance predictors;prediction task;decision theory
training set;active learning framework;web search;web search engine;training data is;active learning;information retrieval applications;real-world;active learning;great potential;document selection;data sets;document level;wide range;online advertising;learning approaches;ranking functions;labeled examples;ranking model
instance;ranking process;image search;database;image search;semantic concepts;concept map;instances;search interface
information-theoretic;query reformulation;computational costs;combination methods;generative model;desirable properties;semantic information;similarity measures
keyword queries;collaborative question answering;web search;search engine's;black box;verbose queries;search engines;long queries;search engine;query processing techniques
unlabeled data;label information;active learning;active learning methods;labeled data;text classification;learning method
weighting function;classification model;document classification;document classification;naïve bayes;information retrieval;document collections;learning strategy
logistic regression;model complexity;multilabel classification;fine-grained;instance;feature space;level features;controlled experiments;multi-label classification;discriminative patterns;multi-label classification;instances;multiple categories;link-based;retrieval algorithms;high-dimensional space;benchmark datasets;instance-based
document collection;statistical translation;mutual information;translation probabilities;language models;retrieval accuracy;translation model;exact matching;regularization;information retrieval;pseudo-relevance feedback;training data;computationally expensive;mutual information between;semantic relations;ad hoc information retrieval;query words;estimation method
search results;real-world datasets;keyword queries;structured databases;taking into account;relevance ranking;database;probabilistic model;keyword search over;keyword query;query results;structured data
ad-hoc;entity ranking;entity-ranking;named entity
search results;result sets;document collection;training data;high-quality;relevant documents;data fusion;ir evaluation;related algorithms;competitive performance;data fusion;average precision
click logs;document relevance;large scale;click models;click model;search result;bayesian approach;log data;highly scalable;user behaviors
semi-structured;retrieval performance;recommendation approaches;amazon mechanical turk;text documents;user feedback;relevance feedback;pseudo relevance feedback;retrieval models;low recall;document retrieval;faceted search;select relevant;search interface
individual users;topical relevance;search experience;expression data;implicit feedback;support vector machines;information retrieval systems;models trained on
user browsing;web pages;analysis reveals;browsing behavior;interesting patterns;log data;information retrieval tasks;low-level;web browsing;reliability analysis;web page;real world;giving rise to;web browsing
community-based;average precision;syntactic features;retrieval models;retrieval performance;graph based;retrieval framework;user studies;finding similar;question answering
query language model;language model;query likelihood;search engines;query logs;related queries
query term;probabilistic model;query topic;opinion retrieval;proximity-based;proximity-based;blog posts;pseudo-relevance feedback;trec 2008 blog track;query-specific
multiple users;community-based;amazon mechanical turk;answer quality;contextual information;retrieve information;question answering
duplicate detection;similarity measure;similarity learning;detection method;real-valued;similarity function;similarity computation;web news;locality sensitive hashing;cosine similarity
web pages;web search;finding task;anchor text;web graph;retrieval performance;language modeling
web-based;false positives;false positive;social systems;community-based;machine learning;strongly correlated;rates;statistical analysis;social networking;high precision
involving multiple;web pages;web search;starting points;search engines;query type;information seeking;search tasks
context information;data set;web search;ranking model;search log;context-aware;click data;human judgments;small scale;context models;search engine;web search;aware search;context-aware
labeling scheme;web search;low cost;real-world;high quality;human judgments;training samples;retrieval accuracy;test sets;training sample
web scale;mixture components;ranking tasks;mixture model;anchor text;large scale;document retrieval;text streams;language model;web scale;information retrieval;search queries;labeled data;language models;language modeling approach;web document;web documents;training algorithm
training set;semi-supervised learning;text categorization;labeled documents;unlabeled examples;unlabeled documents;training algorithm
clustering;clustering algorithms;text classification tasks;text documents;multiple clusterings;text clustering;user feedback;accuracies;clustering algorithm;common practice
user study;regression method;sentence-level;multi-document summarization
user experience;helping users;web pages;image based;web page
scientific articles;retrieval functions;test statistic;retrieval evaluation;implicit feedback;retrieval function;search engine;test statistics;confidence level
similarity measure;query recommendation;semantic similarity between;euclidean space;query logs;query expansion;low-dimensional
applications including;web search;web search engine;demographic information;improving web search;query suggestions;search engine users;instance;data sources;query log;wide range;large sample;modeling tool;search behavior
search engine;user interactions;relevant documents;average precision;behavior model;user behavior;user models;query logs;average precision
amazon's mechanical turk;search topics;quality control;relevance judgments;test collection
reusability;reusability;relevance judgments;simulation experiments;test collections;information retrieval;test collection
user preferences;effectiveness measures;retrieval systems;user preference;test collections;wide range;search engine;evaluation measures;web retrieval;search tasks;test collection
world wide web;sponsored search;estimation method;user experience;real-world;user clicks;search engines;sponsored search;search engine;long-term;search engine queries;power-law;short-term
long queries;ad-hoc retrieval;search engine's;web queries;reduction techniques;great potential;performance predictors;queries issued;ranking algorithms;long queries;search engine;constraints imposed by;web search engines;query terms;trec collections
query words;query topic;passage-based;retrieval results;pseudo-relevance feedback;relevance model;pseudo-relevance feedback;query expansion;irrelevant information;feedback documents
search systems;recommendation systems;query types;user effort;search result;web logs;search result pages
retrieval precision;user interfaces;human performance;human performance;retrieval systems;performance gains;search tasks
precision-recall;retrieval effectiveness;discriminative power;average precision;objective functions;relevance judgments;ranking functions;desirable properties;retrieval systems;average precision
taking into account;retrieval effectiveness;patent retrieval;relevant documents;information retrieval applications;evaluation metric;ir) evaluation;information retrieval;growing importance;ir systems;search effort;retrieval task;recall-oriented;evaluation measures
training data;text based;classification;classification framework;classification methods;content features;features extracted from;text features;main idea;classifier
music information retrieval;retrieval precision;compressed-domain;frequency distribution;database;intermediate result;software systems;data set;compressed domain
music information retrieval;acoustic features;large scale datasets;temporal information;vector machine;gaussian mixture models;machine learning;classification framework;comprehensive evaluation;information integration;low level;statistical modeling;discriminative information;great promise;classification approach;music collections
search topics;data fusion;optimization method;multimedia information retrieval;large number of;collection size;weighting scheme;normal distribution;optimal performance;optimal set of
statistical properties;classification models;query term;query translation;cross-language information retrieval;query terms
related terms;semantically related;obtained by combining;initial retrieval;pseudo-relevance feedback;expansion terms;query terms
retrieval effectiveness;user behavior;information retrieval;discounted cumulative gain;user clicks;average precision;information retrieval systems
web applications;duplicate detection;document level;sequence matching;web collections;sentence level;main idea;search engine;opinion mining;detection task
modeling assumptions;discriminative models;discriminative models;language models;trec enterprise;expert search;generative models;generative models;machine learning applications;information retrieval;learning framework;language modeling
machine learned;training data;learned models;predictive model for;search engines;vertical selection;web search results;accurate predictions;vertical selection;labeled data
web-based;web pages;web usage
data resources;user activity;automatically extract
long-term;information filtering;query suggestions;information access
retrieval results;document set
search results;web search;related topics;query logs;search engines;search engine
data-aggregation;user studies;data files
medical records;free text;medical data;medical research;semi-structured data;classification;text analytics;text documents;decision-making;text-analytics;text data;management systems;enormous amounts of;traditional database;clinical data;rich information
search queries;search services;social networking;enterprise search;book search
information network;special case;similarity matrix;closed form solution;fast algorithms;random walks
selection process;user generated;user access;information retrieval
domain experts
document collection;similarity metric;retrieval tasks;search queries;retrieval performance;distance based;expansion terms;query expansion
social network analysis;knowledge based
search queries;online advertising;user behaviors;search engine
multi-modal;multi-modal;search techniques;query expansion;video search;query suggestion;search engines;visual presentation;query expansion;query terms;clustering
context information;hybrid approach;conditional random field;classification;user queries;ambiguous queries;aware query;classification approach
search results;classification model;regression model;classification
search results;search topic;multiple documents;retrieval results;collection size;multiple aspects
language model;spatial relationships;graph-based;language model;vector machine;image content;spatial relationships;classification method;graph model;image categorization
search results;web queries;user evaluation;database
natural language queries;average precision;user queries;query translation;content words;user query;user queries;stopwords;search engines;natural language processing;query expansion;cross-language information retrieval
sentence level;cluster-based;detection process;novelty detection;novelty detection;query-dependent;cluster analysis
semantic tags;group recommendation;tensor decomposition;social relations
compressed domain;image processing;compressed-domain
trec test collections;probability ranking principle;retrieval performance;document relevance;subtopic retrieval
user ratings;pre-retrieval;prediction methods;query quality;query performance;query quality;average precision;effectiveness measures
text classification;document structure;linear combination;spam filtering
language-model;related terms;classification
cluster-based;boundary detection;query logs
spam filtering;error rates;motivating application;compare favorably with;global consistency;unlabeled examples;labeled examples;graph based;semi-supervised;semi-supervised
multiple views;image classification;image-based;classification problems;classification
portfolio theory;information retrieval;portfolio theory;parameter tuning;quantum theory;probability ranking principle
classification performance;low accuracy;classifier;spam filtering
performance metric;collection statistics;training data;relevance judgments
audio-visual;relevance judgments;retrieval evaluation;implicit feedback
web search results;user experience;probabilistic model;user's location;web search results;search engine;physical location
context information;short queries;entity recognition;topic model;information retrieval;entity recognition;context aware;search session;conditional random field
search engines;result page;document relevance;search engine results;relevance assessment
ranking algorithms;web page;search sessions;search interfaces
contextual information;automatically identifying
machine translation;translation model;cross-language;language models;cross-language retrieval;retrieval model;link-based;training corpus;latent dirichlet allocation
relevant information;information retrieval systems;search tasks;retrieval tasks
relevance ranking;monte carlo sampling;opinion retrieval;evaluation measures;classifier;topic relevance
retrieval effectiveness;language model;retrieval model;sentence retrieval;information retrieval
document retrieval;focused retrieval;relevant content;xml retrieval;relevant documents
clustering;domain knowledge;optimization problem;text document;semi-supervised clustering;document clustering;metric learning
ranking algorithm;performance prediction;prediction accuracy;web queries;web environment;search engine;retrieval scores;query reformulation;query performance;performance prediction;query logs;query performance prediction
relevance feedback;language modeling approach
wikipedia articles;amazon's mechanical turk
multimedia documents;search engine
statistical properties;dirichlet prior;pitman-yor;pitman-yor;language model;smoothing techniques;information retrieval;retrieval performance;power-law distribution;dirichlet distribution;word frequency;natural language
temporal information;relevant entities;entity retrieval
query segmentation;keyword query;search engine
click logs;conditional random field;search engine's;sponsored search;document expansion;automatically extract
web-based;text-based
web search engine;log data;test set;entity retrieval;entity retrieval
feature subset;data points;document understanding;matrix factorization;document clustering;feature subset
trec collections;ranked list;web queries;query reformulation;query reformulation;reformulated queries;query logs;retrieval effectiveness
summarization method;multi-document summarization;combination methods;data sets;multi-document;summarization methods
human-computation;anchor text;information-seeking;target pages;query logs;web page
relevant document;relevant documents;term selection;retrieval performance;pseudo-relevance feedback;test collections;pseudo-relevance feedback;expansion terms
ir) systems;information retrieval
web corpus;opinion retrieval;large-scaled;opinion retrieval;high precision;topic relevance
query node;document summarization;multi-document summarization;multi-document summarization
multi-view clustering;clustering technique;multi-view;clustering method;clustering results;multilingual documents
edit distance;stack based;string matching;sequential patterns
combination methods;image features;score distributions;image databases;compact representation;heterogeneous databases;databases
ranking techniques;web search;web search engine;search process;textual content;user intent;social annotations
ranking approaches;query term;verbose queries;query term;query terms;features extracted from
ranking problem;classification approaches;binary classification;target detection
result sets;recommender systems;ad-hoc;information retrieval;similarity metrics;term frequency;probabilistic inference;result set;latent semantic indexing;graphical model
search engines;user study;health information;information retrieval
contextual information;cross lingual information retrieval;select relevant
test collection;anchor text;web pages;valuable information;language modeling approach
classification;features extracted from;information filtering;classification methods;short text;raw data;short texts;domain-specific;text messages
contextual information;xml element retrieval;xml element;xml retrieval
search systems;user behaviors
general purpose;large-scale;query log analysis;information retrieval;query log;web search engines
probabilistic model;search history;click data;query reformulation
similar users;gaussian kernel
search results;test collections
retrieval performance;retrieval models;ranking algorithms;retrieval scores;distributed search
visualization techniques;automatically generate;document summarization;multi-document;summarization methods
page rank;web pages;web page;baseline methods
clustering;clustering algorithm;hierarchical clustering
ground truth;automatic evaluation;information retrieval;instance;highly correlated;ir systems;average precision
retrieved documents;query aspects
search engines;web search;search interfaces;search tasks
search results;ranked list;link structure;graph based;query-likelihood;result set
search results;web corpus;commercial search engines;web search;information retrieval methods;ranking performance;real-world;page content
large volume;case study;online reviews;sentiment analysis;strongly correlated;performance prediction;model called
wikipedia articles;web corpus;supervised machine learning;diversity measures
salient features;goal-directed;blog posts;search behavior;information seeking
machine learned;decision trees;test set;ctr;machine learned;features extracted from;search experience;user click;linear combination;retrieval performance;learned model;search engine
service provider;social media;recommendation service;user behavior;relevant information;semantic information;online news
social media;local features;global information;entity types;recognition systems;entity recognition;named entity
query terms;ranking models;trec test collections;document term;high accuracy
generative model;probability distribution over;topic model;topic model
ir models;ir) systems;ir evaluation;information retrieval
taking into account;concept-based;concept detection;prediction method;visual concepts;automatically detected;video retrieval;spoken content;query expansion
multi-modal;dirichlet process mixture;video frames;clustering;large-scale
training data;web search;multiple sets of;learning procedure;learning framework;high quality;common sense;online advertising;training datasets;unlabeled data;search intent;learning framework;classifier
semantic features;semantically related;rates;ctr;search engine
graphical models;efficient representation;xml data;vector-space;vector-space model;text data;knowledge discovery;text representation
survival analysis;biomedical information retrieval;modeling approach;search result diversification
reusability;web pages;low cost;low-cost;relevance judgments;information retrieval;relevance feedback;test collections;ir research;wide range;additional cost;statistical inference;user feedback;evaluation measures;large collections;average precision;probability theory;low cost
generalization ability;information retrieval;learning theory;data preprocessing;ranking methods;query-dependent;semi-supervised;loss functions;evaluation measures;benchmark datasets
cross-language;special attention;smoothing techniques;probabilistic models;language modeling approach;information retrieval;multimedia retrieval;retrieval models;random variables;probabilistic model;language modeling framework;real world;basic concepts;conditional probabilities;probability ranking principle;language modeling;poisson model
relevance feedback;multimedia information retrieval;efficient retrieval;database;text search;search experience;multimedia information retrieval;news video;resource discovery;indexing problem;search engines;document types;mobile phone;image annotation;content-based search
data analysis;step forward;document-centric;retrieval methods;query logs;link analysis techniques;usage data;user interaction;web retrieval
text summarization;classification techniques;entity recognition;classification;information retrieval;relevance feedback;computational advertising;natural language generation;web search results;information retrieval;computational advertising;computational advertising;additional features;user context;prior knowledge;statistical modeling;human interaction;query expansion;large scale;named entities;machine learning;query-specific;text analysis
automatically extracted from;web search;knowledge bases;web-based;semi-structured;annotated data;web documents;search queries;supervision;information extraction;building blocks for;instances;data sources;extraction methods;wide range;web collections;faceted search;relevant attributes;information retrieval;building block
search systems;web search;search techniques;information retrieval;web content;click logs;result page;instance;aggregated search;real world;search results;web documents;federated search;video databases;aggregated search;major search engines;auxiliary;federated search;retrieval systems;ranked list;digital libraries;search tasks
search results;relevance feedback;difficult" queries;query performance;prediction methods;ir systems;users' queries;prediction quality;information retrieval;search engines;ir) systems;retrieval performance;query difficulty;query performance prediction
search results;web information retrieval;ir researchers;web information retrieval;large scale;massive amounts of;document understanding;huge amounts of;log mining;large-scale;search engines;search engine;log data;information retrieval systems
relevance feedback;enterprise search;relevance assessment;information retrieval;united states;supervised machine learning;test collections;text classification
cosine measure;high rate;cross-language;statistical machine translation;source documents;kullback-leibler;1;instance;3;2;probabilistic models;cross-language information retrieval;similarity measures
user interface;web search;search strategies
relevance feedback;social tags;information retrieval systems;cross-lingual information retrieval;cross-lingual;information access;digital libraries;user interaction;user input
heterogeneous information sources;large-scale;probabilistic models;related entities;text data;information retrieval;entity ranking;probabilistic framework;cross-language;discriminative models;expert search;information management;1;3;2;4;real world;multi-lingual;graphical models;social media;social network analysis;complex networks;document search;retrieval task;multiple types of;network analysis;web environment;entity search;entity retrieval
social media;product development;topical relevance;information contained in;community question answering;user generated content;1;social networks;blog posts;knowledge sources
news search;web search engine;user-generated content;web search engines;online news
text mining;tracking methods;ground-truth;data collections;automatic evaluation;user centered;real-life;user oriented;users interact with;tracking method;user interaction
feature space;document collection;classification;improving accuracy;instances;retrieval performance;computationally expensive;fast approximate;nearest neighbor;large number of;1;digital library;document-level;document image;clustering;text queries;image collections;test set;document images;retrieval results;stopwords;character segmentation;search engine;digital libraries;large collections
probabilistic models;hidden variable;model offers;retrieval task;hidden variables
clustering;search algorithms;temporal properties;ad-hoc search;smoothing methods;query topic;user query;resource selection;operator;search methods;aggregation methods;3;1;content similarity;2;4;aggregation operator;related documents;distributed information retrieval;federated search;takes into account;growing rapidly;user generated;ad-hoc;ad hoc retrieval;text analysis
classification;social network;topic modeling;data mining;clustering;knowledge discovery;acm sigkdd international conference on;kdd community;technical program;review process;large datasets;program committee;graph mining;high quality;knowledge discovery;data mining;temporal data;transfer learning;emerging applications;frequent sets;knowledge discovery;program committee;acm sigkdd international conference on
large scale data mining;data analysis;large scale;data mining;search advertising;online services;data mining;higher quality;online services;decision making
massive data streams;digital data;medical images
large scale data mining;statistical methods;data-mining;marketing strategies;machine learning;search advertising;digital media;effective classification;transactional data
raw data;causal models;data analysts;selection bias;robust estimation;observational data
web applications;user interface;search engines;machine learning algorithms
ctr;ranking systems;reinforcement learning;simulation framework;short term;log data;automatically learning;online advertising
web-based;high throughput;large-scale;stream mining;resource management;user-interface;data streams;wireless networks;distributed data;data mining system
detection problem;control systems;multiple kernel learning;data bases;dynamical systems;application domain;anomaly detection;case study;high dimensional data streams;continuous data streams;rapid rate;real-world data sets;multiple kernel learning;continuous data
large volumes of data;emerging trends;case studies;blog posts
active learning;data mining;design choices;feature selection;machine learning techniques;hit rate
question arises;reinforcement learning;markov decision process;credit card;optimization techniques;data analytics
combining multiple;hidden markov models;stock data;behavior analysis;high probability;identify patterns
clustering;clustering algorithms;ensemble framework;feature representations;hierarchical clustering;domain knowledge;hierarchical clustering algorithm;cluster ensemble;case studies;malware detection
expert knowledge;high dimensional;feature vectors;software systems;databases
graph mining;multiple layers;aggregate information;storage space;real-world;real-world applications;access path
recommendation engine;web based;information sharing;input data;data mining techniques;clustering techniques;information extraction;data mining technologies;user-friendly;information network;information exchange
similarity query;longest common subsequence;parameter learning;similarity/metric;distance metric learning;data sequences;event sequence;spatio-temporal;arbitrary length;user-defined;instance-level;search problem;similarity measure;instance;similarity search;metric learning;similarity search;dimensionality reduction;sensor data
data mining models;data intensive;data mining algorithms;specifically designed to;data mining
visual analytic;text analytics;large collections of;real-world applications;visualization techniques;interactive visualization;user feedback
multi-level;data collection;storage capacity;graph analysis;computationally expensive;graph data;network traffic;real-world graphs;sensor data
domain knowledge;classification performance;false positives;instance labels;labeled features;active learning;real-world;linear programming;active learning;real-world applications;predictive model;labeled data;learning strategy;decision theory;increasingly popular
feature selection algorithm;auc;medical datasets;regression algorithm;machine learning;united states;missing data;feature selection;long-term;support vector machines;margin-based;risk factors
medical records;automatically learns;free text;classification problem;classification;information contained in;shared information;prior knowledge;united states;multi-label;large-margin;classifier
search performance;finding task;graph model;learning algorithm;expert finding;information network;objective function
prediction accuracy;prior information;map-reduce;predictive accuracy;applications including;high dimensional;large scale;massive data;log-linear models;categorical data;computational advertising;log-linear;rates;data mining;modeling method
search results;user browsing;ctr;user browsing;user behavior;instance;search engine results
seed set;social graph;social graphs
auc;classification;prediction methods;link prediction;variance reduction;link prediction;supervised learning;prediction task
high utility;data structure;itemset mining;database;pattern growth;pattern tree;large number of;candidate itemsets;space requirement;transactional database
concise representations;itemset mining;concise representation;closed itemsets;frequent itemsets;sparse datasets;interpretability;readability
synthetic datasets;frequent patterns;data uncertainty;probabilistic database;probabilistic guarantees;probabilistic databases;probabilistic data;mining uncertain data;location-based services;association rules;biological databases;monitoring systems
synthetic data;frequent items;sliding window;real-life;data stream;controlled experiments;sliding windows
itemset mining;high-quality;interesting itemsets;real-world;models built;efficient discovery of;posterior distribution;desirable properties;occam's razor;statistical models;model selection
greedy strategy;real data sets;parameter learning;low efficiency;gradient descent;high dimensional;estimation problem;structure learning;algorithm called;markov random fields;feature selection;complex data;maximum likelihood estimation;structure learning of;automatically discover;feature selection
regularization;data dimensionality;sample size;spectral learning;linear discriminant analysis;generalized eigenvalue problem;dimensionality reduction techniques;equivalence relationship;original formulation;canonical correlation analysis;data mining;real-world data sets;dimensional data;applications involving;dimensionality reduction
high computational complexity;building block
clustering;relevant information;data analysis tasks;cluster structure;feature selection techniques;classification;unsupervised feature selection;feature selection methods;unsupervised feature selection;optimization problem;subset selection;manifold learning;computationally expensive;feature subset;class labels;feature selection problem;dimensional data;real-life data sets;combinatorial optimization problem;unsupervised learning;feature selection
feature space;support vector regression;feature selection method;real-world;data set;feature selection;feature selection methods;support vector regression;density functions;exact computation
real-world datasets;data publishing;real-world;sensitive-attribute;normal form;privacy requirements
network analysis;high quality;user interests;regression models;hierarchical structure;user profiles;user activity;privacy-preserving;multiple sources;social networking
distance measure;theoretical framework;classification;large-scale;time series;distance measures;nearest-neighbor queries;privacy-preserving
cold start;cold start;data collection;graph-based;social network;link prediction problem;link prediction;graph-theoretic
minimal cost;misclassification costs;cost-sensitive learning;expected cost;vector machine;cost-sensitive;real-world applications;distribution information
open-source;database;data flows;categorical data;data processing;data flow;data generation
classification models;training examples;active learning;low-cost;sampling techniques;data acquisition;building classifiers;classifier;positive examples
patterns mined;gene expression data;np-hard problem;scientific applications;order-preserving;significant patterns;data matrix;rank based;gene expression;model called
detection algorithms;database;mesh terms;topic hierarchy;rates;fundamental problem
complex systems;network models;biological networks;kdd community;time series;systems biology;rank-order;feature selection;systems biology;data mining problem
gene expression data;database;correlation coefficient;real-life;high efficiency;original data;bipartite graph;transactional database
storage overhead;association rule mining;generalized association rules;internal node;frequent itemset mining;frequent itemsets;taxonomy tree;association rules;protect privacy;internal nodes
semi-honest;high level;sensitive information;private information;large number of;privacy-preserving data mining;data mining;general problem;personal information;increasingly popular
accurate classifier;privacy guarantees;data mining results;data mining;tree induction;naive implementation;privacy preserving;data access;data mining;data mining algorithms
privacy guarantees;false positive;frequent patterns;sensitive information;user behavior;differentially private;significant patterns;data set;algorithms require;data sets;external information;rates;sensitive data;data mining;frequent pattern mining;pattern mining algorithms
nearest-neighbor search;prediction accuracy;query node;algorithms typically;graph mining;real-world graphs;approximation error;disk page;nearest neighbors;local neighborhood;high degree;running times;small size;data files;link prediction;clustering algorithm;cluster boundaries;random walks;disk-resident;high-degree;similarity measures
computational advertising;general problem;real-world graphs;convex programming;min-cost
data structure;compression rate;real data sets;large social networks;social network;directed graphs;social networks
real data sets;memory bandwidth;markov chains;aggregation techniques;graphics processing units;graph-based;large graphs;graph theoretical;markov chain;dynamic graphs
mathematical model
clustering;spectral clustering;hierarchical agglomerative clustering;image segmentation;constrained clustering;benchmark data sets;clustering techniques;graph laplacian;real-world data sets;lower-bound;objective function
clustering;clustering algorithms;information sharing;highly competitive;information theory;mutual information between;information theoretic;clustering quality
clustering;clustering;similar objects;social behavior;data set;fully automatic;desirable properties;intrinsic structure;real world;minimum description length
clustering;real-world datasets;optimization framework;data mining;unlike prior;objective function;synthetic data
clustering;minimum spanning tree;large-scale;adaptive algorithm;sloan digital sky;data sets;minimum spanning tree;wide range;efficiently computing
temporal dependencies;pair-wise;state transitions;mining algorithms;software systems;simulation data;pruning algorithm;automatically discover
user feedback;helping users;large datasets;user studies;theoretical guarantees;web users
mining results;probabilistic semantics;frequent subgraph mining;measure called;approximation quality;uncertain graphs;frequent subgraphs;high quality;uncertain graph data;graph data;frequent subgraph mining;graph databases
optimization algorithms;loss function;classification models;semi-structured data;classification;pattern based;high quality;linear combination;boosting algorithm;graph classification;structure information;base learners;weak" classifiers;basis functions;classification algorithm
manifold structure;probabilistic latent semantic analysis;fall short;data analysis;latent dirichlet allocation;semi-supervised setting;linear dimensionality reduction;text documents;classification performance;manifold learning;topic model;topic models;low-rank;local consistency;text corpora;hidden structures;topic modeling;manifold structure of;topic modeling;model fitting;classification accuracy
computational efficiency;topic models;topic model;predictive performance;topic-specific;document collections;em algorithm
latent topics;topic distribution;latent variables;pitman-yor;model assumes;specific properties;topic models;observed data;knowledge discovery;topic model;dirichlet distribution;data mining;latent factors;real data;latent factor;multiple topics;power-law;latent dirichlet allocation
probabilistic generative model;social annotations;social tags;generalization performance;probabilistic model;generative models;social tagging;generation process;tag prediction
ensemble learning;source code;ensemble methods;collaborative filtering;recommender systems
web applications;web search;test data;meta-data;rank correlation;computationally intensive;online learning;historical data;squared error;online advertising;high dimensionality;model selection
test data;test results;objective functions;recommender systems;hit rate
user preferences;recommender systems;real datasets;random walk;user behavior;session-based;long-term;short-term
service provider;maximum likelihood estimation;globally optimal;information contained in;classification methods;generative models;human experts;generative model;network model;locally optimal
ensemble learning;machine learned;parameter learning;spam filtering;classification accuracy;computing environment;data mining and machine learning;ensemble methods;learning paradigm;multiple classifiers;individual classifiers;test instance;instances;data sets;training samples;ensemble methods;ensemble classifier;single classifier;classifier;query-intent
text mining;pattern-based;user preferences;reuters corpus;text documents;pattern based;large number of;low-level features;vector machine;level features;classification methods;term-based
document collection;mixture model;variable selection;dirichlet process;clustering approaches;dirichlet process;stochastic search;document clusters;document datasets;document clustering;feature selection;document clustering
semantic content;relation extraction;semantic relations between;classification;semantic features;structured data;extraction techniques;data set;tree kernels;taking into account;dependency trees;natural language
text mining;entity ranking;data analysis;text data;data set;regression model;wide range;problem called
large amounts of;search space;evaluation criterion;labeled training;semi-supervised feature selection;real-world;mining process;graph classification;semi-supervised feature selection;subgraph features;vector spaces;graph data;feature selection methods;classification performances;feature set
markov chain;monte carlo;generative model for;social networks;expectation-maximization algorithm;real-world data sets;collapsed gibbs sampling;predictive accuracy
synthetic data;inference problem;mixture model;outlier detection;networked data;closely related;joint distribution;randomly generated;data sets;user profiles;social networks;probabilistic model;markov random fields;blog data;web data;information networks
clustering;model complexity;classification;remote sensing;land cover;artificial data;instance;domain experts;constraint-based;class labels;real world;classification task;separability
document classification;memory capacity;data sets;memory size;training methods;linear classification;random access
binary classification;base classifiers;ensemble classifiers;lower bound;generalization error;base) classifiers;binary classifiers;error bounds for;classifier performance;false alarms;class-specific;applications involving;decision making;class-specific
probability distribution;expected cost;classifier
classification;decision tree;uncertain attributes;rule-based;categorical datasets;minimum confidence;discriminative patterns;probability distribution;data mining;classification problem;frequent patterns;minimum support;mined patterns;categorical data;associative classification;vector machine;svm classifier;classification;instances;classification accuracy;uncertain data;database;feature selection;information-gain;svm) classifier;classifier;classification algorithms
uci data sets;resource consumption;learned models;ensemble members;ensemble methods;ensemble pruning;pruning method;computational cost;ensemble pruning;memory consumption
proximity measures;query execution;random-walk;directed graph;random walk;retrieval tasks;particle filtering;retrieval models;query nodes;random walks;supervised learning;random walks
rating data;probabilistic model;generative models;data-set;collapsed gibbs sampling;formal model
location traces;data analysis;case study;large-scale;energy-efficient;search space;knowledge discovery;distributed nature of
data point;archived data;dimensional data;bayesian framework;mixture models;low-dimensional
clustering;unique features;density-based;accurately identify;clustering algorithms;biased samples
information diffusion;event tracking;social communities;user generated;social network;statistical model;statistical method;topic model;blog posts;network structure;network structures;online communities;text content
query nodes;detection problem;graph mining;real datasets;greedy algorithm;density based;search problem;heuristic algorithms;distance constraints;query-dependent;upper bound;algorithm to compute
instance;relational clustering;web sites
parameter estimation;prediction methods;probabilistic model;real-world;social tagging;bayesian approach;web resources;social tagging;tag prediction;accurate predictions;tag prediction;real world;web page;page content;increasingly popular
markov logic networks;internet users;web-scale;statistical model;data set;inference process
unseen data;large-scale;real-world;data mining tasks;skewed;ranking method;click prediction
anomaly detection;information retrieval;theoretical basis;task-specific;data mining
effectively exploit;learning process;bayesian network structure;feature set;label set;multi-label learning;label sets;data sets;multi-label learning;additional features;label sets;single-label;network structure;multi-label;classification problems;classifier
ranking approaches;ranking algorithm;information network;summarization task;text documents;random walk;data items;ranking methods;information networks;mining tasks;information networks
information diffusion;optimization problem;large datasets;approximation algorithm;optimal performance;np-hard
algorithm performs;viral marketing;online social networks;large-scale;real-world;social network;heuristic algorithms;scalable solution;heuristic algorithm;large-scale social networks;greedy algorithm;extensive simulations
information diffusion;taking into account;community-based;influential nodes;real-world;greedy algorithm;mobile devices;social network;computationally expensive;approximation guarantees;social networks;greedy algorithm;approximate algorithm;algorithm called;dynamic programming algorithm;np-hard
baseline methods;markov random field;user actions;case study;dynamic social network;predicting future;social network;interesting patterns;graph model;real-world data sets;social influence;tracking problem;network structure
dynamic-programming algorithm;propagation model;social networks;np-hard
statistical properties;spatial data;detection performance;outlier detection;normal distributions;detection methods;robust estimation;extensive simulations
dirichlet processes;text analytics;real-world;text data;text corpora;data sets;dirichlet processes;sampling scheme
clustering;data rates;window size;classification;worst-case;online algorithms;higher-level;time series;discovery algorithm;case studies;databases;rule discovery;pre-processing;data mining algorithms
involving multiple;real data sets;fourier transform;hierarchical clustering;moving objects;probabilistic model
13,18;correlation-based;mutual information;linear models;causal discovery;conditional independence;learning algorithms;bayesian network;bayesian networks;discovery algorithm;gaussian distribution;dependency analysis;5;continuous data;markov random field
low-dimensional space;high-dimensional space;training examples;classification;distance preserving;linear discriminant analysis;low-dimensional;random projections;upper bound;classifier
explicitly represented;hash function;large scale;data mining applications;kernel function;data sets;similarity search;vector data;kernel based;space complexity
proximity measures;metric learning methods;unlabeled data;input data;data mining applications;compared with conventional;supervision;machine learning;distance measures;labeled data;euclidean space;performance gains;distance metric;directly optimize the;semi-supervised;sparse metric learning;semi-supervised;automatically discover;metric learning
memory footprint;high-dimensional;multi-dimensional;lower dimensional;cost functions;large data sets
target class;labeled training;classification;maximum entropy model;text categorization;label information;class information;data sets;classification model;problem domain;learning problem;theoretical analysis;labeled examples;transfer learning;auxiliary
nonnegative matrix factorization;social media;auxiliary;retrieval tasks;retrieval systems;retrieval performance;shared subspace;subspace learning;tag-based;increasingly popular;learning framework;web retrieval
regularization term;multiple tasks;semidefinite programming;global convergence;low-rank;optimization problem;small-size;multi-task learning;rates;objective function;benchmark data sets
data sharing;multitask learning;regularization;decision trees;web search;web-search;learning tasks;joint model;data sets;multi-task learning;multi-task learning;learning task;task-specific;learning method
special case;target task;distance metric learning;learning task;transfer learning;relies heavily on;labeled data;multi-task learning;metric learning;covariance matrix;source tasks;convex formulation;data mining algorithms;convex optimization problem;multi-task
distance measure;theoretical framework;classification;large-scale;time series;distance measures;nearest-neighbor queries;privacy-preserving
data mining;virtual world;high quality;data mining and knowledge discovery;human computation;amazon mechanical turk;human computation;information retrieval;relies heavily on;lessons learned;acm sigkdd;human computation;pattern discovery;machine learning;knowledge extraction;game theory
search results;mechanical turk;large scale;result page;instances;main findings;human computation;high frequency
large-scale;data mining;human judgments;lessons learned;data freshness;human computation;human input;design decisions;data entry
design space;mechanical turk;human computation
human computation
low-quality;high-quality;large number of;social tagging;online communities;social annotation
large scale;9;online communities;user communities

iterative process;human computation
data-acquisition;real world;digital cameras;large quantity of;design decisions
music information retrieval;human computation;data collected;collected data;supervised machine learning;human computation
web-based;enabling users to;question answering;human computation
user interface;mechanical turk;human computation
online users
collecting data;training data;usage patterns;individual users;automatically discovering
training data;knowledge acquisition;human players;problem domain;human computation;human beings;word sense disambiguation;machine learning algorithms;word sense disambiguation
correct answers;algorithm generates;amazon mechanical turk;low quality;large number of;cost-sensitive;classification errors;error rate
amazon's mechanical turk;problem domains;human computation
quality metric;simple models;amazon mechanical turk;accurately predict;budget constraints;problem solving;human computation;design decisions;image labeling
search space;frequent subgraph mining;candidate patterns;frequent subgraphs;mining algorithm;margin
clustering;classification;regression problems;multimodal data;independent variables;cost function;cluster assignment;local minimum;data matrix;predictive model for;step procedure;wide range;customer behavior;complex data;dependent variable;collaborative filtering;prediction models
frequent itemset;total number of;minimum support;poor performance;database;frequent items;closely related;large number of;mining process;frequent itemset mining;candidate itemsets;adaptive algorithm;optimal performance;support counting;memory usage
large-scale;directed graphs;broder et al. 1998;external memory;massive graphs;quality assessment;clustering coefficient;large graph;large graphs;theoretical analysis;web graphs;social networks;approximation algorithms;main memory;sequential scans
feature space;lower-dimensional;multiple labels;dimensionality reduction method;data mining and machine learning;original data;dimensionality reduction;class labels;dimensionality reduction
clustering;feature space;high-dimensional;data points;real-world applications;exploratory data analysis;dimensional data;complex data;traditional clustering;benchmark data sets
service provider;large volume;data stream management system;efficiently computing;computation costs;content analysis;aggregate queries;network monitoring;sensor networks;query processing;data streams;high-speed data streams;traffic analysis;high speed
synthetic data sets;structural consistency;low recall;xml data;relevance-feedback;xml keyword search;object-relational dbms;instance-level;query performance;query result;structural consistency;false dismissal;keyword search;spurious results;query results;user-friendly;high precision
greedy algorithm;prediction techniques;moving-object;vehicle tracking;real data;indexing scheme;individual objects;concise representation;location update;road network;databases;spatial-network;maximum likelihood;range queries;short-term
clustering;document sets;similarity measure;document detection;similar documents;privacy-preserving;cosine similarity
clustering;classification;dataset characteristics;nearest-neighbor;similarity search;desirable properties;data-mining tasks
partial orders;ranking queries;uncertain information;markov chains;web search;distance metrics;applications including;efficient query evaluation;uncertain attributes;probabilistic model;query types;large databases;query answers;partial order;sampling techniques;rank aggregation;total order;incomplete data;data integration;synthetic data
hierarchical classification;hierarchical classification;large-scale;large scale;hierarchical text classification;information retrieval;large scale;evaluation measures
personal information;information access
ir research;entity ranking;structured documents;focused retrieval;xml mining;test collections;wide range;evaluation measures;ad hoc
information seeking;test collections;trec blog track;search tasks;natural language processing
high potential;speech recognition;information retrieval;broadcast news;retrieval models;spoken content;visual features
web services;information retrieval
mobile user;predictive model;emerging area;context-aware;mobile devices;information retrieval;highly dynamic;computing devices;search effort;information access;search tasks
trec test collections;pre-retrieval;low quality;data sets;wide range;correlation coefficients;meta-search;collection statistics;estimation accuracy;retrieval effectiveness;retrieval tasks;prediction algorithms;search systems;automatic methods;test collections;query expansion;evaluation methodology;prediction methods;ranked list;accurate ranking;relevance judgments;retrieval systems;external sources;query terms
information systems;mental models;information organization;semi-structured;user requirements;complex tasks;model construction;external sources;search session;user models;development process;search tasks
large-scale;generation algorithm;parallel algorithm;feature vector;ctr;hadoop mapreduce;probabilistic model;data structures;learning algorithm;user behavior;training dataset;skewed;sparse representations;fine-grained;caching scheme;regression model;machine learning problems;highly scalable
high-dimensional;data analysis;privacy requirements;privacy concerns;healthcare data;real-life;essential information;model called;large datasets;healthcare data;hong kong;data anonymization
web search;search log;click data;exact inference;instances;document relevance;exact inference;document relevance;log data;query-url
likelihood function;likelihood ratio;anomalous behavior;user-supplied;anomaly detection
distributed computing;database;32;programming languages;database researchers;data-centric;query languages;distributed systems
rdf data;continuous queries over;application domains
data-driven;database applications;database;plans;database systems
search interfaces;database;deep web;deep web;search engines;graph based;search interface
result diversification;result diversification;web search;recommender systems;search result diversification
sensor readings;incremental maintenance;data acquisition;data centers;query optimization techniques;sensor devices;data sources;low-power;highly distributed;wide range;query processing algorithms
1;edbt/icdt;international workshop on
sensor readings;bayesian approach
everyday life;wide range;plans
considerable effort;high cost;diverse applications;monitoring systems;sufficient conditions for
geometric information;body parts;control architecture;active vision
fast response;classification;control scheme;additional information;causal model;sensor fusion
robot control;plan-execution;human-robot;robot vision;context-sensitive;active vision;environmental conditions;recognition tasks;operator;image processing
image features;learning phase;directed graph;scene recognition;image sequences;geometric model;image-based;visual features
produce accurate;large-scale;grid-based;problem solving;grid-based;artificial neural networks
linear algebra;model-based diagnosis
domain knowledge;problem-solving;inter-dependencies;problem solver;ai research
competing methods;model-based diagnosis;constraint propagation;dynamic systems;constraint satisfaction problem
feedback controller;model-based diagnosis;software systems;based reasoning;past experience
qualitative simulation;model checking;temporal logic;temporal constraints;dynamical systems
phase space;physical systems
np-complete problem;search-space;large number of;physical systems;automatically identifying
qualitative simulation;common sense;partial knowledge;state transitions
case-based reasoning;problem domains

spatio-temporal;object-based;physical systems;dynamic model;qualitative model;spatio-temporal
desired behavior;parametric model
production rules;tree bank;context-free;tree-bank;tree-bank
domain knowledge;computational complexity;structure information;natural language processing;average-case;natural language
manually annotated;semantic tags;statistical techniques;information extraction;text corpora;automatically generating;natural language processing;heuristic rules;extraction patterns;training corpus;corpus-based
corpus-based;database;hand-crafted;database queries;counterpart;inductive logic programming;inductive logic programming;natural-language interface;logic program;control knowledge;natural language
search techniques;constraint satisfaction;search space;large-scale;natural language
semantic interpretation;additional knowledge;computational approach;semantic interpretation

plans;takes into account;partial plans;text descriptions;reasoning capabilities;computational model;natural language
recognition performance;training data;automatic speech recognition;word recognition
low-resolution;mental models;physics-based;autonomous agents;optical flow;virtual worlds;moving targets
sensor data is;common sense;sensor data
vision systems;recognition rate;multiple classifiers;similar objects;real world;single classifier
multiple classes;object recognition;wind speed;takes into account;invariant features;transformation parameters
control architecture;complex relationships;linguistic information;vision systems;context specific
uniform sampling;spatial resolution;fundamental problem;visual information;takes place;short-term
decision-support system;plans
plans;situation calculus;plans;sensing actions
everyday life;plan execution

decision theoretic;state space;state spaces;temporal logic;markov decision processes;decision process
planning problems;decision theoretic;dynamic programming;approximation methods;high computational cost;trade-offs;partially observable;belief space;partial observations;bayesian networks;partially-observable markov decision processes;belief state;conditional probability;optimal policies;decision processes
incomplete information;modeling language;qualitative model;temporal reasoning
completeness;plans;space requirement
operator;partial-order;plan generation
planning problems;planning algorithms;scheduling problems;search problem;stochastic search;instances;wide range;planning systems;search algorithm;classical planning;times faster than
heuristic functions;finding optimal;random instances;heuristic function;cost-effective;depth-first search;search problems
complexity bounds;optimal planning;blocks world;blocks world;worst case;upper bound
planning algorithm;plans;temporal logic;generate plans
pre-defined;low-cost;planning algorithm;evaluation function;plans
algorithm's performance;problem-solving;artificial intelligence
linear-programming;linear programming;temporal reasoning;temporal constraints;constraint reasoning
production systems;production systems;real life problems;argumentation based;operator;rule based
based reasoning;knowledge intensive;rule-based;problem solving;constraint solving;expert systems;constraint propagation;constraint satisfaction problem
algorithm produces;learning process;reasoning tasks
algorithm performs;upper bounds;bayesian networks;goal oriented;goal oriented
additional knowledge;observed data;real data;theoretical basis for;gibbs sampling;posterior distribution;bayesian learning
learning bayesian networks;naive bayes;naive bayes;naive bayes classifier;probability distributions;bayesian classifier;feature selection methods;bayesian networks;building classifiers;supervised learning;classifier
bayesian network;pattern-recognition;generalized queries;context-free;bayesian networks;dynamic-programming approach;network structure
expected utility;qualitative decision;agent model;decision theory;agent behaviors

probabilistic logic
conditional probability
world wide web;web users;web pages;world wide web

multi-agent systems
planning systems;unified framework
database systems;database;database theory;information-integration;knowledge representation;long-term
grid-based;indoor environment;large-scale
search space;local optima;fitness function;design choices;meta-level;rates;fitness
large scale;knowledge based;assembly process;data warehouse;high speed
motion tracking;ai research;intelligent behavior
applications including;dynamic nature of;object-oriented;autonomous agents;computational model;intelligent tutoring systems
markov decision processes;role based;plans
multi-agent;individual agents;reusability;plans;decision-theoretic
multi-agent;takes into account;data allocation;distributed systems
human interactions;electronic commerce;decision makers;belief update;game-theoretic;theoretical analysis;decision making
closed-form solution;situation calculus;model-based diagnosis;representation language;state constraints
response times;incremental update;database
design space;data analysis;design choices;large datasets;design parameters
interface design;decision-making;specific task;user interface;statistical analysis;problem-solving;problem-solver
probabilistic semantics
decision rules;qualitative decision;decision-making;utility function;qualitative decision
nearest;nearest neighbor;classification;classification accuracy;learning algorithm;data repository;classifier
classification;classification;real-world;machine learning;high confidence;feature vectors
model-based diagnosis;model-based diagnosis;reasoning tasks;discrete-event systems
causal model;model-based diagnosis;qualitative reasoning;physical systems;higher order
markov decision processes;planning problems;state space;finding optimal
markov decision processes;decision theoretic;planning problems;temporal logic;decision processes
qualitative simulation;complex systems;inference mechanism
qualitative simulation;instances;state space;potentially infinite;dynamical systems
low-level vision;higher-order;high-level;plans;hidden markov models

domain theory;spatial reasoning
object boundaries;sensor data
computational model
temporal information;temporal reasoning;constraint propagation
temporal logics;temporal logic
constraint satisfaction problem;tree-structured
algorithm called;inference algorithm;constraint satisfaction problems;finding optimal;bayesian inference
constraint satisfaction;multi-dimensional;constraint satisfaction problems;multi-dimensional
planning problems;search space;sufficient conditions;large-scale;constraint satisfaction problems;search problems
constraint satisfaction problem;real-world;davis-putnam;instances;sat instances;propositional satisfiability
search methods;scheduling problems;search techniques;constraint satisfaction;semantic analysis;optimization problems;graph coloring;heuristic search;constraint satisfaction problems;natural language
constraint satisfaction;constraint satisfaction problem;path-consistency
problem instances;computationally hard;highly structured;instances;random instances;search strategies;control mechanisms;search problems
knowledge bases;model-theoretic;general-purpose;constraint propagation;knowledge representation;model-theoretic
scheduling problems;resource constraints

query optimization;completeness;data stored;information sources;query interfaces;source-specific;query plan
success rate;semi-structured;logical structure;electronic documents;text-based;tree-structured;template-based;information retrieval techniques;html documents
local search algorithms;real-world;local search;instance;local search algorithms
local search;test problems;constraint programming;stochastic local search;optimization problem;integer linear programming;case study;case studies;domain-independent;propositional satisfiability;provably optimal;combinatorial problems
variable-selection;random instances;variable selection;sat instances;search strategy;randomly generated;local search;satisfiability testing
search methods;random sat;random walk;tabu search;stochastic process;tabu search
problem instances;problem solving
problems require;multi-dimensional;evaluation model
scheduling problems;constraint satisfaction;domain constraints;scheduling algorithm;domain-specific;scheduling problem
scheduling problems;stochastic sampling;high computational cost;constraint satisfaction;search heuristics;sampling technique
local search algorithms;constrained problems;phase transition;direct comparison;search cost;davis-putnam;problem domains;constraint satisfaction;accurate predictions
local search;statistical measures;problem domains;stochastic local search;search strategies;search process;search heuristics
probability distributions;probability distribution;randomly generated
instances;constraint satisfaction problems;phase transition;random 3sat
clustering;problem instances;phase transition;search cost;instances;random 3sat;average number of;search algorithm
virtual camera;learning environment;plans;knowledge-based;learning environments
data-rich
neural networks;gaussian distributions;skewed;machine learning models;gaussian distribution;statistical tests;machine learning
competence;knowledge bases;knowledge base;knowledge-based systems
knowledge base;knowledge-based;knowledge acquisition;knowledge bases;knowledge-based systems
pattern matching;additional features;description logics;inductive learning
description logic;inference procedure;description logics;inference algorithm;knowledge representation;bayesian networks;basic properties;algorithm relies on;description logic
meta-level;problem instances;concept learning;logic program;object-level
background information;completeness;program analysis;knowledge representation;type-inference
test cases;rule-based;rule-base;sufficiently high;test data
production rules
closely related;special case;computationally expensive
logic programming;computational properties;nonmonotonic reasoning
user community;world wide web;development environment
high-end;complex queries;wide range;data base;memory management

description language;query language;sensing actions;plans
reasoning tasks

instance-based
completeness
learning algorithms;learning algorithm;worst-case;upper bounds;worst-case;concept learning;perceptron algorithm;weight vector
training data;version spaces;typically performed;version spaces;machine learning;concept classes;version-space;concept class
search space;genetic algorithms;search algorithm;local neighborhood
geographically distributed;discovery algorithms;induction algorithm;distributed databases;decision-tree;distributed data;databases;pattern discovery;pattern discovery
decision tree learning algorithm;class distributions;learning algorithms;decision tree algorithms;data sets;noisy data
data-driven;natural-language;social science;simpler models;data sets;scientific discovery;model building
sparse representations;induction algorithm;sparse representations;distinctive features;constraints imposed by;computational model
statistical methods;hill climbing;temporal locality;computationally intensive;major components;spatial locality;genetic algorithms;transition probabilities;major limitation;hard disk
synthetic data sets;bayesian network structure;learning bayesian networks;ad-hoc;expectation-maximization;missing data;incomplete data;conditional probabilities;learning bayesian networks
unseen data;multi-layer;neural networks;generalization error;model generalizes;training error;bp) algorithm;machine learning models;real world;neural network;network size
decision trees;neural networks;data set;instances;neural-network;ensemble method;classifier;classification algorithms
training data;auxiliary;hypothesis space;learning tasks;estimation techniques;model selection;training sample
search task;problem instances;search space;search strategy;search process;machine learning;planning domain;heuristic search;artificial intelligence;load balancing
computational requirements;training sets;parallel implementation;production rules;comprehensibility
planning algorithms;control architecture;completeness;state detection
reinforcement learning algorithms;state space;learning algorithms;reinforcement learning;reinforcement learning problems
training phase;poor performance;feature selection method;generalization performance;correlated features;text categorization;addressing this problem;diverse set of;perform poorly;classifier;collaborative learning
training examples;active learning;large number of;text categorization;real-world domains;labeled training examples;supervised learning;learning method
context-free;language model
nearest-neighbor classification;model complexity;decision trees;probabilistic models;log-linear models;word-sense disambiguation;probabilistic model;supervised learning algorithms;supervised learning algorithm;word sense disambiguation;classifier;model selection;training sample
natural language
semantic interpretation;knowledge sources
semantic relations between;semantically related;spreading activation;multi-document summarization;related documents;natural language;graph representation
data-driven;high-level;local constraints;natural language texts
real-world;robot control;multi-robot;general-purpose
multi-agent systems;machine learning;sensory data;hidden state;robot learning;multiple agents;assignment problem
spatial reasoning;spatial reasoning
landmark-based;illumination conditions;image region;landmark-based;image regions;salient regions;operator
processing algorithms;fourier transform;context-dependent;signal processing
high similarity;ai planning;plans;real-world domains;worst case
similarity assessment;problem-solving;semantic similarity;adaptation knowledge;plans
state representation;plans;automatically selecting;state space;embedded systems;search space
planning domain;plans;sketch-based;ai planning;automated methods;user-supplied;plan recognition;partial plans;abstraction levels
optimal planning;partial-order;linear programming;planning algorithms;instances;blocks-world;plans
pattern databases;state space;finding optimal;random instances;heuristic function;cost-effective;lower-bound
planning algorithm;rewriting rules;taking into account;plans;local search techniques;domain-independent planning;low-cost;plan quality;high-quality
planning problems;planning algorithm;intelligent behavior;action selection;dynamic environments;heuristic function;domain independent
plans;representation language;partial-order;partial order;generate plans;multiagent systems
planning problems;partially observable markov decision processes;goal oriented
lower bound;partially observable markov decision processes;optimization problem;belief space;partially observable markov decision processes;grid-based;upper bound;control problem
potentially infinite;general purpose;stochastic processes;context-free;probability distribution over;inference algorithm;bayesian inference;bayesian networks;inference algorithms for;conditional probabilities;programming language
planning problems;computational complexity;plans
lower cost;artificial intelligence
knowledge representation;knowledge representation and reasoning;artificial intelligence
intelligent agents;classification;reinforcement learning;intelligent systems;machine learning;ai research;machine learning
agent behaviors;agent architecture
electronic commerce;distributed environments


state university;mobile robotics
autonomous navigation;multi-robot
case base;multi-layer;basis functions;artificial intelligence;ai techniques
neural networks;feature construction;probabilistic learning;probabilistic reasoning;learning theory;neural nets;intelligent systems;knowledge based systems;bayesian networks;artificial neural networks;learning environment;information theoretic;machine learning methods

knowledge management;rule-based;internal memory;decision trees;problem domain
completeness;plans;semantic level;domain constraints;constraint-based;constraint based
case based reasoning;high-quality;plans;world wide web;cost-effective;plans
decision process
case-based reasoning;application called;rule-based;knowledge-based;knowledge based systems;knowledge sources
long-term;ai techniques;input data;semi-automatic
constraint satisfaction;decision support;resource utilization;low cost;domain specific
lessons learned;designed to support;constraint programming
case study;social security;expert systems;rule-based;problem domain
high level;case-based-reasoning;raw data;extract information from;data mining;credit card;knowledge base
relational database;case-based reasoning;knowledge-based;knowledge bases
free text;knowledge bases;information extraction;natural language processing;end user;information extraction system
control architecture;software architecture;robot control
multimedia presentation
automatically generates;plans;operator;highly sensitive;hierarchical task;knowledge base
software agents;agent systems
hand-held;multi modal;lessons learned;agent architecture;artificial intelligence;agent-based
design decisions;design process
knowledge engineering;domain experts;extracted information;free-form;information extraction;knowledge based;natural language processing;knowledge based systems
problem instances;supply chain;plans;information sources;legacy systems;process-planning;process planning;highly dynamic;decision making;user-oriented
lessons learned;plans;large-scale;case study;intelligent agents;intelligent agents
information extraction;classification;document classification;information network;information extraction;information network;semantic constraints;training corpus
control architecture;problem-solving;knowledge based;pattern recognition
optical flow;video camera;single image
domain experts;plans;based reasoning;real-time monitoring;utility theory;fully automatic;generate plans;hierarchical task;plan execution
automatically selecting;knowledge base;large-scale
knowledge bases;knowledge-base;large graphs;knowledge representation;graphical user interface;programming interface

training set;test set;internet users;feature-based;decision-tree;vector based
multiagent systems;information flow
query-driven;plans
case-based reasoning;information systems;rule-based;historical data;information technology;decision making
user profile;classification model;relevant documents;probabilistic model;expert finding;topic model;user profiling;tree-structured;information extracted from;user profiling;baseline methods;user profiles;conditional random fields
transaction data;em) algorithm;parameter-free;question-answering;knowledge-sharing;knowledge-sharing;synthetic data;specific set of;expectation-maximization;case study;real-life;clustering algorithm;question-answering;transactional data;bayesian information criterion
individual users;web pages;social annotations;extraction task;user-generated;probabilistic model;collaborative tagging;inference methods;web resources;bayesian approach;real-world;relevant information;bayesian framework;social annotation;social annotation;latent dirichlet allocation
network analysis;data analysis;naïve;sensor monitoring;databases;moving objects;rates;realistic data;data mining;theoretical analysis;data streams;object tracking;relative error;resource consumption
online social networks;information-sharing;sensitive information;mathematical models;privacy concerns;privacy risk;social networks;privacy issues;real-world;individual users;online social networks
text mining;structured documents;text retrieval;xml) documents;concept-based;text search;multiple representations;search engine;queries posed;text corpus;ad-hoc;retrieval models;information retrieval research;retrieval model;data structures;search engine;document content;interactive search;question-answering;exact-match
approximation algorithms;synthetic datasets;database schema;database;primary key;1, 2;exploratory data analysis;databases;automatically extract;information theoretic
social network;sentiment analysis;web browsing;entity recognition;natural language processing;real life applications
web pages;temporal dynamics;temporal dynamics;information retrieval;management systems;web content;search engines;retrieval model;user intent;temporal evolution;information extraction
24;ontology-based;classification;information extraction;case study;information extraction
clustering;related concepts;frequently occurring;semantic network;large-scale;information theoretic
individual users;structured databases;database;text streams;query feedback;relevant entities;user preference;entity extraction;space complexity
automatic extraction of;automatically extracting;extraction process;domain constraints;data records;user-generated content;algorithm called;data record;similarity measures
search space;point-based;pruning techniques;event data;interval based;interval-based;mining patterns;sequential pattern mining;large database
document relevance;user interactions;ad-hoc;probability distribution over;information retrieval;vector spaces;retrieval task;retrieval framework;quantum theory
entity ranking;web pages;redundant information;entity ranking;entity types;entity ranking;ranked list;category structure;ranking problem;performs poorly;main findings;test collections;text retrieval;entity types;manually assigned;entity retrieval;high precision;relevant pages
ranking algorithm;query execution times;ranked list;joint model;ranked retrieval;ranking algorithms;test collections;feature selection;temporal constraints;prediction models
weighting function;weighting schemes;information retrieval
relevant document;relevant documents;relevance-feedback;relevance judgments;relevance feedback;retrieval performance;relevant-documents;ad hoc;quantitative analysis
store data;nearest neighbor;poor performance;query operators;similarity search;distributed environments;query processing;queries efficiently;result set;query routing;multidimensional data;network traffic;range queries
indexing schemes;xml databases;reachability queries;directed graph;indexing scheme;large graphs;reachability query;processing speed
optimization technique;compression rate;wavelet decomposition;uncertain data;time series;data sets
query efficiency;large scale;optimization strategies;short text;theoretical analysis;information retrieval tasks;correlation coefficient;databases;operator;hidden) databases;relational database management systems
data structure;database;search performance;user transactions;databases
high potential;web pages;classification approaches;search engines;link-based;machine-learning;web search engines
retrieval effectiveness;concept-based;domain-specific;cross-lingual;concept-based representation;biomedical information retrieval
multi-modal;multi-modal;factor analysis;ranking list;face images;news items;probabilistic matrix factorization;search results;statistical analysis;news events;image information;news event;natural language
detection algorithms;classification;textual data;image search;event-driven;text-based;user generated content;multimedia retrieval;direct comparison;multimedia content;social media sites
viral marketing;aggregation algorithm;heterogeneous networks;link information;user behavior;textual content;graphical model;citation networks;data sets;social influence;social networks;influence propagation;information retrieval;user behaviors
link formation;frequent subgraph mining;social network analysis;link structures;real-world;social network;link formation;social networks;link structures;mining algorithm
clustering algorithms;parameter-free;multiple communities;baseline methods;large-scale;density-based clustering;complex networks;optimization methods;hierarchical structure;density-based;real-world and synthetic datasets;community detection;clustering algorithm;community detection;community structure
ranking algorithm;public domain;movie database;interaction networks;modeling technique;real-life;network structure
search methods;information diffusion;real-world;search strategy;path length;social networks;complex networks;graph-theoretic
data sets;retrieval effectiveness;ad-hoc retrieval;language models;hierarchical structure;term dependencies;document structure;markov random fields;unified framework;query expansion technique;expansion terms;query expansion;markov random fields;query terms
term weights;training data;document relevance;relevant documents;retrieval accuracy;relevance judgments;standard datasets;predictive model;retrieval models;document frequency;documents retrieved;query-dependent;supervised learning;query terms;relevance model
document corpus;data analysis;latent dirichlet allocation;tool called;topic models;low-dimensional;retrieval performance;topic model;principal component;principal component analysis;principal component;latent semantic indexing;low-dimensional;dimensionality reduction
document summarization;summarization method;hierarchical clustering;clustering framework;summarization methods
large scaled;regression method;unlabeled data;labeled data is;real world datasets;knowledge management;kernel based;information retrieval;coordinate descent;labeled data;data mining;theoretical analysis;semi-supervised;semi-supervised
relational data sources;query processing;highly heterogeneous;data dependencies;real world data sets;human experts;data integration systems;open world;conjunctive queries;logical properties;design process;source schemas;data integration;tuple-generating dependencies
large numbers of;privacy guarantees;online social networks;large volumes of;geo-spatial;multiple users;location privacy;social networks;location-based services;privacy preferences
real data sets;query evaluation;database systems;multi-objective;database;data stored in;query processing;web services;query processor;user-defined functions;web-service
large volume;query evaluation;large-scale;energy-efficient;query processing in;environmental monitoring;sensor networks;real datasets;unique characteristics;network lifetime;energy-efficient
storage space;embedded systems
data point;multi-label classification;relevance score;multiple labels;nearest neighbor classifier;multi-label learning;text categorization;learning tasks;real-world applications;multi-label classification;instance;multi-label;information retrieval;margin;classifier
probabilistic latent semantic analysis;classification problem;em) algorithm;multiple target;classification tasks;cross-domain;expectation maximization;key words;latent factors;statistical model;source domains;text classification;multiple domains;transfer learning;data distribution
classification accuracy;content features;classification tasks;movie database;logistic regression
discriminative training;labeled samples;machine learning and data mining;unlabeled samples;test collections;generalization ability;generative model;classification method;learning problem;text classification;semi-supervised;training algorithm;transfer learning;classifier;overfitting problem
learning process;multi-view clustering;similarity measure;multi-view learning;multi-view;constrained clustering;multi-view clustering;clustering model;view based;instances;clustering performance;pairwise constraints;algorithms typically;constraint propagation
user visits;high-dimensional
clustering;clustering problem;12;19;efficient clustering;clustering method;image databases;image data;multi-view;minimum spanning tree;image clustering;clustering methods;takes into account;distance metric;18;image clustering;metric spaces;learning method
dimensionality reduction methods;spectral clustering;manifold learning
synthetic datasets;frequent itemsets;data uncertainty;high degree of;uncertain database;large databases;mining process;emerging applications;frequent itemset mining;uncertain databases;monitoring systems;location-based services;highly accurate;data integration
inference method;log data;large scale;click models;unified framework;click model;bayesian inference;inference methods;user search behavior;bayesian framework;bayesian inference;click data;bayesian learning
search accuracy;exhaustive search;search cost;computational resources;query processing cost;interactive search;large collections;query sets;trec collections
average precision;semantic role labeling;feature set;semantic features;rank learning;passage retrieval;qa) systems;question answering;retrieval models;ranking model;named entity;semantic constraints;question answering;learning framework;arbitrary length
objective function;topic detection;ground truth;ranked list;browsing behavior;greedy algorithm;objective functions;user feedback;performance metric;retrieval performance;session-based;desirable properties;evaluation measures;user query;named entities;search session;np-hard
set cover;set cover;greedy approach;random accesses;large datasets;finding an optimal;data mining;machine learning;disk resident;wide range;analysis tasks;very large datasets;larger datasets;data quality;greedy algorithm;np-hard
filtering systems;information sources;ir techniques;sliding window;highly dynamic;user interests;performance gains;real world
shortest paths;biological networks;reachability queries;sketch-based;shortest-path;graph database;index structure;query response times;large graphs;instances;estimation accuracy;graph data;real-world;estimation errors;real world graphs;disk-resident;online social networks
shortest paths;search space;large scale;optimization strategies;pre-computed;general case;worst-case;shortest path;social networks;graph data;increasing attention
factors affecting;search results;search interfaces;result page;search sessions;factors affecting;user studies;search interface
commercial search engines;web search;search engine;queries issued;search engines;upper bound;web search
completeness;retrieval effectiveness;relevant documents;sampling method;retrieval systems;information retrieval tasks
search systems;personalized search;search performance;feature sets;classification;social tags;free-text;relevance judgments;retrieval systems;information from multiple sources;complex systems;features including;filtering methods;search engine;benchmark datasets;link analysis;collaborative filtering;multi-faceted
information overload;recommendation approaches;learning algorithm;recommendation algorithms;music recommendation
data set;close connection;demographic information;end users;personalized recommendation;generative models;social network;observed data;prior knowledge;multiple dimensions;large data sets;social networks;real-life applications;data characteristics;iterative algorithm
prediction accuracy;markov models;topic models;user preference;united states;real-life;location information;behavior model
online social networks;recommendation service;network connectivity;dynamic social network;communication costs;real datasets;social network;poisson process;limited number of;long range;social networking;information propagation
large-scale;lower bounds;desirable properties;approximation guarantees;fitness;semi-automatically;application scenarios
iterative process;behavioral patterns;retrieval models;iterative learning;retrieval model;average precision;classifier
search results;users' interests;web page;user clicks;social tags;query recommendation;query logs;search queries;helping users;search engines;query recommendation;search intent;social annotation;search session
document ranking;generative process;weighting model;language model;retrieval tasks;statistical language models;retrieval models;topic/relevance;term frequency;generation process;information retrieval models;text processing
ad hoc;classification;language models;spatial information;short texts;data set;multi-scale;analysis tasks;text retrieval;document categorization;text analysis;natural language understanding
document space;causal relationships;topic model;topic model;user interests;generative model;low-dimensional;dyadic data
extracted information;extracted information;information retrieval techniques;decision support;past experience
partial orders;real data sets;multimedia databases;recommender systems;similarity measure;uncertain databases;access latency;similarity queries;similarity queries;computational cost;hit rate;metric space;query results;similarity measures
web-based;related concepts;specific information;search terms;text-based;common sense knowledge;image search;retrieval performance;image collections;semantic relationships;semantic information;event-based
existing indexes;natural language text;memory usage;index structures;highly scalable;wide range;query logs;index structures
large-scale;keyword queries;large volumes of;text analytics;text data;keyword search over;query processing;web archives;optimization problem;high cost;efficient approximate
information-retrieval;query optimization;query results;database systems;selectivity estimation;database;0, 1;document frequency;text databases;information retrieval;data structures;summary statistics;term frequencies;query processing;selectivity estimation;query-specific;size estimation
user navigation;structured databases;search algorithms;amazon mechanical turk;real datasets;cost model;cost-based;query results;user study;information-overload;np-hard
search systems;retrieval effectiveness;database;real-world;indexing techniques;search strategies;databases;ad hoc;structured data
large networks;link prediction;prediction method;graph kernels
training set;test image;texture features;efficiently identify;feature selection;supervised learning algorithms;transfer learning;detection accuracy;additional cost
probabilistic framework;knowledge bases;classification;user's location;location based;location information;information services;location-based;key features;automatically identifying
accuracy compared to;classification model;classification;synthetic data sets;mining data streams;generation process;highly accurate;instances;main idea;data streams;learning framework;quality measure;data distribution
diverse set of;sensor data;temporal properties
link detection
relevance feedback;user profile;vector space;specific features;reuters corpus;extracted features;retrieval accuracy;learning algorithms;sliding window;relevance feedback;knowledge based;training documents;information filtering;pattern mining;negative feedback
information discovery;user profile;web search;collaborative filtering;collaborative filtering;real-world datasets;social tagging;user profiling;web resources;tag-based;graph structure
recommender systems;similarity metric;ranking function;user feedback;item recommendation;active learning approach;future events;user study
real-world datasets;information spaces;social annotation;matrix factorization;wide range;real world applications;social annotation
specific applications;xml schemas;xml schema
multi-dimensional;data analysis;clustering structure;data cube;on-line analytical processing;great success;on-line analytical processing;visual features;rich semantics;user requests;search engines;image collections;olap operations;image information;web images;social networks
clustering;class information;classification;database;class-attribute;data types;large databases;post-processing;discover patterns;discrete data;pattern discovery;pattern discovery;real world;continuous data;large database
classification;pruning techniques;machine learning methods;frequent patterns;data mining techniques;pattern mining;mining process;discriminative patterns;classification performance;highly correlated;feature selection;correctly identify
real data sets;support vector regression;sampling methods;regression function;large number of;regression problem;preference data;random sampling
document classification;training data;unlabeled data;training examples;input data;related data;text classification tasks;accuracies;machine learning methods;text data;human generated;rule base;large datasets;rule based;rule based;supervised learning;text processing;test data
feature space;external knowledge;cross-validation;database;text descriptions;image content;image data;wikipedia articles;lda) model;high quality;highly-reliable;latent semantic;image annotation
clustering;feature space;multi-class classification;feature extraction;kernel function;feature extraction method;classification;extracted features;classification accuracy;data mining and machine learning;feature extraction;discriminative features;distance-based;input data;real-world datasets;feature extraction methods;multi-class classification problems
worst-case;approximation algorithms;general problem;computationally intractable;synthetic data
15;22;text documents;originally designed;wikipedia articles;principal component analysis;algorithm produces;qualitative analysis
supervised methods;web based;scoring methods;product review;user evaluation
active user;user similarity;user interests;algorithms produce;accurately identify
traditional collaborative filtering;information systems;collaborative filtering;community-based;large-scale;data set;implicit feedback;user's search;baseline methods;collaborative filtering;positive examples
11;personalized search;user profile;resource-sharing;web sites;personalized search;collaborative tagging;data set;baseline methods;user profiles;retrieval quality;tag-based;collaborative tagging
user ratings;retrieval effectiveness;prediction methods;query suggestions;user's perspective;query performance;performance predictors;query performance;query performance prediction;pre-retrieval
web search;information retrieval;search engines;information access;dynamic bayesian network;relevant information;search interface;context information;clustering algorithm;log data;search results;search-session;taking into account;real-world;user behavior;web information systems;user intent;user interaction;user actions;ranked list;privacy concerns;cognitive load;predictive performance;search engine;search intent
search results;personalized search;users' preferences;social systems;private data;real-world;large-scale;integrating information from;data collected from;user interests;social activities;adaptive approach;integration process
search systems;predictive accuracy;large-scale;optimal combination of;search result;contextual information;web page;short-term
search results;query rewriting;auc;language models;ad-hoc;tf-idf;retrieval tasks;information retrieval;search advertising;query segmentation;vector space model;search engine;precision-recall;language modeling
search results;world wide web;meta-data;query interface;search problem;faceted search;faceted search;query-result
predictive models;online social networks;predicting future;user behavior;social network;high degree;strong evidence;social influence;online advertising;large-scale social networks
result sets;retrieval algorithms;query formulation;inverted lists;term frequencies;documents retrieved;retrieval algorithms;collection statistics;information retrieval systems
training set;high-quality;conditional random field;verbose queries;labeling problem;retrieval models;retrieval performance;information retrieval;query words;long) queries
pseudo-relevance feedback;search strategies;optimization framework;term dependency
core components;context modeling;language models;ranked list;related entities;language modeling approach;category information;contextual information;high recall;relevant entities
real users;information sources;semantic tags;social tagging;search engines;search engine results;semantic relations;similarity measures;online advertising
graph partitioning;knowledge bases;equivalence classes;large-scale;class hierarchy;markov chain;knowledge base
social annotations;large-scale;random walk;concept hierarchy;semantic web applications;hierarchical clustering algorithm;data sources
bayesian model;sampling algorithm;topic segmentation;user interfaces;dirichlet process;closely related;multiple documents;document clusters;multi-document;baseline models;specific topics;multi-document
information resources;end users;information sources;digital information;data structures;template-based;case studies;software architecture;application developers;information source
web documents;web search;clickthrough data;document retrieval;translation model;retrieval models;retrieval systems;data set;statistical machine translation;search queries;web search;real world;quantitative analysis;phrase-based
search results;low-quality;fundamental properties;classification problem;web search;language-independent;temporal information;temporal behavior;user queries;query log;search tasks;query logs;query results;temporal profiles;search logs
search results;web pages;web search engine;improving web search;learning algorithm;search result quality;search engine;search quality;web search engines
clustering;web search;user interactions;large-scale;query suggestion;real-world;web search engine;query logs;user effort;query frequency;search queries;user experience;user intent;search engine;enabling users to;high-level;user studies;web search engines;clustering techniques
search results;ambiguous queries;web search results;search result diversification
text mining;text mining;social media;real-world;real-life;dictionary-based;case studies;large datasets;unstructured data;efficient construction
text mining;latent semantic;opinion mining;cross-lingual;product-feature;cross-lingual;latent semantic;opinion mining
text mining;domain knowledge;database;customer service;textual data;text analytics;text analysis;building blocks for;hierarchical classifier;text documents
large graphs;ir systems;dynamic programming;dynamic programming approach;information retrieval systems
retrieval strategies;web search;large-scale;inverted lists;data sets;information retrieval;retrieval models;retrieval model;query processing;index structures;query terms;term proximity
index structures;index structure;compression techniques;lessons learned;data sets;document collections;query processing;processing speed;information retrieval systems;index size
web applications;data updates;multi-core;query response times;query processing;concurrency control;search nodes;multi-threaded;web search engines;search nodes
medical records;million records;web pages;parallel algorithm;multi-core;increasing number of;diverse sources;data redundancy;massive datasets;data records;enormous amounts of;application domains;massive amounts of data;disk-based;data intensive
search log;search-log;mutually exclusive;search-engine;user privacy;target function;search logs;personal data
real datasets;12;social networks
high quality;image processing;data collected
automatically extracted from;data model;real-world applications
matching scheme;online social networks;synthetic datasets;database;data types;massive graphs;large number of;disk accesses;subgraph matching;global information
real data sets;manually assigned;xml retrieval;relevance ranking;xml documents;highly correlated;retrieval performance;retrieval effectiveness
large datasets;uncertain data;query answers;computational overhead;query processing;skyline queries;queries efficiently;query answer;margin;probability threshold;skyline query processing
performance metric;flash-based;database systems;databases
archived data;storage schemes;data warehouses;social network analysis;distributed file system;network monitoring;data sizes;business intelligence applications;cost-effective;structured data
distributed query processing;ranking queries;query processing;threshold algorithm;bandwidth consumption
search results;service descriptions;semantic web;service discovery;entire process;semantic annotations
social influence;search strategies;web search;social networks;document search
multi-dimensional;query optimizer;query plan
retrieved documents;web-based;data sets;pruning algorithm;structured documents
skewed data;data-centric;sensor networks;storage scheme;data-centric
domain knowledge;database instances;object-oriented database;databases;instances;case studies;object-oriented database
attribute values;complex queries;time series;similarity queries;time-series;anonymized data;model called
instances;social networks;data management
entity extraction;dictionary-based;efficiently extract;entity recognition;distance constraints;entity extraction;approximate string
transaction processing;solid state;flash-based;solid state;flash-based
integration process;source data;heterogeneous sources;data provenance
query recommendation;query suggestions;apriori algorithm;query recommendation;small samples;enabling users to;approximate results
data objects;attribute values;data uncertainty;uncertain data;data acquisition;nearest-neighbor
fully connected
high-dimensional;image retrieval;object recognition;multimedia databases;search performance;overlapping clusters;index structures;index structure;similarity search;data set;similarity search;existing indexes;distance-based;real-world data sets;high dimensionality;metric spaces
query optimization;ontology-based;real-world;semantic web;network latency;data set;data sources;information integration;answer queries
relational database;query processing
massive graphs;hierarchical approach;interaction networks;query efficiency;depth first search;large graph;state space search;transitive closure;reachability query;index size;databases;query answering
ranked list;heterogeneous data sources;search queries;data sources;relevant answers;algorithms for computing;highly-ranked;query returns;search tasks
error propagation;data sets;large number of;schema matching;web-scale;web crawlers;data sources;integration systems;pre-processing;web forms
attribute values;sensitive information;external knowledge;anonymization algorithm;sensitive attribute;sensitive attributes
web sites;web search;retrieval effectiveness;relevance information;large-scale;web site;machine learning;retrieval models;search engines;web search results;search engine;web search;web page;test collection
relevance feedback;search results;user's perspective;optimization problem;machine learning
topic model;topic models;social tagging;latent dirichlet allocation
classification;manually annotated;fine-grained;html tables;large sample;classification algorithm
keyword based search;social media;document content;user behavior;search techniques;internet users;authority flow;personalized ranking;ranking algorithms;personalized recommendations;blog posts;social interaction
text retrieval systems;approximate matching;query language;exact matching;large collections of;indexing scheme;retrieval systems;complex structures
document relevance;large-scale;user behavior;click models;click model;ranking function;search result;user search behavior;web search;manually labeled
error rates;high quality;source text;automatic speech recognition;video content;selecting relevant
web pages;web page classification;large-scale;web sites;human performance;supervision;web page;user study
pattern-based;incoming documents;specific information;based reasoning;pattern- based;information filtering;pattern mining;rough sets;sheer volume of;irrelevant documents;term-based;large number of
retrieval performance;fusion method;ranking methods;ranking process;concept-based;external knowledge;information retrieval;cross-lingual information retrieval;test collections;cross-lingual;conceptual model;computational cost;semantic information;retrieval results
structured output;greedy algorithm;document cluster
image retrieval;search results;ranking results;aggregation techniques;aggregation methods;information processing;rank aggregation;search engines;theoretical analysis;objective function;problem called
recommendation quality;user preferences;collaborative filtering;multiple datasets;collaborative filtering algorithms;matrix factorization;user interests;explicit feedback;user feedback;real world;statistical models;implicit feedback
loss function;evaluation measure;ranking function;information retrieval;baseline methods;rank based;evaluation measures;machine learning algorithms;selection algorithm
context information;trec collections;pseudo relevance feedback;graph-based;random walk;pseudo relevance feedback;context-dependent;feedback documents
recommender systems;recommendation systems;recommender systems;meta-data;clustering method;similarity graph
random graph;degree distribution;large scale;path length;social networks;online social media;real world;reward function;power-law;online social networks
image retrieval;search results;highly relevant;taking into account;content-based retrieval;individual objects;image search;spatial distribution;image collections;retrieval results;content-based image retrieval
personalized search;decision trees;support vector machines;web pages;retrieval performance;search engines;ranking methods;information retrieval;machine learning algorithms
machine learned;document relevance;user session;search engine;user behavior;ranking function;data collected from;ranking schemes;relevance model;behavior model;user behaviors
sequence alignment;sentence level;web search;conditional random field;training data is;query reformulation;user queries;iterative learning;parallel corpora;web search;objective function;vice-versa
text mining;semantic labels;topic models;topic model;topic distributions;image annotation;image annotation
syntactic structures;retrieval accuracy;tf-idf;ranking function;information retrieval;retrieval performance;weighting scheme;natural language processing;query-dependent;information retrieval;query terms;natural language
ranking algorithm;training set;source domains;unlabeled data;web search engine;training data is;target domain;information retrieval applications;algorithm iteratively;ranking function;auxiliary data;labeled data;online advertising;ranking functions;labeled examples
term dependencies
query reformulation;trec collection;query reformulation;relevant documents
commercial search engines;web search;ranked list;learning algorithm;ranking algorithms;user feedback;online learning;ranking functions;machine-learned;learning framework
community question answering;community question answering;mathematical model;classification models;selection bias;answer questions
user-generated content
real-world datasets;web documents;web pages;large-scale;target pages;domain-independent;limited resources
relevant entities;query dependent;search engines;entity retrieval
census data;web search engine;vice versa;demographic information;instance;query log;query logs
search results;search engine
recommendation quality;recommendation methods;recommendation systems;collaborative filtering;similarity measurements
personalized recommendation;meta-data;manually assigned;filtering techniques;online content
pseudo-relevance feedback;search queries;document corpus;query processing;additional information
keyword query;attribute-based;data obtained from;feature-space;distance metric
benchmark data;document summarization;summarization task;propagation model;multi-document summarization;bipartite graph
2;web search;domain independent;1, 9 11;opinion mining
image retrieval;search results;image descriptors;web image;user feedback;search engines;semantic relationships;graph structure;semantic graphs;graph representation
image retrieval;storage cost;local features;high dimensional;data collections;content-based image retrieval;global features;randomly generated;real-life;low computational cost;random sampling;sampling technique
user behavior;evaluation metric;users interact with;search sessions;search engine results;user models;search engine;information retrieval evaluation;search evaluation
latent dirichlet allocation;community-based;inference mechanism;complex networks;social tagging;detection algorithm;topic modeling;tag prediction
content-based filtering;user profile;neural networks;classification;language models;model assumes;logistic regression;prior models;user interests;real users;user profiles;prior models;logistic regression;hierarchical model;bayesian hierarchical;classifier;modeling approach;baseline models
storage requirements;relevance models;ranking documents;relevance model;initial retrieval;pseudo-relevance feedback;search quality;pre-processing;query expansion;language modeling
clustering;video sequence;association patterns;video data;mixture components;predict future
labeling effort;sampling algorithm;sampling techniques;large-scale;web-scale;sampling strategy;large collections;classifier;random sampling
increasing number of;community question answering;community question answering
training set;loss function;multiple labels;document retrieval;lower level;data set;ranking performance;ranking model;higher level
clustering;training data;ranking models;lower-bounds;data-sets;unified framework;conditional probability;latent topics;objective function
clustering;low-rank;information retrieval;data matrix;retrieval performance;document matrix;latent semantic indexing
algorithm finds;real-world;selection algorithm;information retrieval
community question answering;question answer;domain-specific;retrieval model;term weighting scheme;retrieval models;multiple aspects
related concepts;concept-based;potential function;retrieval accuracy;semantic representations;information fusion;detection results;concept detection;video retrieval;video retrieval
hierarchical dirichlet process;textual documents;real world;community discovery;baseline models
clustering;world wide web;web service discovery;test data;distance measure;web service;semantic similarity;clustering approaches;data set;web service;web services;randomly generated;threshold selection;clustering algorithm
global model;active learning;multiple views;active learning;active learning methods;classification accuracy;labeled training examples;active learning approach;limited number of;class-specific
text fragments;search-engine results;5,8
ranking approaches;outlier ranking;outlier mining;analysis task;detecting outliers;outlier mining
graph model;textual content;data set;social networks;network structure;power law distribution
web pages;database;knowledge management;database records;web pages;web page;structured data;real world data sets
recommendation accuracy
behavioral patterns;communication network;maximum-flow;social networks;information propagation;social networks
web documents;computational costs;classification;5,4;large-scale
text mining;large volume;document collection;topic model;text data;document collections;latent topics;discover meaningful
knowledge sharing;baseline methods;keyword extraction;classification method
wikipedia articles;instance;attribute values;structured information
service provider;high quality;extraction algorithm;web sites;information retrieval;semi-automatic
genetic algorithms;genetic algorithms;time series
community question answering;ground-truth;community question answering;modeling approach;information seeking;semi-supervised;modeling methods
mobile user;association mining;data collection;user interactions;context-aware;pattern mining;mobile devices;behavior patterns;real life;takes into account;user interaction;mobile users
citation network;biological networks;real datasets;large graph;interesting patterns;large graphs;data structures;data processing;real-world graphs;million edges
multi-domain;classification model;labeled features;class distributions;classification methods;labeled documents;information extracted from;domain-specific;high confidence;weakly-supervised;unlabeled instances;labeled examples;classifier
information fusion;complex data;knowledge representation and reasoning;formal concept analysis
multi-task;training data is;online algorithms;online learning;feature selection;closed-form solutions;real-world;learning framework;multi-task
collaborative filtering;recommender systems;personalized ranking;topic model;cold-start;representation scheme;user interests;rates;benchmark data sets
training set;large datasets;classification;large scale;search strategy;classifier;support vector;data description;support vector;computational costs;boundary detection;support vectors;classification quality
data model;natural language processing
prediction accuracy;training data;incremental-learning;machine learning methods;learning problem;vector machine;learning tasks;pattern classification;transfer-learning;incremental learning;data volumes;knowledge transfer;training and test data;real-world data sets;dimensional data;support vector machines;transfer learning;auxiliary
text mining;semantic concept;document collection;ontology-based;clustering task;classification task;document representation;document set
kdd-2009 conference;data sources;domain-specific;iterative algorithm;data mining;vector based;supervised learning;training corpus
power-law distribution;large number of;index size;labeled dataset
web corpus;similarity joins;web applications;candidate pairs;text data
classification;naïve bayes classifier;semi-supervised learning;naïve bayes;text classification;unlabeled documents;labeled documents;training and test data;text classification;class labels;transfer learning;classifier
location traces;large-scale;outlier detection;detection method;real-world;early stage;false alarm
clustering;online social networks;classification;learning algorithm;multi task learning;multi-task learning;prior information;real-world data sets
click data;finding task;semantic representation;feature-based;personal information
factors affecting;ranked list;real-world;product review;prediction quality;baseline methods
search results;semantic knowledge;competitive performance;search result;regression model;web search results;search engine;support vector;clustering algorithm
singular value decomposition;classification;document classification;feature selection method;classification accuracy;svd-based;document-term;algorithm called;document matrix;accuracy compared to;dimension reduction;feature vectors;inter-class
ranking process;extracted information;multi-document;multi-document summarization;manifold ranking;manifold ranking;topic relevance
random sample;high accuracy
topic-aware;highly relevant;benchmark data sets;multi-document summarization;probability model
text mining;taking into account;context modeling;language modeling approach;text streams;bursty features;context models;language models;term frequency;bursty features
rule-based
clustering;point-based;temporal information;video content;principal component analysis;low-dimensional;dimensionality reduction
text collections;dynamic content;user studies
data point;singular value decomposition;high computational complexity;million images;massive data;clustering techniques;web-scale;matrix factorization;matrix factorization;instances;matrix factorization methods
text retrieval;taking into account;historical information;pose queries;real data;association rule mining;natural language processing;blog posts;web-pages;association rules;query translation
quality metrics;textual features;high quality;recommendation algorithms;target object;heuristic function;final ranking;information quality
takes into account;detection techniques;machine-learning algorithms;manually labeled;machine learning
clustering;feature space;unsupervised classification;quality metrics;classification quality;classification;feature subspace;unsupervised classification;feature subspace;distance-based;quality metric;directed graph;graph representation
recommender systems;recommender systems
web portals;web pages;knowledge bases;hand-crafted;deep web;domain-specific;information retrieval tasks;wikipedia categories;search engines;automatically generated;knowledge base
wikipedia-based;clustering;concept space;concept-space;information retrieval tasks;data mining;concept-based representation;semantic relatedness
share information;social network;data set;control strategies;social networks;diffusion process;social networking
context information;domain-independent;instance;neighborhood graphs;rdf graphs;instances;semantic web;domain-independent;distance-based
review text;additional information;opinion mining;real life;customer reviews
classification model;active learning;active learning;data set;instances;real-world data sets;objective function
sentiment analysis;supervised learning;document representations;document length;machine learning
clustering algorithms;ad-hoc;effectively identify;salient features;clustering techniques;computational cost;highly-dynamic;clustering approach;automatically identifying
supervised learning;classification accuracy;information-theoretic;random walk;fine-grained;specific) topics;specific topics
semi-) automatic;knowledge bases;textual data;knowledge base;graph-based;databases;semi-) automatically;case study;interactive visualization;semantic relations;information extraction;information extraction system
search results;rank documents;topic models;generative models;query log;query terms
algorithm finds;user browsing;hierarchical structure;user studies
boosting framework;partial information;heterogeneous sources;real-world;heterogeneous data sources;information from multiple sources;biological datasets;ensemble method;data integration;class prediction
clustering;social systems;background information;real-world;short texts;data items;data collected from;instances
learning approaches;cost-sensitive learning;learning algorithms;cost-sensitive;real-world
benchmark data;nearest-neighbors;data collection;large volume;large scale;data streams;static databases;data stream;data structures;similarity search;stream data;missing data;data volumes;databases;missing data;high-level;missing values
social media
clustering algorithms;latent dirichlet allocation;large scale;web sites;semantic model;data collected from;topic detection;mobile phone;natural language processing;text messages
problem setting;real-world;feature-based;event detection;manually annotated;detection methods;real world
web pages;keyword extraction;pattern based;selection method;naive bayes classifier;web page;classifier;page content
clustering;unlabeled data;mixture model;classification;large-scale;large scale;semi-supervised learning;learning algorithm;unlabeled samples;information retrieval;real dataset;gaussian mixture model;graph-based;label propagation;small size
regularization;classification model;regularization;prior knowledge;optimization algorithms;instance;microarray analysis;feature selection;logistic regression;learning problems;sample sizes;real world;high dimensionality
regression trees;1;classification;large scale;gradient boosting;machine learning;high efficiency;data sets;2;high accuracy
sequential nature;census data;classification;united states;natural language processing;machine learning algorithms
archived data;data warehouses;social network analysis;business analysts;data stored in;data access;business intelligence applications;structured data;data management
social media;ranking results;aggregation techniques;similarity measure;clustering method;real data;user query;content similarity;final ranking;similarity measures;query dependent
domain experts;data compression;database schema;knowledge management;domain knowledge into;data access;database engine;text processing;data storage
search results;search result quality;margin;multi-objective;svm-based
discounted cumulative gain;combining multiple;search applications;quadratic programming problem;web search results;search engine;multiple domains;rank order;learning method
efficient query evaluation;accurate ranking;query answers;ir-style;entity-relationship;ranking model
faceted search;query-dependent;search result
domain knowledge;automatically discovering;multiple documents;interactive visualization;assists users;text mining techniques
wikipedia articles;users' interests;user defined;event detection
tf*idf;ranking scheme;xml data;search strategy;user query;keyword search over;search engine
personalized services;classifier;mobile devices;usage data
text classification;machine learning algorithms;hierarchical structure;classification methods;classification
search engines;large amounts of;structured documents;real-world;source code;general-purpose;information retrieval;xml documents;test collections;structured information;web search engines
digital content;information management;social media sites
large-scale;large corpora;data flows;query performance;xml documents;xml content
large volumes of data;query loads;cost-effective;software components;multidimensional data;data-warehousing
rdf data;search space;taking into account;maintenance cost;query workload;query response time;query evaluation;space constraints;completeness;semantic information;numerous applications
matching algorithms;vertex set;web service;graph structures;emerging applications;service-oriented;structured information;graph data;graph matching;web-server
information contained in
privacy-aware;access control mechanism;access control;query language;sensitive data;fine-grained;personal data
index structure;multi-column;pattern mining;patterns mined;query logs;unstructured data;high-utility
personalized services;data extraction;data acquisition;data integration;web data
execution environment;execution environment;prototype systems;data streams;data stream applications;data elements;emerging applications;real-world
application domain;user queries;document collection;document clusters;online databases
social network analysis;web-based;databases;social networks;automatically extracted from
digital libraries;large collections of;information retrieval
on-line analytical processing;data warehousing;query processing
knowledge management;natural language processing;information retrieval;goal-directed;information access;databases;semantic annotations
patent retrieval;international workshop on;knowledge management;domain specific;information retrieval;information retrieval;international workshop on
knowledge management;information retrieval;wide range;databases
user-generated;international workshop on
user preferences;learned knowledge;real-world;data mining tasks;optimization problem;single snapshot;data set;data sets;gradient boosting;accurate predictions;positive class;single snapshot;positive class
clustering;large numbers of;large-scale;private information;fault-tolerance;computational complexity;privacy-preserving;data mining;user-centric;privacy-preserving data mining
classification;classification systems;data model;compact representation;data sets;specifically tailored;high dimensionality;pre-processing;separability
large networks;social network analysis;graph partitioning;mobile phone;case study
markov random field;biological entities;data collections;context-sensitive;edit distance;token-based;entity matching;biological data
database systems;retrieval accuracy;aggregation functions;query types;image databases;retrieval model;query processing;region-based;ranking model;nearest neighbors;query processing
real-life datasets;synthetic datasets;cluster structures;distance matrix
web documents;web pages;high-recall;extraction process;web sites;mining process;query generation;extract information from;total number of;web mining;web sources;relevant information;service descriptions;application scenarios
web documents;fixed length;large-scale;string data;database size;high degree;edit distance;input/output;databases;short strings;log data;finding similar;feature vectors
index structure;object databases;mobile computing;mobile devices;buffer management;wireless networks;continuous queries
algorithm performs;streaming algorithms;streaming applications;memory usage;query response time;xml applications;xml data;xml streams;tree-pattern queries;memory space;xml documents;incoming data;algorithm called;complex queries;evaluation strategy;evaluating queries;operator;tree-pattern;query languages;memory consumption
data structure;fundamental problem;main memory;text-based;data structures;search engines;hash table;databases;incur high
ranking queries;tree based;database;building block;indexing structures;large data sets;block size;indexing scheme;query performance;data set;instance;temporal data;optimal number of
growing number of;data analysis;spatio-temporal databases;large sets of;query points;data structures;mobile applications;moving object;tree index;fundamental problem;moving objects;nearest neighbor search;margin;refinement step;nearest neighbors;nearest neighbor search
desktop search;retrieval performance;index structures
execution plans for;query execution;query evaluation;vertically partitioned;plans;database;evaluation strategies;execution plan;cost model;xml databases;relational systems;native xml;execution plans;xml database;database instance
search space;data analysis;data uncertainty;uncertain data;real-world applications;generic framework;query processing;nearest neighbor query;filtering technique
past queries;naïve;sql queries;user types;context-aware;large-scale datasets;sloan digital sky;database;query logs;average precision
query answers;np-complete;conjunctive query;conjunctive queries
classification problem;pattern recognition;classification;classification;multimedia retrieval;application scenarios;content based image retrieval;instance;similar characteristics;pattern analysis;unsupervised techniques;multimedia data;moving targets;classification algorithm
classification;data mining;microarray data analysis;data mining research;information science;similarity-based;bioinformatics research
base classifiers;decision tree;causal discovery;ensemble classifiers;naive bayes;multilayer perceptron;machine learning;feature selection algorithm;data mining systems;classifier performance;support vector machines;high dimensional feature space;feature selection
clustering;clustering algorithms;evaluation techniques;clustering algorithms;path length;data sets;clustering techniques;social networks;graph based;network topologies;clustering algorithm;real world;network data;real world data sets
dimensional space;distance metrics;genetic algorithm;information theory;data clustering;optimization problem;metric distance;evolutionary computation;clustering algorithm;optimization techniques
data bases;classification;data exploration;temporal sequence;conceptual clustering;medical diagnosis;clustering methods;clustering algorithm;image classification;objective function;hierarchical clustering methods
matching algorithm;variable selection;instances;classifier;support vectors;similarity measures
mining results;association mining;association mining;analyzing data;data mining techniques;market basket data;association rules;data mining;transaction data;market basket analysis
clustering;agent based;data mining tasks;data mining;multi-agent;data mining;multi agent;clustering algorithm;agent-based
global model;local models;application domains;distributed environments;support vector;distributed data streams;traffic analysis
clustering;discriminant analysis;high quality;unsupervised learning;ensemble method;discriminant analysis;problems arise;clustering scheme;data sets;clustering algorithm;ensemble technique
traditional databases;valuable knowledge;data warehouses;association rule mining;large databases;mining frequent itemsets;memory usage;data stream;data streams;graph structure
artificial neural networks;learning algorithm;image compression;neural networks;feature spaces
time series;recognition rate
algorithm finds;image pixels;processing algorithms;pattern based;manufacturing process;visual saliency;information contained in
image sets;classification;classification accuracy;classifier performance;time series;image data;histogram based;image classification;image classification;image enhancement
digital images
text classifier;classification accuracy;indexing approach;textual features;key words;text) classification;language-independent;feature selection;text classification;feature selection
clustering;low cost;meaningful information;efficient access to
ct images;relevance scores;classification;jensen-shannon divergence;redundant features;feature selection
desired properties;production line;joint model;product design;regression model;lower cost
multi-core;real-world;application server;frequent sequences;mining algorithm;application server
clustering;hybrid approach;customer base;decision trees;data mining techniques;data warehouses;hidden information;data mining technologies;data stored in;databases;increasing amounts of;marketing strategies;decision making
real-world;data mining system;data mining system
support vector machines;support vector machines;game theory;data mining;business opportunities;game theory
hybrid approach;classification model;vector machine;independent component analysis;classification method;process control;input variables;independent components
tuning parameters;historical data;large databases;data set;decision makers;welding process;relative accuracy;pattern extraction
clustering;takes into account
pattern-based;rule-based;production data;local maximum
attribute values;data loss;false alarm;single attribute;time series;storage systems;prediction models;detection rate
data flows;real data;change detection;environmental conditions;health monitoring;meaningful information
medical datasets;frequent patterns;database;temporal patterns;pattern mining;mining process;data set;data warehousing;user defined;frequent pattern;frequent pattern mining
medical diagnosis;high risk;piecewise linear
medical datasets;decision rules;database;rule set;learning models
data-driven;regression trees;spatial data;neural networks;cross-validation;learning approaches;regression models;data set;data records;regression model;regression models;support vector machines;data mining problems;prediction task;small-scale
occurrence frequency;database;large social networks;social network;social network data;social networks;data base;frequent pattern mining
user profiles;support vector machines;naive bayes
social media;social media;link structure;knowledge discovery;domain-specific;topic detection;high degree of;search services;tedious task;trend analysis
database;process mining;business process;hidden markov models;formal concept analysis;process models;breast cancer
web pages;user's interests;term-frequency;document frequency;web-browsers;web browser;data-mining techniques;neural network
detection problem;web-site;boundary detection;clustering techniques;wide range;feature selection;web-site;information retrieval;clustering approach;web-page
auc;classifier performance;credit card;error rate;classifier;model building
classification performance;unlabeled data;data set;supervised approach;data sets;labeled data;semi-supervised;support vector machines;training algorithm
open source;neural networks;classification;regression tasks;real-world;data mining;image analysis;data mining;support vector machines;neural networks
instance selection;data-warehouses;finer-grained;relevant data;raw data;processing cost;information quality
intrusion detection systems;label information;semi-supervised learning;large number of;learning mechanism;semi-supervised learning;false alarm;false alarms;semi-supervised;supervised learning;labeled data;detection rate
databases;mining framework;data transformation;structured objects;vector machine;shape representation;shape features;feature representation;body-parts;learning performance;low-level
text-mining techniques;occur frequently;semantic graphs;knowledge base
pattern mining;rates;sequential pattern mining;monitoring systems;database
intrusion detection;intrusion detection systems;feature set;input features;detection accuracy;detection process;large number of;decision model;test data
service provider;location based services;database;4;relational databases;location data;location based services;end user;mobile users
multiple criteria;large-scale;complex relationships;observed data;data set;instances;data sets;data mining;multi-criteria;cluster analysis;clustering approach
amazon mechanical turk;diverse set of;image annotation;image annotations;human computation
climate data;long-range;spatial relationships;complex networks;detection algorithm;cross correlation
predictive models;biological networks;data describing;graph identification;inter-dependencies;explicitly models;social networks;missing information;incomplete data;communication networks
sequential nature;feature selection techniques;classification;semi-supervised learning;application domains;feature vectors;classification problem;sequence classification;information retrieval;classification task;sequence classification
cross-validation;auc;classification performance;machine learning;cross-validation;classifier performance;text classification
web objects;social tagging;social tagging;collaborative tagging
knowledge discovery;high-quality;selection process;knowledge discovery
classification;semi-supervised learning;network classification;structural characteristics;specific set of;classification performance;specific task;statistical relational learning;real-world data sets;class labels
expected number of;online social networks;network connectivity;community detection;optimization methods;network data;community detection
phase transitions;case study;large social networks;high-level;communication networks
dynamic networks;clustering coefficient;social networks;social interactions
link analysis;product recommendation;random variables;network model;hidden variables;complex networks;relational models;random variable;social network;user preference;link prediction;social network data;relational structure;statistical relational learning;community detection;relational learning;cluster analysis;network applications;mixture models;social networks
prediction accuracy;precision-recall;structural features;real-world;predictive power;social network;social networks;link prediction;prediction models;social networks
information-theoretic;sparse graphs;information-theoretic;community detection;artificial data sets;compression-based;information theoretic;community structure
clustering;query rewriting;closely related;open source;classification;data integration systems;databases;pattern mining;frequent itemset mining;itemset mining;data mining tools;pattern mining algorithms;data mining;relational databases;data mining problems;artificial intelligence
learning approaches;imbalanced datasets;imbalanced datasets;skewed;rare class;data complexity;class distribution
classification quality;classification;real-world;real-world situations;imbalanced data;supervised learning
information-retrieval;multilabel classification;application domain;text categorization;text categorization;classification performance;training sets;binary classifier;text-categorization;multi-label
distance measure;classification;base classifiers;data mining applications;data streams;streaming data;streaming data;fundamental problem;imbalanced data
association rule mining;association rule mining;instance;support threshold
auc;decision tree;numerical values;performance degradation;categorical variables;logistic regression;data mining
correlation-based;classification;classification accuracy;feature selection;feature selection methods;classification algorithms;correlation based;feature selection;information gain
high precision;search space;high level;natural language text;kernel based;machine learning techniques;vector machine;mining process;intelligent systems;kernel method;manually annotated;svm) based;sheer volume of;databases;automatically discover;classification quality
training data;extraction algorithm;translation probabilities
times higher;automatic extraction of;candidate pairs;search engine;automatic extraction of;candidate generation;fold cross validation
data stream;time series;streaming data;global constraint
decision tree algorithms;decision tree algorithms;requires minimal
data mining and knowledge discovery;classification;real-world;computing environment;classification performance;learning framework
human effort;active learning;supervision;text classifier;accurate classifiers;classifier
finite sample;regularization;vector valued;theoretical properties;vector field;kernel matrix;special case;reproducing kernel hilbert space;iterative algorithms;kernel methods
tree)-edit distance;dynamic programming;dimensional space;classification accuracy;tf-idf;subgraph isomorphism;information retrieval;edit distance;weighting scheme;structural information;image classification;real-valued;classification task
concise representation;association rule mining;mining association rules;data mining research;rule sets;association rule mining;association rules;uci datasets
spectral clustering;euclidean distances;weighted graphs;graph clustering;higher-dimensional
state space;dynamical systems;particle filters;probability distribution over;hidden states;adaptive sampling;particle filtering;dynamic bayesian network;real world;dynamic bayesian networks;hidden variables
evolving data streams;real-world datasets;ensemble methods
clustering;data set;cluster structure;data compression;molecular biology;information-theoretic;scientific domains;probability density function;hierarchical clustering;statistical model;minimum description length;hierarchical clustering algorithm;parameter settings;compression rate;objective function;hierarchical clustering methods;clustering method
users' interests;valuable knowledge;query-log analysis;graph-mining;implicit feedback;search engines;query logs;graph mining;semantic information;query log
formal models;process models;process mining;process mining;information systems
dynamic programming;data samples;computation cost;reinforcement learning;sampling method;markov decision processes;ad-hoc;posterior distribution;state-action
partial orders;completeness;machine learning
predictive distribution;domain adaptation;nearest neighbor;19;predictive distribution;predictive distribution;regularization;source domains;multi-domain;target domain;labeled data;labeled data from;unlabeled data;multiple sources;negative transfer;classifier
rank aggregation;optimization problems;np-hard
order statistics;structured domains;kernel-based;classification;probability distributions;human action recognition;reproducing kernel hilbert space;human actions
vice versa;classification;single class;large number of;theoretical results;multi-label classification;instance;risk minimization;multi-label;loss functions;classifier
clustering;clustering;moving object;edit distance;clustering performance;kernel k-means;clustering algorithm;compression method;positive effect
function approximation;reinforcement learning;fitness
data source;pascal 2007 voc;classification performance;learning theory;fisher discriminant analysis;multiple sources
learning algorithms;prediction methods;learning algorithms;large number of;real-world domains;large networks;citation networks;biological networks;link prediction problem;link prediction
dimensionality reduction methods;low-dimensional space;principal components;dimensionality reduction;principal component analysis
learning process;generalized queries;active learning;specific queries;real-world situations;multiple-instance learning
multi-modal;social interactions;temporal patterns;social network;social relations;social networks;multiple relations;evolving networks;social networking;real datasets
clustering;meaningful clusters;document collection;real-world datasets;pair-wise;supervision;clustering performance;semi-supervised clustering;data items;clustering framework;data mining;dimensional data;cluster-level;algorithms typically;instance level
frequent subgraph mining;domain-specific;feature selection;data mining;graph mining;data mining problems
decision trees;knowledge bases;tree-induction;tree nodes;description logics;semantic web;description logic;ontology languages
data points;classification;classification;low rank;learning algorithm;benchmark data sets;large data sets;tensor product;classifier
high-dimensional;mutual information;unknown environments;model-free reinforcement learning;reinforcement learning;machine learning;selection criterion;feature selection;mutual information between;state spaces;high dimensionality;decision making;feature selection
real datasets;microarray data analysis;microarray data
gene expression data;scale-free;large number of;topological structure;selection methods;feature selection methods;reconstruction algorithm
bayesian model;computational complexity;expectation propagation;multi-task;instances;learning approaches;bayesian inference;prior distribution;probability distribution;training data;feature selection;posterior distribution;approximate inference;expectation propagation;relevant features
multi-view learning
search results;content features;movement data;feature space;relevance feedback;feature spaces;multiple kernel learning;feature extraction methods;content-based image retrieval
classification problem;heterogeneous information network;label information;classification;heterogeneous networks;networked data;graph-based;optimization problem;data set;link structure;knowledge extraction;regularization framework;multiple types of;network structure;classification accuracy;real world;labeled and unlabeled data;heterogeneous information networks;information networks
effective tools;real world;spatial-temporal;markov network;conditional random fields;markov network;markov random fields;learning models;loopy belief propagation;maximum margin;hidden states;maximum margin;integer programming
sample complexity;continuous domains;reinforcement learning;highly accurate;function approximation;gaussian processes;specifically designed to;theoretical analysis;benchmark domains
real-world datasets;automatically extracted;low accuracy;belief propagation;amazon mechanical turk;semantic-web;inference rules;user feedback;1;step forward;entity-relationship;knowledge base;probabilistic graphical models;knowledge bases
high-dimensional feature space;training data;high-dimensional space;semidefinite programming;classification;linear transformation;linear dimensionality reduction;instance;sample points;dimensionality reduction
pattern-based;search space;classification;pattern-based;mined patterns;mathematical programming;feature values;discriminative patterns;numerical features;pattern mining algorithms
ranking approaches;latent variables;classification;prediction performance;random fields;underlying assumption;instance;explicitly models;random fields;sequence classification;classification problems;conditional random fields
complexity bound;regularization;generalization error;optimization algorithms;risk minimization;optimization criterion;multiple kernel learning
multi-agent systems;learning agents;multi-agent;learning algorithm;parameter settings;reinforcement learning;learning theory;agent systems;normal form;game theory
hierarchical structure
social networks;np-hard
text mining;multi-level;kernel framework;relation extraction;string kernel;annotated data;unified framework;unlabeled data;semi-supervised;string kernel;linguistic features
taking into account;takes into account;prior knowledge;learning algorithms;knowledge-based;prior knowledge;online learning;accurate predictions;margins;support vector machines
numerical results;learning algorithms;margin
markov decision processes;instance;reinforcement learning;relational domains
search paradigm;high-quality;user-friendly;complete search;search tool
semantic classes;classification;free-form;human computation;social tagging;topic modeling;data collected;machine learning approaches
association rules;quality measure;statistical analysis;interestingness measures
minimum description length;training data;structured domains;candidate models;high quality;total number of;large space of;selection techniques;model parameters;objective function
markov logic networks;data obtained from;collective classification;traffic management;statistical relational learning;supervised learning
distance measure;density-based;instances;distance-based;detection methods;distance-based
large-sample;nearest;unlabeled data;parameter estimation;semi-supervised learning;classification scheme;small sample sizes;counterpart;high-dimensional feature spaces;relevant information;semi-supervised;biomedical applications;classifier
search space;neural networks;online learning;online learning;learning problems;reward functions
clustering;order statistics;parameter-free;high-quality;binary data;database;distance measure;minimum description length;frequent itemsets;correlated attributes
feature space;classification;dynamic nature of;classification techniques;data points;classification;concept-evolution;data stream classification;data streams;text data;stream classification techniques;concept-evolution;incremental learning;data stream;data streams;data stream classification
data-driven;singular value decomposition;graph mining;pattern mining;pattern mining;subgraph features;data preprocessing;graph data;latent structure
probabilistic models;bayesian network;statistical relational learning;random variables;probabilistic inference
supervised learning methods;classification problem;kernel-based;generalization performance;inverse reinforcement learning;state-space;computational cost
recommendation quality;computational complexity;large-scale;demand-driven;instance;tag-based;collaborative tagging;demand-driven
regularization;prediction performance;structured sparsity;optimization procedure;structured sparsity;machine learning algorithms;objective function
joint probability;markov logic networks;parameter learning
efficient algorithms to;data points;clustering methods;graph clustering;graph cut
itemset mining;graph mining;large databases;pattern mining;specialized algorithms;constraint programming;mining tasks
efficient inference;modeling framework;personalized recommendation;meta-data;topic models;document-term;topic modeling;dyadic data;topic modeling;latent variable models;learning method;transactional data
data objects;domain knowledge;computational complexity;loss function;social network analysis;real-world;predictive power;target object;learning tasks;information retrieval;generalization performance;kernel framework;relational data
data sharing;streaming data;data quality;decision makers;multi-sensor;produce accurate;data integration
clustering;allowing users to;numerical attributes;hierarchical agglomerative clustering;data anonymization;information loss
real-world datasets;personal information;information loss
constraint satisfaction problem;sensitive information;constraint satisfaction;production data;databases;anonymized data;constraint satisfaction problems
span multiple;publish/subscribe systems;service-oriented;event-based;lessons learned;event driven;privacy preserving;information exchange;health information
join operations;database;sensitive data;outsourced data;instances;service providers;data privacy;data items;relational database;join predicates;data management
database
information systems;security constraints;data warehouses;historical information;sensitive information;on-line analytical processing;case study;olap operations;highly sensitive;security issues
privacy policies;online services;composite service;composite services;helps users;privacy preferences
graph theory
tree based;private data;differentially private;individual privacy;data analysis;privacy preserving
planning algorithm;computationally intractable;state space;reinforcement learning problems;partially observable markov decision processes;graph based;kullback-leibler divergence;plans
mobility patterns;sampling algorithm;data mining;vector representation;databases;similarity-based
binary classification;clustering;frequent patterns;knowledge discovery;pattern mining;scoring functions;instances;locally optimal;scoring function
conditional likelihood;baum-welch;synthetic data;classification;gaussian mixture models;margin;remote sensing;large margin;1;bayesian classifiers;support vector machines;learning framework;neural networks
decision trees;generalization error;majority voting;basis functions;voting scheme;post-processing;instances;ensemble classifier;support vector machines;machine learning and data mining
structured output;multiple instance learning;loss function;linear-chain;large margin;latent variable models;support vector machines;margin;conditional random fields
simulation studies;5;evolving networks;propagation model;real data
instance;aggregation function;pairwise classification;benchmark data sets;probabilistic approach
large networks;approximation methods;link structure;learning algorithm;compact representation;space constraints;linear algebra;link prediction;label propagation;semi-supervised;dynamic graphs
explicitly model;relation extraction;knowledge bases;related entities;supervision;error reduction;knowledge base
bayesian network;bayesian network structure;applications including;statistical test;structure learning;drug design;sample sizes;target variable;feature selection;constraint-based;real-world data sets;dimensionality reduction;combines ideas from
parameter values;information diffusion;real networks;predictive accuracy;learning algorithm;selection method;diffusion model;social networks;observation data;model selection;model parameters
selection problem;false positive;markov network;regularization;markov networks;1;instance;real-life;reconstruction error;7;greedy algorithm;network structure;regularization parameter
frequent subgraph mining;graph database;frequent subgraph;sufficiently large;computationally expensive;clustering algorithm;clustering approach;graph clustering;real world data sets
optimal parameters;svm algorithm;linear model;cutting-plane algorithm;large-scale;real-world applications;text classification;tree kernels;training instances;large datasets;natural language processing;support vector;tree kernel;question answering;natural language
outlier detection;data set;real world
competing methods;large margin;instance;graph laplacian;margin;classification algorithm
multi-class classification;learning algorithm;classification;version spaces;hypothesis space;version spaces;test instance;instances;noisy data;6;version-space;classifier
problem instances;complexity bounds;learning algorithm;unlabeled data;classification;active learning;upper bounds;active learning;machine learning;1;instances;active learning algorithms
clustering;clustering;regularization;target domain;pairwise constraints;source domains;data sets;geometric structure;clustering methods;semi-supervised;transfer learning;semi-supervised
structural learning;graphical models;bayesian network learning;sample-size;variable selection;conditional independence;constraint-based;categorical data;small sample sizes;multiple datasets;bayesian networks;semi-parametric;statistical tests;underlying structure;bayesian network;monte-carlo;learning problems;level α set
feature space;kernel-based;classification;optimization procedure;basis vectors;classifier;internal nodes
individual users;surprising patterns;skewed;mobile phone;power-law;mobile users
exploratory analysis;artificial data;generative models;bayesian inference;noisy data;canonical correlation analysis;variational bayesian
formal concept analysis;search strategy;pattern mining;high-order;case study;formal concept analysis;data mining
latent dirichlet allocation;gaussian process;language model;share information;topic model;topic models;topic models;topic distributions
em algorithm;gaussian mixture model;multi-task learning;classification;real-world;shared information;time series;data sets;gaussian processes;em algorithm;multi-task learning;special case;learning performance;synthetic data
clustering;consensus clustering;nonparametric bayesian;clustering ensembles;ensemble methods;bayesian inference;inference methods;collapsed gibbs sampling;nonparametric bayesian
real data sets;classification problem;semi-supervised;link structures;directed graph;high-order;directed graphs;data mining and machine learning;learning problem;multi-label;real world applications;classification problems;directed graph
learned models;state-transition;monte-carlo;observed data;global optimization;reinforcement learning;high cost;benchmark domains;model free;domain models
sparse learning;regularization;linear model;low accuracy;loss function;massive data;higher degree of;gradient descent;update rule;error rate;large margin classifiers;generalization error;data distribution
computational efficiency;13;training data;preference learning;gaussian processes;active learning;relevance score;implicit feedback;incremental update;gaussian processes;learning problem;link-based;web search;information retrieval;feature-vector;learning method
graph-based;optimization problem;machine learning applications;object representation;graph matching;benchmark datasets;matching problem
data set;monte carlo;loss function;upper bounds;linear regression;generalized linear models;data sets;wide range;real world;markov chain
source-domain;cross validation;target-domain;conditional distributions;cross validation;text categorization;cross-domain;conditional distribution;learning performance;cross-validation;cross validation;source-domains;real-world applications;transfer learning
local optimization;training data;predictive accuracy;frequent patterns;equivalence classes;activity-relationships;training set;large sets of;selection techniques;training instances;sufficient condition for;pattern set;pattern set mining
document summarization;semantic graph;information extracted from;common sense;textual documents;question answering
context information;user context;taking into account;popular items;real-world
graph structures;graph mining;existing graph;open source data mining;interactive visualization;interactive exploration
singular value decomposition;document summarization;open-source;summarization task;summarization method;text analysis
retrieval effectiveness;query expansion;named entities;automatically determines;named entity;query expansion;query expansion
regression model;ground truth;rates;web interface
designed specifically for;integrated environment;dimensionality reduction techniques;knowledge discovery;data mining process;dimensional data;data mining algorithms;dimensionality reduction;database management systems
open source;outlier ranking;outlier mining;easily extensible;knowledge extraction;analysis task;ranking methods
text documents;semantic similarity;confidence values;web resources;knowledge-driven;text analysis;main components;knowledge-driven
long-term;statistical properties;detection method;suffix tree;event detection
news search;data collections
communication patterns;privacy concerns;higher accuracy;information gathering;simulation results;agent-based
user queries;large-scale;topic areas;intelligent agent;assists users
multi-agent;agent communication;decision making;problem solving;trade-offs
finite-state
automatically generates;planning algorithm;domain-independent;plans;information sources;legacy systems;user query;information gathering;relational databases;integrate data from
world-wide web;large numbers of;information sources;query-answering;fine-grained;query-answering;information gathering;query planning
domain knowledge;hill-climbing;knowledge-based;keyword-based;user's search;input text;computational complexity;information overload;knowledge-based systems
world wide web;user profile;learning algorithms;web sites;making predictions;information retrieval;rates;web page;machine learning;classifier;naive bayesian
multi-agent system;intelligent agents;input/output;unsupervised algorithm;learning models;optimal strategy
problem instances;problem solving;problem instance;application called;multi-agent;parametric-design;search process;search spaces;short term;long term;agent communication;search efficiency
machine-learning methods;data set;data analysis;learned knowledge;distributed processing;evaluation criteria;machine learning;rule-learning;data sets;search effort;increasingly popular
individual agents;plans;large-scale;multi-agent;real-world;dynamic environments;higher-level;constraint propagation;applications ranging from;constraint satisfaction problems
problem-solving;raw data;problem solving
theoretical framework;optimal path;optimization problem;theoretical results;instance;network routing;utility theory;network topology;communication networks
human behavior;intelligent agents;artificial intelligence;multi agent systems
game trees;game-tree;multi-agent;process-planning;application domains;search techniques;total-order;hierarchical task;diverse domains
complete information;agent architecture;individual privacy;knowledge acquisition;multiagent systems;decision making
general case;theoretical framework;evaluation function;sufficient condition for
future events;large numbers of;game theory
simulation results
multimedia presentation;cross-media;user interface;temporal constraints;multimedia objects
video game
large corpora;semantic analysis;semi-automatic
social interactions
satisfiability problem;instance;lower bound;constraint satisfaction problems;search tree
constraint satisfaction;constraint satisfaction;decision problems;incomplete knowledge
search methods;scheduling problems;search algorithm;lower bound;variable ordering;constraint optimization problems;constraint satisfaction;constraint optimization problems;poor quality
lower bound;constraint satisfaction problem;upper bounds;lower bounds;upper bound;inference process
filtering algorithms;space complexity;constraint programming;higher order;local consistency;data structures;algorithm called;path-consistency;filtering methods;constraint satisfaction problems;path-consistency
inverse consistency;cost effective;constraint satisfaction;inverse consistency;search effort;space requirement;highly relevant
real-life problems;0,1;filtering techniques;li, ci. [cardinality constraints;resource allocation;space complexity
real life problems;algorithms produce;low complexity;constraint satisfaction problems;randomly generated
desired properties;game-tree;memory constraints;game trees;search algorithm;pruning power
dependency analysis;theoretical results
search space;game trees;directed graph;graph properties;state space;game-tree search
domain knowledge;computational complexity;game trees;game-tree search;game tree;evaluation function
np-complete problem;phase transitions;combinatorial problems
phase transitions;random 3-sat;constraint satisfaction problems;artificial intelligence
problem instances;prediction accuracy;phase transition;problem instance;sat instances;satisfiability problem
instances;formal framework;highly structured
scheduling problems;search algorithms;stochastic sampling;greedy search;real-world;underlying assumption;search technique;stochastic sampling
search algorithms;state spaces
number partitioning;discrepancy search;search algorithm;depth-first search;tree search
state space;search algorithms;search effort;performance guarantees;goal-directed;average-case
search space;polynomial-space;constraint satisfaction problem;learning algorithm;additional constraints;learning algorithms;linear space;arbitrarily large;learning scheme;avoid redundant;constraint satisfaction problems
upper bounds;salient features;search algorithms
state space;search algorithms;evaluation functions;evaluation function
general purpose;constraint satisfaction;domain specific;search process;case study;inference methods;problem solving;easily extensible;constraint satisfaction problems
constraint satisfaction;randomly generated;hill-climbing;search space;constraint network
search methods;search space;local search;local minima

genetic programming;learning rate
algorithm finds;search algorithm for;graph-based;tree structure;propositional satisfiability;constraint satisfaction problems
local search algorithms;local search;optimization method;instances;wide range;key parameters;random 3sat;satisfiability testing
search techniques;np-complete;tabu search
local search;extreme values;large-scale;constraint satisfaction;local search techniques;instances;search procedure;random sat;constraint satisfaction problems
constraint networks;temporal information;transitive closure;interval-based;constraint reasoning;temporal reasoning;path consistency;artificial intelligence
answer queries;pre-processing;temporal reasoning;knowledge-based

temporal relations;sufficient condition for;path-consistency
accurate models;training examples;intelligent tutoring system;machine learning;classification task;knowledge base
multiple agents;common-sense;plans;based reasoning
domain knowledge;evaluation methodology;plans;large-scale;knowledge base;formal representation;domain experts;knowledge base;knowledge bases
problem-solving;learning environment;knowledge-based;learning environments
world wide web;natural language processing techniques;machine learning;user profiles;machine learning;information retrieval;user interaction
knowledge engineering;information systems;applications including;specific information;information organization;information sources;case studies;video retrieval;information retrieval systems
unknown word;combination function
large number of
retrieval strategies;instance-based;knowledge-based;knowledge-based;helping users;relevant information;information space;information spaces
domain-independent;knowledge acquisition;problem-solving;knowledge base;domain-dependent;knowledge-based systems
completeness;knowledge-based;planning tasks;real-life;knowledge acquisition;planning systems;artificial intelligence;knowledge-based systems
domain experts;knowledge bases;domain model;knowledge representation;knowledge base;natural language
object-oriented;rule-based;path-based;object-oriented;production rules;path-based;general problem;ai techniques
knowledge compilation;knowledge compilation;knowledge base
algorithm (called;knowledge base;knowledge bases
hybrid approach;knowledge bases;brute-force approach;knowledge compilation;davis-putnam;brute-force" approach;random 3sat;normal form
spatial aggregation;equivalence relation;data types;higher-level;problem domains;image analysis;neighborhood graph;low-level;spatial aggregation
operator;numerical simulation;plans;abstraction levels
state space;instance;search spaces;caching techniques;problem solving;search technique
broader range;hierarchical task;planning domains;search tree
estimation algorithms;human beings;factors affecting;artificial agents
description language;knowledge bases;domain descriptions;logic programming;belief update;search efficiency
common-sense;model checking;dynamic nature of;computational complexity;knowledge base
knowledge base;model-theoretic;knowledge bases;minimal change
representation language;probabilistic logic;inference algorithm;irrelevant information;knowledge bases
description logic;hierarchical structure;description logics;artificial intelligence;description logics
domain knowledge;large amounts of;verification problem;knowledge bases;domain experts;knowledge based;description logic;query containment;knowledge base
knowledge engineering;additional constraints;description logics;problem solving
completeness
join-tree;graph structure;graph-based;knowledge-base;tree structure;graph-based;simple algorithm;knowledge base
large-scale;logic-based;learning environments;storage overhead
np-complete;decision procedure for;automated reasoning
formal methods
situation calculus;situation calculus;temporal properties;nonmonotonic reasoning
logical reasoning;high-level;dimensional space;dynamic systems
logic programming;focus primarily on
fall short;completeness;situation calculus;logic programming;knowledge representation;specification language;event calculus
complex systems;model checking;complete information
action language;deterministic domains

question answering;case retrieval
hybrid method;hybrid approach;case-based reasoning;adaptation knowledge;rule-based
attribute values;data record;detection method;case-based reasoning;underlying assumption;data bases
source selection;assumption-based;problem domains;problem solving;source selection
training set;training examples;algorithm runs in;finding optimal;tree-structured;decision tree learning
learning algorithms;feature vector;decision tree;real-world;fixed-length;learning systems;vector representation;text categorization;instance;set-valued;learning problems;real numbers;feature vectors
instance;training set;decision trees;training phase;decision tree;test instances;learning algorithms;test instance;nearest-neighbor;caching scheme;missing values
decision trees;predictive accuracy;predictive power;learning systems;training instances;classifier
scientific discovery;medical research;inductive inference
numerical data;line segments;simple heuristics;data acquisition;plans
genetic algorithm;rule-based;convergence rate;mathematical formulation;mathematical model;incomplete knowledge;key parameters;modeling approach;optimization techniques;fuzzy logic
search systems;large scale;hidden structure;evaluation criteria;build models;operator
explanation-based learning;problem solving;search tree;constraint satisfaction;constraint satisfaction;explanation based learning;search strategies
explanation-based learning;learning mechanism;learned knowledge;learning systems;problem solving;problem-solving;control knowledge
domain knowledge;knowledge compilation;software architecture;dynamic environments;instance
worst-case;inductive learning;sample size;training data;learning problems
internal state;computationally hard;learning algorithm;hidden state;supervised learning;stochastic domains
building block;problem solving;genetic algorithm
training data;classification;training data is;learning algorithm;land cover;learning algorithms;class noise;predictive accuracy;instances;classification accuracies;training instances;remote sensing;noise levels;base-line;fold cross validation
learning algorithms;machine learning algorithms;induction algorithm;learning algorithm
regression trees;regression trees;predictive accuracy;numerical values;statistical method;real-world domains;algorithm called;inductive logic programming;statistical methods;machine learning algorithms
query optimization;database;decision support;knowledge discovery;databases;database integration;rule discovery;closed-world;discovered knowledge
data analysis;real-life applications;highly interactive;classification rules;data mining;existing knowledge
existing knowledge;knowledge integration;machine learning;knowledge engineers;knowledge integration;knowledge base
multi-strategy;planning algorithm;inductive learning;planning domains;partial-order;domain-specific
explanation-based learning;partial order;randomly-generated
problem solving;domain-independent;control knowledge;domain-dependent
context-dependent;automatically learning;domain model
desired behavior;high level;plans;low-level
generally applicable;reinforcement learning;reinforcement learning algorithm;requires solving;algorithms for computing;optimal policies;optimal control;minimum cost
counterpart;state space;real-time dynamic programming;reinforcement learning;learning method
genetic programming;agent behaviors;learning tasks;complex tasks;randomized algorithms;learning problem
maintenance costs;dynamic environments;semantic correspondences;data integration
skyline algorithms;skyline computation;performance gain
query evaluation;efficient query evaluation;xml data management;real-life;business processes;analysis tasks;13, 16, 17, 18;query languages
web-based;data analysis;web interfaces;data warehouses;main memory;large-scaled;business intelligence;decision support systems;business intelligence;traditional olap;decision making
acm sigmod;query engine
multi-level;matching algorithm;information sharing;hybrid approach;multi level;partial order;semi-automatic;similarity measures
human centered;spatial data;heterogeneous sources;semantic similarity;database queries;human generated;heterogeneous databases;ontology matching;semantic interoperability;knowledge base;semi-automatic;ontology mapping
semantic interoperability;data sources;scalable solution;high level
domain ontologies;information systems;multi-agent system;information sources;scalability issues;domain ontologies;information extracted from;heterogeneous databases;databases;semantic heterogeneity
information systems;database schema;domain ontology;database;enterprise-wide;data model;semantic web;schema evolution;natural language processing;database schemas
aggregate information;data warehouse;decision-making;on-line analytical processing;dimension hierarchies;case study;decision makers;data warehouses
hybrid approach;index structure;ontology language;case study;query processing;spatial index
discovered patterns;ontology-based;application domain;prior knowledge;interestingness measure;knowledge extraction;data mining process;expert knowledge;data-mining process
personal information management;database architecture;search engine;collected data;search tools;stored data;semantic relationships;semantic information;text analysis
user interfaces;typical patterns;human-centered;emotion recognition
building blocks for;high-level;low-level;operating systems
user interfaces;human behavior;human-centered
multi-stream;hidden markov model;emotion recognition;learning scheme;audio-visual;audio-visual
real world situations;database;human-centered;facial expressions;human-machine;video sequences;dynamic events
robot learning;facial expressions;reinforcement learning;strong evidence
information content;classification problem;auxiliary;database;human actions;human action recognition;human action;particle filtering;image sequences;longest common subsequence;salient points;particle filter;estimation algorithm;specifically tailored
signal processing;human centered;real life;human face;social sciences;multi-media
data corruption;multiple agents;sensor-network;classification;latent structure
social intelligence;takes place;social intelligence;social interaction;social intelligence
data-driven;real-life;provide feedback
emerging trends;emerging applications
decision making
web-based;ontology-based;knowledge bases;user queries;web service;semantic representations;multiple modalities;web service composition;web services;data structures;semantic web;semantic web services;xml-based;information services;knowledge base
high-level;specific information;database;single images;user interface;facial expressions;specific problem;human faces;reconstruction algorithm;user study
human users;1, 2;text-based;instance;user interface;high-level;natural language
spatial relationships;test image;markov random field;object recognition;efficient access to
rates;shape space;quantitative analysis
general case;pose estimation;motion capture;human body;synthetic and real images
object recognition;estimation techniques;estimation algorithm
computational efficiency;increasing complexity;discriminative features;image database;feature representation;object detection;error rate;classifier
multiple objects;unlabeled images;object recognition;unsupervised learning
markov random fields;motion tracking;video data;sequential monte carlo;human motion;accurate tracking;bayesian networks;human body;tracking framework
11;training examples;depends crucially on;11, 8, 5;face detection;database;detection rates;multiple objects;linear features;object detection;face detection;databases;multiple views;single object;local orientation;positive examples
shape representation;classification;random walk;rich information;aspect ratio;local orientation
face recognition;database size;recognition performance;closed sets;rates;special case
search engine;world wide web;web pages;web search;database;image-search;image-based;databases;mobile devices;general-purpose;text-based;image search;image set;images captured;allowing users to
object classes;object class;shape features;hierarchical clustering algorithm;object level;object classification
local structures;functions defined;shape features;scale-invariant;wide range;shape matching;object categories
object recognition;low-resolution;nearest neighbor;object instances;error rates;recognition task;image pairs;instances;natural scenes;lighting conditions;support vector machines
object recognition;appearance-based;genetic algorithm;recognition performance;test image;decision-making;large number of;image regions;integrating multiple;discriminative power;building block
high quality;automatically extract;classification
instance;object recognition;linear model;database;pruning methods;locality-sensitive hashing;shape descriptors;feature-based;higher order;image database;object identity;constraint based;indexing techniques;geometric constraints
tracking methods;object recognition;classification;contour based;image sequence;model parameters;object shape
reconstruction algorithms;low resolution;moving objects;high resolution;theoretical result;spatial resolution
vector field
span multiple;squared-error;additive noise;human face;spatio-temporal;graphical model;data structures;video database;facial expressions;high resolution;probabilistic inference;video data;high-resolution;feature vectors
lower bound;feature detection;prior knowledge;particle filtering;basic properties;em algorithm;image quality
ad-hoc;calibration method
probability models;high-quality;dynamic range;hidden variables;probability models;prior models;intensity values;input images
specific features;classification;specific context;classification systems;low resolution;wide range;object classification;video sequences;unlabelled data;classifier;object classification
illumination conditions;dynamic range;vision applications;mathematical formulation;appearance variations;computational model
dynamic range;vision algorithms;image acquisition;image sequence;multiple frames;control mechanisms
unsupervised learning;learning algorithm;multi-camera;theoretical justification;observation data;camera views
real-time tracking;training set;tracking algorithm;correlation-based;depth images

multiple views;takes into account;multiple view;shape model;input images;lighting conditions;reflectance model
vision applications;high quality;analytical models;rates;video frame;depth maps;depth estimation;applications requiring
classification problem;lower computational cost;classification;error rates;large number of;target object;pose estimation;classification method;local affine;ad hoc;computational burden;local descriptors
high-dimensional;supervised learning methods;high dimensional feature spaces;classification;feature selection method;classification accuracy;microarray datasets;feature selection;feature selection;numerical data;selection algorithm;information gain
face recognition;feature vector;high dimensional;multiple features;feature extraction technique;linear discriminant analysis;training set;face recognition system;training samples;random sampling;small sample size;discriminative information;null space;random sampling
compares favorably with;database systems;database;database applications;embedding methods;distance measures;pose estimation;euclidean space;efficient approximate;similarity measures;learning task;nearest neighbors;computationally expensive
detection problem;detection methods;sufficient conditions for;detection performance;cost function;learning task;fully-automatic
discriminative learning;distance metrics;classification;margin-based;pairwise constraints;video object;surveillance video;labeled data;sample points;decision boundary;learning framework;classification task
high-speed;high-speed;video sequence;rates
density estimation;vision systems;background modeling;detection performance;dynamic behavior;optical flow;kernel density estimation;dimensional space
clustering problem;dimensional space;data points;missing data;point correspondences;linear subspaces;motion segmentation
training data;training examples;video stream;manually labeled;moving objects;manual effort;labeled data;object detection;moving object detection;motion information;learning framework;classifier;detection task
recognition algorithm;human body;general setting
data-driven;state space;high dimensional;static images;solution space;test images;observation data;human body
sampling methods;consistency constraints;single view;sequential monte carlo;graphical model
equivalence class;metric space;shortest path;classification task;shape analysis
processing algorithms;image processing;graph-theoretic;small-world
natural scenes;success rate;image features;learning algorithm;strong classifier;statistical analysis;test set;natural images;weak classifiers;classifier
clustering;face recognition;svm-based;recognition performance;large number of;face databases;vector machine;recognition accuracy;database;vector machine;hierarchical agglomerative clustering;clustering method
matching algorithm;similarity scores;face recognition algorithms;logistic regression;statistical models;human face;classifier
video surveillance;specular reflections;long-range;spatio-temporal;region-based;highly dynamic;human detection;foreground detection;detection performance;unified framework;pre-processing
principal components analysis;probabilistic framework;classification problem;support vector machines;activity recognition;compact representation;person tracking;human body;stereo camera
appearance models;shape models;dynamic scenes
feature points;video sequence;synthetic data;iterative process;face model;scaling factors;face image;motion parameters;multiple frames;computational cost;video sequences
clustering;clustering algorithms;sampling techniques;objective functions;optimization problem;data clustering;meta-level;data partitions;real-world data sets;conflict resolution;clustering approach
globally optimal solution
clustering;feature space;energy function;feature vector;feature space;image space;graph cuts;clustering methods;major limitation;segmentation algorithm;image segmentation
tracking algorithms;probabilistic framework;sample-set;state spaces;tracking performance;probability distribution function
image data sets;manifold learning;detection method;local linear;spanning trees;noise reduction;building block for
partially supervised
complete model;markov random field;graphical models;additive noise;inference problem;binary attributes;simpler models;global optimality;graph matching;line segments;fully connected
singular value decomposition;face recognition;linear projection;linear projection;linear discriminant analysis;face recognition;10, 17, 11;2;principal component analysis;optimization criterion
algorithm takes;seismic data
image retrieval;search results;false positives;object-based;feature vector;database;image database;object-based;relevance feedback;bayesian classifier;bayesian approach;probability density;classifier;image domain
face images;1, 2;face recognition;3, 5, 6, 7;13, 14, 15;4, 9;16, 17;illumination conditions;light sources;unified framework
11;principal components analysis;object recognition;image registration;14;feature detection;image gradient;image retrieval;local image;image descriptors;local descriptors
magnetic resonance imaging;quadratic programming;deformable models;information derived from
high dimensional;probabilistic model;image space;image sequences;probabilistic approach;facial expression recognition;low dimensional space;posterior probability
information-theoretic;invariant features;training samples;recognition process;small samples;small sample size
instances;active appearance models;active appearance models;generative models
10;13;algorithms typically;random variables
semidefinite programming;similarity measure between;quadratic optimization;dense set of;integer programming;optimization framework
binary classification;training set;nearest neighbor;machine learning methods;classification;decision tree;worst-case;imbalanced data;instances;classifier;naive bayesian
face recognition;discriminative features;high dimensional;linear discriminant analysis;feature extraction technique;linear discriminant analysis;discriminant analysis;face space;small sample size;discriminative information;null space
feature space;facial images;gaussian mixture model;em algorithm;margins;natural images;image retrieval;unlabeled data;product space;boosting algorithms;large database;image retrieval;metric learning methods;distance functions;data points;learning paradigm;binary classifiers;distance function;database;training process;margin based;databases;margin;learning method
object classes;decision tree;voting scheme;large margin;recognition algorithm;recognition systems;real world;support vector machines;classifier
image retrieval;relevance feedback;database;svm based;component analysis;vector machine;relevance feedback;svm) based;svm-based;content-based image retrieval
clustering;data set;video segmentation;nearest neighbor;linear subspace;low rank;moving objects;pattern classification;image sequences;noisy data;subspace clustering;cluster structure;subspace clustering
high-resolution;color image;synthetic and real images
high-dimensional;time series;linear dynamical systems;visual tracking;probabilistic inference;human motion;efficient optimization
pattern recognition;face detection;linear combination;making decisions;imaging conditions;special case
classification scheme;bayesian network;image sets;classification;classification performance;scene classification;low-level vision
clustering;video segmentation;information-theoretic;selection criteria;parametric model;algorithm called;geometric structure;noisy data;model selection criterion
large space of;bayesian network;test set;face detection;structure learning;face detector;pre-computed;np complete;object detection;human eye;network structure;cost functions;high dimensionality;classifier;classification error;likelihood ratio
image retrieval;relevance feedback;training set;sampling techniques;sampling based;vector machine;relevance feedback;svm based;svm classifier;negative feedback;small size;content-based image retrieval
auxiliary;data points;vector field;level sets;level set;data sets;piecewise linear
data point;feature points;data points;similarity measurement;multiple subspaces;linear projection;inference method;dimensional data;motion segmentation
order statistics;face recognition;pattern analysis;independent component analysis;independent component analysis;wide range;density function;joint probability;kernel density estimation;independent components
image patch;graphical models;computationally intractable;regression function;large number of;image pixels;graphical model;powerful tools for;graphical models;belief propagation;approximate inference;image data
manifold learning;input space;view-based
curve evolution;boundary detection;sparse linear
probabilistic framework;real-world;markov random field;image databases;image data;global structure;image labeling;conditional random fields;classifier
recognition performance;gait recognition;ground truth;low-level;database
optimal matching;point-sets
feature space;discriminative power;local minima;feature subspace;convergence rate;boosting algorithm;shape model;prior distribution;optimization algorithms;facial features;feature selection;human face;strong classifier
feature vector;recognition performance;feature extraction;gaussian kernel;multi-scale;local image
multi-level;cut algorithm;high level;low level;1;motion segmentation;hierarchical representation;higher level;conditional probabilities;posterior probability
image pixels;graphical models;image segmentation;variational inference;bp algorithm;graphical model;hybrid method;simpler models;deformable model;deformable models;mrf model;inference problem
false positives;local image features;long range;spatial constraints;object recognition;recognition process;image matching;spatial constraints;transformation parameters;exemplar-based
speech recognition;multi-class;visual cues;multi-class boosting;automatic selection;hidden markov models;recognition systems;visual feature;informative features
sample complexity;common features;large number of;total number of;training data;object classes;object detection;multi-class boosting;features selected
image retrieval;large numbers of;classification;object recognition;feature selection;large amounts of data;databases;low complexity;natural images;information theoretic
gait recognition;recognition rate;gait recognition;monitoring applications;statistical analysis;motion estimation;image information;vision-based
recognition performance;gait recognition;appearance-based;database
local search;genetic algorithm;efficient search;complex objects;identification problem;combinatorial problems;geometric constraints
feature space;low-resolution;face representation;face detection;video sequences;svm classifier
face recognition;video sequence;database;probabilistic framework;object identity;single image;pose variations
instances;ground truth;motion analysis;detection performance
globally optimal;extracted features;activity models;transitive closure;real-life;euclidean space;video segments
vision systems;xml based;probabilistic reasoning;visual analysis;action recognition;database technologies;visual learning;action recognition
face representation;face recognition;training images;face recognition algorithms;database;correlation filters;pca based;principal component analysis
recognition performance;baseline algorithm;feature extraction;feature fusion;database
clustering;face images;face recognition;automatically extracted from;face image;obtained by applying;clustering procedure
face representation;gaussian mixture model;face verification;database;face detection;conditional distributions;density function;excellent performance
state space;probability density function;partially ordered;particle filter
description language;classification;application domain;real-world;dynamic nature of;dynamic bayesian network;general-purpose;prior knowledge;facial features;facial expressions;dynamic bayesian network;belief propagation
large numbers of;data association;data association;visual tracking;fundamental problem;moving targets
relevance vector machine;nonlinear regression;body parts;shape descriptor;single images;image sequences;human pose;motion capture data;image sequence;human body;test data
data set;expression recognition;classification results;classification
gait recognition;real-world;spatio-temporal;recognition rates;large database
multi-level;video sequence;recognition algorithm;spatial-temporal;information contained in;video based;face recognition;video database;subspace analysis;classification results;single image;processing speed
semantic content;visual content;classification;decision tree;post-processing;hidden markov model;video content;decision making
multi-view;instances;image sequences;moving objects;video sequences;dynamic scenes
linear classifier;classifier fusion;recognition rate;classification;database;spatio-temporal;training data set;video sequences;hidden markov models;facial expressions;data (108 sequences;optical flow;lighting conditions;classifier;fold cross validation
activity recognition;tracking objects
instances;input image;single image;edge information;bayesian approach;image set;images captured
real world;fundamental problem;image specific
11;segmentation problem;image feature;ground-truth;classification;database;learning approaches;large-scale;region-based;edge image;classifier trained on;classifier
point cloud;registration algorithm;large-scale;real data;pose estimation;sensor data
cut algorithm;level set;multiple frames;graph cut;automatically extract
multi-modal;particle filters;principal component analysis;particle filter
pattern recognition;semidefinite programming;unsupervised learning;high dimensional data sets;manifold learning;locally linear embedding;low dimensional;handwritten digits;dimensional data;dimensionality reduction
image retrieval;statistical information;posterior probabilities;database;image database;probabilistic model;semantic similarity;designed to support;shape features;content based image retrieval;region based;semantic concepts;region-based;concept discovery;expectation-maximization
training set;image feature;feature vector;model assumes;relevance models;real-valued;data set;joint probability distribution;relevance model;test set;image annotation;video annotation;feature vectors;kernel density
recognition performance;face recognition
appearance model;audio-visual;hidden markov model;emotion recognition;low level features;feature tracking;emotion recognition
utility function;decision theoretic;video data;prior knowledge;facial expressions;human players;control problem
video sequences;pca based;processing speed;shape model
data analysis;data feed;data warehouse;massive amounts of data;data sources;data collection;data warehousing;data stream management system;databases;highly scalable;data management system
large numbers of;high-end;time series;image data;image datasets;extract information from;image processing
high rate;history data;search engines;theoretical analysis;relevant information;web data
replication;database;database replication;databases;fault-tolerance;replication;database replication;high cost;distributed systems
data residing;security concerns;storage devices;sensitive information;optimizer;main memory;query optimization techniques;storage engine;query processing;encrypted data;aware query;query optimizers;relational databases;relational dbms
durability;database research;storage capacity;sensitive data;personal data;traditional database;data server;privacy risks;personal data
database;sql queries;information propagation;access control
databases;fine-grained;database;partitioning strategy;database replication;machine learning techniques;distributed databases;social network;workload-aware;replication;partitioning scheme;graph-based;test cases;distributed transactions
keyword queries;parallel computing;relational databases;intermediate results;real datasets;cost estimation;large number of;relational database;keyword query;keyword search
database systems;replication;replication;databases;concurrency control;distributed database;high availability
database instances;complex) relationships;target instance;source schemas;instance;similarity measure between;operator;automatically generate;schema mappings
8, 10, 14, 15;vice versa;database;structural properties;data exchange;database instance
theoretical foundation;data exchange;large databases;highly scalable;algorithm produces;functional dependencies;negative result;schema mappings;key constraints;data-exchange
higher degree of;search queries;interactive search;heuristic algorithms;real-world
energy management;cluster-level;energy management;energy efficiency
real-world datasets;running times;query forms;search engines;keyword search over;relational databases;keyword search;relational data
query execution;optimization techniques;xml processing;regular expressions;database;xml queries;query language;real-world applications;event streams;pattern matching
query evaluation;database size;database;data model;data instances;query operators;relational database system
data cleaning;fall short;master data
instance;query results;source data
sampling approach;data cleaning;user-defined;web data extraction;functional dependencies;functional dependency;data integration
clustering;distance measure;cost functions;basic operations;measure called;ad-hoc;real-world;edit distance;special case
regular-expression;schema-based;user-defined;streaming data;rates;pattern-detection;operator;change frequently;pattern matching
stream processing;stream processing systems;query results;window-based
high efficiency;point-based;large-scale;real-world applications;monitoring applications;partial order;wide range;event-based;formal semantics;increasingly popular
knowledge sharing;rdf databases;computational biology;data model;indexing method;real-life;multi-user;rates;serializability;concurrency control;data consistency;social communities
np-complete problem;graph pattern matching;performance guarantees;emerging applications;graph patterns;subgraph isomorphism;graph pattern matching;real-world networks
large graphs;directed graph;real-world graphs;reachability queries
data analysis;applications including;hadoop mapreduce;real datasets;large-scale data mining;graph analysis;data processing;data mining;model fitting;data-intensive;highly scalable;arise naturally in
counterpart;query evaluation;view definitions;conjunctive queries
database;database schema;query interfaces;completeness;databases
generic model;data sources;web-applications;query evaluation;web-services
multi-level;mapreduce-based;instances;ad-hoc;web-scale;data layout;aggregation queries over
shortest paths;query optimization;graph structures;indexing method;optimizer;graph-structured data;np-complete;large-scale;search space;subgraph isomorphism;query plan;large networks;index construction;query processing;graph queries;synthetic data sets;highly scalable;graph indexing
instance;vector space;feature vector;tight bound;real-valued;join algorithms;rank join
data objects;user preferences;database;real-life datasets;prohibitively expensive;candidate set;query type;databases;result set;processing queries
web objects;textual descriptions;spatial data;web services;spatial web objects;real-world;location-aware;location-aware;text retrieval;query keywords;query returns
clustering;compression methods;compression ratio;real datasets;time series;reconstruction error;motion capture data;linear complexity;sensor data;clustering methods;traffic data;mining tasks
index structure called;index structures;main memory;high efficiency;memory size;range queries;key-range;hard disk;data generated from;times faster than
feature space;multimedia databases;mathematical models;range queries;databases;dimensional data;query results;feature vectors;range queries
erroneous data;clustering problem;attribute values;fall short;record linkage;integrating data from;real-world;real world;data-management applications;clustering
data structure;explicitly represented;data cleaning;probabilistic databases;query processing in;answer queries;query answering;query conditions;data integration
matching score;data source;computational efficiency;record linkage;real data;behavior patterns;matching method;candidate generation
existing indexes;graph database;database;np-complete;indexing techniques;subgraph isomorphism;storage engine;growing importance;chemical compounds;large graph;complex structures;disk-based;real datasets;indexing methods;code base
database applications;database researchers;computing resources;collected data
open source;cost effective;database;19;fine-grained;large-scale data analysis;compute nodes;data processing;parallel database systems;fault tolerance
real-world;attribute values;similarity functions;machine learning
taking into account;cost model;large-scale data analysis;long running queries;query processing;optimization problem;multiple queries
data objects;multi-version;specially designed;data partitioning;large-scale;range query;data items;replication;transaction management;control scheme;highly scalable;replicated data;load balancing
large data sets;parallel dbms;open-source;join processing
long-running queries;query optimizer;long-running;resource management;fault tolerance;load balancing
query optimization;query execution engine;low-overhead;statistical summaries;plans;intermediate results;greedy algorithm;search space;1;data partitions;highly correlated;query processing;databases;graphical models;join queries
ontology-based;database technology;database;sql queries;description logics;query-answering;instance;modeling language;data access;conjunctive queries;functional dependencies;tuple-generating dependencies
extraction algorithm;statistical measures;high-precision;large number of;market-basket;data set;query log;large datasets
attribute values;database;real-life;structured data from;similarity metric;web sites;web information extraction;seed set;template-based;similarity scores;web data
database;unstructured text;rule-based;rule refinement;information extraction;ranked list;extraction rules;data provenance;databases;information extraction from;extracted data;information extraction system;structured queries
search terms;precision-recall;search session;semantically related;document retrieval;text retrieval;confidential information;relevance scores;user query;search queries;search engines;user privacy;user intent;search engine;privacy risks;query terms
partition-based;private information;privacy preserving data mining;privacy preserving data publishing;protect privacy;random perturbation;sensitive values
nearest neighbor;sharing information;sensitive information;spatial queries;user privacy;information retrieval;knn) queries;location privacy;nearest neighbor search
clustering;tuning parameters;uncertain data management;uncertain databases;maintenance costs;uncertain attributes;query performance;data structures;cost models;maintenance cost;probability values;real datasets
data analysis;gaussian distributions;probability distribution;18;decision support;ranking function;probability distributions;nearest neighbor queries;ranking functions;uncertain objects;efficient approximation
set similarity join;set similarity join;pruning techniques;synthetic data;data cleaning;upper bound;real-world applications;probabilistic data;databases;operator;false alarms;data integration
cpu-based;field-programmable gate arrays;event detection;data stream processing;complex events;high-volume;complex event processing;data streams;processing power;data distribution
open source;database;main memory;low-bandwidth;data transfer;compression schemes;compression ratio;databases;column-oriented;processing speed
log records;database systems;database;software architecture;multi-core;intensive workloads;data structures;modern database;high volume
clustering;clustering algorithms;overlapping clusters;variable size;large scale;community detection;large graphs;directed graphs;clustering techniques;scale poorly;real world datasets;connected components;real world
algorithm relies on;latent variable models;topic models
keyword search over;user queries;keyword search over;databases;keyword query;relevant entities;query keywords;structured queries;search interface;keyword search on
search space;synthetic data;arbitrary shape;real data;pruning strategies;moving objects;moving object
spatio-temporal;database server;location-based queries;database;indexing structures;spatio-temporal;database research;spatio-temporal;moving object;safe regions;proposed protocol;nearest neighbor queries;cost model;moving object databases;query distribution;moving objects;query results;indexing techniques;range queries;database systems
shortest paths;location-based services;shortest path;location-dependent;road networks;path queries
probabilistic data;linear programming;database;operator;search problem;probabilistic databases;probabilistic data;similarity search;nearest neighbor queries;sensor network;distance metric;relational database;index structures;range queries;existing database
probabilistic xml;xml documents;probability distributions;probabilistic xml;markov chains;xml schema
data values;monte carlo;database;stochastic model;database;probability distribution over;risk analysis;query-result;query results
database content;graphical models;monte carlo;real-world;probabilistic inference;relational queries;view maintenance;desired level of;probabilistic databases;answering queries;databases;relational database;markov chain;information extraction;relational algebra
approximation algorithms;data analysis;synthetic datasets;database;multi-column;relational tables;primary key;key constraints
context information;greedy heuristics;sensor monitoring;synthetic datasets;database;valuable information;database objects;large databases;statistical information;location-based services;databases;higher quality;measurement errors;data integration;user input
uncertain information;user preferences;web sources;query interfaces;extracted data;information extraction;probabilistic databases;query plans;query processing;databases;multiple sources;rank-aware;web users
concise representation;search space;skyline points;formal concept analysis
database instances;database;database applications;databases;database application;query workloads;data generation
web-accessible;data sets;random access;databases;query result;join queries;upper) bounds
data record;large datasets
query optimization;xml indexes;fine grained;optimization techniques;internal nodes;tree automata
search systems;query node;worst-case optimal;twig queries;structural relationships;optimal algorithms;join algorithms;data structures;worst-case;average-case;solution space
materialized view;xml updates;xml queries;xquery update;rewriting algorithm
growing number of;graph structures;xml data;user queries;search result;business processes;web services;search engine;diverse applications;keyword search on
storage engine;higher throughput;lock;increasing number of;hardware technology;transaction execution;data structures;real-world;transaction processing
business process;query evaluation;business processes;provably optimal;weighting function
single agent;large-scale;real-world;scientific domains;optimization techniques;spatial joins;spatial locality;object-oriented;language called;high-level;agent-based
data structure;bayesian network;database systems;large-scale;graphical model;joint distribution;answering queries;query processing;multidimensional data;array data
structural patterns;frequently occurring;network applications;graph streams;pattern mining;social network;probabilistic approach;synthetic data sets
detection problem;euclidean distance between;plays an essential role in;large number of;location update;communication cost;massively multiplayer online games;mobile users
shortest paths;search space;uncertain graphs;complex networks;real-world;probabilistic graphs;biological data;similarity search;nearest neighbor queries;query processing;nn queries;fundamental problem;distance functions;nearest neighbors;communication networks
gps data;semantically meaningful;spatio-temporal;mutual reinforcement;baseline methods;massive amounts of;random walks
post-processing;range queries;differentially private;differentially-private;consistency constraints
transaction data;medical records;data analysis;partial information;privacy guarantee;market basket data;data integrity;query logs;association rules;privacy threat;information loss
data sharing;anonymized data;private information;16
maximum-likelihood;relational query processing;relational queries;database;trade-offs;relational operators;inference algorithm;information extraction;query answers;probabilistic data;data obtained from;query processing;answer quality;information extraction;conditional random fields;standard database
probabilistic databases;query evaluation;plans;large-scale;efficient query evaluation;database;conjunctive queries;probabilistic databases;algorithms require;normal form;inference algorithms for;result tuples;specifically designed to;databases;real world applications;special case;tuple-independent
correct answers;uncertain databases;uncertain data;uncertain database;special case;data integration
social media;large-scale;user generated;user generated content;text collection;social networks
geographic information;database;database design;database size;query performance;integer linear programming;materialized views;space budget;commercial database;correlated attributes
database systems;synthetic datasets;database;database size;scoring functions;utility functions;databases;multi-criteria decision making;large database
multiple databases;large distributed;data source;programming model;database;cost-aware;object retrieval;data sources;probabilistic approach;data sets;multi-source;databases;real-world data sets;source selection;multiple sources;optimal number of;query engine;minimum cost
response times;optimizer;plan selection;query optimizer;database;query templates;plan generation;query execution times;plan choices;selectivity estimates;estimation errors
optimizer;plans;query optimizer;database;real-life;sql query;sql tuning;higher-level;execution plan;multi-tier;skewed;join order;sql-tuning;aware query;tuning tools;monitoring data
approximation algorithms;fall short;optimization problems;np-complete;web site;subgraph isomorphism;topological structure;real-life and synthetic data;decision problems;emerging applications;graph matching;structural similarity
database;index structure;data item;matching process;pattern matching;diverse domains;large database
synthetic data sets;indexing method;computational biology;approximate matching;large graphs;spanning trees;social networks;large database
solid state;search performance;lower level;tree index
data residing;computing infrastructure;tree index;end users;maintenance cost;query patterns;large number of;tree nodes;indexing scheme;tree based;high-throughput;compute nodes;data processing;adaptive algorithm;huge volumes of data;data management;query processing
real data sets;pruning techniques;join algorithms;data sets;short strings;distance constraints;similarity joins;similarity join;data integration
access method;nearest neighbor queries;index structures;index structure;tree nodes;local neighborhood;spatial queries;nearest-neighbor;hierarchical structure;nearest neighbor queries;search region;queries efficiently;result set;search region
nearest;attribute values;synthetic data;real-world datasets;query returns;disk access;block-based;response times;query processing in;query processing;tree index;real world;sequential scans;aggregation functions;similarity measures
partial orders;false positives;indexing approach;require expensive;attribute domains;partial order;partially ordered;total order;multidimensional data
stream processing systems;input data;optimization strategies;output quality;xml streams;xml elements;rates;higher quality;result set;limited resources
database;data centers;data integration;stream data;sensor networks;sensor data;query processor;standard datasets;web data;limited-bandwidth
intrusion detection;network intrusion;optimization framework;database;probabilistic models;real-world;performance gains;real-time monitoring;decision support applications;resource management;predictive model;cost-based;streaming data;dynamic) bayesian networks;high probability;optimization techniques;threshold-based
approximation techniques;complex queries;real-world;data model;data-stream;monitoring applications;probability distribution;randomized algorithms;uncertain data streams;queries involving;uncertain data streams
error-prone;schema mappings;data provenance
clustering;real data sets;naïve;database records;distance-based;large datasets;real world
web pages;web search;knowledge bases;world knowledge;html tables;graphical model;search tool;relational information;web crawl;machine learning techniques;high-quality;relational data
search techniques;text analytics;ad-hoc;real-world;named entities;large-scale;social-tagging;great potential;business-intelligence applications;text corpora;indexing methods
data sharing;fall short;synthetic data;real-world;web technologies;data items;detection algorithm;answering queries;multiple sources;data integration
data publishing
high-speed;parallel execution;data visualization;network flow;large volumes of;real-world;behavior analysis;increasing number of;indexing scheme;forensic analysis;query response times;large-scale;service provider;rates;general-purpose;analysis tasks;databases;network traffic;measurement data;network data;million records
data warehouse;database;relational queries;real-world applications;distributed stream processing
database
multi-player;low power;high throughput;storage layer;real-world;data center;energy efficiency;hash table;performance gain;hard disk;working set
source code;database;real-world;database;source database;high cost;commercial databases;application code;application development
database systems;high-end;solid state;unlike prior;performance gains;access patterns;solid-state;hard disk;database management systems
real dataset;efficient storage;large number of;file-systems;data collected from;data warehousing;operating conditions;existing database
access method;multi-query optimization;open source;designed specifically for;data compression;data warehouse;virtual view;ad-hoc;query language;raw data;materialized views;large-scale data analysis;online advertising;distributed computing;schema design;optimization techniques;unstructured data;query processing
detection algorithms;parameter-free;distance-based outlier detection;distance-based outlier detection;network intrusion detection;data cleaning;diverse set of;outlier detection;distance-based;optimization strategies;detecting outliers;statistical approaches;design decisions;real-life datasets
fast response;database;log records;mobile devices;user requirements;digital cameras;mobile applications;real world;database workloads;data management
unique features;query evaluation;real world;energy efficiency;mobile objects;location update;query aware;mobile clients;scheduling algorithm;location update;spatial resolution;road networks;higher accuracy;high cost;location based services;query-aware;road-network
search results;service quality;web search;large number of;social networks;data mining;massive data sets
decision support;plans;query optimizer;sql queries;high degree of;database;execution plan;query response times;20, 21;data warehousing;query optimizers;database systems
evolving data;schema design;database;data warehouse;database;sql queries;column oriented;open source;query processing;schema evolution;databases;relational databases;query results;database schema
financial data;event processing;high-frequency;publish/subscribe system
user preferences;database systems;database;uncertain attributes;context-aware;query processing;location-based services;answer queries;location-based;location-based queries
query languages;data storage;data model;application requirements;flexible architecture
continuous queries over;streaming applications;continuous query;data types;increasing demand for;query processing capabilities;domain expertise;stream query processing;historical data;user defined;high-rate;microsoft sql server;los angeles
data loss;xml data;domain-specific;xml document;xml query language;query engine
continuous query;real-world;fine-grained;event based;complex event processing;pattern matching;complex event processing
large data volumes;data structures;relational database systems;data model;operator
xml documents;human users;view maintenance;web services;data-centric
network monitoring;stream processing;replication;data streams;internet-scale;resource usage
commercial dbms;data management system;active functionality
high-quality;relevant answers;search paradigm;real datasets;query interfaces;user types;user experiences;faceted search;access information
search results;life sciences;database management systems;database;temporal information;text documents;spatio-temporal;relevant documents;document collection;application domains;information extracted from;information embedded in;document collections;information retrieval techniques;temporal data;structured data
knowledge bases;data sets;1;text collections;2;5;schema information;semantic information;node labels;web data
xml schemas;database systems;data transformation;heterogeneous data sources;xml documents;hierarchical structure;data management;schema evolution;matching algorithm;xml schema
search results;web search;feature set;np-hard;exploratory search;information exploration;user-friendly;error prone;structured data;keyword search on
domain-independent;real-life;structured data from;web sources;html pages;extracted data;template-based;application scenarios;fully automatic;real-world;structured data;web data
ranked list;knowledge bases;extracted information;specific queries;document retrieval;information extraction;background information;future queries;relevant information;query results;knowledge base
search systems;keyword queries;database;real-world;information theory;xml keyword search;data sets;structural information;query languages
similar queries;database;sql queries;recommendation algorithms;query log;analysis task;scientific databases;similar users
desktop search;personal information management;classification;information management;user generated content;communication cost;collaborative tagging;data mining algorithms;content management
query results;text search;web archives;temporal evolution;assists users;query result;tedious task
image retrieval;visual content;real-world;image search;interactive visual;information source
web query interfaces;deep web;query interface;deep web;application domains;data structures;integration systems
erroneous data;high quality;data sources;data sources;user-friendly;incomplete data
data sources;great flexibility;data integration systems;mapping rules;data integration
data analysis;fault-tolerance;massively parallel;web-scale;scientific data management;large-scale data analysis;schema free;data processing;parallel database systems;data management
multi-dimensional;sequential patterns;data cube;data warehouse
domain knowledge;cluster based;data mining methods;concept-based;data resources;final step;subspace clustering;hidden knowledge;concept discovery;subspace clusters
database contents;database;keyword-based;instances;data integration systems;relational databases;numerous applications in
real data sets;integrity constraint;completeness;data quality;user interface;data consistency
web based;large number of;distributed caching;data access;data-centric;transaction processing;high availability;distributed caching
computing infrastructure;database management systems;update intensive;database;decision support systems;service oriented;big data
similarity searching;multimedia databases;nearest neighbor;distance-based;search process
event processing;1;management systems;3;2;5;data stream management systems;6;9;temporal databases;business rules;distributed computing;event processing;active databases;inference rules;12;15;14;database;19;event processing;data stream management;commercial products;data management
clustering;collecting data;database;uncertain databases;data mining applications;pattern mining;mining uncertain data;application domain;similarity search;similarity queries;queries efficiently;uncertain data
association rule mining;semi-structured;data management;sales data;data mining problems;data management;data stream management systems
map-reduce;data analysis;database management systems;data management;databases
virtual worlds;data management problems;information management
database;real-world;threshold queries;scoring functions;scoring function;database relations;data mining;distributed data
memory footprint;fully-connected;streaming environment;main memory;mining process;neighborhood graph;problem setting
shortest paths;search space;naïve;query type;large-scale datasets;formal language;road networks;shortest path;road network;location-based services;indexing technique;path queries;graph indexing;million edges
error-prone;data integrity;low cost;database;database design;user's perspective;data model;data structures;data entry;graphical user interface;ad-hoc;user-friendly;user studies;data management applications
data objects;query execution;spatial neighborhood;wide range;query processing;location-based;preference queries;processing cost;feature objects
analytical queries;database;database design;main memory;highly accurate;storage engine;sequential scans
integrity constraint;database schema;data migration;web information systems;scientific databases;schema evolution
web documents;tree structures;spatial relationships;frequent updates;information extraction;combined complexity;web documents
privacy preserving;privacy requirements;social networks
prohibitively expensive;hadoop/mapreduce;massive graphs;real world datasets;large graphs;large networks;algorithm to compute
higher accuracy;main memory;synthetic data;efficient indexing;record linkage;brute force approach;sliding window;real data;real-world applications;record pairs;high scalability;implementation details;databases;database queries;disk-based;highly scalable;data integration;record linkage is
singular value decomposition;numerical experiments;tensor-based;temporal link prediction;data mining tasks;link structure;link-based;social networks;temporal link prediction;link prediction;periodic patterns;predicting future;temporal data;tensor decomposition
high-dimensional;data objects;theoretical analysis;algorithm produces;high dimensionality;dimensionality reduction;clustering;clustering algorithms;data mining tasks;landmark-based;large datasets;produce high quality;data mining algorithms;clustering quality;faster convergence;data preprocessing;dimensionality reduction;data pre-processing;data mining research;landmark-based;clustering quality;low-dimensional
clustering;attribute values;topological structure;existing graph;social networks;clustering methods;automatically learn;structural similarity;clustering;biological networks;clustering techniques;sensor networks;clustering algorithm;optimization techniques;graph clustering;large graph;summarization methods;information networks;distance measure;application domains;large graphs;edges represent;theoretical analysis
singular value decomposition;sampling approach;sampling methods;singular value decomposition;massive data;low-rank;sparse datasets;high accuracy;computational requirements;fast algorithms;low-rank matrix approximation;applications involving
end-users;user study;privacy policies;inference techniques;social networking;social network;privacy policies;social network;social networks;social networking;privacy preferences;game theory;social networks
users' satisfaction;user behavior;group members;recommendation algorithms;data set;group recommendation;space constraints;space budget
network datasets;theoretical results;social networks;network structure;network data;privacy risks
resource sharing;spam detection;social network analysis;search engine;real-life;information retrieval;knowledge discovery;ranking algorithms;knowledge representation;user-generated content;data mining;log data;implementation issues;information extraction techniques;user communities
knowledge sharing;real data;search effectiveness
entity recognition;real-world;business intelligence;sentiment analysis;data sets;social networks;business intelligence applications;social intelligence;social intelligence;data analytics;social networks

search systems;search evaluation;search evaluation
search systems;search systems;search engines
sigir workshop on;desktop search;desktop search
real-world;increasing number of;sigir 2010 workshop on;information retrieval;user interface;great potential;ir systems;user interaction
sigir 2010 workshop on
search systems;query throughput;optimization techniques;web search;large-scale;information retrieval;collection selection;geographically distributed;query response times;large-scale distributed systems;similarity search;distributed information retrieval;query processing;search efficiency;information retrieval;information retrieval systems;large-scale distributed systems
language modeling approaches;information retrieval;language modeling;workshop brought together
ir researchers;information retrieval workshop;information retrieval workshop
user studies;international workshop on;information access;information access;international workshop on
information retrieval;information retrieval;workshop brought together;international workshop on

large-scale;text retrieval;retrieval function;real-life applications;multimedia documents;portfolio selection;concept-based;ranking function;great potential;document representation;multimedia retrieval;retrieval performance;document representations;detection performance;retrieval framework;retrieval engine;ranking models;document representation;language modeling;monte carlo
false positives;retrieval strategies;question answering;graph representation;information retrieval;rank-learning;retrieval methods;qa systems;highly ranked;text collection;semantic constraints;retrieval model;poor quality;question answering;natural language
user profile;multimedia documents;explicit feedback;semantically related;data representation;retrieval results;retrieval systems;user profiling;user interests;user profiles;text retrieval;long-term;information gathered;video retrieval;implicit feedback;low-level
clustering;search performance;network size;network connectivity;information retrieval;global structure;provide evidence;information retrieval models;relevant information;information space;distributed systems;distributed environment;distributed environments;search systems;information spaces;complex networks;high scalability;scalability problems;information networks;information systems;large scale;retrieval methods;increasingly large;network structure;network clustering
task type;document usefulness;background information;information retrieval;search performance;information seeking;taking into account;individual users;user behaviors;contextual factors
information seeking;user-centric;large-scale
domain-specific knowledge;domain knowledge;retrieval effectiveness;concept-based;information retrieval methods;classification process;representation language;scientific literature;retrieval performance;cross-lingual;document representations;concept-based representation;biomedical research;matching problem;biomedical information retrieval
retrieval engine;tf-idf;retrieval techniques;correct answer;document structure;linguistic information;named entity;artificial intelligence;web) corpus;ranking model;exact answers;syntactic structure;natural language
probabilistic approach;data-rich;web interfaces;bayesian framework;data values
search result;sponsored search;input query
expected number of;database;monte carlo;large-scale;dynamic social network;large social networks;random walk;shared memory;social network;prohibitively expensive;instance;incremental computation;power-law;social networks;social networking;equation;random walks
point-based;skyline queries;large-scale datasets;skyline queries;dimensional data;multi-criteria;space partitioning
partial orders;skyline computation;indexing scheme;coding scheme;indexing method;efficient indexing;partially ordered;attribute domains;skyline queries
high accuracy

database;category information;sql server
modeling method;vehicle motion;vehicle motion;low precision;auto-regressive;kalman filter
parametric design;configuration parameters;parametric design


water distribution;high-quality
compression ratio



fuzzy neural network;low accuracy;genetic algorithm;fuzzy neural network;simulation results;genetic algorithm;fuzzy control;membership functions;neural network;artificial neural network

test results

multi-objective;optimization problems;optimization method;optimization algorithm;optimization model;solving complex
heat storage




control points;optimization algorithm;surface reconstruction
11
flow fields
task decomposition;task decomposition;multi-robot;task allocation;task decomposition
design requirements
supervision;supervision
operating conditions;wide range
theoretical analysis

control systems;high cost;high order



low cost
complete model;quantitative analysis

removal rate
wireless communication;motion control
special properties;multi level


control scheme;simulation results;control strategy;simulation result
high quality
working principle
working principle

control theory;mathematical model;control method;dynamic performance;dynamic performance
frequent item sets;frequent patterns;database;related information;pattern growth;apriori algorithm;fp-growth;frequent items;apriori algorithm;fp-tree;fp-growth;frequent pattern
specific task;task allocation;multi- agent systems

instance;optimization model;multi-objective;objective function

simulation analysis;development process
simulation analysis;finite element
flow field;moving object


monitoring data;low-level;wind speed
low-cost;mathematical model;machine tools;machine tool;machine tool
multi-dimensional;flow field

optimization method
simulation results;simulation model;control parameters
finite element model;theoretical basis;process parameters
parametric design;finite element method;finite element analysis;test data
operating parameters
removal rate;flow rate
finite element model;finite element model;numerical analysis

finite element model;empirical mode decomposition;health monitoring;finite element method
agent technology;source code;design method;data acquisition;data acquisition;multi-agent;agent technology;communication protocol;load balancing
function approximation;control algorithm;highly complex;dynamic model;adaptive control;simulation results;neural networks;neural network;neural networks
signal processing;fourier transform;signal processing

design requirements

distribution function;theoretical basis
hybrid method;closed loop;fuzzy control
quality assessment;principal component analysis;comprehensive evaluation;principal components;water quality;principal component analysis
data rates;closed-loop;sufficient condition for;theoretical results;control strategy;lower bound
control systems;control policy;communication network;control systems;sufficient condition for;upper bound;control scheme;simulation results
data rates;feedback controller;sufficient condition for;control scheme;simulation results;linear systems
image quality;image processing
theoretical basis for;error analysis
magnetic resonance;fourier transform
long-term
finite difference
error-prone
neural networks;case study;0, 1;optimization model;case study;neural networks

numerical simulation
numerical simulation;simulation model;flow field
small scale;numerical simulation;small-scale;power consumption;small-scale
numerical simulation;optimization design;simulation results
simulation results;numerical simulation;auxiliary;numerical simulation;auxiliary
auxiliary;operating parameters
matlab/simulink;simulation results;simulation model;state variables
sensitivity analysis;finite element model;key parameters
simulation results;flow field

test results
large scale;large scale;dynamic model

fourier transform
working principle;design requirements
internal structure

numerical simulation;numerical simulation;process parameters
high temperature;numerical simulation;numerical simulation
simulation results;numerical simulation;simulation result;optimization technique;process parameters
numerical simulation;special structure
numerical simulation;numerical simulation
increasing attention;air quality
low-cost;data analysis;design principles
high-frequency;high-frequency

design method
removal rate

working principle;dynamic model;optimal design
storage structure;traffic control;road network;instance
accurate tracking;control law
theoretical analysis;case study;test results;machine tool;machine tool
high-speed;high temperature;high temperature
high cost;low power;image acquisition
flow rate
high speed;rates;high speed
software development;optimization model
machine tool;machine tool
development process;low temperature
design method;mathematical model;finite element;theoretical basis for;finite element method
design method;optimal design;optimal design
genetic algorithms;genetic algorithm;optimal design;fitness;operator;objective function;optimal design
rates
numerical simulation;flow rate
theoretical basis for;energy efficient
light source
removal efficiency;removal rate
image analysis
aspect ratio

image noise;image restoration;machine vision
mathematical analysis

high potential
finite element;finite element;reconstruction algorithm
correlation coefficient;neural network model;neural network model;bp neural network;relative error;process control
algorithm parameters;predictive models;optimization technique;vector machine;predictive models;neural network;relative error;model parameters;absolute error;simulation results
rates;flow rate
theoretical analysis;heat storage;heat storage;heat storage
time series;correlation analysis
high frequency

modal analysis;finite element analysis;machine tools;design method;finite element model;finite element;dynamic performance;machine tool



finite element;high quality;design process
optimal parameters;finite element;finite element method;process parameters
intelligent control;intelligent control;data acquisition;artificial intelligence
reusability
human activities


impact factors
welding process;signal processing

data characteristics;data mining system;data mining
low cost
flow rate
large quantity of
correlation coefficient

theoretical basis for
image processing;image processing
maintenance cost;neural network

correlation coefficients

machine tools;high-precision;feature extraction;large number of;performance degradation;machine tools;reliability analysis;high-level;low-level;machine tool
cost-effective;removal efficiency
intelligent control


information fusion;modal analysis;fusion method;evidence theory;modal analysis
times higher;indoor environment;energy efficiency;finite difference
removal rate
numerical simulation;flow rate;initial conditions;flow rate;geometric model;simulation results
sample sizes;survey data;sample size;sampling method
comprehensive evaluation;variable size

extended kalman filter;wireless sensor network;target tracking;1;single target;sensor networks;target tracking;target detection
simulation study


removal rate;water quality

generation algorithm;decision tree;data model;decision tree
simulation model;heterogeneous systems;multi-body;multi-body
simulation results;long distance

removal rate
high efficiency;simulation results
high efficiency;fuzzy controller;genetic algorithm;fuzzy controller;genetic algorithm
signal processing;processing speed;parallel programming;computational power;large volume
impact analysis
design method;finite element method
data acquisition;higher accuracy
change analysis;finite element method
mathematical models;numerical simulation
fast response;vector machine

modeling method;web service;meta-model;meta-model;instance;modeling method;modeling methods
surface shape
high-speed;high-speed;data collection;machine tool
dynamic performance


numerical simulation;dynamic performance;simulation model
large number of
factors affecting

motion model
model construction;simulation analysis
key points;lower level
lock;high speed;lock
control theory;fuzzy control

network structure;water resources;forecasting accuracy;water resources;power grid

cost-effective
hierarchical structure;database
modal analysis;modeling method;machine tools;dynamic model;machine tools
error analysis
selection criteria;calibration method;recognition rate

complex networks;large numbers of
sensor nodes;database;adaptive control;sensor node
kalman filter;gaussian mixture model;1;image sequence;human body;image processing;moving targets
removal efficiency
singular value decomposition;quantitative analysis;simulation result

image points;numerical simulation;intrinsic parameters;pose parameters;matching problem;pose parameters

machine tool
reference data
wavelet analysis;high sensitivity
database technology

data acquisition;data processing;data-acquisition;high speed

multi-point;simulation results

classification;prediction methods

high speed;dynamic analysis;dynamic behavior;energy efficiency;dynamic analysis
dynamic analysis;finite element;multi body;finite element method;dynamic analysis
test results
numerical results;finite element method
cost function;input-output
case study
simulation results


relative error
rates;energy efficient

finite element method
rates;light sources



small scale
theoretical analysis



energy efficiency;energy efficiency

high-quality;motion control;accurate tracking;high precision;closed-loop;closed-loop;high speed
optimization design;optimization design;finite element analysis;optimization model;parametric design
finite-element method;physical characteristics
power factor

air quality;quantitative analysis;quality assessment;comprehensive evaluation;quality assessment;cluster analysis;qualitative analysis
air quality
flow rate;rates;low efficiency
general concept;simulation result
high-speed;numerical simulation
prediction model;genetic algorithm;bp neural network;learning algorithm;time series;bp algorithm;multiple linear regression;genetic algorithm;network structure;test results;neural network;artificial neural network
correlation coefficient;impact factors;quantitative analysis;correlation method
multi-domain;multi-domain;reliability analysis;simulation results
complex structure;extended kalman filter;state variables;dynamic performance;high cost;simulation results

observed data

rates;industrial applications
data transmission;low-frequency
rates;control parameters
high-speed;specially designed
factors affecting;removal efficiency
received increasing attention
times higher
reusability
process planning;process planning;process planning;database
small scale;small-scale;confidence levels;small-scale
human face;human face
local minimum

low temperature;energy efficiency;end users;flow rate


feature space;learning mechanism;classification;svm-based;generalization performance;large number of;vector machine;gaussian kernel;fuzzy model;fuzzy model;fuzzy model;learning scheme;support vectors
optimal design;optimal design
simulation results
high-dimensional;phase space reconstruction;dimensional space;time series;reconstruction algorithm;phase space reconstruction;noise reduction;phase space;low-frequency
high level;selection methods;behavior patterns;intelligent behavior
energy function;test set;image segmentation;optimization algorithm;stereo matching;belief propagation;stereo matching;belief propagation
face detection;face detection;face detection;vision systems;field programmable gate
high-speed;illumination conditions;success rate;machine vision;template matching;machine vision;quality measures

simulation results
control method;finite element model;finite element;control method;simulation results
finite element;finite element;simulation result
flow-field;numerical simulation;flow-field
approximation method;singular value decomposition;processing speed;low-rank;data matrix
learning process;decision-making;neural network
human brain;human brain
agent model;agent model;agent technology


blind source separation;extracted features;principal component analysis;welding process;independent component analysis;acoustic features;principal component analysis;welding process;blind source separation

automatically generates;process planning;process parameters;post processing;motion control;neural network model;multi-axis;multi-axis;neural network;machine tool
operating conditions;low cost
test results;signal processing;field programmable gate arrays;motion control
high accuracy

inference method;control scheme
iterative learning;iterative learning;simulation results
fuzzy model;conventional methods;fuzzy logic;error model
dynamic model;fuzzy control;matlab / simulink

feature space;machine tools;manufacturing process;feature-based;error model;machine tool;quality control;error model;multi-body;mapping function;modeling method;transformation matrix;machine tool
data analysis;decision support;real data;fuzzy logic;production data;membership functions;expert knowledge;fuzzy logic
physical characteristics
robot control;closed-loop;wireless communication;decision-making;robot control;machine tool
high-speed;high-speed;complete information;high throughput;prior-knowledge;simulation results
complex network;fast algorithm;real-world;network data;community structure;community structure
high level;control method;neural network
rates

theoretical analysis;1,2
large scale;low cost;long distance
human activities
shape descriptor;local features;analysis tasks;graph representation

impact factors
water quality
highly correlated;correlation analysis

statistical model;simulation results;filtering algorithm;statistical model
water quality;water quality

high frequency;partial differential equations;selection criteria;image restoration;image restoration
case study;reference set;assembly process;evaluation index system;reference set

low cost


control strategy;control strategies;simulation results;control strategy;simulation results
dynamic model;dynamic performance;simulation results


control theory;small size
multi-class;classification;vector machine;vector machine;classification results;svm classifiers;multi-class
fourier transform
great significance;behavior analysis;target detection;behavior analysis
numerical simulation
free-form;free-form;high quality;low cost
scheduling algorithm;scheduling strategy;control algorithm

multi criteria;resource utilization;service-oriented;manufacturing process;case study
video data;estimation algorithm
remote sensing images;object recognition;multi-agent system;remote sensing image;classification;remote sensing;multi-agent;human vision;effectively identify;recognition accuracy;multi-agent;multi-source;specific characteristics;recognition rate;multi-sensor;image recognition
data set;numerical experiments;error analysis
machine tools;signal processing;data processing;machine tools
description language;matlab/simulink;modeling tool;development process;high availability;target detection
low-temperature;application development;great potential
fast response;data acquisition;high precision
case study;water quality

design method
water quality
operating parameters
removal efficiency;energy-efficient;collect data
removal efficiency
high-speed
feature space;distance based;optimal parameters;support vector machines;radial basis function;inter-class


design method;design method
pso algorithm;pso) algorithm;total cost
closed loop;simulation study;simulation model;negative feedback
security problems;data mining;information management;information management;classification rules;data mining;association rules

higher quality;high-quality;high-quality
linear programming;neural network model;logistic regression;continuous variables
takes into account;decision makers;energy efficiency;high priority
mathematical model of
multi-user;low cost;power consumption

higher precision;simulated annealing;simulated annealing;higher degree of;recognition accuracy;learning method
data processing;data processing;data storage
large scale;case study;large-scale;large-scale
theoretical analysis;structure parameters
flow rate

simulation analysis;simulation results
statistical significance;regression model;collected data
case study;multiple factors;specific problem;problem solving
multi-axis;machine tools;multi-axis;machine tools;unified framework

pattern recognition;classification;neural network;process control;automatically detected;neural network

simulation analysis;design requirements
multi-point;data processing;object-oriented;multi-point
von-mises;finite element analysis
mathematical model;traffic flow
power consumption;high reliability

sensitivity analysis;high temperature
wireless network;wireless communication
industrial applications
inverted pendulum;inverted pendulum;control theory;mathematical model;fuzzy controller;fuzzy control;fuzzy logic
image information;position information;image processing;machine vision;machine vision
finite element method;strong correlation
decision-making;problem solving;process control
machine tools;machine tools
operating conditions;complex structure;matlab/simulink;relative error
case study;service-oriented;service-oriented
database design;process planning

control method;control method;multi-sensor;multi-sensor
initial conditions;simulated annealing;simulated annealing;physical resources;optimization problem
modeling method;vector machine;training data;data description;support vector;local learning
mathematical model;numerical simulation;simulation results
multi-pass;simulation model;process parameters;multi-pass;welding process;welding process
high precision;control problem;control strategy;simulation results
high cost;simulation results
machine tools
matlab/simulink;control strategy;simulation model;control strategy;fuzzy control;fuzzy rules;simulation results
process model;product design;modeling technique;object-oriented;multi-level
optimization algorithm;multi-objective optimization;multi-objective;multi-objective;objective function
spatial distribution
development process
network model;network model
fuzzy neural network;neural networks;inverted pendulum;learning algorithm;lazy learning;knowledge extraction;simulation results
healthcare data;domain experts;audit data;unsupervised algorithm;databases
automatically extracted from;computational power;ground-based;probabilistic models;genetic algorithm;high-availability;data set;data sets;probabilistic models;data mining;cluster analysis
domain experts;data mining techniques;spatio-temporal;generally applicable;case study;rates;temporal data;spatio-temporal;extracting features from
visualization tool;predictive power;neural network
neural networks;prediction quality;machine learning;1;high complexity;gaussian processes;gaussian processes
search task;search task;end users;query suggestion;query suggestions;search engine;query suggestion;query sequence;user search behavior;search log;search behavior;search tasks;search behaviors
sample selection;sample selection;training set;large scale;high quality;vector machine;sampling method;learning tasks;selection method;recognition accuracy;training samples;distance-based;svm classification;classification task;classifier
data stream;sliding window;sliding window;data stream processing;instance;data stream;theoretical results
dynamic programming algorithm;classification;time series;time series;weighting scheme;cost function;piecewise linear
time series;multi-step;highly complex;time series;time series;real data;prediction algorithms
distributed algorithm for;distributed algorithm for;graph theory;large scale;structural properties;social networks
generalization performance;real-world data sets;classification accuracy;map-reduce;classifier
hidden knowledge;providing users;social interactions;social interactions
data mining;international workshop on
large amounts of;clinical data;molecular biology;data mining
knowledge-based;machine learning;data mining;long term;international workshop on;decision making
clustering;clustering;customer base;customer segmentation;classification models;data warehouses;decision tree;vector machine;hybrid approach;customer relationship management;increasing amounts of;cost-effective;marketing strategies;predictive analytics;neural network;predictive analytics
social security;decision trees;data mining techniques;real-world;decision trees;test results;historical data

heuristic algorithm;correctly identify;matching problem;computational biology;topological structure
random graph;vertex set;web service;pair wise;metabolic networks;increasingly large;high-throughput;network topologies;finding similar;biological network;metabolic networks
intrusion detection;parameter-free;fast algorithm;weighted graph;machine vision;traffic monitoring;surprising patterns;real data;large graphs
general purpose;data volume;large-scale;massively parallel;decision-making;mobile devices;optimization algorithms;analyzing data;storage management;conditional random fields;learning method;resource consumption
prediction problem;prediction accuracy;data mining
traffic flow;predicting future;context-dependent;predict future;traffic flow;context-dependent;simulation framework
baseline algorithm;predicting future;training sets;predict future;predicting future
ensemble approach;predict future
data mining;data mining
aggregation algorithm;data streams;data-stream;adaptive clustering;sensor networks;data streams;aggregation algorithm;network lifetime
similarity join;databases;similarity join;product descriptions;user feedback
multi-view;multi-source;clustering methods
flexible architecture;temporal data;classification;classification;rule-based;classification methods;rule mining;algorithm named;real dataset;classification rules;temporal data;discovered rules
web-sites;data mining methods;data mining techniques;large number of;deep web;data sources;deep web;everyday life;data sources;rule mining;analyzing data;hash-table;data mining problem;high-level;pruning method
bounded treewidth;metabolic networks;increasingly large;high-throughput;tree width;finding similar;metabolic networks
gene ontology;database;gene expression;manifold learning;pattern mining;compact representation;application systems;data dimensionality;periodic patterns;spatial patterns;gene expressions;gene expression;knowledge base
large-scale;data mining processes;large number of;hidden markov models;sequence databases;data mining
local structures;local sites;conventional methods;local-structure
feature vector
interestingness measure;knowledge discovery;interestingness measure;diverse domains;interesting patterns
nearest;case based reasoning
gps data;pre-processing;gps data;nearest neighbor;major components
simulation framework
exploratory analysis;social network analysis;community mining;community mining;user interface;social networks;social networks
mining algorithms;stock-market;mobile devices;visualization technique;data stream;data mining;visualization techniques
real-world;cost effective
clustering;interactive exploration;data mining methods;clustering approaches;multiple clusterings;subspace) clustering;knowledge extraction;large amounts of data;interactive exploration

clustering results;learning algorithm;classification;regularizer;optimization algorithm;learning algorithms;linear combination;human experts;complementary information;data sources;semi-supervised
ensemble learning;feature space;classification method;label space;classification methods;multi-label data;instance;label sets;multi-label data;optimal parameters;multi-label;classification problems;classifier;learning method
confidence scores;labeled documents;web documents;document classification;relevant documents;semi-supervised learning;classification tasks;content information;sufficient training data;web document;classification task;model training;click graph;sparseness problem;training data;multiple tasks;class label;random walk model;huge number of;classification quality;seed set;data sources;classifier
individual classifiers;feature vector;classification;classification accuracy;image data;feature-based;feature-based;obtained by applying;classifier;classification method;feature vectors;image classification;image classification;confidence level;original data;classifier
small sample;classification model;classification;classification;database;temporal sequence;item set;mining algorithms;sequence classification;mining algorithm;real world;sampling technique
statistical significance;data mining tools
set covering;learning algorithm;low-cost;medical diagnosis;cost-sensitive;feature selection algorithm;instance;feature selection;feature selection;informative features;main idea
clustering;human brain;human subjects;synthetic data;similarity measure;density-based clustering;time series;time series;human brain;density-based clustering;real data;lower bound;density-based clustering algorithm;local similarity
prediction techniques;amino acid;accuracy compared to;feature representation;svm classifiers;local structure;knowledge-driven
feature space;high risk;anomaly detection;large volumes of data;dynamically changing;labeled training data;machine learning;nearest;nearest neighbor algorithm;locality sensitive hashing;nearest neighbor;typically requires
clustering;biclustering algorithms;microarray datasets;mining frequent closed;biologically relevant;microarray datasets;frequent closed
clustering algorithms;direct comparison;evolving data streams;clustering performance;evolving data streams;evaluation measures;evaluation measures
data-mining;distributed data mining;highly distributed;agent-based;data mining;distributed data mining;agent-based;distributed environment
initial query;relevant documents;user-defined functions;database;on-line analytical processing;information retrieval;knowledge discovery;data cubes;digital libraries;keyword search;standard sql;vector space model;query refinement
recommender systems;database;database;analysis task;scientific databases;similar users
data mining results;rule-based classification;data uncertainty;uncertain data;probability distribution;network latency;classification performance;rule-based classification;categorical data;data uncertainty;uncertain data;classification rules;real-world applications
resource manager;stream processing systems;game theoretic;large scale;optimization strategies;large-scale;vector machine;stream mining;processing nodes;concept detection;distributed stream processing system;semantic relationships between;classifier
feature space;data source;classification;classification;knowledge transfer;source-specific;bayesian approach;instances;data sources;sensor networks;generative model;tracking problem;data mining;feature spaces;classifier;weak classifiers;auxiliary
learning agents;classification accuracy;learning approaches;data distributions;distributed classification;general problem;distributed classification;transfer learning
clustering;clustering;scientific domains;time series;time series
training data;prediction model;test data;genetic algorithm;fitness function;data mining techniques;prediction errors;fitness;historical data;feature weights;data mining;fold cross validation;prediction models;markov model
spatio-temporal;climate data
data analysis;utility functions;monte carlo;time series;model fitting;data analysis;moving average;los angeles;markov chain;arima model
compares favorably with;machine learning methods;classification;svm-based;rates;class labels;real world;classification problems;error rate
auc;microarray experiments;semi-supervised learning;data mining and machine learning;systems biology;label propagation
digital data;digital data
word vectors;conventional methods;temporal patterns;tf-idf;text categorization;key words;document clusters;document clustering;feature sets;document categorization;temporal patterns;similarity measures
financial data
web based;decision making;data visualization
negative result;frequent item sets;upper bound;worst-case;upper bounds;item set;data stream;space usage;data streams;support threshold;similar items;finding similar;data mining algorithms;similarity measures
minimum spanning tree;cluster based;classification model;nearest neighbor;synthetic data;minimum spanning tree;massive data;classification methods;data sets;classification model;classification method;real world;massive data
clustering;domain experts;clustering method;density based;simulation data;hadoop/ mapreduce;data volumes;produce high quality;high density;clustering algorithm;computational model;automated techniques
takes into account;sensor networks;information processing;data aggregation;information processing
large scaled;recommender systems;recommender systems;user profiling;large scaled;user profiling;personalized recommendations;real world;large scaled
taking into account;optimization algorithm;local minimum;instances;transaction database;upper bound;objective function
satisfiability problem;temporal logic;temporal logic
estimation methods;training samples;rule induction methods;medical databases;databases
classification tasks;task specific;real life;learning strategies;supervised learning;classifier
decision support system;multiple criteria;decision problem
knowledge discovery;frequent episodes;completeness;discovered knowledge;knowledge discovery
clustering;clustering algorithms;cluster based;distributed computing;large scale;large scale;data sets;prior knowledge;equivalence relation;data sets;distributed clustering;clustering algorithm;clustering algorithm;algorithm called
large scale;data mining and machine learning;general-purpose;real-life;fault-tolerant;application developers;computing platform;programming interface
clustering;em) algorithm;multi-core;graphics processing units;communication costs;expectation maximization;large data sets;faster convergence;fast convergence;specific characteristics;data mining;em algorithm;computing power;distributed environment;cluster models;clustering
clustering;data analysis tasks;clustering algorithm;distributed memory;large number of;shared memory;mining algorithm;information theoretic;large datasets;data mining;clustering algorithm;high-level;data mining algorithms;information theoretic
tuning parameters;multi-dimensional;scientific data;query language;data sets;traditional database systems;spatio-temporal;scientific data
trec collections;relevant documents;query expansion;query expansion;individual queries;text data;information retrieval;retrieval performance;query difficulty;query processing;user request;query difficulty;information retrieval;query sets;query terms
classification;active learning;labeled training data;error-bounded;classification methods;data sets;instances;sparse representation;unified framework
high level;high level
conditional likelihood;classification model;classification accuracy;takes into account;bayesian network classifiers;topological structure;na篓ýve bayes;search spaces;learning method
statistical methods;training data set;sample size;bayesian network classifiers;bayesian network classifiers;data set;data mining;small samples
remote sensing data;ground based;decision-making;time series;time series;long-term;data sources;data driven;remote sensing data;segmentation methods;model fitting;data mining based
domain knowledge;domain knowledge;similarity measure;trajectory data;classification task;low-level;clustering
computational complexity;causal relationships;causal modeling;time series;application domains;prior knowledge;modeling methods
clustering;simulated annealing;information systems;census data;spatial clustering;case study;geo-referenced;personal information
moving objects;mobile devices;graphics processing
image retrieval;image retrieval;multiple-instance learning;multiple-instance learning;instances;low-level features;training samples
edge information;link prediction;social networks;social networks;link prediction;communication networks;link-prediction problem;problem called;link prediction problem
user profile;recommender systems;recommender systems;closely related;recommendation algorithms;real data;recommendation systems
mining task;association rule mining;application domain;analytic hierarchy process;case studies;association rules;association rules;association rules;interestingness measures;association rule
text mining;support vector regression;regression analysis;text-mining;long-term;higher accuracy;long-term;feature vectors
detection techniques;distance based;anomaly detection;real datasets;random walk;density based;detection approach;underlying data distribution;gaussian distribution;distance-based;distance based;principal component analysis;traffic data
database;real-world;sequential patterns;specific context;contextual information;sequential pattern mining
growing number of;missing data;missing data;mobility patterns;data mining research
temporal information;real-world;temporal sequences;observed data;time series;time series;simulated data;data mining;spatio-temporal;data-mining tasks
real-world;clustering results
shortest paths;search space;real-world;computational savings;spatial network;data set;case study;graph based;spatial network
remote sensing images;remote sensing images;national security;remote sensing;classification approaches;feature extraction;semantic categories;high-resolution;high resolution;high-resolution;latent dirichlet allocation
clustering;success rate;social network;social network;recommendation methods;matching process
unlabeled data;svm-based;learning phase;data streams;ensemble classifier;data streams;data stream environment;classifier
end-users;data-mining;domain-driven;statistics-based;real-life;domain-driven data mining;data mining approach;statistical analysis
domain knowledge;mining closed;multi-dimensional;mining algorithms;domain-specific;search terms;sequential patterns;rule sets;instance;data sources;stream data;click streams;product recommendation;memory consumption;sequential pattern mining;sequential pattern;application scenarios
data mining application;motion capture;nearest neighbor;data mining application;classification accuracy;time series;machine learning;decision tree classifier;machine learning algorithms;classifier
hyper graph;imprecise data;domain specific;imprecise data;database;databases;knowledge base
dynamic programming;temporal dimension;multiple times;ad hoc;algorithm to compute;longest common subsequence;similarity measures
multi-modal;community discovery;interaction networks;pair wise;discovery algorithm;community structure
predictive models;classification;large volumes of
textual features;social communities
clustering;search space;biological networks;real-world;baseline algorithm;social networks;community structure
sequence alignment;similarity scores;decision support;sequence alignment;feature selection;databases;model building;behavior analysis
domain knowledge;process model;data source;workflow management systems;process management;database;process management;business process;agent performance;data mining
pre-defined;classification;classification;computation cost;training error;learning algorithm;problem domains;classification performance;base learners;real-world data sets;benchmark data sets;penalty) term
learning process;case study;learning approaches;real dataset;instances;accurate predictions;real world applications;supervised learning;instance level
ensemble learning;large-scale;rule-based;online advertising;ensemble classifier;coarse-grained;classifier
ranking problem;total number of;real-world;social network;ranking problem;social graph;extensive simulation
user ratings;user behavior;social relations;social influence;social networks;social networking;individual users
social relationships;network datasets;community structures;synthetic datasets;generative model for;generative models;temporal behavior;temporal dynamics;social relations;temporal dynamics;social network data;social influence;social relations;link prediction;real world;rating prediction;evaluation criteria;social networks
collaborative filtering;recommender systems;success rate
online reviews;online reviews;web mining
desktop search;personal information management;classification;database;classification framework;rule-based;content analysis;desktop search;machine learning;text classifier;file systems;classifier
rule set;machine learning algorithms;skewed;data sets;detection methods;user feedback;class distribution;classifier;sampling technique
predictive models;market segments;accurately predict;decision-making;predictive models;predictive performance;wide range;customer behavior
search space;domain-independent;case-studies;real-life datasets;automatically discovering;discovery algorithm;real-world
temporal dependencies;mining algorithms;temporal features;application domain;sensor data;rule mining;application domains;activity models;clustering techniques;activity patterns;temporal features;data collected;data mining process;real world;activity patterns
data mining-based;hospital information;clinical information;exploratory data analysis;multimedia databases;stored data;stored data;temporal data mining
domain ontologies;semantic relations between;information retrieval;real world;external sources;ontology learning
detection problem;user profile;graph structure;auxiliary;hierarchical approach;partially observable;regularization;social network data;detection methods;network structure;additional information
data analysis;visual analytics;microarray data;microarray data;principle component analysis;interactive visualization;interactive visualization;visual analytics
visualization tool;visual analysis of;multiple datasets;large datasets;large-scale
clustering;data clustering;application domains;clustering process;algorithm selection;knowledge extraction;user-friendly;ensemble-clustering
visual analytics;data mining methods;large number of;data mining;interactive visualization;data mining algorithms;user-interaction;data mining methods
web based;user preferences;product recommendation;collaborative filtering;recommender systems;user opinions;history data;product review;user behaviour;user profiling;user profiles;user navigation;query expansion
clustering;latent topics;classification;collaborative filtering;target function;latent topics;text analysis
clustering;em algorithm;user community;mixture model;mixture model;rating data;clustering approach;variational approximation;original formulation;pattern discovery;pattern discovery;preference data;rating prediction;preference data;user communities
search results;search experience
classification scheme;sentence level;interactive learning;classification;document level;sentence-level;sentence-level;document-level;high accuracy;document-level;classifier
active learning strategies;training data;sampling methods;active learning;active learning;active learning;instance;data sets;information visualization;interactive visualization;takes place;classifier
long term;real-world;case studies;unstructured information;scientific literature
high-dimensional;high-dimensional;label information;low-dimensional space;real-world;embedding methods;pair wise;distance-based;high-dimensionality;dimensional data;dimension reduction;low-dimensional
rates
query language;image data
ranked list;high risk;predictive accuracy;classification;database schema;multiple relations;database;confidential information;instance;predictive performance;multi-relational classification;data mining;relational database;classification task;class attribute
spam detection;association mining;language model;online reviews;term association;detection method;high-order;high-order;baseline methods;language modeling framework;language modeling
opinion mining;recommender systems;information source
data collection;online social networks;social network;database schemas;case study;face book;social networks;data processing;data mining;clustering algorithm;high accuracy;social interaction;text mining techniques;online social networks
user-item;collaborative filtering;rating data;rating matrix;supervised approach;online video;sentiment classification;online video;classification approach;sentiment classification
recommender systems;network connectivity;meta-data;social network;optimization problem;social networks
privacy preserving data publishing;answering queries;privacy preserving;privacy requirements;data publication
case study;face book;user privacy;online social network
mining closed;itemset mining;frequent item sets;frequent item set;database;distributed databases;item sets;privacy preserving;communication cost;formal concept analysis;databases;transaction databases;frequent closed;preserving privacy;privacy preserving
real-world datasets;preserving privacy;weighted graph;graph properties;topological structure;knowledge discovery;graph data;data mining;real-world graphs;online communities;weighted graphs
classification;gaussian mixture model;test data;mixture model;private data;data owners;learned models;gaussian mixture models;privacy preserving;privacy-preserving
equivalence class;privacy concerns;social network;social networks;social networks
classification;high dimensional;space constraints;dimensionality reduction;databases;computational burden;dimensionality reduction;main idea
clustering;web documents;tensor space;real-life;xml documents;xml documents;semi-structured documents;hierarchical structure;content information;document representation;vector space model;clustering
optimization algorithm;optimization algorithm;optimal path;ant colony
clustering;spectral clustering;high-dimensional;real data sets;low-rank;general-purpose;sparse reconstruction;computational cost;linear subspaces;optimization scheme;low-dimensional
credit card;mining process;data mining
clustering;real-world datasets;heuristic algorithms;large datasets;greedy algorithm;local information;objective function;graph clustering;np-hard
domain knowledge;semantic concept;semantic content;query efficiency;domain ontology;content filtering;document filtering;content filtering;knowledge-based;support vector machines
mining results;movement data;pattern analysis;semantic annotation;large-scale datasets;movement patterns;movement data;pattern discovery;pattern discovery;mining algorithm;end user
search terms;commercial search engines;initial query;social media;semantic graphs;query terms
markov models;designed to support;raw data;real data;data collected from;hidden markov models;semantic interpretation
clustering;web navigation;web pages;user interactions;web navigation;semantic properties;web sites;behavior patterns;semantic level;distance function;user sessions;databases;semantic information;collaborative filtering;ontology based;keyword based;recommendation systems
clustering;clustering algorithms;real world;probabilistic latent semantic analysis;unsupervised clustering;data set;label assignment;supervision;probabilistic representation;large scale;semi-supervised clustering;document clustering;pair wise constraints;expectation maximization;optimal parameters;data item;semi-supervised;latent topics;semi-supervised
domain-specific;matrix factorization;svd-based;review process;program committee;sparse matrix
maximum flow;approximation method;fundamental problem;distributed algorithms;similarity based;minimum cost
regularization;encoding scheme;large datasets;linear models;large scale;spatial information;predictive power;linear svms;optimization problem;linear svm;training methods;additional cost;logistic regression;linear classifiers
optimization framework;social network analysis;community detection;objective functions;objective functions;learning problem;community detection;community detection
nonnegative matrix factorization;11;excellent scalability;large-scale;large-scale;matrix factorization;real-world datasets;massive data sets

enterprise search;classification;classification;database;document-centric;knowledge management;feature extraction;feature sets;document search;machine learning;data set;text data;real-world;feature selection methods;structural features
prediction accuracy;classification;rule mining;lazy learning;imbalanced datasets;rule generation;main idea
case study;knowledge-based;data mining;case study;data mining
vector machine;information entropy;word frequency;long term
hybrid model;data mining technique;support vector regression;forecasting model;prediction model;sales data;information technology
nonnegative matrix factorization;spectral clustering;clustering framework;real-life data sets
large-scale;instance;training set;multi-class;active learning;multiple classes;active learning;maximum number of;active learning;large number of;unlabeled examples;binary classifiers;integer programming;document categorization;multi-class;integer programming;multi-class classification problems
classification model;source domain) to;synthetic data;classification;target domain;information filtering;higher-level;large number of;reconstruction error;text classification problems;transfer learning;limited number of;source domain;labeled examples;machine learning;partial observations;feature vectors;higher level
ranking algorithm;agent based;matching algorithm;multi-agent;ranking scores;matching process;solving complex
binary classification;binary classification;regularization;loss function;classification;data sets;loss function;logistic regression;loss functions;neural network;classification algorithms
clustering;clustering results;data points;kernel) matrix;closely related;data sets;similarity matrix;clustering algorithm;adjacency matrix
spectral clustering;spectral clustering;expected error;constrained clustering;pair wise;supervision;semi-supervised;soft constraints;benchmark data sets
social media;community structures;social media;social behavior;social activities;social activities;clustering framework;social networking;overlapping communities;community structure
fixed length;training data;instances;real-world;graph classification;distance measures;learning problems;image annotation
real-world datasets;multiple instance learning;instance labels;instance;multiple instance learning;learning phase;similarity-based;instances;multiple instance learning;similarity-based;classification accuracy;supervised learning;classifier;svm-based
term space;information loss;mining tasks;feature) space
web page;navigation patterns;collaborative filtering;collaborative filtering;graph-based;topic-aware;markov model;real dataset;web page;markov model;web mining;model called;topic-aware;topical relevance;web page;web-page;web users
informative samples;feature space;unlabeled data;sampling methods;classification;active learning;rank learning;sampling techniques;active sampling;active sampling
database;3;16;model called;6
causal discovery;false positives;classification;feature generation;causal discovery
clustering;data mining;data streams;rates;data streams;real-world data sets;resource constraints;usage data
social media;information diffusion;social network;temporal dynamics;social networks;large datasets;blog posts;information diffusion
ensemble learning;ensemble learning;unlabeled data;accuracies;highly competitive;ensemble methods;labeled data;generalization ability;base learners;ensemble method;unlabeled data;semi-supervised;error-prone
domain knowledge;synthetic data sets;search space;relevant features;real data sets;correlated features;voting scheme;search problem;constraint based;subspace clustering;clustering high-dimensional data;clustering algorithm;dimensional data;semi-supervised;constraint based;subspace clustering
high-accuracy;training examples;active learning;low-cost;amazon mechanical turk;learning algorithm;instances;accuracies;learning problem;active learning
training data;unlabeled data;multiple sets of;high quality;large scale;high quality;training dataset;theoretical guarantees;classification performance;training data;learning algorithms;learning framework;real world;data samples
labeled samples;cluster ensembles;data mining;mining data streams;ensemble learning;ensemble framework;large number of;cluster ensemble;class label;prediction models;classifier ensemble;real-world;data streams;unlabeled samples;structure information;label propagation;classifier;label information;weighted average;ensemble classifiers;stream data;data streams;labeling process;building classifiers
parameter-free;similarity measure;graph-based;linear combination;semi-supervised learning algorithms;gaussian kernel;graph-based semi-supervised learning;graph construction;user input;similarity estimation
clustering;computational complexity;user-defined;prior knowledge;data set;purity;user defined;computational cost;theoretical analysis;clustering algorithm;clustering quality
web-based;historical information;graph model;accurately predict;social network;graph model;baseline methods;social influence;social networks;network structure
ensemble techniques;clustering results;partitional clustering;clustering framework;hierarchical clustering;metric distance;clustering methods;consensus) clustering;clustering problems;ensemble clustering;ensemble clustering;hierarchical clustering methods
pre-processing;real networks;semantic analysis;probabilistic graphs;quality function;lower bounds;wide range;biological databases;graph connectivity;data mining algorithms;weighted graphs
kernel machines;classification;conventional methods;gaussian process;input data;data transformation;kernel function;hyper-parameters;bayesian formulation;transformation parameters;complex data;complex data;bayesian framework;kernel methods;kernel approach;regression tasks
scene recognition;scene classification;support vector machines;svm models;classification;step size;linear programming;convergence rate;large number of;line search;gradient method;gradient method;support vector machines;event recognition;artificial intelligence
clustering;distance measure;biological networks;large graphs;random walk;graph clustering;large graph;clustering process;large networks;existing graph;edge weight;sensor networks;social networks;clustering methods;clustering algorithm;graph clustering;clustering quality
source code;data analysis;data mining methods;classification;database;sequential pattern mining;sample data;patterns mined;sample data;candidate generation;sequential pattern mining;low frequency
real-world datasets;search space;mining algorithms;event types;event sequence;level wise;depth first search;episode mining;optimization technique;computationally expensive;frequent episodes;mining process
spatial datasets;domain experts;satellite images;classification;data mining techniques;spatio-temporal;data mining;post-processing;moving objects;mobile devices;knowledge engineers;remote-sensing images;sensor networks;behavior analysis;location-based services;data mining;data preprocessing;temporal data;spatio-temporal
knowledge discovery;drug design;knowledge discovery;data mining;huge volumes of data;machine learning;statistical learning
data mining;high quality;data mining research
lower bound;color information;local color;data mining tasks;higher-level;similarity search;data mining tools;data mining
data set;unlabeled data;classification;classification;clustering methods;semi-supervised learning;clustering approaches;supervision;real-world applications;semi-supervised clustering;instances;data sets;labeled data;pair wise constraints;modeling approach;class labels;semi-supervised
nearest neighbor algorithm;sequential nature;nearest neighbor;multi-core;multi-core;wide range;rank aggregation;dimensional data;efficient approximate;nearest neighbor search
latent factor models;latent factor models;pair wise;user feedback;data set;instance;bayesian framework;information gain;user study;additional information
communication patterns;parallel algorithm;mapreduce-based;partitioning strategy;distributed file system;set-similarity join;pruning strategies;document similarity;document collections;user defined;real world;similarity join
clustering;clustering;database;multiple attributes;traditional clustering algorithms;complex data;dimensional data;real world;application scenarios
communication network;communication networks;rates;communication networks
clustering;clustering algorithms;multi-agent;random walk;random walk on;multi-agent;graph clustering;random walks
synthetic data;large scale;spatial information;temporal patterns;detection method;poisson process;event detection;poisson process;data set;service providers;network traffic;temporal data
unlabeled data;unsupervised learning;unsupervised learning;unsupervised feature selection;cluster ensembles;data sets;clustering performance;feature selection;clustering quality
classification problem;learning task;instances;training instances;class distribution;classifier
traditional collaborative filtering;user preferences;heuristic approaches;collaborative filtering;recommender systems;online social network;active user;social networks;gradient descent;social networks;user similarity;similar users;online social networks
input data;item sets;uncertain data;uncertain database;real data;mining frequent;item set;approximation method;databases;dynamic programming approach
streaming algorithms;frequent patterns;frequent patterns;event sequences;movement patterns;data set;space usage;event sequences
attribute values;classification tasks;adaptive algorithm;cost model;budget constraints;object representation;data mining;computing power;supervised learning;attribute sets;selection algorithm
unsupervised learning;context modeling;data sequences;context-aware;mobile environment;topic models;mobile users;real-world;mobile users;unsupervised techniques
domain experts;tree based;disk access;time series;rates;databases;time-series;search algorithm;video streams
data structure;data collections;large collections of;massive data;time series;web-scale;time series;massive datasets;specifically tailored;diverse domains;image collections
predictive models;markov models;markov models;data set;prediction tasks;high accuracy;sequence classification;higher order
probabilistic framework;conditional random field;multi-core;sequence data;large-scale datasets;crf model;parallel implementation;inference algorithms;sequence data;inference algorithms for;soft constraint;linear chain;conditional random fields
text mining;order statistics;training data;label information;loss function;target domain;jensen-shannon divergence;normal distributions;distance metric;feature representation;source domain to;document classification;statistical learning
regularization;retrieval precision;computational cost;stochastic gradient descent;pair wise;sparse representation;retrieval task;memory requirement;text retrieval;benchmark datasets;learning framework;fast convergence
multi-modal;labeled data is;label sets;databases;satellite images;times faster than
learning process;real-world situations;uniformly-distributed;active learning;active learning;human experts;real-world applications;uncertainty sampling;uniformly distributed;real-world data sets;labelled data;unlabelled data;active learning algorithm
graph mining;classification;classification;high-frequency;graph-based;tf-idf;addressing this problem;supervised learning;ranking model
influential nodes;social network;fast computation;heuristic algorithms;real-world networks;social networks;influence propagation;approximation algorithm;extensive simulations;social networks
specific features;network applications;efficiently computing;massive amounts of;data streams;clustering approach;data collected from;power consumption;sensor networks;sensor network;data streams;sampling rate;clustering approach
spectral clustering;maximum margin clustering;discriminative clustering;prior knowledge;prior distribution;gaussian processes;clustering methods;maximum margin;maximum margin clustering;clustering model;domain-dependent
nonlinear regression;classification;semantically meaningful;data mining applications;target variable;high confidence;global models;high accuracy;neural networks;regression trees;regression problems;multimodal data;local structure;massive data sets;prediction accuracy;global model;gaussian process regression;joint distribution;theoretical analysis;gaussian processes;covariance matrix;real data sets;gaussian process regression;large scale;input space;application domains;linear regression;multimodal data;input variables
ordinal classification;classification;classification trees;original data;training data;data set;data sets;predictive performance;prediction error;data mining;building block for;training samples;nearest neighbour;training sample;loss functions;algorithm to compute;response variable;models trained on
learning process;decision trees;window size;recommendation systems;prediction performance;information filtering;real datasets;learning algorithms;sliding window;naive bayes;instances;supervised machine learning;nearest neighbor;support vector machines;machine learning algorithms
clustering;linear space;data structure
click" model;linear programming;web pages;learning algorithm
parameter values;vector machine;optimization problems;margin;parameter selection;lower bounds;parameter selection;margin
hill climbing;social interactions;problem (called;social network;real-life;viral marketing;social graphs;greedy algorithm;extensive simulations
high-dimensional;nearest;data analysis;dimensional space;random projection;data set;nearest-neighbour;real-world data sets;energy efficiency;theoretical analysis;real world;high dimensional space;nearest neighbour;outlier factor
efficient parallel;micro-array data;real world datasets;memory usage;closed frequent;databases;mining frequent;item sets;sensor data;real-world;numerical data;closed frequent;frequent pattern mining
topic distribution;taking into account;latent dirichlet allocation;dirichlet process;dictionary-based;collapsed gibbs sampling;latent topics;topic structure
consensus clustering;clustering ensembles;object-based;feature-based;optimization problem;counterpart;projective clustering;objective function
clustering;data objects;increasing demand for;distance measure;mixture models;high efficiency;uncertain objects;local minima;clustering methods;data mining;uncertain objects;mixture models;objective function;data management;clustering
clustering;attribute values;clustering results;graph mining;clustering approaches;subgraph mining;large graph;data sources;subspace clustering;graph data;multiple types of;real world;high similarity;subspace clustering
window size;multi-stream;probabilistic model;theoretical analysis;multi-stream;sliding-window
bayesian network;customer relationship management;large-scale;data fusion;knowledge based;latent semantic indexing;data fusion
bayesian network;mining framework;scene classification;bayesian networks;target variables;distance-based;distance metric;bayesian networks
predictive accuracy;recommender systems;latent features;higher-dimensional;matrix factorization;item recommendation;model trained;cold-start;cold-start;additional information
unlabeled instances;base classifiers;unlabeled data;semi-supervised;labeled data is;learning process;semi-supervised learning;classification methods;semi-supervised learning methods;bayesian classifiers;labeled data;graph-based;base classifier;semi-supervised;uci datasets;classifier;learning method;benchmark datasets
benchmark data;search algorithm;globally optimal;categorical attributes;high dimensional data sets;search strategy;effective pruning;dependency analysis;statistical significance;efficient discovery of;lower-bound
probabilistic modeling;missing-values;multi-dimensional;missing-values;latent variables;gaussian process;anomaly detection;predictive distribution;shared information;exponential-family;bayesian inference;exponential family;em algorithm;simulation experiments;missing values
web-sites;demographic information;machine-learning approaches;web-site's;regression models;web-page;web-site;information derived from;prediction models
pruning strategy;training data;predictive accuracy;decision trees;decision tree learning;classification problem;labeled dataset;decision tree learner;classifier
connected components;web graphs;rates;generative model;connected components;real-world graphs
accurately predict;multi-channel
mobility patterns
detection problem;global model;training data;classification;real-life;online learning;online learning;model trained
theoretical framework;synthetic data;instance;classification accuracy;real-world;selected features;feature selection algorithms;margin based;micro array data;knowledge discovery;variance reduction;feature selection;high accuracy;variance reduction;dimensional data;feature relevance;training instances;feature selection
rare classes;gradient method;separability;convex optimization problem;optimization framework
data objects;statistical properties;upper bounds;real-valued;upper bound;clustering algorithm;real world;low-variance
theoretical analysis;random projection;sampling method;efficient retrieval;optimal number of
opinion mining;feature vector;related information;opinion mining;tree kernels;user generated;tree kernels;machine learning algorithms;linear combination;data set;tree kernels;structure information;feature selection;online product reviews;online product reviews;opinion mining;sentiment classification
clustering;anonymization techniques;similarity based;semantic similarity between;greedy algorithm;clustering approach;query-terms;query log;query logs;data utility;similarity based;privacy risks
probabilistic latent semantic analysis;face databases;local features;mixture model;probabilistic latent semantic analysis;hidden structure;visual representations;modeling technique;expectation maximization;clustering tasks
clustering;clustering;validation measures;clustering validation;clustering validation;validation measures;application scenarios
transfer learning;nonnegative matrix factorization;text classification;label information;real data sets;target domain;transfer learning;cross-domain;document clusters;word clusters;sentiment classification;source domain to;cluster analysis
prediction accuracy;structured sparsity;face book;auxiliary;commercial applications;learning framework;real-world;social network;application domains;topological structure;link prediction;multiple sources;social networks;effectively learn;fundamental problem;link prediction;multiple sources;social network analysis;path-based;feature selection
sparse data;input data;computationally hard;factor matrices;decomposition methods;data mining;real-world data sets
private information;aggregation methods;pair wise;statistical databases;information loss
graph classification;search space;evaluation criterion;classification performances;multiple labels;classification;real-world;feature selection methods;mining process;graph classification;vector spaces;feature selection;feature selection;graph data;multi-label;multi-label;real world applications;feature set;single-label
decision diagram;training data;synthetic data;decision diagram;labeled training data;realistic data;classifier;minimum description length;compressed representation;classification problems;classifier
search space;brute-force;dual problem;pruning rules;directed graph;large graph;directed graphs;instance;real-world;real world
text mining;document summarization;multi-document summarization;information-theoretic;multi-document summarization;natural language processing;document summarization;interpretability
detection techniques;historical data;classification techniques;concept-evolution;concept-evolution;outlier detection;data stream classification;data streams;temporal behavior;concept-evolution;data stream;data streams;data streams;probabilistic approach
stochastic search;input data;variable selection;sufficient statistics;large number of;parallel processing;linear regression;stochastic search;large scale;algorithm requires;regression model;dimensional data;aggregate functions;variable selection;linear regression;relational dbms
distance measure;false positives;distance-based outlier detection;distance-based outlier detection;data collections;data cleaning;network monitoring;broad applications;distance-based;detection methods;data type;streaming data
data point;feature space;training set;density-based;training examples;data points;anomaly detection;irrelevant features;information-theoretic;anomaly detection;data sets;semi-supervised
data sharing;parameter values;data analysis;data mining results;real-life datasets;real-life;real-valued;manual tuning;original data;basic properties;missing values;data mining results
hybrid approach;confidence values;training data;input data;detection algorithms;data uncertainty;class label;outlier detection;real life datasets;training dataset;outlier detection;false alarm;kernel k-means;data uncertainty;clustering algorithm;learning framework;classifier;negative examples;detection rate
labeled and unlabeled data;sequence labeling;auxiliary;level features;neural network;labeled data;baseline models;semi-supervised learning;learning framework;supervised learning;transfer learning;conditional random fields
sampling method;association mining;databases;data source;sampling methods;deep web;data mining;deep web;rule mining;web source;data mining;data dissemination;estimation error;data mining problems;mining tasks;random sampling
global model;decision trees;local optima;probabilistic model;learning algorithms;pair wise;large space of;network structure;decision trees;logistic regression;network structure
high quality;markov chain;real world networks
location-based services;mobile phone;recommender systems;large number of;cold-start;mobile phone;location data;identify patterns;mobile users

multiple data sources;kernel methods;svm models;neural networks;kernel matrices;linear combination;machine learning;machine learning models;traditional models;data mining models;multiple kernel learning;multiple kernel learning
graphical models;conditional independence;machine learning;knowledge discovery;data sets;rates;common task;relational data
optimization algorithms;real valued;feature vector;recommender systems;parameter estimation;factor analysis;matrix factorization;model parameters;support vector;input data;support vector machines;expert knowledge;feature vectors;prediction tasks
auxiliary;real world data sets;data mining applications;black box;machine learning;sparse representation;iterative algorithms;learning problem;optimization techniques;multi-task
linear model;large datasets;collaborative filtering;latent features;cold-start;latent features;prediction tasks;linear} model;link prediction;additional features;sample-selection bias
clustering;regularization;feature types;classification;similarity measure;similarity learning;coordinate descent;optimization problem;1;large data sets;edge weight;regularization framework;feature spaces;objective function;relational data;classification algorithm
instance;regularization;instance labels;instances;multi-label learning;real-world;single label;label set;quadratic programming;text categorization;alternating optimization;multi-instance;data set;multi-label learning;scene classification;class labels;multi-instance;real-world applications
probability estimates;numerical experiments;lower bound;linear combination;class membership;binary classifiers;logistic regression;binary classifiers;classification problems
classification;problems require;time series;graphics processing;distance measures;similarity search;case studies;wide range;data mining;lower-bound;field programmable gate
databases;case-based reasoning;decision trees;classifier;data mining
visual analytics;large-scale;np-hard problem;social networks;graph data;approximation algorithm;structured data;weighted graphs
network models;learning algorithm;social network;group structure;generative model;em algorithm;network data
prediction accuracy;taking into account;collaborative filtering;collaborative filtering;simpler models;probabilistic matrix factorization;predictive performance;inference algorithms for;latent factors;great promise;efficient approximate
models learned;theoretical foundation;large corpora;classification accuracy;privacy preserving;topic modeling;real-life data sets;topic modeling
efficient inference;state-space;applications ranging from;inference procedure;traffic control;large number of;instances;inference methods;object tracking
markov models;heuristic search;browsing behavior;data quality;data sources;web site;click stream;real world;markov chains;heuristic search
classifier performance;statistical tests;machine learning;cross-validation;data sets;classifier performance
transaction data;application domain;stream mining;sequential patterns;application domains;data collected from;data stream;wide range;sensor data;data mining;activity patterns
prediction accuracy;tree based;pruning methods;classification approaches;machine learning;decision trees;modeling techniques;classifier;class probabilities
data structure;huge amounts of;clustering process;log analysis;clustering techniques;structural information;computing systems
nonnegative matrix factorization;collaborative filtering;matrix factorization;probabilistic matrix factorization;data mining;building block
clustering;clustering algorithms;matrix decomposition;mutual information;correlated patterns;social tags;minimization problem;social network;supervision;prior knowledge;data set;instance;prior information;semi-supervised;times faster than;internal structure;cluster structures;clustering quality
sample selection;target task;heterogeneous sources;data distributions;text data;data sets;feature spaces;labeled data from;feature spaces;labeled examples;color-histogram;transfer learning;error rate;spectral embedding
customer base;negative examples;collaborative filtering;semi-supervised learning;binary matrix;low-rank;matrix factorization;instances;missing data
dimensional data;spanning trees;cluster structures
clustering;data objects;finding an optimal;real-world;stock market;artificial data;np-complete problem;real life;missing values;monte-carlo
response times;classification;classification;data streams;highly variable;individual objects;classification performance;rates;data streams;wide range
synthetic datasets;subspace clusters;subspace clusters;clustering approaches;statistical techniques;mining process;case study;real-world;dimensional data;subspace clusters;information theoretic measure
clustering;clustering problem;density-based;baseline methods;agglomerative clustering;large-scale;community detection;real-world;spanning tree;density-based;complex networks;clustering algorithm;similarity threshold;user interaction;network clustering;connected components
domain knowledge;feature extraction;clinical data;classification;database;mining temporal;similarity metric;decision support;time series;retrieval tasks;stream processing;data streams;temporal data;similarity measure;patient data
convergence properties;stochastic gradient descent;stochastic gradient descent;general purpose;large datasets
large volume;real-world scenarios;observed data;noisy data;sensor networks;false alarms;meaningful information;cyber-physical systems
real-world;spreading activation;spreading activation;similarity measure
large graphs;real graphs;fast algorithm;specific set of;large graph
clustering;data analysis;basic operations;classification accuracy;real datasets;principal component analysis;real-valued;cluster structure;data mining;statistical significance;cluster structure
mining closed;auxiliary;closed patterns;data describing;post-processing;item sets;discovering patterns;data mining;operator;mining algorithm
clustering;multi-dimensional;density estimation;anomaly detection;data mining tasks;clustering method;multi-dimensional;information retrieval;density-based;distance-based;arbitrary-shape
clustering;clustering algorithm;clustering solution;high quality;data-transformation;probability distributions;entropy measure;information theoretic;dimensional data;objective-function;traditional clustering;semi-supervised;information theoretic;clustering quality
clustering;clustering;kernel-based;kernel-based;learning framework;convergence rate;kernel-based;data point;optimization problem;exhaustive search;large-scale;clustering methods;real-world datasets;local minima;np-hard
topic discovery;feature) selection;document understanding;unsupervised feature selection;document understanding;data clustering;information retrieval;matrix factorization;document clustering;feature subset;feature subset selection;document categorization;matrix factorization
random projection;signal processing;machine learning;basis vectors;real world;sparse linear
window queries;temporal data;temporal data;skewed;sensitive values;temporal data
regularization term;regularization;solution path;boosting framework;semi supervised;parameter selection;regularization;boosting algorithms;objective function
social media;supervised approach;large number of;community-based;high volume
anonymized data;privacy guarantee;probabilistic inference;privacy preserving data publishing;high complexity;probability computation;anonymized data
image retrieval;visual similarity;collaborative learning;similarity measure;visual content;semantic similarity;similarity measure;collaborative learning;retrieval systems;relevance feedback;retrieval performance;unlabeled images;log data;prior knowledge;visual content
frequent patterns;graph-based;long patterns;data mining research;fundamental problem;promising candidates;level-wise;frequent pattern mining;large database;frequent pattern mining
ranking queries;search space;query processing in;uncertain databases;uncertain data;uncertain database;high dimensional;query processing in;pruning methods;query processing;query type;data uncertainty;answer queries;lower dimensional space;synthetic data sets;probability threshold;high dimensional spaces
ranking queries;sampling algorithm;ranking queries;answer set;uncertain data;construction algorithm;query retrieves the;uncertain data;query answering;synthetic data sets;query returns;probability threshold
plan execution;large data streams;database;synthetic data;intermediate results;high quality;real data;plan generation;information retrieval;high efficiency;high scalability;data stream;query processing;keyword query;relational database;high speed;database schema;demand-driven;keyword search on
kernel discriminant analysis;discriminant analysis;regression problems;data points;regularizer;computational cost;large number of;kernel matrix;linear discriminant analysis;input space;regularization;kernel discriminant analysis;training samples;reproducing kernel hilbert space;graph analysis;incremental version;efficient computation;image data;highly nonlinear;separability;dimensionality reduction
database;query-processing;queries efficiently;concept called;keyword-search;result quality;keyword search over;search efficiency;relational databases;ranking mechanism;keyword search;relational database management system;relational data;np-hard
database;partial knowledge;efficient algorithms to;sensitive data;set-valued data;data dimensionality;level-wise;common practice;high cost;real datasets;memory consumption
sensitive attribute;streaming environment;equivalence classes;upper bounds;privacy guarantee;anonymization algorithm;data set;data sources;information quality;sensitive attribute;privacy threat
global solution;large-scale;real-life datasets;entity matching;machine learning;instances;em algorithm;principled framework;entity matching
training data;induction algorithms;regular expressions;large scale;extraction techniques;supervision;information extraction;generic framework;web-scale
random walk;graph mining;graphics processing units;sparse matrices;data representation;representation scheme;graph data;data mining algorithms;power-law;sparse matrix
replication
real data sets;set intersection;database systems;data structures;worst-case;equation;set intersection;information retrieval;linear space
database technology;international conference on;database theory;integer linear programming;business processes;program committee
classification;high-risk;data-mining;decision boundary;predictive model;classification problems
association rule mining;knowledge discovery;pattern discovery;discovered rules;complex relationships;discovered rules
social network analysis;problem remains;link based;common interests
text mining;event extraction;online news
human subjects;sensitive information;classification framework;text documents;user-defined;text data;fine-grained;data sets;text data sets;multi-class
decision support
interestingness measures
dynamical systems;web collection;web forums;anomaly detection;anomaly detection;detection results;data sets;network structure;analyzing data
text mining;web portal;latent dirichlet allocation;social network analysis;social network;dark web;social network analysis;homeland security;virtual communities;social networks
large volume;ranking results;national security;social media;web forums;social media;dark web;content similarity;influential users;ranking algorithms;link analysis techniques;social networking;manual effort
access patterns;access pattern;realistic data;transaction processing
data management;data management;databases
workshop report
database management systems;international conference on;data stream management;knowledge management;medical databases;database;reasoning tasks;information retrieval;click-streams;exploratory search;natural language processing;data streams;artificial intelligence;database systems
database technology;international conference on;high quality;large number of;review process;higher quality;high-quality;program committee
mechanical turk;image segmentation;human computation;optimizer;interactive search;general problem
training set;decision-theory;active learning;real-world;database;user feedback;data quality;learned model;machine learning methods to;user feedback
search results;user-generated;web queries;user location;query logs;data structures;web browsers;web search engines
training examples;classification;database;statistical techniques;incrementally maintaining;relational database management systems;imprecise data;data sets;text processing;statistical technique;optimal strategy
systems require;high throughput;higher throughput;lock;execution model;increasing number of;execution strategies;general-purpose;counterpart;features including;online services;cpu-based;databases;high-throughput;transaction processing;high-throughput;transaction processing
real-world;rfid data;query processing in;distributed settings;location information;query processing;high-level;distributed stream processing system
game theoretic approach;artificial intelligence;machine learning
autonomous navigation;learning algorithm;navigation systems;dynamic environments;reinforcement learning;reinforcement learning;autonomous navigation;decision making
theoretical analysis;learning agents;game theory;learning agents
multi-agent systems;learning agents;multi-agent;learning algorithm;algorithm called;agent systems
support vector machines;ordinal classification;classifier;ordinal classification
classification;text documents;external knowledge;classification methods;text representation;noise reduction;classification approach;centroid-based;training corpus;information loss
learning algorithm;power supply;intelligent control;network structure;operating conditions;control scheme;neural network
artificial neural network;artificial neural network
wind speed;independent component analysis;statistical method;high accuracy;time-series;wind speed
power factor;simulation results;matlab/simulink;control algorithm;simulation model;active power;industrial applications;fuzzy logic;active power;fuzzy logic
artificial neural networks;neural network;high accuracy;neural network

control method;neural network
class information;spatial information;image representation;recognition task;appearance based;recognition accuracy;case study;high-dimensional spaces;feature selection;appearance based;feature selection
multi-class classification;loss function;classification error;loss function;database
clustering algorithms;clustering results;clustering algorithms;complex systems;local models;complex systems;competitive learning;parametric model;control parameters
multi-agent;multi-agent system;observed data;based reasoning;training algorithm
predicting future;learning systems;hidden markov models;imitation learning;imitation learning;imitation learning
parameter space;inverted pendulum;large-scale;normal distributions;learning tasks;model-free;policy gradient;faster convergence;learning method
learning algorithm;selection process;learning algorithm;naive bayes;training process;machine learning;data collected from;data collected;artificial intelligence
genetic programming;sensor data;energy efficiency
correlation coefficient;cross-validation;predictive models;bayesian networks;bayesian network
joint model;auc;feature-selection algorithm;posterior probabilities;variable selection;spatial information;predictive power;cross-validation;bayesian logistic regression;markov chain;selection algorithm;monte carlo
multi-objective;quality measures;learning systems;user-defined;multi-objective
relevant features;classification;irrelevant features;optimization method;multi-objective;feature selection;bayesian networks;feature selection;key features;search algorithm;higher level
ranking techniques;classification models;local optima;data mining applications;real-world;selection method;data sets;feature selection;feature subset;feature selection methods;threshold-based;feature selection techniques
real-world datasets;data points;classification;feature selection;classification;relevant features
feature selection techniques;case study;data set;micro array data;dna microarray data;comparative analysis;feature selection techniques
von mises;selection problem;density estimation;monte carlo;mixture model;von mises;probability distributions over;parameter values;dirichlet process;inference algorithm;ensemble selection;mixture model;clustering problem;markov chain;low-dimensional
automatic segmentation;genetic algorithm;typically performed;genetic algorithm;automatic segmentation;margins
data set;automatic segmentation;prohibitively expensive;data sets;kernel density estimation;computed tomography
classification;ranking method;takes into account;discriminative power;missing values;feature ranking;multiple classifiers;ranking methods;missing values
lower dimensional;base classifiers;simulation study;nearest neighbor;high dimensional feature spaces;design parameters;prediction accuracy;simulation study;naïve bayes;machine learning;base classifiers;rates;simulation results;base learners;sampling rate;high dimensional feature space;high dimensional feature spaces
parameter values;propagation algorithm;active learning algorithm;probabilistic graphical model;information processing;learning tasks;hidden states;simulated data;iterative algorithm;promising candidates;probabilistic graphical models
markov logic networks;learning algorithms;auc;precision-recall;real-world domains;heuristic method;training dataset;structure learning of
object recognition;learning framework;limited number of;database
clustering;matrix factorization;mixture model;binary data;continuous data;data matrix;variational approximation;model-based clustering;continuous data
utility function;nearest neighbor;classification;similarity learning;similarity learning;learning algorithm;cost function;maximum margin;similarity learning
clustering;similarity measure;similarity matrices;unsupervised feature selection;similarity measure;instances;pruning algorithm;document clustering
ad hoc;object detection;local patches;binary features
clustering;clustering algorithms;large volumes of;high dimensional;time series;clustering techniques;subspace clustering;high dimensional;sampling rate;machine learning algorithms
regression method;aggregation methods;feature values;machine learning;algorithms require;support vector
instance selection;classification problems;relevant features;vice versa;instances;feature selection algorithm;instance;classification problems;selection algorithm
unlike earlier;maximum likelihood estimation;transformation matrix;covariance matrix;error rate;classification error
decision trees;bayesian networks
naive implementation;memory requirements;pair wise;selection method;estimation method
real data sets;support vector regression;input data;input data;missing attribute values;nearest neighbor classifier;unseen) data;classifier
face database;stereo images
map-reduce;data points;distance matrix;graphics processing units;parallel processing;distance matrix;scientific computing;large datasets;data intensive
case study;basis functions;neural networks;radial-basis function
conditional entropy;conditional entropy;multiple kernel learning;machine learning problems;linear combination;automatic selection;fisher discriminant analysis;kernel methods;benchmark data sets;multiple kernel learning
collaborative filtering;kernel-based;collaborative filtering;large-scale;low rank;kernel matrix;pair wise;global information;similarity matrix;accurate predictions;local information;classification approach;kernel-based
kernel learning;kernel learning;classification;kernel functions;machine learning;hypothesis testing;classifier;nearest neighbors
clustering;window size;synthetic data;unsupervised clustering;clustering method;density function
neural networks;artificial neural networks;heuristic function;game tree;expert knowledge;neural network
virtual machine;auc;intrusion detection systems;intrusion detection;imbalanced data;feature selection methods;detection rates;feature selection;feature selection;individual features;high-dimensionality;false alarms;imbalanced data;increasingly popular
predictive models;learning algorithms;nearest neighbor;frequent item set;support vector machines;structured data;fixed-length;case study;accurate classifiers;machine learning algorithms;pre-processing;machine learning algorithms;feature vectors;relational data
search process;development process;automatic selection;selection method;plans
process model;local optimization;agent based;formal models;optimization algorithm;large-scale;evolutionary computation;local optimization;complex systems;evolutionary computation;agent-based;data processing;identification method;agent-based
syntactic structures;hybrid approach;support vector machines;semantic network;kernel functions;graph kernels;text corpus;tree kernels;higher accuracy;semantic networks;kernel approach;pattern matching
nearest;clustering results;classification;real-world datasets;similarity-based;constrained clustering;pair wise;cluster assignment;document datasets;similarity-based;pair wise constraints;class prediction;nearest neighbors;high dimensional space
manifold learning;manifold learning;learning algorithm;manifold learning;image data;data set;minimum spanning tree;real world
real-world;prediction accuracy;data streams;sliding window;data streams
regulatory networks;regulatory networks;network connectivity;biological processes;component analysis;simulation data;micro array data;biomedical applications;gene regulatory networks
pair-wise;digital camera;image data;image denoising;similarity matrix;low dimensional
search space;reinforcement learning;dynamically changing;multi-agent systems;reinforcement learning;reinforcement learning;road network;agent-based;fast response
domain adaptation;training data;domain adaptation;linguistic patterns;sentiment classification;natural language processing;highly sensitive;feature vectors;sentiment classification
regularization;vice versa;classification;instance;classification accuracy;large scale;vector machine;classification performance;instance;training instances;similarity matrix;positive definite;learning problems;quadratic optimization;instances;support vectors;training sample
associative memory;associative memory;topological structure;multi-scale;rule based;multi-scale
clustering;hierarchical clustering;real datasets;clustering method;large datasets
linear support vector machines;loss function;independent variables;large number of;vector machine;linear svm;instances;coordinate descent;sparse datasets;class label;sparse datasets;geographical information;cutting-plane algorithm
clustering;prediction accuracy;protein–protein interaction network;classification;vector machine;distance metric;multi-class
statistical models;high-risk;decision tree;vector machine;machine learning techniques;classification approach;tree model
clustering;clustering algorithm
parallel computing;propagation algorithm;linear algebra;learning systems;benchmark data sets;times faster than;artificial neural networks;neural network;training algorithm
multi-core;training process;general-purpose;graphics processing;sparse matrix;support vector machines;machine learning algorithms;sparse matrix
data point;data structure;multi-dimensional;data partitions;space partitioning;brute force;wide range;geometrical constraints;high-dimensional spaces;clustering algorithm;space partitioning;search trees;distance computations;hierarchical clustering methods;brute force’ algorithm
clustering;simulation results;unsupervised learning;temporal information;spatial information;spatio-temporal;neural network;data set;machine learning;pattern classification;handwritten digits;recognition accuracy;high-dimensionality;belief state;image classification;bayesian inference;deep learning
simulated annealing;gene regulatory networks;simulated annealing;power law;network model;gene regulatory networks
continuous functions;function approximation;neural networks
factor analysis;tree induction;induction algorithms;input data;decision tree;factor analysis;machine learning;decision trees;data preprocessing;formal concept analysis;data preprocessing;benchmark datasets;machine learning
classification problem;classification;machine learning
models learned;learning algorithm;human action recognition;labeled data;video sequences;multi-class;classifier
forward selection;data set;feature set;classification;large scale;training examples;vector machine;selected features;cross-validation;feature selection;sparse linear;training algorithm
clustering;clustering;statistical properties;high-frequency;real-world;high-frequency;case study;stock data;stock data
support vector machines;high level;machine learning methods;classification;invariant features;machine learning;logistic regression;classification;supervised learning;neural networks
potential function;facial expression recognition;pattern recognition;face recognition;multi-dimensional data
training set;classification;large-scale;features extracted from;accuracies;manually labeled;sloan digital sky;classification performance;classification results
model complexity;multi-class;simple models;random fields;activity recognition;fmri data;brain images;structure learning;multiple models;magnetic resonance imaging;high-level;low-level;theoretical justification
hidden variable;hidden variables;hidden variables;time series;time series;1;time series;process control;neural network
traffic load;predicting future;resource-constrained;social network;machine learning methods to;network traffic;mobile ad hoc networks
selection problem;spatial information;machine learning techniques;feature generation;machine learning;feature selection;machine learning techniques;access patterns;machine learning;selecting features
desired properties;mutual information;statistical dependence;statistical dependence;variable selection;information theory;correlation coefficient;variable selection;real world
search space;parameter estimation;regression analysis;optimization problem;regression algorithm;object-oriented;training sets;programming language
multiple tasks;multi-task;label set;large scale;data sets;algorithm called;predictive performance;multi-task learning;theoretical analysis;classifier;social context;multi-task
binary classification;prediction accuracy;ensemble methods;accuracies;ensemble methods;binary classification;lower bounds;lower bounds;ensemble methods;accuracies;individual classifiers;ensemble method
statistical methods;regression trees;support vector regression;neural networks;genetic algorithm;learning algorithms;linear regression;data set;environmental conditions;artificial neural network;environmental data;locally weighted
parameter values;reinforcement learning algorithms;communication overhead;computationally hard;learning agents;optimization problems;selection problem;learning algorithms;reinforcement learning;optimization problem;reinforcement learning;stochastic search;optimization problems;locally optimal;memory requirement;simulation results;np-hard
multi-agent;inverse reinforcement learning;multi-agent;weighted sum of;inverse reinforcement learning;multiple agents;reward functions;reward function;apprenticeship learning
learning process;reinforcement learning algorithms;neural networks;optimal policies;network topologies;reinforcement learning;training process;multiple times;powerful tools for;majority voting;local minima;optimal control;neural networks
decision process;highly sensitive;reinforcement learning;programming languages
neural networks;stock market;data preparation;sliding windows;pre-processing;stock market;artificial neural network
clustering;constrained clustering;high-order;clustering accuracy;prior knowledge;matrix factorization;data structures;data/matrices
source text
training data;automatically extracted;related terms;knowledge-based;learning approaches;knowledge-based;naïve bayes;information extraction;manually annotated;annotated data;information retrieval;query expansion;word sense disambiguation
semantic classes;complete set of;entity recognition;automatically discover
relational reinforcement learning;computational complexity;perceptual information;relational reinforcement learning;relational reinforcement learning;state space;intelligent systems;instances;knowledge base
bayesian network;learning algorithm;learning bayesian networks;worst-case;main memory;joint distribution;rates;worst case;bayesian networks;execution times;execution times
classifiers trained on;real world;feature extraction;classification;classification algorithm
evolutionary algorithms;variable ordering;bayesian network structure;genetic algorithm;operator;multi-point;bayesian networks;operator;evolutionary algorithm;network structures
deep belief;multi-layer;anomaly detection;vector machine;anomaly detection;highly variable;large amounts of;data sets;semi-supervised;neural network
ground truth;nearest-neighbor classifier;feature vector;recognition performance;activity recognition;data distributions;classifier;instances;online learning;learning speed;real world;data distribution;learning strategy;activity recognition;classifier
training set;fold cross validation;vector machine;rule set;tag-based
quality control;web interface;statistical analysis;machine learning methods;easy access
computational efficiency;feature set;selection process;selected features;learning method;cross-validation;micro array data;statistical significance;feature selection;feature selection;feature selection methods;statistical significance;individual features;classifier;feature selection;gene selection
representation scheme;position information;hypothesis generation;representation scheme
clustering;clustering algorithm;unlabeled data
clustering;feature space;speech recognition;feature space;speech data;spoken language;clustering
data-driven;incremental learning;relational reinforcement learning;completeness;action rules
high-dimensional;optimal values;problems require;long-term;simulation results;decision making
ground truth;classification;heuristic rules;surveillance video;incremental learning;vision based;learning strategy

learning algorithm;database;classification accuracy;mesh terms;structural svm;valuable information;structural svm;biomedical research
similar objects;predicting future;human relevance judgments;document pairs;human judgments
clustering;clustering;gaussian mixture model;clustering problem;class label;sequential data;random projection;sequential data;mining patterns;clustering procedure
source code;taking into account;large corpora;breast cancer;wide range;entity recognition;online communities
supervised learning techniques;training examples;manually labeled;rule-based;learning algorithm;training data;machine learning;labeled data;feature weights;labeled data;text analysis;text processing;high precision
parallel algorithm;secondary structure;secondary structure;speed-ups;parallel algorithm;input sequence
breadth first search;heuristic algorithm;randomly generated;complex networks;directed graph;database;directed graphs;edges represent;chemical compounds;systems biology;heuristic algorithm;search efficiency;np-hard
labeled data;classifier;classifier performance
prediction model;feature set;document classification;database;classification accuracy;hand-crafted;rule-based;feature selection method;history data;knowledge discovery;chi-square;feature selection;automatically-generated;automatically generate;expert knowledge;prediction task;feature sets
accurate models;microarray data;instance;feature selection method;micro array data;classifier;data set;micro array data;classification accuracies;feature selection method;selection criterion;high dimensionality;classifier
biological networks;classification;breast cancer;vector machine;gene expression;large number of;expression data;euclidean spaces;classifiers trained on;distance measures;biological networks;feature vectors;micro array) data;increasing amounts of;machine learning algorithms;gene expression;classifier;gene expression
growing number of;ensemble method;ensemble methods;nearest neighbor;computational methods;classification;prediction accuracy;protein structures;data set;ensemble method;voting rule;support vector machines;single classifier;fold cross validation
clustering;data point;pattern recognition;dynamic environments;input-output;real-valued;dynamic systems;prior information
ordinal data;multi-class classification;ordinal classification;conventional methods;regression problems;ordinal data;decision tree;consistency constraints;classification models;real data sets;nearest neighbour;global constraints;classification model
kernel matrix;manifold learning;manifold learning;input space;1;kernel principal component analysis;training samples;2;approximation method;low dimensional;6
spectral clustering;matrix decomposition;kernel methods;low-rank
decision makers;segmentation problem;main idea;sensor network
informative samples;informative samples;sample selection;classification problems;real-world data sets;learning scheme;classification problems
statistical approaches;classification;similarity measure;multi-classifier;hybrid method;classification results;multi-classifier
fuzzy model;classification performance;positive effect
intelligent control;fuzzy logic;decision making;genetic algorithm
clustering;clustering;optimization strategy;optimization model;recommendation accuracy;human intervention;clustering approach
clustering;relevant features;continuous variables;categorical data;real datasets;categorical data;correlated features;statistical test;dimensionality reduction
state space;auxiliary;linear models;time series;implementation details;detection methods;error rate
usage patterns;increasing attention;usage patterns;prediction error;real-world
classification;automatically created;dictionary based;machine learning;automatically creating;sentiment classification;algorithm takes;error rate;support vector machines;semi-automatic;additional information
entropy-based;nearest;labeled samples;active learning;fine-grained;classification;nearest-neighbor;feature space;class label;semi supervised;accurate classification;active learning methods;fine-grained;data sets;uncertainty sampling;weighting schemes;active learning;classification accuracy;labeled and unlabeled data;sampling methods
low-quality;classification;imbalanced data;training dataset;cross-validation;instances;data preprocessing;imbalanced data;classifier;filtering algorithm
data point;clustering;classification model;unsupervised learning;batch mode active learning;biometric recognition;batch mode active learning;batch mode active learning;active learning methods;data instances;instances;learning scheme;video camera;real world
classification problem;applications including;prediction performance;smoothing methods;highly skewed;data set;domain-specific;language modeling;real-world;data characteristics;data sparseness;user input
specially designed;classification;rule set;feature selection method;markov blanket;intelligent systems;data analysis;bayesian classifier;comprehensibility;classification method;fuzzy rules;real world;main idea
feature extraction
clustering;clustering;feature space;constrained clustering;feature selection methods;constrained clustering;feature selection;pair wise constraints;clustering methods;similarity-based;music collections;music collections;feature selection
acoustic features
clustering;meaningful clusters;multi-view clustering;visual vocabulary;canonical correlation analysis;visual words;human action recognition;visual words;canonical correlation analysis;human actions;semantic relations;svm classifier
high levels of
social media;online social media;large volumes of;diverse sources;social media;user-generated content
singular value decomposition;scoring methods;trec collections;retrieval accuracy;regularization;matrix factorization;latent semantic indexing;negative result;query expansion;latent semantic indexing
predictive models;classification;ensemble methods;auc;real data;cross-validation;data mining;probability estimation;positive examples
collaborative filtering;low rank;data mining and machine learning;rating matrix;data matrix;missing values
instances;training sets;classifier;class hierarchy;additional information
structured data;automatically extract;finding task
controlled experiments;lessons learned;controlled experiments;web services;world-wide-web
large volume;real-world;machine learning techniques;reduced model;active learning
multimedia data mining;acm sigkdd international conference on;multimedia data mining;international workshop on;knowledge discovery;data mining
clustering;clustering solution;multiple clusterings;international workshop on;data set;data mining;complex data;traditional clustering;multiple clusterings;biological data
knowledge discovery from sensor data;raw data;acm sigkdd;sensor data;data mining;distributed data;homeland security;sensor data is;sensor data is;sensor networks;real world;massive volumes;acm-sigkdd;decision support systems;computational resources;sensor data;data mining;unique characteristics;data collection;remote sensing;knowledge discovery from sensor data;data fusion;mobile devices;international workshop on;knowledge discovery;high-priority;knowledge discovery;common interests
pattern mining;acm sigkdd international conference on;pattern mining;international workshop on;data stream;knowledge discovery;data stream;data mining
workshop report;acm sigkdd
future directions for;acm sigkdd;massive data sets;large-scale
biological networks;graph mining;social science;semi-structured data;lessons learned;statistical relational learning;graphical models;social networks;analyzing data;machine learning and data mining;communication networks
text mining;network analysis;web portal;latent dirichlet allocation;social network analysis;social network;dark web;homeland security;virtual communities;social networks
training data;data mining applications;activity recognition;mobile devices;daily activities;time series;predictive model for;wide range;user activity;data mining;activity recognition
total number of;aggregate queries;emerging applications;worst case;data streams;massive data sets
pattern sets;prior information;databases;pattern set;2, 3;interestingness measures
surveillance systems;tracking results;data sets;traffic monitoring;image sequences;image sequence;video surveillance;appearance variations;object tracking;simulation results
clustering;clustering;illumination conditions;feature extraction;appearance-based;clustering problem;image data sets;image space;image gradient;individual objects;image data;high-dimensional;images acquired
1;illumination conditions;training data;pose variation;linear subspace;face recognition;face recognition;recognition rates;statistical model;shape information;face image;lighting conditions;multiple sources;low-dimensional;wide range
clustering;ad-hoc;numerical analysis;distance metrics;appearance based
fast approximate;markov random field;segmentation methods;image pairs;partial occlusion;motion estimation;motion segmentation;random sampling
principal components analysis;factor analysis;locally linear embedding;learn models;generative model;input images;modeling techniques
regularization;image segmentation;image gradient;spatio-temporal;level set;variational approach;energy functional;image sequence;image domain;motion estimation;conditional probability;variational framework;motion segmentation
information fusion;density function;multiple sources;motion estimation;takes into account
search methods;sampling method;image sequences;specific problem;image matching;cost function;human body;local minima
motion capture;body parts;moving object;articulated objects;moving objects;iterative algorithm;articulated object;video sequences;human body
subset selection;video sequence;maximum variance;subset selection;computational cost;video sequences;support vectors
vector space;statistical analysis;principal component analysis;boundary points;statistical framework;euclidean space
global optimal solution;matching algorithm;energy functions;finding an optimal;local search techniques;wide range;matching problem;natural images
feature points;shape model;shape model;weighted average;bayesian inference;shape analysis;hidden state;pose parameters;bayesian inference;map estimation
data set;statistical properties;edge detection;feature vector;real-world;conditional independence;observed data;feature vectors;prior model;3;generative model;class labels;natural images
word recognition;shape context;object localization;shape matching;edge image;matching method;transformation parameters;real images;shape context;feature based;distance transform
object recognition;success rate;shape context;instances;general problem;test sets;word recognition
clustering;segmentation problem;real-world;weighted graphs;high-level;optimization techniques;graph-theoretic;game theory;programming language
computational efficiency;large numbers of;data structure;level set;data structures;deformable model;real images;deformable models
image sequences;finite element;magnetic resonance images;extended kalman filter;parameter estimates
multi-view stereo
multiple images;input images;segmentation algorithm
principal components analysis;multiple cameras;multi-view;image-based;bayesian approach;bayesian approach;extraction process;class-specific
structured light;ground truth;light sources;high-accuracy;high-complexity;image pairs;data sets;depth maps;stereo algorithm
12;vision applications;trade-offs;field-programmable gate arrays;multi-resolution;stereo vision;video-rate;video rate;design issues
high-level;algorithm relies on;stereo algorithm;multi-resolution
illumination conditions;object recognition;test image;object recognition;random sample;face database;light source
object recognition;monte carlo;database;recognition performance;linear representations;simulated annealing;databases;image classification;classifier
detection algorithms;multimedia applications;context-aware;large number of;context models;major limitation;content-based image retrieval
object recognition;feature vector;shape context;error correcting output codes;benchmark dataset;classifier
distance measure;classification;input image;training sets;local shape;distance measures;object detection;feature spaces;multi-class;detection task;nearest neighbor search
multiple cameras;multi-camera;moving objects;unique characteristics
multiple cameras;unique characteristics;moving objects
high confidence;kalman filter;control method;measurement errors;fixed size
motion tracking;motion model;data association;multiple frames;arbitrarily complex;false alarms;motion models;false-positives
vision problems;deformable objects;object tracking;finite element method;energy minimization
monte carlo;image features;dynamic bayesian networks;hidden states;robust tracking;image sequences;generative model
optimal parameters;face representation;reinforcement learning;database
face recognition;input image;image space;video sequences;partial occlusion;transition probability;principal component analysis;video-based;human faces;low-dimensional
face images;local features;structural features;rich information;facial features;shape model;fine grained;feature distribution;local image;face alignment;statistical models;local minima;face alignment
fixed point;local shape;level set;curve evolution;contour extraction;human face
face recognition;recognition algorithm;face recognition;database
face recognition;unsupervised learning;video sequence;majority voting;image-based;temporal dynamics;hidden markov models;training process;video sequences;hidden markov models;recognition process;databases;video-based;temporal characteristics of
support vector machines;classification;classifier training;expression recognition;linear programming;classification methods;search process;recognition accuracy;case study;feature selection;bayes classifier;ad hoc;margin;classifier
matching techniques;face recognition;classification;image space;distance measures;optical flow;training sample
deformable model;appearance model;active appearance models;adaptive strategy;deformable objects
estimation algorithms;error analysis;error analysis
clustering;similar object;real data;finite mixture;class membership;hidden markov models;simulated data;time-series;expectation-maximization
multiple types of;image sequences;image motion;flow field;motion segmentation
image patches;vehicle motion;real data;estimation process;motion parameters;weighting scheme;motion estimation
synthetic and real images
matching algorithm;optimization algorithm;6;markov random fields;computational cost;moving objects;object segmentation;color segmentation
pattern detection;classification;face detection;input space;databases;real world;test results;classifier
feature space;object classes;classification;object class;vector representation;real data;rates;false alarm;support vector;false alarms;support vector machines;classifier
high-dimensional;face images;importance sampling;parameter learning;monte carlo;maximum entropy model;markov-chain;training process;joint distribution;4;optimization process;information gain
estimation accuracy;maximum likelihood estimation;lower bound;vision problems;wide range;noisy data;optical flow estimation
classification tasks;kullback-leibler;image content;graph nodes;distance measures;specific problem;transition probability;classification task;task-specific;graph representation
vector field;computational complexity;user interaction;distance function;parameter free
ct images;image data;computed tomography;computed tomography;intrinsic parameters;error bounds
medical imaging;magnetic resonance imaging;limited memory;raw data;real data
computational approach;tensor-based;magnetic resonance images;brain images;tensor-based;statistical analysis;operator;mathematical framework;surface shape;laplace-beltrami
maximum likelihood;line correspondences;euclidean space;real data
digital camera;multiple view;estimation technique;eigenvalue problem;6;perspective camera
general case;real images;euclidean structure;closed-form solutions;intrinsic parameters
real data;svd based;perspective images;point correspondences;line correspondences;projection matrix
instance;high accuracy
multiple images


illumination conditions;spatially-varying;input sequence;shape reconstruction
dynamic programming;real data;intensity values;surface reconstruction;image pair;geometric constraints;light source
multiple cameras;surface reconstruction;light sources
window sizes;database;window size;ground truth;stereo correspondence
error-prone;bayesian network;naïve;computational framework;hidden markov models;visual feature;natural language understanding
probabilistic framework;view-based;recognition systems;temporal constraints;observation model;exemplar-based
face images;pose variation;recognition algorithm;classification;face recognition;component analysis;face image;face space;independent component analysis;principal component analysis;pose variations;statistical models;independent components
high-dimensional;data-driven;kullback-leibler;classification;classification framework;image space;kullback-leibler;face detection;linear features;kullback-leibler divergence;classifier
human users;modeling assumptions;training set;unlabeled data;search algorithm for;classification;bayesian network classifiers;bayesian network classifiers;classification performance;facial expression recognition;facial expressions;missing data;training sets;labeled and unlabeled data;naive bayes;perform poorly
graphical models;kernel-based;gaussian distributions;hidden variables;face model;particle filters;potential functions;belief propagation;vision problems;inference algorithms for;filtering methods
algorithm performs;graphical models;latent variables;vision applications;probability distributions;belief propagation;probabilistic models;graphical model;state-space;particle filtering;vision problems;gaussian distribution;real-valued;low-level
segmentation problem;data points;principal component analysis;principal component analysis;linear subspaces;optimization techniques;motion models
image retrieval;learning process;active learning;image database;retrieval performance;theoretical analysis;svm classifier
face recognition;high-dimensional feature spaces;classification;instances;image sequences;image sequence;positive definite;linear subspaces
input image;missing information;natural scenes;automatically infer;real images
variational methods;regularization;local geometry;vector-valued;vector-valued;image processing;image restoration
camera motion;motion information;spatial resolution;motion paths
physics-based;vision systems;single image;image processing;light sources
real images;color segmentation;statistics-based;dimensional space;error prone
multiple views;multi-view;graph cuts;graph cuts;multi-camera;energy functional;scene reconstruction;map estimation;real-world;additional information
feature extraction;graph cuts;visual correspondence;motion sequences;algorithm produces;parameter tuning;feature based
hybrid approach;obtained by applying;real data;sample points;complex objects
image sequences;matching problem;graph cut;stereo matching;vision algorithms
scene structure;energy minimization;input images;graph cuts
spatial relationships;object recognition;image features;multiple-view
order statistics;appearance-based;image features;local appearance;hybrid model;database;high-order;joint distribution;independent component analysis;gaussian mixture models;object detection;statistical test;salient points;joint probability;computationally tractable;modeling method;feature vectors
image points;object recognition;local features;14;multi-scale;local feature;local feature;2;multi-scale;9;feature based;scale invariant feature transform
simulated annealing;object tracking;parameter estimation;image data;model parameters;image matching;evolutionary algorithm;model fitting
major components;computational model;cognitive processes
classification problem;classification;database;classification accuracy;real-world;high reliability;low-cost;digital signal;lighting conditions;vision-based
extracted features;visual recognition;feature selection;image classification;classification problems;natural images;low-level;feature selection
single image;approximation techniques;particle filters;video sequence;importance sampling;variational inference;particle filters;state space;visual tracking;variational inference;filtering techniques;variational approximation;tracking applications;particle filter;contour extraction
probabilistic framework;image gradient;features including;spatial distribution;feature distribution;partial occlusion;general form;spatial distributions;objective function
appearance-based;sampling-based;probabilistic models;multiple view;generative model for;sequential monte carlo;dynamic bayesian network;statistical inference;target tracking
feature selection method;template matching;feature detectors;feature selection;upper bound;selection criterion
appearance model;registration algorithm;gaussian filter;view-based;appearance models;object tracking
shape similarity;discrimination power;shape representation;translation invariant
clustering;hierarchical organization;object recognition;directed graph;search strategy;multi-resolution
takes into account
vector field;theoretical justification;distance function;7
input data;vector field;range images;large number of;data acquisition;linear complexity;surface reconstruction;modeling approach;range data;simulated data
distance measure;object recognition;matching algorithm;graph matching;matching algorithms;vector spaces;graph matching;weighted graphs
stochastic search;probabilistic model;real-world scenes;maximum likelihood estimation;large space of
program committee;databases;high-quality;data management;international workshop on
emerging trends;uncertain data;high quality;international workshop on;edbt/icdt;similarity search;uncertain data;program committee;multimedia data;metric space
conference series;database technology;international conference on;database theory;edbt/icdt;review process;database technology;program committee
pre-defined;approximate answers;multiple data streams;multi-relational data mining;heterogeneous sources;database;multi-relational;aggregate queries;time series;machine learning;data stream;knowledge discovery;multi-dimensional;regression trees;data mining;multi-target;data streams;data streams;model trees;error-bounded
pattern matching;database;data collected;data stream processing;service providers;privacy-preserving;databases;data streams;information disclosure;data transmission;information loss
web databases;web applications;web-users;fine-grained access control;web-based;business logic;database;web application
access control rules;base relations;fine-grained access control;access control;query rewriting;materialized views;automatically generating;databases
service-oriented;domain specific;unstructured data;service-oriented;information extraction
efficient query evaluation;probabilistic xml;query language;query processor;databases;data management system;long-distance
database;conjunctive query;databases;supervision;query answers;key constraints;conjunctive queries;query answering;answer sets
rdf data;linked data;distributed data sources;auxiliary;indexing structures;linked data;efficient querying;graph data;real-world;relational databases;query results;storing data;xml technologies
information systems;business data;business intelligence;international workshop on;business intelligence;data warehousing;data mining;cloud intelligence
conceptual modeling;simulation data;multi-dimensional;traditional databases;data formats;database;massive volumes;query language;time series;database technology;edbt/icdt;service providers;information integration;information services;databases;ad-hoc;array data
scientific applications;scientific applications;relational database systems;query language;science applications;query processing;window-based;user-defined functions
large-scale;database server;scientific applications;relational database systems;microsoft sql server;lessons learned;data warehouses;wide range;data type;microsoft sql server
hybrid approach;multi-dimensional;scientific data;data analytics
data sets;real-life;spatio-temporal;query processing;computationally intensive
software architecture;data model
linked data;database;web data management;international workshop on;semantic web technologies;information space;data management
rewrite rules;database schemas;schema mapping;schema mappings are;data exchange;database research;aggregate queries;query answering;algorithms for computing;high-level;schema mappings;data integration;tuple-generating dependencies
expected number of;completeness;digital content;prior knowledge;web sites;scheduling strategies;great potential;data quality;business analysts;rates;web archives;accurate classifiers;quality measures;data quality;quality measure
large sample;web pages;semantically meaningful;multi-column;large number of;relational tables;domain independent;high accuracy;html tables;missing information;multiple sources
data analysis;decision support;ranking function;probabilistic databases;query processing in;markov networks;optimization problem;large datasets;user preferences;ranking functions
preference relations;computational complexity;skyline queries;negative) examples;preference elicitation;database research;np complete;skyline queries;relational database;preference queries;high accuracy;preference queries;preference relation
complex queries;plans;large-scale;data warehouses;large data sets;data analysis;commercial systems;join queries;concurrent queries;data analytics;execution times
computational power;sensitive information;emerging area;privacy preserving;confidential information;international workshop on;edbt/icdt;individual privacy;data privacy;database;privacy issues;data privacy;information retrieval;similarity join;data anonymization;vast amounts of
set similarity join;similarity threshold;sampling based;similarity join;tf-idf;locality-sensitive-hashing;real-world data sets;locality sensitive hashing;similarity joins;random sampling;similarity join;size estimation
clustering;ground truth;classification;query expansion;related queries;query semantics;search quality;exploratory queries;search engines;result set;keyword query;ambiguous queries;query results;query expansion
problem instances;index tuning;database;soft constraints;index tuning;margin
logical reasoning;text mining;markov logic networks;local search;optimizer;stochastic local search;parallel algorithms;markov logic networks;information extraction;real-world data sets;statistical inference;data intensive
free-form;relational databases;optimization opportunities;traditional database;query optimizations
pruning techniques;integer programming;social relations;np-hard
semantic correspondences;ontology matching;provide evidence;benchmark dataset;ontology alignment;ontology matching
data stores;durability;database;data model;data warehouses
minimal change
information integration;review process;cyber physical systems;international workshop on
mechanical turk;low cost;amazon's mechanical turk;data items;data quality;web page
sensor networks;cyber physical systems
unstructured text;open data;filtering techniques;open data;pattern matching;knowledge base;inference rules
web search;web search engine;user's search;usage information;query logs;search intent;additional information
data collection;traffic information;large-scale;low-cost;traffic monitoring;data quality;traffic management;high accuracy;traffic data
collecting data;linked data;data collection;meta-data;sensor network;real world;semantic information
ranking algorithm;service quality;multiple criteria;web service discovery;web service discovery;web services;analytic hierarchy process;web service;analytic hierarchy process;service-oriented;web services;web service discovery;distributed computing;service discovery;development process;decision making;user request
multiple users;gps data;relational algebra;ranked list;trajectory data;fine grained;location information;real life
matching techniques;labeled data is;data fusion;large number of;automatically created;data extraction;schema matching;instance-level;search engines;training sets;product search;search engine;automatically generate;automated techniques
column-oriented;column-oriented;map-reduce;design choices;replication
database;data management;multi-core
trade-offs;sensitive information;recommendation algorithms;social network;differentially private;lower bounds;privacy preserving;social networks;personalized recommendations;privacy risks;social graph
search results;result diversification;test set;scalability issues;detection method;web search results;unified framework;query results;query refinement
social media;matching algorithms;trade-offs;real-world;web sites;social-media;approximation guarantees;information consumers;large datasets;very large datasets;produce high-quality
relevance feedback;web service discovery;structured documents;focused retrieval;xml mining;data centric;test collections;wide range;evaluation measures;ad hoc
data mining;hong kong;data mining
high quality;large collections of
end users;user interfaces;semantic model;information retrieval;semantic annotations;information access;semantic structure;complex tasks;indexing methods;higher level
information retrieval workshop;ir evaluation;information retrieval workshop
3, 6;international workshop on;detection task;information access;international workshop on
international workshop on
information integration;review process;cyber physical systems;information integration
social media;social media;user behavior;world wide web;international workshop on
world wide web;semantic information;usage data
information retrieval
web pages;related topics;relevant documents;automatically extracting;digital information;search tools;natural language processing techniques;temporal dimension;information retrieval;expert finding;semantic web;search engines;relevant entities;information retrieval evaluation;relevant information;entity retrieval;vector space model
search systems;search task;cognitive states;data collection;web search;semi-structured;real-life;web searches;web searching;search logs;web searching
sentence level;expectation-maximization algorithm;cluster-based;language models;query-independent;smoothing methods;named entities;retrieval method;statistical language models;formal framework;retrieval models;retrieval model;search effectiveness;novelty detection;query-dependent;retrieval methods;mixture models;sentence retrieval;context-based;additional information
large-scale;video content;text queries;retrieval evaluation;log analysis;test collections;visual concepts;automatically generated;retrieval model;user study;search behavior
topical relevance;query-independent;link information;great success;information retrieval;search engines;semantic relatedness;ad hoc search;test collection;relevant documents;link structure;text-based;trec 2004 web track;ad hoc;search results;link graph;web page;ad hoc;web search;trec 2009 web track;link information;semantically related;information derived from;link evidence;search tasks
matching algorithms;manually assigned;language models;large scale;open data;domain-specific;finding optimal;information retrieval;statistical information;semantic web;test collections;retrieval model;information access;access information;optimal performance
database systems;business processes;relational model;database research;business process;business processes;database researchers;query languages;optimization techniques;data management
fast approximate;span multiple;partial information;change detection;applications ranging from;data sets;instance;multi-instance;instances;original data;queries posed;sensor measurements;relative error;resource constraints;low-variance;random sampling
heavy hitters;lower bound;upper bounds;1,2;lower bounds;relative error;data streams;communication complexity;special case;sampling algorithms
private data;sketching techniques;data streaming;dynamic model;lower bounds;internal memory;algorithms rely on;projection matrix
data structure;linear space;query cost;access methods;nearest neighbor search
specific applications;knowledge bases;database;data exchange;query language;incomplete information;instances;wide range;query answering;data exchange;source instances
incomplete information;general concept;structural properties;data exchange;xml queries;incomplete information;instances;general setting;database objects;applications involving
bounded size;database;functions defined;data complexity;combined complexity;real-life;data sources;query answer;databases;answer queries;query languages;correlated attributes;lower bounds
indexing schemes;linear space;block size;external memory;aggregate function
data point;disk block;external memory;database;main memory;external-memory
query cost;data analysis;database;frequent items;indexing techniques;database queries;aggregation queries;linear space;business intelligence;sheer volume of;data analysts;aggregation queries;data distribution
data structure;frequent patterns;selectivity estimation;textual data;error rates;8, 11, 6, 17;data structures;15, 14, 5;databases;answer queries;exact answers
query evaluation;relational algebra;database;simple" queries;aggregate queries;provenance information;aggregation operator;databases;query languages;provenance information
database systems;equivalent query;view maintenance;probabilistic databases;provenance information;query answering;direct computation;data management;provenance information
stream processing systems;privacy-preserving;relational systems;complex event processing;event streams;complex event processing
parameter settings;data item;workflow systems;data items;provenance information
classification;graph data;regular expressions;application domains;combined complexity;instances;graph patterns;graph data;query answering;graph queries;key features;constraint satisfaction problem
approximation algorithms;linear program;database;conjunctive query;conjunctive views;greedy algorithm;approximation ratio;np-hard
plans;partial information;conjunctive queries;data sources;relevant data;web sources;long-term;query answering;query containment;relational data
data analysis;massively parallel;parallel computation;data centers;large datasets;conjunctive queries;query languages
rdf data;web scale;rdf databases;graph structure;database;database applications;query language;data model;data model for;semantic web;distinctive features;web data;open world;web data
massive data streams;memory size;data stream;data stream management systems;databases;computing power
large volumes of;data exchange;electronic documents;regular tree;xml transformations;internal structure
xml documents;edit operations;temporal logic
regular expressions;tree pattern;tree patterns;1;xml tags;automatically generate;np-hard
web data management;rule-based;distributed applications;data model;formally defined;data management
computation model
rewrite rules;rewrite rules;database systems;database;rewriting rules;user query;auxiliary data;search engine
special characteristics;access latency;high scalability;power consumption;file systems;access patterns
algorithm named;flash-based;buffer management;flash-based
high throughput;flash-based;distinguishing feature;data center;log-structured;hash table;real-world;load balancing
join algorithm;input data;multi-core;analysis reveals;main memory;join algorithms;parameter settings;hash table;query optimizers;simple algorithm
queries involving;optimizer;database systems;optimization techniques;query language;query optimization techniques;query optimization techniques;user queries;generate plans;query optimizers;query performance;plans
query operators;complex queries;database systems;database;amazon mechanical turk;search engines;answering queries;traditional database systems;query processing;human input;closed-world
multi-dimensional;input tuples;access methods;execution plans;efficiently computing;join result;skyline query processing;existing database
efficient parallel;multi-dimensional;parallel processing;skyline computation;data set;computationally expensive;skyline queries;optimization strategies;data distribution
query optimization;query rewriting;rewriting queries;data warehouses;graph-based;common patterns;maximally-contained rewritings;answering queries;np-complete;conjunctive queries;data integration
clustering;social security;external knowledge;synthetic datasets;database schema;statistical measures;data types;common properties;schema matching;database;connected components;object oriented;relational databases;database instance;key constraints
information extracted from;life sciences;schema mapping;mapping generation;query logs
computational complexity;schema mapping;schema mappings are;worst-case;np-complete;data exchange;decision problem;source schema;real-life;data examples;automatically generate;schema mappings;data integration
rdf data;benchmark data;real datasets;real data;management systems;resource description framework;integer programming;enterprise data;benchmark dataset;benchmark datasets;data management
rdf data;index structure;upper bounds;probabilistic graphs;rdf graphs;cost model;query cost;effective pruning;pre-computation;graph data;structural information;answer queries;query answering;answering queries;pruning power
search systems;query-logs;user preferences;web search;entity types;real-estate;probabilistic models;structured data sources;web queries are;text queries;keyword-based;log mining;user study;user intent;fundamental problem;faceted search;data source;structured data;search behavior
database operations;database;providing users;query interface;large number of;heterogeneous data;data sources;data management;user-oriented;data integration;query processing
social network;query answers;tabular data;data privacy;privacy-preserving;statistical databases
service provider;sql queries;query processing;data confidentiality;database
query answers;data analysis;synthetic data;data cube;privacy guarantee;decision support;differentially private;sensitive data;fact table;privacy guarantees;data cubes;data cubes;np-hard
multi-dimensional;differentially private;sensitive data;real data;query answers;aggregate queries over;relative error;privacy-preserving;query results;data utility
optimizer;plans;cost-based optimization;fault-tolerance;intra-query;query-processing engine;query plan;operator;cost-based;query plans;fault-tolerant
parallel processing;scientific domains;real data;range-queries;storage-management;user-defined functions;storage manager;storage management;data storage
oltp systems;durability;main-memory;main memory;long-running;database;massively multiplayer online games;rates;application developers;transaction processing
error-prone;data-integrity;database;data corruption;comprehensive evaluation;stored data;data loss;ad hoc
query optimization;search algorithms;optimizer;data processing;trade-offs;optimization problem;optimization opportunities;optimization criterion;optimization framework
open source;database;requires minimal;database architecture;data transfer;databases;database engine;load balancing
multiple databases;virtual machine;database;higher throughput;virtual machines;real-world;database servers;workload-aware;performance degradation;databases;database workloads;resource utilization;data collected from
query execution;prediction model;materialized views;maintenance cost;query traffic;data collections;data structures;service providers;online services;stochastic model
keyword queries;query processing;nearest neighbor;spatial locations;knn queries;textual similarity;textual descriptions;optimization algorithms;spatial proximity;queries efficiently;similar objects;search algorithm;excellent performance;nearest neighbor search
type-ahead search;formal model for;spatial data;location-aware;synthetic datasets;type ahead search;spatial objects;user's location;query string;query processing cost;location-aware;current location;perform poorly;spatial databases;keyword search on
nearest;keyword queries;textual description;spatial web objects;individual objects;np-complete
clustering;pattern-based;temporal relations;resource consumption;refinement process;time series;pruning strategies;time series;hidden markov model;pattern discovery;log data;real datasets
indexing techniques;databases;temporal dimension;temporal behavior;model checking;web services;large repositories of
large graphs;model called;link graph;shortest distance;sensitive information
shortest paths;real data;query processing in;shortest path;road network;databases;theoretical analysis;spatial network
shortest paths;operator;shortest path;dimensional space;spatial databases
sequence alignment;hash-based;high-throughput;data processing;string matching;indexing methods
12;lower bound;22;data sequences;equivalence class;real datasets;cost-aware;density-based;query sequence;distance functions
vice versa;database;data cleaning;real-world;efficient algorithms to;master data;real-life;record matching
search space;database;data residing;parameter settings;data consistency;databases;wide range;integrity constraint
labeling scheme;reachability queries;labeling schemes;real-life;reachability query;relevant data
real dataset;multiple queries;user-generated;prior knowledge;satisfiability problem;query result
hybrid approach;monte carlo;database;probabilistic database;inference algorithm;information extraction;probabilistic inference;markov chain;inference algorithms
filtering algorithms;edit distance;pruning techniques;dictionary-based;edit distance;dictionary-based;unified framework;cosine similarity;entity extraction;token-based
individual records;extracting information from;frequent patterns;automatically extracting;semi-structured;extraction process;high quality;discovery algorithm;information extraction;data records;extraction methods;user intervention;information extraction;extraction methods
web databases;hidden web;database;database schema;attribute domains;hidden databases;databases;theoretical analysis;interface design;web interface
integration systems;keyword queries;semantically meaningful;sql queries;query semantics;databases;query capabilities;keyword queries;database content;instances;inter-dependencies;keyword search over;database;relational databases;typically rely on;web interface;schema elements
end-users;keyword queries;data streams;continuous query;database;databases;query optimization;queries posed;query plan;cost-based;data integration systems;query processor;keyword search;keyword search over;synthetic data sets;ad hoc;data integration;ad hoc queries
nearest;query evaluation;fast algorithm;nearest;worst-case;indexing scheme;xml databases;xml documents;query cost;queries efficiently;operator;keyword search;query returns
ranking" queries;query evaluation;random accesses;ranked queries
stream processing engine;input-output;query modification;performance tradeoffs;general-purpose;continuous queries;continuous queries
window-based;multi-core;higher throughput;field-programmable gate arrays;processing algorithms;window sizes;high-throughput;data processing;query languages;stream joins
high-dimensional;index structures;index structure;comprehensive evaluation;tree data structure;finite domain;high-dimensionality;phase space
search results;ranking scheme;fall short;database;query loads;indexing scheme;efficient indexing;search result;high probability;search quality
program analysis;answering queries;demand-driven;space complexity
data-driven;matching algorithm;constraint satisfaction problem;sql queries;large scale;np-complete;result tuples;applications involve;data-centric;9;query workloads;data management systems;data management;standard database
handle complex;data generation;database;cardinality constraints;databases;data characteristics;data generation
privacy guarantees;sensitive data;sql queries
feature space;classification;vector machine;ranking function;relevance feedback;search engines;kernel space;indexing methods;applications including;rank learning;large data sets;clustering techniques;ranking functions;query processing algorithms;kernel parameters;machine learning methodology;data points;index structure;machine-learned;database;processing algorithms;reference-based;data set;support vector machines;approximate results
clustering;clustering algorithms;discovery algorithms;accuracies;main idea;similarity-based;real networks;graph clustering
clustering;distance computation;clustering results;object-based;instance-based;feature-based;clustering solution;cluster-based;clustering ensembles;ensemble methods;data clustering;optimization problem;objective function;counterpart;projective clustering;algorithms for computing;benchmark datasets;clustering ensembles
real data sets;12;21;large-scale;sampling based;sampling based;communication cost;data sets;sensor networks;approximation error;network size
materialized view;query evaluation;context-sensitive;materialized views;document retrieval;query efficiency;context-sensitive;ranking scores;view-based;data structures;queries efficiently;keyword query;ranking quality
search queries;plans;traditional database;user-defined
search results;data accesses;instances;large-scale;emerging applications;instance;comprehensive evaluation;low-overhead;data access;aware search;aware search;quality guarantees
estimation technique;search engine;search interfaces;search engine's;document corpora;estimation error;query cost;search queries;search engines;search engines;theoretical analysis;search interface;data analytics
ranking queries;query results;ranking queries;scoring functions;user-defined;scoring function;user's preferences
data uncertainty;uncertain data;high quality;large number of;real-life;query processing;query results
parameter space;parameter values;desired properties;probabilistic database;database queries;probabilistic databases;probability distributions;parameter space;simulation framework;enterprise data;efficient optimization
query evaluation;probability values;database systems;efficient query evaluation;incremental evaluation;sensitivity analysis;unified framework;conjunctive queries;probabilistic databases;threshold queries;query processing;uncertain data;high probability;input tuples;query results;aggregation queries;uncertain data management
special characteristics;data cube;data warehouses;decision support;olap queries;data warehousing;online analytical processing;real world data sets;effective tools;relational data;multidimensional space
data residing;fall short;data warehouses;data warehouses;query performance;data freshness;query processing;range scans
data analysis tasks;dimensional space;latent variables;data cube;database;aggregate functions;observed data;data cubes;common practice;real data;data points;bayesian hierarchical;application scenarios
pattern-based;supply chain;execution strategies;data stream;complex event processing;decision making;multi-dimensional;pattern analysis;intermediate results;management systems;olap operations;online analytical processing;optimizer;event sequence;data streams;sequential data;tag-based;real world;abstraction levels;query workloads;stock market;rfid-based;stream processing
distance computation;information loss;search algorithms;high-quality;graph isomorphism;similarity measure;propagation model;similarity search;large networks;similarity search;incomplete knowledge;exact match;np hard;information network;approximate matches;information networks
data structure;reachability queries;transitive closure;bit-vector;large graph;bit vector;compact data structure;data structures and algorithms;path-tree
incremental algorithms;graph patterns;synthetic data;graph pattern matching;matching problem;incremental algorithm;emerging applications;prohibitively expensive;real-life;subgraph isomorphism;social networks;graph pattern matching;frequently updated
association mining;graph structure;strong correlation;online shopping;graph datasets;real-life;highly correlated;social networks;network structure;communication networks
data analysis tasks;randomized algorithm;requires minimal;join algorithms;provide evidence;data flow
join algorithm;database systems;data warehouse;cluster-based;large-scale;high reliability;query engine;join processing;query performance;data warehouse systems;distributed file system;data management system;mapreduce-based
real-life;random walk;monte carlo;graph data;random walks
analytical queries;state space;data analysis;programming model;parallel processing;theoretical analyses;data set;large datasets;real-world;batch processing;data-intensive
high dimensional;candidate pairs;emerging applications;similarity function;similarity search;social networks;search algorithm
high-dimensional;clustering methods;high dimensionality;real-life;lower bounds;multimedia objects;search process;similarity search;efficient retrieval;databases;distance computations;original data;feature vectors;indexing methods
signature schemes;lower bound;database;similarity queries;signature scheme;edit distance;pruning methods;similarity search;query string;similarity query processing;query processing algorithms;dynamic programming
systems require;increasing complexity;data independence;query complexity;database systems;legacy systems;scientific data;scientific applications;general-purpose;data management problems;rates;scientific datasets;data management techniques;data management
years ago;database systems;database;cloud services;data centers;data center;internet scale;web services;real estate;internet-scale
sentiment analysis;business intelligence;increasingly popular
data collection;data feed;large-scale;data feeds;data quality;specification language;data files;data intensive applications;ad hoc;data delivery;data feed
designed to support;design choices;data model;web-scale;database
data stream;incremental algorithms;map-reduce;data processing;disk-based
real data sets;store data;data loss;parallel computing;semi-structured data;data warehouse;data availability;database applications;database;scientific computing;data blocks;data sources;data warehouses;data warehouse;distributed file system;approximate algorithm;network traffic;disk space;parallel dbms;data distribution
batch processing;fundamental properties;data management systems;individual records;large-scale
solid-state;hard disk;sql server
processing units;response times;query results;optimizer;database systems;update processing;highly compressed;main-memory;increasing number of;query performance;data structures;business intelligence;relevant data;data warehouse systems;query performance;data distribution
query execution;data transfers;query optimizer;data warehouse;partitioning strategy;massively parallel;relational operators;relational operations;data movement;compute nodes;data processing;parallel database systems;microsoft sql server;vast amounts of
data integrity;multimedia applications;poor performance;database management;data transformation;content management;oracle database;database cluster;storage engine;oracle database;database;relational data;databases;relational database;data storage;atomicity;unstructured data;data management;database systems
emerging trends;data volume;data warehouses;ibm db;machine learning;log analysis;business intelligence;data mining;data analytics;model building
execution environment;parallel database;open-source;database;storage layer;data warehouse;very large datasets;execution strategies;data warehousing;fault tolerance;queries efficiently;structured data;data analytics
query optimization;data warehouse;sql server;query processing and optimization;query performance;query operators;query processing;sql server;decision support queries;feature based
data visualization;query-processing;real-life;basic concepts;data sets;data processing;column-oriented;interactive visualization
probabilistic modeling;data management problems;selectivity estimation;feature construction;database research;machine learning;probabilistic databases;collective classification;information integration;statistical relational learning;data representations;statistical models;relational data
domain-independent;knowledge bases;web data management;scientific data;text documents;web data management;large collections of;improving web search;html tables;database schemas;structured data
privacy-aware;data analysis;sharing information;private information;privacy concerns;individual users;privacy-preserving;network data;individual users;data management;information networks
large-scale;data providers;copy detection;large-scale;web technologies;information flow;structured data;data integration
query processing;data management;data management;database systems
program analysis;software systems;application domains;information extraction;query processing;optimization techniques;emerging applications;data integration
sql queries;relational dbms;large data sets;data mining research;data set;data structures;data mining tools;data mining system;data mining;user-defined functions;data mining algorithms

node failures;fault-tolerance;web interface;intermediate results
fault tolerance;database servers
control mechanisms;main-memory;transaction processing;fault tolerant;schema information
poor performance;lock;oltp systems;large number of;transaction execution;access patterns;scalability problems;execution engine;modern hardware;processing power;transaction processing
existing indexes;indexing techniques;features including;chemical compounds;6;complex structures;disk-based;real datasets;code base
life sciences;designed specifically for;data exploration;life sciences;daily activities;scientific literature;allowing users to;semantic annotations
visualization tool;user-friendly
analytical queries;web-based;information network analysis;web-based;semi-structured;web communities;database;huge amounts of;information network;web documents;information networks
ad-hoc;query interface;stream query processing;sentiment analysis;data sources;user-generated content;data processing;structured data
web databases;hidden web;database;user experience;mobile devices;databases;user-friendly;user studies;data analytics
data collected from;data services;rank-join;multi-domain;execution engine;data sources;web sources;domain-specific search engines;information space;query engine
archived data;emerging trends;user interests;topic detection;real-time monitoring
news items;semantic properties;natural language processing;machine learning;user interface;distributed architecture;data mining;relational database;real world;ai technologies;data management system
queries posed;query results;provide feedback;data integration
query answering over;ontology-based;rewriting queries;high level language;user queries;data integration systems;automatically detected;query expansion;data integration
data records;data exchange;schema matching;mapping rules;usage information;query logs;query log analysis;data integration
machine learning;high-quality;regular expressions;iterative process;unstructured text;rule-based;extraction rules;information extraction;enterprise applications;development environment;structured information;data provenance;labeled examples;semantic search;data management
xpath queries;query language;data model;xml documents;query processor;query answering
web pages;ad-hoc queries;probabilistic databases;query engine;databases;query answering;web data;structured queries
parameter space;parameter values;parameter optimization;uncertain data;probabilistic database;monte carlo simulation;probability distribution;probabilistic models
query answering;real world;provide answers;database
workflow management systems;continuous data streams;execution model;workflow execution;streaming data;underlying assumption;supply chain management;data sources;data streams;execution engine;multiple sources
human-generated;tuning parameters;high level;amazon's mechanical turk;query interface;user's perspective;data sources;data processing;query processor;relational databases;increasingly complex
statistical properties;database technology;databases;query log;automatic generation of;operator;ad hoc
query language;security analysis;distributed settings;network provenance;distributed systems;network provenance
query formulation;query interfaces;indexing scheme;similarity queries;query processing;data structure called;query languages;data management;visual query
data-driven;3;database
database technology;specific domains;textual information;database;large number of;general-purpose;databases;structured data;data management
web-based;query forms;xml data;semi-structured
document cluster;interpretability;summarization method;summarization methods;language model;document understanding;clustering method;clustering process;document clusters;document datasets;document clustering;clustering methods;document clustering;document-term
clustering;real data sets;space complexity;shortest-path;low-cost;social network;large networks;knowledge discovery;data sets;network structure;analysis tasks;network structure;path tree
medical records;individual privacy;data privacy;privacy-preserving data publishing;benchmark dataset;anonymized data
social media;community structures;user actions;community discovery;real-world;prediction tasks;data collected from;web site;tensor analysis;social networks;baseline methods;aspect model;long term;factorization method;community structure;social context;community structure
ir evaluation;information retrieval;ir research;knowledge creation;ir) research;access information
major search engines;user experience;search experience;ir evaluation;knowledge-based;retrieval systems;1;computational advertising;utility functions;aggregated search;online advertising;web services;user-generated content;2, 3;social networks
retrieval evaluation;user feedback;retrieval quality;long-term;information retrieval systems;short-term
training data;global information;classification;large-scale;hierarchical text classification;individual nodes;global models;text classification
clustering;web documents;web pages;social annotations;high quality;topic-specific;original document;web document;web page;model called
structural relationships;text documents;text categorization;brute-force algorithm;classifier;selection algorithm
topic aware;user profile;recommender systems;recommendation algorithms;collaborative tagging;topic aware;tag-based;tag based
free-text;automatically extracting;compare favorably with;user opinions;product review;machine learning;joint model;sentiment analysis;product features;customer reviews
collaborative question answering;recommendation systems;recommender systems;large-scale;question answering systems;answer questions;contextual factors
web search queries;web search engine
link-based;semantically related;query-independent;link structure;link information;main findings;category structure;underlying assumption;ranking algorithms;link-based;implicit assumption;ad hoc retrieval;query-dependent;link evidence;semantic relatedness
search results;memory footprint;cache performance;search engines;computational overhead;search engine;hit rate
search results;query-logs;web collection;meta-search;ir research;privacy issues;search engine;query expansion
limited resources
search results;amazon's mechanical turk;aggregated search;web search results;search services;user study
low cost;amazon mechanical turk;relevance assessment;relevance judgments;user interface;document pairs
higher quality;label quality;search engines;search engine
fast algorithm;similar documents;short documents
graph structure;user-item;nearest neighbor;online social networks;collaborative filtering;large-scale;data sparsity;relevant content;recommending items;filtering techniques;user-generated content;link prediction;user-generated content;prediction algorithms;recommendation algorithms
social media;classification;social media;classifier performance;topic classification;topic classification
multiple views;social media;case study;news sources
machine translation;latent dirichlet allocation;parameter estimation;variational bayes;information extracted from;sentiment analysis;cross-lingual;sentiment classification;weakly-supervised;support vector machines
selection criterion;training data;cross-lingual;feature selection;relevance ranking
similar queries;target language;cross-language;relevant content;search problem;query log;source language;issue queries;retrieval effectiveness
click models;data sets;implicit feedback;online learning;retrieval performance;retrieval systems
entity ranking;web search;free-text;category structure;relevance feedback;pseudo relevance feedback;category information;relevant entities;general problem;structured data
trec data;ad hoc;retrieval effectiveness;query reformulation;information retrieval
discriminative training;expectation maximization algorithm;discriminative models;hidden markov model;support vector;error reduction;support vector machines;learning framework;conditional random fields;semantic annotations;natural language understanding
distributed algorithm for;data collections;collection size;latent semantic analysis;latent semantic analysis;matrix decomposition
weighting function;collaborative filtering;collaborative filtering;information retrieval;information retrieval;prediction tasks;ir) models;retrieval methods;ir models;similar users;ranking tasks
discriminative power;information filtering;text representation
image retrieval;data structure;semantic content;visual content;visual cues;locality-sensitive hashing;relevance feedback;nn search;digital libraries;high dimensional spaces
image retrieval;global features;content-based image retrieval;large databases;databases
social media;lda model;content analysis;topic models;topic modeling;text mining techniques;information source
web search;language models;document retrieval;language model;smoothing methods;data set;retrieval task;retrieval accuracy;language modeling framework;unique characteristics
retrieval model;query expansion;ir methods;language modeling approach
10;sentiment analysis;classification errors;supervision;fine-grained;machine-learning;document-level;coarse-grained;conditional random fields;prediction models
local context;user query;statistical method;documents retrieved;document expansion;global context;combined method;query expansion;biomedical information retrieval
vertical search;clickthrough data;improving web search;graph analysis;clickthrough data;aggregated search;limited number of;relevant information
search results;enterprise search;large distributed;summarization task;learning algorithm;search engine
reinforcement learning;classification process;small training sets;training sets;decision process;text classification;text classification;markov decision process
domain adaptation;concept space;unlabeled data;labeled features;domain adaptation;target domain;text categorization;supervision;supervised methods;source domain
relevance feedback;query expansion;retrieval tasks;performance gains
ranking techniques;user-generated content;user evaluation
simple algorithm;statistical properties;topical relevance;effectively identify
commercial search engines;major search engines;web search engine;large scale;related queries;navigational queries;instance;retrieval performance;web search results;search quality;quantitative analysis;dynamic nature of
large data sets;power law;data sets;sampling process;power-law
web search;web search engine;result page;search interface;search engine;web search engines
web pages;theoretical foundation;ad-hoc;linear combination;textual features;query semantics;link-based;retrieval function;result set;result pages
ranking algorithm;retrieval performance;search engine's
search results;hybrid approach;search engines;query processing;result pages
ranking approaches;trec test collections;document ranking;learned model;expert search;instance;final ranking;ranking tasks;learning models;relevant expertise
text-queries;pruning method;query processing;document collection
query times;language models;document-centric;inverted lists;pruning methods;term frequency;statistical significance;hypothesis testing;term-based;retrieval effectiveness
data structure;compression methods;search performance;indexing techniques;block-based;information retrieval systems;block-based;cost model;web search engines;inverted lists
source-domain;relevance ranking;target domain;source domain;cross-domain;instance;ranking algorithms;loss functions
collection statistics;logistic regression model;user study;finding task;contextual factors
test set;expert finding;ranked list;real-world;social network;expert finding;social graphs;search engine;user-oriented
relevance feedback;end-users;short queries;test collections;retrieval results
search systems;evaluation methodology;user interactions;query suggestions;search engine;adaptive algorithms;query modification;search engines;query logs
natural-language;video search;relevant content;retrieval techniques;difficult queries;classifier;natural language
syntactic structures;named entities;passage retrieval;retrieval models;question answering systems;question answering
user preferences;segmentation results;supervised learning;text segmentation;language modeling;document set
relevance scores;relevance score;random walk;data set;ranking algorithms;graph based
context information;accuracies;context-dependent;computation cost;word sense disambiguation
main memory;information retrieval systems;scoring functions;text retrieval
free-text;text search;text queries;web forms;user study;search tasks
cross-language;user behaviour;log analysis;query logs;long-term;log data
text corpora;text collections;specific set of;configuration parameters;large-scale
machine translation;designed specifically for;computational power;cross language information retrieval;information retrieval;retrieval quality;pre-processing;training corpus;training phase
spatial proximity;local features;spatial relations;visual words;video database;visual words;content based video;video retrieval;increasing attention;video retrieval
user study;readability;automatically identifying
classifier ensemble;classification;data fusion;multiple classifiers;information retrieval;supervised machine learning;machine learning;classifier ensembles;result set;information retrieval
user-item;multiple modalities;collaborative filtering
recommendation approaches;naïve;great potential;social-trust;social trust;social trust;rating prediction;social trust
semantic indexing;video indexing;ranking method
target language;query translation;cross-language information retrieval
document relevance;document ranking;document length;information retrieval;original formulation;retrieved documents
ad-hoc retrieval;information retrieval;data sets;retrieval model;quantum theory;retrieved documents;probability ranking principle
prior-art;ir) techniques;retrieval effectiveness;search technique;information retrieval
quantum theory;theoretical foundation
temporal information;user's interests;information retrieval;temporal context;temporal evolution;latent topics
collaborative recommendation;data sets;kernel-based;collaborative filtering
ranking functions;takes place;training sets;visual features
data-set
instance;query reformulation;search session;processing queries
theoretical framework;user study;test collection
score-based
combination methods;test collections;wide range;feature selection;feature selection methods;feature selection
temporal relations;document content;graphical interface
web pages;web search engine;search result;web search results;search engine;retrieval engine;end user
interface design;search engines;web search;current web;information systems
web information retrieval;social network;search process
correctly identify
domain knowledge;textual data;body parts;information retrieval;cross-media;ir) systems;recognition problem;search interface;health information;visual exploration of
additive noise;collected data;sensitive data;privacy preserving data mining
data mining;learning algorithms;preserving privacy;data mining
database;binary class;privacy preserving;classifier;distributed environment
association rule mining;databases;quality function;data mining;association rules;rule discovery;distributed data;multi-party
network analysis;intrusion detection;vector space;network applications;network intrusion detection;forensic analysis;security applications;network traffic;automatically extracts
classification techniques;recommender systems;collaborative filtering;recommender systems;classification algorithms
graph structures;attack detection;large-scale;graph-based;network monitoring;automatically identifies;benchmark datasets;attack detection
growing number of;low-cost;classifier;instance;machine learning;classifier
regularization term;loss function;classification;large margin;data instances;large margin;theoretical analysis;upper bound;increasing amounts of;data repositories;classifier;personal information
data matrices;privacy preserving;proposed protocol;secure multi-party computation;semi-honest
content-based filtering;content-based filtering;rule-based;machine learning;social networks;classifier;social networks
response times;web pages;query optimization;query workloads;index selection;query processing;data repositories;million edges;index size;random walks
line segment;nearest neighbor;data points;synthetic data sets;data-partitioning;spatial queries;data set;query processing in;effective pruning;query processing;nearest neighbor query;unique characteristics;dimensional space;query result;query returns;spatial databases
text document;computational complexity;classification algorithm;link mining
real-world datasets;web service;semi-) automatically;computationally expensive;instance;data sources;business processes;event correlation;web services;event correlation
event streams;data distributions;tree structure;parameter settings;efficient retrieval;future events
access control rules;access control;access controls;xml queries;post-processing;emerging applications;deterministic finite automata;pre-processing
web-based;desired properties;multi-tier;web applications;databases;application programs
automatically extracted from;database;class label;formal model for;6;class labels
poor performance;database;data locality;main memory;query performance;query processing;query plans;modern hardware;hand-written;execution plans
estimation accuracy;uncertain graphs;synthetic datasets;network applications;user-defined;sampling process;fundamental problem;sampling scheme
service provider;cost-aware;cloud computing environment;computing systems;scheduling algorithm;cost-based;aware query;piecewise linear
related data;large-scale;data partitioning;data partitions;data analytics;relational database;data placement;fault tolerance
query execution;database;main-memory;index construction;indexing techniques;adaptive indexing;index structures;adaptive indexing
large scale data mining;data collection;large-scale;large scale;machine learning algorithms;data mining and machine learning;data mining;distributed processing;performance gains;data sources;business processes;large-scale data mining;machine learning techniques;machine learning;machine learning and data mining
efficient parallel;real-world datasets;web pages;linear regression;search algorithm for;graphics processing units;segmentation techniques;parallel computation;graphics processing;data sizes;query loads;search engines;query processing;parallel structure;index compression;web search engines
rdf data;search algorithms;sparql queries;frequent updates;large graph;pruning rules;query processing;graph-based;relational database;subgraph matching;maintenance algorithm
resource sharing;large numbers of;low cost;database systems;database;data migration;transaction execution;serializability;databases;relational databases;persistent database;load balancing
nn search;point sets;nearest neighbor;linear scan;similarity measure between
graph connectivity;sensitive information;social networks;graph queries;query results;path-based
wireless communication;data mining;health monitoring;cost effective;international workshop on
domain experts;data mining approaches;static data;raw data;acm sigkdd;data mining;high end;distributed data;knowledge discovery;homeland security;sensor data is;knowledge discovery;sensor networks;data streams;applications including;massive volumes;problems require;decision support systems;data mining;data fusion;knowledge discovery from sensor data;mobile devices;international workshop on;knowledge discovery from sensor data;high-priority;long-term
statistical properties;theoretical framework;entity types;database;real-world;average case;query interfaces;data items;ranking quality;schema free;databases;real-world data sets;information-theoretic;schema free
keyword queries;language model;query suggestion;xml data;query suggestions;large-scale;query semantics;user queries;xml document;search experience;real datasets;keyword search on
keyword query;xml data;keyword search over;keyword search
regular expression;relational query processing;optimizer;regular expressions;selectivity estimation;language models;real-world;cost estimation;natural language processing;join operators;text data;query plan;text corpus;query plans;selectivity estimation;processing queries
multi-level;multi-dimensional;multi-dimensional data;flash storage;data management systems;indexing techniques;database;access path;tree index;relational databases;disk storage;concurrency control;file systems;data organization
database;query optimizer;query optimizer;sql queries
object databases;xml processing;database management
clustering;graph-structured data;evolving data;keyword-based search;xml data;data streams;query language;learning curve;query processing;databases;ranking functions;search quality;query result;relational data;keyword-based search
dynamic programming;join queries;random variables;synthetic datasets;uncertain attributes;database applications;uncertain data;multidimensional space;algorithm called;real datasets;uncertain data;join result;probability theory;join queries;query processing algorithms;similarity join;join operations
individual records;query execution times;real-life datasets;information retrieval;algorithms require;interval-based;keyword search
multiple criteria;dimensional space;minimum set of;skyline computation;scoring functions;theoretical analysis;expected utility;utility functions;multi-dimensional;np-complete;skyline operator;uncertain objects;uncertain data;applications involving;decision making
data-driven;synthetic data sets;data cleaning;real-world;multi-type;data quality problems;data sets;domain-specific;complex patterns
data-driven;hypothesis testing;scientific discovery;additional information;hypothesis testing;frequent pattern mining
data mining applications;data stream;push-based;high-level;query optimization;pull-based;online mining;instance;definition language;data stream management systems;data streams;lower-level;data stream mining;data structures;data stream management system;mining tasks;mining algorithms;database management systems;fault-tolerance;user-defined;retrieval methods;knowledge discovery;application development;continuous queries
low-quality;ground truth;modeling task;synthetic data;feature extraction;cluster membership;knowledge transfer;knowledge transfer;auxiliary data;predictive performance;feature values;data mining;real world;learning task;auxiliary
data analysis tasks;visual analytics;data visualization;scientific data;scientific data management;social networks;visual analytics;data-intensive
proximity measures;shortest path;road network;continuous query;pre-computation
spatial locations;databases;real data;spatial queries;road network;road networks;unified framework;query processing algorithms
parallel database;database systems;database servers;business intelligence;performance gains;database engines;transaction processing;parallel execution;database systems
query optimization;query rewriting;optimization methods;completeness;database;cost-effective;conjunctive query;database research;equivalent query;query rewriting;description logics;rewriting algorithm;description logic;conjunctive queries;relational database
massively multiplayer online games;virtual worlds;fundamental problem;databases
np-complete;high quality;online services;scoring functions;iterative process
hash-based;pruning rules;spatio-temporal;candidate pairs;real-time monitoring;large collections of;join predicates;space usage;real data;spatio-temporal;data management applications
maximum number of;real world;space partitioning;lower bounds;nearest
network bandwidth;query planning;physical resources;distributed stream processing system;resource allocation
query execution;data streams;low-cost;optimal algorithms;efficiently computing;data sets;cost model;query plan;real-life;data stream;rates;query plans;aggregation queries over;data distribution
join tree;join operations;finding an optimal;cost-based query optimizer;theoretical result;join (tree
feature space;web pages;retrieval accuracy;ir techniques;social tagging;retrieval models;tag-based
counterpart;completeness;reachability queries;regular expressions;pattern queries;subgraph isomorphism;emerging applications;regular expression;real-life;graph patterns;social networks;graph pattern matching;synthetic data
massive graphs;real networks;online social networks;large-scale;main memory;large networks;algorithm requires;random access;real-world networks;external-memory;np-hard
features extracted from;search results;search engine;web users
synthetic data;real-world;parallel computation;web-scale;massive datasets;real world;mapreduce based
storage management;log records;original data;database systems;database
buffer space;external memory;record linkage;data cleaning;inverted lists;queries efficiently;tree-based;candidate matches;large data sets;query string;cost-based;adaptive algorithm;answer queries;disk-based;real datasets;storing data;approximate string
markov chain;location-based services
viral marketing;social networks;social networks
sensitive information;random perturbation;wide range;social networks;network data;information theoretic
monte carlo;database systems;database;uncertain data;probabilistic graphical model;inference algorithm;markov random fields;query processing;real world;array data
resource sharing;data center;distributed processing;computing resources;physical resources;highly scalable
predictive models;database systems;business model;cost-aware;database;cloud computing environment;resource management;real-life;learned model;machine learning techniques;cost-effective;margins;resource allocation;main components
data sharing;efficient storage;database;main-memory;database schema;master data;enterprise applications;databases
multi-modal;query optimization;long-running queries;plans;large volumes of;optimization algorithm;semantic query optimization;real data;logical level;query plan;stream processing;query execution times;stream data;data stream management systems;performance gains;optimization techniques;resource utilization
false positives;dynamic programming;higher throughput;publish-subscribe systems;massively parallel;user queries;xml-based;xml stream;path-based
selectivity estimation;structured databases;dimensional space;applications including;selectivity estimation;twig queries;directed acyclic graphs;estimation error;semantic web;graph data;reachability queries
multiple views;xquery queries;tree patterns
cardinality estimates;query optimization;rdf databases;open-source;database;cardinality estimation;relational database;relational dbmss;estimation methods
join query;applications including;multiple relations;multi-objective;performance gain;multi-criteria decision-making;join result;data preparation;meta data;evaluation techniques;databases;operator;preference queries;preference queries;join operator
complex event;execution strategies;business intelligence;shared execution;increasingly complex;clustering;supply chain management;monitoring applications;memory consumption;subexpressions;high-speed;complex queries;optimizer;real-world;rewriting rules;complex event processing;pattern detection;applications ranging from;pattern queries;stock market;query language;query processing strategy;query processing;event streams;query workloads;normal form
clustering;synthetic data sets;storage overhead;anomaly detection;outlier detection;real-life;data mining task;distance-based;data streams;sliding windows;data arrive
sensor readings;query processing;dynamic programming algorithm;wireless sensor network
reachability queries;efficient storage;transitive closure;directed graph;reachability query;spanning trees;share common;real world graphs;computational cost;graph databases
preference queries;optimization techniques
distance functions;external memory;scoring functions;scoring function;memory requirement;theoretical analysis;multidimensional space
case studies;real datasets;mining algorithm;sequence database
real datasets;multi-criteria decision making
database design;real-world;traffic monitoring;query performance;fine-grained;query workload;databases;data-intensive
data-driven;database;computation cost;digital information;web sites;deep web;algorithm called;real-world applications;business analysts;query processing;real-world;query results;structured data;excellent performance;continuous queries
high efficiency;pruning techniques;database;signature schemes;similarity functions;similarity metrics;token-based;cosine similarity;similarity join;result quality
real data sets;historical data;data warehouses;optimal plan;large number of;stored information;business intelligence;disk based;memory space;disk-resident;decision making;data distribution
multidimensional data;distance functions;distance function;dimension hierarchies;shortest path;data cubes;multidimensional space;data-cubes;user study;similarity measures
memory management;information systems;multiple queries;resource consumption;replication;data warehouse;main-memory;main memory;query response times;database;data freshness;rates;query sessions;online analytical processing;transactional data;online transaction processing
search space;mining algorithms;taking into account;upper-bounds;subgraph mining;search history;greedy algorithm
data objects;spectral clustering;web applications;real-world;multiple data sources;supply chain management;large data sets;data sets;fast algorithm;social networks;clustering algorithm;real world
clustering;spectral clustering;computational complexity;high computational complexity;high-quality;data points;large-scale;real world datasets;large number of;spectral analysis;matrix factorization;large-scale data analysis;similarity matrix;clustering methods;computational cost;clustering problems;spectral analysis;data management applications
real data sets;graph mining;sampling algorithm;massive graphs;community detection;data reduction;data set;data sets;web mining;dimensionality reduction;graph data;database applications;dimensionality reduction methods;limited number of;social networking;disk-resident;numerous applications in;massive data sets
active learning;sampling based;relational databases;query interface;databases;deep web;streaming data;frequent itemset mining;data sources;data mining;data dissemination
open-source;linear algebra;higher-level;machine learning;building blocks for;massive datasets;programming paradigm;optimization strategies;cluster sizes;low-level
mining large graphs;parallel algorithm;social network;web graphs;fixed point;fixed point
real-world datasets;estimation technique;cost estimation;plan selection;worst-case;event processing;cost estimation;long-running;data center;data sources;operator;estimation techniques;event-based
monte carlo;theoretical properties;np-complete;linear programming;greedy algorithm;optimization problem;publish/subscribe system;approximation algorithm;computationally feasible
random projection;existing indexes;nearest neighbor;hash functions;multimedia objects;high probability;search algorithm;similar objects;nn) search;nn search;locality sensitive hashing;sequential scan;multimedia database;false positives;nn queries;data points;index structure;linear scan;efficient retrieval of;high dimensional space;query result;index structures;tree structure;hash table;multimedia data;query point;retrieval applications
biological networks;completeness;database;critical task;large databases;subgraph isomorphism;instances;visual analysis of;chemical compounds;subgraph queries;databases;query pattern;real world;graph indexing
network bandwidth;spatial data;current commercial;spatial analysis;database;spatial features;spatial database;database;information management;database operations;risk analysis;databases;relational databases;transaction processing;open source;database performance;processing power;open-source
open source;input data;data-intensive;data-intensive applications;operator;end user
data objects;index structure called;spatial objects;index structure;spatial queries;query processing in;power consumption;query processing;sat-based;simulation results
web based;data volume;large scale;data extraction;ad-hoc;designed to support;large-scale data analysis;analytical processing;data freshness;data intensive;individual users;data management system;data storage
probability distribution;probabilistic model;single attribute;inference algorithm;real-world applications;probabilistic databases;probability distributions;missing data;high accuracy;multi-attribute;gibbs sampling;missing values
probabilistic databases;relational algebra;efficient query evaluation
tuple-level;mathematical models;probabilistic database;time series;probabilistic databases;probability distributions;wide range;information derived from;time-series;probability values;real datasets
result diversification;running times;candidate set;result set;query results;result quality
human effort;data generation;sql queries;complex query;constraint-based;operator;test data
multi-dimensional;mining algorithm;sequential pattern mining;data cube;data warehouse
production systems;data analysis;mapreduce-based;efficient storage;data warehouse;space utilization;user behavior;social network;highly dynamic;service providers;data warehouse systems;data processing;databases;distributed systems;data placement;data analytics;query processing
web scale;web pages;high-precision;web sites;web-scale;information extraction;template-based;web site;extraction rules;million records
dimensional space;uncertain databases;uncertain data;uncertain database;uncertain attributes;similarity queries;uncertain objects;database objects;random variable;wide range;density functions
real data sets;sql queries;query suggestion;keyword-based;algorithm to compute;template based;databases;relational databases;user-friendly;ranking model;query keywords
score function;keyword queries;directed graph;valuable information;real dataset;edges represent;nodes represent;data driven;real datasets;databases;keyword query;keyword search;aggregate function
query execution engine;service calls;multiple queries;real-life applications;database;web service;transformation rules;performance gains
high-speed;log records;database systems;database;main memory;durability;disk-based;input/output;disk based;hard disk;atomicity;transaction processing
ad-hoc queries;query workloads;database queries;database systems;database administrators
continuous queries over;streaming applications;user defined;future directions for;diverse range of;long-running;user-defined;broader range;stream processing;domain expertise;event streams;operator;microsoft sql server
relational databases;relational databases
storage engine;database;sql server;microsoft sql server;replication;relational database system;high availability;microsoft sql server
response times;database;database instance;clustering framework;databases;database performance
clustering;skyline points;threshold-based;skyline" points;database
outlier detection;graph streams;mobile computing;sampling method;structural summaries;outlier detection;social networks;high volume
ranking scheme;distance-based outlier detection;database;outlier detection;high dimensional data sets;data set;outlier detection;efficient discovery of;distance-based;locality sensitive hashing
real data;synthetic data;instances;outlier detection;uncertain data;inference algorithm;object level;outlier detection;detect outliers;uncertain objects;approximation algorithm;probability density;instance level;filtering algorithm
ranking approaches;synthetic data;outlier ranking;outlier mining;local neighborhood;analysis task;detecting outliers;outlier mining
web directories
query rewriting;fall short;highly heterogeneous;data dependencies;conjunctive queries;human experts;data integration systems;automatic generation of;query answering;design process;source schemas;data integration;tuple-generating dependencies
query execution;query rewrite;database;static analysis;optimization opportunities;database access
finding similar;large-scale;spatio-temporal;storage model;query response times;instance;query processing;relevant answers;query processor;answer queries;location data;finding similar;search applications;similarity measures
complex event;field-programmable gate arrays;1;click stream;cpu-based;event streams;pattern matching
data-distribution;social networking;knowledge base;social-network
synthetic data;database;cost model;data quality;real dataset;case study;data sources;business rules;large datasets;functional dependencies
data dependencies;maximum number of;efficient approximation;semantic query optimization;heterogeneous data;real data;instance;databases;functional dependencies;np-hard
open source;database management systems;database;serializable;2;5;4;serializability;9;concurrency control;serializable
high level;movement patterns;spatial resolution;multiple factors;road networks;spatial constraints;road network;lower-bound;location privacy;mobile users;higher level
data analysis;data processing;workflow systems;data collections;scientific domains;data model;base data;data stream model;scientific workflow;software components
database;intermediate result;specific domains;schema matching;distinctive features;matching systems;ontology alignment
human users;monte carlo;database;data cleaning;collected data;data quality;declarative framework;markov chain;question answering;answer questions;large database
semantic information;highly ranked;recommender systems;collaborative filtering
service provider;response times;multi-dimensional;query response time;pre-computed;data records;skyline queries;search engine;data-intensive applications;web browser;frequently updated;skyline query processing;user-oriented
streaming model;aggregation function;1;sensor networks;main idea;sensor network;sensor nodes;sensor networks
approximate answers;network processing;sensor readings;large-scale;resource-constrained;aggregate queries;sum queries;bandwidth consumption;sensor networks;data confidentiality
anonymized data;sensitive values;anonymization algorithm;graph-based;individual records
keyword queries;textual descriptions;query processing;web users
real-world and synthetic datasets;pattern recognition;duplicate detection;database;large scale;linear scan;range query;application domains;replication;algorithm called;query string;query processing;databases;prior art;times faster than
service provider;safe regions;query results;synthetic datasets;communication costs;knn queries;mobile clients;communication cost;query result;query point;nearest neighbors
xml schemas;xml schema;xml format
rdf data;sparql queries;semantic web;data model;object-oriented;semantic web;data sources;multiple data sources;relational data;programming language
preference-based;user preferences;java-based;ad-hoc;preference queries;user interaction
large amounts of;water distribution;large-scale;spatio-temporal;data mining;real-time monitoring;sensor data streams;data stream;sensor networks;water quality;patterns discovered;evolving data
software development;source code;semantically related;large-scale;1;3;search engines;2;interactive search;development process;syntactic structure
query optimization;query language;query plan;instances;modeling techniques;query processing;high volume;streaming environments;resource allocation
nearest neighbor;rknn queries;processing algorithms;efficient algorithms to;decision support systems;theoretical analysis;location based services;algorithm to compute;nearest neighbors
node failures;intermediate data;large-scale;fault-tolerance;intermediate results;scheduling strategy
query processing;tree index;data outsourcing;data privacy;nearest-neighbor queries;large datasets;theoretical analysis;optimization techniques;increasing importance;query processing
occurrence frequency;classification;database;isolation level;multi-tier;data items;serializability;concurrency control;application server;detect anomalies;online transaction processing
multi-version;database systems;database;isolation level;replication;algorithm called;serializability;concurrency control;low overhead;serializable
matching systems;poor performance;heterogeneous data sources;schema matching;databases;semantic information;semantic relationships;real world scenarios;schema elements
background information;automatically identifies;relevant objects;visual information;search engines
graphical user interface;single item;automatically generate;recommender systems;local information
search results;pagerank algorithm;visualization tools;query interface;wide range;sensor data;ranking mechanism;keyword search;advanced search
database technology;relational model;1;databases;distributed database;data management
performs poorly;query processing;data management
similarity search;large collections;search problems
life sciences;database;ad-hoc;databases;large number of;data sets;high-throughput;human genome;integration systems;data integration
algorithm performs;xml keyword search;xml databases;9;keyword search;query keywords;desired information
search results;matching techniques;text search;information integration;search engine;search algorithm;query terms
database
international conference on;content analysis;information retrieval;retrieval models;program committee;document representation

search results;topic models;impact analysis;search engines;search engine;relevant information;information needed;text analysis;actionable knowledge;trend analysis
search engine;search behavior;individual queries;classification framework;related queries;improving search;search sessions;user query;queries issued;search engines;user search behavior;long-term;search engine users;search tasks
trec test collections;analysis reveals;search process;cognitive load;interactive information retrieval;cost models;search session;search strategies;theoretical foundations;relevant information;interactive information retrieval;extensive simulations
named entities;regression models;search tools;retrieval models;user-study;search evaluation;user study

classification scheme;vertical search;classification;search behavior;search engines;search engine;event-based;search logs;query refinement
search task;regularization;web pages;search performance;relevance ranking;graph-based;share similar;instance;search engines;specifically designed to;search engine;log data;web page;graph regularization;topic detection;search tasks;additional information
search behavior;large-scale;information-seeking;search sessions;classifier;information seeking;log data;risk factors;health information
web documents;web search;user behavior;product) search;click stream;product search;product descriptions
low variance;ranking models;data set;data sets;predictive performance;parameter tuning;higher accuracy;variance reduction;high precision
search results;web corpus;web search;user experience;temporal characteristics of;closely related;feedback documents;search engine;learning framework
query evaluation;directly optimize the;large-scale;increasingly complex;higher quality;boosting algorithm;ranked retrieval;retrieval models;document collections;ranking models;ranking functions;result set;ranking model
domain adaptation;training data;web search;target-domain;active learning;target domain;data sets;supervision;prior knowledge;instances;domain-specific;labeled data;source domain
recommendation systems;special properties;information from multiple sources;news items;content information;access patterns;named entities;principled framework;individual users
search results;web documents;web pages;web search;user location;search engines;web search queries;document relevance;web search;location information;physical location;location-based
document ranking;active learning;relevant documents;active learning;trec collection;interactive information retrieval;relevance feedback;user effort;active learning approach;user feedback;learning strategy;user interaction;negative feedback
trec collections;query term;information retrieval;retrieval performance;retrieved documents;term proximity;information retrieval;query terms;term proximity
boosting framework;optimization framework;loss function;individual queries;language models;learning algorithm;pseudo-relevance feedback;retrieval performance;pseudo-relevance feedback;average precision;retrieval applications
density estimation;trec collections;ad-hoc;probabilistic models;probability density functions;document length;information retrieval;weighting model;retrieval performance;ir) models;density functions
prediction problem;sharing information;social network;information retrieval;matrix factorization;social influence;gradient method
search results;high-quality;social systems;large-scale datasets;topic modeling;social networking
test set;ir systems;low quality;label sets;ir evaluation;relevance judgments;information retrieval;data set;ir) systems;search evaluation
search systems;web pages;web search;classification;page segmentation;web sites;web page;segmentation results;segmentation method;fully automatic;link analysis
information sources;multiple information sources;search applications;image data;databases;real world applications;multiple information sources
kullback-leibler divergence;web-based;classification;information retrieval
content extraction;object model;web pages;content extraction;data mining
context information;web documents;online social networks;graph model;mutual reinforcement;user generated content;data set;social context;unified framework;social context
users' interests;heterogeneous data sources;sponsored search;information filtering;very large datasets;web site;recommendation approaches;dimensionality reduction techniques;web site;wide range;web applications;web users
trec data;document relevance;relevance information;clickthrough data;click models;retrieval evaluation;retrieval systems;main idea;search engine;optimization problem;user study;web retrieval
search results;segmentation results;training data is;retrieval accuracy;query segmentation;probabilistic model;language model;information retrieval;text corpus;query segmentation;labeled data;model parameters;em algorithm;relevant information;clickthrough data
utility function;user preferences;user actions;collaborative filtering;recommender systems;large scale;real-world;optimization algorithm;user behavior;user's preference;data sets;missing data;dyadic data;latent factor model
clustering;data arrives;gps data;human activities;incremental update;real-world;recommendation process;mobile devices;trajectory data;clustering process;similar users;location-based services;clustering algorithm;rich information;user-location
regularization;decision tree;takes into account;preference elicitation;decision tree construction;cold-start;matrix factorization;tree structure;data sets;optimization algorithm
collaborative recommendation;recommendation approaches;recommendation service;collaborative filtering;spatial clustering;real datasets;large-scale datasets;preference based;user preference;location-based social networks;social influence;power law distribution;naive bayesian
search results;user preferences;search experience;search engine;search behavior;search engines;search engine;web search engines
interaction data;data collection;web search;large-scale;data collected from;behavior analysis;search strategies;search engine;search behavior;small-scale
user interface;search performance;information retrieval systems;search efficiency;search engine
search results;web search;large-scale;result page;search behavior;search result quality;search result;search engine;eye movements;user studies;search behaviour;search tasks
mining framework;trade-offs;search log;privacy policies;log mining;private information;query logs;preserving privacy;search behavior;search logs
ranking results;large-scale;related queries;search queries;physical location;average precision;search engine queries
multi-dimensional;training data;user intent;extensible framework;search experience;multiple dimensions;tree structured;multi-faceted;query words;training sample
web pages;trec test collections;social annotations;ranking method;automatic query expansion;information retrieval tasks;social annotation;retrieval performance;feedback documents;expansion terms;term-dependency;retrieved documents;machine learning;query expansion;social annotation;learning method
community-based;community-based;community question answering;large number of;web search engines;web searches;search engines;web search queries;answer quality;unique characteristics;question answering;search engine users
discriminative power;link analysis;question answering;answer quality
multiple tasks;online discussion;probabilistic model;valuable information;conditional random fields;learning problem;question answering;conditional random fields
textual content;document corpora;community discovery;community discovery;large-scale
rewrite rules;syntactic structures;high computational complexity;feature set;classification;tree mining;syntactic features;subtree patterns;feature sets;computational burden;real-world datasets;syntactic information
classification performance;relevance ranking;classification
classification models;classification techniques;prediction performance;query term;training data;classifier;classification models;sentiment analysis;labeled data;data stream model;demand driven;association rules;lower-bounds;limited resources;demand-driven
social media;taking into account;language model;query likelihood;language models;traditional models;instances;retrieval models;blog posts;ad hoc retrieval;bayesian model;language modeling
specific information;language model;temporal information;query likelihood;pseudo-relevance feedback;query expansion;estimation methods
rank documents;large-scale;discriminative power;document retrieval;indexing techniques;similarity-based;text corpus;sparse representation;similar documents;addressing this problem;related documents;locality sensitive hashing;high dimensionality;dimension reduction;special properties
high-dimensional;feature space;training images;discrimination power;image content;feature subset;principal component analysis;image classification;natural images;selection algorithm;large number of;feature subsets;visual features;multi-modal;classifier training;multi-label;image annotation;weak classifiers;positive results;low-dimensional;feature selection;feature subset selection;classifier
image retrieval;ranking algorithm;real world;nearest neighbor;efficient computation;image database;graph-based;large scale;information retrieval;data set;large data sets;adjacency matrix;geometrical structure;data types;graph construction;manifold ranking;excellent performance;manifold ranking;content-based image retrieval
label quality;facial images;large-scale;database;learning tasks;optimization algorithms;facial image;machine learning techniques
10;keyword queries;20;random accesses;large-scale;space overhead;index entries;web sites;index size;temporal constraints;web archives;takes into account;query processing
range queries over;query throughput;disk-based;large-scale;web collection;index structures;compression techniques;wikipedia articles;search queries;index size;temporal constraints;index compression
search results;highly relevant;server selection;search experience;replication;search engines;search engine;search quality;query load;ambiguous queries;web search engines
result diversification;result sets;continuous query;algorithms require;random access;real-world data sets;continuous data;np-hard
result diversification;explicitly model;search result diversification;retrieval models;retrieved documents;ambiguous queries;aware search
pseudo-relevance feedback;weighting model;verbose queries;collection statistics;retrieval models;wide range;ranking methods;information retrieval models;query expansion
private information;privacy requirements;user profile;personalized web search;user profiles
cold start;feature space;latent dirichlet allocation;collaborative filtering;data sparsity;user interaction;semantic information;collaborative filtering
model complexity;optimization method;predictive accuracy;recommender systems;context-aware;prediction quality;context-aware;contextual information;rating prediction
filtering systems;user profile;effectively learn;feature-based;semi-structured documents;text classification tasks;amazon mechanical turk;learning algorithm;relevance judgments;prior knowledge;semi-structured documents;data sets;relevant document;user profiles;user feedback;user study;search interface
lda model;test set;latent dirichlet allocation;online reviews;digital camera;product review;numerical values;real life;problem domain;online product reviews;opinion mining;probabilistic graphical models
search task;large amounts of;statistical translation;web search;modeling framework;clickthrough data;topic model;ranking models;information retrieval;data set;language independent;vector space;latent semantic analysis;semantic space;baseline models;language modeling framework;latent semantic;web search;semantic representation;real world;projection matrix
learning process;relevance ranking;loss function;optimization problems;real-world;million documents;topic models;information retrieval;topic modeling;topic modeling;document representations;scalability issues;document collections;latent semantic indexing;larger datasets;latent semantic indexing
question/answer;classification;video data;video search;query generation;community question-answering;answer questions
music information retrieval;ensemble techniques;multiple classifiers;genre classification;classification performance;multi-label;machine learning
user study;training set;arbitrarily large;aggregation algorithm
search results;web search;helping users;web search results;multimedia objects;web page;search result pages;search behavior
manually labeled;ranking method;test set;ranking process;news event
web documents;web mining;optimization framework;optimization problem
future events
entity linking;web text;semantically related;graph-based;web text;inference algorithm;entity linking;high-probability;knowledge base
true values;attribute values;human effort;structured data from;false positives;search engines;efficiently extract;web data extraction;databases;structured data
local search;web search;data sources;feature generation;search result;search engine;incomplete data
search systems;end-user;desktop search;end-users;query suggestions;search engine;personalized search;document corpus;user query;user queries;search engines;higher quality;query logs;web-search engines;query suggestions;highly correlated
high utility;real-world;critical task;major search engines;large-scale;query formulation;query suggestions;high quality;user query;search engines;query-suggestion;search queries;search engines;web search queries;query sessions;query logs;related concepts;desired information
search results;search engine's;query suggestion;related queries;random walk;search engine logs;labeled data;search engine users;query suggestion
boolean queries;query suggestion;labeled documents;retrieval results;query quality;decision trees;query generation;unique characteristics;search tasks
weakly supervised;classifier trained on;text-based;classifier;video content
poor performance;news sources;rule-based;recognition systems;large datasets;online news;natural language
machine translation;document collection;trec test collections;auxiliary;information retrieval;source language;original document;term frequency;term frequencies;document representation
rule based;rule-based;corpus-based;text retrieval
clustering;similar objects;clustering technique;clustering method;target domain;clustering performance;document clustering;document clustering;maximum margin
clustering;clustering problem;web scale;spectral clustering;related information;rich information;large amounts of;web-scale;iterative learning;multiple sources;geographical information;mining tasks
cluster-based;fusion method;rich information;retrieval scores;similar documents
user behavior;user models;effectiveness measures
information seeking;evaluation metric;user study;information sources
search task;textual descriptions;large-scale;ad-hoc;real-world;ranking systems;keyword-based;object retrieval;web-pages
web page classification;web pages;cross-language;web page classification;auxiliary;classification accuracy;nonnegative matrix tri-factorization;knowledge transfer;classification methods;classification performance;word clusters;theoretical analyses;data set;knowledge transfer;real world;web page;nonnegative matrix tri-factorization;cross-language
text mining;specific applications;brute-force approach;statistical machine translation;locality-sensitive hashing;sliding window;feature vectors;solution space;brute force;cross-lingual;point based;pairwise similarity;cross-language information retrieval;multi-lingual
query throughput;current commercial;fine-grained;query processing;search engine;coarse-grained;web search engines;query terms
prediction accuracy;document collection;large-scale;real-life;query log;query results;web search engines
large-scale;query workloads;spatio-temporal;search engine;data centers;query processing in;real-life;query workload;web queries;web search engines
11;large parts;index structure called;document retrieval;30;performance gains;search engines;query processing;conjunctive queries;optimization techniques;query terms
utility function;singular value decomposition;real world;recommendation algorithms;data set
clustering;web scale;input data;generative process;takes into account;naive bayes;ranking models;probability distributions;scoring function;real-world;principled framework;clustering model;transactional data
user preferences;semantic analysis;social network;item recommendation;latent semantic analysis;unified framework;real world;rating prediction
ranking functions;genetic programming;textual features;target object
search results;trec 2009 web track;discriminative power;search queries;wide range;search result diversification;ir metrics;test collection
user interactions;real users;retrieval systems;model-free;test collections;query sessions;search engine;average precision
high level;retrieval systems;test collections;wide range;similar documents;ground truth data;information retrieval systems;test collection
web corpus;relevant documents;training data;web search;anchor text;web graph;manual construction of;information retrieval;test collections;test collections;ranking function;model trained;ranking functions;specific features;trec web track;query log;test collection
large-scale;ranking functions;ranking methods;training data;information retrieval
discriminative features;relevant features;information gain
relevant documents;relevance models;user query;relevance feedback;blog posts;query expansion
ranked list;information retrieval;query performance prediction;query performance prediction
ranking function;gradient descent;neural network;loss function
search results;search systems;keyword queries;approximation algorithms;optimization problems;online shopping;attribute values;product search;search result diversification;np-hard
ad hoc;human performance;ranking function;ranking function
visual similarity;training images;image datasets;rating matrix;target image;training dataset;ranking method;image annotation;image annotation
relevance feedback;ad hoc retrieval;ad hoc;retrieval effectiveness
real-life;test collection;retrieval model;query terms;information retrieval
computation cost;term frequency;retrieval function
social media;information sources;news events;search engines;query logs;information source;obtain information
search results;search quality;high-quality;relevant documents
clustering;heterogeneous information network;heterogeneous networks;topic model;multi-typed;topic modeling;heterogeneous network;latent topics
graph-cut;graph-cut;graph cut
user queries;semantic similarity between;query expansion;user profile;social tagging
web pages;web search;large-scale;search behaviour;search queries;web search engines
web search;web search;search behavior;search problems
relevance scores;conventional methods;feature set;data set;image search;multiple modalities;visual content;feature sets;distance metric;million images;integrates multiple
learning process;multi-layer;programming model;large-scale;image datasets;semi-supervised learning;graph-based;image data;multi-layer;textual information;graph-based semi-supervised learning;higher quality;unlabeled data;computing power
additional knowledge;machine learning;labeled data;string-matching;computational cost;machine learning approaches
relevant) document;automatically generating
competence;mechanical turk;relevance judgments;human computation
cross-language;web service;quality assessment;feature extraction;web content;features including;ranking problem;web content;language-independent;ecml/pkdd
social relationships;viral marketing;reference data;social network;user's preference;item recommendation;social influence;bipartite graph;allowing users to;social networking;query terms
search systems;trec collections;verbose queries;reformulated queries;query distribution;long) queries
search systems;domain experts;search topic;finding task;information retrieval systems;domain-specific;search strategies;user study;search behavior;query terms
sample selection;general-purpose;collection size;text corpora;dictionary-based;storage requirements;pre-processing
medical records;real-world;classification;information retrieval
point-based;region-based;gaussian kernel;region-based;geo-referenced;automatically identifies
mining task;classification;classification accuracy;multi-stage;semantic analysis;short text;short texts;topic detection;svm classifier;classification approach;information retrieval
benchmark data;collaborative question answering;temporal information;latent semantic analysis;generation process;information retrieval;traditional models;topic modeling;latent semantic analysis;natural language processing;latent semantic analysis;tensor decomposition
social network analysis;cold start;recommender systems
information diffusion;blog data
graph model;question answering;community-based
high precision;web pages;false positive;data set;real-life;human efforts;search behaviors;user intent;query logs;page content
user study;web search;web search engines;web search engine
multiple users;concept space;recommendation accuracy
related entities
semantic information
matching techniques;similar results;retrieval systems;indexing method;music retrieval;retrieval method
machine translation;cross-language;patent retrieval;information retrieval;parallel corpora;ir) effectiveness
frequency distribution;temporal information;related queries;correctly identify;search engines;high accuracy;time-series;anchor-text;usage data;search logs
budget constraints;training set;ranking quality;ranking methods
performance gains;query processing;index structure;query processing;text retrieval
result merging;result merging;document scores;database;federated search;semi-supervised learning;user query;explicitly models;regression methods;source-specific
query response time;retrieval strategies;information retrieval systems;pruning strategies
retrieval effectiveness;linear regression;relevance judgments;keyword-based;query performance prediction;performance predictors;ranking model;query performance prediction;neural networks
search task
learning process;objective functions
short queries;web search;query suggestions;web search results;search result diversification;ambiguous queries;web search engines;multiple aspects
search systems;special characteristics;expert search;query log analysis;query logs;query log;users interact with;expert search;web search;advanced search
social relationships;expert finding;model assumes;textual content;hierarchical model;data set;expert finding;retrieval performance;social networks;retrieval algorithms
transductive learning;multiple documents;transductive learning;sentence extraction;multi-document summarization;automatically detected;query-biased
regularization;collaborative filtering;regularization;implicit feedback;rating prediction;objective function
visual content;visual words;feature descriptors;visual words;retrieval performance;video retrieval
query term;operator;proximity-based;blog posts;opinion retrieval
training data;web search;general purpose;mobile search;large scale;mobile search;ranking performance;web search;log data
network analysis;ranking problem;ranking model;real-world
prediction accuracy;rating prediction;feature vector;data sparseness;rating prediction;data sparseness;customer reviews
rank aggregation;data set;random walk on
heterogeneous information sources;discriminative learning;probabilistic models;real-world;social networks;probabilistic model;user similarity;social graph
automatically extracted from;user profile;recommender systems;recommender systems;ad-hoc;query intent;information retrieval;retrieval quality;increasing attention;giving rise to
retrieval performance;mutual information
data set;selection problem;document selection;ranking tasks;ranking models;ranking problem;ranking algorithms;boosting algorithm;simple algorithm;real world applications;objective function;specific problem
user visits;web pages;real-world;class models;historical data;regression model;online advertising;time-series;web page;automatically learn
search results;domain knowledge;document relevance;document selection;mesh terms;user study
binary classification;1, 7;learning curve;relevance judgments;test collections;7;multi-class;5, 3
ir) techniques;language modelling;information retrieval;common practice;probability ranking principle;information retrieval
domain knowledge;query length;sampling method;regression models;search behaviors;recall-oriented;user study;search tasks;selection methods
probability ranking principle;theoretical framework;document rankings;portfolio theory
desktop search;web search;mobile search;machine learning techniques;mobile devices;search engine;increasingly popular
document relevance;relevant documents;information retrieval;true positive;data collected;user studies;retrieval systems;user study
web documents;textual features;social network;effectively identify;social networks;graph based;bipartite graph
network analysis;link formation;sampling methods;community detection;social network;social networks;data collected;prediction task;information networks
query results;search engine;web search results;large-scale
ranking approaches;citation network;ranking model;information networks
multiple criteria;algorithms typically;multi-objective;multi-criteria;search quality;supervised learning;multi-objective optimization
training set;large-scale;large number of;training datasets;training sets;ranking functions
retrieval effectiveness;trec ad-hoc;selection process;information retrieval;long-term;test collections;pseudo-relevance feedback;temporal profiles
8;selection techniques;multi-document summarization;multi-document summarization;document set
edit distance;inverted lists;query string;approximate string
visual-words;visual-words;image descriptors;retrieval effectiveness;image representation;feature representations;image retrieval;content-based image retrieval
search results;query term;queries efficiently;large number of;labeled instances;query term;search result;search engine;query terms
long queries;web queries are;term based;user intent;search engine;rank based;web search engines
document collection;relevance ranking;mixture model;probabilistic model;textual similarity;relevance judgments;annotated corpus;amazon mechanical turk;retrieval model;ranking methods
graph structures;training data;classification;classification tasks;classification performance;risk minimization
community-based;community-based;question answering;information seeking;question answering;contextual factors
clustering;search results;clustering results;clustering algorithms;search result;search engines;search engine;clustering;clustering procedure
temporal sequence;optimization algorithm;personalized services;topic model;user requirements;maximum weight
semi-structured;data sources;open source;knowledge base
context-aware;desktop search;search engines;hidden markov model;keyword search
information sources;multi-typed;multiple information sources;social networks;social network
web objects;mobile internet;cyber-physical systems;web search;search engine
large volume;semantic role labeling;classification;fine-grained;named entities;sentiment analysis;sheer volume of;entity recognition;natural language processing;allowing users to;advanced search
user interfaces;user interface;online video;user interactions;face recognition
data sources;information services
word recognition;search queries;search engines;online content;search engine;learning environment;audio-visual
community-based
large data sets
ir researchers;ground) truth;learning curve;ir evaluation;ir) evaluation;information retrieval;retrieval evaluation;statistical significance;additional features
unsupervised learning;active learning;information retrieval applications;information retrieval;wide range;information retrieval systems;clustering;information retrieval research;recommendation systems;semi-supervised learning;online advertising;machine learning techniques;text classification;optimization techniques;real-world;machine learning;information retrieval;probabilistic models;optimization techniques;machine learning methods;collaborative filtering;machine learning methods to;multiple-instance learning;machine learning;stochastic optimization;supervised learning;web search engines
data mining;ir researchers;web search;data summarization;large scale;major components;massive amounts of;document understanding;huge amounts of;log mining;large-scale;search engines;web search results;search engine;log data;information retrieval systems
social media;pattern recognition;search engines;social media;web site;information retrieval;information science;question answering systems;pattern analysis;information retrieval
lower cost;amazon mechanical turk;information retrieval
commercial search engines;evaluation techniques;user behavior;retrieval evaluation;information retrieval;retrieval quality;information retrieval systems
commercial search engines;data analysis;web search;step forward;document-centric;retrieval systems;retrieval methods;large scale;user interaction;query logs;link analysis techniques;usage data;user interaction;web retrieval
large-scale;ir researchers;semantic analysis;software tools;retrieval algorithms;information organization;information organization;generation process;extraction methods;information access;large repositories of;word sense disambiguation;ir methods;user interactions;web users
search results;personalized search;future queries;past queries;search behaviors;search engine;user preference
user preferences;search performance;manual tuning;online learning;search engines;search engines
document collection;uniform distribution;information retrieval;document collections;domain-specific knowledge;document relevance;specific information;4;related documents;query expansion;random-walk;semantic similarity;word senses;relevant document;uniformly distributed;pseudo-relevance feedback;information retrieval tasks;word sense disambiguation;mutual information;similarity measure;graph model;1, 2, 3, 4, 5;semantic information
heterogeneous sources;linked data;information systems;ad-hoc retrieval;classification;knowledge engineers;ir systems;vice versa;complex data;retrieval models;task-specific;information retrieval;text classification;specific task;ir) systems;knowledge transfer;text classification;multiple sources;high-level;long-term
results merging;relevance scores;relevant documents;federated search;resource selection;meta-search;estimation errors;3;user query;1;user queries;direct access to;2;distributed information retrieval;resource selection;score distribution;retrieved documents;distributed information retrieval;document scores;integrates multiple
required information;web search;information retrieval;user profile;retrieval effectiveness;1;3;2;5;4;users' search;individual users;search results;demographic information;ir systems;information retrieval;query expansion;major search engines;user's interests;user's search;search history;user profiles;search logs;ir metrics;search tasks
search results;world wide web;language-independent;temporal information;temporal dimension;content analysis;relevant content;search engines;ir systems;ambiguous queries;information network;clustering
user visits;similar queries;user profile;search effectiveness;retrieval results;ir model;user behavior;domain-specific;information retrieval;domain-specific;addressing this problem;search history;information retrieval;personal information
search results;interaction data;web based;user preferences;search engine's;level features;search result diversification;user query;multiple dimensions;search engines;search engine;search result diversification
motion patterns;power consumption;information management;retrieval applications;mobile devices
sample set;user click;search problems
recommendation engine;cognitive states;development process;large scale;user preference;search queries;underlying data distribution;user-generated content;social networks;search engine;machine learning;learning framework
web scale;main findings;lessons learned;web scale

data objects;result sets;skyline queries;database;high degree of;increasing number of;trade-offs;complex query;large data sets;skyline queries;rank-aware
real-world data sets;semi-structured;data sets;real-world entities;information spaces
high-speed;data warehouses;processing algorithms;query workloads;field-programmable gate arrays;event detection;stream processing;database;business intelligence;cpu-based;data processing;data mining;data analytics
query optimization;search space;optimizer;plans;query optimizer;dynamically generated;object-relational;execution plan;prohibitively expensive;optimization process;effective pruning;generation process;query optimizers
monitoring applications;data stream management;cloud services;data stream management system;continuous queries
stream processing systems;stream processor;high throughput;input data;stream processing;stream data;high-throughput;input streams
update processing;query execution;optimizer;data warehouse;highly compressed;main-memory;query performance;data structures;business intelligence;storage structures;access patterns;data distribution
database administrator;database systems;database;index maintenance;long-running;index tuning;concurrent queries;database management systems
data corruption;database administrators;database research;database
web pages;relation extraction;natural language text;unstructured information;web text;knowledge discovery;classification algorithms
data lineage;data fusion;1;data lineage;data integration systems;operator;2;data integration
domain-independent;user feedback;scientific literature;extraction methods;biological entities;semi-automatic
database;sql queries;closely related;data-centric
evolving data;data management systems;path expression;model called;complex objects
xml documents;update operations;xml schema;xml document
large-scale;multiple attributes;large graph;data-sets;data-storage;data stores;social networks;graph data;high-level;data organization;data storage
rdf data;social media;search space;rdf databases;graph database;graph data;instances;social network;data set;graph datasets;queries posed;social networks;real world networks;query answering;cost models;subgraph matching;matching algorithms;real world;subgraph queries
domain-specific;structured data;graph databases
data analysis;graph structure;large-scale;data cleaning;data describing;observational data;computationally expensive;domain-specific;social networks;network data;ad hoc;data management system;information networks
graph structures;database;similarity search;similarity) measures;similarity measure between;graph queries;graph databases
query answering over;semi-structured;semantic web;increasingly large;query answering;knowledge representation
graph database;social network analysis;large graph;large graphs;management systems;database management system;graph databases;graph representation
vertex set;relational algebra;multi-relational;tensor-based;formal language;graph traversal
keeping track of;stored procedures;higher order;programming language
formal framework
dependency analysis
open source;schema evolution;large-scale;automatically extracting;schema evolution;databases;long-term;database schemas;application code
database updates
database;graph-based;relational tables;stored procedures;data-centric;data management systems
java based
arbitrarily large;large-scale

xml databases;xml data;computational complexity;completeness;query answering
rdf databases;database;query workloads;relational database systems;optimization algorithms;view selection;space requirements;query processing cost;query workload;greedy algorithm
object-oriented;class models;information integration
domain knowledge;world wide web;ontology-based;product recommendation;recommendation systems;dimensionality reduction method;domain ontology;user ratings;semantic web;ontology-based;social networks;clustering
training set;classification;text analytics;named entities;wikipedia articles;fine-grained;named entity;feature selection;classifier;semi-automatic
social networks;user profiles;social network;social networks
network analysis;process model;ad-hoc;semi-structured;emerging applications;international workshop on;disparate sources;business processes;heterogeneous databases;social networking;unstructured data;data management
end-users;reference model
process model;high quality;business operations;process models;discovery problem;execution times
knowledge workers;real world;business operations
virtual world;optimization techniques;business process;real-world;fine-grained;complex event processing;event streams;pattern matching;event-based
monitoring applications;similar results;complex event;real-world;semantic caching;online stores;supply chain management;business processes;complex event processing;event streams;intermediate results;query types;subexpressions;pattern extraction
process model;semi-structured;business process;business processes;graph analysis;semi-structured;change frequently;execution traces;decision making
process management;instances;event types;semi-structured;document management;heterogeneous systems;business process;data sources;business processes;wide range;real-world;massive data sets
risk management;control points;application code
management systems;takes into account;text-based;lessons learned;higher accuracy;finding similar;social networking;similarity matching
density-based;pattern queries;large quantities of;nearest-neighbor;similarity queries;motion patterns
user generated;structured information;data streams
user ratings;recommender systems;collaborative filtering;online shopping;similar) items;online stores;application domains;data structures;similar) users;recommendation algorithms;recommender systems;large number of
main-memory;disk-based
xml documents;xml data;xml updates;data lineage;xml update
instance selection;nearest neighbor;training data is;classification;decision rule;classification accuracy;class label;data reduction;data reduction;nearest;instance;nearest neighbor;sample set;instances;selection algorithm
clustering;linear program;dimensionality reduction;classification;hierarchical approach;feature selection problem;large number of;large scale;feature selection;learning problem;desirable properties;game theoretic approach;game theory
training process;optimization problem;feature selection;text classification;feature selection process;feature selection
feature space;document collection;feature weighting;tf-idf;computational complexity;text clustering;link-based;feature weighting;benchmark data sets;local feature;clustering quality
support vector machines;feature selection method;classification accuracy;text categorization;feature selection;feature selection methods;informative features
tree kernel;classification;kernel function;tree-structured data;kernel method;1;tree kernels;predictive performance;web mining;tree-structures;classification tasks;space complexity
test data;domain adaptation;class information;label information;classification;principle component analysis;labeled training data;classification;expectation maximization;matrix factorization;classification problem;probabilistic pca;posterior distribution;semi-supervised;dimensionality reduction;latent variables
clustering;sparseness problem;relation extraction;semantic relations between;probabilistic matrix factorization;probabilistic matrix factorization;dimension reduction;feature vectors
unlabeled data;algorithm combines;semi-supervised learning;conditional independence;classifier;semi-supervised learning methods;instance;labeled data;class labels;training algorithm;single view;style algorithms
regularization term;positive definite;training examples;similarity measure;hits algorithm;imbalanced datasets;neighborhood graph;learning problems;benchmark datasets;support vector machines;feature vectors;structural similarity
training data;predictive accuracy;classification;learning algorithm;learning algorithms;vector machine;naïve bayes;machine learning;instance;decision tree;base learners;instance-based;perform poorly;missing values;classifier;decision tree learners;classification algorithms
social media;explicitly model;retrieval tasks;partial knowledge;heterogeneous data sources;gibbs sampling;multiple data sources;probabilistic matrix factorization;data sources;bayesian formulation;bayesian framework;formal framework
singular value decomposition;global optimal;global convergence;real world applications;fundamental problem;theoretical analysis;tensor decomposition;machine learning and data mining;objective function;high order
clustering;21;machine vision;machine learning;binary code;fundamental problem;binary code;information retrieval;retrieval techniques;nearest neighbor search
clustering;text mining;theoretical foundation;document-term;language models;document-(term;high-order;text data;high-order;semantic smoothing;language modeling approach;contextual information;clustering algorithm;benchmark data sets
clustering;high-dimensional;lower-dimensional;nearest neighbor;data points;clustering algorithms;clustering;feature subspace;large quantities of;clustering high-dimensional data;dimensional data;data-mining techniques;arise naturally in
entropy-based;clustering;density-based;synthetic data;spatial datasets;clustering method;effectively identify;clustering process;spatial attributes;clustering methods;correlated patterns;spatial correlation
data objects;spectral clustering;clustering methods;spectral clustering;clustering method;data dimensionality;unified framework;theoretical basis;clustering
density-based;gene expression data;database;clustering technique;tree-based;1;real-life datasets;finding clusters;biologically relevant
clustering;regularization;uci data sets;cluster structure;weighted graph;dimensionality reduction method;high dimensional;data manifold;clustering high dimensional data;optimization problem;data set;original data;cluster assignment;iterative algorithm;low dimensional;manifold structure;dimensionality reduction;real world data sets
parameter optimization;discriminative features;prediction model;multi-core;classification accuracy;real-world;classification methods;auc;global search;feature selection;nearest neighbor;parallel implementation;classifier
clustering algorithms;parameter-free;numeric data;unsupervised clustering;hierarchical clustering;hierarchical algorithm;similarity measure;categorical data;instance-level;semi-supervised clustering;real-life;semi-supervised clustering;clustering categorical data;fully automatic;semi-supervised;semi-supervised
data structure;label information;classification;case study;class label;classification tasks;recognition task;instance;clustering techniques;data preprocessing;accurate classifiers;input data;real datasets
target language;machine translation;classification problem;training data;cross-lingual;sentiment classification;training and test data;cross-lingual;source language
bayesian methods;efficient inference;markov process;classification;bayesian methods;state variables;data stream;logistic regression;learning problems;real data;multi-class;learning task;feature vectors;auxiliary
algorithm performs;incremental algorithm;decision trees;decision trees;noisy data;feature selection;data streams;online algorithms;classification algorithms
data cleaning;automatically constructed;classification performance;instances;multiple classifiers;sentiment classification;training corpus
pattern analysis;association patterns;pattern mining;optimization problem;discriminative patterns;pattern mining algorithms;biomedical data;higher order
algorithm generates;search space;association patterns;apriori-based;instance;effective pruning;promising candidates;pattern discovery
filtering systems;incoming documents;specific information;pattern-based;information filtering;pattern mining;data stream;user profiles;sheer volume of;information overload;term-based;irrelevant documents;large number of
instances;mining algorithm;moving objects;mining process;moving object
data stream environment;distance function;association rule mining;pattern mining;data streams;times faster than
success rate;database;class distributions;databases;classification problems;classifier;business decisions
classification;rule-based;classification methods;classification algorithm;rule-based;high classification accuracy
search results;keyword search;heuristic algorithms;xml keyword search;scoring function
test data;topic models;topic distributions;variational bayesian;trend analysis;latent dirichlet allocation
automatically extracted;latent dirichlet allocation;large scale;large margin;topic modeling;product features;topic modeling;modeling method;opinion mining;existing knowledge
document corpus;latent dirichlet allocation;sentiment analysis;product feature;semantically related;generative model for;word pairs;fine-grained;generative model;product features;product features;semantic relations;opinion mining;baseline models
term frequency;graph model;text document;term frequency;text classification;svm classifier;term weighting scheme
clustering;tensor space;xml documents;tensor space;content information;document representation;vector space model
detection problem;rule-based;information extraction;detection performance;machine learning techniques;digital libraries;heuristic rules;pre-processing;boundary detection
training set;training data;named entities;data set;entity recognition;6;semi-supervised;supervised learning;conditional random fields;learning method
lda model;web access;user behavior;user profiling;topic modeling;clustering framework;matching method
web page;web pages;duplicate detection;page content;algorithm named
classification;probabilistic model;labeled training data;topic model;text categorization;cross-lingual;knowledge transfer;latent topics
web-based;multi-agent systems;multiple domains;intelligent behavior;knowledge representation;multiple sources;cyber-physical systems;limited resources;resource usage
kdd conference;kdd conference;acm sigkdd international conference on;san diego;knowledge discovery;data mining;instances;conference on knowledge discovery and data
data analysis;signal processing;large-scale;convex optimization;convex optimization;code generation;model fitting;resource allocation
data analysis;efficient computation;machine learning;data center;data sets;internet scale
error-prone;massive amounts of;human genome;clinical practice
graph theory;applications involving;sample-selection bias
computational complexity;instances;separability;random projections;benchmark datasets;classifier
source code;classification;open-source software;labeling schemes;supervised learning;similarity based
multiple classes;classification problems;linear model;training examples;classification;large-scale;accuracies;stochastic gradient descent;linear svms;linear svm;large data sets;noisy data;limited number of;iterative algorithm;error rate;support vector machines;automatically determined;classification algorithms
large-scale;optimization problem;generalized linear models;coordinate descent;logistic regression;implementation issues
multiple tasks;generalization performance;low-rank;optimization problem;real-world applications;multi-task learning;operator;multi-task learning;learning performance;benchmark data sets
matrix factorization;desirable properties;factor matrices;data mining;highly accurate;likelihood function;model order selection;fine-grained;global structure;data mining tools;data matrix;requires solving;model order selection;interpretability;minimum description length
rank aggregation;rank aggregation;incomplete data;synthetic data;rating data
sufficient conditions for;loss function;large-scale;optimization algorithm;stochastic gradient descent;web-scale;matrix factorization;matrix-factorization;weighted sum of
set cover;training data;subtopic retrieval;sparse set of;social network;search engines;rates;maximum entropy;search engine;document summarization;np-hard
real-world;graph structure;predictive accuracy;data describing;graph identification;communication network;social network;observational data;probabilistic inference;link prediction;node labels;communication networks
real data;transition probability;semi-supervised;human supervision;learning framework;objective function;ranking scores;web graphs;social networks;graph structure;random walk on;large graph;structure information;semi-supervised;rich information;markov model;entity ranking;question arises;supervision;large graphs;algorithm called;search engine;optimal parameters;semi-supervised
outbreak detection;desired properties;applications including;real-world;sampling process;social networks;sampling strategies
temporal dependencies;web applications;baseline models;user profiling;user behavior;topic models;web pages;instance;user interests;activity patterns;user profiles;user activity;scalable distributed;user profiles;inference algorithm;statistical framework;unsupervised} fashion;online advertising
hierarchical bayesian model;analyzing data;latent dirichlet allocation;easily extensible
web portals;evaluation methodology;optimization framework;17;multi-objective;multiple objectives
real-world datasets;collaborative filtering;matrix factorization;online advertising;logistic regression;weighting scheme
social media;web applications;transfer-learning;user opinions;learning task;textual features;textual content;human behavior;highly dynamic;sentiment analysis;labeled data;analysis task;stationary distribution;classification models;accurate models;learning strategy;relational data
user community;web site;text features;data sparseness;high quality
high-quality;real-estate;user-generated;decision-making;real data;approximation guarantees;information consumers;mobile phone;information overload
ensemble learning;base classifiers;real-world;large volumes of;spam detection;data stream classification;large number of;data stream applications;indexing structure;stream data;linear complexity;linear scan;web traffic;data streams;ensemble learning;intrusion detection;real world;prediction models;spatial databases
web pages;active learning;cost-sensitive learning;expected-utility;skewed;predictive model;instances;density estimation;decision theory
user-generated;active learning;binary classification problems;linear classifiers;large number of;unlabeled samples;real-world applications;stochastic process;data stream;sampling process;unlabeled samples;data streams;maximum likelihood;random sampling;classification error;sampling strategies
class label;learning algorithm;online learning;learning strategies;learning problem;real-world data sets;8;multi-class
mining algorithms;user experiences;semantically related;sql server;document similarity;fully automatic;related documents
software tools;data mining methods;naive bayes;classifier;detection results;file content;semi-parametric;support vector machines;data mining based;malware detection;programming interface
classification;high-precision;multilabel classification;information bottleneck;document classification;large-scale;high-precision;maintenance costs;classification algorithms;lazy learning;classification results;manual labeling;natural language;phrase-based;small-scale
human activities;synthetic datasets;rates;high accuracy;statistical framework;application scenarios
estimation techniques;log data;primary goal;false positive;user privacy
data-driven;data-driven;classification models;logistic regression;classification;classification accuracy;probabilistic model;predictive modeling;multi-channel;logistic regression model;data set;modeling approach;real-world;statistical framework;user interaction;cross-validation
attribute values;decision trees;business model;gradient boosting;linear regression;baseline methods;tree data structure;online advertising;mutually exclusive
large scale data mining;skewed data;false positives;classification;low quality;human experts;automated methods;online advertising;high cost;expert knowledge
social media;query forms;information sharing;data mining techniques;probabilistic models;information retrieval techniques;mobile devices;information management;summarization techniques;data mining
decision model;real-world;decision-making;quantitative analysis;data driven
data analysis;computational methods;huge amounts of data;association rule mining;data describing;data miners;data mining techniques;graphical models;cluster analysis;computational methods
discovered patterns;classification scheme;single item;surveillance systems;semantically meaningful;large scale;video data;major source of;vector machine;weighting schemes;surveillance video;build models;true positive;pattern discovery;pattern matching;text streams
real-world;end user
domain experts;interactive learning;cost-sensitive learning;classification performance;data mining systems;building classifiers;classifier
parallel algorithms;data collection;specifically designed to;parallel programming;large-scale;machine learning algorithms;low-level;large datasets;data mining;machine learning and data mining;programming models;programming paradigm;open-source
labeling scheme;binary classification;training set;classification;ground truth;labeled dataset;labeled examples;ground truth data;classifier trained on;classifier
comprehensive evaluation;increasing importance;sales data
medical records;frequently occurring;mining temporal;data reduction;partial order;frequent sequences;event sequences
multiple sets of;association rule mining;breast cancer;statistical test;valuable information;alzheimer's disease;strongly correlated;wide range;alzheimer's disease;association rules;statistical significance;association rules;magnetic resonance imaging;image data;support measures;alzheimer's disease
collaborative filtering;domain knowledge;recommender systems;collaborative filtering;real-world;collaborative filtering algorithms;case study
counterpart;svm algorithm;feature set;classification accuracy;large-scale datasets;data collected from;computational resources;training instances;algorithm produces;class labels
large amounts of;high risk;high-risk;management systems;prediction accuracy;case study;domain experts;learning models;healthcare data;test results
data-driven;search engine;product descriptions;search engine
user profile;user preferences;specific information;large-scale;user experience;user profiles;online advertising;search engine;approximation algorithm;np-hard;user activity
smoothing techniques;language models;smoothing techniques;topic models;short texts;topic tracking;language modeling
social media;web search;classification;large-scale;user profiles;automatically constructing;learning framework
keyword queries;highly relevant;scientific literature;individual preferences;fine-grained;modeling approach;keyword search;user study;objective function
online communities;latent structure;topic modeling;traditional collaborative filtering;scientific articles
text mining;interpretability;web pages;unsupervised learning;generative models;dirichlet process;topic models;partially labeled;supervised classification;case studies;partially labeled;latent topics;partially supervised
social science;suffix tree;information sources;evolutionary process;observed data;inference methods;real life;temporal features;additional features
partition function;regularization;prediction performance;probabilistic models;learning algorithm;topic model;application domains;topic models;topic discovery;coordinate descent;closed-form solution;prediction tasks
social media;supervised learning;general purpose;text collections;real-world;topic models;topic models;temporal dynamics;general-purpose;text corpora;data sources;topic model;text corpora;level features;time-series;latent topics;state-space;real-world applications;latent variables
privacy guarantees;anonymized data;classification;private data;anonymization algorithm;database;aggregate queries;sensitive data;tree induction;raw data;data mining;privacy-preserving data publishing;classifier
classification model;classifier;classification
social media;large-scale;user's perspective;social network;user privacy;social networking
semantic annotation;multi-label classification;data cleaning;feature extraction;vector machine;social network;algorithm learns;features extracted from;location-based social networks;real dataset;svm) classifier;location-based
real-world datasets;search space;data analysis;influence-propagation;cascade model;dynamic-programming algorithm;instance;brute force;social graph;pre-processing;exhaustive-search;data-reduction
problem instances;online reviews;maximum number of;approximation algorithm;increasing number of;real data;training dataset;collaborative tagging;optimization problem;naive bayes classifiers;np-complete;tag prediction;brute-force algorithm
service provider;test data;spam filtering;optimization problem;predictive model;instances;prediction problems;training and test data;prediction models
data mining;social network;real-life;data mining projects;data mining;data mining problem;data management
data mining techniques;information theory;probability distribution;game theory;data mining;data mining process;data mining algorithms;information exchange;information theoretic
benchmark data;data analysis;bayesian information criterion;maximum entropy model;interesting itemsets;iterative process;correctly identify
pattern-based;monte carlo;frequent sets;item-)sets;frequent set;pattern discovery;markov chain;sampling algorithms;highly scalable
graph mining;mining frequent closed;pattern mining;data stream mining;data streams;evolving data streams;mining frequent closed
search results;latent topic;text documents;user feedback;user feedback;information retrieval performance;keyword search;latent topics;information retrieval
global model;sparse data;predictive accuracy;real-world;kalman filter;multi-resolution;cold-start;context-specific;model fitting
entity ranking;generative model for;text data;data sets;analysis task;supervision;major limitation;multiple domains;wide range;behavior analysis
regression trees;density estimation;classification trees;decision tree;exploratory data analysis;density function;automatic feature selection;density estimation;squared error;decision trees;joint probability;interpretability;feature selection
clustering;distance measure;high computational cost;unsupervised clustering;real-world;usage patterns;large datasets;data mining;kullback-leibler divergence;information theoretic
density estimation;gaussian mixture model;image segmentation;anomaly detection;mixture models;data mining processes;gaussian distribution;mixture models;model selection
sample selection;graph classification;selection problem;feature vector;active learning framework;real-world;large number of;labeled graphs;basic assumption;graph classification;subgraph features;graph data;learning problems;feature selection problem;real-world applications;np-hard
real graphs;graph mining;structural features;classification;feature extraction;network classification;large graphs;class labels;transfer learning;mining tasks
approximation algorithms;clustering coefficient;sampling algorithms;complex networks;main memory;disk access;disk accesses;numerous applications;estimation algorithm
clustering;clustering algorithms;26;local search;clustering algorithm;parallel algorithms;sufficiently large;data set;approximation guarantees;data sets;large datasets;clustering problems;clustering problems;numerous applications
clustering;clustering;multi-dimensional;taking into account;synthetic data;database;multi-dimensional data;cost functions;disk accesses;user-defined;real dataset;processing nodes;subspace clustering;clustering methods;high dimensionality;clustering quality
informative samples;multi-class classification;linear model;limited memory;linear models;large-scale;stable model;training examples;disk access;data sets;1;faster convergence;globally optimal solution
high-dimensional;training set;classification;high dimensional;coordinate-descent;biological sequence;wide range;logistic regression;linear classifiers;sequence classification;support vector machines;loss functions
domain adaptation;classification;multi-source;target domain;training set;source domains;data set;probability distribution;multi-source;weighting scheme;classification accuracy;multiple domains;learning framework;data distribution
human genome;huge number of;genomic data
location traces;gps data;large-scale;real-world;distributed nature of;knowledge discovery;business intelligence;data mining
network datasets;citation graph;large graph;machine learning;relevant information;belief propagation;helps users;user interaction;learning method
ground truth;web pages;visual cues;link detection;web sites;structural information;automatically extracts
unique features;data mining and knowledge discovery;distributed memory;memory requirements;data mining tools;data intensive;data intensive
fully operational;valuable knowledge;event extraction;news sources;large-scale;database;event extraction;structured information;automatically extracted
social media;mining algorithms;social media;large scale;online users;large number of;user interests;network model;allowing users to;text messages
mining algorithms;processing algorithms;pattern mining;interesting patterns;interestingness measure;interesting itemsets;decision trees;association rules;interactive visual;mining algorithm;interestingness measures
sensor technology;classification;data mining
domain experts;degree distribution;social network analysis;community detection;spatio-temporal;clustering coefficient;path length;social network;dynamic model;real-life;social networks;information propagation;power-law;simulation framework
language model;influential users;retrieval method;social network;topic model
pre-defined;binary pattern;classification;feature extraction;vision based;location based services;image processing;color segmentation
modeling assumptions;ground truth;online-advertising;data availability;statistical techniques;sample sizes;statistical significance;controlled experiments;observational data;observational-data;causal effects;user behaviors;online advertising
statistical models;database;massive data;data warehouse;computationally intensive;data movement;case studies;data volumes;user-defined functions;allowing users to;large-scale data mining;predictive analytics
case studies;social media;social media;machine learning methods;customer satisfaction;customer service;data mining and machine learning;data mining;real-life;web logs;customer behavior;closed loop
high-speed;predictive accuracy;data mining techniques;predictive modeling;large number of;linear regression;short term;long-term;statistical models
knowledge discovery;data mining
supply chain management;future events;predict future;predictive analytics
high risk;online shopping;machine learning;credit card;credit card fraud;credit card
customer satisfaction;data analysis;closed-loop;lessons learned
predictive models;data analysis;lessons learned;case studies;logistic regression;data mining;data mining problems;correlation analysis
lessons learned;real world;data mining
real world domains;target variable;expected values;time series;water quality
clustering;game theoretic;overlapping clusters;heterogeneous information network;unsupervised learning;clustering problem;applications ranging from;heterogeneous information networks;feature sets;clustering methods;network topologies;reward functions;information network;real world;heterogeneous information networks;information networks
clustering technique;optimization algorithm;graphics processing;sparse datasets;specifically designed to;times faster than
prediction problem;multi-task;regularizer;database;regression models;regression problem;multi-task learning;temporal smoothness;alzheimer's disease
classification;data sets;predictive performance;associative classification;association rules;test set;association rule mining;large number of;modeling technique;interesting rules;class association rules;hypothesis testing;interestingness measures;classification model;association rule mining;class label;high-order;statistical model;interpretability;rule set;data set;classification rules;association rule;classifier
large-scale;social media;unseen data;news sources;information dissemination;real-world;text streams;inter-relationships;topic model;topic models;temporal dynamics;unified framework;multiple sources
classification;active learning;information-theoretic;accurately predict;social network;real-world networks;network structure;class labels;active learning algorithm
lower bound;active learning;active learning;labeled instances;motivating application;learning algorithms;instances;learning problems;unlabeled instances;support vector machines;data collected;standard datasets
disk block;distance-based outlier detection;data points;multi-core;outlier detection;large number of;13;indexing scheme;data blocks;very large datasets;distributed algorithms;reference point;nearest neighbors
clustering;mining algorithms;ground-truth;synthetic data;evaluation measure;data streams;evolving data streams;evaluation measures;data distribution
ranking algorithm;23;ranking function;information retrieval;ranking algorithms;graph laplacian;euclidean space;low dimensional;graph laplacian;image data;higher dimensional;query point
anomaly detection;detecting anomalies;application domains;pca based;stream data;structure information;principal component analysis;real-world data sets;data streams;low dimensional space
clustering;clustering;computational complexity;digital data;kernel-based;clustering algorithms;large scale;memory requirements;large data sets;kernel k-means;data organization;separability;real world data sets
labeled data is;active learning;classification accuracy;real datasets;learning algorithm;active learning methods;active learning methods;rates;higher accuracy;unlabeled instances;learning method
modeling approach;latent space;hierarchical classifier;compare favorably with;classification process;computational overhead;low dimensional;real estate;classification approach
similarity measure;complex networks;similarity metric;closely related;link-based;social networks;real datasets;iterative algorithm
bayesian network;knowledge discovery;penalty term;bayesian network;sample sizes;alzheimer's disease;theoretical analysis;structure learning of;alzheimer's disease
fmri) data;classification;cross-validation;higher accuracy than;classification;magnetic resonance imaging;classifier
clustering;domain knowledge;completeness;metric learning;instances;hierarchical algorithm;relative constraints;knowledge representation;higher accuracy than;relative constraints;clustering problems;knowledge sources;clustering
component analysis;synthetic data;covariance matrix;approximation error;stock market;component analysis;approximation guarantees;decomposition methods;iterative algorithms;low dimensional;principal component analysis;tensor decomposition
large networks;approximation error;real networks;weighted graphs;weighted graph
free-text;news sources;relevant content;textual content;information extraction;instances;iterative algorithm;structured data
cost-aware;real-world;massive amounts of;latent factor models;cost-aware;margin;decision making
mining task;synthetic data sets;network routing;uncertain graphs;computationally intractable;network applications;social network analysis;wide range;high probability;highly reliable;sampling scheme;depth-first search
ground truth;graphical models;stock market;scoring functions;nodes represent;social sciences
large volume;data mining and knowledge discovery;spatio-temporal;road network;data streams;traffic data
search queries;social influence;social graph;user search behavior;query-url;click graph
large graphs;mining task;ranking list;diversified ranking;real graphs
wikipedia-based;knowledge bases;real-life datasets;topic models;semi-supervised;model called;latent dirichlet allocation
online social networks;social interactions;real-world;prediction performance;link prediction;location-based social networks;user activity;temporal evolution;link prediction;location-based;learning framework;recommendation systems
fast approximate;fast approximate;nearest neighbor;similar object;greedy search;search cost;parallel processing;neighborhood graphs;data set;similarity search;success probability;search performance;greedy-search;graph-construction
nonnegative matrix factorization;text mining;variable selection;image processing;matrix factorization;coordinate descent;dyadic data;dimension reduction;objective function;convex optimization problem;times faster than
nearest-neighbor search;clustering;running times;euclidean distance between;large-scale;locality-sensitive hashing;nearest-neighbor;data processing;locality-sensitive hashing
social relationships;structural patterns;mobility patterns;human movement;high degree of;social network;human motion;location-based social networks;social networks;network structure;location data;human mobility;long-distance
real graphs;graph mining;recommendation systems;applications including;storage space;large graphs;instance;compression scheme;graph queries;social networks
human mobility;prediction accuracy;mobility patterns;large-scale;social interactions;predictive power;social network;social ties;mobile phone;link prediction;classifier
recommendation approaches;user interactions;user experience;community question answering;multi-channel;large scale;question answering systems;multiple types of
target distribution;test data;aggregate information;data obtained from;squared error;regression model;diverse domains
clustering;cluster structure;linear models;data compression;attribute dependencies;categorical attributes;heterogeneous data;minimum description length;data sets;data mining;clustering algorithm;interpretability
constraint satisfaction;hierarchical clustering algorithms;hierarchical clustering;constrained clustering
graphical models;natural language processing techniques;graphical model;quality measures;higher-quality;prior art
search space;classification techniques;classification;time series;time series;local patterns;case studies;data mining;building classifiers;benchmark datasets
convex optimization problems;predictive models;optimization methods;portfolio selection;stock market;optimization algorithms;optimization method;data mining algorithms
mining closed;search space;real-world datasets;discovering frequent;directed acyclic graphs;sequential patterns;complex patterns;data mining;pattern discovery
huge volumes of data;prediction problems;large datasets;data mining techniques
application domain;matching problem;data sources;user profiles
training data;hierarchical dirichlet process;semi-supervised learning;training process;statistical model;unseen data;main idea;semi-supervised;real-world applications
transfer learning;multi-view;optimization method;target domain;multi-view;large margin;large margin;transfer learning;multiple views;labeled data;source domain;multi-view;labeled data from;real world applications;web page;transfer learning;classifier
ranking scheme;ranking results;real-world;information retrieval;probability distribution;iterative algorithm;data mining;relational data
data-driven;data collected by;data collection;computational power;large volumes of;functional dependencies;highly correlated;sensor networks;data collected;lower cost;sensor nodes;rates;power consumption
vertex set;online social networks;computational power;graph model;statistical techniques;social network;instance;data sources;anonymized data;network size
semantic concept;learning process;document collection;feature space;semantic concepts;document representation;mining tasks
private information;anonymized data;accurate classifier;realistic data;private data
graph data;social network;data set;large-scale social networks;social networks;privacy-preserving;relational databases;heuristic approach;rich information;integer programming;small-scale
latent topics;web applications;heterogeneous information network;textual information;heterogeneous network;random walk;topic models;multi-typed;topic model;topic modeling;topic models;regularization framework;textual documents;rich semantics;network structures;heterogeneous information networks
sequence data;real datasets;candidate generation;skyline points;brute-force
context-aware;mobile devices;location privacy;location-based service;location data;location information;semantic information;location privacy;increasingly popular
data objects;network analysis;ranking results;heterogeneous information network;traditional classification;classification;classification framework;graph-based;classification methods;networked data;class membership;highly ranked;multiple types of;graph structure;real world;ranking model;heterogeneous information networks;ranking algorithm
dual problem;constraint satisfaction;real-world;linear programming;optimization problem;fine-grained
ubiquitous computing;web usage analysis;sequential pattern mining;sequential pattern;information loss
real-world datasets;nearest-neighbor;hidden web;databases
nearest-neighbor search;feature space;sequential-scan;vice versa;time series;multi-resolution;pre-computed;data mining task;time-series
clustering;supervised learning methods;synthetic datasets;classification;label space;large scale;labeled/training;optimization algorithm;large margin;optimization problem;online learning;labeled examples;real world applications;learning method
movement data;weighted average;privacy-preserving;spatial analysis;logistic regression
predictive accuracy;decision tree;large scale;data sparsity;log-linear;computational advertising;categorical variables;rates;small sample size;log-linear models;kalman filter;model called
model complexity;real data;data centers;machine learning;data center;forecasting model;sensor data;sensor measurements;automatically learning
synthetic data;data warehouses;sequence database;sequential patterns;real data;frequent itemset mining;case study;upper bound;sequential pattern mining
individual queries;user browsing;user-click;dynamic bayesian networks;search-behavior;probabilistic models;user click;user behavior;search queries;user search behavior;multiple queries;user modeling;search result pages;search session;click model
social relationships;transductive learning;social-network;textual features;sentiment analysis;social networks;semi-supervised;support vector machines;sentiment classification
markov logic networks;web pages;taking into account;structural features;real-life datasets;structured data from;web information extraction;information extraction;instance;markov logic networks;extraction methods;wide range;unified framework;maximum weight;inference methods
uncertain databases;uncertain database;large databases;ranking function;pruning methods;wide range;ranking functions
desired properties;database;index structure;transaction processing
synthetic datasets;data cleaning;index structures;efficient algorithms to;large number of;similarity functions;entity matching;similarity function;high accuracy;optimization techniques;entity matching
query execution;active rules;pattern queries;high-volume;complex event processing;event streams;data streams;query results;streaming environments;complex event processing
sampling technique;million nodes;trend detection;online social networks;share information;shared information;data set;social networks;detection methods;sampling rate;trend analysis;average precision;network topology;vast amounts of
compression methods;data compression;data compression;database design;query performance;physical design;decision support queries;storage requirements;real world;database engine;microsoft sql server
nearest neighbor;database;uncertain data;query processing;databases;uncertain objects;high probability
real data sets;poor performance;keyword search over;graph based;approximation algorithm;keyword search;query keywords
individual nodes;spectral analysis;spectral analysis;social networks;theoretical results;adjacency matrix
sparse matrices;social network;spectral analysis;adjacency matrix;real-world graphs;highly scalable
frequent subgraphs;sequential nature;unlike conventional;graph mining algorithms
distance computations
clustering;clustering algorithms;graph partitioning;distance function;distance measure;graph-based;clustering method;clustering process;graph-based;uci datasets
bayesian network;correlation-based;hill-climbing;learning algorithm;structure learning;correlation-based;bayesian network structure;local learning;combines ideas from
data sets;latent dirichlet allocation;large number of;social network;computationally expensive;user interests;topic distributions;link prediction problem;modeling approach;data mining problem;data mining problems;social networks
cluster based;detection algorithm;meaningful results;data set;expert finding;location information;social networks;information propagation
collaborative filtering;recommender systems;success rate
degree distribution;social network analysis;large social networks;clustering coefficient;social network;sampling method;data sets;social networks;sample sizes
clustering;clustering problem;community structures;ant colony optimization;complex network;community detection;optimization strategy;random walk model;data sets;ant colony;ant colony optimization;real-world;community structure;markov random walk;random walks
parameter-free;distance function;optimal values;time series;algorithm exploits;time-series;time series;algorithms rely on
instance selection;training set;training data;classification;distance measure;classification accuracy;selection problem;nearest-neighbor classifier;industrial applications;execution times;selection method;time-series;data mining task;instance;instance selection;time series;time-series;score-based;instances;nearest neighbors;classification algorithms
prediction model;historical information;regression analysis;real-world;clustering method;time-series;data mining;time-series
latent variable model;classification;feature extraction method;gaussian process;analysis reveals;spatio-temporal;feature extraction;latent variable models;time series;action recognition;temporal constraints;low dimensional representation;spatio-temporal
computational efficiency;access logs;data compression;detection accuracy;code-length;time series;code length;malware detection;risk management;data sources;data mining;maximum likelihood;artificial data sets;security issues
spam filtering;learning algorithms;compression-based;training data;worst case
frequent itemset mining;dynamic programming;c. aggarwal et al. kdd'; chui et al., pakdd'; chui and kao, pakdd';probabilistic database;sufficiently large;cpu cost;probabilistic databases;frequent 1-sequences;efficiently computing;mining sequential patterns;sequential pattern mining;sequential pattern
large-scale;large scale;activity recognition;large-scale;data set;temporal sequence;real-life;training methods;action recognition;real world;conditional random fields;action recognition
score function;dynamic programming;5;optimization problem;pattern extraction
data distributions;classification;error rates;learning algorithm;large margin;data set;data sets;classification error;data description;novelty detection;support vector;learning method
data point;clustering;density-based;class-label;anomaly detection;active learning;human experts;instances;rare class;real-world data sets;rare classes;class labels;nearest neighbors
real data sets;kernel density;kernel-based;large data sets;outlier detection;knowledge discovery;large databases;local neighborhood;detection methods;outlier detection;detect outliers;local density;detection performance;kernel function;outlier factor
evaluation measures;data mining problem;sentence level;entire process;retrieval results
classification;discriminative models;active learning;supervision;instances;rare class;inter-related;data mining;rare classes;supervised learning;classifier;standard datasets
margin;data distributions;margin-based;sampling method;large margin;imbalanced datasets;margin-based;sampling algorithms
nearest neighbor;classification;instances;exemplar-based;instance;training instances;learning strategies;nearest neighbor;class distribution;classifier;nearest neighbors
base classifiers;auc;optimization technique;classification;class distributions;negative examples;vector machine;accurate classification;positive examples;class distribution;model training;biological data;classification algorithms
classification performance;attribute values;imbalanced data;test instance;class labels;theoretical analysis;imbalanced data sets;nearest neighbors
agent based;ordinal classification;imbalanced class;classification;classification rules;multi-agent;classification approaches;individual agents;generated dynamically
data objects;algorithm performs;agent-based;heuristic rules;objective functions;subspace clustering;clustering methods;dimensional data;real world applications;subspace clusters;stock market;agent-based
pattern mining;constraint programming;concept-learning;pattern set mining;search strategies
learning process;low cost;generalized queries;predictive accuracy;active learning;specific queries;real-world situations;high cost;total cost;active learning algorithms;minimum cost
network analysis;pre-defined;web pages;web graph;individual nodes;random walk;propagation model;link-based ranking;link-based;influence propagation;fundamental problem;real datasets
bayesian network;search algorithm for;markov blanket;classification tasks;markov blanket;feature selection;algorithm exploits;search algorithm;computational savings;conditional independence
instance-based;classification;label ranking;learning algorithms;mining association rules;ranking algorithms;similarity functions;apriori algorithm;association rules;tree-based
data objects;similar objects;similar object;similar behavior;temporal context;complex data;subspace clusters
error-prone;retrieving information from;database;large amounts of data;similarity measure
clustering;distance-based outlier detection;classification;similarity measure;data-intensive;continuous data;real datasets;data mining techniques;categorical data;instances;direct comparison;similarity measure;categorical data;data-intensive;similarity measures
neural networks;classification;vector machine;ranking function;information retrieval;ranking problem;ranking methods;ranking functions;machine learning;loss functions
frequent item set;minimum support;database;item set;high quality;item set;item sets;mining algorithm;benchmark data sets;similarity measures
users' interests;ctr;major source of;search engines;search engine;automatically determined
prediction accuracy;multi-player;base learners;video game
interesting patterns
local features;manifold learning;3;1;algorithm called;2;feature based;feature descriptor;feature spaces;kernel framework;structured data;feature extraction methods;dimensionality reduction
hypothesis generation;decision support;free text;information retrieval;information fusion;data mining;security issues;health information;text mining;kdd community;data mining technologies;information visualization;missing data;observational data;quality assessment;healthcare data;data mining algorithms;medical records;pattern detection;real-world;medical imaging;business processes;topic areas;data driven;knowledge transfer;workshop on data mining;data mining applications;statistical analysis;databases
business applications;predictive model;predictive models;related topics;data transformation;post-processing;related information;kdd-2011 conference;predictive model;future directions for;semantic web;web services;data mining;data mining models;language modeling;predictive analytics
acm sigkdd;conference on knowledge discovery and data;machine learning;knowledge discovery;web site;data mining
game theory;electronic commerce;information processing;international workshop on;supply chain management;multi-agent;trading agents;distributed systems;artificial intelligence
data publishing;spatial information;data exchange;user location;user's location;anonymization techniques;social networks;data transformations;location-based;location based social-networks;location data;location based services;data management;user privacy
mobile user;service provider;user location;privacy-preserving;data publication;privacy concerns;location information;traffic monitoring;data analysis;mobile devices;service providers;user's location;location-based services;current location;desired information;social networking;location privacy;mobile users
data analysis;anonymity preserving;data points;sensitive information;data publishing;spatio-temporal;traffic control;trajectory data;location-based service;external information;moving object;location-aware;temporal data;moving object databases
privacy-aware;privacy concerns;mobile devices;extra information;privacy preserving;location-based services;data server
data objects;information sources;heterogeneous network;multi-typed;information providers;sensor networks;data mining;heterogeneous network;social sciences
mining algorithms;high-end;output quality;data stream mining;data stream processing;optimization problem;data streams;quality measures;resource constraints
distributed algorithm for;instances;algorithm performs
count-based;image segmentation;31;28;integer linear programming;vision problems;markov random fields;cost function;marginal probability;histogram-based;high order
competitive performance;instances;globally optimal;image labeling;globally optimal
graph-cuts;image restoration;labeling problems
spatio-temporal;domain models;regularization;computational complexity
high dimensional;takes into account;probability distributions;color images;numerical results;kullback-leibler divergence;texture segmentation
dual decomposition;graphical models;dual problem;convergence rate;graphical structure;large number of;energy minimization;1;upper bound;graph connectivity
image segmentation;optimization methods;graph cut;numerical results;continuous optimization;segmentation algorithm
labeling problem
upper bound;continuous domains;globally optimal;labeling problems
appearance model;semi-supervised learning;interactive segmentation;object appearance;multi-class;interactive segmentation;spatial correlation
linear program;14;25;linear programming;linear programs;markov random fields;original formulation;multi-label;higher order
regularization;taking into account;image segmentation;optimization methods;probability density functions;input space;bayesian formulation;segmentation accuracy;spatially varying;lighting conditions;variational framework
video stream;temporal information;linear programming;energy minimization;object detection;model selection
original formulation;regularization;consistency constraints;optimization framework;global optimization
image segmentation;large-scale;high resolution images;maximum flow;labeling problem;labeling problems
multi class;energy function;image segmentation;local optima;level sets;gradient descent;energy functional;discrete optimization;prior information;graph representation;discrete optimization
energy function;approximate algorithm;image segmentation;segmentation techniques;shape model;shape prior;coordinate descent;markov random fields;global constraints;segmentation quality;low-level;shape variations
high throughput;high-throughput;high resolution;manual labeling;images acquired;high resolution
video sequence;ground-truth;post-processing;optical flow;image sequences;unified framework;optical flow;benchmark datasets
data-driven;data-driven;high-dimensional;particle filter;euclidean space;computational cost
high correlation;regularization;flow fields;linear combination;low-rank;optimization scheme;benchmark dataset;energy functional;multi-view;optical flow;long term;soft constraint;motion capture data;ground truth;variational framework
energy functional;question arises;flow field
optical flow;1;regularization;vector valued
singular value decomposition;database;patch-based;sliding window;gaussian noise;1;2;squared error;motion estimation;video sequences;higher order
structured output;million images;latent variables;input features;multiple objects;single label;evaluation measure;case study;binary classifiers;limited number of;multi-label;object category;loss functions;object classification
conditional random field;multiple-instance learning;multiple-instance learning;instance-level;instances;classifier
multiple objects;theoretical results;data set;pascal voc;object detection;inter-related;greedy algorithm
data-driven;general case;implicit assumption;general problem;similarity measures;objective function;shape analysis
shape representation;fourier transform;frequency domain;higher-order;density function;distance transform
multi-class problems;mutual information;classification;data mining;large number of;classification performance;data sets;weighting scheme;computational cost;feature weighting;benchmark data sets;model called
survival analysis;linear classifier;pattern recognition;feature space;regression models;censored data;regression methods;separability
data points;data description;classification;anomaly detection;cost function;outlier) detection;support vector;data description;support vector
cross-validation;decision trees;data mining methods;decision tree;black box;data set;tree induction;neural nets;visualization methods;data collection;huge volumes of data;error rate;support vector machines;identify patterns;user interaction;data mining methods
base classifiers;classification;database;error rates;machine learning;classifier;integrates multiple
naïve;data mining applications;learning algorithms;data acquisition;data acquisition;learned model;cost-sensitive learning;total cost;machine learning;model building
clustering;medical datasets;hidden patterns;medical data;intelligent agents;classification;extracted knowledge;medical data;medical datasets;knowledge discovery;data mining tools;artificial intelligence;data mining;data mining algorithms;huge number of;decision making
classification;learning models
gene clusters;model-based clustering;clinical practice
rule sets;special attention;data mining tools;knowledge base;classification
clustering;high-dimensional;unsupervised classification;von mises;unsupervised classification;local neighborhood;multivariate data;remote sensing data;statistical models;feature vectors;expectation-maximization
key features;decision trees;decision tree
data collection;ground-based;spatial clustering;hierarchical clustering;data records;data set;data sets;spatial resolution;similar characteristics;data mining;data mining problem;hierarchical agglomerative clustering;information technology
ground truth;gps data;success rate;classification;low-cost;activity recognition;feature extraction;position data;low-cost;rates;machine learning techniques
search space;meaningful patterns;time series;time series;frequent sequential patterns;temporal evolution;real data;support threshold
processing units;statistical methods;database;manufacturing process;data set;data sets;feature selection
vector machine;independent component analysis;recognition tasks;process control;input variables;independent components;artificial neural network;recognition problem
data mining technique;neural networks;classification;decision tree;decision-making;data instances;inference mechanism;decision trees;data mining
growing number of;data analysis;graph-based;data mining;model offers;large graph;multiple data sources;multi-view;data collection;data warehousing;graph-based;graph analysis;data-mining techniques
decision trees;training data is;data mining techniques;time series;accurate predictions;data mining algorithms
high precision and recall;web pages;dynamically generated;application domain;web browsers;web sites;deep web;data records;data items;instance;data sources;web pages;visual features;relevant information;extracting data from;learning method
operator;management systems;data mining
classification;nearest-neighbor;lessons learned;bioinformatics datasets;feature selection methods;dimensional data;classification algorithms
statistical methods;data preprocessing
5,8;1,6;wikipedia) articles;optimization problem;2;4;7;data mining;9;data mining based
human mobility;mobile communications;predictive power;resource management;activity patterns;data mining
web search;recommendation systems;real-world;machine learning;case studies;probabilistic inference;data storage
problem remains;sponsored search;computational advertising;high dimensional;users' behavior;computational advertising;online advertising;search engine;page ("content;search problems
kernel machines;naturally leads to
multidimensional 0-1 data
customer segmentation;data mining
search engines;geographical information
recommender systems;classification;data availability;machine learning;labeled data;machine learning algorithms
data mining
desired behavior;preference-based;search space;policy learning;policy learning;reinforcement learning;machine learning approaches;optimal control
clustering;learning process;background information;constrained clustering;constraint sets;clustering task;databases;semi-supervised;instance level
clustering algorithms;quality metrics;cluster quality;cluster structures;graph clustering;real world graphs;quality metric;graph clustering
real-world datasets;instance-based;transfer learning;learning algorithms;boosting algorithm;classification performance;classification results;knowledge transfer;labeled examples;learning performance
information-propagation;statistical significance;information diffusion;large number of;real data
real-world datasets;predictive models;classification;data distributions;pattern mining;logistic regression model;statistical analysis;logistic regression;class labels;data distribution
clustering;data structure;38;25;31;theoretical foundation;data sets;algorithm exploits;density function;clustering algorithm;objective function;clustering algorithms;clustering quality
data analysis;sensitive information;propagation algorithm;semi-supervised learning;learning algorithm;cryptographic techniques;labeled graphs;privacy preserving;real world networks;label propagation;privacy preserving
binary classification;information fusion;object recognition;pattern recognition;linear programming;ensemble methods;regularization;complementary information;fusion methods;multiclass problems;level fusion;classifier fusion;multiple kernel learning;classifier
gradient-based;13;local optima;real-world;learning algorithm;finite-state;input-output;real data;large data sets;hidden markov models;finite state;operator;numerous applications in;computationally expensive
error metrics;probabilistic framework;recommendation process;collaborative filtering
edit distance;classification models;distance functions;classification;generalization performance;similarity learning;learning algorithms;similarity functions;similarity function;discriminative power;structured data;theoretical guarantee
graph theory;data analysis;unlabeled data;semi-supervised feature selection;labeled data;feature selection;feature selection methods;dimensional data;informative features
objective function;machine learning methods;semi-supervised learning;learning algorithm;neural network;cost sensitive;optimal parameters;label propagation;semi-supervised;prior knowledge;machine learning algorithms;node labels
regularization;latent variables;time series;large data sets;data sets;feature analysis;sparse kernel;learning method
random variables;monte carlo;model selection
probability estimates;multi-class classification;classification model;conditional distributions;calibration method;ranking function;probability distribution;ranking algorithms;documents retrieved;information retrieval performance;benchmark datasets;multi-class;scoring function
network datasets;information diffusion;sampling algorithm;active learning;diffusion model;model parameters;real world networks;learning problem;objective function;random sampling
auxiliary;sampling algorithm;hierarchical dirichlet process;dirichlet process;dirichlet process;sampling based;convergence speed;data item;hierarchical dirichlet process;latent dirichlet allocation
preference-based;preference learning;preference-based;label ranking;policy learning;classification methods;reinforcement learning;machine learning;case studies;ranking functions;learning method
social media;loss function;entity types;multi-relational;training dataset;item recommendation;multiple relations;random walks
clustering;clustering;numerical experiments;fourier domain;unsupervised learning;0, 1;functions defined;real data;instance;automatic feature selection;fourier domain;cluster analysis
classification;active learning;real world datasets;graph laplacian;classification method;operator;laplace-beltrami;classification algorithm
classification technique;classification problem;classification;classification;linear models;linear classifier;decision process;average number of;multi-class;classifier
manifold structure;unlabeled data;labeled data is;classification;real world datasets;semi-supervised learning;label propagation;semi-supervised learning;theoretical analysis;data points;data arrive
highly relevant;ground-truth;semi-supervised setting;labeled instances;automatically generated;labeled data is;instance;named entity;recognition tasks;natural language processing;semi-supervised;prediction models
clustering;model complexity;training set;training data;test set;test data;generalization error;model-order selection;correlation clustering;real-world;cross-validation;controlled experiments;learning problems;image denoising;singular-value decomposition;access-control;unsupervised learning
special attention;reward function;decision-making;markov decision processes;mathematical framework;optimal policies;reward functions
classification models;label noise;label noise;inference algorithm;hidden markov models;noise model
optimization strategy;svm models;classification;maximum number of;multi-instance;vector machine;optimization problem;multi-instance;instances;convex optimization problem;learning problem;support vector machines;classifier;prediction models;benchmark data sets
dual decomposition;decomposition approach;markov decision processes;markov decision processes;upper-bound;lower-bound;planning algorithms;markov decision process
predictive model;partially observable;reinforcement learning;unsupervised learning
training data set;ensemble methods;data set;incremental learning;training data;cost function;theoretical guarantees
memory requirements;search space;pruning techniques;efficient discovery of;discovery algorithm
face recognition;linear transformation;great success;linear discriminant analysis;data sets;optimization problem;dimensionality reduction;gradient descent;subspace learning;feature selection;integer programming;unified framework;dimensionality reduction
clustering;density-based;arbitrary shape;clustering approaches;cluster model;subgraph mining;data sources;knowledge extraction;subspace clustering;clustering algorithm;feature vectors
generative process;data sets;probabilistic logic;em algorithm;estimation algorithm;programming language
ranking algorithm;ranking techniques;information-theoretic;jensen-shannon divergence;probabilistic approach;feature selection;dimensional data;rank correlation;feature selection
partial orders;web-based;mining frequent;sequential patterns;mining frequent;partial order
game theoretic;multiple users;high-quality;recommender systems;highly ranked;data privacy;personalized recommendations;game theory
closely related;data generation;knapsack problem;real world datasets;anomaly detection;learning algorithm;multiple datasets;coordinate descent;data mining
clustering;topic-specific;data set;social influence;social network
design method;labeled data
instance-based;classification;decision tree;database;time series;instance-level;algorithm named;classification;classification process;multiple datasets;data mining;distance metrics;time series;indexing technique;classifier;benchmark datasets
classification techniques;vector representation;tree structure;graph mining;image classification;mining algorithm
markov logic networks;parameter learning;training examples;main memory;markov logic networks;structure learning;online learning;computationally expensive;large datasets;natural-language
probability distributions over;hybrid approach;low dimensional;multi-target tracking
high-dimensional;spectral clustering;spectral clustering;irrelevant features;feature selection method;large number of;data clustering;graph laplacian;feature selection;feature selection methods;multi-class;feature based
real-world datasets;numerical experiments;probabilistic generative model;multiple layers;hidden nodes;multi-view learning;belief networks;multiple views;deep belief;deep belief
clustering;automatically extracted from;video sequence;object tracking;mixture model;feature trajectories;em) algorithm;numerical results;expectation-maximization;key points;image sequence;model parameters;model-based clustering;translation invariant;motion segmentation
search space;algorithm performs well;input data;database;major source of;life sciences;tree structure;high-throughput;statistical significance;greedy heuristics
application domains;pruning techniques;real-life datasets;low support;mining association rules;meaningful results;frequent itemsets;transactional databases;large datasets;correlated patterns
classification;evaluation measure;ranking performance;learning models;supervised learning algorithms;probability estimation
learning process;classification;predictive power;learning tasks;local information;classifier;weak classifiers
domain knowledge;clustering;high levels of;dynamic programming;classification;sign language;data mining tasks;application domains;distance measures;interval-based;sensor networks;bipartite graph
fast algorithm;classification;anomaly detection;semi-supervised learning;real datasets;random walk;belief propagation;higher accuracy than;fast algorithms
clustering;high-dimensional;high-dimensional feature space;gaussian mixture models;multivariate data;transaction data;kalman filter
gene-expression data;classification;classification method;highly correlated;relational structure;statistical relational learning;normal distribution;numerical data;classification problems
low-quality;reviewing process;high-quality
target task;gaussian process;transfer learning;structure learning;data set;data sets;gaussian processes;dirichlet processes;process models;multi-task learning;negative transfer;learning task
stochastic search;monte carlo;decision problem;reinforcement learning;reinforcement learning;stochastic search;stochastic process;knowledge representation;reinforcement learning;upper bound;multiple agents;high-level;autonomous agents;partially observable domains
large numbers of;large corpora;takes into account;false positives;natural languages;text corpus;text corpora;spatial distribution;word frequency;spatial patterns
association rules;interval-based;real-world domains;information theory
dimensionality reduction technique;nearest neighbor;minimum number of;intrinsic dimensionality;probability density function;noisy data;machine learning techniques;dimensional data;estimation techniques;real datasets
clustering;special case;machine learning problems;graph-based data;graph evolution;semi-supervised learning;large number of;graph evolution;stochastic process;wide range;diffusion process
clustering;machine learning problems;classification;data points;optimization approach;learning approaches;data representation;learning tasks;low rank;dimensional data;real world;classifier;discovery problem
clustering;clustering problem;gene expression data;low variance;learning algorithm;pca based;feature selection;feature selection process;pca-based
large graphs;latent features;stochastic gradient descent;topological structure;matrix factorization;link prediction;link prediction problem;directed) graph;link prediction
feature space;discriminative models;decision tree;generalization performance;learning tasks;data sets;individual features;internal nodes
linear programming;map) estimation;optimization problem;probabilistic graphical models;map estimation
classification performance;label information;multi-label classification;ranking tasks;multi-label;evaluation measures;classification task
prediction accuracy;regularization;numerical experiments;low rank;low-rank;iterative algorithms;tensor decomposition;auxiliary information;benchmark datasets
classification problem;prediction methods;latent features;real data;application domains;large networks;link prediction problem;link prediction;network structures;kernel framework;network structures
prediction accuracy;regularization;informative features;convergence rate;highly skewed;online learning;noisy data;computational cost;supervised learning;low-frequency
web search;movie database;social-network analysis;real datasets;iterative algorithm;link analysis
fast approximate;clustering;clustering problem;fourier transform;text document;clustering task;document clustering;high accuracy;discrete cosine transform;sampling technique;document set
dynamic programming;posterior probabilities;latent) variables;conditional independence;bayesian networks;observational data;equivalence class;network structure;network structures;algorithm to compute;bayesian learning
base classifiers;classification;information sources;multi-view learning;performance guarantees;training error;making predictions;multi-view;high probability;data types;data type
multi-agent systems;single agent;multi-step;decision-theoretic;continuous variables;partially observable;finite state;reinforcement learning;expectation maximization;partially observable markov decision processes;convex optimization problem;partially observable domains;local optima
expectation maximization algorithm;plans;plan recognition;bayesian networks;plan recognition;benchmark data sets;probabilistic graphical models
regularization;manifold learning;regularizer;data manifold;feature extraction;input space;higher order
main memory;multiple queries;selectivity estimation;data-mining;database;large scale;maximum number of;sql queries;random sample;large database;selectivity estimates;theoretical analysis;high probability;estimation techniques;microsoft sql server;selection predicate;join operations
sparse kernel;capacity control;sparse kernel;basis functions
social media;information propagation;information consumers;vice-versa;social media
statistical methods;inverse reinforcement learning;preference elicitation;preference elicitation;posterior distribution
machine learning;manual effort;feature selection
regularization term;classification;linear models;regularizer;parameter values;learning task;learning problem;application domain;predictive accuracy;domain expertise;risk minimization;related data;depends crucially on;text classification;transfer learning;feature weights;prediction error
mixture components;directly optimize the;sampling approach;gaussian distributions;active learning;faster convergence;computational cost
domain knowledge;real-world datasets;domain adaptation;active learning;target domain;active learning;source domain
high-dimensional;synthetic data sets;density estimation;filtering step
rfid technology;cost-effective;data collected;machine learning approaches;real-world
takes into account;classification tasks;sampling method;learning tasks;target variable;target variables;single-label;multi-label data;multi-label;evaluation criteria;random sampling
6;relevant objects;object-based;query types;query type;learning problem;learning algorithms
cutting plane algorithm;support vector machines;classification;directed acyclic graphs;parallel implementation;sampling strategy;support vector machines
taking into account;class label;data instances;multi-class;classifier;multiple) labels
target task;classification accuracy;transfer learning;code length;source task;text data sets;source tasks;high cost;negative transfer;transfer learning
ensemble learning;optimization method;multi-objective;multiple labels;multi-label learning;learning approaches;real-world;learning systems;objective functions;learning tasks;instance;generalization ability;single-label;base learners;multi-label;multi-label
training set;labeling effort;active learning;rule-based;sampling method;inference rules;labeled examples;training sets;rule-based;active sampling;query results;ranking model;algorithms rely on;test sets;unlabeled set
clustering;running times;cluster membership;large graph;parameter settings;graph clustering;databases;large database
structured output;unlabeled data;active learning;real-world;active learning approach;conditional random fields;data sets;marginal probability;labelled data;unlabeled data;semi-supervised;supervised classification;conditional random fields;learning method;natural language
order statistics;probabilistic models;machine learning;kullback-leibler divergence;real world;comparative analysis;markov model
numerical results;feature selection;regression problems;highly scalable
learning approaches;nearest neighbor;covariance matrix;dictionary learning;manifold structure of;machine learning;taking into account;positive definite;dictionary learning;nearest neighbor
clustering;network datasets;predictive models;decision trees;clustering trees;multi-objective;related entities;classification;regression models;tree induction;prediction problems;multi-target;network data;real world;machine learning and data mining;network data;response variable;data mining method
clustering;prediction performance;kernel k-means;quality control;low) quality;labeled examples;supervised learning;real world applications;model selection
clustering;clustering;clustering results;encoding scheme;data set size;real data sets;code length;clustering result;lower bounds;code length;numerical data;input parameters;low computational cost
social relationships;online social networks;graph model;learning algorithm;large networks;data sets;social ties;semi-supervised;automatically infer;social relationship;partially-labeled
mining algorithms;data mining results;binary data;meaningful results;data mining;kullback-leibler divergence;interpretability;data mining methods
received increasing attention;predictive models;monotonicity constraints;logistic regression;multiple criteria;machine learning;aggregation operator;input variables;additional features;decision making;positive class
prediction accuracy;domain adaptation;real world datasets;machine learning algorithms;invariant features;source) data;labeled data;feature selection;learning approaches;labeled data from;common assumption;real world;transfer learning;convex optimization problem
ranking algorithm;classification;semi-supervised learning;rank documents;labeled documents;ranking functions;single-view;semi-supervised;multilingual documents
beam search;result sets;mining algorithms;high cardinality;exhaustive search;numeric) attributes;complex data;discovery algorithms;correlated attributes
real-world datasets;latent dirichlet allocation;multiple documents;topic models;variational bayes;document collections;stochastic optimization;objective function;latent dirichlet allocation
social relationships;parameter estimation;community-based;probabilistic model;conditional random field;social network;markov networks;social networks;real-world;traditional classifiers;community structure;large-scale social networks
error rate;classification models;real networks;statistical tests;instances;6;network data;classifier
binary code;language model;compression technique;classification;machine learning
numerical experiments;factor matrices;user-item;model parameters;sampling methods;collaborative filtering;variational inference;inference algorithm;cold-start;matrix factorization;data matrices;content information;users' preferences;bayesian approach;data matrix;matrix factorization
gaussian mixture model;bayesian information criterion;classification;instances;majority voting;maximum-likelihood;probabilistic approach;map) estimation;prediction tasks;data mining;supervised learning
clustering;document understanding;summarization method;multi-document summarization;unsupervised learning;users' satisfaction;semi-supervised learning;visual representation;valuable information;document summarization;interactive visualization;multi-document summarization;multi-document;summarization methods;semi-supervised;visualization techniques;automatically generate;user study;user interaction;benchmark datasets
unlabeled data;data points;active learning;distribution information;labeled data;discriminative information;selection criterion;margin-based;benchmark data sets;learning method
active learning strategies;search space;active learning;decision boundary;active learning;streaming data;instances;instance;streaming data;data distribution
statistical methods;pattern analysis;final step;social networks;automatically-generated;social intelligence;online news
clustering;open source;online algorithms for;classification;data sets;online learning;easily extensible;evolving data streams;graph mining;real world;data feeds;real world data sets
1;efficiently identify;space) constraints;similarity functions;graphical interface
image classification;classification;image features;large-scale
activity recognition;activity recognition;gait recognition
mining algorithms;processing algorithms;pattern mining;interesting patterns;interestingness measure;interesting itemsets;decision trees;association rules;interactive visual;mining algorithm;interestingness measures
classification scheme;vector space;visual content;web pages;high accuracy;svm classifier;classifier
multi-dimensional;spatial locations;user specifies;sql queries;web service;spatial objects;data model;duplicate elimination;database;geo-referenced;user-friendly
textual data;association rule mining;large-scale;text corpora;text collections;association rules;evaluation measures
graph theory;visual analytics;computational methods
life sciences;end users;management systems;web) services;allowing users to;scientific workflow
data management techniques;energy-efficient;energy management;data management;energy-efficiency
review process
service provider;takes into account;service providers;location information;social networks;communication costs;location data;location privacy
multiple queries;database systems;database;ibm db;statistical modeling;aware query;database performance
large volume;spatial distributions;approximate algorithm;database systems;brute-force approach;large-scale;spatial distance;tree nodes;spatiotemporal data;space-partitioning;wide range;scientific databases;modeling approach;spatial distance;scientific discovery;brute-force algorithm
schema design;matching methods;data sources;efficiently identify;matching records;similarity predicates;high-quality;special case;record matching
data sharing;semi-honest;game-theoretic approach;data bases;game theory;sensitive information;data providers;private data;large data sets;real-life;essential information;data tables;data integration;data analysis;integration process;data integration
maintenance cost;graph mining;database;low-cost;subgraph isomorphism;filtering technique;query answers;index construction;candidate set;query processing
type-ahead search;keyword queries;information systems;user types;efficient algorithms to;information-access;high efficiency;search algorithms;large amounts of data;query keywords
data providers;privacy policies;data management;relational database system;quantitative analysis;user communities;data storage
domain experts;generalization hierarchies;generalization hierarchies;data analysis;small size;anonymized data;numerical attributes;automatically generate;information loss;data anonymization;data owner
graph model;statistical properties;large number of;privacy-preserving
tree index;encoding scheme;encrypted data;data updates;query efficiency;data availability;data security;range queries;security analysis;query processing;data confidentiality;data security;prior works;relational data;data management
sensor networks;sensor network;sensor network;security issues
multiple users;access control mechanism;context-aware;privacy concerns;data stores;sensory information;personal data;privacy-preserving;personal information;user privacy

search queries;real world applications;space complexity;encrypted data
privacy guarantees;sensitive information;database queries;large databases;privacy risk;information retrieval;database;database server;privacy preserving;query processing;computational cost;query response times
personal data;data protection;business model
human mobility;data-driven;data transformations;frequent patterns;large volumes of;mobile communications;quality assessment;mined patterns;large-scale;trajectory data;incremental mining;mobile phone;behavioral patterns;wireless networks;knowledge discovery process;visual exploration of
graph partitioning;historical data;access method;query optimizer;access methods;commercial applications;data flows;related queries;index structure;trajectory data;moving objects;cost model;databases;objects moving;storage requirements;query load;tree-based;moving objects databases;spatio-temporal;range queries;data distribution
uncertain trajectories;nearest neighbor;road networks;efficient algorithms to;nn) queries;tree structure;nn queries;nearest neighbor;naïve;nearest neighbors
data stored;trade-offs;moving object;compression technique;trajectory data;computational costs;mobile objects;communication cost;originally designed;wireless network;moving objects databases
shortest paths;nearest;spatial proximity;synthetic datasets;road network;euclidean space;queries efficiently;location-based;user's preferences;query point;dimensional euclidean space
dimensional space;memory usage;evaluation strategies;data structures;databases;operator;query returns;traffic analysis
medical records;individual records;large numbers of;data accesses;access logs;data stored;database;simple queries;discovering frequent;large number of;sensitive data;databases;user groups;user-centric
naive implementation;ad-hoc;amazon's mechanical turk;query interface;pre-filtering;user interface;workflow engine
service provider;database;streaming model;problems require;complexity classes;theoretical results;linear space;data owner
privacy-aware;filtering step;moving-object;considerable effort;knn queries;privacy-policy;user's location;service providers;query processing;location-based services;increasing attention;location privacy
clustering;data structure;suffix tree;data compression;multi-core;main memory;shared-memory;time series;tree construction;human genome;disk-based
enterprise applications;algorithm exploits;multi-core;update intensive;databases
accuracy compared to;historical data;social network;social influence;social graph;approximation algorithm;np-hard
process planning
removal rate
identification method
parameter space;line segments;large number of;prior information
data structure;service oriented;distributed computing;computing environment;mapping generation;object oriented;distributed computing;mapping systems

supply chain management;supply chain
structure parameters;control scheme;control algorithm;simulation result
agent technology;multi-agent system;agent technology;decision making
high-quality;closely related;design process
semi-automatic;tree model;semi-automatic
high speed;sensor network;high speed
spatio-temporal;control method;control strategy
high cost;high risk
fourier transform;empirical mode decomposition
simulation results;flow field
power factor;simulation analysis;matlab/simulink;auxiliary;simulation results
neural network;hidden layer
design method;parametric design;template-based;knowledge base;template-based;design process;knowledge-intensive;main components
simulation studies
fuzzy neural network;fuzzy neural network;emotion recognition;emotion recognition
matlab/simulink
regression analysis;statistical significance;logistic regression;database
control scheme;control strategy
diffusion process
theoretical analysis;performance guarantees
analysis tool;main idea
feedback controller;large-scale;closed-loop;adaptive control;adaptive control
vector machine;control method;numerical results
simulation results
sufficient condition for
high-speed;data acquisition;data acquisition;data stream;solid-state;high speed;solid-state;high speed
linear programming;relative entropy;maximum flow;maximum flow
video sequence;latent dirichlet allocation;spatial-temporal;human action;probability distributions;video surveillance;identification method;automatically learn
vector machine;pattern recognition;classification;machine learning
main components


case study;systems support
synthetic aperture;motion parameters;motion parameters;finite element method;motion information;high resolution
low-resolution;target recognition;classification;high-precision;multi-channel;target recognition;high resolution;target recognition;field programmable gate;target detection

robot control;data fusion;multi-sensor;path-planning;simulation results;multi-sensor

free energy
numerical simulation;numerical simulation;low cost
active power;control method;control strategy;active power
semantic similarity between;vice versa

higher precision;hard disk;large-scale;digital signal;test results;hard disk;high speed
image deblurring;regularization;image deblurring
clustering;test images;image denoising
shortest paths;path planning;motion paths;multiple tasks;free space;single-item;user interface;position data;shortest path;task allocation;greedy algorithm;heuristic algorithm
intrusion detection;intrusion detection system;network monitoring;fp-growth;security problems;data mining;network structure
prediction accuracy;genetic algorithm;soft sensor;vector machine;support vector machines;radial basis function;simulation results
remote sensing images;domain experts;high-level;classification;image features;concept-based;error rates;heuristic search;case study;reusability;high-level;expert knowledge
domain knowledge;domain knowledge;database;apriori algorithm;threshold selection;association rule
classification approaches;data sets;scene classification;low-level;scene classification;classification approach;low-level
fuzzy control
feature maps;false positive;canonical correlation analysis;classification;correlation-coefficient;low frequency;true positive;spatial structure;rates;canonical correlation analysis;image sequence;hand-labeled;low frequency

flow field
numerical simulation

simulation results

structure parameters;structure parameters
quality function;product family;quality function;product family
high-level;supervision
sampled data


state transition;optimization problems;state transition;optimization algorithms;operator;continuous functions
small-size

digital signal;high precision
wireless network;communication network

structured light;machine vision;structured light

control law;magnetic control;magnetic control
high risk
numerical simulation
numerical analysis
structured light;machine vision;machine vision;structured light;error analysis;information extraction
numerical simulation
wavelet analysis;signal processing

high-speed;cost-effective;area network
geographic information systems;remote sensing

knowledge base;training samples;fuzzy neural network;neural network;bp algorithm
impact analysis;mathematical model of
high speed;high speed;high speed
calibration method
speech recognition;recognition rate;feature vector;speech recognition
propagation algorithm;tracking algorithm;fuzzy neural network;neural network
great significance;data analysis;sample data;mathematical model;detection results;statistical analysis;large sample;large sample
radio frequency identification
wireless sensor network;signal processing;human-machine;wireless sensor network
sufficiently high;finite element analysis
data transmission;long-distance

common features
genetic algorithm;genetic algorithm
additional features

vector machine;extracted features;empirical mode decomposition;feature vector;svm-based
modal analysis;finite element model;finite element;large-scale
simulation analysis;simulation study
high-precision;local minimum;bp algorithm;rbf neural network;regression algorithm;rbf neural network;radial basis function;neural network;low efficiency
high similarity

auxiliary;machine tools

design method
capacity planning;mathematical formulation;long-term;real-life;capacity planning;long-term;integer programming
high accuracy
mathematical model;auxiliary;mathematical model
cold-start
simulation analysis;theoretical basis for
case study
case study;integrated environment
phase space reconstruction;mutual information;forecasting accuracy;vector machine;identification method;phase space;classifier

large volume;high quality;auxiliary
signal processing
finite-element analysis

high efficiency;numerical simulation;low cost;flow field

spatial analysis
long-term
classification learning;multi-objective;multi-objective;classification learning;partial order;machine learning;segmentation method;continuous attributes;higher accuracy
critical task;simulation results;data processing;event-driven
mathematical models;simulation analysis;high precision
mathematical models
control method;simulation results;dynamic behavior
multi-body;machine tools;error model;multi-body;operating conditions;machine tool
high temperature;high temperature;high temperature;error analysis;remote sensing
projection-based;high-accuracy;conventional methods
finite element model
computational complexity;global optimal;design method;error rates;data stream;simulation results

object oriented;simulation model;production line
bp neural network;bp neural network;feature extraction methods

neural networks;membership functions
machine tool
sensor technology;sensor networks;monitoring systems
control theory;fuzzy controller
theoretical basis;prediction model;simulation results;mathematical model of
dcm;sampling methods
numerical simulation;vector machine;control method;support vector;regression model;optimal control;high dimension;support vector;small samples;local minima;regression model;optimal control
quality control
production process;production process;artificial intelligence
low frequency;adaptive control;neural network;accurate tracking;high resolution;control scheme;mathematical model of;neural network
multi-channel;simulation results;low efficiency
simulation results;closed-loop;control law;adaptive control;control problem


numerical simulation;numerical simulation;finite element analysis

multi-axis;machine tool;machine tools
factors affecting


path planning;measurement data;path planning

high temperature
simulation results;simulation model
finite element model;simulation results
computational models
data acquisition;data acquisition;data acquisition
numerical analysis
factor analysis;quality control
numerical simulation;simulation result
instance

instance
database
parameter optimization;design requirements;optimal design
mathematical model;optimal design;optimal design
matching algorithm;similarity measure;optimization method;search strategy;multi-resolution;search strategy;multi-resolution;multi-scale;template matching;multi-scale
code generation;data processing;data processing
error rate
user interfaces;knowledge acquisition;knowledge-based;knowledge base
accurately detect;multi-sensor;data fusion
internet-scale;low-cost;network data;address space
intelligent control;open source;development environment;data transmission
design requirements;design parameters;structure parameters;database;parametric model;automatically generate


high level;ant colony;high reliability;high resolution;particle filter;multi-sensor;design requirements
control systems
high-speed;high-speed;simulation results
current location;relative error;test results;closed-loop
historical data;object-oriented;design method;neural network model;rbf neural network;rbf) neural network;production data;making predictions;radial basis function;neural network;environmental data;programming language
digital signal;high-resolution
genetic algorithms;fitness function;optimization model;genetic algorithms;optimization process;programming language

dynamic range

semi-automatically;high efficiency;cost-effective;low efficiency



power factor;power consumption;mathematical model of
classification;high speed
long-term;working principle;iterative learning;iterative learning
parametric design;parametric design
fitness
ant colony optimization;sensor networks;higher accuracy;sensor nodes;sensor networks;ant colony optimization
high-speed;data acquisition;data processing
open source;application development
design method;multi-body;long-distance
working principle;description language;image sensor;image sensor
intelligent control;multi-sensor
position data
vision-based;image sequence
linear regression
large numbers of
spectral analysis
air quality
4-(4-chlorobenzoyl)phenoxyl
low-temperature;low temperature;high-temperature
generation algorithm;instances
threshold algorithm;wavelet transformation
detection accuracy;detection method;low accuracy;detection approach
design method;simulation analysis;finite element
high-speed;high-speed;modal analysis;dynamic analysis;static analysis;dynamic analysis;finite element analysis
great significance;finite element analysis;qualitative analysis;finite element model;finite element;finite element analysis;theoretical analysis;finite element model


regression algorithm;data integration

assembly process;finite element analysis;static analysis
source nodes;iterative algorithm;simulation results
compared with conventional;finite element analysis
optimization process;parameter optimization
embedded systems;industrial applications
1;measurement data;image analysis
real-time monitoring;real-time monitoring;production line;production line;data processing
svm-based;vision-based;detection approach;vehicle detection;svm classification;classifier
recognition algorithm;blind source separation;recognition algorithm;noise reduction
high reliability
error rate;fast response
special characteristics;mathematical model;risk management;risk management;evaluation index system;theoretical basis for;quantitative analysis;risk factors;power grid
data analysis;signal processing
bp neural network;local minimum;neural network
qualitative analysis;powerful tools for;remote sensing
power factor;closed loop;control algorithm;control strategy;mathematical model of
simulation results;dynamic range
low-temperature;higher degree of
supply chain;supervision
simulation results
climate data;theoretical basis for;classification;local region
higher precision;neural networks;inverted pendulum;bp neural network;inverted pendulum;training data is;bp algorithm;hidden layer;artificial neural networks;training algorithm
hidden state
simulation model;dynamic performance;dynamic performance;simulation results
remote sensing images;remote sensing;quality assessment;water resources;spatial distribution;remote sensing data
clustering;clustering method;clustering method;similarity matrix;fuzzy clustering
historical data;real-time monitoring
data structure;activity models;assembly process;quality control;object models;low efficiency
wide range;mobile communication
quality control;process planning;quality control;data acquisition;process control
database management;information flow;database;sql server
correlation method
high-quality;numerical analysis;manufacturing process

finite element method;sensitivity analysis;frequency domain;low frequency;design parameters
unknown environment;optimal control;optimal control;control problem
theoretical analysis
design method;mathematical model
simulation model;simulation model
evaluation model;evaluation index system;evaluation model
network analysis
hierarchical structure;detection methods
higher precision;inverted pendulum;inverted pendulum;control method;mathematical model of;information integration;fuzzy controller;fuzzy control;simulation experiments;information integration
matching algorithms;matching algorithm;correlation coefficient;target image;hierarchical algorithm;document images;fast retrieval;document-images;digital libraries;reference point;approximate string
high efficiency;high efficiency;high quality
path planning;dynamic programming algorithm;path planning;unknown environment;optimal path;dynamic programming algorithm;user interface

control theory;mathematical model;identification problem;high speed;high precision;moving average
finite element analysis

dynamic model;high precision;simulation results

simulation model
detection accuracy;test results;wide-range;low accuracy;high precision
information management
scheduling algorithm;scheduling algorithm;data storage
multi-point;reliability analysis;control problem
database;question/answering;answer questions


spatial distribution;simulation model
low-cost;detection algorithm;detection method;image features;single image
high-speed;feature extraction;empirical mode decomposition;large-scale;feature extraction;condition monitoring;features including;condition monitoring
design method;database design
threshold algorithm;simulation results;multi-resolution;random noise
complex structure
sensor networks;sensor networks;application scenarios;sensor network
local neighborhood
theoretical foundations
frequency domain;euclidean distance between
test data


flow field

high efficiency

machine tools;machine tools;data acquisition;data acquisition;simulated data;real data;data-exchange
wavelet analysis;wavelet transformation
quantitative analysis
working principle;detection method;theoretical basis for
support vector regression;prediction model;vector machine;time series;time series;water resources;prediction method;long-term;support vector regression;special case;forecasting accuracy
replication;replication
high-speed
post-processing
classification;classification;vector machine;vector machine;learning speed;neural network;artificial neural network
fast response;high reliability;simulation results;high precision;adaptive approach
flow rate
matlab / simulink
error propagation;large parts;high precision;high speed
visual attention;visual attention;salient objects
change analysis;change detection;false alarm;detection algorithm;template-based;change detection;template-based;image sequence;high resolution
test results
image retrieval;object recognition;visual saliency;human vision;multi-resolution;visual saliency;multi-scale;real images;visual saliency;multi-scale

simulation results;threshold based
finite element method;finite element analysis
working principle;modal analysis;finite element analysis
state-space;simulation results
high temperature;numerical simulation;numerical simulation;high temperature;low temperature
address space
energy management
high degree of;image denoising;test images;random noise
large-scale;analytic hierarchy process;evaluation index system;machine tools;machine tools

dynamic model;simulation results
computational properties;ontology-based;database systems;database schema;database;queries posed;query answering
query optimization;database technology;database;general-purpose;concurrency control;databases;relational databases;structured data;data management
design decisions
pruning methods;search space;synthetic data sets;uncertain databases;uncertain data;uncertain database;real-world applications;query processing in;parameter settings;query point
algorithm called;nearest neighbor;real-life applications;problem called;spatial database
update semantics;streaming applications;input data;complex queries;data rates;data streams;query result;sliding windows;query results;memory consumption
real data sets;web search;providing users;query logs;content filtering;text document;text filtering;hash table;relevant content;high cost;large number of;key features;irrelevant documents;distributed environment
multi-version;open source;computing environments;replication;multi-tier;building block for;application server;multi-tier
grid-based;multi-dimensional;progressive processing;performance degradation
parallel database;predictive modeling;markov model;automatically selecting;18;main-memory;oltp systems;23, 20;transaction execution;management systems;concurrency control;distributed transactions;additional information;transaction processing
view selection;database;query workload;maintenance costs;selection process;selection method;query reformulation;query answers;semantic web;view selection;query processing;databases
excellent scalability;synthetic datasets;data summarization;massive data;communication cost;large datasets;algorithms for computing
density-based;summarization method;applications ranging from;moving object;clustering process;density-based;multi-resolution;key features;streaming environments;streaming data;cluster structures
machine translation;high precision and recall;training data;cross-language;schema matching;case study;training samples;language pairs;answer quality;cross-language information retrieval;structured queries
false positives;high risk;association rule mining;large number of;association rule mining;association rules;computationally expensive;data mining;false positive
growing number of;probability estimates;instances;semantic web;parameter tuning;instance level;ontology alignment
database;qualitative analysis;index structures;scoring functions;real dataset;scoring function;performance gain;data characteristics;similarity measures
response times;web applications;prediction model;database;user experience;data independence;large number of;query processing in;database architecture;upper bound;relational databases;relational database system;query processing
synthetic data sets;biological networks;query workload;dynamic nature of;existing graph;estimation accuracy;query processing;data streams;communication networks;graph streams;social networks
nn) queries;lower bound;database;normal distributions;uncertain data;index structure;computationally intensive;probability distributions;data sets;normal distribution;nearest neighbor;uncertain databases
selection criteria;ranking approaches;keyword queries;boolean queries;query-processing strategy;retrieving relevant;natural-language;user's search;search tools;semantic content;higher quality;question answering
tree structure;search paradigm;databases;ranked list;efficient retrieval of;relational database;keyword search;query result
knowledge bases;entity relationship;ranked list;search engine;web-scale;search engines;query logs;desirable properties;real world;user studies;knowledge base;problem called;knowledge bases
candidate pairs;pruning techniques;database;data cleaning;real datasets;partition-based;large sets of;edit distance;short strings;distance constraints;similarity joins;similarity join
efficient storage;document retrieval;block size;compression techniques;general-purpose;collection size;large document collections;compression method;random access;web collections;adaptive algorithm;random access;large collections
computational efficiency;topic model;predictive performance;topic-specific;em algorithm;document collections
manifold learning;unsupervised clustering;text documents;topic model;data analysis;topic modeling;em algorithm;dimensionality reduction;latent dirichlet allocation;low-rank;text corpora;manifold structure;probabilistic latent semantic analysis;fall short;topic models;local consistency;classification accuracies;manifold structure of;model-fitting;manifold learning;topic model;hidden structures;topic modeling
information diffusion;information diffusion;optimization problem;large datasets;approximation algorithm;np-hard
multitask learning;regularization term;multiple tasks;semidefinite programming;global convergence;low-rank;optimization problem;low-rank;rates;real-world data sets;multiple tasks;small-size;objective function
document classification;memory capacity;linear classification;training process;data sets;memory size;linear classification;training methods;random access
user studies;user feedback;helps users;web users
clustering;mining algorithms;classification;synopsis construction algorithms;data streams;hardware technology;change detection;data stream;temporal evolution;rapid rate;databases;transactional data
state university;web services;web service;search engine;information extraction;information science;digital library;data mining
perfect information;human players;ai techniques;artificial intelligence;machine learning
data analysis;wireless sensor network;ubiquitous computing;mobile computing;mobile devices;traffic monitoring;everyday life;science applications;sensor networks;information technology;large-scale distributed systems
fast algorithm;synthetic data;sampling-based;mining association rules;fp-growth;data set;missing values
multiple classes;classification trees;decision trees;multilabel classification;classification;instances;decision tree;application domains;case study;predictive performance;tree learning
clustering;feature space;document collection;document space;link structure;learning tasks;scientific literature;graph analysis;corpus-based;information theoretic;clustering
classification;frequent patterns;sequence mining;long range;real data;higher-order;sequential data;data mining;protein sequence;hidden markov models
pattern language;predictive accuracy;data mining;pattern mining;learned model;speed-ups;computational cost;graph databases
search space;maximum number of;real-world;learning systems;bayesian network classifiers;naïve bayes;bayesian network classifiers;attribute dependencies;adaptive algorithms;learning framework
classification scheme;clustering;classification;active learning;unsupervised clustering;active learning;supervision;learning process;high-throughput;low level;large datasets;support vector machines;learning vector quantization
synthetic data;entity relationship;semi-structured;vector-space;real data;ranking function;information retrieval;model parameters;learning parameters;automatically learn;user input
markov random field;propagation algorithm;level features;real dataset;online auctions;online auction
clustering;clustering algorithms;constrained clustering;constraint sets;machine learning;data mining research;clustering
clustering algorithms;spatial datasets;clustering method;multi-resolution;algorithm named;instances;data sets;data mining task;grid-based;supervised clustering;clustering framework
data mining;computational biology;real-world;optimal algorithms;statistical tests;data structures;databases;index structures
learning examples;tree induction;induction algorithms;decision tree;private information;privacy preserving data mining;tree induction;data mining
target class;closed sets;rule learning;negative examples;data representation;closed sets;emerging patterns;labeled data
tree structures;tree patterns;frequently occurring;real data
spectral clustering;web pages;web graph;web communities;random walk;effectively identify;meta-level;random walk;efficiently extract;user communities;stationary distribution;web applications;link information;hyperlink structure;random walks
real-world;learning problem;relevant information;multiple sources;real world;relational data;random walks
time-series;clustering;data mining and machine learning;spectral clustering;data mining
transductive learning;classification techniques;latent variables;data collections;spectral graph;transductive learning;classification results;generative model;text classification;latent variable models;support vector machines
multiple communities;kernel-based;von neumann;generative model;link analysis;weighted graphs
association rules;numeric attributes;visualization techniques;pattern discovery;quantitative association rules
expected number of;information diffusion;large-scale;social network analysis;influential nodes;cascade model;social network;social networks;ranking methods;large-scale social networks
learning process;classification;real datasets;conditional random fields;markov random fields;conditional random fields
data types;contrast-set mining;time series;multimedia data;instances;brute-force algorithm;diverse domains
clustering;nearest neighbor classification;distance preserving;privacy preserving data mining;original data;distance-based;prior information;data mining algorithms
mining algorithms;central server;stream mining;wireless communication;data mining tasks;interesting patterns;scalable distributed;traffic data
algorithm's performance;network nodes;knowledge discovery;distributed environments;dimensionality reduction;dimensional data;dimensionality reduction;clustering quality
greedy algorithm;observed data;accuracies;binary matrix;decomposition methods;real-valued;data matrix;decomposition methods;basis vectors;matrix decomposition;original data
worst-case;information entropy;machine learning;artificial datasets;feature selection;data streams;feature spaces
data set;monte carlo;event types;interestingness measure;large data sets;data sets;correlation patterns;linguistic features;spatial correlation
clustering;detailed comparison;scale-free;soft clustering
refinement operator;tree induction;efficient search;hypothesis space;complex query;relational learning;aggregate function
cluster analysis;cluster analysis;multi-relational classification;classification
high-dimensional;nearest neighbor;efficient search;index structures;nn-search;high-dimensional spaces;databases;dimensional data;low-dimensional
database;rule learning;learning problems;distributed settings;efficiently identify;databases;communication costs;distributed data;mining tasks
web search;network flow;network flow;search engine logs;real-world;random sample;user queries;ranking algorithms;usage patterns;search engine;relevant entities;formal model
clustering algorithms;lower-dimensional;feature space;higher-dimensional;real data sets;subspace clustering;high-dimensional feature spaces;subspace clusters
mining views;mining algorithms;query optimizer;database;query language;pattern mining;relational tables;inductive databases;business applications;frequent set;association rule;relational databases;mining algorithm;ad hoc queries
discovery algorithms;algorithm performs well;time series;real-valued;data stream;discovering patterns;real data
alzheimer's disease;classification;classification algorithms
ensemble techniques;label noise;machine learning;model averaging;sample selection bias;data mining;ensemble technique;classifier
frequent itemset;mining algorithms;probability model;frequent itemsets
traditional classifiers;classification;classification accuracy;takes into account;classification algorithms;real data;data sets;wide range;real world;interpretability;classifier;classification algorithm
computational efficiency;document classification;naive bayes;closely related;naive bayes;1,2;predictive performance;data transformations;data normalization;text classification;centroid-based;classifier
clustering;data clustering
singular value decomposition;lower bound;private information;reconstruction error;estimation error;svd) based;upper bound;privacy preserving data mining
discovered patterns;microarray data;mining frequent;attribute domains;pattern discovery;gene expression
large-scale;clustering method;databases;vector machine;manually annotated;standard svm;distance metric
distance measure;symbolic representation;time series;time series;distance measures;generic framework;scale poorly;information loss;dimensionality reduction;data-mining algorithms
generation algorithm;models built;sequence data;feature construction;feature generation;large space of;feature selection
information fusion;document summarization;world wide web;textual features;web content;cross-media;distributed nature of;retrieval model;similarity-based;specific topics;visual features;document set
instance;data source;input data;temporal intervals;instances;event sequences;knowledge discovery;mining patterns;temporal data
pattern discovery;quality measures;algorithms typically;pattern sets;interesting patterns
support levels;11;classification;frequent item set;database;interesting patterns;item sets;underlying data distribution;rule based;classifier
clustering;web documents;community discovery;social network analysis;text documents;named entities;text corpus;content similarity;overlapping communities
large amounts of;pruning method;itemset mining;frequent itemsets;closed itemsets
clustering;learned metric;database;object identification;quadratic programming problem;data items;metric learning;distance metric;human supervision;semi-definite programming
hidden variable;database;hidden variables;mining association rules;instance;association rules
rule set;event types;manufacturing process;quality control;rule generation;data mining approach;decision theory;selection criterion;huge number of;decision making
document streams;clustering;automatically discover
transaction data;graph partitioning;global model;estimation problem;frequent itemsets;markov random fields;modeling approach;approximate inference;real datasets
similarity function;multi-dimensional;semantic indexing;text data;magnetic resonance;similarity search;chemical compounds;discrete data;retrieval method
clustering;mining algorithms;classification;synopsis construction algorithms;data streams;hardware technology;change detection;data stream;temporal evolution;rapid rate;databases;transactional data
state university;web services;web service;search engine;information extraction;information science;digital library;data mining
perfect information;human players;ai techniques;artificial intelligence;machine learning
data analysis;wireless sensor network;ubiquitous computing;mobile computing;mobile devices;traffic monitoring;everyday life;science applications;sensor networks;information technology;large-scale distributed systems
multi-agent;multi-stage;human subjects;real data;machine learning
labeling effort;unlabeled data;labeled dataset;classification;instance;label ranking;active learning;information retrieval;ranking functions;sampling strategies;7;sampling strategy;9;total order;automatic text summarization;learning strategy;question/answering
graphical models;latent dirichlet allocation;random variables;semi-supervised learning;constrained optimization;clustering task;semi-supervised clustering;markov random fields;combinatorial optimization;random variable;markov random fields;semi-supervised;transfer learning;inference algorithms;inference techniques
secondary structure;structured documents;tree edit distance;web information extraction;complex tasks;expectation-maximization algorithm
protein function;protein sequences;inductive logic programming;predictive power
ground truth;bayesian network structure;random variables;classification accuracy;real-world;general case;bayesian network structure;simulated data;noisy datasets;random variable;high-level;heuristic search;np-hard
support vector machines;classification;string kernel;jensen-shannon divergence;selected features;markov chains;expectation-maximization algorithm;model fitting;classifier
context sensitive;kernel pca;synthetic data sets;high dimensional feature space;inference algorithms
statistical methods;explanation-based learning;real-world;knowledge representations;explanation based learning;simple algorithm;real world
density estimation;classification;support vector machines;generative models;hidden markov models;bayesian networks;statistical relational learning;relational learning;kernel methods;relational data
imbalanced data;machine learning;instances;generic framework;classifier performance;classifier
test set;control-knowledge;training examples;search tree;active learning;machine learning;randomly generated;knowledge acquisition
speech recognition;sequence alignment;dynamical systems;markov models;partially observable;markov models;hidden state;algorithm learns;algorithm produces;hidden markov model;locally optimal;hidden state;markov models;deterministic finite automata;expectation-maximization
probabilistic latent semantic analysis;test data;generalization performance;text queries;ranking performance;average precision
regression trees;hidden markov models;conditional random fields;potential functions;conditional random fields
clustering;data objects;conventional methods;biological processes;multiple-instance learning;multiple-instance learning;expression data;data sets;incomplete knowledge;genomic data;behavioral patterns;complex data;multiple-instance;quality measures;supervised learning
classification models;probability distributions;learning algorithm;naïve bayes;network model;bayesian networks;markov networks;prediction problems;network structure;probability estimation;bayesian learning
high-dimensional;learning process;closed-loop;interactive learning;real-world;learning algorithm;learning paradigm;real-valued;visual features
closed-loop;decision tree;reinforcement learning;reinforcement learning algorithm;action space;state-action
classification performance;em algorithm;competitive performance;bayesian networks;models learned
clustering;state transitions;markov chains;signal processing;real-world;transition probabilities;maximum likelihood;markov chain;higher order;estimation algorithm
global optimality;reinforcement learning
variational methods;lower bound;gaussian process;variational inference;variational approach;variational inference;upper bound;process models;approximate inference
finite sample;monte-carlo;estimation error;state-space;decision problems;monte-carlo
clustering;graphical models;monte carlo;synthetic data;expectation–maximization algorithm;data clustering;tree-structured;1;markov chain;dependency structure;bayesian learning
classification;gaussian process regression;predictive accuracy;gaussian process regression;inductive inference;test instances;learning algorithms;training and test data;machine learning;learning problems;real world;information contained in;model selection
question classification;semantic role labeling;average case;tree nodes;tree kernels;support vector machines;dependency trees;natural language
auc;induction algorithms;rule learning;high quality;large number of;evaluation functions;data sets;artificial data sets
genetic programming;data sets;automatically generated;induction algorithms;induction algorithm
monte carlo;active learning;sensitivity analysis;high dimensional;gaussian processes;computationally expensive;learning rate;learning scheme
probability distributions;free energy;probability distribution;evolutionary computation;expectation-maximization;weighted sum of;probability model
auc;rule learning;training error;base classifier;benchmark datasets;lower-bound
point-based;evaluation measure
graph-based;input space;semi-supervised learning algorithms;graph based semi-supervised learning;optimization criterion;ad hoc;benchmark data sets
structured output;training data;semantic role labeling;synthetic data;active learning;learning approaches;multiclass classification;margin;machine learning applications;instance;structural information;output variables;instances;high cost;margin-based;local classifiers
source task;inductive logic programming;transfer learning;reinforcement learning;target task
classification;learning rate;weight vector;maximum margin;large margin classifiers;margin
real life problems;real-life;mathematical programming;classification methods;classification
probability estimates;auc;decision trees;instance-based;sample size;decision tree;decision tree algorithms;test instances;accurate ranking;test instance;instances;ranking performance;training instances;statistical technique;probability estimation;class probabilities
computational efficiency;vector machine;instance;classification accuracy;multiple-instance learning;random walk;multiple-instance learning;random walk on;instances;svm) classifier;closed form solution;learning problem;benchmark data sets
clustering;clustering algorithms;global model;ensemble methods;cluster ensembles;distributed clustering;personal preferences;learning task;objective function
information contained in;text categorization;term frequency;additional cost
clustering;lower dimensional;microarray data;similarity matrices;multiple subspaces;semi-supervised clustering;semi-supervised clustering;dimensional data;high dimensionality;high dimensional spaces
clustering;clustering algorithms;semi-supervised clustering;clustering process;clustering approaches;pairwise constraints;real data;supervision;kernel method;manual tuning;semi-supervised clustering;optimization criterion;semi-supervised;gaussian kernel;objective function
probability estimates;classification;classification accuracy;expected error;higher classification accuracy;naive bayes classifiers;model selection
nearest neighbor;globally optimal;individual queries;common practice;nearest neighbor;optimal number of;model selection
domain knowledge;accurate models;scientific domains;time series;expectation maximization;missing data;dynamic systems;process models;missing values
confidence scores;label ranking;decomposition techniques;instances;label ranking;loss functions
clustering;clustering algorithms;simpler models;probabilistic models;1;2;data objects
probability estimates;multiple criteria;multi-objective;user's perspective;machine learning;risk management
spectral clustering;clustering results;data points;spectral clustering;large-scale;spectral clustering algorithms;algorithm called;computational complexity;laplacian matrix;low-dimensional
clustering;contingency table;clustering problem;information-theoretic;linear combination;high-order;objective functions;weighting scheme
parameter space;large numbers of;efficient inference;conditional random fields;scale poorly;conditional random fields;natural language
statistical methods;gaussian kernels;multiple images;linear combination;time series;gaussian noise;controlled experiments;kernel-based
decision trees;classification;decision tree;classification accuracy;real-world;cost-sensitive;decision tree learning;classification problem;computational overhead;classifier;information theoretic
feature spaces;learning theory;real data;video tracking;novelty detection
nearest;decision trees;classification;nearest;training instances;benchmark datasets
markov decision processes;learning approaches;reinforcement learning
state spaces;large state spaces;partially observable;real-world;reinforcement learning;hidden state;neural networks
clustering;high-dimensional datasets;real-world;computational savings;kernel matrix;multiple clusterings;optimal number of;text corpora;original data;document clustering;computational cost;condensed representation;cluster structures
test statistic;incomplete data;naive bayes;synthetic data
clustering;spectral clustering;video data;binary matrix;nearest-neighbor;maximum weight;standard datasets
binary classification;multi-class problems;active learning;ensemble members;data acquisition;active learning strategies;instance;uncertainty sampling;training instances;margins;multi-class
learning approaches;learning process;active learner;classification;active learning;real-world;active learning;learning rate;benchmark datasets;active learning algorithms;classifier
model complexity;classification;classification;classification task;machine learning;data sets;classification method;support vector;margin
higher-order;learning theory;machine learning
regularization;relevant features;microarray data;microarray data analysis;feature selection;dimensional data;support vector machines
textual documents;vector space;naive bayes;retrieval task
hill climbing;reinforcement learning;real-world domains;reinforcement learning;state-space;algorithm called;action space;optimization problem;space complexity
1;3;2;wide range;calibration method;4
kernel pca;test data;feature extraction;missing data;kernel principal component analysis;missing data;gaussian processes;latent variable models;objective function
clustering;training set;test set;document classification;classification;text categorization;information retrieval tasks;similar features
real data sets;support vector machines;support vector regression;synthetic datasets;large scale;linear programming;decomposition method;large datasets;desirable properties;support vector machines;highly competitive
greedy search;search space;learning algorithms;greedy search;efficient approximation;inductive logic programming;machine learning algorithms
domain knowledge;closed-loop;hybrid” approach;poor performance;application domain;reinforcement learning;reinforcement learning;data center;data collected;computing systems
discriminant analysis;classification;regression problems;vector machine;classification performance;large data sets;maximum margin;massive data sets
classification;prediction performance;majority voting;dynamic integration;instance;dynamic integration;combination function
statistical queries;generally applicable;data set;probability distribution;ensemble method;information gain
phase transition
reconstruction error;data points;dimensionality reduction;low-dimensional
binary classification;loss function;classification;cost-sensitive learning;ranking svm;cost-sensitive;information retrieval;instance;document search;cost-sensitive learning;support vector machines;information retrieval tasks;simulation results
bayesian methods;closely related;dirichlet process;exponential family;variational bayesian;variational bayesian;bayesian framework;variational bayesian;mixture models;model selection
ir effectiveness;information retrieval;ir research;knowledge creation;information access;ir) research;access information

entity-oriented;international workshop on;entity-oriented;international workshop on
sigir workshop on;plans;workshop brought together;search strategy;complex query;explicit feedback;contextual information;search tasks
image retrieval;sigir workshop on;15;workshop brought together;information retrieval;information retrieval
real users;information retrieval;query sessions;search engine;information retrieval;query sessions

search results;query intent;document retrieval;information retrieval;search engines;international workshop on
world wide web
information retrieval;information retrieval;international workshop on
search results;entity ranking;keyword queries;redundant information;anchor text;web collection;ir systems;focused retrieval;result page;search process;document structure;web resources;category information;frequency counting;keyword query;term weighting scheme;query returns;ir model
search task;xml retrieval;search behavior;structured documents;search log;real-world;extensible markup language;ir evaluation;automatically creating;information retrieval;domain-specific;test collections;search behaviors;search engine;user groups;information seeking;retrieval techniques;test collection
access method;storage devices;index scan;memory size;index scans;selection predicate;table scan
optimization strategies;data analysis;open source;data processing;database
end-users;data manipulation;user-generated;ad-hoc;structured data from;aggregated data
database;distributed query processing;relational database systems;domain-specific;relational database management systems;business applications;distinctive features;relational data;data processing;column-oriented;text processing;data management
big data;big data;database;1;semantic web;database researchers
data center;query optimization;data center;query processing;random accesses;cost-effective;hard disk
design principles;tree index;optimization method;flash-based;solid state;hard-disk;optimization methods;search performance;times faster than;range search
database;main-memory;databases;long-running;specifically designed for;rates;concurrency control;control mechanisms;atomicity
fall short;synthetic data;np-complete problem;graph pattern matching;subgraph isomorphism;real-life;bounded number of;graph pattern matching;pattern matching
complete model;database;optical character recognition;query performance;probabilistic data;data sources;relevant answers;probabilistic model;enterprise data;relational database;relational database management system;probabilistic models;text processing
labeled trees;tree edit distance;worst-case;edit operations;instance;minimum-cost;worst case;worst-case optimal;real world
internal state;black-box;database;desired level of;compact representation;fine-grained;coarse-grained
algorithm finds;search space;optimization strategies;search queries;large graphs;high scalability;shortest path;search tasks;relational database;disk-based;data management
discovered patterns;naïve;vice versa;real-life datasets;pattern mining;large datasets;pattern discovery;abstraction level;higher level
model generalizes;performs poorly;decision support;large number of;statistical model;real-life;sql queries
continuous optimization;pattern recognition;energy minimization;shape analysis;international workshop on
data collection;private data;large data sets;service providers;data mining tools;personal data;data mining;vast amounts of
data mining
learning algorithm;classification method;high dimensional spaces
regularization term;regularization;label information;algorithm combines;real world datasets;semi-supervised learning;large number of;semi-supervised classification;unlabeled examples;graph laplacian;decision boundary;neighborhood graph;semi-supervised;conditional probability;supervised learning;labeled examples;classification algorithm
computational efficiency;manifold learning;pattern recognition;feature extraction;similarity-based;feature extraction;feature representation;similarity matrix;classification performances;data mining;dimension reduction
conditional entropy;data sets;decision trees;decision tree
real world;text classification tasks;naive bayes;classification tasks;naive bayes;instances;generative model;naive bayes classifiers;accuracies;protein sequence;sequence classification;support vector machines;decision tree learner;classifier
human-generated;learning algorithm;induction algorithm;rule learning;learning algorithms;application domains;reuters 21578 corpus;rules generated;benchmark datasets
training data;unlabeled data;predictive accuracy;knowledge base;data-mining applications;classifier;nearest neighbor classifier;unlabeled examples;labeled data;nearest neighbor;labeled and unlabeled data;unlabeled set;machine learning algorithms;labeled examples;classification algorithm
data set;feature selection algorithms;feature selection method;accuracies;data pre-processing;meta-level;data sets;feature selection;data mining process;classification task;feature selection
irrelevant features;decision tree
classifier ensemble;ensemble members;pattern based;emerging patterns;rates;scoring function;individual classifiers
training set;decision trees;regression problems;classification;density estimator;machine learning;training data;training sets;learning method;decision tree learners
prediction accuracy;highly skewed;prediction performance;sampling techniques;imbalanced data;great success;imbalanced datasets;decision boundaries;sampling technique
ranking algorithm;hierarchical clustering algorithms;completeness;clustering structure;database size;density estimator;hierarchical clustering;large data sets;data sets;clustering algorithm;nearest neighbor;interpretability
clustering;clustering;clustering results;expression patterns;missing values;clustering method;clustering process;data matrix;missing data;data preprocessing;clustering analysis;gene expression
clustering;data objects;data mining applications;expectation maximization;multi-instance;cluster quality;em algorithm;feature vectors;real world data sets
search space;real life datasets;pruning techniques;frequent patterns;frequent patterns;database;high dimensional;algorithm's performance;large data sets;synthetic datasets;share common;discover meaningful;data mining models;dimensional data;naïve;correlated patterns;cluster models
real-world;hierarchical clustering;optimization algorithm;databases;hierarchical clustering;optimization problem;optimization model;numerical results
clustering;clustering;complex objects;hierarchical clustering algorithm;semantic relationships;data objects;data mining algorithms;real world data sets
clustering;clustering algorithms;distance functions;multimedia applications;result sets;density-based clustering;real-world;complex objects;multi-step;hierarchical clustering algorithm;data sets;query processing;similar objects;density-based clustering algorithm
clustering;clustering;synthetic data sets;clustering accuracy;clustering process;high density;clustering algorithm
clustering;data mining techniques;data uncertainty;high quality;moving-object;data uncertainty;data mining;clustering algorithm;location data
svm training;database;numerical results;convergence rate;vector machine;2;sampling technique
pattern selection;support vector machines;support vector regression;selection method
clustering;multi-relational data mining;classification;kernel function;multi-relational;support vector;algorithm produces;support vector;clustering algorithm;relational data
numerical experiments;support vector machines;large number of;laplacian matrix;graph kernels
feature space;multi-objective;real-world datasets;hierarchical text classification;text corpus;information integration;document/term;taxonomy tree;text classifiers
instance selection;training set;training examples;text classification tasks;svm) classifiers;feature-based;feature-based;instance;training documents;text classification;svm classifier;classification accuracy;support vector machines;generic algorithm;selection algorithm
theoretical analysis;text filtering;classification;classification decisions
emerging trends;scientific articles;user-interactions;trend detection;finite-state;scientific literature;utility functions
large-scale;large scale;document detection;million documents;precision/recall;sampling strategy;similarity threshold
classification techniques;classification;text documents;comprehensive evaluation;text classifier;real-world;real world;classification approach;partially supervised;classifier
xml documents;clustering algorithm;structural information;pair-wise;clustering
clustering;domain knowledge;clustering results;clustering algorithms;document cluster;clustering approaches;bipartite graph;helps users;clustering approach;vector space model;graph representation
world wide web;web search;power law;hierarchical structure;common knowledge;web users
clustering;pattern-based;web access;web access;web usage mining;web access;statistical measures;dynamic nature of;algorithm called;web services;knowledge based;web site;pattern-based clustering;web usage data
web sites;product descriptions;automatically extracting;real-world;information contained in;web sites;labeling problem;highly dynamic;online auction;product features;unified framework;web page;conditional random fields
clustering;clustering;clustering approach;similarity computation
cut algorithm;web pages;related data;produce high quality;event detection;usage patterns;web page;real world;web usage data
pair-wise;web documents;web based;information systems;document detection;data set;similarity computation;million web pages;web page
ranking scheme;communication patterns;large-scale;large scale;temporal dynamics;social network;representation scheme;detection results;activity patterns;graph based;large scale social networks;single snapshot;user study
graph isomorphism;large social networks;recommendation process;social network;directed graphs;real life;large datasets
feature construction;typical patterns;attribute values;decision trees;decision tree;classification;graph-structured data;graph-based;construction algorithm;decision tree;machine learning;graph-based;real-world
regularization;classification;hidden variables;classification accuracy;classification methods;random walk model;graph representation;data set;classification results;high dimension;high frequency;real world;markov chain;semi-supervised classification;learning method;classification algorithms
transaction data;data mining methods;outlier detection;data mining;data items;case study;discovering patterns;data mining
mobile user;mobile user;data mining method;database;pattern mining;spatio-temporal;sliding window;data mining;sliding windows;mobile users
discovered patterns;mining temporal;pattern-growth
general case;closed itemsets;heuristic algorithms;ieee transactions on knowledge and [data engineering
low-quality;quality-aware;data source;interesting rules;association rule mining;cost-based;data quality;quality-aware;interestingness measures;discovered association rules;probabilistic model;quality measures;discovered knowledge
tree mining;real datasets;xml mining;interesting patterns;web mining;frequent subtrees;frequent induced;tree model
mining results;computational complexity;minimum support;sliding window;highly accurate;frequent itemsets;high-speed data streams
frequent patterns;data mining
minimum support;low support;algorithm called;apriori algorithm;high confidence;association rules;data mining problem
data structure;candidate itemsets;ad-hoc;online mining;efficient algorithms to;data structures;query-driven
databases;categorical data;nearest neighbor classification;data mining
frequent itemsets;decision makers;minimum-support;minimum support
frequent itemset;mining algorithms;bit-vector;vector representation;frequent itemset mining;sparse datasets;mining patterns;mining algorithm
rule evaluation models;human experts;case study;learning algorithms;post-processing;post-processing;mined results;classification rules;rule evaluation support method;data mining;data mining process;objective rule;uci datasets
classification;0, 100;rule base;classifier;microarray data
mining results;mining process;input sequences;efficient discovery of;high accuracy;protein sequences;biological sequences
search spaces;small sample sizes
relational algebra;classification;complex objects;1;instances;data representations;support vector machines;svm) algorithm
pruning technique;binary attributes;correlation coefficient;large data sets;data sets;quantitative attributes;pattern mining algorithms;real-world data sets;pairwise similarity;information loss
algorithm generates;mining algorithms;synthetic datasets;outlier detection;global features;real life;efficiently identify;mining algorithm
data objects;detection algorithms;local-search;small groups;synthetic datasets;outlier detection;outlier mining;very large datasets;categorical data;optimization problem;optimization model;greedy algorithm;real datasets
mining algorithms;database;2,11;local outliers;data set;nearest neighbors
intrusion detection;genetic algorithms;anomaly detection;anomaly detection;intrusion detection systems;normal behavior
clustering;detection algorithm;intrusion detection systems;kernel-method
intrusion detection;intrusion detection system;database;data mining;valuable information;rule mining;data items;algorithm named;extracted data;databases;sensitive attributes
clustering;public data;minimum number of;large databases;interesting patterns;data records;distance-based;privacy-preserving;data mining;abstraction level;privacy requirements;privacy-preserving data mining
real data sets;collaborative filtering;data collected from;randomized response;increasing attention;privacy risks
privacy-preserving;classification;security concerns;data mining projects;classification;diverse domains;vector machine;data mining problem;knowledge discovery;privacy-preserving;svm) classification;svm classification;data mining algorithms;vertically partitioned data
data structure;database;main-memory;data mining;relational database management systems;data structures;storage manager;data mining;relational database;machine learning;machine learning algorithms;mining tasks
test set;movie database;cross validation;special case;test sets;random sampling
minimum description length;graph mining
data objects;human perception;database;linguistic analysis;image representation;space utilization;feature-based;feature values;mathematical analysis;data sources;logic-based;efficient computation;real world;multimedia data;distributed database
singular value decomposition;data matrices;indexing approach;sign language;tree structure;similarity search;large variations;feature vectors
mining framework;spatial locations;frequent patterns;pattern mining;mining frequent;image databases;image content;spatial patterns;data management
magnetic resonance;similarity metric;real-life;general-purpose;1;3;2;classification results;real datasets;image classification;kernel approach;string kernel;computational overhead;classification tasks;classification algorithms
hierarchical algorithm;shared information;local patterns;case study;multiple streams;pattern discovery
clustering;clustering;multiple data streams;event-driven;clustering results;event detection;data streams;piecewise linear;clustering quality
mining frequent itemsets;streaming environment;incoming data;streaming data;frequent itemsets;data stream;data streams;support threshold
data mining tasks;mining framework;high throughput;power consumption;data stream environment;higher throughput;temporal patterns;growing rapidly;mobile devices;data streams;traditional database;data mining;association rules;field programmable gate;high complexity
storage space;matching methods;stock data;moving average;time-series;sequential scan;moving average
data structure;data mining technique;clustering method;markov chain
real data sets;database;similarity measurement;time series;data set;hierarchical representation;similarity searching;hierarchical representation
multiple linear regression;recurrent neural networks;multi-stage;time series;predictive model;hidden markov model
sequential pattern mining;sequential pattern mining;sequential patterns;frequent sequences
wavelet decomposition;conventional techniques;time series;wavelet analysis;moving average;data processing;data mining;wavelet transformation;short-term
optimization problems;fitness function;multi-objective
kernel pca;real-world;data compression;principle component analysis;kernel function;input space;principal component analysis;artificial neural networks
training set;training examples;locally linear embedding;locally linear embedding;dimensional data;lower-dimensional space
evaluation criterion;extreme values;evaluation metric;distinguishing feature;search process;application domains;performance metric;instance;regression models;prediction model
data-driven;domain-driven;knowledge discovery;data mining;pattern discovery;real world;domain-driven
public data;data generation;synthetic datasets;recommendation algorithms;real data;content information;recommender systems;synthetic data
factors affecting;climate data;spatial data;temporal patterns;kernel k-means;data analysis;spatial constraints;complex data;kernel methods
machine learning methods to;machine learning techniques;dynamic environment;machine learning;data sets;fuzzy logic;data mining;data mining problem;resource allocation;learning method
automated design;multi-objective;genetic algorithm;large-scale;human experts;data mining;existing knowledge
association mining;detection techniques;temporal association rules;interestingness measure;data preparation;knowledge representation;databases
feature space;life sciences;data analysis;data bases;information sources;data mining and machine learning;predictive models;life sciences;1;data analysis;analysis task;context dependent;life science;2;similarity measures
data sets;machine learning methods;vice versa;text-based;general-purpose;machine learning;text-understanding;document collections;natural language processing;text-processing;machine learning;machine learning algorithms
parameter estimation;instances;hidden markov models;bayesian networks;7,5;logic programming;probability distributions;3;10,8;4;6;14,18,13,9,6,1,11;inductive logic programming;statistical relational learning;2;logic program;11;13;12;15;17;16;19;probabilistic model;probabilistic learning;naïve bayes;machine learning;context free;probabilistic logic;statistical relational learning;probability theory
time series;detect anomalies;data mining
electronic commerce;data warehouse;data collection;data mining and knowledge discovery;massive amounts of data
data streams;data structures;data stream;data streams;data intensive applications;data synopses;massive data sets
data mining results;association rule mining;large number of;effectively identify;source database;statistical significance;data mining models;pattern discovery;minimum support threshold
mining framework;efficient computation;constraint-based mining;pattern mining;interesting patterns;constraint-based;pattern discovery;soft constraints
clustering;web usage mining;web navigation;markov models;clustering technique;takes into account;markov models;higher-order;transition probabilities;real world data sets;conditional probabilities;probability threshold;navigation paths;markov model
decision trees;pruning techniques;classification;tree mining;accuracies;tree structured data;comprehensibility
agglomerative hierarchical clustering;hierarchical clustering;instance;agglomerative clustering;purity;np-complete;agglomerative hierarchical clustering;cluster-level
multi-level;nearest neighbor;hierarchical clustering;hierarchical clustering algorithm;similarity-based;real life applications
multi-class problems;decision trees;randomly-generated;1;tree structure;learning problems;multi-class
itemset mining;interesting patterns;pattern mining;biological sequence;mining process;real life;databases;protein sequence;mining algorithm;small size
nearest neighbor;update cost;classifier ensembles;data representation;data stream applications;multi-resolution;error bounds;real-life;computational costs;data streams;classifier;excellent performance;nearest neighbors;classification algorithm
probability distributions;spatial correlations;synthetic data;classification;multi-dimensional data;random fields;real-world;local-consistency;kernel functions;support vector;explicitly models;random fields;support vector;support vector machines;conditional random fields;detection task
real graphs;real graphs;power law;degree distribution;main idea
adjacency matrix;closed patterns;data mining
real-world datasets;training set;training examples;learning algorithms;artificial datasets;margin
spatial data;spatial relationships;real-world;spatial objects;regression models;model trees;spatial data mining;spatial databases
svm algorithm;general concept;classification accuracy;training set;semantic relationships;text classification;semantic relations;semantic information;word sense disambiguation;classification task;benchmark datasets;high precision;word sense disambiguation
web documents;natural-language;web search;unstructured text;information retrieval applications;textual documents;natural language;information source
synthetic data;sequential patterns;general case;mining sequential patterns;frequent sequential patterns;mining sequential patterns;sequential pattern mining
feature selection;maximum entropy;linear classifiers
user preferences;product recommendation;recommender systems;knowledge discovery;user feedback;knowledge discovery;product space
density estimation;tree-based;unsupervised discretization;unsupervised discretization;scoring function
mutual information;feature set;short documents;weighted average;mutual information;text categorization;objective function;feature selection;mutual information;evaluation measures;weighted average;model selection
em) algorithm;gaussian mixture model;speech recognition;discrete cosine transform;em algorithm;expectation-maximization
decision tree;cost-sensitive learning;naïve bayes;test strategies;cost-sensitive;learning models;cost-sensitive;total cost
clustering;domain knowledge;feature ranking;database;feature ranking;case study;svm-based;support vector machines;cluster analysis
cut algorithm;synthetic datasets;transitive closure;object identification;information integration;perceptron algorithm;conditional random fields;natural language
web service;distributed data mining;data mining tasks;user interface;web services;data mining algorithms;distributed data;mining tasks
interaction data;gene ontology;database;biological processes;expression data;inductive logic programming;inductive logic programming;genomic data;high sensitivity
high-dimensional;data point;synthetic data;data structure;linear combination;locally linear embedding;locally linear embedding;data sets;real-world;nearest neighbors
sampling algorithm;predictive power;knowledge discovery;algorithm requires;biomedical data;active sampling;active sampling;random sampling
distance measure;data analysis;distance functions;data visualization;classification;features extracted from;knn queries;index structure;data-mining applications;distance measures;query processing;computational cost;gene expression;index scan
medical data;multi-stream;data-structures;financial data;novelty detection;time series;skewed;burst detection;databases;network traffic;indexing technique
semantic indexing;dimensionality reduction technique;features extracted;statistical techniques;keyword-based;machine learning;retrieval performance;problem solving;latent semantic indexing
frequent subgraphs;distinctive features;graph databases;code base;database
classification;database;heterogeneous databases;high accuracy;data mining;automatically detected;high cost
clustering;clustering;clustering structure;bayesian model;latent space;feature representations;em algorithm;discrete data;unified framework;text data sets;intrinsic structure;low-dimensional
decision trees;collaborative filtering;collaborative filtering;decision tree;massive amounts of data;recommendation algorithms;collaborative filtering algorithms;data stream;data streams;data arrive
itemset mining;pruning technique;pruning strategies;apriori-based;apriori algorithm;databases;memory consumption
mining results;social network analysis;multi-relational;linear combination;community mining;community mining;social network analysis;social networks
interesting rules;data mining;real-world data sets;objective rule;interestingness measures;discovered rules
similarity function;clustering algorithm;kernel based;metric space;market segments
unsupervised algorithm;database;neural network;corpus-based;unknown word
real-world datasets;classification;decision tree;highly competitive;time series;cross-validation;time-series;supervised learning algorithm
accurately reflect;clustering problems;dimensional data;primary goal;cluster analysis;clustering model
predictive accuracy;classification;prediction methods;naive bayes;instances;data streams;naive bayes models
real-world datasets;customer relationship management;classification;theoretical foundation;data mining applications;search engines
ensemble learning;ensemble learning;classification;data mining tasks;ranking performance;data mining;real-world data sets
exploratory data analysis;source separation
score function;multi-dimensional;data mining technique;database;database query processing;ranked queries
domain knowledge;learning examples;feature extraction;high level;feature extraction method;classification;classification accuracy;feature extraction;high dimensionality
relational structure;multi-relational data mining;multi-relational data mining;numeric data;numeric data
maximum likelihood;likelihood ratio
misclassification costs;naive bayes;highly skewed;naive bayes;text categorization;cost-sensitive;skewed;high confidence;feature distribution;document collections;classifier;class probabilities
clustering;mobile user;prediction accuracy;user location;mobile computing;location data;online algorithms
retrieval accuracy;automatically determines;partial matching;time series
entropy-based;multi-dimensional;single attribute;sequential patterns;transaction databases;entropy measure;computational complexity;sequential pattern;information theoretic
real-world datasets;high-dimensional datasets;data analysis;similar objects;real-world;skewed;computationally expensive
clustering;classification;database;outlier detection;prior knowledge;hierarchical clustering algorithm;data sets;retrieval performance
contingency table;clustering problem;mixture model;classification;takes into account;simulated data;poisson model;maximum likelihood
data distributions;imbalanced data;learning algorithm;categorical data;weighting scheme;imbalanced data
computational complexity;input data;frequent itemsets;frequent items;data types;data partitioning;data pre-processing;frequent itemsets;final step
web pages;regular expressions;learning algorithm;user feedback;key parameters;adaptive filtering
clustering;closed sets;categorical data;local patterns;instance;generic framework;clustering framework;benchmark datasets;clustering approach
privacy-preserving;collaborative filtering;collaborative filtering;data owners;privacy concerns;privacy-preserving;databases;vertically partitioned data
mining frequent sequences;synthetic data;database;main memory;real data;data access;data mining;sequential pattern mining
predictive accuracy;ensemble methods;feature selection algorithms;feature selection algorithm;classification process;stochastic process;feature selection;feature selection problem;ensemble approach;classifier
cross-validation;computational complexity;tree induction;classification accuracy;regression models;1;model trees;8
intrusion detection;classification;high dimensional datasets;streaming data;instance;learning process;data streams;class labels;probability distributions;high speed
classification;clustering method;instances;subspace clustering;class association rules;association rule;numeric attributes
concise representation;incremental algorithm;frequent itemsets
simulated annealing;genetic algorithms;tabu search;artificial neural network
web search;classification;network intrusion detection;temporal dimension;data collected from;instance;web graphs;web mining;social networks;web usage;data-centric;web mining;web personalization;web data
web usage analysis;data warehouse;multi-channel;storage model;instance;object-oriented;data warehousing;formal model;web usage;web usage analysis;relational data
access logs;recommender systems;web access;web site;web resources;navigational patterns
recommender systems;ground-truth;collaborative filtering;main memory;context sensitive;neural network;neural networks
review process;ground truth;factor analysis;generative model
user preferences;optimizer;classification;clickthrough data;negative examples;algorithm called;ranking function;ranking svm;relevance feedback;clickthrough data;ranking algorithms;search engines;naïve bayes;naïve bayes;search engine;unlabelled data;positive examples
personalized search;web search;personalized search;features extracted from;tree nodes;pre-computed;user profiles;search effectiveness;web search;similarity based
data-driven;entity extraction;linear model;entity types;supervision;information retrieval systems;model parameters;question answering systems;machine learning;feature vectors
conceptual modeling;inductive database;web applications;web usage mining;ad-hoc;meta-data;web application;user requests;case study;information embedded in;web logs;conceptual model;data mining;query languages;navigation paths
document content;classification;semantic features;classification systems;machine learning;semantic level;text classification;semantic web technologies;document representation
machine learning methods;high-frequency;unstructured text;data set;online news;feature set
service provider;database systems;database;dynamic nature of;potential impact;data structures;data encryption;key management;encrypted database;data management
service provider;database;access control;databases;database outsourcing;metadata management;increasingly popular
encoding scheme;database;storage space;access control;encrypted data;encrypted database
web service;database systems;access control;sharing data;web service;web services;caching techniques
access control;object databases;role based access control;databases;access control;rule based
graph model;semantic web;semantic web;access control model;access control
database content;access control model;database;query language;xml databases;xml database
sampling methods;general purpose;database;microarray datasets;prior knowledge;risk analysis;sampling framework;benchmark datasets;data owner
search space;query processing;sensitive data;database
xml databases;multiple users;xpath queries;sensitive data
collecting data;customer relationship management;classification;large-scale;confidential information;collected data;data mining technique;data set;data values;data collection;data mining technology;data analysis;classification models;high-level;decision making
medical research;high degree of;medical databases;medical data;query modification;databases
service provider;privacy issues;sensitive information;user-requests;personal identification;location information;location-based services;location-based
ubiquitous computing;specific context;mobile computing;information management;context-specific;information leakage;computing environment;data repository;data management
web applications;interactive applications;extensible markup language;information processing;security requirements
communication cost;computational complexity;signature schemes
social media;predictive models;online social media;social media;large scale;temporal patterns;information diffusion;social media data;incomplete data;information networks
modeling task;large-scale;large scale;higher-level;data mining;data-intensive applications;data mining algorithms;model training
random sample;selection bias;random sampling;machine learning
diverse applications;model complexity;object recognition;data mining methods;recommender systems;trade-offs;multi-core;large_scale;machine learning;online learning;algorithm requires;spectral clustering;semi-supervised;belief propagation;task-specific
modeling assumptions;hidden patterns;graphical models;latent dirichlet allocation;collaborative filtering;variational inference;topic models;document search;large collections of;topic modeling;social networks;prediction problems;algorithms for computing;topic modeling
data sharing;data grid;access controls;storage systems;digital libraries;distributed data;distributed environment
geographically distributed;xml data sources;data sets;query reformulation;xml documents;distributed processing;data integration;data mining;distributed sources;schema mappings;data integration;data sources
information systems;dynamic environments;data stored;semi-structured;heterogeneous data sources;human experts;information integration;cost-effective;autonomous data sources;integration process;closed-world
query processing;query processor
data grid;query execution engine;optimization strategy;query execution;distributed query processing;scientific applications;grid services;scheduling algorithm;adaptive strategy;grid environment;data intensive
user query;data exchange;data files;parallel execution;computational resources;long running queries;query processor;grid environment;scientific data
web service;grid services;grid computing;data resources;service providers;data access;web services;working group;data access
scientific applications;wide range;simulation model;data intensive
large-scale;data files;keeping track of
grid computing;distributed databases;distributed computing;unstructured data;data management
simulation data;data services;simulation studies;grid services;object-relational;data access;large datasets;data services
highly distributed
approximation algorithms;wireless sensor network;sensor node;sensor networks;theoretical analysis;network topology
growing number of;network bandwidth;location-aware;mobile devices;information access;mobile communications;web server;computing systems;information management;mobile objects;mobile applications;computing environments;web services;location-based;location-based services;location based services;application server;monitoring systems
real-world;large text collections
mining results;web log;visualization tool;web log;web log mining;web logs;data preprocessing;web-site;access patterns;retrieve information;mining algorithm;sequential pattern mining;user behaviors;web log
clustering;nearest;nearest point;similarity measure;higher dimensional;real data;distance measures;subspace clustering;data mining;dimensional data;subspace clusters;low-dimensional;synthetic data
sample size;relevant documents;document collections;sampling strategy;sample sizes;distributed information retrieval;obtain information
fault-tolerant;agent-based;mobile agents;distributed systems
clustering;web documents;clustering results;data analysis;web documents;data model for;knowledge engineers;web document;document representation;vector space model;clustering
xml data;xml view;query evaluation;xml tree
wireless sensor network;large number of;energy efficient;collected data;simulation results;low-power;sensor networks;sensor network;sensor nodes;network lifetime;data gathering
web applications;web service;service selection;domain specific;web service composition;web service composition;web services;integer programming
data source;low accuracy;database;scientific data;specifically designed for;distributed queries;agent technology;agent-based;databases;data management;query processing
web service;schema matching;web service composition;web services;fundamental problem;data flow
end user;content providers;web servers;evaluation criteria
rdf data;efficient storage;storage space;data sets;current web;query performance;semantic web;resource description framework
access control rules;web applications;fine-grained access control;xml format;access control;web applications
large scaled
high quality;search engine;heuristic rules;web sites;machine learning;automatically constructing;natural language processing;optimization techniques
multiple data streams;cross correlation;theoretical foundation;real data;data streams;correlated patterns;space complexity
data-driven;clustering problem;sparse data;high dimensional;large enterprises;business transactions;clustering high dimensional data;large number of;data set;subspace clustering;clustering algorithm;transaction data
web directories;search engines;web data
instance;classification model;similar objects;classification;similar object;database;similarity based;semantic web;current web;information retrieval;instances;semantic web;classification models;web mining;baseline methods;search engine;semantic retrieval;web application;ontology based;machine learning methods
join results;simple queries;aggregate queries;large number of;join processing;communication cost;sensor networks;query processing;sensor network;join queries
data-flow;composite service;petri nets;petri nets;web services
web content
role-based access control;role-based;role-based;access control model;role-based;information sharing;distributed environment
distance computation;edit distance;xml document;xml applications;tree-based;xml repositories;cost model;structural summaries;xml documents;similarity search;weighting scheme;digital library;queries efficiently;xml trees;structural similarity
case study;computational model;trust model
sensor networks;sensor networks;key management;location-aware;location-aware
mining algorithms;web communities;structure mining;web mining;real world;web communities;information technology
web-based;web environment;web-based
clustering results;textual information;level features;hierarchical clustering;low-level features;image description;data records;hierarchical representation;image description;clustering algorithm based on
end-user;classification technique;vector space;classification;text classification;specific topics;text analysis
web documents;search algorithms;web search;search engine;search engines;meta-search;quantitative analysis
web services;web service;semi-automatic;semi-automatic;web services
approximate matching;edit operations;data exchange;tree edit distance;xml documents;xml document;algorithm runs in;relevant information;minimum cost;efficient access to
learning process;data dimension;data warehouse;knowledge base;semantic web;query log;relational database;relational database;ontology learning;ontology learning
parameter tuning;sensor nodes;sensor networks;diverse applications;sensor network
temporal logic
content-based filtering;received increasing attention;user profile;multi-layer;collaborative filtering;personalized recommendation;user interests;user interests;dynamic events
search results;search term;web search;initial query;query term;domain knowledge;query modification;baseline methods;web page
keyword based search;search results;ontology-based;search performance;instance;ranked list;semantic similarity;concept hierarchy;data model;semantic search;instances;query keywords;knowledge base
data collection;specific context;modeling method;software systems;semi-structured
web resources;learning performance;security issues
web resources;high level;group structure;role-based;communication protocol;web resources
graph theory;large-scale;semantic web;concept map;interactive visual;concept map
single object;algorithm called;highly heterogeneous
clustering;web-based;sparseness problem;nearest neighbor;collaborative filtering;collaborative filtering;cluster-based;cluster-based;large-scale;collaborative filtering algorithms;data sets;similar users;smoothing method;clustering algorithm;clustering approach;nearest neighbors
search results;web search;web search;classification;semantic relationships between
extracting information from;event logs;process mining;petri nets;business process;process models
service provider;service providers;privacy concerns;multi-channel;privacy policies
user profile;community-based;overlay network;community members;resource discovery;individual users
plans;semantic web applications;web application;logic programming;formal representation;semantic web;description logic;logic program
query execution;genetic algorithms;semantic web;information retrieval;xml documents;semantic web;genetic algorithms;real data
small groups;knowledge management;scientific data;information exchange;knowledge creation;human interaction;knowledge-intensive
xpath queries over;xml processing;content extraction;information management;lessons learned;dynamic content;information overload;information integration
web service;business processes;main components;web services
case studies;text based;text based;high dimensional;text-based;information flow;text corpus;knowledge discovery;information flow;knowledge representation;semantic space;scientific discovery
web services composition;web services
web services;web services composition;prototype systems;information services;optimization techniques;data intensive
semantic web services;semantic web services;semantic web services

join operations;sharing data;view maintenance;simulation experiments;peer data management systems;push-based;simulation results
personalized recommendation;personalized recommendation;information overload;collaborative filtering
service-oriented;agent-based;implementation details;agent-based
utility function;web based;trust model;semantic web;prior information;decision theory
location based service;access method;index structure;optimal path;highly variable;mobile devices;view-based;hierarchy levels;road networks;traffic control

web pages;automatic extraction of;textual information;textual features;web pages;visual information;search engine results
high-dimensional space;data points;high dimensional;real datasets;high scalability;projected clustering;data stream;high-dimensional data streams;arbitrarily complex
web pages;web page;information extracted from;case study;low resolution;low-resolution;high resolution;automatically generated;web page;high-resolution;readability
xml documents;hierarchical structure;xml structure;information exchange
data grid;join operations;join results;join algorithm;join processing;grid environment;data transmission
high degree of;distributed environments;distributed applications;formal specification
computing environment;hierarchical structure;wireless networks;mobile devices;high-efficiency
gene ontology;web-based;gene ontology;database;semi-structured;early stage;information integration;genomic data;data integration
text mining;web pages;baseline methods;machine learning methods;classification;table extraction;table extraction;naïve bayes;information retrieval;document structure;classification model;knowledge acquisition;detection performance;information extraction from;machine learning
great significance;mining task;database;mining frequent;mining frequent;algorithm called;partially ordered;data mining;discovering patterns;real world;episode mining

edit distance;web applications;similarity metrics;web pages;tf∙idf;web page
semantic relatedness;web pages;key words;highly relevant;web page
web sites;database;query result caching;database server;database-backed;web sites;instances
natural language processing
collaborative filtering algorithm;collaborative filtering;database;collaborative filtering algorithm;clustering process;active user;scalability problems;collaborative filtering algorithms;nearest neighbors
web service;web services;large-scale
multi-modal;multimedia presentation;domain-independent;huge amounts of;user requests;search engines;news search;web users
grid computing;access control;access control;grid computing;role-based access control;markup language
web services composition;web services composition;service discovery;low recall;web services;service discovery;highly reliable
search results;web pages;high quality;web archive;search engine results;search engines
data stream applications;network monitoring;data stream processing;data stream;multi-tier;data processing;network model
algorithm generates;dynamic programming;large quantity of;mathematical formulation;scheduling algorithm;scheduling problem;distributed systems
description language;agent based;petri nets;partial matching;multi-agent;class hierarchy;service discovery;confidence levels
social relationships;control mechanisms;design choices;management architecture;modeling approach
web-based;ontology-based;data warehouse;domain ontology;data warehousing;semantic relationships;problem-solving;user-friendly;high-level
lower) bounds;related concepts;low quality;upper bounds;similarity matching;lower bounds;heuristic rules
software architecture
search space;location-aware;frequent patterns;mobile environment;location-based services;data mining
navigation systems;extracted information;target object;keyword-based;search engines;web search engines
web-based;case based reasoning;workflow systems;human learning;web-environment
workshop report;management systems;data resources;large scale;water resources;data sets
data collection;hierarchical approach;water distribution;heuristic method;sensor network
gradient-based;dynamic programming;linear model;optimization method;quadratic programming;optimization methods;objective function
information services
information service;water resources;web service;information technology;water resources
information systems;water resources;knowledge management;water resources;heterogeneous systems;integration process;high cost
theoretical foundation;constraint language;meta-model;case study;modeling language;ontology mapping;ontology mapping
spatial information;spatial data;geographic information;high scalability
federated database;data resources;data warehouse;service-oriented;water resources;service-oriented;multi-source;data integration
application systems;generated automatically;decision making
decision-making;geographical information;remote sensing
business transactions;workflow systems;workflow views
web pages;semi-structured;web sites;pattern mining;html pages;web contents;pattern discovery;semi-automatic
ontology driven;sparql queries;large number of;ontology driven;instances;case studies;data management;business transactions
context information;web-based;information sharing;context-aware;service-oriented;xml-based;application developers
service-oriented;service-oriented;data-source;web services
web-based;web pages;large-scaled;knowledge base;information extraction system
decision support;problem solving;water quality
water resources;data collection;monitoring data;data transmission;database
application systems;large-scaled
web services;sql queries;web service;text descriptions;3;service-oriented;1;web service;environmental conditions;2;5;4;web services;template based;programming languages;operating systems
grid computing;multiple users
web service;fine-grained;user groups;high resolution
source code;web information systems;user interfaces;user requirements;ontology driven;web information systems
clinical information;medical data;data stream processing;data stream;data stream;clinical information
web pages;data source;data volume;end users;index structure;dynamic nature of;domain specific;web sites;collected data;instance;search engine;relevant information;query interface;web search engines;data storage
world wide web
database;business logic;direct access to;web application;user interface;user request;web server;entire process
resource sharing;supply chain management;web services;heterogeneous data;1;web services;business process management
context information;context-awareness;mobile computing;computing environment;context awareness
network management;resource utilization
hierarchical structures;database;ontology-based;meta-data;information retrieval
database;database
generation algorithm
web pages;graph structure;database;database entries;highly ranked;web page;link analysis
web content;web page;web interface;visual presentation
database architecture;database technology;data store;database
database;data stream;large collections of;web searching
data sources
fast approximate;high-speed;multi-dimensional;streaming applications;high level;network monitoring;general-purpose;hierarchical structure;data stream;space requirements;data-stream;data streams;rates;high-speed data streams;streaming data;data distribution
approximate algorithm;adaptive” algorithm;streaming data;incremental computation;sensor networks;memory resources;streaming environments
selectivity estimation;data mining applications;hardware technology;data stream mining;query processing in;data sets;data stream;query processing;large amounts of data;data streams;range queries
language independent;real-world;information systems;ontology alignment;databases
web databases;count-based;web query interfaces;complex matchings;query interface;query interfaces;schema matching;schema matching;user queries;real data sets;greedy algorithm;matching problem
real world datasets;relational databases;structured data sources;data management;instances
query execution;skyline computation;skyline queries;data partitioning;query processors;large number of;partial order;total number of;skyline points;skyline queries;complex data;skyline query processing
data values;load distribution;data-access;hash function;replication;query processing in;fault tolerance;replication;data replication;query processing;range queries;load balancing
web search engine;data collections;large number of;comprehensive evaluation;information retrieval;data sets;keyword query;query routing
query optimization;multi-dimensional;score based;selectivity estimation;multi-dimensional data;decision support;ranking function;data mining;access method
nn) queries;search algorithms;nearest neighbor;storage overhead;data distributions;tree-based;euclidean spaces;mathematical analysis;sparse datasets;road networks;network topologies;nearest neighbor;network topology;nearest neighbor search
response times;approximation techniques;index structures;main memory;query throughput;static data;dimensionality reduction techniques;solution space;nearest neighbor queries;high frequency;dimensional data;real world data sets;dimensionality reduction;nearest neighbor search
security policies;database schema;access control;user queries;xml document;access-control mechanism
private information;game-theoretic approach;information sharing;information-sharing
multi-dimensional;temporal aggregation;multi-dimensional data;database;interval data;multi-dimensional;large data sets;business intelligence;query processing;aggregation operator;operator;temporal data;temporal databases;result tuples
temporal dependencies;access method;partial information;threshold queries;time series;user-defined;similarity search;similarity search;databases;real world
input data;application requirements;xml data;data representation;indexing scheme;xml databases;query processor;operator
text queries;document representation;multiple times;document collections;related documents;information retrieval systems
keyword queries;xml retrieval;relevance information;extensible framework;xml data;text-based;keyword-based;relevance feedback;ranked retrieval;search engines;retrieval quality;query conditions;query expansion
text search;input text;completeness;tightly integrated with
database instances;database;user interfaces;instance;operator;database schemas
physical design;database;database design;refinement process;optimal” set of;physical design;materialized views
user transactions;databases;high availability;relational schema;database schema
maintenance cost;random samples;main memory;disk access;random sample;incremental maintenance;base data;large datasets;disk-based;random sampling
multi-dimensional;density-based;aggregate data;histogram construction;clustering techniques;evolving data;incremental clustering;main idea;data synopses;cluster analysis
conjunctive predicates;hybrid approach;selectivity estimation;query optimization;ad-hoc queries;single attribute;sampling-based;multiple attributes;real data;optimization problem;sampling-based;minimal overhead;random access;commercial database systems;error bounds
multi-dimensional;skyline computation;skyline query;data points;high dimensional;approximate algorithm;decision-making;large data sets;high-dimensional spaces;skyline points;high-dimensional space;efficient approximate
end-users;interactive exploration;user-defined;data model;large data volumes;data sets;data cubes;visualization techniques;desired level of
statistical properties;query patterns;network size;simulation studies;pattern tree;index structure;data items;information retrieval;data sets;tree index;queries efficiently;query keywords
probabilistic framework;complex queries;selectivity estimation;twig queries;real datasets;query plan;probabilistic approach;approximate queries
xml updates;data dependencies;xml data;xml documents;update operations;np-complete
schema information;query processors;relational systems;structural join;xpath processing
virtual world;data collection;application logic;rfid data;event detection;complex events;business applications;complex event processing;data processing;data streams;rfid technology;high volume;virtual worlds;event-based;data integration;physical objects
window queries;data stream systems;data streams;long-running;stream processing;serializability;data stream systems;concurrency control;sliding windows
span multiple;optimization techniques;publish/subscribe systems;indexing methods;data streams;formally defined;finite state
data objects;algorithm generates;search space;tree based;nearest neighbor;database;data point;nearest neighbors;large datasets;queries efficiently;buffer size;nearest neighbors
mobile clients;mobile objects;trajectory patterns;input data;synthetic data sets;communication network;mobile devices;sequential patterns;mobile objects;instance;discovery algorithm;wide range;data mining;mining sequential patterns;sequential pattern;discovered patterns;classifier
prohibitively expensive;itemset support;efficiently extract;high accuracy;frequent-itemset mining;cost-effective;power-law
target) image;target image;query points;relevance feedback;retrieval performance;content-based image retrieval;query point;query refinement
algorithm finds;black-box;retrieval effectiveness;access methods;database;similarity search;multimedia database
natural language interface;xml data;query interface;application domain;xml database;user study;natural language
xpath expression;storage manager;xml storage;storage structure;xml data
customer relationship management;duplicate detection;similarity measure;xml data;data model;xml elements;data warehousing;relational data;similarity measures
data residing;xml database;xml instances;exi
set cover;query optimization;evaluation cost;dual problem;real-world;target object;web-accessible;life sciences;life science;query planning;greedy heuristics;computational overhead;navigational queries
problem remains;life sciences;total number of;randomly generated;evaluation strategies;scientific data;conjunctive queries;relational model;monitoring queries;databases;life science;distributed data;np-hard
skewed data;relational data sources;database management systems;federated queries;accessing data;query optimization;query execution plan;plan selection;intermediate results;federated queries;query plan;case study;real world;query processing;access plan;query optimization;cost model;statistical information;relational queries;query performance;data sources;execution plans
nn queries;index structure;multi-resolution;distance measures;lower bounds;similarity queries;spatial structure;distance metric;feature spaces;high dimensionality;sequential scan;spatial location;range queries;feature vector
query execution;multi-dimensional data;indexing structures;indexing techniques;missing data;databases;answer queries;synthetic data sets
indexing schemes;indexing method;vice versa;trade-offs;performance guarantees;access latency;indexing scheme;mobile clients;service providers;usage scenarios;access latency
query execution;workload management;sql queries
data-management problems;rewriting queries;conjunctive) queries;general case;select-project-join;maximally contained rewritings
data objects;high-quality;reachability queries;web navigation;directed graph;fast computation;large graphs;graph datasets;semantic web;reachability query;queries efficiently;complex data;construction cost;internet traffic;theoretical bounds
geographic regions;clustering;network size;synthetic data sets;spatial clustering;communication costs;performance gains;index structure;path queries;spatial regions;underlying data distribution;sensor networks;clustering algorithm;sensor networks;data characteristics;real world;range queries;clustering quality
query execution;cluster based;temporal properties;performance degradation;cluster-based;spatio-temporal;continuous queries over;fine-grained;moving objects;result quality;continuous spatio-temporal queries;data streams;temporal data;spatial join;real datasets;spatio-temporal queries;continuous queries
data objects;location-based queries;caching scheme;multi-granularity;location-based services;future queries
context-aware;distributed environments;dynamic environments;databases;event-condition-action
distributed query processing;xml queries;xml data management;7;complex applications;data management applications
xml) database;instance;semi-structured;probabilistic information;web) content;keeping track of;information stored in;web services;tree pattern queries;implementation issues;tree model
event-condition-action;end users;database;rewriting algorithm;peer data management systems;databases
business activities;business data;providing users;domain model;data model;business processes
business information;service-oriented;data analysis;business information
xml databases;concise representations;data sets;data synopses;approximate queries
conjunctive predicates;query execution plans;optimizer;partial information;db2 udb;1;maximum-entropy;cost-based query optimizer;relational database management system
extended abstract;database management systems;database systems
xml schemas;schema evolution;sql server;instance;schema evolution;microsoft sql server;xml schema
service providers
data warehouse;query performance;data management;data warehouse systems;large amounts of data;column-oriented;olap applications;query processing
data-driven;12;aggregation techniques;query processing engine;large data sets;data replication;data analysis;compute nodes;data processing;query processor;data analytics
data analysis;1,8;database;data warehouse;user requirements;data sources;real world
data store;xpath query;native xml;visual interfaces
database;spatio-temporal;query language;moving objects;location-based services;6;9;location-based;object-relational dbms;data management
text mining;data lineage;database;semi-structured;business process;meta data;data provenance;operating systems
efficient computation;error estimation;memory space
xml schemas;instance;real-world;xml documents;data structures;schema evolution;6;error prone
temporal dependencies;query execution;response times;threshold queries;data mining;threshold queries;time series;application domains;instance;large sets of;user defined;databases;query processor
data objects;clustering;density-based;distance functions;hierarchical clustering algorithm;multiple views;complex objects;data set;instances;databases;feature spaces;multimedia data;data distribution
visualization tool;xml documents;xquery queries;xml trees
free text;decision-makers;decision support;location information;sensor data;video streams
storage devices;classification;tool called;genre classification;hierarchical classification;user feedback;music collections
gene ontology;database;visual interface;scientific databases;aware query;distinguishing features
ontology languages;declarative queries;query language;5;4;language independent;ontology languages;data integration
data sources;spatial data;increasing demand for
data objects;multi-dimensional;cost models;index structures;index scan;data set;tool called;nearest neighbor queries;query processing;nearest neighbor query;dimensional data;sequential scan;index scan;spatial databases
approximate answers;xml document collections;highly heterogeneous;1,2,4,5;xml documents;score functions
1;xml updates;12;storage schemes;performance bottlenecks;lock;fixed-length;relational tables;xml documents;tree structure;5;query processing;7;9;8;relational dbms
xml documents;web directories;xml schemas;high-quality
query processing strategies;large-scale;database;simulation experiments;peer data management systems;peer data management systems;query processing techniques;skyline computation;distributed environments;structured data
ranking algorithm;large numbers of;social web;trust management;social interactions;grid computing;web services;data management
small sets of;user-preference;data mining tasks;pattern mining;user query;data sets;skyline queries;pattern discovery;pattern discovery
prior information;multi-relational;database;real-world;data mining approaches;multi-relational;generally applicable;databases;7;data mining;mining patterns;mining patterns;relational data
ordinal classification;data set;original data;predictive performance;data mining;building block for;class labels;response variable;models trained on
session based;multi-step;time series;time series
statistical approaches;parameter-free;theoretical framework;item sets;interesting itemsets;reduction techniques;pattern mining;item set;original data;mining frequent patterns;noise model
clustering;density estimation;estimation method;classification;anomaly detection;density estimator;large databases;density-based;information retrieval;task-specific;density estimation;nearest neighbour;estimation methods
mining task;mining framework;database;information-theoretic;item set;transactional databases;collaborative tagging;item sets;mutually-exclusive;application domains;real datasets;objective function
class label;real-world;anomaly detection;data instances;anomaly detection
decision trees;kernel matrix;learning tasks;feature mapping;multi-label classification;feature representation;artificial datasets;missing values
training data;imbalanced data;real-world;real datasets;data mining;learning tasks;training samples;dataset characteristics
clustering;learning procedure;image region;visual patterns;spatial context;image regions;spatial structure;pattern discovery;pattern discovery;high-level;discover meaningful
data mining method;complex systems;dynamic networks;multiple datasets;dynamic networks;provide evidence;temporal evolution;dynamic networks;real world applications
inter-relationships;graph mining;graph isomorphism;frequent patterns;database;real-world;configuration management;large graph;graph mining algorithms;databases;pattern discovery;configuration management;support counting
clustering;factor matrices;type information;large-scale;nonnegative matrix tri-factorization;multi-type;high-order;optimization problem;high-order;data set;multi-type;clustering techniques;relational data;clustering methods;multiple types of;real world;manifold regularization;nonnegative matrix tri-factorization;information contained in;relational data;real world data sets
optimization framework;large social networks;social behavior;instance;influential users;social networks;large social networks;problem called;community structure
problem remains;web pages;user interactions;social network analysis;scientific literature;data sets;error rate
clustering;latent topic;web pages;document level;relevant documents;probability distribution over;reconstruction error;optimization problem;vector representation;baseline methods;document clustering;limited number of;information retrieval tasks;document clustering;relative accuracy;latent topics;information retrieval;vector space model;clustering quality
clustering;spectral clustering;trajectory data;classification;real-world;cross-domain;trajectory data;cross-domain;hybrid method;feature representation;behavior analysis;intrinsic structure;high-level;behavior analysis
ensemble learning;unlabeled data;high dimensional datasets;benchmark datasets;feature selection;semi-supervised;semi-supervised;labeled examples;combines ideas from
data point;evaluation cost;decision tree;ensemble members;massive data;large-scale;large datasets
data point;clustering;ground truth;set-intersection;data points;application domains;correlation clustering;meaningful results;pair wise;optimization problem;overlapping clustering;amino acid;label sets;main idea;structured objects;requires solving;search algorithm;feature vectors
transfer learning;learning algorithms;classification;spam filtering;target domain;transfer learning;unlabeled} examples;input features;supervision;latent space;large-margin;learning problem;labeled examples;source domain
network routing;synthetic datasets;frequent patterns;database;tree patterns;log analysis;xml document
bayesian network;learning algorithms;learning bayesian networks;gaussian distributions;learning algorithm;bayesian network;time series;time series;statistical tests;high-order;search space
high utility;high utility;synthetic datasets;mining closed;item set;database;transactional databases;high efficiency;item sets;algorithm called;data mining task;mining task
classification models;test set;label information;mixture model;information-theoretic;learning algorithms;collective classification;real-world;collective inference;estimation methods;models learned
anomaly detection;data mining tasks;data sets;matrix factorization;matrix factorization methods;anomaly detection
clustering;clustering;hierarchical algorithm;pair wise;probability distribution;data sets;markov model;clustering algorithm;model-based clustering;markov model
multiple datasets;data mining applications;classification tasks;realistic data;class labels;confidence levels;machine learning algorithms;confidence levels
social media;network size;upper bound;large graph;instances;effective pruning;rates;dynamic behavior;dynamic networks;evolving networks;evolving networks;np-hard
data point;regularization term;regularization;data analysis tasks;multiple classes;data mining techniques;global convergence;vector machine;optimization problem;multi-class;data sets;feature selection;data mining;competitive performance;select features;multi-class;feature selection
document collection;topic distribution;visual analytics;optimization algorithm;multi-relational;visual analytic;text corpora;case studies;user evaluation;temporal evolution;relational data
clustering;markov random field;graph theory;heterogeneous information network;probabilistic model;multi-type;pair wise;real dataset;clustering performance;real-world networks;digital library;em algorithm;clustering algorithm;information network;rich information;multi-type;heterogeneous information networks
activity recognition;metric learning methods;multi-instance;distance function;distance metrics;classification accuracy;weighting scheme;metric learning;data mining tasks;density-estimation;machine learning;instances;kernel methods;metric learning;distance based;distance metric;multi-instance learning;multi-instance;multi-instance learning;multi-instance
em algorithm;real-world datasets;multiple tasks;multi-task;gaussian process;prediction performance;pair wise;relational information;gaussian processes;process models;relational learning;multi-task learning;multi-task
clustering;clustering;clustering methods;private information;social network;gibbs sampling;privacy-preserving;complex structures;real world
social media;face book;database;probabilistic model;time series;periodic patterns;model called
structured output;regularization;model parameters;gradient method;classification;input features;generalization performance;high dimensional data sets;learning algorithm;optimization problem;multi-task learning;feature selection;covariance matrix;real-world data sets;structure information;optimization algorithm;feature selection
large scale;real-world;parameter-free;statistical model
semi-supervised learning;data points;semi-supervised learning;real-world;classification method;graph construction;graph construction;semi-supervised;theoretical bounds
directed graphs;graph mining;community detection;directed graphs;real-world graphs;graph-theoretic
expected number of;real data sets;propagation model;social network;seed set;greedy algorithm;memory consumption;np-hard
feature space;redundant features;classification rule;high dimensional;associative classifiers;support threshold;markov blanket;bayesian networks;associative classifiers;emerging patterns;association rules;associative classification;highly sensitive;associative classification;model complexity
matrix factorization;bayesian methods;collaborative filtering;data sparsity;matrix factorization;gibbs sampling;multi-task learning;dirichlet process mixture;multi-task learning;variational bayesian;automatically determined;feature vectors
spectral clustering;low rank matrix approximation;image segmentation;spectral clustering;large scale;learning tasks;approximation error;clustering performance;clustering tasks;sampling scheme;low-rank matrix approximation
transductive learning;unlabeled data;classification;graph-based;graph-based;high accuracy;spanning tree;graph construction
interval-based;search space;point-based;closed patterns;mining closed;temporal patterns;mining algorithms;pattern mining;pair wise;real dataset;interval-based;closed sequential patterns;optimization techniques;real datasets
boosting framework;special case;loss function;prediction methods;link structure;real-world;network datasets;cost-sensitive;cluster) level;link prediction;prediction method;boosting algorithm;community detection;link prediction;network data;data skew
related topics;hierarchical dirichlet process;finer grained;dirichlet process;conditional distribution;real-world applications;data items;dirichlet processes;posterior distribution
quality measures;quality measures;statistical model
greedy algorithm;real data sets;relevant features;unsupervised feature selection;learning algorithms;unsupervised feature selection;reconstruction error;data instances;features selected;data matrix;learning tasks;data mining applications;class labels;supervised learning;huge number of;feature selection
graph classification;selection problem;evaluation criterion;discriminative features;real-world applications;graph classification;accurate models;feature selection;graph data;learning problem;feature selection problem;class labels;np-hard
taking into account;real datasets;closely related;real data;micro array data;algorithm called;high accuracy;higher accuracy;real world
clustering;clustering algorithms;hierarchy levels;clustering framework;background information;hierarchical clustering;hierarchical clustering;knowledge-based;instance level;data objects;semi-supervised clustering;clustering methods;optimization technique;relative constraints;traditional clustering;semi-supervised;semi-supervised;hierarchical clustering methods
historical data;classifier;sensitive attribute;supervised learning
sparse graphs;theoretical properties
execution times;real world;closely related;real world graphs;connected components
query intent;short queries;search engine;sufficient training data;large number of;random walk;random walk;pattern mining;query intent;share similar;cross domain;transition probability;query patterns;named entity;mobile phone;search engine;high probability;semi-supervised;transfer learning;log data
computational complexity;data analysis tasks;fault tolerant;synthetic data;missing values;high quality;subspace clustering;fault tolerant;subspace clustering;efficient computation;subspace clusters;missing values;fault tolerance;subspace clustering
data arrives;random subspaces;real-world applications;window sizes;wide range;class labels;individual features
training data;video data;semantic tags;human actions;human actions;topic model;supervision;learning problem;data mining;multi-label;real world applications;low dimensional representation
large number of;automatically constructed
classification;sampling methods;classification;structure preserving;time series;time series;structure preserving;gaussian distribution;support vector machines
high-dimensional;low-dimensional representation;test data;vice versa;manifold learning;missing data;image restoration;missing data;dimensionality reduction;dimensional data;missing values;dimensionality reduction
large-scale datasets;classification problem;learning algorithms;theoretical results;training data;multi-class;multi-class
random walk;graph structure;multi-document summarization;ranking process;random walk;multi-document summarization;benchmark dataset;related documents;text processing
intrusion detection;intrusion detection system;false positive;mixture model;variational inference;increasing number of;data set;feature selection;mixture models;variational framework
biological datasets;transition matrix;rank-based;markov-chain;rank aggregation;expression levels;gene expression analysis;rank aggregation;np-hard
clustering algorithms;spectral clustering algorithms;clustering methods;real datasets;random walk;parameter selection;clustering results;clustering process;parameter tuning;similarity matrix;data perturbation;clustering algorithm based on;laplace-beltrami
network model;large scale;information network;united states;database
data mining tasks;data reduction;compression methods;linear algebra;scientific data;sparse matrices;data reduction;multi-resolution;compression techniques;original data;pearson correlation;relative error;scientific datasets;massive amounts of data;data complexity;compression technique;user-defined
markov logic networks;tree-based;gradient boosting;statistical relational learning;benchmark data sets;gradient boosting
search space;sliding window;sliding window;sequential patterns;data mining research;tree structure;stream data;closed sequential patterns;data mining;frequent closed;sequential pattern mining
spectral clustering;clustering structure;anomaly detection;multi-source;multiple sources;multiple information sources;random walks
sampling algorithm;hierarchical dirichlet process;text data;multiple topics;topic detection;dirichlet processes;helps users;global analysis
energy function;energy functions;vector machine;ranking svm;optimization problem;machine learning;classification approach
discovering patterns;large scale data analysis;pattern mining;meaningful patterns;pattern discovery
semi-supervised feature selection;feature selection methods;feature selection;pair wise constraints
textual descriptions;textual description;classification;classification;search engine's;discriminative power;high quality;large number of;accurate classifier;high precision and recall;product search;classification approach;classification task;classifier
minimum set of;greedy algorithm;pattern mining;item set;high confidence;wide range;fundamental problem;pattern mining;log data
probabilistic modeling;probabilistic modeling;collaborative filtering;probabilistic model;model parameters;real-life data sets;search services;estimation algorithm
parameter-free;background model;database;sample data;databases;pattern mining;optimization techniques;missing values;real-valued;instance;real-valued;basic properties;maximum entropy;data mining;interesting patterns;hypothesis testing;knowledge discovery process;analytical model
local models;pattern mining;surprising patterns;bayesian networks;complex patterns;local models;rates;rule discovery;real world applications
binary classification;process model;gaussian process;high computational cost;binary classification;machine learning;gaussian distribution;gaussian processes;gaussian processes;benchmark data sets
clustering;parameter-free;classification;time series;multi-resolution;time series;original data;data mining;abstraction level
clustering;similarity metric;temporal data;gene expression;temporal patterns
gene expression data;computationally hard;order-preserving;order-preserving;linear programs;order preserving;order preserving;benchmark datasets
virtual machine;historical data;high level;service quality;time series;data mining system;computing resources;ensemble method;prediction algorithms
free text;negative examples;negative examples;set-expansion;set-expansion;instances;centroid-based;set-expansion;positive examples
feature space;multi-instance learning;context-aware;kernel based;kernel matrix;sparse representation;classifier;instances;sparse representation;data sets;context-aware;online learning;multi-instance learning;learning framework;multi-instance learning
markov random field;inference problem;mixture model;user-generated;evolutionary process;social communities;user behavior;joint inference;joint inference;probabilistic model;textual documents;diffusion process;online communities;real world;synthetic data
multiclass classification;generalization performance;multiclass classification;large margin;binary classifiers;main idea;margin;clustering procedure;classification algorithms
hyper graph;connected component;search strategies;database;domain specific;random walk model;item sets;laplacian matrix;data mining problem;graph representation;high dimensionality;similarity measures
unique characteristics;real-world;recommendation methods;business data
entropy-based;clustering algorithms;biological networks;complex systems;real-world;graph clustering;competing methods;scale-free;complex systems;entropy-based;locally optimal;social networks;information-theoretic;clustering approach;graph clustering;social networks
regularization;numerical experiments;similar objects;hash functions;unlabeled data;semi-supervised learning;linear discriminant analysis;pair wise;hamming space;separability;labeled data;pair wise constraints;semi-supervised;dimensional data;semi-supervised;supervised learning;low-dimensional;nearest neighbor search
input data;closed itemsets;matrix decompositions;data set;item sets;data sets;dimensionality reduction;frequent closed;matrix decomposition;dimensionality reduction
source code;similarity analysis;high-level;regular expressions;similar behavior;similarity analysis;behavior patterns;regular expression;instances;behavior model;high-level;low-level
clustering;clustering problem;real-world data sets;categorical data;categorical data
clustering;image retrieval;user experience;high-order;tag based;clustering
social relationships;viral marketing;error guarantees;real datasets;social network;seed set;viral marketing;viral marketing;problem setting;greedy algorithm;np-hard;limited number of
social network data;social network;privacy risk;privacy risk;social network data;privacy preserving;social networks;high probability
matrix factorization;real-world;data mining
binary classification;domain adaptation;test data;labeled data is;linear programming;domain adaptation;similarity functions;similarity function;similarity functions;linear classifiers;image annotation;classifier;hyper parameters
data mining applications;kullback-leibler;naive bayes;feature selection;feature weighting;feature weights;bayesian learning
mining task;large graphs;real networks;page rank;ranking list;large graphs;selection algorithm;ranking algorithms;diversified ranking;greedy algorithm;space complexity;diversified ranking
energy function;vector machine;context-aware;optimization algorithm;content filtering;classifier;instances;image set;context-aware;multi-instance learning;image recognition;multi-instance learning;image recognition
classification accuracy;classification model;latent dirichlet allocation;lda model;class label;classification methods;topic model;real world datasets;training data;approximation method;estimation algorithm;image classification;strong classifier;weak classifiers
high-dimensional;cutting plane algorithm;large-scale;real world datasets;feature selection;information retrieval;feature selection;feature selection methods;dimensional data;loss functions;classification error;feature selection
multi-dimensional;activity recognition;sensor readings;hidden markov model;sensor fusion;computing systems;higher level
low cost;anomaly detection;numerical results;data mining approaches;data sets;raw data;real-life;sensor networks;construction algorithm;normal behavior;data distribution
learning process;classification;sample complexity;auxiliary;classification;real world data sets;class label;probabilistic information;classification algorithms;auxiliary data;classification tasks;learning problem;class labels;auxiliary information;data mining and machine learning;probabilistic) information;auxiliary
nearest;centroid based;training phase;classification;theoretical properties;imbalanced data;vector machine;text categorization;pair wise;classification performance;text classification;class distribution;real-world;kullback-leibler divergence;perform poorly;multi-class;centroid-based;classifier;class-specific
recommendation quality;recommender systems;recommender systems;optimization problem;recommendation methods;produce high quality;sparse linear
preference-based;popular items;recommender systems;recommendation accuracy
problem remains;memory-intensive;classification techniques;human effort;concept-evolution;data stream classification;data streams;human experts;concept-evolution;data stream;data streams;takes place;special case;instances;stream classification techniques;classification error
real networks;online social networks;scale-free;path length;small-world;retrieval results;page rank;citation networks;estimation methods;community structure
data objects;feature space;real-world;face book;temporal smoothness;latent features;temporal link prediction;dynamically changing;dynamic environments;observed data;generalized eigenvalue problem;low-dimensional;training data;multi-task learning;social networks;temporal link prediction;link prediction;dimension reduction;data mining and machine learning;problem called
computational complexity;vertex set;shortest-path;distance preserving;large graph;large graphs;upper bound;mining algorithm;discovery problem
clustering;clustering;attribute-level;taking into account;clustering problem;real-world;clustering results;constrained clustering;attribute level;instance-level;instances;attribute values
test data;real-world;large number of;theoretical results;theoretical analysis;binary classifiers;binary classifiers;classifier
anomaly detection;large scale;false alarm;document understanding;detection method;mathematical analysis;text corpora;data stream;principal component;video surveillance
viral marketing;20;complex networks;12;influence propagation;special case;numerous applications;extensive simulations
clustering;clustering;parameter-free;data mining application;time series;time series;minimum description length;gene expression

em) algorithm;unlike earlier;tree structures;tree structures;interpretability;benchmark data sets;expectation-maximization
clustering;data points;hierarchical structure;step procedure;linear complexity;multivariate data;clustering algorithm;clustering algorithm
automatically learns;improving accuracy;multi-task;large-scale;activity recognition;learning method;learning method;learning framework;activity recognition;data sparseness;multi-task
data publishing;social network;instance;social networks;dynamic social network;social networks
emerging topics;emerging topics;anomaly-based;anomaly detection;term-frequency;social network;real data sets;social networks;generated dynamically;maximum likelihood;probability model
approximation algorithms;community structures;synthetic data sets;33;32;semi definite programming;social network;optimization problem;social networks;social networks;cost model;community structure;social networks
prediction accuracy;domain adaptation;negative transfer;unlabeled data;imbalanced class;target domain;margin;class distributions;linear combination;sample selection bias;classifier;counterpart;sample selection bias;labeled data;multiple kernel learning;accuracies;skewed;margin;classifier
real graphs;massive graphs;graph model;mathematical analysis;power-law distribution;social networks;random noise;graph analysis;real-world graphs
semi-supervised clustering;clustering;regularization;spectral embedding;loss function;graph-based;information contained in;vector-based;linear programs;data sets;semi-supervised clustering;pair wise constraints;manifold structure of;real-world data sets;semi-supervised;machine learning and data mining;clustering approach;spectral embedding
clustering;clustering;data mining technique;cluster structure;lower dimensional;highly complex;subspace clusters;valuable information;meaningful clusters;correlation clusters;subspace clustering;dimensional data;dimensional data;high-dimensional data sets;vast amounts of data;highly scalable
online reviews;spam detection;review text;valuable information;graph based
parameter-free;instance-based learning;feature set;classification;instance;categorical data;rule-based;learning algorithm;categorical data;classifier;instances;feature sets;rule-based;predictive accuracy
tensor decomposition;social tagging;social tagging;produce high quality
selection problem;graph mining;spatio-temporal;mutual information;supervision;instances;visual words;lighting conditions;pattern discovery
fast algorithm;distance function;regularizer;low rank;optimization algorithm;uci data sets;low rank;metric learning;manifold structure;semi-supervised;manifold regularization
multiple data streams;learned models;spatial regions;real-world;local neighborhood;event detection;event types;spatial region;bayesian framework
objective function;training data;multi-document summarization;ranking svm;learning method;cost sensitive;sentence extraction;multi-document summarization;training corpus;feature weights;document set
binary classification;low cost;multiple labels;low-cost;majority voting;pair wise;data items;pair wise;real world applications;supervised learning
data objects;string kernel;efficient computation;instance;tree kernels;kernel framework
partial orders;web-based;reference model;frequent patterns;partial orders;sequential patterns;generative models;partial order;reference model;statistical significance;result set;sequential pattern mining;traffic data
graph mining;graph isomorphism;counterpart;subgraph queries;graph queries;verification framework
nonnegative matrix factorization;nonnegative matrix factorization;real-world;text clustering;pair wise constraints;text data sets;text mining;semi-supervised;semi-supervised;clustering
intelligent agents;takes into account;planning graphs;planning problem;natural language understanding
basic concepts
multi-agent system;electronic commerce;strategy-proof;autonomous agents;multi-attribute;increasing attention;resource allocation
personal information;multi agent
information systems;decision-makers;decision support system;decision support;decision support systems;grid based;grid based
electronic commerce;goal-directed
data fusion;estimation problem;particle filter;monte carlo simulation;state estimation;particle filter
distributed computing;higher throughput;database server;lower cost;mobile agent;load-balancing
fuzzy model;fuzzy controller
fuzzy neural network;multi-sensor;data fusion
unknown environments;control method;information extracted from;theoretical foundations;control architecture;simulation results
fault detection
directed graph;sufficient conditions
convergence speed;neural network model;control policy

data mining;optimal values;historical data;data mining techniques;association rule mining;process control;operating parameters;large amounts of data;databases;excellent performance
negative association rules;minimum support;association rule mining;broader range;sparse datasets;mining algorithm
ant colony;database;real datasets;clustering method;connected components;clustering algorithm
mining frequent sequences;algorithm performs;database;data sequences;data mining problems;sequential patterns;incremental mining;mining process;sequence database;sequential pattern mining
clustering;input parameters;clustering algorithm based on;clustering algorithm called
prediction accuracy;measure called;classification;associative classifiers;classification approaches;high accuracy;association rules;associative classification;confidence measure
classification learning;training data is;learning process;hypothesis space;class label;pattern based;data representation;instance;learning framework;benchmark data sets
high-level;induction algorithm;decision rules;decision support;rule-set;data-sets;decision rule;machine learning;rough sets;conceptual clustering;data mining process;data-set
overlapping clusters;cluster validity;data set;data sets;clustering algorithm;cluster analysis
clustering;global optimal solution;genetic algorithm;mri data;clustering method;time series;higher accuracy than;clustering approach
pattern discovery;feature extraction methods;extracted features
extracting information from;matching algorithm;rule set;electronic documents;extract information from;structured information;information extraction from;tree model
case based reasoning;numerical simulation;based reasoning;data-mining;design parameters;knowledge-based;algorithm named;product design;knowledge-based
information systems;classification;data set;large data sets;rule generation;knowledge acquisition;automatically generate
classification;knowledge base;intelligent systems;knowledge integration;knowledge sources;knowledge base
knowledge extraction;description logic;formal concept analysis;knowledge bases
web documents;content similarity;similarity-aware;prediction algorithms;content management;similarity measures
web pages;knowledge based;search engines;search engine;information overload;knowledge base
world wide web;increasing demand for;search performance;information retrieval;relevance feedback;retrieval performance;theoretical analysis;query expansion;related words
fuzzy sets;structural characteristics
closed-loop;fuzzy controller
optimization problems;genetic algorithm
sufficient conditions
0, 1;0,1
portfolio selection;genetic algorithm
reference model;dynamic model;fuzzy rules;model parameters;dynamic performance;control scheme;adaptive control

workflow management system
basic properties
multi-objective;genetic algorithm;evolutionary computation;evolutionary computation;fitness;human interaction;human interaction
evolutionary algorithm;objective function;optimization problem;ranking method
database;temporal reasoning;spatial reasoning;artificial intelligence;geographic information
linear programming
singular value decomposition;mutual information;probability distribution;convergence speed;cost function;blind source separation;entropy based
conditional likelihood;sequence labeling;human effort;unlabeled data;conditional random fields;probabilistic approach;data sets;labeled training examples;labeled and unlabeled data;limited number of;labeled examples;conditional random fields
local search;global convergence;optimization methods;local minimum;global search;global optimization;search procedure;gradient method;local minima;objective function
multi-objective;optimization algorithm;objective functions;search process;analytic hierarchy process;multi-criteria;multi-objective optimization
operator;optimization problems;genetic algorithms;rough set theory
genetic algorithms;simulation model;case study;large-scale;complex systems;hong kong
clustering;pair-wise;nearest neighbor;feature set;classification;transitive closure;feature subset selection;clustering method;irrelevant features;feature subsets;feature selection;feature subset;databases;voting rule;combining multiple;classifier
nearest neighbor algorithm;support vector machines;takes into account;high dimensional spaces;pattern classification;empirical bayes;real-world;benchmark datasets;nearest neighbor rule;classifier;nearest neighbors;classification algorithms
long-term;hidden markov model;hidden markov model;em algorithm
incomplete information system;heuristic algorithm;continuous data;rough sets;rough set
attribute reduction;decision making;inconsistent information;rough set theory;rough set theory

dna sequences;protein sequences;spectral analysis
stock market;real data
simulation results;data migration;data transfer;access pattern;data distribution
dynamic programming
text mining;information retrieval;document frequency;term frequency;natural language processing;neural network
clustering;intrusion detection;local optimization;local search;false positive;global optimal;clustering method;detection method;global search;stochastic search;detection algorithm;data set;behavioral patterns;operator;clustering algorithm;simulation results;selection algorithm;detection rate
tree based;tree based;model parameters;simulation results
clustering;network traffic;clustering method;fuzzy inference system;anomaly detection
threshold selection;detection algorithm;simulation experiments;operating systems;large-scale
classification performance;security problems;network security;fusion strategies;network intrusion detection;fusion methods;classifier
intrusion detection;network security;anomaly detection;detection approach;detection rates;traffic management;case studies;covariance matrix
text classification;text classification;context-sensitive;semantic features;linguistic features
structured document retrieval;document collection;inference process;learning method;information retrieval
similarity information;kernel matrix;kernel-based;feature space;learning algorithms;pairwise constraints;learning tasks;locally linear;real-world data sets;metric learning;learning problem;distance metric;semi-supervised;kernel-based
semantic analysis;principal component analysis
multi-stage;discriminative features
matching techniques;visual field;classification;database;reference data;data set;query sequence;typically rely on;similarity measures
learning algorithm;neural networks;analysis reveals;learning rate;fuzzy neural network;parameter learning;multi-scale;fuzzy rules;basis function;wavelet transformation
operator;artificial neural networks;artificial neural networks;human-machine;human operators
network size;past experience;database;rbf neural network;convergence speed;iterative learning;iterative learning;simulation results
neural network
instances;large number of;neural network;artificial neural network;simulation results
fuzzy-neural network;test results;membership functions
genetic algorithm;genetic algorithm;neural network model;neural network;behavior model;neural network
sensitivity analysis;upper bound;sensitivity analysis
rbf) neural network;training algorithm;radial basis function;neural network;rbf neural network
operator;mathematical model;genetic algorithms;simulation model;genetic algorithm
intrusion detection system;generalization error;false alarm;artificial datasets;high precision;radial basis function;neural network;high precision
distance functions;distance function;string data;context-sensitive;kernel functions;support vector machines;context-based
binary classification;real valued;support vector machines;pattern recognition;genetic algorithm;parallel processing;decision trees;inverse problem;function approximation;heuristic algorithm;learning machines;maximum margin;support vector machines;margin;support vectors
data points;vector machine;forecasting model;short term;forecasting accuracy;artificial neural network
efficient parallel;distributed computing;low cost;general-purpose;small scale;science applications;object oriented
pre-processing;image processing
optimal values;genetic algorithm
feature space;gaussian kernels;vector machine;original data;separability;data set;data sets;high dimensional feature space
feature vector;classification;detection method;neural network;distance measures;edge image;fuzzy logic
face detection;detection method;motion model;input images;processing speed;classifier
stochastic model;hidden markov model;stochastic process;sketch recognition;hidden markov model
pattern recognition;general-purpose
moving average
speech recognition;training data;basis functions;feature extraction;independent component analysis;training samples;recognition systems;recognition tasks;image processing
target recognition;classifier
speech recognition;multi-stream;database;production process;audio visual;recognition performance;hidden markov model;multi-stream;audio-visual
post-processing;texture features;statistical method;texture analysis
correct classification;fusion method;classifier fusion;classification;multiple classifiers
video sequences;higher quality;markov random field;post-processing;simulation results
high frequency;detection process;discrete cosine transform
input image;visual learning;feature vector;efficient learning
real images;dynamic programming
operator;high frequency
fuzzy rule
color features
specially designed;detection algorithm;image sequence;size estimation;image processing;segmentation algorithms
high frequency;edge information;maximum likelihood;hidden markov model
event types;frequent closed;broad applications;traffic monitoring;real-world datasets
natural-language;natural language text;semantic network;sentiment analysis;knowledge base;opinion mining;common sense knowledge;dimensionality reduction techniques;common sense;common knowledge;opinion mining;knowledge base;natural language
mobile phone;real life data sets;graph evolution;skewed;large scale;multi-scale;multi-scale;mobile phone;generation process;data distribution;communication networks
related information;dynamic nature of;support vector machines;machine learning algorithms
social relationships;social graphs;conditional random fields;conditional random fields
propagation algorithm;overlapping communities;overlapping communities;social networks;excellent performance;social networks
network models;structural properties;np-complete;greedy algorithm;high degree;large networks;computationally expensive;optimization method;algorithm relies on
information content;graph mining;data compression;similarity measure;information-theoretic;large graph;parameter free;fully automatic;similarity measure between
social media;real networks;online social media;mixture model;social media;network motifs;social interactions;detection algorithm;complex networks;network motifs
real life datasets;network topology;social networks
scientific literature;text based;search engine
network models;community structure;community structure
large enterprises;recommendation quality;learning algorithms;data set;specific domains
random walk;social network analysis;resource allocation;random walk;network structure;operator;prediction task;link prediction;mobile phone;operator;communication networks;resource allocation;communication networks
communication cost;social networks;social network
temporal order;complex network;complex networks;dynamic nature of;predicting future;page rank;citation networks;predictive performance;regression model;citation networks;real world
distance measure;community detection;community mining;network nodes;real world;information networks
tree structure;action rules;action rules;fp-tree;action rules
real networks;complex networks;information extracted from;complex networks;link prediction;network data;increasing attention;diverse domains
detection problem;information content;sensor network;collaborative filtering;large-scale;wireless sensor network;health monitoring;sensor networks;sensor measurements;data-intensive applications;sensor nodes;collaborative filtering
association rule mining;association rule mining;domain knowledge;prediction models
classification;data mining techniques;human experts;classifier ensembles;ensemble approach;decision making
data bases;web sites;automatically constructed;information retrieval;similar sequences;relevant information;classifier
high degree of;power consumption;baseline models;data collected;individual features;tree models
informative samples;real-world datasets;classification problem;classification;active learning;imbalanced data;active learning;skewed;class membership;active learning approach;imbalanced data;active learning algorithms
individual records;census data;record linkage;valuable information;similarity measures;domain driven;data cleaning;census data;approximate string;additional information
mining framework;causal relationships;patient data;interestingness measure;databases;mining algorithm
regularization term;vector machine;linear programming;vector machine;knowledge-based;prior knowledge;knowledge based
data arrives;unknown environment;partial observations;real data;grid -- based
survival analysis;survival analysis;semi-supervised learning;predictive modeling;regression problem;instances;predictive performance;semi-supervised;breast cancer;semi-supervised
resource consumption;markov process
graph mining;classification;classification;mri data;graph based;mr images;support vector machines;alzheimer's disease
high-dimensional;prediction techniques;classification problem;mining framework;data mining;medical data;great potential;data mining;stream data;data mining technology;multi-scale;data mining;automatically identifies;measurement errors;data streams;cyber-physical systems
false positives;search algorithms;vast number of;data-mining approach;sequence databases;databases;protein sequence
prediction problem;clustering;event logs;extraction algorithm;real-world;time series;supervised learning techniques;missing data;operating parameters;high recall;semi-supervised;data mining algorithms
domain knowledge;training set;training data;high-quality;classification quality;training data is;instances;data cleaning;training examples;large quantities of;human experts;linear svms;class-labels;text classification;data cleaning;text classification;class labels;optimal performance;human intervention;algorithms rely on
domain-driven;increasing number of;domain-driven data mining;resource management;data mining research;domain-driven data mining;resource management;data mining
customer relationship management;product recommendation;recommender systems;recommender systems;cross-validation;knowledge discovery;data-mining based
domain experts;domain-specific;regression model;domain-specific
nonnegative matrix factorization;magnetic resonance;pattern recognition;magnetic resonance;matrix factorization;data mining;complex data
human brain;gene expression
search methods;clustering;statistical measures;profile based;text search;data mining;high sensitivity;search efficiency;structural similarity
clustering;mining results;mining algorithms;fault-tolerant;gene ontology;noisy data;fault-tolerant
association rule mining;case study;real world datasets;description logic;complex patterns
domain knowledge;process model;data source;workflow management systems;process management;data mining techniques;database;process management;poor performance;resource management;business process;business processes;data mining;role based
meaningful clusters;real data sets;gaussian kernels;data points;kernel function;cluster model;fuzzy clustering;multiple kernels;optimal kernel;multi-resolution;clustering algorithm;high dimensional feature space
classification;recommender systems;recommender systems;cross-domain;cross-domain;instance;mobile applications;main idea;multiple domains;collaborative filtering
duplicate records;higher level;variable size;data types;real-world;sampling technique;multiple data sources;data sizes;databases;key features;record matching;record matching
multitask learning;regularization;support vector machines;binary classification problems;optimization problem;multiclass problems;classification method;support vector machines;objective function;learning method
constraint programming;search space;constraint programming;real world;frequent patterns
data analysis;databases;pattern mining;data pre-processing;item set;databases;operator;data mining problem
search space;exhaustive search;heuristic search;depth-first search;pattern mining;heuristic search;declarative framework;search strategies;constraint-based;constraint programming;high-level;pattern set mining;pattern set mining;pattern mining algorithms
clustering;end-user;constraint-based;pattern sets;data mining;pattern mining;local patterns;constraint-based;pattern discovery;constraint programming;higher level
multi-relational;database technology;constraint networks;multiple relations;pattern mining;constraint-based;pattern mining;multi-relational databases;finding patterns;constraint-based;distinguishing features;graphical representation of;multi-relational databases;constraint programming;mining tasks
frequent pattern mining algorithms;frequently occurring;frequent patterns;frequent patterns;visual representation;frequent pattern mining
visual content;video data;text-based;news video;broadcast news;search engines;facial expressions;video frames
black-box;multi-task;classification;prediction performance;learning tasks;computational costs;cross-validation;data sets;baseline methods;multi-task learning;feature selection;selection criterion;training algorithm;multi-task
classification accuracy;time series;counterpart;document frequency;term frequency;natural language processing
formal representation
hybrid approach;music recommendation;face book;user profiles;user-supplied;tag-based;music recommendation;multi-domain;user profiling;hierarchical structure;latent semantic analysis;contextual information;tag-based;dimensionality reduction methods;semantic relations;user interests;dimensionality reduction
emerging patterns;pattern-tree;large number of;emerging patterns;item sets;emerging patterns;tree based;accurate classifiers;distinguishing features
clustering;contrast patterns;classification;similarity measure;data mining;data mining tasks;image analysis;pattern mining algorithms;data mining
link formation;conventional methods;frequent patterns;contrast patterns;real world datasets;application domains;item set;link formation
end-user;classification techniques;pattern mining;pattern mining;real-world applications;classification rules;emerging patterns;mining framework
decision making;response variable;database
radio frequency identification;database;kernel density estimation;rfid data;kernel density estimation;kernel density estimation;probability density
similarity functions;large number of;event detection;data stream mining
classification models;classification;classifier;evolving data;transactional data;classifier;transactional data
detection problem;false positives;high speed data streams;detection accuracy;false-positives;detection algorithm;computational resources;high speed data streams;computational overhead;data streams;long term;test statistics;traffic control
stream mining;data collected;sensor data;sensor technology;detection task;sensor data
decision trees;soft sensor;process control;soft-sensor;real-world;input variables
learning process;classification;feature) distribution;heuristic method;data stream classification;classifier;conceptual clustering;classification process;data stream;data streams;stream classification
web sites;web sites;information extraction;instances;web site;information extraction from;information extraction;decision making
clustering;data mining methods;decision tree;hospital information;mining process;stored data;temporal data;temporal characteristics of
linear programming;estimation technique;sales data
multi-dimensional;nearest neighbor;query efficiency;high-dimensional) space;real datasets;similarity search;query processing in;similarity search;distance-based;query type;theoretical analysis;distance functions;range queries;search efficiency
domain knowledge;multitask learning;bayesian network;active learning;scientific domains;learning algorithm;structure learning;data partitioning;algorithm learns;data partitions;domain experts;network structures
nonnegative matrix factorization;support vector machines;vice-versa;data analysis
classification model;classification;unlabelled data;labelled data;supervised learning;learning strategy
classification;great promise for;machine learning;instances;labelled data;classifier
classification;classification performance;sequential data;comprehensibility;multi-label;data characteristics;interpretability;class distribution
differentially private;differentially private
user study;social networking;automatically extracts;privacy policies;semantically meaningful
data sharing;statistical methods;privacy guarantees;logistic regression;differentially private;privacy preserving;chi-square;privacy-preserving;databases;simulated data;external information;statistical databases
graph structure;social networks;social network analysis;structural properties;social network analysis;social networks;information networks;information networks
high-dimensional;nearest neighbor classification;nearest neighbor;nearest-neighbor;dimensional data;dimensional data;nearest neighbor
synthetic datasets;sentiment analysis;large scale;increasing number of;statistical model;sentiment analysis;noisy data
domain knowledge;knowledge bases;knowledge discovery;web service;knowledge discovery;data mining;data integration
social relationships;user-input;online social networks;behavioral patterns;pattern mining;temporal order;real-life;graph patterns;relational structure;sensor data;structural information;contextual information
communication cost;total number of;social network;social networks;social networks
predictive performance;predictive model for;predictive models;climate data;variable selection
temporal dependencies;climate data;complex networks;term dependencies;climate data;social networks
hidden patterns;training data;low cost;low-cost;decision boundary;sensor measurements;learning algorithm;training set;data collected from;knowledge discovery;training samples;labeled data;cross validation;sensor data;sufficient training data;domain specific knowledge;future events;prediction algorithms;supervised learning;classifier;model parameters
prediction techniques;prediction techniques;wind speed;web-based;data mining techniques
probability distribution;statistical method;rates;energy management;data mining
prediction techniques;regularization;logistic regression;discriminative models;sensitive information;regularization;machine learning;data mining technologies;classification results;privacy-preserving
synthetic datasets;business data;statistical analysis;machine learning;synthetic data;regression trees;business data;highly skewed;tree-based;1;original data;sensitive values;synthetic data;larger datasets;synthetic datasets;simulation studies;business data;tree-based;survey data;linear regression;exploratory data analysis;statistical models;model estimation
clustering;hyper graph;real-world datasets;data publishing;community detection;rank-based;data publishing;privacy preserving;attack model;graph construction;information loss;data utility;security issues;relational data;increasingly popular
outlier detection;outlier detection;distance based outlier detection;privacy preserving;large datasets;locality sensitive hashing;high dimensionality;privacy preserving
data analysis;application domain;large number of;social network;social network;social networks;analysis tool
moving objects;uncertain trajectories;large-scale datasets;moving objects;uncertain trajectories
search results;search engine;social networks;search engine
social media;detection techniques;face book;sentiment analysis;database;language identification;short texts;user generated content;sentiment analysis;sentiment classification
classification;data publishing;multiple attributes;multiple attributes;sensitive data;privacy preserving;algorithms for computing;real world;privacy requirements;sensitive attributes
spatio-temporal;spatio-temporal;spatial analysis;dynamic nature of
computational complexity;large datasets;large-scale;spatial datasets;parallel implementation;high accuracy;making predictions;maximum likelihood
classification accuracy;parallel corpus;machine learning;sentiment analysis;cross-lingual;training documents;latent semantic indexing;machine learning;training sample
scoring methods;online social media;social media data;vast amounts of
sentiment classification;multiple classifiers;data mining task;sentiment classification;machine learning techniques;feature spaces;classifier;sentiment classification
distribution information;person tracking;image noise;illumination conditions;moving object
decision process;markov decision process;markov decision process;sampling-based
parameter values;learning examples;learning algorithm;algorithm performs;relational reinforcement learning;window sizes;relational reinforcement learning;reinforcement learning;artificial data;online learning;inter-related;addressing this problem;data streams;relational learning;data streams;data driven;inter-related
machine learning
clustering;common features;special characteristics;data mining techniques;data mining;data clustering;mining process;data mining;hidden knowledge;association rules;interesting patterns;discovery algorithms;similar characteristics
summarization techniques;multi-document summarization;multi-document summarization
online reviews;online users;classification methods;sentiment classification;product features;data mining
multi-aspect;supervised approach;prior knowledge;topic models;topic model;sentiment analysis;analysis tasks;multi-aspect;fully supervised;rating prediction
high quality;question answering;digital camera;product review;question answering;qa systems;real life;preprocessing phase;sentence extraction;opinion mining;customer reviews
sentiment analysis;special attention
multiple kernel learning;classification;linear programming;support vector machines;similarity matrices;multiclass classification;text fragments;multiple sources;multiple kernel learning;semantic information
collaborative filtering;product recommendation;rating data;collaborative filtering;target user;personalized recommendations;query expansion
evolutionary algorithms;evolutionary algorithms;genetic algorithms;multi-layer;neural networks;classification;predictive power;neural network;powerful tools for;logistic regression;neural network
case studies;data collection;statistical analysis
classification;classification accuracy;data mining approach;machine learning techniques;automatically detects;image processing;classifier
online reviews;sales data
user opinions;digital camera;user generated;data set;fine-grained;real life;optimization techniques;opinion mining;conditional random fields;customer reviews
user ratings;classification;free-text;automatic evaluation;feature-based;accurate models;detection task
social media;face book;special characteristics;sharing information;large volume;textual information;decision tree;logistic regression;mobile devices;user generated content;social media;domain-specific;social networks;social media data;machine learning;neural network;customer reviews
algorithm finds;real-world datasets;frequent items;data streams;skewed;lower bounds;data stream;data streams;high probability
historical data;classification;sequence data;time series;statistical model;hidden markov models;hidden markov model
case study;obtain information;identify patterns;databases
data mining techniques;data mining algorithms;data mining methods;data mining techniques
vector space;related terms;information retrieval;retrieval strategies;information retrieval models;information retrieval;vector space model
algorithm generates;real data sets;distributed monitoring;anomaly detection;cloud computing environment;low overhead;anomalous behavior;huge number of
clustering;sampling methods;parallel algorithm;data mining methods;large-scale;clustering solution;distributed memory;aggregated data;cluster analysis;distributed systems;fault tolerant
local search;classification problem;large scale;search experience;million web pages;data mining technologies;data quality;classification accuracy;service providers;scalable solution;online advertising;real-world;missing information
energy efficiency;information processing;information processing;sensor networks;sensor network;cyber-physical systems
real-world datasets;open-source;large-scale;community detection;large-scale;high efficiency;massive datasets;data access;community structure;programming model;community structure;complex applications;fault tolerance;map/reduce
data set;classification techniques;classification;data mining techniques;classification methods;united states;forecasting model;temporal features;data mining;temporal data
clustering;data structure;large-scale;spectral clustering;manifold learning;low-rank;large-scale;machine learning;information embedded in;graph based;data mining;intrinsic structure;kernel method;low-rank matrix approximation;massive data
image retrieval;image retrieval;database;retrieval accuracy;indexing method;efficient indexing;visual words;visual phrases
classification;large-scale;multiple features;kernel based;feature fusion;logistic regression model;multiple kernels;visual recognition;recognition tasks
mining results;hierarchical data;frequent patterns;frequent patterns;mined patterns;frequency information;visual representation;frequent pattern;frequent pattern mining
image super-resolution;target image;image super-resolution;high resolution;fully automatic;human supervision
shortest paths;source nodes;distributed computing;large graphs;shortest path;graph analysis
sensitive information;cloud services;cloud computing environment;storage capacity;risk analysis;data analytics;analyzing data;cost-effective;markup language;processing power
learning process;data model;gaussian mixture model;feature space;privacy concerns;heterogeneous data;heterogeneous data;data set;probability distribution;data sources
multiple criteria;classification model;data mining method;instances;linear programming;real dataset;credit card;case base;classification model;credit card;actionable knowledge;minimum cost
clustering;information retrieval applications;text data;external information;prior information;similar items;semi-supervised;clustering;online news;labeled data;sample points;web text;learning performance;human effort;real-world;web text;cluster membership;pair wise constraints;prior information;clustering model;textual features;clustering analysis;cluster analysis
classification performance;meta-learning;automatic selection;meta-learning;evaluation measure;multi-label classification;multi-label;multi-label;classification algorithm
image retrieval;image retrieval;ad-hoc;ranking scores;databases;ranking svm;ranking models;information retrieval;content based image retrieval;image database;visual words;text retrieval;ranking functions;ad-hoc;computational burden;ranking model
data set;video content;high accuracy;classification;large-scale
heterogeneous networks;visual analysis of;visual analysis of;detect outliers;heterogeneous network;high-degree
clustering;clustering;clustering results;historical data;theoretical analysis;data points;large-scale;large-scale datasets;clustering method;theoretical analysis;support vector;support vector;cluster labeling;support vector;memory consumption;support vector machines;support vectors
classification model;local features;large-scale;multimedia information retrieval;data set;scene classification;scene classification;multi-class boosting
large-scale;real world datasets;semi-supervised learning;global consistency;gradient descent;labeled data;factorization method;semi-supervised classification;theoretical bounds
clustering;clustering problem;optimization technique;cluster structure;multi-label learning;clustering approaches;overlapping clustering;multi-label learning;scalability problems;multi-label
real-world situations;unlabeled data;active learning;active learning;learning algorithms;problems arise;uncertainty sampling;active learning;unlabeled data;probability density;active learning algorithms;active learning algorithm;multi-class classification problems
word vectors;classification method;classification;database;classification accuracy;naive bayes;influential users;topic classification;tf-idf;text-based;information retrieval;similar topics;high accuracy;topic classification;text classification;decision tree learner;classifier
target values;data mining applications;pattern mining;reinforcement learning;reinforcement learning;long-term;pattern discovery;real world
spatial temporal;topological patterns;database;spatial-temporal;spatial temporal;incremental maintenance;topological patterns;databases;spatial-temporal
high efficiency;feature types;candidate patterns;computation cost;spatial information;pattern mining;mining process;memory space;index structure;mining algorithm;spatial data mining
high-resolution;features extracted from;high-resolution;machine learning;features including;contextual information;image classification;image classification;high-resolution;machine learning approaches;remote sensing image
dynamic programming;similarity measure;similarity measure;time series;time series;data mining;line segments;piecewise linear
bayesian model;classification;classification rule;model offers;temporal patterns;detection method;time series;rates;detection performance;data mining;bayesian hierarchical;simulation results;detection rate
user profile;page rank;information integration;random access;real world;social networking
boosting framework;clustering algorithms;clustering problem;pairwise constraints;clustering algorithm;semi-supervised;uci datasets
classification model;fold cross-validation;classification;classification accuracy;classification methods;quadratic programming;linear discriminant analysis;vector machine;feature selection;logistic regression;multi-criteria
multi-modal;gradient-based;real life datasets;real-life datasets;optimization algorithm;learning algorithm;underlying assumption;gaussian mixture models;locally optimal;maximum-likelihood;mixture models;likelihood function;expectation-maximization
clustering;clustering algorithms;clustering results;kernel-based;learning framework;clustering method;real-valued;counterpart;kernel k-means;kernel space;clustering methods;kernel k-means;local minima;kernel-based
clustering;similarity information;local minimum;clustering method;clustering approaches;clustering method;data sets;clustering methods;theoretical analysis;clustering algorithm
1;network data;end users;big data
real-world;spatio-temporal;data extraction;web logs;web services;temporal data;web log
high correlation;high correlation;data mining and knowledge discovery;data mining techniques;spatio-temporal;traffic information;traffic data
fast algorithm;local features;robot vision;object detection;video sequences;object detection;robot vision
complex network;web pages;graph mining;graph-based data;unstructured text;complex networks;link prediction;structured information;rule extraction;link prediction;knowledge base
dynamic networks;dynamic networks;original data
medical imaging;image analysis;image analysis;potential impact;medical imaging;potential impact
human perception;visual analytics;applications including;quality assessment;broader range;cognitive processes
ground truth;image segmentation;genetic algorithm;fitness;document images;case study;evolutionary computation;rule based;rule based;fuzzy rules
illumination conditions;local color;face recognition;human face;image collection;content based image retrieval;foreground detection;color images;images captured;facial expression recognition;standard database;human face;color images
character segmentation;character segmentation;success rate;character recognition;skewed
segmentation methods;image segmentation;interactive segmentation;segmentation method;low level;fundamental problem;image processing;image segmentation
instance;low quality;score (based
feature space;classification;genetic algorithm;fitness function;trade-offs;feature-selection method;genre classification;rates;feature-set;feature subset;genre classification
visual feature;classification performance;classification tasks;visual words;soft clustering
graph structure;image representation;image collection;feature representation;multiple kernel learning;multi kernel
clustering;clustering;clustering problem;clustering technique;clustering process;consensus clustering;benchmark datasets;consensus clustering
training set;classification;classification;extracted features;block size;feature extraction;block size;training set;error analysis;correct classification;nearest neighbour;pre-processing;color space
classification;classification;biometric recognition;face recognition system;facial features;facial feature;classification method;extensive simulations
natural language;natural language
probabilistic framework;database;multi-resolution;information fusion;multi-resolution;image matching;document image
test set
optical character recognition;ground truth;component analysis;segmentation methods;pixel level
suffix tree;suffix tree;similarity measure;suffix tree;probabilistic approach;error rate;character recognition
video sequence;scale invariant feature transform;post processing;gaussian filter;feature extraction;camera motion;motion parameters;feature matching
social networking;multiple datasets
color features;background model;video surveillance
video sequence;empirical mode decomposition;wavelet decomposition;coding scheme;image data;compression scheme;multi-resolution
high frequency;low quality;compression scheme;information derived from
hybrid approach;hybrid approach;face recognition system;appearance based;facial features;bayesian classifier;feature analysis;feature based;recognition problem
singular value decomposition;discrete cosine transform
hybrid method;dynamic programming;video frames;detection accuracy;object-tracking;intra-class;3;2;object detection;wide range;object categories
regularization;image based;image restoration
regularization term;regularization;edge detection;image restoration;edge detection;real world;image restoration
digital images;wavelet based;high quality;gaussian noise;image denoising;noise reduction;image denoising;quantitative analysis
spatial distance;image denoising
gaussian filter;gaussian noise;gaussian noise;specific characteristics;gaussian filter
edge preserving;synthetic aperture;wavelet based
storage space;high quality;summarization method;vice versa
signal processing
robotic systems
region based
public domain;matching algorithm;weighted average;detection algorithm;matching accuracy;fourier domain;databases;matching algorithm;error rate
unique features
feature extraction;verification problem;personal identification;recognition problem
face recognition;feature descriptor;linear discriminant analysis;face recognition;principal component analysis;binary pattern
sampling rate
clustering;clustering techniques
detection algorithm;shadow detection

information processing;information processing;decision process
multi-objective;multi-objective;genetic algorithm;objective functions;optimization problem;lighting conditions;light source;light source
data utility;low cost;privacy issues
clustering;clustering;vision applications;vision applications;database;color transfer;complementary information;color transfer;simulation results;color image

absolute error;image processing
image segmentation;genetic algorithm;test images;algorithm called;entropy based;image segmentation
feature space;classification;classification;feature extraction;vector machine;principal component analysis;feature set
image processing
image processing;exemplar based;image denoising;image restoration
image compression;document images;compression technique;multi-resolution;frequency information;high frequency;image compression;document image
clustering;video tracking;image analysis;statistical technique;video sequences;ground truth data;human intervention
multi class;ground truth;classification;image features;binary class;traditional models;low level;ensemble classifier;classifier
hybrid approach;fusion strategies;hybrid approach;information fusion;comparative analysis;optimal number of
recognition algorithm;fourier transform;feature extraction;human action recognition;human robot;human action recognition;action recognition;optical flow;video retrieval;pre-processing;action recognition
character segmentation;success rate
image quality;real life;hardware architecture
detection algorithm;13;breast cancer;14
directed graphs;rknn) queries;nearest neighbor;rknn queries;computation cost;safe regions;communication cost;location update;euclidean space;nearest neighbors;spatial networks
query evaluation;special case;optimizer;plans;execution times;optimal plan;duplicate elimination;general case;cost-based;optimizer;decision support queries;query optimizers;approximation algorithm;commercial database;interesting orders;increasingly complex;np-hard
type inference;type inference;type checking;partial information;formal models;business processes;type inference;execution traces;type checking
processing units;field-programmable gate arrays;power consumption;general-purpose;computation model
data objects;world wide web;synthetic data sets;data values;correlation coefficient;similarity metric;distributed applications;remote sites;data set;data sets;underlying data distribution;sensor networks;data items;automatically detects;cosine similarity;data distribution;similarity estimation
computational properties;test data;database;instances;databases;functional dependencies;data management
search space;data management problems;candidate matches;large graph;shortest path;reachability query;distance-based;databases;pattern matching;graph embedding;graph databases
application area;formal methods;information retrieval;search engines;natural language processing;information retrieval;web ir
search experience;search engines;web search;large-scale
machine translation;question answering;information retrieval systems;natural language processing
probabilistic framework;textual data;multimedia information retrieval;image databases;performance gains;image search;automatically generating;natural language processing;image annotation;latent topics;image annotation;retrieval applications
retrieval effectiveness;temporal information;amazon mechanical turk;temporal dimension;annotated corpus;retrieval models;retrieval model;language modeling approach;inherent uncertainty
web pages;anchor text;information retrieval methods;queries submitted to;language model;extract information from;term frequencies;digital library;search engine
text collections;structural features;classification
information retrieval;retrieval effectiveness;information retrieval
web-search;individual queries;weakly supervised;instance;web search queries;query sessions
low-dimensional;machine translation;transliteration;cross-language;feature space;text data;entity retrieval;canonical correlation analysis;canonical correlation analysis;cross-language information retrieval;cross-language
document ranking;queries submitted to;trec collection;documents retrieved;search result diversification;diversified ranking
search results;web searches;retrieval performance;user activity;retrieved documents;desired information;search tasks
feature selection method;ranking svm;ranking function;retrieval performance;feature sets;ranking functions
web search;anchor text;real-world;anchor text;target pages;web crawl;single snapshot;ranking quality
language-model;class information;document content;classification;language models;query performance;search result quality;quality measures;query difficulty
13;trec test collections;retrieval effectiveness;analysis reveals;relevance judgments;retrieval systems;data set;data sets;ranking methods;estimation methods
pattern-based;rank-based;text summarization;ordered list;human experts
amazon mechanical turk;information retrieval;user preference
user supplied;online news
retrieval effectiveness;performance predictors;retrieval performance;correlation coefficients;meta-search;query expansion;query performance prediction
vector machine;image annotation;specific properties;image annotation
real data sets;sequence alignment;image features;detection problem;dna sequences;multimedia information retrieval
search systems;semantic video;language models;probabilistic model;language model;news items;semantic concepts;retrieval framework;concept based
fusion methods;error rate;retrieval performance;matching methods;information retrieval
heuristic algorithms;information retrieval;random sample;computational resources;iterative algorithm;upper bound;theoretical analysis;occur frequently;random sampling
query traffic;learning algorithm;training dataset;logistic regression model;predictive model;web search engines;search nodes
clustering;clustering algorithms;probabilistic approach;probabilistic guarantees;information retrieval;high scalability;distributed environments;text clustering;million documents;clustering quality
pruning techniques;xml retrieval;queries over xml;retrieval performance;query processing;ad hoc
entity ranking;probabilistic framework;category-based;category-based;entity search;category information;competitive performance;term-based
pair-wise;convergence properties;support vector machines;optimization problems;ranking measures;evaluation criteria;machine learning;ranking algorithms;machine learning;ranking tasks;ranking methods;maximum margin;information retrieval
regularization;semantically related;query term;retrieval functions;rank documents;tf-idf;query aspects;information retrieval;data sets;retrieval models;retrieval performance;retrieval accuracy;semantic relations;length normalization;query terms
portfolio theory;subtopic retrieval;parameter estimation;information retrieval;quantum theory;probability ranking principle;probability ranking principle
wikipedia-based;semantic smoothing;language model;smoothing method;translation model;information retrieval;retrieval performance;trec ad hoc;semantic smoothing;em algorithm;language modeling approach;contextual information;query terms
collaborative filtering;collaborative filtering;baseline algorithm;performance predictors;performance prediction;performance prediction;increasing attention
user ratings;collaborative filtering;recommender systems;recommendation process;algorithm requires;recommending items;user similarity
user ratings;collaborative filtering;collaborative filtering;rank-based;data set;performance gains;expected) error;optimization framework
search results;user profile;web search;web search engine;social tags;web search;personal preferences
baseline methods;latent dirichlet allocation;collaborative filtering;large-scale;topic model;social tagging;topic models;probability distributions;real life;social tagging
related topics;translation model;predictive power;topic models;data sets;cross-lingual;cross-lingual;model called;related words
pseudo relevance feedback;patent retrieval;prior-art;large number of;relevance feedback;prior-art;search queries;selecting relevant;query expansion
sequence labeling;web pages;cross language information retrieval;hand-crafted;large quantity of;extraction rules;cross language information retrieval;web page;extraction patterns
user community;data set;united states;candidate set;performance gains;language independent
query topic;cost-based;biomedical information retrieval;ranking method
large-scale;building block;web archives
user queries;web pages;past experience;query independent
strong evidence;search effectiveness;information seeking;expansion terms;user studies;interactive query expansion;search interface
helping users;case–based reasoning;search strategies;relevant documents
language model;language models;language modelling;weighting scheme;term weighting scheme;user study
interaction data;data analysis;relevance criteria;user behaviour;emerging patterns;user study
irrelevant documents;pseudo-relevance feedback;test collections;retrieval performance;initial retrieval;pseudo-relevance feedback;expansion terms
similar documents;similarity search;large number of
image retrieval;classification models;machine learning algorithms;query difficulty;data set;query difficulty;query words;prediction task;prediction models
similarity-based;parallel corpora;translation probabilities;structured queries
search results;enterprise search;relevance score;web communities;social tagging;search result
query-logs;query length;query performance prediction;query terms;query generation
ir effectiveness;semantic relatedness;term frequency;information retrieval models;semantic information;probabilistic retrieval model;query expansion
ranked retrieval;relevance assessment;xml retrieval
federated search;result merging;selection techniques;server selection;selection methods
information retrieval;user query;language identification
document filtering;information retrieval
correlation coefficient;preference based;recommendation systems;collaborative filtering
potential impact;case study;retrieval quality;query performance
information retrieval;user-oriented;test collection
web service;ranked list;biased sampling;retrieval quality;response times
association rule mining;association rules;query logs
concept space;cross-language;brute force;web search engines;similarity search;similar documents;linear scan;cross-language information retrieval;cross-language
speech recognition;training data;clustering structure;individual queries;data sets;retrieval task;spoken content;query expansion
social networks;trec collections;short documents;user-generated;user-generated
search results;pagerank algorithm;web pages;ranking models;web pages;authority flow
test collections
information-theoretic;information content;text analysis
search results;sensitive information;temporal information;amazon mechanical turk;large-scale;annotated corpus;manually annotated;user interface;keyword search;search interface
web sites;provide feedback
machine translation;training phase;domain ontology;domain specific;target domain;domain-specific;cross-language information retrieval
web service
data analysis;user behavior;ir systems;1,2;interactive information retrieval;data obtained from;interactive information retrieval
feature space;life sciences;data analysis;data bases;information sources;data mining and machine learning;predictive models;life sciences;1;data analysis;analysis task;context dependent;life science;2;similarity measures
data sets;machine learning methods;vice versa;text-based;general-purpose;machine learning;text-understanding;document collections;natural language processing;text-processing;machine learning;machine learning algorithms
parameter estimation;instances;hidden markov models;bayesian networks;7,5;logic programming;probability distributions;3;10,8;4;6;14,18,13,9,6,1,11;naïve;inductive logic programming;statistical relational learning;2;logic program;11;13;12;15;17;16;19;probabilistic model;probabilistic learning;machine learning;context free;probabilistic logic;statistical relational learning;probability theory
time series;detect anomalies;data mining
electronic commerce;data warehouse;data collection;data mining and knowledge discovery;massive amounts of data
data streams;data structures;data stream;data streams;data intensive applications;data synopses;massive data sets
clustering;clustering algorithms;frequently occurring;clustering approaches;matrix decompositions;meta-clustering;meta-level;nonnegative matrix factorization;clustering methods;data mining;gene expression data
domain-specific knowledge;constraint network;constraint programming;version space;real world;sat-based;constraint satisfaction problems;combinatorial problems;solving complex
clustering;web pages;instance;multiple views;clustering method;instances;text collections;optimization criterion;mixture models
regularization;spectral methods;speech data;low-dimensional;low-complexity;constraint sets;spectral methods;dimensionality reduction
baum-welch;discriminative learning;multi-view;learning algorithm;generative models;information extraction;multi-view;sequential data;entity recognition;semi-supervised learning;support vector;unlabeled data;semi-supervised
modeling assumptions;computational complexity;ensemble classifiers;ensemble methods;expectation maximization;model averaging;classification results;classifier ensembles
accuracy compared to;learning bayesian networks;rule learning;classification tasks;learning algorithms;naïve bayes;3,4,14;instances;inductive logic programming;classifier;learning rules
search results;training data;spam pages;web spam;page rank;web site;highly ranked;search engine;common practice;link spam
learning process;linear support vector machines;classification accuracy;text categorization;prior knowledge;classification framework;generalization ability;input variables;large-margin
description language;general concept;application domain;real-world;learning systems;real-world domains;inference mechanism;integrating multiple;learning problem;relational learning;information needed;learning task;machine learning approaches
binary classification;auc;test set;classification tasks;highly skewed;pre-computed;normal distribution;statistical significance;training set size;classifier;class distributions
regularization;parameter-free;regression problem;reproducing kernel hilbert space;fitness;operator;kernel methods;optimal set of
maximum-entropy;learning models;instance-based;classification;classifier
training data;data acquisition;feature values;instance;learning strategies;relevant information;classifier
dual decomposition;minimal optimization;large number of;vector machine;semi-parametric;support vector regression;support vector machines
appearance model;classification accuracy;static images;facial features;facial expression recognition;image quality;lighting conditions
set covering;margin;learning algorithm;data-compression;margin;classifier
classification;data distributions;unlabeled examples;perform poorly;positive examples;unlabeled set
greedy strategy;learning rate;model-free reinforcement learning;reinforcement learning;finite-sample;theoretical guarantees
ensemble learning;learning algorithms;noisy data;ensemble classifier;support vector machines;learning framework
feature space;6;real data;em algorithm
sample selection;probability estimates;training examples;active learning;classification accuracy;jensen-shannon divergence;active learning;jensen-shannon divergence;similarity measure;machine learning;misclassification costs;data-collection;probability estimation;probability distributions
function approximation;policy-gradient;model-free reinforcement learning;reinforcement learning;linear regression;policy gradient;basis function
sparse data;question arises;tree-bank;natural language
web pages;negative examples;information extraction
reinforcement learning algorithm;multi-layer;high quality;model-free;neural network;learning method
network models;monte carlo;markov blanket;markov blanket;network model;markov chain
conditional likelihood;discriminative models;linear discriminant analysis;generative models;generalized linear models;missing data;logistic regression;cost function
pomdp model;partially observable;learning paradigm;model-free;online learning;learned model;learning phase
decision trees;misclassification costs;cost-sensitive learning;medical diagnosis;cost-sensitive;test strategies;case study;decision tree learning;real-world;test examples;total cost
large sample;hidden variables;bayesian inference;statistical inference;em algorithm;latent variable models;special case;maximum likelihood
search methods;control theory;multi-agent systems;test problems;search algorithm for;markov decision processes;finite state;step forward
ensemble learning;decision trees;kernel-based;kernel selection;heterogeneous data;machine learning;missing data;recognition tasks;kernel methods;missing values;excellent performance
regression method;learning task;reinforcement learning;knowledge-based;knowledge learned;knowledge based;learned model;knowledge transfer;support vector;problem solver
large number of;decision-making;machine learning and data mining;distance-based;classification rules
optimization problems;real-world;greedy strategy
discriminant analysis;local optima;linear discriminant analysis;distance-based;logistic regression;performance gains;cost function;classification algorithms;classifier;overfitting problem
random graph;synthetic data;machine learning;noisy data;learning method;classifier;game theory
decision trees;decision tree;expected error;selection techniques;model selection;standard datasets
bayesian network learning;clustering technique;real-world domains;tree-structured;context-specific;bayesian networks;context-specific;conditional probability
predictive models;instance-based learning;instance-based;text data;document collections;natural language
computing resources;training examples;accuracies;update rule;large datasets;margin;classifier;support vectors
term dependencies;hidden markov models;markov chain;hidden markov models;stationary distribution
cross-validation;pattern discovery;heuristic approach
spectral clustering;posterior probabilities;cluster assignment;objective functions;semi-supervised clustering;semi-supervised classification
error rate;learning algorithms;classifier
high-dimensional;approximation algorithms;set covering;labeled training;feature space;performance guarantees;instances;bounded number of;integer programming;np-hard
decision trees;bayesian information criterion;decision tree learner;probability distributions;predictive performance;probability estimation
training set;test set;classification;real world situations;information retrieval;random sample;supervised machine learning;mixture model;text classification;class distribution
covariance functions;computational complexity;regression models;gaussian process
function approximation;genetic algorithms
probability distribution over;partially observable markov decision processes;partially observable markov decision processes;belief update;hidden state;belief state;decision making
planning problem;active learning;finding an optimal;partially observable markov decision process;pomdp model;partially observable markov decision processes;theoretical guarantees
instances;planning domain;plans;real world;machine learning
optimization problems;constraint programming;machine learning;decision support systems;constraint-based;machine learning;classifier
clustering;historical data;approximation error;data matrix;optimization procedure;automatically infer;clustering model;cluster structures
1;learning method
distance measure;large scale;feature construction;feature construction;learning tasks;large space of;case base;learning tasks;learning problems;communication costs
approximation algorithm;estimation problem;euclidean spaces;classification;machine learning
domain knowledge;prediction accuracy;learning algorithms;similarity-based;user behavior;input space;input-output;similarity function;instance;hidden markov models;hidden markov model;similarity-based;graphical user interface
nonnegative matrix factorization;matrix factorization;protein fold;dimensionality reduction methods;classifier;dimensionality reduction
hill-climbing;12;rule learning;multi-relational;search space;concept learning;inductive logic programming;relational data;increasing attention;relational domains
classification model;statistical modeling;classification;large number of;data items;training data;maximum entropy;higher accuracy;association rules;patterns discovered;classifier
ordinal data;0,1;conventional methods;neural networks;classification;problems require;real life;probabilistic model;classification;classification problems
temporal dependencies;independent component analysis;multi-dimensional;subspace analysis;temporal structure
logic programming;inductive logic programming;inductive logic programming;complex applications;machine learning
markov logic networks;stochastic processes;learning models;structure learning;relational structure;learning models;real world
ensemble learning;prediction accuracy;classification techniques;base classifiers;cost-sensitive learning;multiple classifiers
learning agent;hypothesis space;knowledge-based;inductive logic programming;long term;logic program
regression trees;variance reduction;reduction techniques;gradient boosting
weight vector;maximum margin;large margin classifiers;margin;classification
rates;pso) algorithm
unsupervised learning;similarity learning;sparse representations;knowledge discovery in databases;statistical learning;semi-supervised;machine learning;dimensionality reduction;ecml pkdd;transductive learning;knowledge discovery in databases;data mining and machine learning;ensemble learning;graphical models;graph mining;inductive logic programming;application domains;machine learning;tensor analysis;hidden markov models;kernel methods;artificial intelligence
clustering;real world and synthetic datasets;high-dimensional;categorical attributes;clustering approaches;categorical data;hierarchical clustering algorithm;clustering task;categorical data;data mining applications;numerical attributes;learning framework;context-based
clustering;mining closed;mining algorithms;high-quality;classification;data-mining problem;mining frequent;pattern based;complete set of;sequential patterns;application domains;mining frequent;clustering approaches;input sequences;closed sequential patterns;input sequence;long patterns;sequential pattern;feature selection
auc;data points;high dimensional;anomaly detection;large number of;memory-requirement;irrelevant attributes;training sample
simple linear;large networks;traffic data;classification;classification;data partitioning;error rates;machine learning;large networks;error rate;binary classifiers;wide range;high accuracy;spatial distributions;network traffic;machine learning;classifier;vast amounts of
clustering;clustering problem;attribute values;model-based clustering;online social media;synthetic data sets;probabilistic model;online shopping;clustering results;iterative algorithm;multiple types of;cyber-physical systems;heterogeneous information networks;increasingly popular
query efficiency;shortest path;shortest path;road network;road networks;query results;path queries
ad-hoc networks;approximation algorithms;computational complexity;redundant information;information filtering;data items;information flow;sensor networks;social networks;real-world;data dissemination;combinatorial optimization problem;extensive simulations;information networks
false positive;similarity measure;unlike standard;locality-sensitive hashing;candidate pairs;probabilistic guarantees;search problem;efficient indexing;similarity search;locality sensitive hashing;candidate generation;similarity estimation
random walk;recommender systems;sparse matrices;real-world;random walk;theoretical analyses;data structures;link prediction
spam detection;data analysis;streaming model;community mining;biological network;real-world graphs
frequent itemset;valuable knowledge;citation network;real-world;pattern mining;expected values;pruning strategies;attribute sets;statistical significance;correlation patterns;correlated patterns
database administrator;high level;optimization problems;semi-automatic;index tuning;semi-automatic
query evaluation;relational algebra;probabilistic data;knowledge compilation;probabilistic databases;probability distribution;database engine;probability distributions
spatial resolution;process models;image data;fmri data;machine learning;human brain;knowledge discovery;data sets;time series;data mining tools;data mining;scientific discovery;magnetic resonance imaging;machine learning;cognitive processes
case studies;lessons learned;decision support;actionable knowledge;actionable knowledge
personal information
gene expression;data mining;dna microarray data;expression levels;data mining;9;cluster analysis
target language;automatic extraction of;low frequency;large-scale;word pairs;learning method;source language;parallel corpora;rates;low-frequency
clustering;nearest;density-based;kernel-based;input data;cluster analysis;kernel function;clustering methods;data mining;cluster analysis;separability
game theoretic;data mining results;sample size;data mining techniques;sensitive data;privacy preserving data mining;privacy preserving
frequent itemsets;minimum support
clustering;theoretical foundation;black box;vector machine;user-defined;generalization ability;rule extraction from;support vector;support threshold;support vector machines;support vectors
rule discovery;pruning techniques
association rules;real-life;formal concept analysis;databases
data structure;main memory;extraction techniques;compact representation;data structures;partially ordered;association rules
minimum support;candidate itemsets;low support;large number of;frequent itemsets;apriori algorithm;high confidence;support threshold
association rule mining;user-defined;view selection;view selection;accurately detect;classification problems;learning task
level-wise;frequent pattern;data-reduction;frequent pattern mining
mining task;frequent itemset;frequent itemsets;data mining techniques;association rule mining;main memory;compression technique;larger datasets;algorithms require;association rule mining;compression ratio;association rule mining algorithms
association mining;extended abstract;association patterns;association rule mining;search space;candidate itemsets;query sequence;temporal evolution
online algorithms for;online mining;discovering frequent;mining frequent;stream data;sensor networks;sensor network;mining algorithm
mining frequent patterns;mining frequent;mining frequent;data set;data mining research;formal model
prediction problem;prediction methods;probabilistic model;domain knowledge into;crf model;conditional random fields;1;human genome;structural information;high-resolution;conditional random fields
occurrence frequency;multi-dimensional;molecular biology;applications including;position information;sliding window;index structure;indexing method;efficient indexing;exact match;query sequence;dna sequence;query processing
ranking algorithm;query translation;extraction algorithm;extraction task;rule-based;information extraction;automatically discover;text documents;query expansion technique;biomedical data;query expansion;large database
amino acid;feature extraction;large-scale;voting scheme;classifier;nearest-neighbor;protein sequences;huge number of
tree based;false positive;error rates;data mining tools;higher accuracy;breast cancer
sequence data;protein sequences;databases;neural network;data mining
search performance;molecular biology;suffix tree;database;storage space;sequence matching;index structure;post-processing;indexing method;databases;storage overhead
training data;classification;classification;instances;association rule mining;accurate classification;association rule mining;lower cost;association rules
pattern recognition;rough set;recognition accuracy;feature selection;rough set theory;recognition problem;feature selection
genetic algorithms;genetic algorithm;large number of;categorical variables;classification tree;data mining
data mining application;classification models;classification;classification rules;associative classifiers;classification methods;association rule mining;association rules;class labels
evolutionary algorithm;classification problems;neural network;artificial neural network;classifier
transition probability;tree based;privacy-preserving;classification;decision tree;data types;perturbed data;probability distribution;original data;privacy-preserving;data mining;mining algorithm
decision rules;5;classifier;word sense disambiguation
hand-crafted;rule-based;machine learning;combination methods;support vector machines;learning method
query language model;language models;relative entropy;information retrieval;feedback documents;language modeling approach;language model
classification problem;hierarchical text classification;tree-based;tree-structured;data set;text classification;multi-label;artificial intelligence
support vector machines;sentiment classification;text mining techniques;sentiment classification
domain knowledge;learning approaches;data set;rough set theory;artificial data sets;classifier
high quality;query processing;raw data;dense regions
clustering;clustering problem;fast response;real data sets;dynamic nature of;databases;time series;streaming data;produce high-quality;multi-resolution;clustering methods;data streams;time-series;clustering algorithm;clustering approach;nearest neighbors
time-series;clustering;heuristic approaches;total number of;dimensional space
clustering;parameter-free;massive graphs;simulated annealing;large graphs;algorithms require;graph-theoretical
clustering;high-dimensional;arbitrary shape;clustering algorithms;input parameters;databases;clustering algorithm
clustering;learning process;algorithm iteratively;learning algorithm;competitive learning;prior information
hierarchical agglomerative clustering;pruning power;high complexity
data point;clustering;density-based;arbitrary shape;density-based clustering;level set;kernel density estimation;cluster boundaries;level set;density functions
benchmark data;cluster structure;vector field;visualization technique;data mining;dimensional data
data set;real-world;data sets;database
large number of;clustering method;algorithm called;data stream;data stream model;clustering data streams;clustering algorithm based on
clustering;visualization tool;visualization tools;high dimensional;outlier detection;high dimensional data sets;data clustering;data set;data sets;selection techniques;visualization techniques;evolutionary algorithm
clustering;high-dimensional;local sites;feature vector;distance function;clustering algorithms;real-world;data sets;approximation technique;distributed clustering;dimensional data;feature vectors
mining algorithms;international conference on;database;dynamically changing;complete set of;sequential patterns;incremental mining;knowledge discovery;sequence databases;basic properties;data mining;mining algorithm;large database
classification;sample size;association rule mining;image data;random sample;large sample;continuous data;mining tasks;random sampling
clustering results;rough set;equivalence classes;attribute reduction;decision rules;clustering process;incomplete information;decision rule;rough set theory;cluster-based;data mining applications;real world;cluster analysis
incomplete data;learning bayesian networks;evolutionary process;fitness function;minimum description length;solution space;bayesian networks;mining algorithm;algorithm combines
input data;collaborative filtering;collaborative filtering algorithm;fuzzy clustering;clustering algorithm;recommender systems;collaborative filtering
mining results;protein function;data samples;sequence data;low quality;limited bandwidth;data acquisition;markov networks;data model;data model for;posterior probabilities;belief propagation;missing values;data mining methods
face images;face image;local features;high dimensional;classification accuracy;lower dimensional space;face databases;face recognition;low-dimensional;feature selection;image data;class distribution;unstructured data;feature selection
error estimation;progressive sampling;sampling algorithm;sample size;database;mining association rules;real data;association rules;error estimation;synthetic data
classification;classification accuracy;principal components;data mining tasks;time series;data pre-processing;time series;principal components;data mining task;feature subset selection;feature subset selection
principal components;categorical variables;test data;principal component analysis
density-based;tree structure;data cube;data summarization;aggregate query;aggregate queries;tree structure;olap queries;aggregate queries over;density-based
parallel database;frequent patterns;frequent itemsets;database tuning;data mining;frequent itemset mining;data mining;pattern discovery;relational databases
knowledge management
mining algorithms;frequent pattern mining algorithms;frequent patterns;pattern growth;large databases;long patterns;pruning techniques;real world;frequent pattern mining
training data;test data;classification;classification accuracy;class distributions;conditional distribution;machine learning;image database;feature distribution;class labels;uci datasets
prediction accuracy;microarray data;classification;sampling techniques;classification accuracy;adaptive sampling;feature selection technique;classifier
relational algebra;instances;learning algorithm;kernel functions;instance;data sets;relational schema;instance-based;kernel-based
error rate;manifold learning;locally linear embedding;databases;learning algorithm
selection criterion;decision trees;ensemble methods;generalization error;decision tree
training set;learning algorithm;training process;algorithm named;unlabeled examples;edge weight;labeled data;semi-supervised;labeled examples;training algorithm
data point;probabilistic latent semantic analysis;training data;gaussian mixture model;data points;mixture models;gaussian mixture model;overfitting problem;gaussian mixture model;mixture models
computational complexity;capacity control;high computational cost;training samples;worst case;support vector machines;margin
pair-wise;typical patterns;graph-structured data;graph-based;search strategy;machine learning;beam search
inter-related;classification;spam filtering
operator;constraint-based mining;taking into account;search space;large datasets
apriori algorithm;discovering patterns;frequent patterns;data mining
computational efficiency;search algorithm for;real-world;feature selection algorithm;feature selection;feature subset
high utility;large databases;complete set of;association rules mining;databases
query optimization;relational queries;frequent itemset;optimization methods;queries submitted to;knowledge discovery;algorithm called;execution plans;data mining;subexpressions
intrusion detection;detection techniques;anomaly-based;false alarm;theoretical basis for;detection performance;rates;detection rate
storage space;location data;pattern mining
spatial proximity;mining frequent patterns;mining framework;frequent patterns;spatial datasets;spatial features;fp-growth;algorithm called;mining algorithms;database scans;frequent pattern;projection based;frequent pattern mining
data-driven;parameter-free;compression methods;data points;data compression;pattern mining;original data;compression method
distance measure;euclidean distance between;fourier transform;compression technique;time series;1;data mining;distance metric;likelihood ratio

dimension reduction;time series
pair-wise;temporal data;objective function;temporal constraints
time series;synthetic data;real data
clustering;high level;time series;raw data;larger datasets;similarity search;data mining;data mining problems;lower bounds
pre-defined;mining results;temporal sequence;sequence mining;temporal features;pattern discovery
probabilistic model;data mining techniques;detection method;detection methods;anomaly detection
bounded treewidth;kernel based;predictive performance;labeled graphs
clustering;vector space;text documents;text data;clustering process;key words;subspace clustering;real-world;clustering algorithm;feature weighting;feature weights;text clustering
clustering;training set;machine learning problems;kernel-based;similarity measure;text classification;cosine similarity

clustering;cluster structure;high-quality;web news;clustering method;topic discovery;stream data;web news;algorithm produces;clustering algorithm;nearest neighbors
mining algorithms;anchor text;link structure;web pages;low recall;high precision;topic relevance
intermediate data;database;artificial data;mining process;web logs;real data;web logs;access patterns;conventional methods
kernel pca;gaussian kernels;feature space;feature extraction;data set;input space;content based image retrieval;kernel principal component analysis;principal components;dimensionality reduction;content-based image retrieval
mining results;10;web pages;mining frequent;obtained by applying;frequent subtrees;search engine
data structure;dimension reduction;pairwise constraints;data sets;dimensional data;semi-supervised;pairwise constraints;dimension reduction;nearest neighbors;manifold regularization
real world datasets;linear space;linear dimensionality reduction;large scale;data mining tasks;network nodes;knowledge discovery;distributed environments;data mining problems;dimensionality reduction
mining algorithms;map/reduce;cloud computing environment;sequential patterns;frequent sequential patterns;mining algorithm;sequential pattern mining;sequential pattern
hierarchical clustering algorithms;graphics processing units;block size;data mining;hierarchical agglomerative clustering;data mining algorithms
clustering;ontology-based;3, 6
learning algorithm;selection method;microarray data analysis;feature selection) method;data sets;feature selection;support vector machines;gene selection
accurate classifier;classification approach;data-mining;storage space;distributed classification;communication cost;learning environment;distributed classification;distributed data;generalization performance
prediction methods;performance prediction;models built;3;2;prediction models
clustering;feature space;classification;similarity measure;classification accuracy;microarray datasets;similarity measure;high dimensional;microarray datasets;small sample size;high dimension;feature selection methods;feature selection method;data repository;classifier;gene selection
nearest neighbor algorithm;nearest neighbor;cross-validation;classification accuracy;similarity function;learning algorithm;benchmark data sets;algorithm learns;real world;classifier
face recognition;face databases;image feature;feature extraction;vector machine;dfld method;principal components;dfld) method;databases;multi-class;separability
statistical dependence;variable selection;learning algorithm;gaussian processes;higher accuracy;gaussian processes;bayesian framework;learning framework
synthetic data sets;data visualization;optimization algorithm;visualization tool;data sets;optimization techniques;separability
mining algorithms;dense clusters;subgraph mining;dense regions;clustering methods;weighted graphs;clustering quality
citation network;itemset mining;graph mining;social network analysis;large graph;interesting patterns;itemset mining;social networks;data mining;biological network;million edges
relational database management system;breadth-first search;main memory;large graphs;databases;relational databases;mining large graphs
query nodes;random graphs;biological databases;real graphs
sequence patterns;frequent patterns;frequent subgraphs;complete set of;large graph;graph data;frequent pattern mining
clustering;document-cluster;data points;clustering model;clustering process;semi-supervised clustering;semi-supervised;prior knowledge;nonnegative matrix tri-factorization;labeled data
clustering;similar items;data collections;meaningful patterns;databases
clustering;computational complexity;nonnegative matrix tri-factorization;data matrix;document datasets;computationally expensive;dimension reduction;nonnegative matrix tri-factorization;clustering quality
clustering;sheds light on;link structures;web-page;web-pages;density-based clustering algorithm;mining tasks;similarity matrix
clustering;mixture models;dirichlet process mixture;dirichlet process;vice versa
user-generated;semi-structured;digital camera;user queries;retrieval model;product features;query answering;customer reviews
mixture model;blog posts;information retrieval;annotated data;feedback documents;relevance model;opinion retrieval;pseudo-relevance feedback;retrieval task;topic relevance
inductive learning;machine learning;algorithm's performance;transductive learning;sentiment classification;focus primarily on
internet users;weighted average;user-generated content;real data sets;user-generated
data-driven;clustering;classification;classification;analysis reveals;feature sets;textual features;text categorization;machine learning;feature selection;blog posts;pattern discovery;feature set
summaries generated;human generated
real-world datasets;naive bayes;highly competitive;naive bayes;decision tree learning;squared error;learning strategies;data streams;evolving data streams;naive bayes models
high-speed;classification techniques;data points;classification;concept-evolution;data stream classification;human experts;concept-evolution;data stream;data streams;naturally leads to;instances;labeled data;classification error
data analysis;classification;classification accuracy;large number of;data streams;mixture models;stream classification
graph structure;high-quality;potentially infinite;graph-based;frequent itemsets;data stream;data streams;summarization method;data distribution
data mining tasks;storage devices;storage space;data streams;compact representation;anomalous behavior;data stream;data streams;query results;exact answers;historical data;high speed
real datasets;histogram-based;synopsis structures;pattern matching
proximity measures;feature space;cosine measure;real-world datasets;pca based;clustering tasks;data-mining and machine learning;clustering approach;numerous applications in;similarity measures
classification problems;graph kernels
classification problems;classification;target values;class label;similarity learning;learning algorithm;similarity function;instances;learning problem;classification approach;classification task
clustering;information discovery;schema-free;xml keyword search;clustering approaches;xml keyword search;meaningful results;xml documents;distance based;ranking mechanism;user-friendly
real graphs;weighted graph;anomaly detection;user-defined;million nodes;weighted graphs
laplacian matrix;pruning technique;outlier detection;outlier ranking;detection method;random walk on;local outliers;local neighborhood;distance based;direct computation
real-world datasets;fast algorithm;singular vectors;sparse graphs;surprising patterns;large graphs;social graphs;community structure
large graphs;real data sets;social network
ensemble learning;base classifier;decision tree;multi-task;input features;ensemble methods;naive bayes;classifier;training instances;ensemble method;class labels;class attribute
learning process;supervised learning;human learning;training examples;real-world;learning paradigm;learning tasks;machine learning;supervised learning;learning strategy
imbalanced datasets;ensemble framework;sampling methods;random subspaces;data mining
classification accuracy;theoretical analysis;multi-classifier;multi-classifier
real-world datasets;unlabeled data;semi-supervised learning;information contained in;large-scale datasets;vector machine;machine learning;instances;labeled data;margin;semi-supervised;semi-supervised;margin;maximum margin;support vectors
approximate answers;sliding window;continuous queries over data streams;traffic flow;data stream;sensor networks;query processing;evaluating queries;data streams;credit card;sliding windows
database systems;information processing;relational database systems;information processing;natural languages;information retrieval systems;query processing;web search engines;flexible architecture
domain knowledge;structural heterogeneity;mediator systems;query planning;concept-based;structured data from;query interfaces;index structure;web sources;keyword search;query languages
semi-structured data;concept-based;search engine;data mining;semantic relations;structured data
stored data;dna sequences;complex objects;data structures;similarity queries;nearest neighbor queries;scalable distributed;similarity searching;metric space;metric spaces;distributed environment
pattern-based;data mining results;pattern based;distributed applications;distributed environments;structural information;data-intensive
digital content;semantic web;application domains;user request;data files
semantic web;semantic web applications;recommender systems
xml based;prior knowledge
incremental evaluation;spatio-temporal databases;query processors;query processing in;shared execution;continuous spatio-temporal queries
clustering;load distribution;distributed computation;distributed stream processing;data-intensive;long running;stream processing;fall short;data processing;push-based;continuous queries;load distribution;load balancing
xml queries;optimization techniques;query processing and optimization;storage structures
data types;similarity retrieval;similarity search;exact match;metric space;metric spaces;range queries
query optimization;database;large-scale;application domains;access patterns;relational database;data-intensive applications;relational databases
relational schema;database;database systems;database management systems;data model
instances;data warehouses;aggregation queries;data warehouses
data sharing;data sharing;data values;peer data management systems;pose queries;data management system
relevance feedback;ontology-based;highly heterogeneous;search result quality;xml retrieval;data collections;ir techniques;relevance feedback;ranked retrieval;digital libraries;data repositories;query conditions;query refinement;query expansion;global schema
web applications;database technology;database;increasing number of;9;years ago
labeling scheme;update cost;order-preserving;xml data;data representation;labeling schemes;xml databases;xml database systems;query processing;xml query processing
multi-dimensional;xml tree;path expressions;xml data;xml query languages;user query;xml documents;data structures;xml document
structural joins;dynamic range;labeling schemes;data set;update operations;node labels;xml query processing;xml trees
xml document collections;large amounts of;large-scale;xml documents;document collections;application scenarios
memory footprint;optimization approach;xml query;increasing number of;xml queries;xml repositories;statistical method;size estimation
storage space;xml data;semistructured data;huge amounts of;association rules
pruning strategy;query optimization;query execution;optimization approach;index structure;document management;xml transformations
xml-based;xml schema;xml schema
query optimization;multimedia documents;knowledge management;information management;information retrieval;data sources;common interests;databases;distributed sources;transaction processing
access control mechanism;vice versa;role-based;xml-based;control theory;data structures;xml-based;document type definitions;control mechanisms;distributed environment
data exchange;mobile communication;memory space;communication bandwidth;xml-based;data streams;multimedia data
database research;xml data management;xml documents;3;database technologies;databases;document representation
context-aware;higher-level;data model;client applications;data structures;contextual information
relevant information;mobile users
mobile clients;queries posed;information systems;large number of;indexing scheme
context-aware;mobile computing;distributed processing;wide range;mobile agents;distributed architecture;mobile users;location-dependent
user-centric;computational model;location-based services;data volume
load-balancing;wireless network;wireless networks
mobile ad hoc networks;mobile ad hoc networks;mechanism design
ad hoc networks;service discovery
database technology;end users;mobile computing;on-line analytical processing;mobile devices;numerous applications;olap applications
similar queries;small-world;large amounts of data;user query
trust model;query results;design choices;social networks
clustering;simulation-framework;schema-based;simulation framework;schema information;query routing;simulation results
distributed algorithm for;data sharing;database;data exchange;dynamic networks;local database
general purpose;xml data;querying xml data;human intervention;database
keyword search;data placement;distributed applications;selection queries
query answering;data exchange;queries posed;semantic constraints
database systems;large number of;schema) information;semantic web;optimization opportunities;query plans;query routing;data management systems
resource sharing;query processing in;query language;information retrieval;data model for;query processing in;information retrieval;super-peer
hash table;adaptive approach
geographically distributed;spatial data;specifically designed for;spatial objects;tree-based;search space;spatial queries;spatial indexing;distributed environments;space usage;main features;spatial index
clustering;web documents;data protection
clustering;clustering;schema-based;deep web;web sources;query capabilities;generative models;clustering categorical data;databases;source selection;hierarchical agglomerative clustering;hypothesis testing;objective function
clustering;xml documents;structural summaries;clustering;clustering algorithms
computational complexity;web pages;monte carlo;web graph;indexing techniques;similarity functions;link-based;hyperlink structure;similarity measures
decision trees;data mining results;modeling framework;5,10;clustering techniques;server logs;association rules
user preferences;profile-based;irrelevant documents;xml documents;user profiles;taking into account;xml schema
similar queries;query recommendation;related queries;query logs;search process;clustering process;search engines;search engine;query log
clustering;clustering;web pages;access logs;related topics;link structure;web data management;search queries;wide range;search engine results;web sources;search results;web data;web users
vector space;probabilistic logic;information retrieval
clustering;web pages;relevance ranking;query recommendation;web site;search engines;server logs;search engine
random walk;search algorithm;search efficiency
ir systems;document collections;information retrieval systems;application scenarios;information retrieval
result merging;large-scale;federated search;text-based;federated search;resource selection;digital libraries;unique characteristics
world wide web;power law;linear regression;degree distribution;world wide web
data sets;formal framework for;schema mapping;schema matching;general-purpose;probabilistic approach;mapping rules;probability theory
vector space;auxiliary;xml retrieval;xml documents;vector spaces;retrieval quality;structured queries
extraction task;textual descriptions;statistical properties;structural features;database
information filtering;user profile;document filtering;multi-document summarization
ranking algorithm;classification;learning – systems;classification framework;learning algorithms;learning systems;data sets;machine learning;automatic text summarization;ranking algorithms;separability;machine learning techniques;desirable properties;word-clusters;single document summarization;classifier
statistical learning;term frequency
cross lingual information retrieval;retrieval effectiveness;information derived from;information retrieval;document frequency;initial retrieval;retrieved documents;automatically extract
information resources;search effort;mobile devices;user feedback;relevance feedback;relevant document;user actions;information gain
term frequency;relevance assessment;information retrieval;retrieval performance;parameter tuning;term frequency;computational cost;7;trec collections
posterior probabilities;structured documents;structured document retrieval;structured documents;retrieval performance;context-based;algorithm to compute;context-based
logic-based;salient features;question-answering;question-answering
cosine measure;access methods;efficient search;intrinsic dimensionality;indexing method;text collections;precision/recall;query processing
data types;probability theory;retrieval functions;weighting schemes;language models
data fusion;correlation coefficient;data fusion;linear combination;trec 5 ( ad hoc;retrieval results
training set;classification;supervised classification;data sets;document collections;real world applications;classifier
compression methods;classification methods;accurate classification;compression-based;specifically designed to;text classification;compression-based
search task;search terms;concept-based;query formulation;users' queries;information retrieval;search tool;search-tool;search results;query expansion
search performance;collaborative web search;search techniques;search sessions;search logs;search behaviour;term-based

search results;relevance judgments;data obtained from;search engines;search engine;cost-effective;correlation analysis
running times;trec collections;encoding schemes;inverted files;retrieval systems;memory size;data dimensionality;dimensionality reduction
trec9 collection;relevant documents;information retrieval;retrieval models;evaluation measures;ir models;information retrieval systems;high precision
document ranking;statistical measures;query expansion;interactive information retrieval;statistical method;pseudo-relevance feedback;interactive query expansion
target language;search topics;highly relevant documents;dictionary-based;relevance criteria;dictionary-based;source language;text database;cross-language information retrieval
video segmentation;gaussian mixture model;suffix tree;classification accuracy;video data;detection process;hidden markov models;classifier;visual features;video segments
high-dimensional;distance measure;distance metrics;1;distance measures;wide range;visual features;content-based image retrieval
image retrieval;image indexing;retrieval performance;retrieval quality;test collection;high-level;low-level;retrieval systems
web search;web pages;search performance;clickthrough data;web pages;spreading activation;search engine;web search;log data;web search engines
factors affecting;web page;textual content;query terms;web pages
retrieval effectiveness;document structure;ad-hoc retrieval;query operations;web retrieval
web ir;ad-hoc retrieval;large-scale;cross-language retrieval;web collections;information retrieval;document collections;information retrieval;query expansion
text documents;retrieval performance;low cost;search engine
query formulation
network analysis;distributed information retrieval;distributed information retrieval;large number of
data set;location information;scene recognition;information access
cross-lingual;named entity;information retrieval applications
relevant documents;on-line analytical processing;concept hierarchy;xml documents;xml document;query terms
search results;meta-search engine;document collection;relevance models;relevance feedback;search engines;anchor-text;search behavior
low precision;web search;online users;1,2;information retrieval;user queries;search result;search engines;information overload;search sessions;result pages
search-engines;past queries;search interfaces;real-estate;mobile search;related queries;result page;mobile devices;3;1;2;search engines;internet applications;mobile internet;search result
semantic classes;classification;visual concepts;named entities;negative examples;knowledge base;content-based video;bayesian classifier;automatic speech recognition;classifier;video segments
web documents;web pages;scale free;link structure;1,4;information retrieval;language modeling framework
search results;relevance feedback;image representation;information retrieval
content-based video;clustering analysis;concept detection;shot clustering;clustering method
java based;information retrieval;information sources
web pages;web ir;1,5,3;increasing attention;search tools;web pages;location-aware;feature weighting;named entities
fusion method;feature extraction;weighting scheme
simulated annealing;monte carlo;minimization problem;takes into account;simulated annealing;stochastic process;high resolution;aerial images;markov chain
object recognition;data analysis;appearance-based;image segmentation;probability density functions;image analysis;region-based;computational approach
clustering;distance measure;computational complexity;density estimation;probability density function;data clustering;data sets;gradient descent;total number of;cost function;information theoretic
matching score;image noise;linear combination;stereo matching;image matching;ad hoc
markov random field;training data;energy minimization;belief propagation;environmental conditions;color images;mrf model
image segmentation;texture features;real data;machine learning;low computational cost;observation model;em algorithm;semi-supervised;real-valued;combinatorial problems
multiple images;real data;expectation maximization;image data;image sequences;missing data
human brain;maximum-likelihood estimation;image data;magnetic resonance;mathematical model;5;maximum-likelihood
deformable-model;large-scale;belief propagation;deformable-model;medical images;deformable models;object segmentation;segmentation algorithm
image denoising;vice-versa
parameter learning;training data is;image analysis;classification performance;inference mechanism;markov random fields;maximum likelihood;inference techniques
tuning parameters;regularization term;object recognition;numerical experiments;probability distribution over;object model;solution space;relational structure;regularization parameter;subgraph matching;structural similarity
synthetic data sets;classification;real-world;regression models;reduced model;1;decision process;data repository;high order
matching scheme;markov model;label set;linear programming;label space;tracking method;object tracking;linear programming;labeling problems
video sequence;iterative process;combinatorial optimization;image domain;motion estimation;motion models
graphical models;specific information;database;belief propagation;image region;image representation;object categorization;shape model;recognition performance;object classes;salient regions;object categorization;semantic information;real-world scenes
local appearance;appearance-based;shape features;shape model;shape models;multi-scale;automatically constructing;exemplar-based
markov random field;approximate algorithm;synthetic aperture;minimization problem;graph cuts;optimization problem;energy minimization;graph cut;magnetic resonance imaging;objective function;np-hard
12;17;database;geometric information;object classes;object models;adjacency matrix;graph matching
clustering;10;video segmentation;14;clustering algorithms;spatio-temporal;feature-based;clustering process;large data sets;clustering techniques;sample points;image-segmentation;3
image points;feature space;point correspondences;image features;efficient computation;1;feature sets;image matching;matching problem
distance measure;spectral methods;graph matching;laplacian matrix;random walks
face images;image matching;medical data;energy minimization;wavelet analysis;wavelet based
optimization methods;gradient descent;numerical experiments;optimal control;image matching
regularization;variational formulation;object boundaries;key features;color image
image sets;image regions;manifold learning;energy minimization;automated methods;low resolution;manifold structure of;video sequences;low-dimensional
image segmentation;gradient descent;shape priors;image data;instances;energy functional
optical flow;gradient-based;level set;motion segmentation
10;27;20;14;energy minimization;level set;4;computational cost;9;curve evolution;variational formulation
optimization process;image processing
pattern recognition;step-size;classification;dynamic-programming;shape context;shape information;local information;hand-written;local similarity
handle arbitrary;image segmentation;shape priors;level set;shape information;segmentation method;6;prior shape;8
spatio-temporal;gradient descent;level set;variational approach;energy functional;prior shape;pose parameters;local minima
variational methods;image segmentation;level set;image regions;medical images;local minima;object segmentation
classification;svm training;vector machine;machine learning;generalization ability;standard svm;objective function;support vectors
locally linear
face images;optimization algorithm;face databases;appearance models;gradient descent;motion” analysis;cost function
data structure;object recognition;data analysis;linear subspace;feature space;recognition performance;component analysis;cluster structures;object classification;kernel methods;dimensionality reduction
convex optimization problems;convex programming;convex programming;global optimization;matrix factorization;supervised learning
surface normals;neighborhood structure;stereo matching;contextual information;stereo algorithm;similarity measures
original data;image analysis;shape matching;shape analysis
higher-order;high-order
rule based;case-based reasoning;radial basis function;fuzzy clustering;problem solving
similarity assessment;case based reasoning;plans;case based reasoning;domain knowledge;nearest-neighbor;matching process;similarity measures
nearest;pattern recognition;classification;distance function;distance functions;data sets;data transformations;rule based
clustering;instance;improving accuracy;based reasoning;retrieval method
case-based reasoning;business processes;data mining
multimedia data mining;computational framework;feature mapping;clustering process;classification process;visual concepts;feature descriptors;content-based image retrieval
semi-automatically;clustering;object-recognition;similarity measure
databases;high quality;data quality
domain experts;cost-sensitive;data set;real-life;cost-sensitive classification;cost-sensitive
audit data;grid computing;data mining;neural network;data warehouse
clustering;statistics-based;cluster validity
domain knowledge;association rules;association rule mining;association rule;interesting rules;customer behavior;association rules;negative association rules;large database
association mining;supply chain;data mining method;data points;data mining techniques;stock market;time series;instance;apriori algorithm;time-series;association rules;pattern matching;continuous data
clustering;clustering;user sessions;classification;scientific applications;user behaviour;linear complexity;web users
mobile user;high level;data mining approach;simulation results;mobile users
temporal behavior;frequent patterns
classification techniques;classification;database;data mining techniques;model construction;feature selection;data mining;data mining algorithms;prediction models;feature selection technique
classification;tree mining;knowledge based;main idea;data mining;artificial intelligence
data mining tool;database;large volumes of data;web application;relational databases;application server
knowledge management;competence;information technology;knowledge management
knowledge management;knowledge management
clustering;hybrid model;linear classifier;dimensional space;classifier
fuzzy classification;learning rules;classification;classification accuracy;classification tasks;instance;learning vector quantization
linear programming
ant colony algorithm;optimization problems;convergence speed;optimization problem
skewed data;network models;training set size;decision tree;continuous data;decision tree algorithms;skewed;data set;data sets;rates;test set;data mining algorithms
financial data;credit card;iterative optimization;data mining;high frequency;artificial intelligence;transactional data
data mining;data objects;database management systems;data updates;database;multimedia objects;computing environments;serializability;data mining
clustering;data analysis;large-scale;data sets;large data sets;clustering techniques;credit card fraud;contextual information;limited resources;biological data
statistical methods;web-based;bayesian network;posterior probabilities;microarray data;network models;gene regulatory network;bayesian network;expression levels;network model;gene regulatory networks;http://genenet.org/bn;gene expression data;posterior probability
13;data analysis;microarray experiments;large scale;gene expression;microarray experiments
clustering;classification;database;clustering method;data mining approach;data mining;large database
numerical experiments;genetic algorithms;integer programming;capacity planning;large-scale
intrusion detection;multiple criteria;network security;network intrusion detection;quadratic programming;multiple-criteria;quadratic programming;reliable detection;cross-validation;classification results;classification method;detection problem;ensemble method
multiple criteria;neural networks;classification;database;linear programming;classification methods;real-life;behavior analysis;credit card
large amounts of;multiple criteria;sample size;database;financial data;linear programming;data mining;multiple criteria;data mining approach;data mining;credit card;discriminant analysis;decision making
game theory
theoretical analysis;knowledge based;external knowledge
knowledge management;information technology
programming model
small sample;fuzzy set;real-world applications;data set;probability distribution;probability estimation
decision-making;objective functions;multi-objective

text mining;web-based;artificial neural networks;prediction performance
web-based;web-based;decision support system;accurately predict;rates;forecasting accuracy;neural network;user-oriented
selection problem;portfolio selection;heterogeneous data sources;data representation;decision support systems;xml-based;xml-based
database
database content;semi-honest;information sharing
private information;context-aware;dynamic bayesian network;privacy preferences;temporal context
dynamic environments;static data;data quality;anonymization techniques;data privacy;databases
keyword search;security requirements;encrypted data;database
keyword search;retrieve information;encrypted data
clustering;15;database;major components;data set;data sets;privacy preserving;finding clusters;tree construction;databases;vertically partitioned;privacy preserving
graph model;role-based access control
access control rules;xml documents;xml schemas;access controls;access control;conflict resolution;xml databases;fine-grained;query modification;database management system;user query
graph-based;tree-structured;directed acyclic graphs;conflict resolution
base table;3;information leakage;12;database
private information;query result;information disclosure;xpath queries;user query
database security;human body;computing systems;database
classification task;case-based reasoning;incomplete knowledge;similarity measures
feature selection techniques;microarray data;microarray datasets;rank-based;large number of;feature selection;class-specific;classifier;relevant features;feature selection technique
signal processing;similarity measure;dna sequences;drug design;sequence database;similarity searching;biological sequences
clustering;nearest;feature space;probability distribution;strongly correlated;data mining approach;distance based;distribution function
medical databases;large databases;interesting association rules;knowledge discovery in databases;association rules;interesting rules;data mining;association rule discovery;interesting patterns;discovered rules
mining task;scientific discovery;higher-level;knowledge-base;semantic relationships
neural networks;medical data;knowledge discovery;data sets;rule extraction;rule extraction from;evolutionary approach;classification task;benchmark data sets
detection problem;intrusion detection system;instance-based;existing knowledge;network traffic;traffic data;artificial intelligence
content-based filtering;mutual information;spam filtering;text categorization;naïve bayes;document frequency;problem domain;feature selection methods;feature selection process;support vector machines;information gain
browsing behavior;web usage mining
log analysis;data preparation;raw data;web log
cold start;vector space;hierarchical clustering;user profiling;web browsing;information exchange;content management
case-based reasoning;data analysis;plans;data mining processes;complex tasks;data processing;data mining;web usage mining
web search engine;concept hierarchies;data exploration;concept hierarchy;retrieval results;search queries;web mining;search engine results;formal concept analysis;search engine;personal information
knowledge discovery;construction algorithm;redundant information;incremental algorithm;computational complexity
graph structures;frequent itemsets;association rule mining;database;association rule mining;rule extraction;rule extraction from;association rules;optimization techniques
attribute values;ordinal classification;monotonicity constraints;class values;classification;classification trees;web environment;multi-relational;classification methods
local models;classification problems;classification error;classification;accurate models
ranking methods;database;feature ranking;supervised classification;data preparation;feature selection;data mining;storing data
feature maps;data sets;clustering tasks;clustering methods;cluster analysis;neural network
search methods;convex optimization problems;global solution;estimation problem;learning algorithm;optimization problem;gradient based;gradient method;limited number of;data mining;benchmark dataset;local minima
association analysis;customer data;association analysis;data-mining;marketing strategies
accurate models;synthetic datasets;database;data mining;heuristic function;feature selection algorithms;knowledge discovery in databases;inter-dependencies;feature selection;databases;search algorithm;feature selection
clustering algorithm;metric space;feature selection process;market segments;similarity functions
predictive accuracy;feature selection method;data mining applications;domain-knowledge;feature selection;great promise;classification problems;classifier;model selection
data mining techniques;data mining
normal form;valuable knowledge;genetic algorithm;case study;quality measures;fuzzy rules;evolutionary algorithm
evolutionary algorithms;clustering problem;search algorithm for;globally optimal;data clustering;prior knowledge;databases;real world
multi-objective;classification;genetic algorithm;optimization problems;multi-objective;vector machine;fitness function;generalization error;training sets;svm classification;evolutionary algorithm;error bounds
intrusion detection;detection techniques;intrusion detection;machine learning algorithms;data mining;data mining based
discriminant analysis;classification models;data base;cost effective;data mining techniques
latent variable model;traditional collaborative filtering;visual content;collaborative filtering;graphical model;generative model;unified framework;user's preferences;visual features
em) algorithm;database;local maximum;likelihood function;recognition accuracy;hidden markov model;em algorithm;hidden markov model;expectation-maximization
video frames;user's preferences;content delivery;visual content
recognition rate;classification;cross validation;feature extraction;classifier;feature selection;feature analysis;features selected
image analysis;classification;drug design
frequent itemset;mining frequent itemsets;equivalence classes;equivalence class;real datasets;upper bound;frequent itemset mining;frequent itemsets;data stream;compact data structure;data streams;lower bound
search results;search space;search procedure;completeness;database;time series;probabilistic reasoning;pattern classification;main idea;search algorithm

distance measure;probability density function;normal distribution;high dimensional
anomaly detection
service provider;case-based reasoning;information systems;cost-benefit analysis;multi-agent systems;33, 28;goal-directed;knowledge-intensive;artificial intelligence
generalized linear models;kernel density;classification
decision tree learning algorithm;genetic algorithm;decision tree;manufacturing process;data mining;high cost
time series;event detection;network monitoring;graph matching
similar” behavior;database systems;large-scale;sensor-network;user queries;environmental conditions;sensor data;high-level;huge number of;data stream management system
continuous query processing;database systems;incremental evaluation;indexing structures;spatio-temporal databases;frequent updates;spatio-temporal;moving object;concurrent queries;queries issued;moving objects;shared execution;query processing algorithms;spatio-temporal;nearest-neighbor queries
completeness;data model;instance-level;schema information;ad hoc;user interaction
temporal aggregation;image data;temporal aggregates;environmental monitoring;wide range;special characteristics
information spaces;graph-based;web environment;data model;information integration;database integration;multiple sources
highly interactive;xml data;efficient query evaluation;structural summaries;user feedback;xml databases;data structures;user interaction
control mechanisms;peer data management systems;access control mechanism;data management system;access control
dimensional space;database;index structures;traffic monitoring;data model;moving objects;query processing;location-based services;databases;moving objects;moving objects databases;database management system
autonomous systems;graph model;information systems;large-scale
fast approximate;wavelet decomposition;data sets;space constraints;data streams;error metrics;massive data sets
application logic;abstraction level;specific set of
approximate answers;ontology-based;database;information extraction;query processing;information extraction techniques;semi-automatically
relational database systems;xml query processing;relational algebra
join algorithm;structural joins;hash-based;query optimizer;query efficiency;worst-case;intermediate results;query evaluation;input sequences;building blocks for;join algorithms;structural joins;hash-based;structured queries
xml documents;data volume;xml data;intra-query;xml queries;xml repositories;xml databases;query processing;databases
xml databases;xml documents;query evaluation;querying xml documents
xml applications;xml updates;xml query language;database
database;lock;view update;conflict resolution;xml views;concurrency control;relational databases
xml documents;semi-structured documents;xquery expressions;data model;databases
xml format;large quantities of;xml query language;poor performance;databases
query processing strategies;xml data;data outsourcing;cryptographic techniques;data outsourcing;outsourced data;service providers;query processing;security issues;data management;data owner
xml documents;increasing number of;xml databases;materialized views;xml views;security views;information leakage;query translation
target instances;xml schemas;mapping language;data exchange;data management;schema evolution;automatic generation of;key constraints;incomplete data;schema mappings;data integration;source instances
completeness;relational algebra;probabilistic models;probabilistic information;probabilistic databases;query languages;probabilistic database
integer linear programming;tabular data;data acquisition;input data
computational complexity;user specifies;database;data cleaning;conflict resolution;query answers;query answering;desirable properties;relational databases;information loss
database;stable model semantics;query answers;management systems;databases;commercial database;key constraints
conjunctive queries over;databases;specifically tailored;conjunctive queries;query answering;relational dbms
application domain;clinical information;clinical data;data model;graph-based
domain experts;clinical data;context-sensitive;user interface;data analysts;data integration
gene expression analysis;wide range;biological data;data warehouse
instances;description logics;web application;information stored in;data integration;databases;relational databases;knowledge base;data integration
web-based;information systems;workflow management;legacy systems;case study;data entry;takes place;data management
medical records;medical applications;information systems;data model for;privacy-preserving;tag-based;health information
continuous queries over data streams;formal framework for;query optimization;prototype systems;relational operators;data items;monitoring applications;data streams;query languages;continuous queries
growing number of;query execution;optimization opportunities;query semantics;temporal constraints;query processing techniques;query languages;stream queries;continuous queries

query optimization;continuous query;query optimizer;query workload;plans;large number of;transformation rules;data stream management system;data stream systems;continuous queries
conceptual modeling;modeling method;reasoning tasks;formal semantics
multi-valued;reasoning problems;description logic;ontology language
application domain;knowledge-based;description logics;user specifies;reasoning tasks
web interfaces;search engine;web source;5;fully automatic;web sources;web resources;web data
information systems;database;object-relational;sql query;query language;data model;databases
adaptive query processing;optimizer;query processors;join operators;evaluation strategy;query processor;operator
database;graph-based;query language;data model;relational schema;text retrieval;relational database system
database;data types;database schema;query language;data model;language called;query operators;object-oriented;domain-specific;databases
temporal data;query language;data model;temporal databases;extract information from
queries over xml;dynamic nature of;xml databases;replication;databases;data dissemination
update transactions;database;individual queries;intra-query;update operations;excellent performance;database;olap queries;query processing;cost-effective;olap applications;database cluster;replicated data;open-source
query engines;xml data;xml instances
spatial data;global schema;query relaxation;query language;xml-based;similarity-based;query languages;query results;spatial domain;geographic information systems
data objects;data stream management system;relational operators;data model;image data;data sets;basic properties;data streams;query processing techniques;basic concepts;vast amounts of
distributed algorithm for;distributed data sources;multi-source;database;regular path queries;databases;link-information;shortest path;information integration;regular path queries;semistructured data;path queries;data management;communication networks
relational queries;scientific data management;semantic annotations;analysis tasks;scientific workflow;inference algorithms;semantic annotations;scientific datasets
information sharing;access control;ad-hoc;role-based;role-based access control;heterogeneous network
context-aware;ontology based;design principles;modeling approach;case study
dynamic range;fine-grained;human activities;application developers;service discovery;resource constraints;service discovery
continuous queries over data streams;application domain;data stream processing;grid computing;queries over xml;data stream;grid-based;data stream management systems;window-based;data streams;increasing importance
pattern based;user-defined;commercial systems;pattern extraction
users' behavior;retrieval methods;clustering tasks;navigational patterns;personal preferences;mining tasks
data mining;inductive databases;inductive database;general purpose;extracted knowledge;management systems;data mining;database;xml-schema;inductive databases;knowledge discovery;xml-based;derived data;automatically generated;databases;mining patterns;knowledge bases
pattern-based;user preferences;pattern-based;database;ad-hoc;query semantics;keyword-based;information stored in;logical database;databases;query answering;query languages
information systems;programming languages;web page;semantic web applications;web services;recommender) systems;high-level
rdf data;event-condition-action;event-condition-action;web-based;semantic web;semantic web;world wide web
query engines;xml data;query language;business rules;active rules
web applications;takes into account;case study;web applications;users interact with;high-level
semantic web;semantic web;event detection
event-condition-action;web applications;database;xml data;rule-based;protein structures;xml documents;information integration;web services;databases
user behavior;online auctions;strong evidence;online auction;vice-versa;formal description
monitoring applications;pattern detection;event processing;distributed applications;composite events;event correlation;complex event
clickstream data;clustering;web usage mining;concept-based;web log data;web site;usage patterns;customer behavior;usage patterns;web data
information interfaces and presentation;graph structures;frequent subgraph mining;[database management;frequent subgraphs;data mining;visualization tool;case study;real-life;web site;web mining;database applications;graph mining;support measures;graph clustering
recommender systems;user preferences;recommendation systems;collaborative filtering;user models;ontology based;prediction models
nearest neighbor algorithm;sparse data;nearest neighbor;classification;collaborative filtering;svm-based;vector machine;growing rapidly;data sparsity;user profiling;real-life;filtering techniques;supervised learning algorithm;web personalization;classification algorithm
user's interests;data sources;recommender systems
collaborative recommendation;recommender systems;collaborative filtering;attack model
clustering;incremental update;web usage;user experience;browsing behavior;computational costs;real dataset;web logs;web usage;modeling techniques
clustering;community detection;data sets;hardware technology;change detection;clustering techniques;data streams
search results;user profile;web pages;user's interests;ranking methods;individual users;personalized web search;search engine;web search engines
query processing;performance bottlenecks;database;main-memory;database;physical design;business applications;desired properties;scientific databases;decision-making;adaptive indexing;human intervention;indexing methods;data storage
query answering;optimal strategy;privacy guarantee
database architecture;traditional database systems;concurrent queries;wide range
query execution;large numbers of;user ratings;social web;database;high quality;data quality;highly-compressed;training samples;query-driven;missing data;databases;missing values
multi-valued;conflicting information;false positive;generative process;real world datasets;sampling-based;probabilistic graphical model;inference algorithm;supervision;data sources;data integration systems;bayesian approach;automatically infer;data integration
materialized views;multiple users;data-management;mechanism design;true values
social media;synthetic datasets;large-scale;social network;meaningful results;theoretical results;edge weight;real-world entities;user-generated content;blog posts;wide range
query execution;large scale data analysis;large scale;distributed file system;execution model;complex tasks;analysis tasks;intermediate results;query languages;high-level
sensor network;lessons learned;sensor networks;communication protocol;data mining;sensor networks;resource constraints;computing systems
high-speed;false positives;density estimation;dynamic environments;data sets;detection rates;change detection;exploratory data analysis;data streams;kullback-leibler divergence;high probability;high-speed data streams;artificial data sets;data generated from
user-interactions;mining algorithms;mobile communications;stream mining;real-world;data stream mining;sensory data;data mining;visualization techniques;fuzzy logic;decision making
plans;applications including;model parameters;anomalous behavior;contextual information;traffic control;plan recognition
intrusion detection;intrusion detection system;network security;frequent patterns;intrusion detection;association rule mining;false alarms;frequent itemsets;apriori algorithm;training data;data mining;association rule;wireless network;wireless networks;detection rate
probabilistic modeling;unsupervised learning;large-scale;real-world;time series;data set;data sets;missing data;data mining;case-study
data mining techniques;outlier detection;spatio-temporal;movement patterns;1;algorithm called;spatial outliers;temporal data;missing values;spatio-temporal
clustering;perform inference;feature extraction;large-scale;semi-supervised learning;large-scale;large-scale;data dimensionality;inference techniques
discovered patterns;context information;data source;temporal information;data collected;kdd community;sensor data;periodic patterns;mining framework
predictive models;sensor data is;real-world;mobile devices;time series;case study;sensor network;data mining;visualization techniques;human-eye
historical data;mixture model;incoming data;sensor data is;predictive modeling;detection approach;massive volumes;principal component;security applications;chi-square;benchmark datasets
spatial locations;temporal intervals;outlier detection;real life datasets;time series;building blocks for;knowledge discovery;real life;sensor data;network data
web search;web search;search engine
data-rich;large-scale;temporal data;data mining
clustering;game theoretic;classification;game theoretic;social network analysis;influential nodes;knowledge discovery;social networks;problem solving;data mining;game theoretic approach;game theory
clustering;distance measure;partitional clustering;similarity measure;data representation;data clustering;real-valued;distance measures;clustering model
cluster structure;highly complex;visual analysis;real-world data sets;major limitation;path-based;distance transform
real-world datasets;automatically determines;correlation clustering;clustering approaches;correlation clusters;correlation clustering;dimensional data;noise levels
clustering;minimum description length;parameter-free;categorical attributes;information theory;heterogeneous data;input parameters;large data sets;iterative algorithm;data mining;real world;clustering approach
clustering;data transformation;gene expression;scaling factors;clustering accuracy;data matrix;data transformations;quality measure
clustering problem;distance function;pattern based;clustering result;social networks;clustering methods;memory utilization;cluster analysis;objective function
synthetic data;closed patterns;social interactions;pattern mining;case studies;social networks
network analysis;clustering;real networks;hierarchical clustering;local patterns;social networks;temporal evolution
sampling strategies;large social networks;real-world;social networks;sampling technique
greedy strategy;social network analysis;influential nodes;computation cost;algorithm called;social networks;monte-carlo simulation;efficient optimization
misclassification costs;expected cost;classifier performance;cost function;multi-class;classifier
pattern-based;frequent itemset;class label;emerging patterns;instance;emerging patterns;classifier;classification algorithms
optimization technique;large scale datasets;generalization performance;noisy datasets;vector machine;margin based;computational resources;overfitting problem;large datasets;very large datasets;margins;margin
nearest neighbor classification;nearest neighbor;learning algorithm;query point;machine learning;classification results;metric learning;databases;distance metric;feature relevance;nearest neighbor rule;conditional probabilities;classifier
support vector machines;classifier;classification;application domain;support threshold;supervised classification;emerging patterns;accurate classifiers;higher accuracy than;numerical attributes;classification results;mining algorithm;high sensitivity;nearest neighbors;class prediction
benchmark dataset;frequent itemsets;emerging patterns;business activities
transaction data;data mining tools;databases;incur high;real world;information loss
data set;privacy requirements;personal information;rating data
semi-honest;latent dirichlet allocation;privacy-preserving;gibbs sampling;multi-party;latent dirichlet allocation
privacy-preserving
computational complexity;data analysis;data publication;privacy preserving;distance-based;theoretical analysis;data utility
clustering;domain experts;false positives;change analysis;spatial datasets;similarity assessment;clustering techniques;cluster quality;clustering framework;clustering approach;clustering algorithms
unsupervised algorithm;temporal information;spatial information;trajectory data;moving objects;location-aware;real world
applications including;click-stream;traffic monitoring;time series;time series;data processing;similarity-based;feature representation
clustering;concept discovery;classification;large quantities of;data acquisition;high-order;medical imaging;tensor-based;complex data;spatiotemporal data;clustering approach
spatial clustering;obstacles constraints;spatial clustering;spatial clustering;obstacles constraints;spatial data mining
search space;genetic algorithm;fitness function;pattern mining;sequential patterns;huge amounts of;data mining research;pruning method;support threshold;sequential pattern mining
association rule mining;uniform distribution;association rule mining;data mining task;weighting scheme;weighted association rule mining;transaction database
text mining;reference model;multi-stream;sequential patterns;topic detection;sequential pattern mining
frequent patterns;mining association rules;interesting patterns;interestingness measure;real-life;high probability;problem setting;data mining;association rules;occur frequently;long sequences;association rule
mining closed;pruning techniques;breadth-first search;mining closed;frequent episodes;event sequences;result set;mining algorithm
intrusion detection systems;data stored;chi-square;databases;diverse applications;normal behavior;approximation ratio
source code;recommender systems;class values;probabilistic models;efficient algorithms to;prediction tasks;human actions;multiple agents;real world;user modeling;open-source
user profile;recommender systems;recommender systems;cold-start;rule sets;user profiles;association rules
recommender systems;real-life datasets;graph-based;valuable information;social tagging;cold-start;user profiles;semi-supervised;perform poorly;semi-supervised;information source
document pairs;ranked list;cost-sensitive;information retrieval;cost-sensitive;discounted cumulative gain;upper bound
background information;sentiment analysis;question answering
question answering;data sets;diverse set of;long-term;ranking model;random walks
multi-level;community-based;document level;weighting model;tf-idf;stopwords;document frequency;retrieval task;term frequency;question answering
synthetic data sets;latent dirichlet allocation;classification accuracy;real-world;matrix factorization;optimal number of;latent dirichlet allocation
lower-dimensional;regression problems;text documents;semantic representations;upper bound;optimization algorithm;topic model;maximum-margin;max-margin;text classification;classification task;low-dimensional;relative accuracy
required information;uncertain information;specific information;database;limited resources;feature values;resource-constraints;probabilistic approach;computational resources;resource-bounded;information extraction;missing values;search interface
database;text search;deep web;deep web;reinforcement learning;web crawlers;search engines;query keywords;web crawling;deep web
text corpus;matrix factorization;decomposition method;summarization method
training data;naïve;data mining results;classification;data uncertainty;uncertain data;training process;data acquisition;classifier;knowledge learned;probability distribution;optimization technique;neural networks;optimization approach;real-world;linear classification;data distribution;neural network;data integration;information loss
data mining;clustering;data mining methods;classification;database;outlier detection;data exploration;similarity measure;analysis tasks;skyline operator;monte-carlo sampling;algorithms for computing;skyline objects
multi-dimensional;dimensional space;synthetic datasets;multi-source;evaluation model;multi-source;skyline queries;queries efficiently;multiple sources;multidimensional space
frequent itemset;mining algorithms;synthetic data;mining frequent itemsets from;uncertain data;pattern mining
attribute values;classifier ensemble;class values;real-life applications;uncertain data;data stream classification;classification;streaming data;data set;real-life;classifier ensemble;uncertain data streams;stream classification
enterprise applications;information sources
business process;service oriented
object model;rule-based;business process;service-oriented;rule-sets;internet applications;application development
knowledge sharing;database;large enterprises;fault-tolerance;enterprise-wide;enterprise applications;enterprise-wide;replication;data replication;data warehousing;meta data;enterprise application;update-intensive;replicated data;online transaction processing
web applications;web browsers;business logic;web applications;enterprise systems;model called

durability;distributed databases;data base;databases;atomicity;replicated data
data set;extraction process;data warehouse
business applications;oltp systems;aggregation functions;data cube;olap applications
data stream;security issues;data stream management system;data stream management systems
multi-dimensional;multi-user;database systems;access methods;database applications;large sets of;data transfer;rates;multidimensional data
generation algorithm;data analysis tasks;naïve;large scale;query language;decision-tree
clustering;clustering algorithms;computational complexity;centroid based;uncertain data management;uncertain data;clustering uncertain data;clustering performance;partitional clustering;uncertain objects;random variable;clustering uncertain data;objective function
sequential nature;large-scale;processing algorithms;real-world;massive data;optimum solution
web-based;access paths;data-source;relational structures;lower-level;database;temporal logic;static analysis;access path;access patterns;relational database
log records;data structure called;data access;database
update transactions;lock;index structures;query processing;concurrency control;concurrency control;adaptive indexing;concurrent queries;adaptive indexing
synthetic data;pattern queries;pre-filtering;directed acyclic graphs;underlying assumption;holistic twig join;pattern matching;stack-based
structured data;structured information;web-scale;information extraction
information retrieval;xml retrieval;information retrieval
web scale;enterprise search;information retrieval research;web search;query-independent;ranking algorithms;search engines;web search engines;search logs
parameter-free;language models;language modelling;information retrieval;bayesian approach;term frequency;large collections;excellent performance
similarity measure;optimal matching;document similarity
user-item;user preferences;collaborative filtering;tf×idf;data sparsity;data set;relevance model;text retrieval;probability ranking principle;formal model
algorithm produces;search term;text collections;competing methods
storage cost;auxiliary;query efficiency;index structures;index structure;text database
large collections;document content;text retrieval systems;relevant documents;term-frequency;retrieval performance;document frequency;term frequency;length normalization;information retrieval systems;query terms;trec collections
vector space;information spaces;domain model;information retrieval methods;social network;information retrieval;social activities;search effectiveness;social networks
evaluation methodology;search result;result page;search engine
data analysis;query performance;text collections;text collection;text retrieval;document collections;query results
singular value decomposition;relevant documents;ad-hoc;selection method;sentence retrieval;retrieval model;identification method;latent semantic indexing;retrieval methods;sentence retrieval
web environment;news sources;online news;news event;web news
clustering;web documents;detection algorithm;topic detection;temporal evolution;online news
clustering;search engine;mobile devices;information retrieval
search results;clustering;rough set;suffix tree;document summaries;outlier detection;label quality;search engine;clustering;clustering algorithms
web documents;simple heuristics;naive bayes classifier;distinctive features;machine-learning;naive bayes classifier;training corpus
large amounts of;trec data;low quality;real users;relevance feedback;highly relevant;feedback documents;pseudo-relevance feedback;retrieved documents;user modeling
relevance feedback;final ranking;statistical analysis
pseudo-relevance feedback;information retrieval;lower computational cost;language modelling;information retrieval
hybrid approach;text retrieval systems;index maintenance;inverted lists;index construction;information retrieval systems
web graphs;parallel computation;web crawling;large-scale
decision-theoretic framework;query routing;selection” problem;network topology;selection method
clustering;machine learning methods;classification;generalization performance;machine learning applications;web contents;decision model;topic hierarchies
web page;web search;retrieval effectiveness;query term;probabilistic model;query sets;search tasks;meaningful information;document retrieval;web search;web page;navigational queries;query terms
query intent;web search;user sessions;named entities;user behavior;web searches;query log;search engine
search results;web-based;web pages;document summaries;search experience;relevance assessment;information retrieval;search result;ir) systems;ir systems;retrieved documents;positive effect
structured document retrieval;xml retrieval;large scale;query formulation;query language;keyword-based;document structure;relevant document;structured document retrieval
relevance information;xml retrieval;xml queries;text-based;keyword-based;relevance feedback;keyword-based;retrieval quality;search engine;keyword query;retrieve information;xml collections
ranking algorithm;machine learning;structured documents;ranked list;training set;information retrieval;relevance judgments;retrieval task;information retrieval;structured information
segmentation methods;query-independent;scientific literature;digital library;search engine;relevant information
image retrieval;clustering;user studies;location data
search results;production process;query formulation;large collections of;motion capture data;ad-hoc;user interface;efficient retrieval;databases;information retrieval;similarity measures
image retrieval;search results;query formulation;relevance feedback;search experience;search tasks
semantically related;search techniques;indexing techniques;search engines;latent semantic analysis;search technique
document collection;cross-language;large-scale;document retrieval;main findings;search engines;information retrieval techniques;digital libraries;queries involving;cultural heritage;retrieval effectiveness
data-driven;data set;nearest-neighbor classifier;nearest-neighbors;multiple features;high quality;parallel corpus;classifier;small scale;manually annotated;parallel corpora;natural language processing;web page;cross-language information retrieval
similar sequences;clustering;protein sequence
semantic smoothing;document ranking;specific information;language model;average precision;concept-based;translation model;language models;language modeling approach;pseudo-relevance feedback;long term;query expansion
relevance criteria;search behaviour
parameter settings;classification model;classifier
long-term;search engines;retrieval model;information retrieval
relevant documents;information retrieval;query words;retrieval effectiveness;test collection
hierarchical classifier;multimedia retrieval;long term;high-level;audio features;decision making
speech recognition;image features;semantic features;image-based;object-based;text-based;5;6;video retrieval;retrieval techniques;low-level
matching algorithms;retrieval effectiveness;language model;efficient computation;language model;computationally expensive;high cost;vector space model
similar queries;collaborative web search;collaborative web search;community members;meta-search;web search engines
information retrieval;classification;ir effectiveness
distance measure;test collections;ir effectiveness;relevance judgments;distance measure
clustering;clustering algorithms;training set;classification;sparse matrices;domain knowledge;clustering algorithm;clustering
web-based;qa systems;classification approach;question answering;question answering systems
rule-based;string matching;pattern matching
dimensionality reduction technique;class information;classification;classification accuracy;information retrieval applications;document matrix;training documents;class labels;latent semantic indexing;support vector machines;latent semantic indexing
data source;web-based;question answering
search results;ranking method;information retrieval
document content;readability;1;2;information retrieval;readability
training data;linear combination;feature weights;video retrieval;image clustering;human intervention;visual features;content-based image retrieval
image search;databases
clustering;clustering;provide evidence;clustering methods
text classification;classification
relational database systems;information retrieval;1;retrieval models;databases;information retrieval
geographic information retrieval
rank aggregation;rank aggregation;score distributions;trec collections;score-based
information retrieval;personal information;long-term;support vector machines;classifier;context-based
search engines;video search;relevance feedback;search engines;video retrieval;optimal number of
closed world;text documents;detection methods;reference collection
information retrieval applications;information retrieval systems;search process
relevance feedback;relevance feedback;test collections
term weights;parameter-free;context-specific;structured documents;structured document retrieval;context-specific;ranking functions
human behavior;extracted information;survey data;instances;hidden markov models;mobile phone
user preferences;user behavior;user interactions;learning method;ranking method
social media;retrieval results;classification framework;real-world;information contained in;multiple attributes;topic model;topic model;classification results;blog classification;blog data
clustering;clustering;user profile;vector space;web usage mining;indexing approach;web caching;user requests;access patterns;user interests;web applications;user profiles;latent factors;clustering approach;vector space model;web log
stochastic model;real-world;text streams;social networks;blog posts;real world
web pages;semantic graph;automatically extract;visual feature;web page;knowledge base
high utility;high utility;sequential pattern mining;online shopping;sequential patterns;behavior patterns;algorithm called;data mining;sequential pattern mining;excellent performance;mobile users
complex network;efficient representation;large-scale;community detection;topological structure;complex networks;data-sets;community structure
prediction accuracy;evolving data;online social networks;large scale;complex networks;network structure;link prediction;graph theory;temporal evolution;link prediction;prediction algorithms;average precision;information networks
privacy preserving;data analysis;real data;data publication;privacy guarantees
frequent itemset;occurrence frequency;support threshold;frequent itemset mining;transactional databases;interesting patterns;concise representation;frequent itemsets;data mining;association rule discovery;memory consumption
distance functions;distance function;similarity measure;computation cost;similarity measure;time series;data mining;visualization techniques;high dimensionality;sparse matrix
clustering;large databases;feature extraction technique;clustering method;time series;multi-resolution;clustering framework;databases;clustering algorithm;clustering quality
clustering;multiple datasets;cluster structures
clustering;real-life datasets;real-life;data stream clustering;time-series;visual exploration of
similarity information;object classes;distance function;correlation coefficient;class label;correlation coefficients;graphical representation of;ranking quality
clustering;cross validation;cluster validity;clustering result;instance;5;supervised learning algorithms;quality measures;supervised learning
clustering;textual data;database;clustering result;complex data;clustering quality
data objects;data structure;data analysis;semi-structured data;data mining techniques;structure preserving;structural properties;tree-structured data;tree-structured;instances;discovering association rules;tree structured;structural information;real world;structure-preserving
world wide web;user experiences;document detection;digital library;digital libraries;huge number of
statistical methods;association rule mining;large number of;mining process;interesting association rules;evolutionary computation;association rules;search engine;evolutionary approach;human beings
frequent patterns;database;pattern-growth;transactional databases;computationally expensive;transactional database;patterns discovered;frequent pattern
sequence data;frequent episodes;frequent pattern;minimum support
induction algorithm;decision tree;uniform distribution;probability distribution;knowledge discovery;entropy based
learning process;sampling techniques;hypothesis space;real-world;objective functions;prediction error;classification problems;classifier
fold cross-validation;svm-based;vector machine;global features;svm) based;classifier
clustering;optimization technique;web usage mining;outlier detection;data clustering;web usage;data mining process
association rule mining;weighted association rule mining;uniform distribution;weighted association rule mining;association rule mining;data mining task;weighted association rule mining;transaction database
clustering;classification;class information;feature set;unsupervised learning;unsupervised feature selection;learning algorithms;feature selection algorithm;feature selection;uci datasets;error rate;supervised learning;machine learning and data mining;dimensionality reduction

feature selection algorithm;genetic algorithms;vast number of;relies heavily on;feature subsets;computationally expensive;data mining;pre-processing;feature selection
interestingness measure;computational complexity;interestingness measures;database
gene-expression;biclustering algorithms;feature representation;microarray analysis;learning problem
mobile phone
learning process;mixture model;classification task;real-life;maximum entropy;em algorithm;maximum likelihood;mixture models
planning problems;estimation method;synthetic datasets;planning problem;multi-criteria;breast cancer
knowledge sources;naïve bayes;machine learning;knowledge extraction;identification problem;problem solving;natural language processing;question-answering;question answering;automatically extract
age estimation;data points;gaussian process;regression function;database;cross validation;kernel function;age estimation;regression methods
ranking list;social network;real-world applications;social networks;ranking model;learning method
majority voting;ensemble methods;application domains;data sets;parallel structure;classification algorithms
data miners;black-box;data mining methods;data mining techniques;predictive modeling;data set;powerful tools for;training samples;comprehensibility;data mining;accurate predictions;preference data;learning framework;decision-making
multiple classes;cost-sensitive learning;binary class;multi-class problems;real-world applications
object recognition;large datasets;large-scale;level features;depth images;linear svms;feature sets;recognition tasks
high-dimensional;multiple images;camera parameters;convex optimization;low-rank;intrinsic parameters;rank minimization;low-level features;wide range;real images;principal component;multiple image
dynamic programming approach;inference procedure;search space;object detection;pascal voc
viewing conditions;light source;pose estimation;real images;invariant features
search space;tracking framework;tracking results;training set;regularization;general case;markov random field;tracking applications;matching cost;computationally intractable
pre-processing;greedy algorithms;global solution;input data;globally-optimal;greedy algorithm;tracking algorithm;cost function;shortest path;dynamic programming;greedy algorithms;video sequences;object tracking
margin;boosting algorithm;boosting algorithms;random variables;loss function
computational complexity;image segmentation;unsupervised learning;multi-scale;image processing;segmentation quality;numerous applications in;color image
dual decomposition;exact inference;human motion;combination methods;tree structure;pose estimation;temporal features;learning problems;optical flow;motion cues;human pose estimation;approximate inference
optimization techniques;optimization framework;optimization approach;real data;quadratic program;model fitting;objective function
real-world data sets;multi-view;point cloud;multi-view
human brain;weighted graph;input image;patch-based;human brain;segmentation method;image denoising;mr images;label propagation
medical imaging;human perception;global structure;low-level
local image features;local features;multiple labels;spatial information;linear svm;shape context;object detection;image classification;classifier
clustering;training data;appearance-based;learned models;large scale;large number of;manually labeled;negative examples;hierarchical clustering;automatically selecting;training samples;training sets
database;features extracted from;image space;large number of;recognition rates;matching problem;low-dimensional
accuracies
large amounts of;object recognition;training data;pascal voc;closed-world
matching scheme;markov random field;training images;nearest neighbor;image database;test image;retrieved images;integrating multiple;similar images;similarity based
bayesian model;mixture components;gaussian mixture model;image segmentation;edge-preserving;variational inference;mixture model;data base;dirichlet prior
multi-modal;face recognition;low-resolution;linear subspace;data set;highly correlated;feature selection;pose variations;recognition rates
large-scale;object recognition;active learning;active learning;large-scale;instances;training sets;linear classifiers;object detectors
recognition accuracy;face recognition;face recognition;image pairs;naive bayes classifier;face database
spectral clustering;efficient retrieval;large-scale;image collections;canonical correlation analysis;multi-class

sparse reconstruction;outlier detection;spatio-temporal;selection method;event detection;basis set;image sequence;benchmark datasets;sparse reconstruction
search space;recognition algorithm;optimization algorithm;composite events;globally optimal solution;event recognition;composite event
image retrieval;semantic indexing;taking into account;semantic labels;large scale;similarity learning;large-scale datasets;prior knowledge;computational cost;special case;additional information
video segmentation;feature set;temporal information;general-purpose;reconstruction algorithms;scene points;feature selection;high probability;classifier;visual features
nearest;data sets;search accuracy;vision applications;large scale;similarity search;machine learning;similarity function;similarity search;web images
gradient-based;markov random field;loss function;optimization methods;markov random field;potential functions;image denoising;learning method;mrf model;image restoration;model parameters
cost functions;bayesian methods;regularization;spatially varying;cost function;real images
real world;cost functions;graphical model;inference problem;labeling problem
search space
ground truth;million images;feature detection;mobile computing;complementary information;evaluation scheme;mobile devices;rates;image data
point features;modeling method
computational efficiency;extraction techniques;faster convergence;computationally expensive;deformable model;maximum likelihood;object segmentation;conjugate-gradient
face recognition;markov random field model;recognition rates;feature selection;discriminative information;classifier
noise level;real-world;theoretical analysis;bayesian approach;imaging conditions;video sequences;noise levels;motion models
process model;energy function;extracted features;multi-view;probabilistic approach;optimization process;observation model;pixel-level;camera views
viewing conditions;search engine queries;data set;optimization criterion;soft constraint;probability model;maximum likelihood;integer programming;scene text
diverse set of;camera parameters;feature extraction;invariant features;performance degradation;input images;databases;image classification;sparse codes;classifier
domain adaptation;feature types;object recognition;domain adaptation;real-world applications;linear transformations;object models;kernel space;imaging conditions;theoretical result;supervised learning
long-term;appearance variations;image noise;image set;motion segmentation
high-quality;synthetic and real images;image acquisition;image pair;blurred image;image restoration;image alignment
high-quality;globally optimal;local minimum;data normalization;estimation problem;global optimality;global optimization;locally optimal;globally optimal solution
data-driven;data-driven;training data;latent variables;attribute-based;human actions;human action recognition;intra-class;semantic concepts;training samples;unified framework;high-level;information theoretic
learning process;training data;label information;feature points;category recognition;reconstruction error;algorithm learns;linear classifier;class labels;sparse codes;objective function;classification error
score function;object detection;score functions
clustering;clustering problem;video sequence;score function;linear programming;multiple image;closely-related;contour-based;lighting conditions;data-set;assignment problem;contour-based
facial images;classification;database;cost-sensitive;age estimation;algorithm called
shape similarity;graph structure;input sequences;databases
data set;amazon mechanical turk;probabilistic framework;image enhancement;collaborative filtering
multiple cameras;probabilistic framework;expectation maximization algorithm;real data
multi-class;detection accuracy;detection algorithm;object classes;object detection;linear complexity;single-class;object detectors
clustering;instances;predictive models;unlabeled images;complex objects
video sequence;video stream;convex optimization;event detection;surveillance video;fitness;real world;sparse reconstruction
high frequency;additional constraints;camera motions;linear programming;user interaction
object recognition;scale-space;image matching;local regions;region-based;weakly supervised;object boundaries;recognition tasks;sampling strategy;nearest neighbor;feature matching;local region
clustering;training data;object category;nearest-neighbor;typically performed;data sets;image regions;pascal voc;structural svm;unlabeled data;multiple kernel learning;similarity metric
point-correspondences;high-resolution;range data
real-world;image data sets;mathematical models
image database;image content;visually similar;image indexing;input image;image domain;local descriptors
operator;real data;multi-perspective
regression analysis;search space;low dimensional
large amounts of;linear classifier;unlabeled data;classification;large-scale;image datasets;semi-supervised learning;gaussian kernel;labeled data;visual learning
fast marching;tree structures;starting points;graph-based;region-based;tree structure;automatically detected
linear model;linear combination;digital images;image processing;independent component analysis
image sets;multiple images;multiple views;multi-view;recognition tasks;single-view;region features;retrieval applications
camera motion;estimation algorithms;scene points;multi-view
markov random field;classification techniques;classification;activity recognition;human activities;spatio-temporal
energy function;image segmentation;interactive segmentation;conventional techniques;global consistency;matching process
mixture components;object classes;training examples;high degree of;model offers;training set;training data;pascal voc;object localization;energy function
camera motion;optimization process;partial knowledge;object tracking;refinement process
ct images;statistical properties;regression method;image segmentation;probabilistic segmentation;high reliability;excellent performance;feature set;object detection;medical images;regression model;supervised learning;multiple instance;class-specific;segmentation algorithm
video sequence;spatio-temporal;obtained by combining;video frame;finding similar;dynamic events
optical flow;search space;scene flow;scene flow;image pairs
face images;illumination conditions;low-resolution;high resolution images;high quality;matching algorithms;facial images;images captured;poor quality;classifier
multiple instance learning;supervision;object instances;segmentation method;object models;object-level;supervised learning
temporal coherence;structural consistency;target image;filtering algorithm;structural constraints
image features;image based;vision applications;learning framework;natural image;benchmark dataset;image quality;reference image;quality measure;low-level
high-dimensional;semidefinite programming;input data;distance metric learning;learning problems;maximum variance;vision problems;high complexity;metric learning;learning problem
image retrieval;image retrieval;memory usage;large-scale;large scale;spatial information;post-processing;retrieved images;long-range;visual phrases;computationally expensive;databases;retrieval accuracy
image patches;specifically designed for;local patches;local features;feature point;arbitrarily large
image retrieval;object recognition;similar objects;features extracted from;similarity" measure;instance;specific task;small sets of
human body;spatial distributions;color images;forensic analysis;personal identification
large amounts of;ground truth;point correspondences;real data;shape recovery;input images;weighted sum of
face images;illumination conditions;low frequency;scale invariant;feature extraction;face recognition;natural images;randomly generated;high frequency;invariant features;natural images;recognition rate
intrinsic image
structured output;object model;16;image understanding;pascal voc;3, 6;detection results;explicitly models;object detection
local appearance;background model;local regions;multi-scale;multi-view;spatio-temporal;learning algorithm;multiple kernels;temporal context;recognition accuracy;human action recognition;action recognition;multiple kernel learning;classifier;local region
clustering;histogram based;scale invariant feature transform;pattern based;highly variable;random sample;prior art;prior art;document image
markov random field model;expression patterns;gene expression data;hamming space;image-based;image registration;brain images;machine learning;landmark-based;higher-order;high-throughput;local descriptors;deformable model;segmentation methods;pre-processing;gene expression;geometric constraints
feature space;hash functions;random projections;memory usage;hash function;index structures;computation cost;training samples;kernel space;maximum margin;large margin classifiers;maximum margin;data distribution
face images;image patches;probabilistic models;object parts;generative models;natural images;deep belief;statistical learning;pixel-level
local context;visual phrases;visual phrases;multi-class;object detectors;object categories;training sets
matrix factorization;regularization;numerical experiments;dictionary learning;partially observable;structured sparsity;desirable properties;learning method
markov random field;graph structure;theoretical properties;class label;markov networks;global constraints;np-hard
binary classification;classification tasks;optimization problem;data set;object detection;worst case;logistic regression;performance gain;rates;object detector;classifier
computation cost;real-world;similarity metric;low-rank;boosting algorithm;pair-wise constraints;metric learning;distance metric;benchmark datasets;content-based image retrieval
video data;depth ordering;video sequences;optical flow;motion cues;computational models;average precision;boundary detection
clustering;optimization framework;data points;minimization problem;multiple subspaces;low-rank;data matrix;noisy data;closed form solution;operator;closed-form solution
data-driven;human interactions;scene geometry;motion capture data;single image;scene understanding
adaptive strategy;graph-cut;energy minimization;object segmentation
detection accuracy;human subjects;feature detection
level features;object recognition;multiple datasets;inter-class;relevant attributes
inference problem;vision applications;minimization problem;inference procedure;reduction techniques;high-order;high-order;algorithm called;markov random fields;real-world;markov random fields;search problem
video sequence;video stream;target image;partial occlusion;local descriptors;low-level
point correspondences;moving object;closed-form solution;video surveillance;closed form solution;eigenvalue problem
image patches;np-complete;particle filter;label assignment;graphical model;instance;loopy belief propagation;particle filter;sampling framework;hidden states
mathematical analysis;expected error;target object;high-frequency
tracking methods;high-quality;feature space;low resolution;closed-form solution;high resolution;motion estimation;computational cost;visual appearance
high-dimensional;class membership;supervised methods;temporal coherence;similar images
matching score;classification;hidden variables;basis functions;generative models;image pairs;joint distribution;generative model;input images;task-specific
incremental algorithms;markov random field;large-scale;scale poorly;local minima;continuous optimization
object classes;classification;large-scale;large scale;data set;visual recognition;knowledge transfer;similarity based
probabilistic framework;estimation method;object class;visual words;recognition tasks;theoretical analysis;distribution function;margin-based;estimation methods
real world datasets;globally optimal
object detector;image segmentation;graph cut
body parts;pascal voc;action recognition;classification;action recognition
local structures;training set;low-resolution;high-quality;gaussian process regression;information contained in;soft clustering;image super-resolution;high-resolution
discriminative learning;ad-hoc;medical imaging;bayesian framework;classifier;low-level
selection criteria;large amounts of;computational complexity;human effort;unlabeled data;data points;active learning;active learning;biometric recognition;batch mode active learning;data instances;real-world applications;instances;real world;classifier;unlabeled set
automatically learns;tracking performance;distance metric;motion estimation;video-based;tracking method;object tracking
long-term;temporal information;statistical models;conditional random field;probabilistic model
graph partitioning;tracking methods;video segmentation;body parts;temporal context;graph nodes;connected components;motion information;motion information
local appearance;class label;quadratic programming;cost function;contextual information;recognition problem
feature space;decision trees;image patches;semantically meaningful;activity recognition;fine-grained;visual information;image categorization;classifier
random projection;auxiliary;large-scale;singular value decomposition;large-scale datasets;low-rank;optimization problem;data matrix;background modeling;principal component analysis;computational burden;sparse matrix
classification performance;classification models;local features;spatial information;geometric information;spatial distribution;feature distribution;theoretical analysis;image classification;spatial correlation;separability;class-specific
object databases;multi-layer;test images;class label;multi-layer;image content;instances;reproducing kernel hilbert space;baseline methods;image classification
matching score;scale-space;shape descriptor;shape matching;sparse set of;matching method;shape matching
fusion method;low cost;image sequences;image sequence;upper bound;weighted sum of
high-resolution;high-resolution;matrix factorization;spatial resolution;data examples;cultural heritage
training data;facial expressions;database;higher-level;real data;statistical model;regression problem;pose estimation;analysis tasks;training datasets;human face
classification performance;takes into account;classification method;classification;local regions
structured light;optical character recognition;document images;perspective projection;text document;character segmentation;single image;laser range;text documents
high-speed;video frames;spatio-temporal;reconstruction algorithm;sparse representations;rates;optical flow;spatial resolution;high speed
feature space;object recognition;common features;classifier;multi-task;loss function;classifier training;lower-dimensional;image data;hypothesis space;object categories;prediction models;image descriptors;multi-class;task-specific;low-level;prediction tasks
instance;cut algorithm;object detection;region-based
human faces;video rate;high-quality;fully automatic
motion field;estimation technique;moving objects;optical flow;moving platform;real world scenarios
multiple views;multi-camera;human eye;visual attention
classification;translation invariant;model selection;classification;classification algorithm
image representation;basis set;image processing;machine learning
category recognition;kernel based;recognition task;scene categorization;scene recognition;classifier
bayesian network;sign language;inference algorithm;recognition accuracy;variational bayes;model parameters;supervised learning;sign language;image alignment
classification model;training examples;hierarchical structure;object localization;recognition systems;detection task;object detection;visual appearance;object detectors;object categories
single view;image descriptors;visual information;image features;database
inference method;multi-level;energy function;lower bound;inference problem;dual decomposition;body parts;lower level;image understanding;multi-level;human pose;benchmark dataset;real world;human body;human pose estimation;higher level
clustering;clustering algorithms;face recognition;multivariate data;positive definite;mixture model;real-world datasets;unsupervised clustering;dirichlet process;feature descriptors;counterpart;distance measures;video surveillance;clustering algorithm;bayesian framework;mixture models;clustering model
random noise;color images;color image
image segmentation;vision applications;input image;flow fields;visual correspondence;edge preserving;labeling problems
image retrieval;nearest neighbor;distance measures;ranked list;object retrieval;image space;retrieved images;ranked retrieval;retrieval accuracy;nearest neighbors;similarity measures
long-term;motion analysis;computational approach;short-term
dual decomposition;structured prediction;special structure;decomposition approach;learning algorithms;high-order;higher-order;max-margin;learning scheme;higher order
shape recovery;single image
clustering;learning process;latent dirichlet allocation;model assumes;large scale;topic model;data set;temporal coherence;spanning trees;markov random fields;high-level
image retrieval;storage cost;matching algorithm;sketch-based;database;search techniques;retrieval accuracy;large-scale;index structure;retrieval tasks;web-scale;natural images;image search;hand-drawn;search engine;million images;contour-based;web images
data-driven;image retrieval;scale-invariant feature transform;category recognition;rule-based;ranking function;imaging conditions;databases;benchmark dataset
feature points;object classes;semantic structure;image features;camera parameters;single images;estimation process;scene points;pose estimation;object detection;geometrical constraints;point-based;semantic structure;semantic information
high-precision;large-scale;high-precision;database;matching accuracy;range data;high precision
highly accurate;detection algorithm;large number of;general purpose;high accuracy
low resolution;high quality
scene understanding;applications including;scene understanding
visual similarity;large numbers of;data association;image-based;sampling method;expectation maximization;multiple hypotheses;instances;image pairs
instance;confidence measure;ground truth;point cloud;real-world
active learning;active learning;active-learning algorithm;energy minimization;algorithms require;user study;algorithms rely on
auxiliary;large-scale;object retrieval;selection process;visual words;image collections;optimization methods;low recall
linear constraints;global optimality;estimation) problem;vision problems;globally optimal solution;model fitting;piecewise linear;linear systems
takes into account;generative model for;object detection;generative model;video sequences;scene understanding
large scale;high-resolution;graphical models;memory requirements
prediction problem;classification;graphical models;structured prediction;point cloud;theoretical properties;inference procedure;single images;graphical model;labeled graphs;approximate inference;belief propagation
illumination conditions;pairwise classification;natural scenes;region based;graph-cut;ground truth;single images;edge information;detection results;single-image;classification results;shadow detection
shape prior;random fields;spatial relations;random fields
structural information;image enhancement;detection algorithm
object recognition;latent dirichlet allocation;image features;probabilistic model;bayesian approach;generative model
visual-words;graph partitioning;recognition rate;multiple views;level features;knowledge transfer;data set;visual words;action recognition;human actions;locally weighted;word clusters;bipartite graph;action models
video sequence;contrast set mining;semantically meaningful;discriminative power;anomaly detection;event detection;video sequences;structure information;action recognition;human actions;optical flow
image retrieval;feature space;regularization;web image;local regions;local models;locality-sensitive hashing;great success;vector machine;data set;regularizer;retrieval task;real-world;local classifiers;local classifiers;classification task;classifier;local region
image data sets;causal relationships;semantic labels;retrieval tasks;image annotation;graph-based;bipartite graph;classification methods;random walk on;input data;data set;learning problem;multi-label;image annotation;semi-supervised
attribute-based;attribute values;labeling effort;prediction performance;test image;attribute level;benchmark data sets;fully-automatic;class probabilities;accurate predictions;image classification;image labeling;prediction models;user input
appearance model;training set;multiple images;face model;complex objects;competing methods;human faces;wide range;deformable model;appearance variations;face alignment;active appearance models
computational efficiency;lower computational cost;point sets;closed-form solution;point correspondences;direct computation
upper bound;moving objects;numerous applications in;scene geometry;size estimation
automatically generate;large quantities of;image content;text data;natural language
query specific;high level;estimation method;low quality;digital cameras;image collections;baseline methods;lighting conditions;high level;classifier trained on
social relationships;tracking performance;decision-making;energy minimization;agent-based
graph-cut;scene structure;shape recovery;single image;single-view;high-level;low-level
large scale;scene reconstruction;cpu based;times faster than
database;tracking algorithm;data sets;tracking method;bayesian framework;reference image
multi-class;classification;optimization method;convex optimization;highly accurate;column generation;data sets;boosting algorithms;optimization problem;multi-class boosting;weak classifiers
speech recognition;human activity;activity recognition;time series;human activities;spatio-temporal;low-level
face recognition;recognition problem
inter-dependencies;inter-related;automatically generate;shape reconstruction
detailed comparison;feature points;computational complexity;intermediate results;feature point;distance transform;distance transform
energy function;random variables;large-scale;memory requirements;energy minimization;computational cost;random variable
video data;temporal structure;visual features;parametric model
parameter space;label information;pitman-yor;learning phase;power law;annotated data;bayesian approach;labelled data;power-law
unified framework;compact representation;human detection;pose estimation;human pose;action recognition;human actions;benchmark datasets
appearance model;learning algorithm;sparse representation;robust tracking;tracking algorithm;object tracking
automatic extraction of;automatically extracting;novelty detection;temporal order;images captured;video camera
object recognition;local regions;accuracies;high-order;local patches;pixel level;coding scheme;local image;image representations;hand-written
context information;appearance model;local features;similar object;key-points;real-world;visual tracking;template-based;high confidence;video sequences;lighting conditions
training examples;labeled data is;instance;multi-label learning;learning algorithm;optimization problem;multi-label learning;labeled data;multi-label;learning framework;training sample
automatic segmentation;image analysis;magnetic resonance images
feature points;reconstruction error;global consistency;mathematical formulation;labeling problem;video sequences;model fitting
appearance model;image descriptors;appearance-based;tracking results;tracking performance;training samples;temporal constraints;person tracking
local image;spatial information
markov logic networks;multi-agent;temporal reasoning;spatio-temporal;multi-agent;interval-based;temporal structure;event recognition;low-level
feature points;pattern recognition;synthetic data;random walk;hyper-graph;machine learning;higher-order;nodes represent;feature sets;graph matching;real images;random walks
random fields;higher order;classification;classifier
object recognition;video segmentation;classification;dynamic bayesian networks;unlike standard;dynamic programming;action recognition;human actions;multi-class
image retrieval;face image;database;retrieval methods;tensor analysis;search engines;missing data;images captured;face alignment;feature vectors;search technique;face database
video camera;unlike conventional;graph-cut;high precision;complex objects
face images;face image;edge-preserving;large-scale;parameter selection;image-based;target image;face alignment;reference image
cost function;matching algorithm
potential function;human detection;spatio-temporal;local features;behavior analysis
face recognition;dynamic programming;recognition performance;matching methods;pose variation;stereo matching;window-based;pose variations;stereo algorithm
matrix factorization;linear space;explicitly modeling;high-frequency
ct images;data-driven;regularization;energy function;regularization term;structure preserving;image data;markov random fields;image quality;simulated data
low-resolution;conditional random field;camera motion;crf) model;recognition accuracy;robust tracking;facial features;probabilistic inference;conditional random fields
classification;vision systems;unsupervised learning;unsupervised learning;static images;vector representation;topic models;real-valued;latent dirichlet allocation;rates;image-specific;optical flow;image representations;video frames
high correlation;video sequence;penalty term;camera motion;real-world;real-world situations;human motion;highly correlated
classification;classifier learning;fmri data;prior knowledge;group structure;predictive performance;prior information;magnetic resonance imaging;classification problems;classifier
graphical models;dual problem;database;optimization scheme;labeling problem;machine learning
noise estimation;noise level;sampling-based;natural image;high-order;image deblurring;map) estimation;squared error;map estimation;regularization parameter;bayesian framework;pre-processing;user interaction
taking into account;limited bandwidth;multi-camera;integrating information from;identification problem;imaging conditions;standard datasets
sampling method;monte carlo;solution space;kernel functions;data association;statistical model;parametric model;data sets;linear dynamical systems;gaussian processes;image sequences;motion model;markov chain
parameter space;parameter values;ground truth;scene geometry;camera parameters;metropolis hastings;image data;data driven;statistical model
data-driven;minimization problem;single image;user interaction;sparse representation;intrinsic image;intrinsic image;global constraints;natural images
cost function;eigenvalue problem;weighted graph;random-walk
high-quality;high-frequency;multi-view stereo;lighting conditions;multi-view;high-quality;multi-view stereo;estimation methods;range scans
image set;image based;image matching;machine learning
feature space;high-level;unsupervised learning;image representation;manifold learning;statistical techniques;matrix factorization;matrix factorization;sparse representation;geometric structure;image clustering;image space;semantic structure;high dimensionality;basis set;visual analysis;matrix factorization
image patch;index structure;binary code;object detection;image matching;matching method;object tracking
15;high-frequency;high-frequency;natural image;frequency information;single image;intrinsic image;stereo algorithm
face recognition;maximum likelihood estimation;classification;face databases;linear combination;regression problem;sparse representation;training samples
visual similarity;distance function;semantic similarity;cognitive science;semantic categories;human visual system
pair-wise;large amounts of;target class;instances;labeled training data;classification systems;detection approach;classifier;optical flow;moving objects;video sequences;benchmark datasets;hand-labeled;object categories
decision trees;local features;data mining method;training dataset;visual recognition;generalization ability;action recognition;individual features;binary features
svm classifiers;benchmark data;training data;million images;classification;svm training;large-scale;classifier training;feature extraction;stochastic gradient descent;large-scale;classification accuracy;class labels;image classification;hit rate
clustering;regularization;dictionary learning;basis functions;optimization problem;variational framework;sparse representation;image denoising;natural images
medical imaging;computational efficiency;locally linear embedding;data points;high dimensional
discriminant analysis;image set;image sets;geometrical structure;image datasets;discriminant analysis;graph-embedding;intra-class;image set;local structure;graph embedding;separability;inter-class
pair-wise;local minima;registration algorithm
face recognition;similarity measure;database;face recognition;unique characteristics;benchmark data sets
linear complexity;markov random fields;graph cut;graph cuts
large number of;pose estimation;machine learning algorithms;vision algorithms
image classification;excellent performance;local feature;databases
appearance model;discriminative power;level set;object-oriented;tracking applications;curve evolution;maximum likelihood;object tracking
dynamic programming approach;regularization;pattern recognition;classifier;optimization problem
segmentation algorithm;color transfer;local color
higher precision;false positives;image patches;pascal voc;object segmentation;object segmentation
image patches;real-world;statistical models;vision systems
multi-modal;spectral clustering;image feature;image features;object categorization;unlabeled images;laplacian matrix;image clustering;benchmark data sets;clustering
dynamic programming;mixture model;spatial relations;static images;standard benchmarks;tree-structured;pose estimation;human pose estimation
training set;image feature;latent dirichlet allocation;image features;traditional models;scene recognition;image analysis;image regions;real estate
local context;input (image;unified framework;global context;absolute error;natural images;nearest neighbors
parameter-free;point cloud;registration algorithm;information-theoretic;point set;transformation parameters;range data;kullback-leibler divergence;kernel density estimation;expectation-maximization
posterior distribution;blurred image;computational complexity
medical applications;local minima;image segmentation;vision applications;shape model
user interactions;local color;discriminative power;parallel structure;local classifiers;support vector machines;processing speed
maximum-weight;contextual constraints;data association;long-term;object appearance;video frame;maximum weight;benchmark datasets;object detectors
multiple views;multiple-view;information embedded in;extraction methods;single image
multiple factors;low-dimensional;geometrical structure;accurate classification
multiple instance learning;correlation-based;map estimation;region features
map estimation;algorithm performs well
regularization term;markov random field;optimization framework;probabilistic framework;control points;energy minimization;stereo matching;graph cuts;explicitly models;matching problem;control points;inference problem
face images;training data;database;real-world;large margin;supervision;age estimation;visual recognition;recognition systems;estimation accuracy;learning models;competitive performance;neural network
input image;feature points;automatically detects;scale-space;image space
accuracy compared to;high computational complexity;success probability;visual tracking;sparse representation;background model;video sequences;dimensionality reduction;times faster than
general purpose;optical character recognition;document processing;document images;image deblurring;upper-bound;document image
real-time tracking;memory usage;data normalization;multiple view;image databases;wide range;cost based
regularization;large scale;sparse representation;optimization problem;image search;semantic concepts;hierarchical representation;image annotation;weighted sum of
object classes;conditional random field;crf model;benchmark data sets;similar images;fundamental problem;image labeling
bayesian model;bayesian methods;synthetic data;temporal information;video data;time series;activity patterns;video sequences
regularizer
nearest;labeled samples;discriminative learning;free space;large-scale;feature extraction;transformation matrix;database;incremental learning;high accuracy;transfer learning;classifier;learning vector quantization
surface normal;initial conditions;motion field;flow fields;shape information;surface shape
feature space;classification problem;classification accuracy;vector machine;tensor-based;action recognition;covariance matrix;tensor decomposition
multiple classes;high-dimensional;face recognition;training data;classification;minimum number of;convex optimization;classification methods;sparse representation;face recognition algorithms;low-dimensional;recognition problem
clustering;image pixels;high-quality;natural image;user effort;higher quality
line search;image set
clustering algorithms;large scale;classification tasks;human action;probabilistic representation;scene categorization;visual words;recognition tasks;maximum likelihood
clustering;ordering constraints;video segmentation;video frames;visual cues;long-range;spatio-temporal;video sequences;object boundaries;cost function;motion cues;spatio-temporal;segmentation algorithm
natural image;lower bound;image denoising;natural images;prior knowledge
search space;energy function;synthetic data;energy minimization;optimization scheme;extended kalman filter;gradient method;video sequences;multi-target tracking;local minima;multi-target tracking
learning process;propagation algorithm;learning algorithms;data sets;visual concepts;knowledge transfer;instance level
object recognition;recognition rate;classification;recognition performance;sparse representations;computational cost;image classification
camera motion;physical location
object recognition;classification problem;nearest neighbor;classification;temporal information;accurately predict;depth images;training dataset;classifier;body parts;human pose;high accuracy;estimation problem;test sets
web-based;semantic space;web image;current commercial;textual information;image search;search engines;search engine;users' search;query keywords;query-specific;visual features
data point;higher accuracy;original matrix;pattern recognition;data points;image representation;feature extraction;linear combination;matrix factorization;global features;sparse representation;basis vectors;naturally leads to;image clustering;objective function;increasingly popular
constrained problems;image patch;image patches;database;spatial distance;predictive power;natural image;data redundancy;large collections of;image-specific;natural images;giving rise to
tag prediction;training set;large-scale;learning framework;learning process
appearance model;video sequence;appearance-based;labeled training;learning procedure;instances;image sequences;learning method
local features;classification;classification framework;image representation;low-rank;decomposition techniques;linear svm;matching method;image classification;feature vectors;sparse matrix;classifier
clustering;pair-wise;data points;similarity measure;uci datasets;max-margin;clustering accuracy;separability;vision problems;max-margin;margins;margin;classifier
shape analysis
clustering;large scale;semi-supervised learning;semi-supervised learning methods;propagation algorithm;graph-based;approximation algorithm
relevance feedback;spatio-temporal;computational cost;large variations;human actions
information content;large state spaces;large scale;probabilistic information;belief propagation;markov random fields;image matching;optical flow;loopy belief propagation;approximate inference;spatial domain
data generation;application domain;multi-view;real data;appearance based;data sets;training samples;wide range;shape models
predictive models;ground-based;tracking results;vehicle tracking;tracking algorithm;road networks
deformable model;multi-view;hand-held;wide range;optimization framework
real valued;gaussian kernels;processing algorithms;gaussian filter;mobile devices;fixed-point
high level;visual cues;automatically discovering;temporal patterns;efficiently extract;activity patterns;learning problem;dynamic scenes;convex optimization problem;low level
pattern recognition;spectral methods;real data;prior knowledge;general setting;model parameters;spectral analysis;low-dimensional
ground truth;real-time tracking;designed specifically for;ad-hoc;markov-chain;activity recognition;sliding window;data association;data association;surveillance video;monte-carlo;multi-threaded;multi-target tracking
parameter space;cost functions;dimensional space;exhaustive search;database;speed-ups;brute-force algorithm;motion segmentation
feature points;object recognition;image sequences;motion estimation;image processing;detection rate
training data;gaussian mixture model;classification;vector machine;topic model;bayes classifier;image classification;benchmark datasets
ranking algorithm;theoretical framework;web image;large margin;graph-based ranking;sample set;image search;search engines;graph laplacian;neighborhood graph;databases;ranking mechanism;compares favorably with;low-frequency
domain adaptation;training data;gaussian process regression;face detection;semi-supervised learning;black box;regularization;data set;training sets;optimization criterion;classifier
multi-modal;training data;high level;low cost;amazon mechanical turk;high quality;large number of;estimation accuracy;imaging conditions;human pose estimation;human body;natural images
initial conditions;markov decision process;interactive segmentation;reinforcement learning;pixel-level
parameter learning;conditional random fields;high accuracy
structured light;shape recovery;structured light;global illumination;global illumination
regression problem;classification;pattern recognition;latent variables;multiple labels;age estimation;large database;high accuracy;learning framework;low-dimensional;dimensionality reduction
markov random field;object classes;single view;depth map;reconstruction algorithm;single image;object reconstruction;rich information
video sequence;high-quality;labeled training;mixture model;classification accuracy;learning algorithms;dirichlet process;topic models;video content;comparative analysis;video indexing;automatically learn
feature trajectories;classification;camera motion;action recognition;flow field;optical flow;image classification;motion information
target class;image patches;synthetic data;object class;linear combination;29;classifier;intra-class;7;algorithm produces;strong classifier;weak classifiers
structured light;computational complexity;structured light;image-based;single image;generation process;single images;counterpart;dynamic environment;object shape
data structure;inference algorithm;joint distribution;interval-based;temporal constraints;event recognition;knowledge base;low-level
classification;regularizer;real datasets;vector machine;matrix factorization;classification results;feature selection;multivariate data;maximum margin;classifier
local features;recognition performance;spatio-temporal;action recognition;local features;action recognition
face recognition;multiple images;prediction methods;data set;generalization ability;human faces
high-quality;multi-view;large number of;multi-camera;limited number of;dynamic scenes
tree matching;dynamic programming;special structure;pairwise constraints;high-order;tree structure;real images;ground truth data
markov random field;energy function;high-quality;image segmentation;stereo matching;image pairs;segmentation results;depth estimation;standard benchmarks;object-level;soft constraint;object segmentation
motion capture;appearance-based;object categorization;human motion;fundamental problem;video sequences;motion capture data;video streams
probability density functions;low dimensional;database;graph-based;graph matching;information-theoretic measure;positive definite;graph matching
image segmentation;energy functions;graph cuts;graph cut;structured objects;approximation algorithm
shape representation;vision systems;real-world;highly accurate;visual representation;computational models
initial conditions;applications ranging from;activity recognition;spatial features;temporal information;main idea;dynamic systems;dynamic scenes;activity recognition
tracking methods;learning framework;probability distributions
image patches;image datasets;spatial distance;saliency detection;detection method;principal components;principal component analysis;dimensional space;visual saliency
feature level;object localization;pascal voc;feature representation;feature selection;local structure;object detector;object localization;low-level
medical applications;training data;instance;linear combination;shape priors;shape prior;probability distribution;image understanding;unified framework;real world;convex optimization problem
greedy algorithm;graph construction;vector spaces;objective function;random walk on
face recognition;sketch recognition;database;features extracted from;information-theoretic;feature extraction;large scale;mutual information between;feature spaces;classification algorithms
image retrieval;similarity information;explicitly represented;weighted graph;database;data manifold;long range;retrieval tasks;graph nodes;retrieval performance;database objects;tensor product;intrinsic structure;neighborhood structure;higher order
algorithm finds;ground truth;image regions;object detection;single image;long-term;great promise;learning framework;stereo images
real-world datasets;minimization problem;low-rank;input data;calibration method
global optimal
object instances;object recognition;object detection;ranking svm;computationally intensive
location estimation;image-based;image data;observation model;hidden markov model;large datasets;visual feature;estimation methods
training set;training data;taking into account;database;rule-based;image content;input-output;image pairs;manually annotated;supervised machine learning;input/output;high-quality;supervised learning
computational complexity;global optimal solution;spatio-temporal;optimal path;event detection;intra-class;video frame;search algorithm;discovery problem
computational complexity;randomized algorithm;sampling-based;large number of;selection process;sampling method;sample set;3;cost function;high-quality
subspace analysis;local features;static images;subspace analysis;classification results;action recognition;temporal features;video data;deep learning
semantic concept;occurrence frequency;image segmentation;natural image;small-size;databases;output variables;similarity graph;objective function;visual features;segmentation algorithms
probabilistic learning;energy functional;markov random fields;optical flow;low-level vision;continuous functions;inference algorithms;image restoration
high-dimensional;data visualization
generative-discriminative;fixed length;classification;discriminative models;semi-supervised learning;feature maps;scene recognition;generative models;expectation-maximization algorithm;linear classification;recognition tasks;protein sequence;algorithms for computing;error rate;supervised learning;likelihood function
markov random field;latent variables;prior knowledge;higher order;inference methods;graphical model;instances;higher-order;single image;high-level;shadow detection;low-level
11;large numbers of;12;large-scale;event types;real-world;moving object;surveillance video;instances;15, 8;action recognition;recognition tasks;benchmark dataset;real world;event recognition
conditional random field;minimization problem;learning approaches;data association;crf model;tracking performance;data sets;crf) model;multi-target tracking
real datasets;convex optimization;global optimization;missing data;point correspondences;maximum likelihood
data points;graphical structure;sparse representation;subspace clustering;connected components;graph connectivity;subspace clustering
clustering algorithms;face recognition;rank-order;precision/recall;distance based;clustering algorithm;rank-order
multi-modal;parameter learning;global optimal;video content;scene structure;generative model;hidden state;fast algorithms
nearest;image classification;pascal voc;local descriptors;databases
object classes;appearance-based;training examples;object class;object categorization;object appearance;object detectors
regularization;data analysis;vision applications;low-rank;optimization problem;visual tracking;quadratic optimization;simulation results
retrieval methods;pascal voc;explicitly models;prediction problems;multi-attribute;multi-attribute queries;query terms
vector machine;object detection;object classification;classification;classifier
ground-truth;fully automatic;prior knowledge
training set;training data;taking into account;database;rule-based;input / output;image content;input-output;image pairs;manually annotated;supervised machine learning;high-quality;supervised learning
color image;comprehensive evaluation;color features;spatial distribution;computationally expensive;single image;operator
energy function;optimization framework;energy minimization;optimization model;graph cuts;unsupervised algorithm;benchmark datasets;multiple images
synthetic aperture;vision applications;object detection;tracking problem;object tracking;quantitative analysis
real-world scenes;propagation algorithm;video data;implementation details;spatial resolution;high resolution;analysis tasks;high resolution;video camera;ground truth data;spatial domain;video streams
variational inference;motion field;direct access to;flow field;optical flow;intrinsic structure;estimation algorithm
image retrieval;spatial correlations;image regions;image datasets;test image;image space;regularizer;image content;fine-grained;image matching;objective function
nearest;image sets;image based;classification;single image;gradient method;image set;classification;convex formulation;nearest point
sampling techniques;graphical model;probability distributions;human pose;sample points;articulated objects
additional constraints;topological properties;energy function;image segmentation;desired properties
high-dimensional;high-dimensional;training set;training set size;data compression;large-scale;classification accuracy;large databases;large number of;dimensionality reduction technique;classifier learning;large datasets;image classification;cpu cost;training algorithm
object recognition;monte carlo;classification;landmark points;matching methods;prior distribution;instances;object detection;shape matching;posterior distribution;real images;markov chain;matching problem;matching cost;shape matching
image segmentation;ground-truth;multiple images;manually annotated;segmentation accuracy;detection methods;higher accuracy;segmentation algorithm
complete model;matching scheme;shape representation;partial matching;weighted graph;shape descriptor;shape information;target object;contour based;shape context;shape similarity;object detection;local affine;histogram based;shape matching
dictionary learning;basis set;sparse representation;image denoising;dictionary learning;theoretical analysis;computationally feasible;convex formulation
sequence alignment;dynamic programming;tracking objects;scene structure;main idea;vehicle tracking
face recognition;training data;linear model;nonlinear regression;image features;subspace learning;image noise;database;training set;regularization;instance;pose estimation;generative model;subspace learning;diverse applications;computational complexity
classification;test data;confidence intervals;decision rules;classification process;wide range;classifier
high-dimensional;feature space;kernel methods;machine learning problems;closed-form solutions;input space;kernel k-means;kernel methods
public domain;computational biology;classification;numerical results;boosting methods;image analysis;machine learning;competing methods;boosting algorithms;databases;strong classifier
energy function;lower dimensional;gaussian process;latent space;object class;shape priors;level set;shape information;video sequences;latent variable models;multiple images
sampling methods;sampling algorithm;energy minimization;metropolis-hastings;solution space;energy functional;posterior distribution;level-sets;inference problem
10;markov random field;image features;multi-view stereo;prior knowledge;generation process;belief propagation;reflectance model;additional information
dynamic range;sensor technology;range images;image sensor;moving objects;typically involves
regularization;expectation maximization algorithm;vector field;vector-valued;reproducing kernel hilbert space;bayesian framework
transformation parameters;sparse representation;global optimality;cutting plane algorithm
design space;template-based;trade-offs
pair-wise;appearance model;multiple images;scale invariant;computational requirements;appearance variations;multiple image
rich information;reconstruction error;computational costs;visual tracking;sparse representation;computationally expensive;particle filter;error bounded
digital images;illumination conditions;computational approach;mri data;human visual system;specific properties;real data;medical images;magnetic resonance imaging
prediction accuracy;perceptual information;natural image;movement data;human eye;eye movements;computational model;visual saliency;natural images
appearance model;motion capture;image segmentation;multi-view;ground truth;input images
data compression;database;large-scale;locality sensitive hashing;semi-supervised;search efficiency
parameter values;weighting function;gaussian mixture model;appearance model;ad-hoc;human vision;parameter selection;spatial information;low-level vision;low-level
energy function;single image;intensity values;local image;intrinsic image;natural images
data set;ground truth;post-processing;video surveillance
clustering algorithms;similarity matrices;graph-based;data clustering;clustering techniques;similarity matrix;clustering algorithm
surface normals;additional constraints;local shape;optimization scheme;single image;real images;local orientation;light source
light sources;camera motion;multiple view;camera motions;motion estimation;blurred image
image retrieval;image features;category recognition;compact representation;text features;web-queries
perspective camera;linear models;analytical model
algorithm finds;training examples;object shape;image-segmentation;meaningful results;object segmentation;structural svm;object-level;segmentation algorithm;learning framework;object segmentation;low-level;scoring function
object model;tracking objects;similar objects;dynamic programming
process model;gaussian process;classification;task-specific;exponential family;inference algorithms for;process models;likelihood function
prediction accuracy;classification;classifier training;compared with conventional;inter-related;discriminative information;alzheimer's disease
matching score;pre-processing;score based;multi-body;scene geometry;landmark points;recognition accuracy;action recognition;point correspondences;human body
image feature;feature vector;partitioning strategy;spatial information;computational costs;discriminative power;local features;weighted sum of;image recognition
feature space;tree-structure;density estimation;image segmentation;closely related;spanning tree;density function;learning framework;diverse applications;weighted graph
multiple images;feature descriptors
spatial patterns;key features;statistical model
tracking algorithm;search strategy;globally optimal
linear programming;discriminative features;explicitly models;training examples;invariant features
stereo algorithm;scene reconstruction;color images
higher precision;object recognition;extraction algorithm;image segmentation;high quality;prior knowledge;data sets;rates;detection methods;visual saliency;image processing
clustering;spectral clustering;image segmentation;spectral clustering;data set size;data set;large data sets;data sets;clustering algorithm
object recognition;sampling methods;object parts;topic models;classifier;competitive performance;visual recognition;topic modeling;visual representation;topic model;gibbs sampling;inference algorithm;model training;visual features
regularization;n^6) linear sub-step. to emphasize this improvement, it is demonstrated that the new algorithms are upwards of 10^4 (ten-thousand) [times faster than;surface reconstruction;large-scale
data-driven;18;high quality;registration algorithm;rank minimization
multiple views;graph-based;image sequences;tracking algorithm;weighting function
large-scale;video sequences;iterative optimization;vision based;current location;euclidean space
pose estimation;real data
clustering;face image;segmentation problem;synthetic data;model parameters;multi-body;model estimation;projection based;face database
set cover;global information;multiple views;large number of;set-cover;moving objects;network-flow;long-term;bipartite-graph;local information;optimization techniques;short-term
pose variations;model parameters;max-margin
background clutter;sample size;intra-class;matching accuracy;temporal constraints;learning problem;identification problem;visual appearance;visual features;camera views
pre-defined;video segmentation;classifier;discriminative models;mixture model;large scale;video data;segmentation problem;supervision;task-specific;time-series;graphical models;statistical analysis;tree structured;patch based;semi-supervised;multi-class;semi-supervised;segmentation algorithm
image retrieval;query expansion;automatic query expansion;tf-idf;spatial context;retrieval results;retrieval performance;correlated features;statistical model;query expansion
hand-labeled;hidden variables;excellent performance;global models;human faces;objective function
real data;theoretical results;surface normal;surface reconstruction;light sources
data cleaning;fall short;master data
database instances;complex) relationships;target instance;source schemas;instance;mapping composition;similarity measure between;operator;automatically generate;schema mappings
database server;location-based queries;database;indexing structures;spatio-temporal;database research;spatio-temporal;moving object;safe regions;proposed protocol;nearest neighbor queries;cost model;moving object databases;query distribution;moving objects;query results;indexing techniques;range queries;database systems
log records;database systems;database;software architecture;multi-core;intensive workloads;data structures;modern database;high volume;times higher
data analysis;applications including;large-scale;fault-tolerance;real datasets;hadoop mapreduce;large-scale data mining;graph analysis;data processing;data mining;model fitting;data-intensive;highly scalable;arise naturally in
twig queries;twig patterns;xml query;rewriting rules;execution plan;cost model;xml repositories;cost-based;query processing;heuristic rules
quality-aware;data source;data services;data sources;data quality;service-oriented;data integration
selection problem;classification;materialized views;semantic web;view selection;data warehousing;selection methods;databases;query processing
linked data;years ago;computational complexity;query languages;social network data;query languages;graph databases;query evaluation
relational databases;temporal features
state university;data management;databases
data-driven;information systems;application area;data mining;data mining;decision making
data mining based;data mining
growing number of;collaborative learning;social network analysis;community mining;social network analysis;social networks;network structures;learning environments;information networks
matrix factorization;high level;upper bounds;matrix factorization;simulated data;visualization techniques;learning environments
performance prediction;ensemble methods;data set size;data set;predictive performance;ensemble approach
end-users;process mining;acm sigkdd;process mining;process mining;business process;highly relevant;data mining;knowledge discovery process;knowledge discovery
spam detection;search results;detection techniques;high quality;web spam;user behaviour;information retrieval;search engines;link-based;graph regularization
data mining and machine learning;post-processing;data mining;machine learning algorithms;modeling techniques;model building
black-box;location-based services;large-scale;private information;privacy concerns;shortest path;road networks;shortest path;privacy-preserving;information leakage;retrieval techniques;personal information
mapreduce-based;real datasets;candidate pairs;real dataset;join algorithms;similarity joins;internet traffic;small size;times faster than
performance gains;graph based;large-scale;machine learning and data mining;parallel computation;learning systems;high degree of;shared-memory;data consistency;data processing;data mining;high-level;machine learning algorithms;fault tolerance;network latency
graph-structured data;computational complexity;join operations;structural constraints;intermediate results;tree pattern;tree patterns;real-world applications;real-life;decision procedures;query languages;synthetic data sets;complex relationships;tree pattern queries;graph representation
end-users;regular expressions;database;data types;input-output;relational tables;string transformations;data type;error prone;relational table
query processing strategies;language constructs;constraint optimization problems;goal-oriented;case studies;wide range;fast convergence;low overhead;wireless network;constraint solving;distributed systems
optimizer;big data;intermediate results;sharing opportunities;execution plans;analysis tasks
replication;data store;data stores;closed-form solution;data store;internet-scale;data consistency
big data;big data;data management
data structure;sketch-based;graph streams;data set;data stream;numerical data;massive data sets;distributed data streams
probability distribution function;n
data sets;heavy hitters;distributed data
worst-case optimal;database;worst-case;conjunctive query;database research;join processing;data complexity;join algorithms;[extended abstract;worst-case optimal;information-theoretic;join queries
xml schemas;regular expressions;xml processing;regular expression;instance;wide range;bounded number of
42, 28, 34, 39, 4, 7, 33, 16, 1, 37, 35, 47, 49;23;38;43, 50, 44, 12, 13, 14, 24, 25, 8;43, 50, 44, 45, 12, 13, 14, 24, 25, 10, 51;programming languages;user interfaces;11, 2, 22;6;database;view update;view definitions;36;definition language;18;9;18, 28, 39, 35, 29;18, 17, 10, 30;5, 19, 3, 36;20, 47, 26, 32, 48, 40, 15, 31, 46, 41, 21, 27;relational algebra
statistical queries;database privacy;years ago;database;special case;sum queries;social networks;structure information;application domains;databases;negative result;graph databases;negative results
application domain;data sharing;continuous attributes;aggregate information
query optimization;query evaluation;query execution plan;web queries;data sources;static analysis;transformation rules;semantic web;graph patterns;data management;static analysis;conjunctive queries;query languages;optimization techniques;normal form;web data;sparql queries
rdf data;world wide web;query evaluation;regular expressions;path expressions;sparql queries;query language;graph data
data structure;external memory;categorical data;block size;data structures;space usage
disk block;query returns;external memory;real-valued
data structure;lower bound;external memory;data points;range search;search problem;arbitrarily large
case studies;regularization;data analysis;input data;database;application domain;database theory;large-scale;statistical properties;worst-case;large-scale data analysis;noisy data;databases
result diversification;result diversification;20;distance function;17;greedy algorithm;linear combination;information retrieval;general setting;databases;approximation algorithm;metric space;search algorithm;approximation ratio
web-based;network flow;np-complete;selection queries;data complexity;conjunctive queries;common practice
conjunctive-query;target instances;database systems;source instances;data exchange;sufficiently large;schema mapping;query equivalence;conjunctive queries;tuple-generating dependencies;schema mappings;data integration;database instances
database;conjunctive query;view-update;view definition;np-complete;functional dependencies;simple algorithm;approximation ratio;np-hard
column-oriented;data structure;data structures;random access;databases
clustering;quality metric;data structures;lower bounds;average number of;multidimensional data
face recognition;nearest neighbor;probability density function;probabilistic framework;nearest-neighbor;sensor databases;nearest-neighbor queries;nearest neighbor queries;wide range;location based services;query point
query optimization;complexity bounds;classification;conjunctive queries over;databases;decision procedures;conjunctive queries;query containment
correct answers;query evaluation;conjunctive queries;databases;exact answers;large database
recommendation systems;query relaxation;selection criteria;data complexity;lower bounds;bounded number of;multi-criteria;query languages;special case;recommendation systems
lower bounds;data stream;heavy hitters
computational geometry;main memory;external memory;streaming model;data streams;data stream;data stream model;databases;special case;heavy-hitters;spatial databases
lower bound;randomized algorithms;communication cost;communication complexity;tracking problem;computing power
randomized algorithm;long-range;regression problem;communication cost;lower bounds;communication complexity;tracking problem;high probability;input streams;special case;relative accuracy;distributed environment
rates
high-dimensional;end-user;sql queries;event processing;user-centric;low-level;data intensive;intrusion detection;xpath queries;relevant content;large collections of;computational advertising;data streams;processing requirements;indexing structures;mobile) devices;data analysis;high-frequency;comprehensive evaluation;parallel) algorithms;applications ranging from;information dissemination;real estate
declarative query language;query execution;fast response;spatial data;image based;cost effective;large volumes of;large scale;access methods;scientific data;data partitioning;query engine;spatial queries;medical imaging;derived data;execution engine;biomedical research;mapreduce based;scientific domains;end users;query processing
multimedia databases;database systems;big data;database;query workload;dynamic environments;design choices;large collections of;physical design;index selection;existing indexes;social networks;adaptive indexing;query processing;adaptive indexing;scientific databases
software systems;data quality;data quality;heterogeneous systems;demand driven;data quality
mutual information;relation extraction;natural language text;large scale;automatic evaluation;named entities;clustering techniques;clustering process;clustering techniques;relation extraction
social media;open source;recommender systems;recommender systems;query efficiency;diverse set of;database;location-based;interesting items;social networking
linked data;regular expressions;regular expressions;data processing;query languages;xml schema
rdf data;meta-data;semantic web;semantic web;relevant data;sparql queries
clustering;erroneous data;fall short;data analysis;spatial data;record linkage;real-world;strong evidence;data sets;real-world entities;clustering methods;data describing
stream processing;pattern detection;probabilistic information;dynamic behavior;stream processing;data streams;machine learning algorithms
domain knowledge;machine learning techniques;computing environments;machine learning;control theory;cost-sensitive;data-centric;lower cost;total cost
adaptive query processing;cost estimation;distributed monitoring;large volumes of;distributed data streams;increasing number of;generic framework;decision-making;environmental monitoring;cost-based;sensor networks;data stream management systems;data streams;databases;efficient optimization;recursive queries
database systems;replication;database;high scalability;data replication;data access;storage systems;distributed transactions;disk-based
query optimization;business decisions;performance bottlenecks;programming languages;distributed computation;increasing number of;execution strategies;optimization opportunities;real-world;data storage;higher level
minimal overhead;user-defined;input data
data structure;unlike earlier;long-running queries;moving-object;main-memory;multi-core;query semantics;mobile devices;concurrency control mechanism;data-store;moving objects;object-level;query results;indexing technique
database systems;database;physical design;replication;materialized views;execution plans;databases;load balancing
parallel database;main memory;solution space;17;database;database design;diverse set of;oltp systems;7, 9;cost model;management systems;stored procedures;databases;design space;unique characteristics;distributed transactions;load balancing;computing systems;data distribution;transaction processing
user-input;end-users;attribute-level;schema mapping;data values;large number of;complex structure;source schemas;target instances;user study;schema mappings
data cleaning;high efficiency;similarity search;similarity join;cost model
higher precision;response times;real-life datasets;information gathering;html tables;high precision;user input
lower bound;database systems;19;vldb;upper bound;multi-criteria decision making
join algorithm;database operations;database systems;main-memory;join algorithms;databases;margins;application scenarios
source code;program analysis;query results;database;web service;performance gains;access patterns;real world applications;web-service
large-scale;communication bandwidth;theoretical analysis;graph databases;data management system;clustering;graph-structured data;biological networks;graph database;large number of;bandwidth consumption;social networks;storage requirements;graph databases;social graph;simple queries;replication;communication networks;open-source;large volumes of;large scale;dynamically changing;large graphs;optimization problems;query processing;dynamic graphs
incremental algorithms;query answers;equivalence relation;pattern queries;real-life;compression techniques;prohibitively expensive;incremental maintenance;social networks;graph pattern matching;graph queries
large graphs;query answering;increasingly large
high-speed;information loss;query results;large datasets;result set;relational database;query result;relational data;high speed
randomized algorithm;end users;large-scale;limited space;special case;integer programming
execution plans for;large volumes of;query processors;raw data;language constructs;intensive workloads;query processing;data-intensive applications;data type
log structured;real-world;log structured;performance metric;data management;general purpose
14;individual queries;query workload;field-programmable gate arrays;finite-state;high-volume;query processing;stream processing engine
query execution;data analysis;positive effect;database systems;performance bottlenecks;data collections;modern database;query performance;efficient access to;lessons learned;social networks;data files;data type;adaptive indexing;query engine;feature set
applications requiring;data exchange;xml streams;social network;general-purpose;optimization techniques;wide range;structured information;continuous queries;complex event processing
real data sets;streaming applications;distributed monitoring;distributed data streams;parameter settings;application scenarios;query type;prediction models
optimization technique;synthetic datasets;molecular biology;real datasets;query semantics;time series;knowledge discovery;data stream;complex event processing;data processing;data mining;approximation algorithm;filtering algorithm
high utility;mobile applications;sensor readings;user context
query workloads;security analysis;service providers;cost models;location-based services;location-based;optimization techniques;social networking;location privacy;range queries
shortest paths;web search;web search results;real datasets;query logs;query cost;shortest path;location-based services;search engine;web search;network traffic;greedy algorithm;path queries;caching techniques
data analysis;open-source;database;enterprise applications;ad hoc;statistical technique;development process;database vendors
data analysis;database;optimization problems;query language;counterpart;instances;data management;relational database system;specification language;integer programming;standard sql
individual records;theoretical framework;data owners;differentially private;sensitive data;privacy preserving;data analysis;data analysts
expected cost;human computation;expected error;data items;query processor
query evaluation;graphical models;statistical inference;local structure;queries efficiently;information extraction;probabilistic databases;probability distributions;speed-ups;evaluating queries;databases;local structure;tuple-independent;data integration
heuristic functions;data manipulation;human operators;np-hard;database
query retrieves the;attribute values;user preferences;large number of;linear combination;user preference;scalable solution;numeric attributes;query results
efficient query evaluation;ranked list;intermediate results;large number of;instance;search engines;wide range;query type;databases;threshold algorithm;relational data
vector space;access methods;spatial web objects;relevant objects;score-based;geo-referenced;data sources;computational overhead;distance-based;location-based services;real estate;space partitioning;low-dimensional
existing database;relational algebra;query optimization techniques;database
labeling scheme;set cover;applications ranging from;sparse graphs;semantic web;social network;shortest path;index size;link analysis;distance computation;real datasets
network analysis;real-world graphs;query processing in;main memory;tree-structured;large graphs;index construction;shortest path;wide range;query processing;disk-based
search engine's;privacy concerns;search queries;search engines;scoring function;search engine;theoretical analysis;keyword search;search interface;web interface
probabilistic modeling;uncertain information;web pages;text understanding
evaluation cost;dynamic programming;query workload;large number of;optimization problem;information retrieval;index structure;space constraints;approximation algorithm;keyword query;keyword search
clustering;distance measure;inference problem;real-world datasets;community detection;clustering method;distance measures;graph clustering;probabilistic model;data mining;clustering;real-world graphs;graph clustering;distance-based
query response time;breadth-first search;large-scale;community detection;random walk;application domains;data partitioning;large graphs;query processing in;social networks;query workload;sparql queries;graph queries;mining large graphs;management architecture
data analysis;synthetic datasets;search paradigm;np-complete;spanning tree;subgraph isomorphism;exploratory queries;spanning trees;efficiently computing;special case
data objects;high-dimensional space;hash function;synthetic datasets;high dimensional;locality-sensitive hashing;search problem;nn search;query quality;hash table;real datasets;locality-sensitive hashing;high dimensional space;hash functions
data set;query evaluation;main memory;external memory;massive graphs;worst-case;real-world;equivalence classes;internal-memory;tree-structured;xml documents;data structures;data sets;data provenance;internal memory;external-memory;algorithm to compute;open-source
selection problem;query workload;xquery processing;search space;materialized views;view selection;heuristic algorithm;answer queries;general problem;space budget
database technology;database;machine learning;broader range;visual interfaces;database technologies;data mining;statistical inference;statistical models;decision making
fault tolerant;network applications;anomaly detection;typically performed;large-scale;case study;data management techniques;data processing;data store;data management

world wide web;biological networks;graph mining;mining large graphs;singular value decomposition;real-world;large graphs;graph mining algorithms;social networks;mining large graphs;dynamic graphs
mining large graphs;general purpose;ad-hoc;graph data;mining large graphs;programming models
database research;database
linked data;database;data publishing;structured data from;database research;distributed databases;data items;database;database researchers;data management;query processing
data publication;database;background information;sensitive data;prior knowledge;data privacy;privacy-preserving
extracting information from;web-scale;web-scale;information extraction;structured information;information extraction
query forms;user friendly;database applications;search engine;large databases;search-engine;search engines;databases;query interfaces;matching records;application developers;indexing structures
allowing users to
multi-level;optimizer;communication cost;cost-based;join queries;processing power
12
data sharing;cloud services;data center;stored procedures;data access;discovering patterns
query optimization;query optimizer;sql queries;nested queries;ad-hoc querying;data mining;sql extension
occurrence frequency;isolation level;data items;graphical interface;serializability;multi-tier
database systems;business data;database design;online analytical processing;online transaction processing;transaction processing
location-based;social networking;query processing engine;location-aware;large number of
related entities;social graph;large collections of
information extraction from;social media;user interface
approximate answers;personalized search;search paradigm;providing users;design choices;social network;social tagging;collaborative tagging;materialized views;general setting;search interface
database;black box;data model;query optimizations;operator;query answering;database engine
xml documents;xml schema;xml document;theoretical foundations
ad hoc queries;relational queries;database;graph-based;directed graphs;natural language;query translation;graph traversal;structured queries
graph datasets;life sciences;finding patterns
web-based;statistical properties;visual analytics;data sources;complex patterns;interactive visual;relational data
data manipulation;data analysis;distributed memory;shared-memory;machine learning;answer queries;query processing;large datasets;fault-tolerant;coarse-grained
pre-defined;data analysis;distance functions;mapreduce-based;database;real-world;metric space;mapreduce based;feature vectors;data types;similar images;data processing;massive amounts of data;similarity joins;similarity join;similarity joins;open-source
input data;large scale;map-reduce;user-defined;single class;scalable distributed;relational database;data analytics
query execution;data analysis;large scale;distributed file system;execution model;complex tasks;analysis tasks;intermediate results;query languages
query processing techniques;data sets;join queries;data processing;database
data residing;data analysis;optimization problems;user feedback;instance;provide feedback;query engine;data management
end-user;database;web service;user interface;data stream;web services
graph mining;degree distribution;anomaly detection;large graph;large graphs;analysis tasks;connected components;interactive visualization;disk-resident
resource utilization;high availability;resource allocation
high-quality;open-source;large scale datasets;decision-making;analytical processing;databases;reward function;manual effort
large scale;relational database;database
execution environment;general concept;database;data representation;database entries;data structures;main features;database engines;highly scalable;data management;transaction processing
storage layer;high-availability;real data;replication;wide range;data storage;data management systems;low-level;load balancing
social media;statistical modeling;social media;massive amounts of;inter-relationships;large amounts of data;unlike conventional;enterprise applications;machine learning;provide feedback;financial services;social media data;statistical models;information exchange
recommendation engine;social media;recommendation engine;taking into account;1;user interests;2;easy access
valuable knowledge;specific domains;link structures;complex tasks;quality control;data processing;produce high quality;distributed computing;design decisions;structured data;markup language;automated techniques
execution environment;query optimization;unlike earlier;optimizer;query optimizer;query optimizer;sql server;query execution;massively parallel;data warehouse;cost-model;microsoft sql server;query processing in;cost-based;cardinality estimation;high-quality;vast amounts of data;microsoft sql server
data loss;higher throughput;data centers;data center;replication;distributed nature of;highly distributed;fault tolerance;fault-tolerant;relational database;query engine
efficient parallel;information content;big data;big data;database;user defined;database;sql queries;large networks;oracle database;data processing;increasing importance;unstructured data;programming paradigm
data store;social graph;social interactions
data-driven;ensemble methods;large-scale;large-scale;stochastic gradient descent;online learning;learning tasks;machine learning;case study;business intelligence;data warehousing;feature generation;large amounts of data;massive amounts of data;user-defined functions;supervised classification;data management;predictive analytics
end-users;data visualization;data warehouses;user experience;heterogeneous data sources;visual interface;ad-hoc;business intelligence;visual analysis of;data cubes;integrate data from;integration systems;schema-mappings;visual analysis;data integration
public data;algorithms produce
text analytics;ad-hoc;data flows;map-reduce;machine learning;business intelligence;execution engine;structured data;unstructured data;multiple objectives
high-speed;data analysis;database operations;performance bottlenecks;large-scale;multi-stage;increasing number of;skewed;analytical model;load-balancing;sampling approach;applications requiring
social network data;data analysis;hierarchical data;recursive queries;data cube;intermediate results;plans;generally applicable;social network;execution plan;dimension hierarchies;real-life;stream data;data stored in;optimizer;optimization techniques;recursive queries
design choices;database
web applications;database;highly interactive;xml-based;application developers;application server;application development;user input
pose queries;queries involving;open-source;large document collections;text search;large quantities of;text analytics;text-analytics;large amounts of;extracted data;information extracted from;text corpora;blog posts;database;natural language processing;xml data;statistical models
kim09vldb, blanas11sigmod;optimization technique;temporal locality;join algorithm;multi-core;data set;join algorithms;hash table
1;statistical queries;synthetic datasets;multi-dimensional data;query workload;index structure;multi-dimensional;sensitive data;statistical information;answering queries;main idea;random noise;margins;query results;query processing
workflow systems;memory usage;closely related;index structure;pruning strategies;real dataset;user interests;provenance information;sina weibo;recent database
detection techniques;water distribution;synthetic datasets;anomaly detection;efficient approximation;baseline algorithm;time series;application domains;real data;instances;real-world networks;road networks;dynamic networks;detecting anomalies;evolving networks;times faster than;error rate;normal behavior;communication networks
edges represent;web based;nodes represent;recommendation approaches;web-based
word vectors;clustering problem;pearson correlation;naïve;hash functions;text analytics;large text collections;high dimensional;word pairs;text data;large document collections;highly correlated;random projection;locality sensitive hashing;fast algorithm;real world scenarios;cosine similarity
long-range;diverse set of;information retrieval;wide range;information seeking;information retrieval
relevance feedback;structured documents;focused retrieval;book search;data centric;test collections;faceted search;evaluation measures;question answering
social media;digital library;special attention;high quality;large collections of;user experiences;digital libraries
document level;user interfaces;named entities;lower level;information retrieval;semantic annotations;information access;low-level;indexing methods;higher level
aggregated search;aggregated search
social-media;web search;federated search;information retrieval;resource selection;web search results;web search engine;digital media;query string;web search results;cost-effective;training data;results merging;query-traffic;search task;web search;machine learning methods;federated search;relevance judgments;machine learning;predictive model for;selection methods
human subjects;keyword-based search;classification;knowledge base;average precision;semantic knowledge;entity retrieval;entity retrieval;retrieval task;multiple types of;information retrieval;semantic search;information extraction;search interface;data quality;movie database
ground truth;retrieval strategies;pseudo relevance feedback;strong correlation;ir evaluation;relevance judgments;query generation;information retrieval;manually annotated;retrieval models;retrieval systems;recall oriented;relevant information;effectiveness measures;query expansion;retrieval applications
evaluation methodology;theoretical framework;xml element;xml retrieval;xml retrieval;theoretical properties;retrieval models;information retrieval;document structure;retrieval model;xml structure
search results;result diversification;query-specific;topic structure;case study;clustering;information retrieval;background information;knowledge base;source text;term frequencies;automatically generating;information access;retrieval results;semantic relatedness;query performance prediction;multi-faceted
search results;term-matching;retrieval effectiveness;language models;multiple features;content analysis;explicitly modeling;ranking models;information retrieval;temporal queries;document collections;web archives;retrieval methods;ranking model
interface design;multi-dimensional;machine learning techniques;gather information;information sources;rule base;user study;result page;search process;user query;instance;document genre;search engine;information gathering;aggregated search;log data;topical) relevance;search interface;aggregated search;design issues
social media;search engine;retrieval effectiveness;topical relevance;social media;web collection;diverse set of;information contained in;query logs;retrieval performance;retrieval model;information access;blog posts;expansion terms;access information;external sources;search behavior
step forward;learned models;multiple features;easy access;text collections;wide range;theoretical foundations;concept hierarchies;contextual information;access information;concept hierarchy;optimization problem;user studies;semantic relations;task-specific;abstraction levels;tree edit distance;similarity metric;machine learning;personal preferences;user study;user behaviors;np-hard
complex systems;information systems;embedded systems;international conference on;complex systems
enterprise data;ontology based;data sources;relevant data;information retrieval
knowledge engineering;knowledge management;domain specific;knowledge management;knowledge sources;knowledge-driven
ontology-driven;application domain;semantic knowledge;ontology driven;case study;digital libraries;data management
complete set of;web access;web access;pattern mining;pattern tree;tree) based;pattern mining;web access;sequence database;support threshold
text summarization;single document summarization;graph traversal;graph based;sentence extraction
test set;success rate;information extraction;text document;human beings;automatic text summarization
analytical queries;large numbers of;historical data;data warehouse;query response time;large number of;materialized views;answering queries;query frequency;query processing;future queries;view selection;decision making;total cost
xml databases;emerging area;xml schemas;database management systems;xml documents
clustering;materialized view;database;data warehouse;similarity function;views defined;clustering approach;selection algorithm
high frequency;compression method;time series;dimensionality reduction
clustering;data set;rough set theory;high dimensional;clustering process;clustering high dimensional data;maximum variance;subset selection;noisy data;traditional clustering algorithms;relevant attributes;dimensional data;clustering algorithm;local minima
false positive;vector machine;margin;network classification;high precision and recall;classification method;vector machine;network traffic;margin;classifier
image retrieval;clustering;retrieve images;database size;input image;training dataset;image databases;user's preference;relevance feedback;supervised approach;clustering techniques;visually similar;retrieved images;content based image retrieval;user query;artificial neural networks;neural network;neural networks
true values
appearance model;face recognition;face detection;principle component analysis;temporal sequences;linear discriminant analysis;face recognition;hybrid method;independent component analysis;markov random field;hidden markov model;graph matching;hidden markov model;feature based
large volume;management systems;pattern mining;pattern mining;fault management;network management
relevant information;data warehouse;decision-making;common practice;data warehouse systems;databases;dimensional data;dimensional data
tree structures;tree mining;pattern tree;database;pattern mining;fp tree;tree structure;data mining;candidate generation;frequent pattern;fp growth;frequent pattern mining
semantic web;management systems;web content;search engines;relevant information;ontology based;intelligent tutoring systems
manual construction of;semantic web;building block;semantic web;inference rules
business analysts;web users;large number of;data sets;web users
classification;higher-level;complex structures;classification algorithms;visual features;class prediction
image retrieval;similarity metrics;wavelet decomposition;distance metrics;similarity measure;image database;feature extraction;content based image retrieval;database;retrieval performance;key parameters;databases;similarity measures
search space;principal component analysis;vector machine;clustering techniques;precision-recall;content based image retrieval;low-level features;semantic categories;machine learning techniques;image categorization;content-based image retrieval
quality metrics;data warehouse;data warehouse;principal components;data warehouses;3;principal component analysis;dimensionality reduction
business operations;statistical techniques;real data;capacity planning;databases
subset selection;database;digital content;digital information;subset selection;relational database;relational databases;numeric attributes
classification scheme;success rate;classification;extracted features;vector machine;hidden information;classifier;feature based;feature set
features extracted;similarity measure;similar images;database
classification;semantically meaningful;learning framework;shape descriptor;human action recognition;classification performance;human action recognition;moving objects;partially-supervised;specific context;video sequences;real world;visual perception;physical objects
case study;data mining technique;data mining
tracking method;real world;primary goal;reference point
user friendly;cost effective;wireless sensor network;limited number of;human body;effective tools
bayesian network;data mining technique;classification techniques;classification;classification;vector machine;categorical data;naïve bayes;data sources;fitness;analyzing data;data mining problem;training algorithm
inductive learning;search engine's;sponsored search;sponsored search;search engine;mechanism design
open source;database systems;database;database design;database;operating systems;transaction processing;application development;database systems
target language;machine translation;source language;data driven
training set;machine learning;learn models;rule based;rule based;machine learning;classification algorithms
target language;nearest;distance function;parallel corpus;source language;boundary detection
clustering algorithms;trade-offs;clustering solution;high quality;functions defined;document clustering;graph based;optimum solution
clustering;clustering algorithms;hidden patterns;low-rank;matrix factorization;large data sets;data sets;meaningful information
clustering;biclustering algorithms;classification model;feature selection techniques;data mining techniques;feature selection methods
user requests;object oriented;web applications;web server;web servers
intrusion detection system;false positive;classification;false alarm;classifier;instances;intrusion detection systems;machine learning algorithms;classification rate;fold cross validation

frequent items;sliding window;tree structure;user defined;databases;real world applications;frequent pattern;market basket analysis
total order;complex systems
computing infrastructure;large-scale;commercial applications;formal methods;software systems;data flow;constraint based
database;databases
query specification;result sets;search logs;search session;result set
search queries;click logs;query terms;automatically generating;web services
behavioral patterns;query patterns;web search;search behaviour
topical relevance;user experience;web queries;web queries are;difficult queries;search engines;search behaviors;long queries;query logs;interactive search;search interface
news events;search engines;search engine;enabling users to;web search engines;result pages
auc;training data;text based;classification;imbalanced data;classification;vector machine;naïve bayes;web content;text classification;classification task
free-form;large number of;page content;high-frequency
web based;social web;meta-data;large numbers of;estimation process;location estimation;geographic information
retrieval effectiveness;relevant documents;structured document retrieval;relevance feedback;retrieval models;document collections;relevance model;applications involve;query terms
term weights;regularization;relevance score;trec collections;retrieval models;retrieval performance;collection statistics;semantic relations;query terms
semantic relations;community question answering;community question answering;answer questions;real-world
retrieval effectiveness;ad-hoc;text-based;semantic search;language modeling framework;entity search;keyword query;keyword search;entity retrieval;test collection
clustering;social media;result pages;search engine
theoretical analysis;average precision
search effectiveness;takes into account;speech data;speech retrieval;retrieval performance;search effectiveness;average precision;search tasks;test collection
search systems;document relevance;majority voting;relevance judgments;expectation maximization;information retrieval;ir systems;test collections;book search;probabilistic model
distinguishing features;heterogeneous sources;personalized recommendations;random walks
reference collection;selection problem;query expansion
web pages;accuracies;large number of;prediction method;feature sets;supervised learning
classification decisions;majority voting;machine learning techniques;vector-space model;data set;feature selection methods;supervised learning algorithms
training data;retrieval function;accurately predict;retrieval performance;term frequency;probabilistic retrieval model
multiple criteria;relevance ranking;linear transformation;linear combination;linear combination;information retrieval;multi-criteria
statistical translation;mutual information;translation probabilities;language models;language model;exact matching;training data;information retrieval;data sets;translation model;ad hoc information retrieval;estimation methods;estimation method
language pairs;cross-language;random variables;information retrieval;power law;retrieval model;4;query terms
large volume;trec data;applications including;ranked list;information filtering;score distributions;inference algorithm;expectation maximization;retrieval systems;recall-oriented;em algorithm;distributed ir
search results;portfolio theory;relevant documents;information retrieval;retrieval models;retrieval model;optimal set of
image retrieval;digital images;user interfaces;relevant objects;large collections of;image collection;user interface;user-friendly;content-based image retrieval
image retrieval;traditional information retrieval;text fragments;retrieval task;text retrieval;storage requirements;visual features
11;svm models;database;image datasets;content-based retrieval;linear combination;ranking models;test query;similarity functions;text retrieval;ranking model;training algorithm
clustering;clustering algorithms;language modelling;retrieval tasks;constrained clustering;language modelling;constrained clustering;objective functions;semi-supervised learning algorithms;relevance models;pseudo-relevance feedback;text clustering;pair-wise constraints;document representation
spam detection;social networking;social network;semi-supervised;link analysis;social networking
wikipedia-based;classification;classification;link-structure;short texts;feature-space;input text;latent semantic analysis;common practice;classification algorithm
clustering;machine translation;topic models;labeling problem;variational approach;cluster labeling;document corpora
query result caching;search results;search result pages;web search engines;search engine
intra-query;intra-query;query processing;text retrieval;parallel execution;concurrent execution
sentiment analysis
web search engine;input data;capacity planning;search engine;memory resources;web search engines
search results;social science;database;ir systems;search process;search queries;digital library;digital libraries;real world;user study;information retrieval systems
average precision;score distributions;score distribution;information retrieval
blog posts;high-quality;test collection;document collections;news-related
graph-based
sentiment analysis;web search queries
high-quality;classification;semi-automatic;document classification;semi-automatic
web documents;textual information;image indexing;summarization techniques;retrieval performance;retrieved documents;automatically generate
cross-lingual information retrieval;string matching;word segmentation
pre-retrieval;prediction methods;difficult queries;query difficulty;search engine;query performance
community detection;nodes represent
classification;expert search;manual construction of;topic hierarchies;domain-specific;pseudo-relevance feedback;query expansion;classifier
clustering;score function;data points;clustering algorithm;threshold-based;expectation-maximization
search results;web search;heterogeneous sources;web queries;1;2;search engine;user study
social media;textual features;prediction model
ensemble pruning;training classifiers;text categorization;real-life applications
multi-modal;social media;complex queries;relevance score;key-words;internet users;multi-relational;social networks;information access;social context
recommendation systems;search log;queries submitted to;web site;web site;search engine
weighted graph;document term;information retrieval;relevance feedback;pseudo-relevance feedback;temporal profiles;retrieval effectiveness
world wide web;web search;search interfaces;domain model;click data;user feedback;formal concept analysis;user clicks;domain models
query formulation;semantic information;user preference
specific queries;user feedback;relevance feedback;recall-oriented;retrieval systems;search tasks
keyword queries;ad-hoc;query sets;semantic search;complex systems;retrieval models;keyword search;semantic search;structured data;entity retrieval
result sets;retrieval evaluation;information space;information retrieval;retrieval techniques;personal information
graph analysis;high level;graph analysis;weighted graphs
document retrieval;retrieval performance;document length;term frequency;positive effect
xml keyword search
retrieving relevant;domain independent;customer reviews;language models
large number of;web information extraction;web information extraction;test instance;extraction accuracy;machine learning techniques;learning strategy
naive bayes;user interactions
image retrieval;user-generated;edge information;user interface;distance metric;sketch based
social media;real-time monitoring
multi-modal;end-user;community-based;rank documents;key words;search engine
information services
temporal information;search engine
relevance feedback;span multiple;information-seeking;faceted search;session-based
web corpus;domain-independent;high-quality;training examples;information extraction;common-sense knowledge;domain-specific;precision/recall;question-answering systems;information extraction;hand-labeled
probability theory;decision theory;artificial intelligence
decision making
domain knowledge;utility function;domain-independent;decision-making;computationally expensive;decision-theoretic framework
common knowledge
multiagent systems
np-complete;social welfare;np-hard
real world;social choice;game theory;game-theoretic;computational complexity
voting rule;computational complexity;np-hard
multi-agent;multiple agents;feature selection methods
voting rules;voting rule;strategy-proof;lower bounds
social welfare;preference relation
social choice
social choice;social sciences
social welfare;social networks;graph theoretic
multi-agent systems;large number of;large-scale
individual agents;multi-agent;multi-agent systems;mechanism design
fixed point;temporal logic
multi-agent systems;decision-theoretic model;multi-agent;decision-making;highly-dynamic;decision making
social choice
worst-case
mechanism design
multi-agent systems
controlled experiments;success rate
computationally feasible;rates
expected number of;uniform distribution;social choice;preference aggregation
information source;argumentation based;information sources
multi-agent;variable ordering;soft constraint;soft constraints
tesauro and bredin, 2002;vytelingum, 2006;das et al., 2001
solution concept;social choice
pre-defined;temporal data;action selection;agglomerative hierarchical clustering;cluster model;behavior patterns;instances;multi-robot;small size
argumentation-based;formal model for;assumption-based;assumption-based
preference aggregation;social choice;theoretical results
accurately reflect
coalition structure;computational complexity;interaction graphs
resource-bounded;probability distribution over;decision making
satisfiability problem;coalition logic;resource management;case study
computational complexity;communication) network;social network;5;social networks;resource selection
incomplete information;model checking;case studies
hazon and elkind, 2010;slinko and white, 2008;voting rules;scoring rules

worst-case;modern hardware;texas hold'em;evaluation criteria;game tree
sealed-bid;plans;individual preferences
game-theoretic;real-world;single target;security applications;normal-form;np-hard
worst-case;algorithms for computing;real-world data sets;voting rules;theoretical results
high levels of;fast algorithm;completeness;instance;instances;multiple agents
2009a;game theory;artificial intelligence
case-based reasoning;multi-agent system;machine learning
dynamic environments
obraztsova et al., 2011;social choice;computationally hard;finding an optimal;computational complexity;bartholdi et al., 1989;voting rule;scoring rules
state spaces;computationally intractable;wireless networks;optimization algorithm;partially observable markov decision processes;worst case;problem size;multiple agents;reward function;expectation-maximization
complex systems;abstraction level;multi-scale
multi-agent systems;sandholm et al., 1999;worst-case;coalition structure generation;coalition structure;exhaustive search
performance metric;training data;intelligent agents;texas hold'em
problem instances

heuristic search;poor performance
computationally expensive;multi-agent systems;reinforcement learning;conflict resolution
1981;social welfare;1997;np-hard
mechanism design
concise representation;multi-agent systems;black-box;computationally intractable;ai research;coalition structure generation;representation scheme
statistical analysis;context awareness

evolutionary approach;agent-based
convergence rates;social network;special attention;learning problems
agent systems;agent systems;plans
gaussian processes
utility function;planning algorithm;ad hoc;monte-carlo tree search
partial orders;computational complexity;social choice;voting rules;maximum likelihood;rule) based;scoring rules
high probability;monte-carlo;error bounds;normal form;importance sampling
human behavior;real-world;human subjects;resource allocation;algorithms for computing
algorithm called;continuous state;temporal constraints;locally optimal
graph theory;temporal constraints;mechanism design
multi-agent;complex tasks;task allocation;task-allocation;solving complex
systems require;plans;multi-agent;plan recognition;real-world domains;multi-agent;plan recognition;intelligent agents
takes into account;weighted voting;zuckerman et al., 2008
constraint satisfaction;ai techniques;special case;general purpose
boolean satisfiability;key features;completeness;stochastic local search;real-world
constraint satisfaction problem;constraint language;description logics;knowledge representation;constraint satisfaction problems;artificial intelligence;np-hard
constraint programming;search tree
domain size;real-world;constraint satisfaction problems;wide range;np-complete;graph coloring;constraint satisfaction problems;combinatorial problems
phase transition;column generation;np-complete problem;instances;logic-based;normal form
local structure;similarity metric;function approximation
instance;domain values;global constraints;bound consistency;global constraint

search algorithms;search strategies
upper bounds;decision makers;decision problem;tracking problem
strong evidence;optimization problem;minimization problem
instances
search space;search algorithms;theoretical properties;multiple times;heuristic function;search algorithm;heuristic search;search problems
sequence alignment;search effort;large number of;multi-core
number partitioning
search space;search procedure;constraint programming;constraint programming;real life;data streams;constraint satisfaction problems
satisfiability problem;optimization problems;instance;specific algorithms;maximum number of
expected number of;total number of;random instances;power law;high probability;random graphs;constraint satisfaction problems
post processing;perfect information;performance gains
algorithm called;constraint propagation;general-purpose;general purpose
scheduling problems;scheduling problem;instances
limited memory;markov models;long-range;constraint satisfaction;heuristic search;markov model
space complexity
human-generated;test problems;optimization problems;instances;domain-specific;monte carlo tree search;monte carlo search;search efficiency
constraint satisfaction problem;large-scale;monte-carlo;evaluation function;constraint propagation;game tree;search algorithm;game tree search;search technique
single-agent search;search tree;multi-agent;optimal path;tree search;multiple agents;search algorithm
multiple agents;completeness
benchmark domains;search algorithms
case study;search effort;search algorithm;computational overhead
katsirelos et al., 2010;standard benchmarks;performance gains
approximate algorithm;number partitioning;skewed;heuristic algorithms;uniformly distributed;np-hard
multi-dimensional;heuristic approaches;free space
desirable properties;inconsistent information
bounded treewidth;conjunctive query;data complexity;combined complexity;worst-case optimal;generic algorithm;tuple-generating dependencies
regular path queries;query language;reasoning problems;logic-based;operator;query containment;xml schema
stable model semantics
real-world;model-based diagnosis;real-world applications
service oriented;model checking
query evaluation;answering queries;action theories;knowledge bases;decision procedure for
interval-based;uncertain information;knowledge bases;0,1;computational complexity;computational costs;interval-based;knowledge base
bounded treewidth;spatial reasoning
complexity bounds
multi-valued;general concept;description logics;application domains;description logic
operator
formally defined
instances;knowledge sources;multi-context systems;multi-context systems;knowledge bases

worst-case optimal;description logics;description logics;theoretical foundations
knowledge bases;regular path queries;conjunctive queries;domain constraints;description logic;databases;query containment;query languages;upper bound
structured information;description logics;description logics;knowledge bases
operator;decision diagram;knowledge bases
completeness;knowledge bases;database;data exchange;incomplete information;incomplete knowledge;databases;relational databases;normal form;data integration;open-world
belief state
sheds light on;classical logic;belief change;logic programming;description logic;databases
complete set of;complete classification

finite model;spatial reasoning
search space;constraint satisfaction;answer-set;theoretical results;instances;small sets of;answer-set programming;propositional satisfiability
search space
description logics

models learned;knowledge integration;parameter settings;computational model;question-answering;human learning
low complexity;description logics
error-prone;high-level;unknown environments;sensing actions;real-world applications
qualitative reasoning
probability distributions;probabilistic information;probabilistic logic;nonmonotonic reasoning
plans;finite-state
logic programming;vice versa;mathematical model of
scheduling problems;computational complexity;temporal reasoning;physical systems;problems arise;temporal constraints;constraint reasoning;np-hard;temporal reasoning;natural language understanding
operator;combines ideas from;nonmonotonic reasoning
conditional probability;temporal data;time series;continuous variables
operator;belief base;distance-based;merging operators
closed sets;np-complete;euclidean spaces;low-dimensional
query answering;tuple-generating dependencies
lower-level;context-sensitive;regular expressions;context-sensitive;context-free;discrete-event systems;normal behavior
multi-agent;situation calculus;action theories
model-theoretic;decision problems;wide range;description logic;description logic
model-theoretic;complexity bounds;description logics;decision procedures;description logic
knowledge compilation;v, ∃;∃;target language;v
operator;case study;computational complexity;artificial intelligence;merging operators
multi-agent systems;completeness
autonomous agents;learning task;formal framework
obtained by applying;classical logic
low-quality;quality control;naïve;knowledge bases
generally applicable;computational cost
high computational complexity;completeness;description logics;regular path queries;conjunctive queries;description logics;data complexity;combined complexity;query answering;query answering
counterpart;logic based
query execution;query rewriting;knowledge bases;optimization framework;database;instance;knowledge base;plans
instance;conjunctive query;high computational complexity;description logic;description logic
logic programming;computational methods
logic programming;incomplete information;representation language;knowledge bases;answer set semantics
description logic;generic framework;real-life;description logics;description logics
fages, 1994;description logic;answer sets;answer set semantics
hierarchical approach;search algorithm
worst-case;goal-directed;ontology languages;classification;based reasoning
finite structures;complete classification;description logics;classical logic
description language;description language;complete information;rule-based;originally designed;high-level;human intervention

computational complexity;agent model;navigation systems
answer set;reasoning problems;finite structures;stable model semantics
alchourrón et al., 1985;delgrande and wassermann, 2010
clustering;running times;linkage-based;hierarchical algorithm;input-output;2;algorithms produce;clustering algorithm;hierarchical clustering methods
function approximation;gaussian mixture model;reinforcement learning;input-output;input space;probability density
belief propagation;kalman filter;queries efficiently;graphical model
data-driven;term space;information retrieval;approximation technique;query expansion;huge number of
data distributions;mixture model;semi-supervised learning;translation model;classification model;logistic regression;semi-supervised learning;real-world data sets;labeled and unlabeled data;labeled data;classifier
domain knowledge;inference method;latent dirichlet allocation;lda) model;domain knowledge into;stochastic gradient descent;topic models;topic modeling;markov logic;latent dirichlet allocation
state space;user preferences;auxiliary;image datasets;text documents;topic models;topic models;wide range;large datasets;gibbs sampling;local minima;inference techniques
training set;learned metric;loss function;counterpart;instance;distance metric;learning framework
baum-welch;fixed length;amino acids;sampling algorithm;information contained in;fixed-length;machine learning;inference methods;hidden markov model;model parameters;hidden markov model;sequence classification;inference algorithms;sequence classification
evolutionary algorithms;evolutionary algorithms;heuristic approaches;multi-objective;multi-objective optimization;problems arise;evolutionary algorithm;multi-objective optimization
training data;test data;distance metrics;distance metric learning;real world datasets;training and test data;sampling method;machine learning;distance-metric learning;metric learning;fundamental problem;learning framework;convex optimization problem;distance metric learning
case-based reasoning;neural networks;learning performance;ai techniques;reinforcement learning;reinforcement learning;reinforcement learning algorithm;case-base;learning problem;transfer learning;neural network
simulated annealing;genetic algorithms;social networks;real datasets;increasingly popular
human effort;high-quality;concept-class;text categorization;supervision;classification models;multi-task learning;text classifiers;semi-supervised
speech recognition;fixed length;unlabelled data;unsupervised learning;unsupervised learning;data sequences;real-world applications;edit distance;data stream;recognition systems;data streams;text classification;benchmark datasets;pre-processing;learning method
error rates;image classification;neural networks;object classification;neural network
state spaces;training examples;decision-making;reinforcement learning;state abstraction;complex tasks;supervised learning
markov logic networks;database;markov logic networks;structure learning;relational information;algorithm relies on
decision rules;data mining tasks;stream mining;rule sets;decision rules;data streams;interpretability
search space
classification;kernel function;dirichlet process;general case;data sets;gaussian processes;computationally expensive;case-study
signal processing;dual problem;convergence rate;quadratic programming;machine learning;sparse representations;algorithm called;computational cost;real-world data sets
ensemble learning;kernel-based;learning approaches;learning algorithms;streaming data;data sets;kernel methods;structured data;kernel-based
clustering;cluster assignment;optimization problem;geometric structure;document clustering;clustering methods;text data sets;graph regularization;nonnegative matrix (tri-)factorization
face recognition;relevant features;machine learning;optimization algorithms;data sets;subspace learning;feature selection;learning problem;dimensionality reduction;dimensionality reduction;projection matrix
effectively exploit;classification performance;multilabel classification;graphical model;multi-label classification;gibbs sampling;binary classifiers;network model;model training
clustering;multi-dimensional;temporal intervals;temporal patterns;clustering method;sequential patterns;interval-based;simulated data;em algorithm;extraction methods
fast approximate;nearest-neighbor search;hill-climbing;nearest neighbor;computational complexity;search algorithm;theoretical guarantees
decision rule;time series;real-world
regularization;unsupervised feature selection;feature ranking;locally linear;graph laplacian;feature selection;feature selection methods;sparse matrix;feature selection
statistical approaches;classification problem;classification;rule learning;rule-based;learning algorithm;regression problem;target variable;heuristic algorithm
high-dimensional;accurate models;binary data;costly process;statistical model;sensor networks;sensor network;handwritten digits;em algorithm
formal concept analysis;closed patterns;real-world;pattern mining;formal concept analysis;numerical data;numerical attributes
activity recognition;finite state machine;multiple sequence alignment;instances;high accuracy;finite state
high-dimensional;principal components analysis;learning agents;unlike standard;position information;feature analysis;feature analysis;learning framework
data objects;similar objects;hash functions;high dimensional;large databases;similarity search;information access;search problems;hash function
community detection;np-hard;measure called;community structure
kernel-based;classification;probabilistic model;model parameters;linear classifiers;feature selection
matrix factorization;data points;linear combination;basis vectors;data representation;data clustering;matrix factorization;sparse representation;learning problems;factorization method;dimension reduction;real world applications
clustering;singular value decomposition;kernel machines;large-scale;low rank;low-rank;matrix factorization;approximation method;support vector machines
training set;synthetic data;large number of;information retrieval tasks;multimedia retrieval;huge number of;document pairs;ranking methods;support vector;approximation algorithm;video retrieval;support vector machines;margin-based;objective function;support vectors
ensemble learning;classification accuracy;dimensional euclidean space;generative process
probabilistic model;activity recognition;conditional random fields;spatio-temporal
multi-kernel;multi-task;covariance functions;real datasets;machine learning;multi-task learning;gaussian processes
regression trees;cost function;gradient boosting;imitation learning;imitation learning;supervised learning;relational domains
classification problem;accurate classifiers;classification;learning algorithm;learning algorithms;time series;effectively identify;real-world applications;instances;training instances;application domains;unlabeled instances;classification algorithms
classification;classification tasks;data distributions;real-life;binary classifiers;covariance matrix;classification task
greedy strategy;high computational complexity;large scale;principal component analysis;real world datasets;principal component analysis;dimensional data;high dimensionality;efficient optimization
data point;prediction accuracy;classification models;base classifiers;bayesian belief;weighting schemes;microarray datasets;single classifier;bayesian belief
large numbers of;simulation model;real-world;state space;reinforcement learning;markov decision processes;state representation;autonomous agents;problem domain;learning strategies
long-term;learning task;learning systems;reinforcement learning;complex tasks
spam detection;domain adaptation;domain adaptation;ensemble classifiers;cross-domain;multiple classifiers;sentiment classification;theoretical analysis;classifier
probabilistic framework;additive noise;instances;random walk;higher classification accuracy;time series;generative models;knowledge discovery;linear transformations;unsupervised algorithm;real-world data sets;projection based
probabilistic modeling;hidden variable;bayesian networks;bayesian inference;logic-based;modeling language;marginal probability
data-driven;data-driven;fold cross-validation;classification;physics-based;network motifs;spatio-temporal;physical systems;dynamic networks;dynamic behavior;complex data;human beings;network structures;network model;model building
information gathering;probabilistic approach;partial knowledge;data acquisition;accurately identify
instances;real world;fast algorithm;feature selection;large datasets
memory footprint;training set size;classification;classification accuracy;active learning framework;text classification;classification approach;information theoretic;classification algorithms
pattern recognition;similarity matrices;kernel matrices;data clustering;data structures;vector data;graph data;theoretical analysis;cosine similarity;dimensionality reduction
detection performance;anomaly detection;detection accuracy;streaming data;tree structure;parameter settings;memory requirement;evolving data streams
multi-strategy;data analysis;utility theory;real world;limited resources;learning method
learning algorithms;training samples;synthetic data;unsupervised learning;natural language
pattern recognition;real-world;learning problems;classifier;learning models;semi-supervised classification
instance;domain adaptation;learning algorithms;cross-language;instances;human efforts;machine learning methods;real-world applications;classification performance;text classification;labeled data;feature sets;text classification;feature spaces;transfer learning;text classification problems;data mining methods
feature space;domain adaptation;common features;target domain;source domains;instances;labeled data from;feature spaces;pre-processing
multiple classes;discriminative features;input features;lower dimensional space;linear discriminant analysis;data instances;labeled and unlabeled data;automatically learn
clustering;nonnegative matrix factorization;data points;input data;large-scale;optimization problem;factor matrices;cluster labels;clustering performance;feature spaces;increasing attention;nonnegative matrix tri-factorization
clustering;structural consistency;real-world;spectral methods;data sets;structural consistency;neighborhood graph;structural information
hill-climbing;search algorithm;monte carlo;unsupervised learning;partially observable;action sequences;meta search;prior knowledge;algorithm learns;continuous-state;policy search;markov chain;finite state;latent variables
ground truth;learning approaches;information retrieval;instance;real-world;maximum likelihood;ranking model;training sample
real-world datasets;supervised learning methods;accurate classifier;training phase;svm-based;learning phase;negative examples;similarity-based;similarity-based;unlabelled data;classifier;positive class;positive examples
nearest neighbor algorithm;real world datasets;negative examples;instance;single-label;multi-label data;multi-label;classification approach;stream classification
discriminative features;label information;unsupervised learning;class label;feature selection algorithms;linear classifier;data types;feature selection;feature subset;unsupervised feature selection;manifold structure;input data;supervised learning;feature set;data distribution
numerical experiments;matrix factorization;random projections;matrix factorization methods;factorization method;compressed data
regularization;ensemble methods;learning approaches;mathematical programming;ensemble methods;theoretical analysis;generalization ability;support vector machines;space complexity
clustering results;specific features;feature set;instance;multi-label learning;class label;learning algorithms;algorithm named;multi-label learning;data sets;feature sets;specific characteristics;multi-label;class labels;clustering analysis;instances
data point;multi-kernel;multi-label learning;margin;real datasets;web pages;multi-label learning;training samples;multi-kernel;inter-related;max-margin;classifier training
bayesian model;classification;large-scale;classification framework;probabilistic models;decision theory;error rate;model called
activity models;pruning methods;probability threshold;video surveillance;video segments
regulatory networks;computational biology;cost-based;multiple data sources;data sources;cost-based;fundamental problem;multiple sources;bayesian inference;biological data
ai technologies
instance;real-world;fine-grained;gaussian processes;high-level;low-level
temporal relations;observed data;real-world scenarios;higher-order;agent model;online learning;real-world applications;agent architecture;knowledge representation;bayesian inference
model checking;fault tolerance
vision applications;moving object;scene analysis;spatial reasoning;multiple object tracking;formal model
supervised learning techniques;predictive accuracy;semi-supervised learning;design space;build models;model trees;squared error;high accuracy;design space;supervised learning;limited resources
eye movements;hidden markov models;hidden markov models;movement patterns;probabilistic framework
cognitive processes;pattern recognition;classification;game theory
minimum description length;minimum description length;query refinement;user query;databases
agent model;mental models
social network analysis;latent factor models;real-world networks;social networks;latent factor model;network structure;model parameters
spatial correlations;remote sensing;spatio-temporal;multiple attributes;multiple sites;water distribution;remote sensing data;supervised learning;missing values;missing attribute values;prediction models
domain knowledge;ai techniques;theoretical framework;case-based reasoning
multiple models;label noise
domain knowledge;machine learning methods;ubiquitous computing;context-aware;activity recognition;feature extraction;data representation;application domains;raw data;recognition tasks;distribution function
agent model;agent-based
agent model;social context;action based
problems arise;network security;classification
labeled data
visual similarity;language pairs;edit distance;visual information;web images;string similarity;visual features;physical objects
semi-structured;related concepts;link structure
short text;mining algorithms;latent dirichlet allocation;classification;short text;topic models;machine learning;data set;multi-granularity;feature spaces;contextual information;latent topics;classification error
semantic role labeling;knowledge embedded in;feature representation;latent structure;internal structure;optimization techniques;classifier
general case;temporal intervals;temporal information;optimization problem;integer linear programming;bramsen et al., 2006; chambers and jurafsky, 2008;temporal reasoning
learning process;decision function;human learning;instances;learning algorithm;supervision;machine learning;domain expertise;labeled examples;machine learning;natural language
clustering;word senses;natural language
temporal order;similarity metrics;graph-theoretic
tree structure;kernel-based
review text;rating prediction;text based;real-world;data sparsity;product information;sentiment analysis;text features;classification problems;learning framework
random subspaces;dynamically generated;semi-supervised learning;semi-supervised learning methods;labeled data;sentiment classification;semi-supervised learning;labeled and unlabeled data;class distribution
clustering;semantic role labeling;sentiment analysis;information extraction;statistical information;fine-grained;wide range;semantic role labeling;occur frequently
machine translation;semantic role labeling;low-cost;knowledge representations;objective functions;semi-automatic
optimization approach;machine translation;rule-based;semantic relatedness;context based
semantic relations;question-answering;question-answering
hypothesis space;learning algorithms
growing number of;unseen data;automatic evaluation;concept hierarchy;topic models;fitness
algorithm learns;graph-based;high-quality;learning approaches
machine learning methods;policy learning;learning algorithms;reinforcement learning;cross-domain;reinforcement learning algorithm;natural language processing;optimal strategy
clustering algorithms;data sets;learning algorithms;training sets
input text;relation extraction
wikipedia-based;wikipedia-based;wikipedia categories;topic models;topic model;category structure;filtering methods;reference point;question answering;knowledge base
user interfaces;context-based
instance selection;diverse set of;rule-based;entity linking;instances;topic modeling;automatically generated;entity linking;supervised learning algorithm;knowledge base
public data;semantic network;large-scale;positive results;large number of;inter-related;automatically generated;query translation;cross-language information retrieval;parallel corpora
multiple features;heuristic rules;statistics-based;statistical information;data sets;retrieval performance;supervised learning;rule-based
markov decision processes;running times;optimal policies;hierarchical model
classical planning;planning problems;state-space;action sequences;keeping track of;belief space;partial observability;search problem
markov decision processes;resource allocation;closed form solution
planning problems;state space
ordering constraints;complexity bound;ai planning;hierarchical task
sensor readings;training data;machine learning methods;recognition task;activity recognition;large number of;automatically learning;instances;transfer-learning;human activities;manual labeling;source domain;learning framework;recognition problem;transfer learning;activity recognition;labeled training
point-based;point-based;linear program;partially observable markov decision processes;dynamic programming;requires solving;optimal policies;multiple objectives
ordering constraints;decision diagram;planning domains;plans;partial-order;action selection;partial-order
fine-grained;benchmark domains;optimal planning;information loss
scheduling problem;optimization algorithm;constraint-based;heuristic algorithm;scheduling problem
standard benchmarks;constraint-based
classical planning;classical planning;planning domain;temporal logics;plans
plan recognition;plans;pomdp model;partial observations;goal recognition;plan generation;probability distribution over;partially observable;action sequences;plan recognition
planning algorithms;state-space search
belief state;domain size;sensing actions;partial information;planning problem
heuristic functions;partially observable markov decision process;multiagent systems;heuristic search;efficient representation
heuristic function;to et al., 2011;to et al., 2010;competitive performance;search algorithm;sensing actions
planning problems;instance;branching factor;state spaces;state variables
process model;state transitions;markov decision;markov decision processes;decision problems;decision making;np-hard
current location
web image;classification accuracy;quality assessment;search applications;stereo vision;level features
human behavior;simulation analysis;plan execution;human-robot
path planning;training set;training images;training phase;sparse graphs;learning algorithm;data collection;object detection;detection performance;object detection;classifier
euclidean structure;real data;general form
indoor environment;sensor readings;aspect model;recognition task;activity recognition;data sparsity;data set;target user;sensor data;real-world;real world;labeled data;activity recognition;similar users
large numbers of;random variables;relational models;state variables;state transition;dynamic systems;algorithm takes;kalman filter;filtering algorithm
naive bayes;problem remains;bounded treewidth;bayesian networks;bounded number of
maximum likelihood;squared error;dirichlet prior;dirichlet distribution;times faster than
bayesian model;real-world scenarios;sequential monte carlo;inference mechanism;computational resources;prediction error;prior information;action recognition
decision tree;1989;general case;decision problems;expected utility;optimal strategy;decision making
cost functions;graphical models;constraint satisfaction problem;combinatorial optimization;belief networks;joint distribution;cost function;additional information
real-world;game theory;detection performance;sensor network
formal models;update rule;graphical model;expectation-maximization;probabilistic inference
historical information;distributed settings;intelligent agent;synthetic data;dynamic behavior
log-linear;description logics;knowledge representation and reasoning;log-linear
markov decision processes;reward functions;reward function;markov decision process
markov decision processes;reward function;error bounds
statistical estimation;constraint satisfaction problems;random variables
knowledge compilation;model theoretic;logical structure;knowledge representation;context-specific;probabilistic inference;normal form;inference algorithms
bayesian network;algorithm learns;solution space;shortest path;bayesian networks;benchmark datasets;search algorithm
bayesian network;classification;conditional independence;dependency relations;naïve bayes;multidimensional datasets;instance;bayesian classifiers;bayesian networks;classifier
cold start;user profile;success rate;collaborative filtering;similarity measure;similar users
lower-dimensional;training data;relation extraction;instances;supervision;information extraction;instance;weakly-supervised;classifier
unseen data;unlabeled data;labeled data is;record linkage;real-world;learning problem;candidate matches;unlabeled) data;large data sets;data sets;labeled data;matching records;record linkage is
web-based;structural properties;community detection;iterative algorithm;application domains;quadratic program;community detection;social networks;optimization problem;complex networks;network structure;convex formulation;community structure
completeness;semantic web applications;semantic web;input query;query answering;highly scalable
instance;logical properties;efficiently extract
algorithm iteratively;fast algorithm;large datasets;data points;propagation algorithm;high quality;clustering method;real-valued;clustering result;clustering performance;clustering methods
web-sites;stock market;kullback-leibler;power-law
search engines;relevance feedback;web search;real-life;structured data from;semantic web;relevance feedback;pseudo-relevance feedback;semantic web;query log;relevance model;real-world;web-pages;web search;structured data;web data
music information retrieval;recommendation quality;music recommendation;recommender systems;cold-start;latent semantic analysis;similarity-based
topic model;visual features
real-world;social networks;large-scale
citation network;web-logs;topic models;evaluation criteria;context sensitive;baseline models;link prediction
single view;multi-perspective;multi-perspective;diverse sources;multi-resolution;natural language processing techniques;text processing;higher level
social networks
rating matrix;real-world scenarios;collaborative filtering;closely related;cross-domain;cross-domain;user interests;latent factor model;gibbs sampling;collaborative filtering
completeness;cognitive load;recommender systems
user similarity;user interests;collaborative filtering;recommendation accuracy
instance;linked data;knowledge bases;large-scale;link discovery;large data sets;large number of;link discovery;semantic web;metric spaces;synthetic data
data source;collaborative filtering;recommender systems;latent space;data sparsity;rating matrix;data sparsity;matrix factorization;knowledge transfer;preference data;transfer learning;auxiliary
information sources;real-world;information extraction;additional information;information retrieval
clustering;statistical methods;mining results;text understanding;short text;clustering accuracy;mining process;inference mechanism;topic detection;topic modeling;short texts;latent semantic;short text;interpretability;knowledge-bases;mining tasks
information diffusion;graph model;real-world;wikipedia based;topic model;semantic graph;edit distance;topic tracking;blog posts;clustering algorithm;semantic relatedness;blog data
real world data sets;concept hierarchy;matching method
real-world;synthetic data;social networks
real-world datasets;target task;target domain;learning approaches;source domains;source data;learning framework;web scale;information sources;transfer learning;labeled training data;large number of;source-selection;text classification;transfer learning;highly scalable;information source;world wide web;online social media;source-selection;knowledge transfer;learning task;auxiliary
regression analysis;related queries;user behavior;log analysis;search queries;search engines;behavior analysis;search logs
ranking algorithm;web browser;commercial search engines;personalized web search;document level
document summarization;word sense disambiguation
recommender systems;recommender systems;online reviews;probabilistic model;learning algorithm;logistic regression;web data
ai techniques;decision-making;making decisions;interactive learning
recognition algorithm;taking into account;plans;plan recognition;data obtained from;high-level;plan recognition
case-studies;based reasoning;real-world;plan generation;reasoning capabilities;formal model
domain knowledge;state space;monte-carlo search;search algorithm for;monte-carlo;decision-making;linear regression;neural network;state-action;external sources;automatically extract;monte-carlo tree search;linguistic knowledge
data fusion;ai techniques;data sets;spatial location;data sources;sensor data;takes place;automatically generate
search results;semantic annotation;false positive;joint model;named entity;natural language processing;markov logic;artificial intelligence;additional information
data-intensive;data mining approach;huge number of;data mining
correct answers;real-world;machine learning;knowledge representation;term-based;natural language processing;question answering;classifier;artificial intelligence;natural language
sketch recognition;correct answer;sketch recognition
decision-theoretic;trade-offs;relational model;real-world;common-sense knowledge;goal-directed;search tasks
domain knowledge;case-based reasoning;learning algorithm;reinforcement learning;video game
million images;chen et al., 2010;large margin;chen et al., 2009;prior distribution;entropy" based;stauffer et al., 2000;chandrasekhar et al., 2009b;chandrasekhar et al., 2009a;gaussian mixture model;locally optimal
higher accuracy;hierarchical classification;great promise for;search engine;topic hierarchy;user interface;search engines;search engine;keyword search;user study
knowledge bases;online users;domain specific;intelligent systems;knowledge acquisition;similarity-based;resource-bounded;resource constraints;knowledge base
multi-agent;computationally feasible;plan recognition;domain-specific
privacy guarantees;private information;ai techniques;cryptographic techniques;service providers;privacy-preserving
unlabeled data;machine learning methods;machine learning methods to;heuristic rules;product review;sentiment analysis;semi-supervised;opinion mining
mathematical formulation;customer satisfaction;customer segmentation;scheduling problem
real data sets;graphical models;conditional random field;spatial relationships;ensemble classifiers;mining process;classification performance;graphical models;inference methods;decision trees;sensor measurements;exact inference;class labels;local information
high-dimensional;social media;users' interests;sparse data;user actions;real-world;graph-based;fundamental properties;tensor analysis;user interests;social graphs;social media data;bipartite graph;graph structure;prediction task;relational data;graph representation
human users;agent architecture
data mining techniques;large scale;free-text;knowledge acquisition;natural language processing;data mining;user modeling
confidence measure;energy management;plan selection;programming paradigm
agent based;simulation framework;agent-based;negative feedback
face images;learning process;verification problem;database;impact factors;images captured;transfer learning;learning method
classification;applications including;decision tree;mobile-phone;activity recognition;decision tree;daily activities;data set;activity-recognition;great potential;real-world;clustering algorithm;transfer learning;activity recognition;recognition problem
specific features
hybrid model;semi-supervised learning;sample points;machine learning
rule-based;learning systems;machine learning;low-level;dynamic environment;unlike prior;health information
lower bound;clause learning;lower bounds;clause learning;lower bounds;propositional satisfiability
detection problem;multiple communities;loss function;community detection;social network;social networks;game-theoretic;overlapping communities;community structure
domain knowledge;case-based reasoning;description logic
protein structures;constraint logic programming;constraint satisfaction problem;additional knowledge;database
answer set;local consistency;constraint satisfaction problems;answer set programming
desirable properties
state space;monte carlo;instances;model counting;real-world;instance;markov chain;algorithm to compute;combinatorial problems
machine translation;generated automatically;statistical machine translation;mobile devices;basic concepts;user interface;source language
growing number of;power supply;mobile devices
answer set programming;answer set;answer set programming;finite model;finite model;domain size;finite model
multi-dimensional;inconsistent information;artificial intelligence;trade-offs
planning problems;instances;situation calculus;plans;theoretical result
cold start;recommender systems;recommender systems;collaborative filtering;social network analysis;social network;social relations;recommendation accuracy;matrix factorization;social networks;real life data sets;social sciences;online social networks
reinforcement learning;reinforcement learning;plans
accessing data;query rewriting;information systems;description logics;ontology-based;data access;databases;relational databases
ontology-based;intelligent agents;based reasoning;multi-agent;semantic web;semantic web;rule set
rule-based;upper bounds;reasoning tasks;description logic;rule-based;main features
tree matching;prediction techniques;matching algorithms;labeled trees;np-complete;tree-matching;problems arise;approximation algorithm
case-based reasoning;retrieving relevant;reinforcement learning;adaptation knowledge;web mining;selection methods;knowledge acquisition;problem-solving;knowledge sources
ranking results;recommender systems;recommender systems;data sets;missing data;statistical models;model estimation;individual users
clustering;clustering results;data mining;generalization performance;specific context;spectral methods;learning algorithms;semi-supervised learning algorithms;supervised methods;semi-supervised clustering;supervision;laplacian matrix;semi-supervised learning;high level;semi-supervised;text clustering
bartholdi et al., 1989;utility function;scoring rules;voting rules;1989
response times
end-users;game-theoretic;application called;game-theoretic approach;large number of;united states;knowledge acquisition;security applications;limited resources;game theory;general purpose
rule-based
latent space;adamic and adar, 2003;prediction quality;social graphs;online social network;graph models;generation process;link prediction;metric space;link-prediction;theoretical justification
user profiles;obtained by combining;individual users;human activities;large-scale
efficient algorithms to;user feedback;helping users;large datasets;user studies;web users
loss function;classification;decision boundary;algorithm runs in;support vector machines;machine learning algorithms;classifier
gaussian process regression;data compression;synthetic data
plans;dynamic environments;higher-level;machine learning;object detection;detection performance;object detectors
social relationships;social science;vice versa;social network analysis;content analysis;semantic web;knowledge representation;social networks;artificial intelligence
ranked list;image annotation;baseline methods;image annotation;low-dimensional
review text;user-generated;online reviews;tf-idf;word pairs;extraction methods;scoring methods
document classification;large-scale;memory capacity;parameter selection;training process;data set;linear classification;natural language processing;linear classification;training methods
argumentation-based;decision support;decision support;decision making;reasoning patterns
multi-robot;sampling strategies;large-scale
probabilistic models;human operators;robotic systems;robotic systems;human robot;human operators;long term;human motion;artificial intelligence
protein structure;computational methods;ai techniques;machine learning;amino acid;protein structures;protein sequences;optimization techniques;protein structure
virtual organizations;virtual organizations
human subjects;social network;social networks;problem solving;problem-solving;artificial intelligence;artificial agents
extended abstract;decision-making;texas hold'em
handle complex;model checking;model checking
constraint optimization problems;constraint optimization problems;search algorithms;distributed search
supply chain management;multi-agent systems;decision model
multi-agent systems
search algorithms
unified framework
knowledge bases;partial information;context-sensitive;knowledge integration;real world applications;data management systems;decision making
real world and synthetic datasets;uncertain information;data repository;multiple sources;query performance;performance degradation;semantic web;knowledge base;key features;queries involving;knowledge base
multi-agent;argumentation-based;computational framework
behavior analysis;high level;video data;behavior analysis;feature extraction;behavior analysis;low level
multi-agent;constraint optimization problems;multi-agent systems;wide range;problem domains
plan recognition;activity recognition;probabilistic logic;reasoning tasks;natural language understanding
partially observable;stochastic domains;programming language
multi-agent systems;computational models;agent-based;agent-based
cognitive architecture;complex tasks
local geometry
search algorithms;heuristic search;additional information;heuristic search;search problems
structural information;information gathered;description logics;automatically generated
mechanism design;dynamic environments;mechanism design
sampling framework;spectral clustering;global model;accurate models;cluster structure;sample size;clustering problem;network resources;takes into account;clustering model;learning algorithms;statistical estimation;data mining research;network nodes;distributed clustering;clustering framework;data mining;sampling framework;clustering algorithm;objective function;data storage
similar features;high-quality;rates
transition matrix;social network analysis;evolutionary clustering;generative models;generative models;hierarchical dirichlet process;dirichlet process mixture;nonparametric bayesian;evolutionary clustering;automatically learning
training data;maximum likelihood estimation;maximum entropy;hidden variables;efficient approximation;maximum entropy model;data samples;optimization problem;log-linear;maximum entropy;log-linear models;maximum likelihood;special case;expectation-maximization;latent variable models;latent variables
clustering;clustering algorithms;target task;similarity measure;source task;wide range;cross-domain;supervision;parameter settings;semi-supervised clustering;real-world and synthetic datasets;supervision;clustering algorithm;data characteristics;clustering accuracy;clustering
relational database management system;database management systems
online social networks;sharing information;interesting patterns;databases;linguistic features;online communities;huge number of;exploratory analysis
prediction accuracy;social network;real life;social networks;bipartite graph;structural information;social graph
ad-hoc;social networking;social networking;user-defined;building-block
data structure;social networks
accuracy compared to;gaussian mixture model;feature vector;gaussian distributions;uncertain data;stock market;probability density function;efficient similarity search;similarity search;medical imaging;gaussian mixture models;similarity queries;sensor networks;uncertain data;efficient similarity search
multi-user;resource consumption;execution model;map-reduce;data set;exploratory data analysis;rates;fixed-size
labeling scheme;labeling scheme;storage cost;xml data;positive results;tree-structured data;labeling schemes;labeling schemes;interval-based;query processing;ordered trees;query answering
database;approximate matching;dictionary-based;entity extraction;edit-distance;dictionary-based;distance constraints;distance constraints;entity extraction
key features;database systems;application level;application-level;energy efficient;data centers;energy efficient;data intensive applications;data intensive applications;storage management;storage management;storing data;data intensive
query-plan;specialized algorithms;real-world applications;data stream;data stream management system;data streams;operator;multiple sources;high availability;input streams
selection predicates;streaming algorithms;storage schemes;aggregate query;memory requirements;data items;linear space;lower bounds;data stream;large datasets;lower bound;selection predicate
database systems;end users;database;decision-making;probability distributions;databases;query results;hypothesis testing;data samples
edit distance;complex structure;high computational complexity;database size;real datasets;search strategy;indexing method;similarity search;query processing
object based;outlier ranking;density-based;synthetic data;search techniques;outlier ranking;outlier mining;local neighborhood;density-based;dimensionality reduction techniques;random projections;ranking algorithms;density-based;data analysis;ranking methods;pre-processing
real world datasets;event detection;topological structure;large graphs;simple algorithm;dynamic graphs
text mining;clustering;dimensionality reduction;search algorithm;nearest neighbor;instance-based;search algorithm for;dimensionality) reduction;text documents;classification;early stage;lower bounds;instance-level;text document;similarity search;dimensionality reduction;reduction techniques;document databases;high-dimensional;dimensionality reduction;document databases
pair-wise;record pairs;window size;duplicate detection;similarity measure;real-world;data set;data sets;high volume;high similarity;duplicate detection
query evaluation;high degree of;schema matching;query performance;evaluating queries;machine learning techniques;database schemas;algorithm to compute
clustering;data structure;clustering results;low cost;synthetic datasets;higher precision;spatio-temporal;broad applications;data stream;computational overhead;data processing;data collected;object tracking
search space;mining algorithm;graph database;graph indexing;database;frequent subgraph;search problem;index structure;objective function;subgraph features;times faster than;iterative algorithm;mining" algorithm;search algorithm;graph databases;graph indexing;feature set
real-world and synthetic datasets;query formulation;data structure called;visual query;algorithm called;query modification;similarity queries;query processing;query formulation;8;query results;query processing
classification;efficient query evaluation;social network analysis;community detection;predicting future;graph analysis;real-world;link prediction
indexing structure;multiple layers;scoring functions;view-based
distributed monitoring;communication cost;outlier detection;sensor node;unlike prior;similarity metrics;detecting outliers;detect outliers;sensor networks;sensor nodes;detecting outliers;sensor networks
instance;probabilistic data;threshold based;probabilistic data;communication cost;large datasets;massive amounts of data;multiple sources;distributed data;data management
real-world datasets;classification;classification;takes into account;information theory;higher classification accuracy;classifier;class labels;graph traversal
matching algorithms;sampling-rate;mobile devices;information extracted from;instance;sampling-rate;higher accuracy than;location-based services;sampling rate;matching problem;historical data;inference process
attribute values;conflicting information;data cleaning;uncertain data;aggregate queries;sensitive data;cardinality constraints;integer programming;query answering;query answering;cardinality constraints;relational operators
probabilistic data;threshold-based;avoid redundant;uncertain data management;frequent item set;item sets;closed itemsets;probabilistic database;result set;item set;moving object;probabilistic data;depth-first search;network monitoring;inherent uncertainty;frequent closed;mining algorithm;threshold-based;small size;search space
probabilistic databases;probabilistic databases;plans;knowledge compilation;conjunctive queries;monte carlo simulation;performance gains;query answers;probability computation;query answers;query engine
data set;real data;database
real dataset;real world;encrypted data;data sources;sensitive data;computationally intensive;similarity search;cryptographic techniques;encrypted data;locality sensitive hashing;efficient similarity search;real world applications;theoretical results;secure multi-party computation;similarity matching;high dimensional spaces
enterprise search;business information;text search;search queries;search engine;individual users
risk analysis;approximation technique;uncertain data management;analysis tasks;databases
decision-making;high level;game-theoretic;large-scale;game-theoretic approach;application domains;sensor networks;large amounts of data;sensor nodes;sensor networks;sensor networks;power grid
query optimization;data management problems;efficient storage;database;query language;data integration;semantic web;resource description framework;data management;data management
schema matching;schema matching;data integration;matching process;matching systems;mapping problem;manual effort;ontology alignment
incremental algorithms;detection problem;incremental algorithm;vertically partitioned data;database;np-complete;distributed data;prohibitively expensive;real-life;computational cost;functional dependencies;distributed database;optimization techniques;frequently updated;distributed data
data sharing;information systems;database instances;data resources;data exchange;instances;cost-based;provenance information;source data;databases;instances;query processing
large volume;temporal data;search performance;sliding window;wireless communication;sliding window;streaming data;indexing techniques;disk based;indexing scheme;data processing;window based;disk based;temporal data;numerous applications;spatio-temporal;data entry
clustering;clustering;clustering methods;database;multiple attributes;traditional clustering algorithms;complex data;dimensional data;real world;application scenarios
large-scale;data providers;web technologies;copy detection;information flow;structured data;data integration
clustering;heterogeneous information networks;information network analysis;information network;heterogeneous information network;classification;database;link analysis;semi-structured;multi-typed;similarity search;data analysis;inter-related;data mining;hidden knowledge;real world;data repository;database researchers;information networks
linked data;shortest path;graph pattern matching;complex structure;rich semantics;expert finding;similarity search;graph queries;graph queries;network data;content information;information networks
text mining;data collection;data summarization;databases;original data;decomposition techniques;data engineering;data sets;application domains;data collected;matrix decomposition;matrix decomposition;data management;knowledge discovery
temporal data;stochastic processes;uncertain data;answering queries;sensor databases;wide range;temporal data;spatio-temporal queries
nearest;selection problem;fall short;optimization problems;optimization problem;decision support systems;massively multiplayer online games;wide range
high-dimensional;nearest neighbor;image datasets;bi-level;knn queries;hierarchical structure;hash table;locality sensitive hashing;nearest neighbor search;high dimensional spaces
hybrid approach;query optimization;query execution;query workloads;great success;resource management;query performance;fine-grained;operator;cost models;high accuracy;query plans;query optimizers;modeling techniques;coarse-grained;query performance prediction;prediction models
maintenance costs;fine-grained;access path;databases;index tuning;coarse-grained;adaptive indexing
stream processing engine;applications including;high-frequency;complex) event processing;stream processing;wide range;query processing;event streams;data analytics;low-level
high throughput;video stream;large number of;linear programming;copy detection;video frame;user-friendly;content-based image retrieval
web-based;document corpus;user friendly;effectively identify;search engine;search interface
web-based;structured databases;database;query interface;user interface;user-interface;user-centric
database systems;database;maintenance costs;decision-making;database;multidimensional data;expert knowledge
query optimization;density-based;optimizer;parameter values;density-based clustering;plan space;locality-sensitive hashing;database;expected values;relational database management systems;query plan;query plans;clustering algorithm;pre-processing;query plan
pruning techniques;join operations;pruning methods;optimization strategies;cost-based;worst case;query optimizers
query optimization;query processing strategies;database;black box;data model;finer-grained;query plan;aware query processing;relational databases;database engine;processing queries
large numbers of;query processing;spatial objects;pre-computed;aware query processing;location-based services;data management;aware query processing
multiple sources;processing nodes;provenance information
streaming applications;continuous queries over;web applications;open source;parallel computing;large scale;distributed file system;main-memory;stream processing;enormous amounts of;rates;data streams;main-memory
relational queries;database
gather information;collected data;web users
query rewriting;user friendly;twig queries;textual information;query formulation;xml query;query language;twig pattern;query processing;xml document;query languages;graphical interface
keyword search;keyword search
recommendation quality;recommendation approaches;location-aware;recommender systems;large-scale;real-world;social network;location-aware;location-based
query nodes;query complexity;road networks;large-scale;query-independent;path tree;large-scale social networks;estimation error;graph nodes;relative error;shortest distance;distance based;query-dependent;query-dependent;optimization techniques;shortest distance;retrieval techniques;shortest distance;index size
search algorithms;pruning techniques;indexing structures;real datasets;location-based services;keyword search;aware query;keyword search;aware search;mobile users
similarity-join;computation model;edit distance;communication cost;similarity joins;real-world applications;similarity threshold
event detection
textual descriptions;high quality;data records;information theoretic
information sharing;access control;social network;designed to support;online social networks;social graph;access control model;information sharing
clustering;clustering results;multiple clusterings;evaluation metric
query execution engine;large distributed;query optimizer;large-scale;ad-hoc queries;map-reduce;query language;query execution engine;large graph;large graphs;large distributed;application called;software components;social networking
energy efficiency;xml processing;speed-ups
database;data stream;programming interface;complex event processing;complex event processing
database;join algorithms;join algorithms;wide range;data transfers;real-life data sets;data-intensive applications;similarity joins;vast amounts of data
cardinality estimates;cost estimation;highly skewed;cost model;load balancing;data sets;science applications;mapreduce based;load balancing;massive data sets;data distribution
social media;detection algorithms;social interactions;community detection;social media;detection process;supervision;dense regions;community detection;social networking
naturally leads to;user experience;closely related;cross domain;higher quality;ad hoc
social media;sensitive information;markov-chain;monte-carlo;cascade model;information flow;data types;broader range;information flow;metropolis-hastings;learning models;training methods;real world data sets
query optimization;search space;optimizer;resource consumption;optimization process;partial plans;query optimizers;cost models;search spaces;optimization process;search spaces;generation algorithm;relational database management system;pruning techniques;plans
multi-dimensional;data cube;database;learning tasks;differentially private;sensitive data;differentially private;privacy preserving data publishing;olap) queries;data cubes;health information;data cubes;online analytical processing;health information
real-world;large-scale;meta-data;rule-based;easily extensible;user-defined;general-purpose;case studies;large sets of;conjunctive queries;query answering;persistent storage
query patterns;query processing engine;database;optimization strategies;index structures;query language;rdf graphs;large sets of;rdf graphs;query languages;query results;data model;data management system
data sets;data collection;data visualization;large scale;analytical models;client applications;social sciences;user interface;data analytics;large datasets;user interaction;extract information from;data analytics;data entry
workflow systems;database;real datasets;indexing scheme;pruning strategies;real dataset;provenance information;sina weibo;recent database
data sharing;data processing;sharing information;network applications;large-scale;business model;large-scale;database;data management;data processing;data management system
distribution function;density estimation;random samples;density estimation;query processing;estimation accuracy;data mining;theoretical analysis;data distribution;load balancing
join results;rank-join;poor performance;communication costs;statistical information;join algorithms;distributed systems;result set;estimation algorithm;distributed systems
domain experts;visual analytic;user interactions;real-world;considerable effort;user feedback;user interface;ontology matching;matching process;user feedback
extensible framework;private data;large collections of;privacy preserving;highly distributed;query processing;privacy-preserving;databases;data repositories
database;databases;data stored in;forensic analysis;databases
query execution;enabling users to;database queries;execution plans;query execution
data analysis;optimizer;plans;intermediate results;optimal plan;multiple times;cost-based;query processing;real-world;optimization techniques;subexpressions
database research;excellent performance;database
skewed data;search space;mapreduce-based;load balancing;data-intensive;data distribution;load balancing
query evaluation;database systems;sql queries;xml data;xml queries;document types;query performance;enterprise applications
data transformation;querying xml data;formal semantics
multi-query optimization;semi-structured data;sparql queries;data model;cost model;heuristic algorithms;multi-query optimization;query patterns;execution plans;query languages;optimization techniques;query engine
approximation algorithms;discovery algorithms;traffic monitoring;error guarantees;counterpart;real data;data quality
order statistics;large scale;machine learning algorithms;large data sets;high-level language;quantitative analysis;data analytics
query evaluation;web search;index structures;information retrieval;search engines;high-throughput;design space;retrieval engine
open source;fault tolerant;database;data store;data sets;user activity;processing requirements;log data;social networking
data warehouse;data warehouses;data structures;data warehouse;data model;business process;lessons learned;search services
sql/xml;xml enabled;xml applications;xquery update;storage model;xml storage;operator;data type;xml enabled;xml query
cost-based;sql queries
handle arbitrary;wide range;requires minimal;specific information;relational database management systems
cost-effective;database workloads;performance goals;vice versa;database
multi-version;graph based;database;lock;support queries;data access;serializability;database functionality;concurrency control;multi-version
domain knowledge;domain-independent;automatic extraction of;structured data from;extraction process;html pages;extracted data;template-based;real-world;structured data;web data
semantic indexing;knowledge bases;query pattern;unstructured text;indexing scheme;trec collection;operator;obtained by applying;domain-specific;named entity;query terms;text (document;complex relationships;unstructured data;oracle database
data analysis;data cube;database;growing rapidly;business intelligence;ad hoc;relational dbms;web interface
query result;database systems;high quality;query-refinement;case study;multi-criteria;query results;database performance
query node;naive algorithm;web search;applications including;collaborative filtering;random walk;scale-free;similarity search;large graphs;similarity search;computationally expensive;heuristic algorithms;similarity matrix;algorithm called;structural similarity
processing cost;end-user;high-dimensional;data analysis;data structure;applications ranging from;information dissemination;index structures;relevant content;large collections of;emerging applications;computational advertising;clustering;real estate;user-centric;pattern matching;data intensive
search algorithms;similarity searches;duplicate detection;applications including;data cleaning;early stage;document frequency;similarity query processing;similarity searches
specially designed;lower bound;expected cost;sliding window;scoring function;query answering;sliding windows;sliding windows
fast algorithms;approximate algorithm;large number of;post-processing;user interests;network traffic;provably optimal;area network;continuous queries
common features;pattern recognition;lower bound;social networks;distance constraints;similarity joins;similarity join;string similarity;data entry
parameter-free;data dependencies;statistical measures;efficient algorithms to;28;parameter-free;data quality;instance;distance constraints;expected utility;distance constraints;data distribution
clustering;real data sets;classification;classification tasks;time series;desirable property;time series;similarity search;similarity search;random variable;error reduction;error reduction
knowledge base;automated reasoning;knowledge base;data management;web users
big data;lessons learned;data rich;data analytics;web services;relational database;data analytics
instance;information systems;information systems
data independence;data bases
taking into account;online social networks;privacy aware;private information;social network;real data;estimation process;social graph;active learning approach;conceptual model;social networks;user interactions;personal data
database;private information;privacy concerns;user's location;location based queries;privacy-preserving;location-based;location data;retrieval techniques
incremental update;cpu cost;extracted information;real-world;information extraction;data set;text corpora;statistical model;cost-based;inference algorithms for;intermediate results;information extraction;conditional random fields
problem instance;optimization problem;instance;approximate match;specific problem;approximate string;approximate string
clustering;real data sets;text documents;clustering approach;text clustering;user-access;text document;web logs;social networks;text clustering;clustering process;probabilistic models;unstructured data;information networks
keyword queries;set intersection;xml data;inverted lists;set intersection;query processing;optimization techniques
poor performance;query pattern;memory usage;pattern queries;data streams;large number of;low overhead;databases;resource constrained;pattern queries;pattern matching
privacy guarantees;spatial data;general purpose;multi-dimensional data;private data;differentially private;post-processing;query answers;tree structure;high accuracy;design space;theoretical guarantees;indexing methods;data distribution
real data sets;statistical queries;privacy guarantees;sensitive information;differentially private;sensitive data;query answers;highly accurate;random variable;range queries
synthetic data;news sources;maintenance cost;takes into account;location aware;location-aware;decision model
search interfaces;search strategies;web forms;data sources;source-specific;high efficiency;hidden databases;search engines;data integration;search strategies;keyword search;web data;product search
query optimizer;query optimization techniques;monitoring applications;data stream management systems;performance gains;operator;continuous queries
query execution;indexing approach;data set size;tree structure;data sets;range queries;spatial index;query result;range queries
reformulated queries;web pages;relation extraction;query generation;structured data;query reformulation;random walk;query reformulation;data set;real-life;query processing;structured data sources;structured data;web search engines
information content;association rule mining;real data;sequence data;association rule mining;association rules;ad hoc;high-resolution;formal model
classification;frequent pattern mining algorithms;classification;frequent patterns;real world datasets;interval data;time series;instance;algorithm called;training instances;unified framework;problem size;classification accuracy;frequent pattern mining
search results;real data sets;database;individual nodes;random walk;query answers;query log;keyword search over;ir-style;query answer;databases;keyword search;query languages;database schemas;indexing methods
data processing;big data;large-scale;map-reduce;real data;instance;temporal queries;cost-based;commercial systems;temporal data;data feeds
distributed queries;fine-grained;database;distributed databases;primary key;fine-grained;distributed transactions;database workloads;social networking;data distribution

database systems;data compression;large volumes of data;scientific applications;data compression;general-purpose;compression techniques;data sets;high-throughput;file systems;high speed
real data;point set;data set;lower bounds;skyline points;product features;cost function;spatial join
attribute-based;attribute-based;data mining problems;sequence mining;database;query sequence;large datasets;sequence matching;sequence classification;matching problem;ordered list
query execution;control systems;array data;database;storage space;high degree of;storage model;databases;storage manager;real world data sets;efficient access to
multi-dimensional;massive data;real-world situations;real datasets;clustering algorithm;sensor-network
large graphs;parallel computing;large scale;distributed memory;increasing number of;web-scale;regular expression;query processing;graph data;subgraph matching
uncertain graphs;sampling algorithm;social network analysis;probabilistic graphs;upper bounds;data extraction;subgraph features;subgraph isomorphism;similarity search;privacy preserving;resource description framework;data integration;graph databases;data management;large number of
main memory;algorithms for computing;real datasets;efficient algorithms to;np-hard
filtering algorithms;textual description;location-aware;synthetic datasets;textual data;spatial information;pruning techniques;efficient search;textual similarity;region-based;similarity search;user profiles;social networks;location-based services;high-quality;query keywords;verification framework
document streams;high frequency;synthetic datasets;ranked list;search engine
query evaluation;reachability queries;sensitive information;mobile devices;reachability query;moving objects;query processing;location-based services;objects moving;query processing techniques;disk-resident;application scenarios;synthetic data
clustering;principal components analysis;search space;dimensional space;index structures;moving object;skewed;wide range;query processing;road networks;real world scenarios
polynomial space;xml query;approximation technique;static analysis
database;user experience;main-memory;increasing number of;long-running;replication;designed to support;scalable distributed;data management systems
user-item;weighted graph;recommender systems;theoretical foundation;graph-based;real life datasets;data sparsity;item recommendation
data structure;xml schemas;high degree of;schema matching;machine learning;instances;twig query;xml document;real-world data sets;database schemas
high-speed;high-speed;data visualization;network flow;bitmap index;locality-sensitive hashing;large volumes of;general-purpose;query response times;service provider;large-scale;behavior analysis;rates;forensic analysis;analysis tasks;databases;space requirements;network data;measurement data;parallel execution;million records
approximation techniques;dynamic programming;temporal aggregation;synthetic data;worst-case;real-world;greedy algorithms;large data sets;error-bounded;operator;temporal aggregation;temporal databases
synthetic data;query optimizer;database applications;specific set of;complex query;evaluation scheme;execution plan;instances;aware query processing;batch processing;processing cost
data objects;skyline computation;query operators;query processing in;distributed environments;skyline queries;data management
range queries over;false positives;synthetic data;data partitioning;data outsourcing;outsourced data;query cost;computational overhead;optimization problem;encrypted data;multidimensional data;range queries;data owner
information filtering;web technologies
strong correlation;social network;computational cost;dynamic networks;increasing attention;social networking;decision making
analytical queries;query rewriting;large datasets;materialized views;graph model;context-aware;massive amounts of;business intelligence;ad-hoc;business intelligence;data warehousing;graph data;complex data
regular expression;low precision;high-recall;user feedback;relevance feedback;rates;high recall;domain independent;classifier
information processing;service-oriented;quality aware;source data;web sources;information-processing
supervised learning;product development;classification;ordinal classification;online reviews;learning algorithm;product design;online product reviews;programming model;classification approach;classification algorithm;posterior probability
data-centric;stored procedures;data management;application logic;relational databases;data analytics
materialized views;query response time;query performance;data structures;power consumption;cost models;budget constraints;databases;cost models;multi-criteria
store data;data processing;distributed stream processing;large number of;data streaming;replication;wide range;data aggregation;storage systems;long-term;fault-tolerant;resource utilization;highly scalable;data analytics;load balancing
rdf data;data stored;fault-tolerance;data items;data sources;efficiently identify;web services;data management
large-scale;sequence data;distributed environments;scientific workflow;data-intensive;biological data
systems require;access latency;power consumption;limited number of;large amounts of data;hard disk
global analysis;human activities;data management;databases
data management system;data management
user interface;user study
linked data;constraint satisfaction problem;data access;control architecture;resource allocation;multiple layers
linked data;high-quality;schema mapping;quality assessment;linked data;fusion methods;data integration;data sources;data access;real-world;completeness;data integration
linked data;web technologies;mobile devices;semantic search;semantic annotations;multimedia content;computing power;web search engines;mobile users
graph model;pattern queries;probabilistic graphs;graph data
linked data;classification;rule learning;training set;data items;item pairs;classification rules;data sets
high-level;content filtering;data exploration
mining algorithms;open data;low quality;business intelligence;making decisions;data quality;business intelligence;quality-aware;data sources;data mining;user-friendly;high dimensionality;mining process
taking into account;optimization approach;data protection;evolutionary algorithm;data utility;information loss
personal information
probability distribution;graph model;differentially private;knowledge discovery;social networks;graph databases
privacy preserving distributed;clustering;clustering results;vertically partitioned data;privacy preserving;finding clusters;distributed data mining;density-based clustering algorithm;hierarchical clustering methods
search terms;keyword queries;database;document retrieval;privacy concerns;sensitive data;information retrieval;documents retrieved;privacy-preserving;efficient computation;relevant documents;keyword search;sensitive information;keyword search on
error-prone;process models;verification framework;privacy requirements
mutual information;information theory;differentially private;probability distribution;learning theory;data privacy;differentially-private;bayesian framework;differentially-private;information theoretic;increasingly popular
formal framework for;database table
data mining technique;data mining techniques;data mining algorithms;privacy concerns;ranking model;application domains;ranking problem;knowledge discovery;privacy preserving;data mining technology;privacy-preserving;identify patterns;vertically partitioned data
privacy preserving distributed;privacy preserving distributed;query optimization;theoretical results;privacy preserving;data cubes;distributed environments;privacy preserving
online social networks;sensitive information;access control;large number of;social network;social networks;access control model;social graph;access control;privacy preferences;social graphs
data stores;test data;data items;data quality
database applications;databases;specific problem;test cases
clustering;clustering;clustering results;cluster structure;data mining tasks;cluster validity;application domains;clustering process;wide range;cluster structures
query optimization;rewriting queries;data warehouses;ontology languages;query reformulation;common patterns;maximally-contained rewritings;answering queries;view definition;conjunctive queries;query expansion;data integration
workshop on data mining;application domains;international workshop on;knowledge discovery in databases;wide range;data mining;data mining models;optimization techniques;data mining based
document corpus;users' interests;dark web;broad applications;topic detection;information consumers;bipartite graph
text mining;network analysis;web portal;data mining techniques;community detection;detection algorithm;valuable information;topic models;homeland security;social networks;topic-model;online communities;text mining techniques
probabilistic modeling;document collection;starting points;user feedback;document collections;association analysis
clustering;scale free;individual nodes;path length;social networks;small world;topological properties;agent-based;simulation framework
clustering;high dimensionality;true positive;service providers;detection rate
text documents;text documents
private information;quantitative analysis;case study;real-world;quantitative analysis;model called
block based;simulation results
action rules;mined patterns;action rules;search problem;data mining;expected utility
web pages;knowledge bases;query segmentation;multi-column;diverse sources;graphical model;search engine;graph cuts;ir methods;inference algorithms
web pages;completeness;communication overhead;search engine;web crawlers;web contents;web data;keyword search;web-content;query result;conjunctive keyword
keyword queries;data warehouse;ad-hoc queries;business users;graph pattern matching;data warehouses;real data;data warehouse;highly-complex;business analysts;high precision and recall;automatically generating;search experience;increasingly complex;financial services
web search;synthetic datasets;instance;anonymization algorithm;sensitive information;valuable information;anonymization techniques;original data;query logs;high probability;real world applications;multidimensional data;query terms
user-item;dimensional space;large-scale;temporal dynamics;latent factor model;clustering;recommender systems;predicting future;large number of;recommendation accuracy;user interests;latent factors;cold start;prediction accuracy;recommender systems;multi-core;real-world;efficient algorithms to;recommendation algorithms;high-order;perform poorly;markov chains;similar users;latent factor models;item pairs
query execution;data analysis;optimization framework;query operations;applications ranging from;incremental maintenance;maintenance cost;higher-order;historical data;rates;cost-based;databases;wide range;data management system
detection techniques;web applications;dense clusters;dynamic environments;conventional techniques;ranking function;interesting patterns;high precision and recall;graph analysis;graph based;data streams;web-pages;keyword search;real world;dynamic graphs
data-management systems;order-preserving;data sets;streaming data;distributed data;distributed data streams;streaming applications;count-based;data updates;ad-hoc queries;sliding-window;query answers;high-dimensional data streams;data streams;sliding windows;window queries;sketch-based;sliding window;high-speed data streams;data distribution;query answering over;emerging applications;distributed environments;real-life data sets;heavy hitters;distributed streams
web applications;database;log-structured;data access;storage systems;disk-based;numerous applications;efficient access to
nearest neighbor;data mining applications;computational costs;knn join;nearest neighbor query;pruning rules;data-intensive applications;nearest neighbors
massive data sets;data distributions;incremental computation;speed-ups;resource constraints;approximate results
answer quality;classification;higher accuracy;designed to support;data analytics;natural language processing;image annotation;processing cost;data analytics;query engine
intrusion detection systems;computational biology;significant patterns;chi-square;statistical significance;high probability;real world;text analysis;monitoring systems
database technology;database systems;database;massively parallel;main memory;multi-core;join algorithms;sort order;data processing;databases;competitive performance;disk-based;massively parallel;query engine;partition-based
access pattern;storage devices;classification;data allocation;highly dynamic;human efforts;level caching;data access;storage systems;storage manager;storage management;semantic information;data management
problems arising;data analysis;data mining;computational geometry;big data;acm sigkdd international conference on;predictive modeling;large number of;kdd conference;online advertising;natural language processing;fast algorithms;social networks;text mining;graph theory;real-world applications;knowledge discovery;reviewing process;information networks;database;machine learning;web mining;time-series;statistical inference;program committee
internet users;search queries;search engine;multimedia content;log data;mobile internet
data objects;network models;heterogeneous information network;database systems;classification;large-scale;semi-structured;similarity search;real world;data mining;heterogeneous network;path-based;clustering;social media;big data;rank-based;rich semantics;social networks;real world applications;rich information;information systems;data mining research;heterogeneous information networks
big data;confidence intervals;theoretical properties;massive data;large-scale;problems arise;massive datasets;statistical inference;big data;parallel computing
graph coloring
public domain;information diffusion;real datasets;real data;search engines;analytical model;influence propagation
highly relevant;graph mining;sparse matrix;user preference
real data sets;naive algorithm;web graph;graph evolution;data set;randomly generated;social networks
information diffusion;social network;social networks;social media data;external sources;social networks
data-driven;clustering coefficient;real networks;degree distribution;optimization approach;probabilistic models;detecting anomalies;autonomous systems;formal representation;network evolution;protein interaction networks;models learned
pattern sets;minimum number of;problem setting;frequent patterns;frequent pattern mining
high-dimensional;feature space;feature set;mining process;emerging patterns;feature selection;feature relevance;classifier;predictive accuracy
real-world datasets;sampling methods;large number of;memory requirements;linear space;biased samples;preprocessing phase
high utility;search space;mining algorithms;synthetic datasets;databases;algorithm named;high efficiency;itemset mining;data mining;excellent performance
normal form;categorical datasets;sampling methods;classification;search space
contextual information;real datasets;nonparametric bayesian;dirichlet process;topic model
latent dirichlet allocation;variational bayes;hierarchical dirichlet process;hyper-parameters;variational bayes;predictive performance;hierarchical dirichlet process
regularization;graphical models;approximate algorithm;random variables;causal relationships;large scale;large graph;graphical model;complex systems;synthetic datasets;objective function
clustering;social media;latent topic;latent dirichlet allocation;textual data;text streams;textual content;topic model;topic distribution;prediction error;latent topic;social networking;accurately predict
clustering;high potential;multi-view clustering;multi-view clustering;mixture components;overlapping clustering;clustering methods;generation process;real-world data sets;bayesian framework;mixture models;model selection;multi-faceted
optimization problem;real-world data mining applications;error rates;cost-sensitive;real-world data sets;cost-sensitive classification;classification errors;error rate;classification algorithms
linear models;classification;large-scale;tree-based;gradient boosting;shape models;high accuracy;arbitrarily complex
black-box;convex optimization;rates;theoretical guarantees
data arrives;instance selection;random subspaces;classification;real-world applications
38;linear svm;linear support vector machines;linear support vector machines
human mobility;real-world datasets;mobility patterns;large-scale;key words;baseline methods
uncertain trajectories;collaborative learning;real datasets;mutual reinforcement;traffic management;graph construction;temporal characteristics of
low cost;mobile applications;latent factor models;lessons learned;3,5,8,12;sparse datasets;low dimensional;high accuracy;explicit feedback;2;latent factor;high dimensional space;neighborhood information;dimensionality reduction;pca-based
high utility;large volume;data analysis tasks;data publication;tree structure;sequential pattern mining;differentially private;trajectory data;differentially private;post-processing;case study;real-life;sequential data;large datasets;prefix tree;consistency constraints;data utility
case studies;human-machine
big data
human beings;probability theory;information processing;raw data;semantically meaningful
data mining applications;data mining;open-source;application development;real-world applications
large-scale;amazon mechanical turk;statistical model;data collected from;correct answer;machine learning
network structure;real-world networks;uniform sampling
social media;user behavior;dynamic model;structural properties
product review;user preferences;social sciences;social science
clustering;search algorithms;real-time monitoring;data streams;higher-level;time series;massive datasets;similarity search;rates;large datasets;data mining;data mining problem;data mining algorithms
long-range;future events;real datasets;click logs;multiple attributes
mining framework;temporal patterns;pattern mining;time series;event detection;data mining research
class membership;classification;decision tree;classification accuracy;time series;machine learning;data set;real-valued;classification algorithm
minimization problem;dual problem;low rank;convergence rate;distance matrix;data matrix;line search;computationally expensive;real world applications;missing values
singular value decomposition;minimization problem;matrix factorization;coordinate descent;latent structure;real world
clustering;distributed algorithm for;data mining tool;real world;intermediate data;multi dimensional;applications including;anomaly detection;large scale;trend detection;data centers;knowledge base;tensor analysis;web graphs;social networks;tensor decomposition;tensor decomposition;knowledge bases
factor analysis;collaborative filtering;active learning;large-scale;active learning;statistical model;matrix factorization;variational bayes;online learning;bayesian approach;mutual information between
clustering;regularization;temporal dimension;optimization problem;data matrices;gradient descent;temporal smoothness;clustering methods;cluster structures
clustering;data sets;gaussian mixture model;data sequences;time series;minimum description length;statistical-test;code-length;maximum likelihood;artificial data sets
clustering;overlapping clusters;pruning techniques;clustering structure;correlation clustering;correlated features;subspace clustering;complex data;traditional clustering;correlation analysis
clustering;large amounts of;data compression;low-dimensional space;attribute dependencies;clustering approaches;categorical data;heterogeneous data;clustering result;real data;input parameters;data types;high accuracy;data type;cluster model;numerical attributes;clustering;interpretability;compression-based;minimum description length;model complexity
storage cost;large margin;real data;efficient querying;topic-specific;social networking
data set;optimization framework;propagation algorithm;semantic tags;real-life;weakly supervised;integrating information from;topic modeling;classification approach
social media;taking into account;information sharing;community discovery;automatically discovering;community members;social communities;model called
search queries;user feedback;query logs
big data;database;knowledge management;real-life;credit card;decision making
management systems;predictive models;business rules;decision making;data driven
user study;multiple datasets;large datasets
clustering;user navigation;graphical models;prediction model;discriminative clustering;discriminative clustering;input space;alternating optimization;cluster labels;problem setting;output variables;interpretability;classifier;market segments
real data;phase transition;phase-transition
probabilistic model;prediction algorithms;search engine
data collection;real-world;real datasets;data mining task;predict future
nonnegative matrix factorization;exploratory analysis;temporal patterns;large collections of;large collections of;scalability problems;pattern discovery;real world;patient data
large numbers of;event sequences;search strategy;pattern mining;sequential patterns;real data;candidate set;sequential data;small sets of;pattern sets;pattern set mining;pattern set;parameter-free;frequent pattern;pattern set mining;transactional data
matching algorithms;pattern matching;matching algorithm;query pattern;real-world;pruning techniques;pre-processing
regularization;global solution;loss function;rank minimization;parameter estimation;efficient algorithms to;low-rank;estimation error;counterpart;rank minimization;solution path;noisy data;np-hard
high-dimensional;sparse matrices;large-scale;optimization problems;dictionary learning;map-reduce;factor matrices;risk minimization;analysis tasks;learning phase
total number of;collaborative filtering;public-domain;optimization problem;data sets;loss functions
clustering;real dataset;naturally leads to;inference problem;low rank;low-rank;theoretical guarantees;matrix factorization;instance;edge weight;analysis task;low rank;high accuracy;real data;analysis tasks;million nodes;loss functions;inference algorithms;graph clustering;million edges
real-world datasets;classification problems;taking into account;semi-supervised setting;similarity information;prediction performance;large number of;graph kernels;unlabeled instances;clustering algorithm;kernel methods;machine learning and data mining;additional information
multi-label learning;multi-label;prior knowledge;automatically discover;real-world
real-world datasets;classification;instance labels;multiple labels;instances;multi-label learning;instance;instance-level;supervised classification;instance;gradient descent;label sets;higher accuracy than;multi-instance;loss functions;efficient optimization
regularization;labeled samples;multi-view learning;multi-view;multiple view;learning algorithm;diverse sources;unlabeled samples;real-world applications;multi-view learning;real-world data sets;multi-task learning;classification errors
large amounts of;lower computational cost;low quality;online video;higher accuracy;real-world data sets;highly scalable
baseline methods;classification;classification;propagation algorithm;propagation model;news video;semantic concepts;topic detection;online video;web news;manifold structure;visual features;web users
search space;web pages;synthetic datasets;database;web sites;pattern mining;ground truth;web page;web pages;transaction database;semi-automatically;web page
short documents;data repository;language identification
knowledge engineering;analytical queries;natural language queries;big data;knowledge-based;machine learning;search engines;knowledge acquisition;social graphs;semantic search;entity extraction
real-time tracking;ctr;recommender systems;relevant content;user feedback;lessons learned;training data;large-scale social networks;user intent;social networks
heterogeneous sources;network applications;ranking model;feature extraction;quadratic programming;social network data;similar topics;social networks;user experiences;identification problem;real world;social communities;user-generated-content
clustering;similar behavior;detection method;clustering method;social network;social networks
world wide web;explicitly model;loss function;real-world datasets;community detection;optimization problem;matrix factorization;citation networks;matrix factorization methods;social networks;complex networks;network structure;nonnegative matrix tri-factorization;overlapping communities;community structure
graph partitioning;real networks;community discovery;community discovery;propagation algorithm;complex networks;web-scale;large networks;knowledge embedded in;real world networks;extraction task;overlapping communities;information networks
detection algorithms;random-walk;network datasets;application domain;structural properties;analysis reveals;detection method;large scale;algorithms produce;separability
temporal dependencies;real data sets;plays an essential role in;special properties;statistical techniques;real-world applications;sequential data;dependency analysis;temporal patterns
temporal dependencies;climate data;large parts;long distance;takes into account;spatio-temporal;large number of;time series;temporal context;data mining;statistical significance
underlying structure;real datasets;detection approach;instances;random noise;similar patterns;dynamic graphs;information theoretic
high utility;complete set of;large scale;sequential pattern mining;sequence mining;decision-making;sequential patterns;pruning strategies;generic framework;behavior analysis;pattern selection;sequential pattern mining;real datasets;frequent pattern mining
mining algorithms;high-quality;low cost;large-scale;trade-offs;data sources;low resolution;high-resolution
addressing this problem;user models;low recall;generative model;personalized recommendations;high precision;feature set
set cover;high level;data-mining;ranked list;real data;set-cover;wide range;selection methods;ranking methods;high-quality;perform poorly;user study
social media;map/reduce;big data;social-networking;user behavior;behavior model;data sparsity;social network;topic models;hierarchical bayesian model;predictive performance;social networks;knowledge transfer;real world applications;user modeling;individual users;user behaviors;predictive accuracy
training data;recommender systems;user behavior;learning algorithm;retrieval functions;user interests;implicit feedback;algorithm learns;online learning;search engines;online learning;theoretical results;theoretical guarantees
collaborative filtering;learning algorithm;optimization problem;maximum-likelihood;matrix factorization methods;natural language processing;automatically generated;markov chains;music collections
ranking algorithm;pattern recognition;recommendation systems;data manifold;real data;ranking function;information retrieval;geometric structure;ranking methods;manifold ranking;vector field;query point;synthetic data
real-world datasets;regularization;fixed point;unlabeled data;semi-supervised learning;classification tasks;kernel matrix;pairwise constraints;learning tasks;labeled data;semi-supervised learning;operator;optimization strategy;knowledge sources
informative samples;data mining technique;unlabeled data;active learning;sampling based;transfer learning;linear programming;quadratic programming problem;optimization problem;machine learning;probability distribution;training data;generalization performance;batch-mode active learning;optimization techniques;uci datasets;classifier;integer programming;np-hard
multiple kernel learning;optimizer;general purpose;data points;takes into account;optimization algorithm;objective function;optimization algorithms;gradient descent;training data;wide range;selection criterion;step size;real world applications;multiple kernel learning;real world
classification;classification;great success;machine learning;classification performance;wide range;classification problems;kernel parameters;model selection
accurately identify;real-world;estimation problem;data sparsity;missing data;logistic regression;probability estimation
historical data;historical information;growing rapidly;features extracted from;prediction algorithms;click prediction;prediction models;click prediction
data-driven;evaluation criterion;controlled experiments
prediction accuracy;sponsored search;ctr;real-world;search advertising;click prediction
optimization approach;supervised learning algorithms;online advertising
clustering;graph-structured data;computational complexity;graph mining;detection methods;large-scale;frequent subgraph;database;pattern mining;data acquisition;data model;edges represent;uncertain graph data;sensor networks;social networks;graph data;data mining;high-throughput;wide range
knowledge bases;simpler models;knowledge discovery;enormous amounts of;automated techniques;automated techniques;knowledge base;simple models
data analysis;combining multiple;machine learning;social networks;latent structure;statistical models;data mining algorithms;relational data;bayesian approach
utility function;cross-media;ontology-based;cloud computing environment;cross-media;knowledge discovery;cross-media
spam detection;low-quality;detection problem;electronic commerce;temporal patterns;online stores;time series;online reviews;pattern discovery;hierarchical algorithm
statistical properties;online reviews;valuable information;narrative structure;online reviews;selection methods;user studies;combinatorial optimization problem;np-hard
real-life;social media;major source of;network structures;fine-grained
temporal characteristics of;valuable knowledge;question-answer;question answering;knowledge-creation;case study;knowledge creation;large repositories of;long-term;question answering
multiple communities;optimization framework;outlier detection;real datasets;temporal dimension;community members;coordinate descent;detection performance;similar patterns
exploratory data analysis;real life datasets;regression model;regression models
high-dimensional;nearest neighbor;data points;high-dimensional data sets;outlier detection;algorithm runs in;outlier mining;point sets;projection-based;data mining task;dimensional data;theoretical analysis;approximation algorithm;real world data sets;estimation algorithm;outlier factor
intrusion detection;intrusion detection systems;community-based;large number of;data collected;operator
received increasing attention;learning algorithm;multi-task;noise level;real-world;learning algorithms;optimization problem;real-world applications;gradient descent;multi-task learning;theoretical result;theoretical analysis;dimensional data;applications involving;multi-task
social media;linked data;unlabeled data;label information;unsupervised feature selection;real-world;unsupervised feature selection;feature selection algorithms;feature selection;social media data;data mining;dimensional data;feature relevance;relevant features;unique characteristics
high-dimensional;information content;training data;feature set;text-mining;ensemble methods;feature sets;feature selection methods;feature selection
high-dimensional;graph structures;estimation accuracy;graph structure;selection methods;classification;synthetic data;real datasets;regularization;classification performance;highly correlated;structure information;feature selection;feature selection
data structure;multiple queries;closely related;data sets;general setting;tree data structure;product search;cosine similarity;search technique
data sets;hash functions;synthetic data;probabilistic model;multiple modalities;hamming space;multimodal data;similarity search;data mining;latent factors;compares favorably with;hash function
pruning techniques;index structure;location-based social networks;user study;social relationship;np-hard
feature space;metric learning methods;distance function;semantically meaningful;worst case;heterogeneous data;data sets;metric learning;computational efficiency;learning problem;classifier;metric learning

domain knowledge;detection algorithms;anomaly detection;qualitative analysis;stock market;hidden markov model;behavior analysis;application scenarios
product recommendation;attack detection;real-life applications;collaborative-filtering;naive bayes;real-world applications;case study;real-life;user profiles;semi-supervised;recommender systems;semi-supervised
anomaly detection;energy management;classifier;supervised approach;hidden markov models;low-dimensional
knowledge-based;audit data;data mining
lessons-learned;high levels of;predictive modeling;machine learning algorithms;predictive models
1, 2;product development;instances;large scale;social network;data-sets;social networks;open-source;knowledge creation;efficient approximate;data-set;np-hard
information diffusion;information networks
probabilistic framework;prediction methods;large scale;social network;user's location;data set;user profiling;user-centric
information diffusion;online social networks;community-based;social interactions;community detection;social network;event-based;data collected from;information flow;location-based social networks;social networks;diffusion model;takes into account;event-based
privacy policy;privacy guarantees;privacy-preserving data mining;data mining
data values;privacy guarantees;bipartite graph;set-valued data;privacy-preserving;query logs
real data sets;learning algorithms;learning strategies;spam filtering;learning models;learning tasks;machine learning;data corruption;credit card fraud;wide range;attack model;learning problems;support vector;learning strategy
prediction problem;image retrieval;million images;globally optimal;database;topic model;parametric model;web image;web images
prediction problem;similarity) function;protein function;data source;prediction model;computational biology;prediction methods;learning framework;directed graph;heterogeneous data sources;multiple data sources;ensemble classifier;multiple kernels;ensemble framework;ensemble classification;genomic data;graph-based;multi-label;automatically infer;classifier;data integration
unique features;common features;specific features;classification;learning approaches;shared subspace;active learning;latent features;data instances;instance;sentiment classification;text classification;active learning;text classification tasks;multi-domain;domain-specific;multiple domains;spam filtering;active learning framework;web sites;multi-domain;supervised learning
temporal patterns;convex programming;alzheimer's disease;alzheimer's disease;multi-task learning;prediction models;operator;gradient method;convex formulation;temporal smoothness;prediction tasks
unlabeled data;large volumes of;event-extraction;event extraction;unique characteristics;latent variable models
clustering;estimation accuracy;data source;sampling methods;clustering method;query interface;databases;deep web;data set;data sources;addressing this problem;data mining;real datasets
information overload;scientific literature;user studies
black-box;training examples;active learning;worst-case;real-world;selecting informative;1;data sets;active sampling;entity matching;active learning algorithm;classifier
clinical data;feature set;real-time monitoring;prediction quality;spectral analysis;data mining approach;feature selection;data collected;time-series;classification algorithms;data mining methods
sparse learning;data source;multi-source;large-scale;data types;multi-modality;data sources;data sets;multi-source;alzheimer's disease;base classifier;magnetic resonance imaging;incomplete data;alzheimer's disease
real-world datasets;monitoring data;production systems;real-world;raw data;user interface;long-term;unique characteristics;network traffic;data storage
large number of;comprehensive evaluation;mining framework;average precision;auc
large-scale;early stage;machine learning;lessons learned;internet applications;multi-class;machine learning and data mining
sponsored search;search log;large number of;optimization problem;performance goals;data analysis;search engine;simulation results
large social networks;total number of;network size;video content
memory footprint;optimization method;problems arising
joint model;user profile;online advertising
simulation studies;real data sets
graph partitioning;large distributed;distributed computation;graph datasets;large social networks;simple heuristics;large graphs;communication cost;incur high
unsupervised learning;classification;automatically extracting;transfer learning;large graphs;exploratory data analysis;mining tasks;network data;automatically determined;community discovery;automatically discover
graph theory;partition based;limited memory;partition-based;real-world graphs;cost model;large graphs;high cost;fast algorithms;disk-resident
clustering;compact representation;data mining tasks;data compression;hidden structure;real data;data set;link prediction;compression scheme;clustering methods;bipartite graph;link prediction;relevant information;input parameters;graph clustering;clustering
real-world datasets;graph mining;closely related;multi-layer;search algorithm;graph clustering;additional information
online social networks;recommender systems;rating data;social network data;recommendation accuracy;category-specific;domain-specific;online social network;social trust
cold start;auc;optimization framework;multi-type;data sparsity;social network;social tagging;social networks;graph based;feature weights
recommendation systems;sensitivity analysis;cross-domain;cross-domain;topic distributions;parameter tuning
collaborative filtering;recommender systems;real world datasets;real world applications;recommender systems;problem called;np-hard
social relationships;em) algorithm;recommendation approaches;collaborative filtering;prediction accuracy;real datasets;social network;recommendation accuracy;social networks;common interests;latent factor model;latent factor;large social networks;social relationship;expectation-maximization
clustering;clustering algorithms;search space;problem instance;data mining;learning problem;clustering algorithm
clustering;clustering problem;algorithm iteratively;randomized algorithm;ground-truth;social media;correlation clustering;baseline algorithm;large number of;real data;parameter free;protein-interaction networks;objective function;np-hard
clustering;spectral clustering;spectral clustering algorithms;image segmentation;arbitrary shape;data points;algorithm parameters;classification;local neighborhood;random perturbation;multi-dimensional;higher quality;synthetic data sets;nearest neighbors;clustering quality
clustering;spectral clustering;spectral clustering;clustering solution;similarity measurements;active learning algorithm
clustering;real networks;real-world;clustering result;probabilistic approach;link-based;iterative algorithm;clustering tasks;heterogeneous information networks;clustering quality
production systems;classification models;high-quality;large-scale;design principles;data mining research;classification performance;quality control;data mining systems;source data;design principles;data distribution
search results;large volume;modeling framework;probabilistic model;real-world;database;valuable information;baseline methods;ranking method;heterogeneous network
public domain;massive datasets;optimization technique;generation process
short queries;large scale;search experience;video search;statistical techniques;real data;similarity functions;information retrieval;query log;automatically discover;entity recognition;concept classes;web data
customer satisfaction;classification approaches;high accuracy;high probability;svm classification;term-based
social media;data analysis;social media;specific context;low quality;users' behavior;information flow;real life;user-generated content;social networks;social media data;sina weibo;network structure;mobile applications;social networking
collaborative filtering;information processing;information processing;huge amounts of;social network;social networks;heterogeneous data;social relationship
users' satisfaction;search result quality;query log;search engine;users' search;search quality
graph-based;similarity search;similarity search;real world networks;social networks
learning process;low-dimensional representation;large-scale;large-scale;text corpora;data mining approach;labeled dataset;semantic relatedness;related words
ranking algorithm;semantic associations;probability distributions;document sets;real datasets;association analysis;randomly generated;generative model;document pairs;latent factor;association analysis
web pages;semi-structured;data sets;manually annotated;entity search;prior probability;real world entities;knowledge base;np-hard
growing number of;additional knowledge;general purpose;specific domains;large scale;clustering approaches;text corpus;domain specific;real life;bayesian approach;information extraction techniques;keyword search;real life applications
data set;pattern discovery;result set;database
parallel implementation;increasing number of;multi-core;frequent items;data mining tasks;higher throughput;hardware architecture;operator;pre-filtering;modern hardware
clickstream data;simple linear;mining algorithms;sequence data;real datasets;web browser
face images;face recognition;input image;decomposition approach;low-rank;illumination conditions;data sets;face image;classifier
fast algorithms;estimation problem;point sets;tree-based;larger datasets;fast algorithms
interestingness measure;real-world datasets;numeric attributes;rank aggregation;ranking scheme
text mining;data analysis;business information;classification;social network analysis;community detection;data mining;user preference;data set;large scale;business intelligence;statistical analysis;data mining;application programs
social media;semantically similar;large-scale;heterogeneous networks;real-world;data mining tasks;data mining methods;high efficiency;similarity search;query-driven;heterogeneous information networks;matching process;multiple types of;query-driven;real world;information network;data mining results;heterogeneous information networks;information networks
clustering;uncertain data;real-world;error guarantees;clustering algorithm
frequent itemset;mining frequent itemsets;uncertain data;frequent itemset mining;frequent itemsets;data mining;benchmark data sets;frequent itemset mining
graph theory;large-scale;real-world;instance;visual exploration of;weighted graphs;visual exploration of
network datasets;user specifies;large graphs;graph data;interactive visualization;user interaction
web environment;total number of;web application
utility function;data-driven;large number of;social networks;human players;online social network;information propagation;information propagation
online social media;keyword-based;learning method;naive bayes classifier;sentiment analysis;online social network;spatial patterns;class labels;user behaviors
data mining technologies;models learned
association rule;exploratory data analysis;information-seeking
search systems;data structure;textual data;video data;index size;video retrieval;range queries
redescription mining;analysis tool
social media;helps users;user interactions;social media data;social media sites
similar objects;recommendation systems;heterogeneous networks;real-world;data set;heterogeneous network;path-based;similarity measures
social media;online social media;knowledge discovery;knowledge discovery;case studies;individual users;multi-lingual
web pages;knowledge bases;high-frequency;instances;high accuracy;structured data
historical data;news-related;users interact with;multi-type;web news;news video;user query;event extraction;million web pages;multiple data sources;web archive
social network analysis;graph mining;agent-based
semantic networks;document collection;related concepts;semantic network;semantic network;construction algorithm;search engine;relevance feedback;retrieved documents;relevant information;helps users
utility function;data-driven;large number of;social networks;human players;online social network;information propagation;information propagation
temporal dependencies;real-world datasets;ensemble methods;classification;temporal information;relational models;learning algorithms
training examples;classification;active learning;hierarchical text classification;active learning framework;classification systems;active learning;large number of;real-world applications;hierarchical text classification;hierarchical classification;topic classification;user feedback
prediction accuracy;multi-player;classification;classifier;general case;classification method;prior probability;prediction task
simple linear;combination weights;real-world;ensemble method;time series;accuracies;ensemble technique;forecasting accuracy
classification;training examples;low variance;learning algorithms;incremental learning;data structures and algorithms;missing values;efficient learning
classification model;optimization problems;margin-based;incremental learning;labeled samples;real-world;margin;classifier
learning algorithm;learning theory;individual classifiers;online learning;single-view;classifier;weighted sum of
svm-classifier;multi-objective;vector machine;entity recognition;ensemble classifier;benchmark dataset;genetic algorithm;large number of;feature subsets;diversity measures;classifier ensemble;majority voting;ensemble method;svm classifier;individual classifiers;machine learning methods;classification accuracy;svm) classifier;ensemble approach;classifier;classifier-ensemble;classification algorithms
instance based;classification;neighborhood graphs;ensemble methods;neighborhood graphs;classification;data mining;single classifier;machine learning algorithms;classifier;nearest neighbors;increasingly popular
decision tree classifier;training set;user behavior;rule-based;learning algorithms;naive bayes;training instances;learning problem;noisy data;machine learning algorithms;classifier;sensor networks
decision trees;binary class;decomposition techniques;real-world applications;instances;imbalanced datasets;multi-class
massive data;decision trees;synthetic data;million records
decision trees;decision tree;decision tree algorithms;text categorization;text data;classification method;multiple types of;text classification
multi-label learning;tree structure;ensemble learning;learning performance;multi-label data
individual records;linked data;multiple instance learning;census data;linkage based;classification;instance;record linkage;instances;data sources;multiple instance learning;databases;synthetic data;record linkage is;record linkage is
recommendation methods;data sets;wide range;classification;decision diagram
target language;training data;unlabeled data;classification problem;classification;active learning;cross language;text categorization;text categorization;classification performance;cross language;labeled data;source language;class labels;labeled examples;classifier trained on;classifier;active learning algorithm
instance;real-world;decision-making;machine learning;complex systems;linear classifiers;multi-class
sample size;imbalanced data;data mining and machine learning;upper bounds;real-world applications;rare class;data mining;error rate;theoretical bounds
label quality;active learning;active learning;noise level;learning paradigm;learning strategy;active learning algorithm
term association;mutual information;language models;joint probability;translation model;naive bayes;information retrieval;naive bayes classifier;text classification;modeling approach;text classification;document representation;long-distance
skewed data;large amounts of;pattern recognition;class-imbalance learning;classification;accuracy compared to;classification accuracy;classification methods;static data;streaming data;skewed;recognition accuracy;data streams;ensemble approach
semi-supervised clustering;labeled samples;unlabeled data;active learning;misclassification costs;cost-sensitive learning;class distributions;highly skewed;biased sampling;active learning methods;training data;data set;private data;training samples;uncertainty sampling;algorithm produces;labeled data;class distribution;classifier;sampling strategy;random sampling
decision function;training examples;low rank;kernel matrix;vector machine;approximation error;model selection;benchmark datasets;theoretical guarantee;model selection
pre-defined;training set;instance labels;real-world;multi-label classification;instances;traditional classification;class hierarchy;instance-based;multi-label;classifier;classification algorithms
ensemble learning;real data sets;13;machine learning;classifier ensembles;classifier;diversity measures
generalized linear models;extreme values;classification;real-world;time series;conditional distribution;multiple linear regression;baseline methods;regression model;regression methods
enterprise search;relevance ranking;expert finding;ranking list;social network;expert finding;social networks;objective function
pruning strategy;training data;neural networks;classification;associative classifiers;uncertain data;real datasets;decision trees;associative classifier;associative classifier;classification model;accuracies;standard datasets;classification algorithms
weighting function;class information;data points;real-world;cluster validity;class distribution
prediction performance;topic model;topic models;topic model;topic distributions;approximate inference;collapsed gibbs sampling
proximity measures;clustering;data mining methods;continuous attributes;neighborhood structure
topic modeling;latent topic;opinion mining
clustering;clustering;small groups;sensitive information;real data sets;privacy-preserving data publishing;ad hoc
ensemble learning;large datasets;knowledge discovery;outlier detection;real datasets;score-based;outlier detection;distance-based;detection methods;aggregation techniques
context-aware;topic models;mobile devices;data set;real-world;contextual information;user context;mobile users
clustering;mining temporal;clustering method;time series;interesting patterns;patterns mined;clustering techniques;multivariate data;databases;temporal coherence;temporal data;subspace clusters;high similarity
real-world;prior knowledge;network structure;network structures;probability model;network data;community structure
greedy algorithm;case-based reasoning;recommender systems;context-aware;context-aware;recommendation process;mobile applications;machine learning techniques
document collection;classification techniques;world knowledge;real-world;classification methods;ground truth;human experts;classification performance;training samples;classification method;text classification;multi-label
social network analysis;text corpora;similar topics;data set;topic distributions
knowledge discovery;fast algorithms;metric-space;multi-dimensional data
clustering;high-dimensional;additional features;relational model;clustering techniques;unseen data;sparse data;nonparametric bayesian;relational data
clustering;distance measure;ground truth;similarity measure;template matching;data mining tasks;time series;data clustering;clustering techniques;computationally expensive;clustering;time series
clustering;clustering problem;privacy-preserving;privacy-preserving;social network;clustering methods;em algorithm
web pages;high-quality;online social networks;automatically extracting;expert search;named entities;classification problem;named entity;scoring function;entity recognition;web page
clustering;clustering;document collection;high-quality;large document collections;clustering method;text document;document clustering;clustering methods;text data sets;clustering quality;semantic relatedness;similarity measures
streaming data;cost-based;data items;data streams;evolving data streams;clustering algorithm;semi-supervised;clustering approach
clustering;clustering algorithm;real-world;density estimator;clustering techniques;business intelligence;sparse matrix;business intelligence;common assumption;cluster structures
users' preferences;collaborative filtering;recommender systems;implicit feedback;matrix factorization;explicit feedback;explicit feedback;latent factor;collaborative filtering;expectation-maximization
ensemble learning;classification performance;high-dimensional datasets;ensemble classifiers;15;21;data stream classification;data streams;naive bayes;13;ensemble classifier;algorithm called;feature subsets;feature selection;data streams;dimensional data;5;feature selection techniques;classifier;classification algorithms
intrusion detection;intrusion detection systems;data mining techniques;false alarm;detection accuracy;information systems;on line analytical processing
fast algorithm;large-scale;community detection;real-world networks;social networks;excellent performance
weighted association rule mining;weighted association rule mining;specific information;association rule mining
numerical data;mining patterns;hierarchical clustering;search space
concise representation;equivalence classes;low quality;classifier;association rules;operator;intrusion detection;correlated patterns
news events;clustering algorithm;energy function;detection method;news event
power law;real networks;mobile phone;communication networks
ranking algorithm;source documents;language model;graph-based;hierarchical structure;graph-based;document set
context information;external knowledge;web pages;latent dirichlet allocation;mobile search;mobile search;mining process;user preference;search queries;user activity;latent topics;graphical model;context-based;activity patterns
viral marketing;social interactions;influential nodes;social network;viral marketing;social networks;spanning tree;limited resources
web pages;click-stream;web sites;transactional databases;data sets;web site;stream data;patterns discovered;real world
recommendation systems;social networking;similar users;social network;social behavior;target user;popular items;user similarity;interesting items;social networking;popular items
theoretical framework;taking into account;database;large quantity of;real datasets;pattern-growth;interestingness measure;specific characteristics;pattern discovery;temporal data;temporal patterns
neural networks;probability density functions;sampling method;fault detection;data mining task;detecting outliers;support vector machines;intrusion detection;anomaly detection;graphics processing;sensor networks;network intrusion;data points;outlier detection;detect outliers;uncertain objects;pre-processing;data mining techniques;outlier detection;uncertain data;classification methods;cluster analysis
gene interactions;social network;databases
link prediction;link prediction;social networks;prediction techniques
domain experts;expectation maximization algorithm;mixture model;structural properties;detection algorithm;detection methods;probabilistic models;network data;network motifs;optimal number of;benchmark datasets
search space;matching algorithm;specifically designed to;multiple streams;real datasets;data streams;time series;network monitoring;similarity matching;noisy data;streaming data;video surveillance;process control;rates;error estimation;real life applications;longest common subsequence
search space;equivalence classes;mining frequent;real-life;broad applications;computationally expensive;operator
detection algorithms;hierarchical data;anomaly detection;detection accuracy;detection process;ranking schemes;ensemble method;real world;temporal data
search results;prototype systems;baseline algorithm;social network;social graph;user clicks
multi-valued;web pages;valuable information;web content;web pages;visual information;web data extraction;web page
kernel learning;optimal kernel;data analysis;labeled data is;feature space;kernel function;cross-domain;kernel discriminant analysis;labeled data;limited number of;labeled data from;low dimensional representation;high dimensional feature space;dimensionality reduction
random walk;random walk;random walk on;regularization framework;graph matching;protein-protein interaction network;random walks
data set;redundant features;synthetic data sets;association rule mining;naive bayes;feature values;feature selection algorithm;classification accuracy;classification results;effectively identify;feature subset;association rules;feature subset selection;real world data sets
mining results;frequent pattern mining algorithms;frequent patterns;visual representation;dimensional space;frequent pattern
feature weighting;classification;supervised learning;feature selection;real-world data sets;feature weighting;feature weights
real-world datasets;data analysis;database;community detection;rank-based;attack model;complex data;data utility;np-hard
information diffusion;sensitive information;data publishing;anonymization algorithm;social network;social networks;anonymization techniques;social networks;community structure
auc;synthetic data;sampling algorithm;sampling algorithms;sampling method;original data;class distribution;nearest neighbors;sampling technique
multiple classes;singular value decomposition;high computational cost;label space;feature space;classification performance;multi-label classification;covariance matrix
text mining;named entity;entity recognition
entropy-based;attribute values;numeric data;data mining and machine learning;uci data sets;discretization methods;provide evidence;real world;supervised learning;information entropy
conventional methods;local optima;locally linear;missing data;semi-supervised;missing values;low-dimensional
distance measure;redundant features;empirical mode decomposition;relevant features;classification;classification accuracy;feature extraction;time series;machine learning;features selected;feature selection;classification method;feature selection methods;linear regression;classifier;relevant features
human mobility;heterogeneous data sources;acm sigkdd;international workshop on
machine learning;classification algorithm;activity patterns;mobile phone;massive amounts of data
human behavior;large scale;high degree of;data collected from;temporal association rules;data obtained from;cost-effective;network data;optimization techniques
real dataset;learning phase;user profiles
high-complexity;large-scale;human activity;complex networks;spatio-temporal;data sets;rfid technology
temporal patterns;high accuracy;sensor data;classification;models trained on
spatial information;spatial data
large scale;mobile phone
large-scale;geo-spatial;statistical model;baseline methods;location-based services;human beings
user preferences;context-aware;tree-based;heterogeneous data;user's preference;real dataset;location-based social networks;excellent performance;model building
location based social networks;path planning;graph based;online social networks;classification;category-based;traffic information;ant colony algorithm;information networks
grid-based;mining algorithm;share similar;gps data;trajectory data
human mobility;predictive models;distributed computation;real-time monitoring;traffic monitoring;instance;distributed systems
spatial structure;data mining techniques;temporal dimension;daily activities;activity patterns;human activities;temporal structure
high efficiency;database;user oriented;user's preference;data model for;similarity search;theoretical analysis
case studies;location traces;mobility patterns;fine-grained;knowledge extraction;optimization techniques;modeling techniques;privacy preserving
human interactions;large volume;ground truth;ground truth data;gps data;data exploration;matching algorithms;trajectory data;road network;cost model;matching process;map matching;map matching
clustering algorithms;clustering techniques;multiple factors;network models
spatio-temporal;information systems;plans
data set;spatio-temporal;regression model;data collected
historical data;poisson process;upper bound
network analysis;outlier detection;spatial analysis;large-scale;outlier detection;case study;shortest path;data volumes;detecting outliers;million edges
database systems;large-scale;database;graphics processing;column-oriented;disk-resident;data management system
data collection;user behavior;conference on knowledge discovery and data;data mining;mobile computing;international workshop on;acm sigkdd;data mining;location based services;context-aware;context awareness
social relationships;classification;social web;business processes;sentiment analysis;international workshop on;social networks;product design;web contents;customer support;emotion recognition;opinion mining;social context;web users
query type;join algorithms;computation cost;highly optimized;query processing in;massively multiplayer online games;moving objects;moving object;join query;computational cost;cost model;query answer;join queries
tree matching;matching techniques;tree decomposition;computationally hard;tree edit distance;xml data;unordered tree;edit distance;data sources;approximate join;data-centric;real world;data integration
join algorithm;pruning techniques;real data sets;string-similarity;join algorithms;data sets;short strings;distance constraints;similarity joins;similarity join;data integration
pre-defined;video segmentation;distance function;video databases;data access;video database;broad applications;query processing;video sequences;similarity matching;local similarity;query sequence
large graphs;directed graph;real-world graphs;open source;reachability queries
nearest neighbor queries;filtering technique;database systems;distance metrics;probability distribution;indexing approach;linear programming;database;copy detection;operator;information extraction;probabilistic data;similarity search;similarity queries;sensor networks;data management;distance metric;relational databases;search problem;range queries
decision-makers;large scale;open data;web communities;database;on-line analytical processing;business intelligence;international workshop on;private data;cloud intelligence;social networks;data mining;analyzing data;data analytics
social media;web ir;international conference on;content analysis;program committee;information retrieval;retrieval models;rates;reviewing process;information retrieval;document representation
medical records;large volume;retrieving information from;high quality;information retrieval;natural language processing;long running;information extraction
document collection;query recommendation;concept hierarchy;computational costs;query logs;major limitation;search logs
query suggestion;query suggestions;real data;difficult queries;query suggestion;features including;search engine;query performance prediction
web search engine;user interactions;query suggestions;relevance models;machine learning;labeled data;learning framework;related queries;search logs;initial query
privacy-aware;classification models;visual content;classification;large-scale;textual features;query result;classifiers trained on;image search;predictive performance;user evaluation;highly sensitive;image classification;query results;visual features
image retrieval;feature space;storage cost;binary-code;large-scale;neighborhood structure;image data sets;binary code;distance based;nearest neighbor search
image retrieval;multi-kernel;high-dimensional space;multimedia applications;large-scale;real-world;locality-sensitive hashing;similarity search;retrieval performance;multimedia retrieval;similarity search;multi-kernel;content-based image retrieval;locality-sensitive hashing;multiple kernels;retrieval applications
search results;ranked list;naturally leads to;diversity measures;search result diversification
relevance model;relevance models;information retrieval
web pages;search performance;web search results;ranked list;search experience;result page;user behavior;multi-aspect;search result;relevant answers;search engine;information seeking;relevant information;user study;search tasks
user behavior;search process;document length;retrieval results;user-oriented;test collection;user study;effectiveness measures
rank based;takes place;real life;user effort;information retrieval
search results;user preferences;web search;heterogeneous sources;result page;information retrieval;aggregated search;selecting relevant;preference data
semi-structured;large amounts of;keyword queries;web applications;ad-hoc;search techniques;large-scale;ad-hoc;object retrieval;database;hybrid approach;data describing;large data sets;ranking algorithms;automatically generating;search effectiveness;open data;object retrieval;entity retrieval;retrieval techniques
search results;keyword-based search;real datasets;complex query;margins;finding similar;evaluation measures
utility function;search results;mechanical turk;document summarization;structured documents;effective search;machine learning;retrieved documents;query-specific
recommendation approaches;data collection;average precision;collaborative filtering;ranked list;context-aware;learning algorithm;implicit feedback;ranking problem;explicit feedback;contextual information;average precision;individual users
real world;algorithm to compute;recommender systems
result diversification;portfolio theory;collaborative filtering;latent factor models;user preference;target user;wide range;text retrieval;latent factors;latent factor;latent factor
search results;search systems;user behavior;user behavior;search experience;long-term;search session;short-term
search results;click logs;web search;user interactions;query suggestions;user behavior;predict future;search result;search engine;document relevance;log data;latent variables
search results;relevance feedback;user preferences;user interactions;search session;personalized search;search experience;search tasks;information-seeking;data collected;search sessions;data set;information retrieval performance;user search behavior;user interaction;prediction models;document usefulness
query quality;highly correlated;search engine results;user evaluation;relevant documents
data structure;data collection;suffix tree;trade-offs;document retrieval;disk access;retrieval tasks;data structures;query processing;indexing structures;search tasks
real-world datasets;large-scale;index maintenance;index entries;index structures;index structure;text search;web archive;web archives;keyword query;query-processing
source code;control systems;zs:redun;index structures;compression techniques;web archive;document collections;query processing;web archives;index size
large amounts of;trade-offs;textual data;textual data;search engines;wide range;result quality;ranking functions;web search engines;query terms;query processing
web search;web content;health information;browsing behavior;search logs
web search;semi-supervised;unlabeled data;user behavior;supervised approach;labeled data;search engine;web search;labeled and unlabeled data;search behavior;search logs
search results;commercial search engines;social annotations;learned models;learning task;query classes;social network;web usage;real-world;optimization criterion;user study;social annotation;social annotations
data-driven;local search;black-box;large-scale;analysis reveals;mobile devices;query log;information retrieval models;web search;average precision;search logs
clustering;search intent;web search;multiple tasks;click data;user behavior;log analysis;search result;clustering algorithm;log data;user clicks
search task;search results;web search engine;long-running;search sessions;behavior patterns;query log;predict future;search engine;search tasks
gather information;search results;data sets;web pages;multi-aspect;relevant pages;ranked list;query aspects;query (dependent;search paradigm;query aspects;search engines;baseline methods;search engine;relevant information;summarization" method;additional information
historical data;browsing behavior;real-world applications;semi-supervised;online news;models trained on
query nodes;real-world;query logs;current web;search engines;highly correlated;large graph;search engine;real world;sufficiently high
type-ahead search;access methods;type-ahead search;user types;high-efficiency;large data sets;search paradigm;random access;queries efficiently;keyword query;query keywords
incremental algorithm;web graph;global convergence;performance guarantees;heterogeneous data;counterpart;real-world networks;structure information;adjacency matrix;dynamic networks;similarity estimation
semantic classes;matrix factorization;large numbers of;specific topics;document classification;relevance ranking;text documents;real-world;readability;million documents;text data;real-world applications;matrix factorization;knowledge discovery;topic modeling;large datasets;latent structure;topic modeling;modeling method;objective function
web-based;web pages;sampling-based;electronic commerce;real-world;web sites;web spam;large corpora;search engines;automatically generated;small samples
web spam;data analysis;spam pages;bipartite graph;web spam;search engine users;link-based;link structure;web search engines
processing units;generation algorithm;hash functions;user generated;detection approach;detection rates;document collections
exploratory search;search engines;semantic relatedness;category structure;exploratory search
search systems;data gathering;user study;search interface;iterative process
search results;search task;information quality;search interfaces;search techniques;users interact with;complex tasks;aggregated search;individual preferences;web search results;aggregated search;search services;user interaction;search tasks
image retrieval;social media;taking into account;social interactions;browsing behavior;ranking algorithms;ranking methods;social interaction
music information retrieval;clustering;acoustic features;signature scheme;large scale;probability distribution over;temporal dimension;data storage;learning framework;feature vectors;low-level;music collections
language pairs;parallel corpus;scoring functions;longest common subsequence
closely related;information extracted from;filtering methods;personalized recommendations;access patterns;social networking
optimization framework;user feedback;data set;dynamic content;rates;main idea;online content;multiple objectives
user preferences;classification;svm based;textual features;preference based;large scale;feature based
search topics;modeling approach;query generation;query terms;specific characteristics;prior art;query expansion;global analysis;retrieval effectiveness
term weights;interactive query expansion;relevant documents;simple queries;diverse set of;user effort;automatic query expansion;retrieval models;query reformulation;trec ad-hoc;search queries;high quality;search engine;normal form;structured queries;query expansion;query terms;query dependent
keyword queries;complex queries;trec collections;query operations;verbose queries;reformulated queries;subset selection;tree structure;search queries;retrieval performance;query distribution;tree based;natural language
proximity measures;trec collections;relevant documents;query topic;direct comparison;proximity-based;relevance feedback;proximity-based;information retrieval performance;term frequency;optimal parameters;expansion terms;relevance model;query expansion;query terms;term proximity
social media;large-scale;user behavior;topic model;user generated content;latent topics
high-accuracy;social interactions;online users;latent factor models;users' behavior;social network;supervision;social networks;social graphs;semi-supervised;social behavior;decision making;social ties
social-network data;latent dirichlet allocation;lda) model;social-network analysis;topic models;social network;instance;graph based
rewrite rules;domain experts;enterprise search;classification approach;heuristic approaches;relevant documents;closely related;enterprise search;large data sets;machine-learning;search engine;search results
search experience;high quality;temporal dynamics;instance;search engines;time-series
discriminative training;globally optimal;filtering step;multiple types of;scoring functions;search engines;hidden markov model;unified framework;promising candidates;hidden markov model;working set
response times;retrieval effectiveness;query efficiency;combining multiple;queries submitted to;accurately predict;pruning strategies;query response times;efficient retrieval;online algorithms;web crawl
query results;web search;query log analysis;query response time;machine learning;queries issued;search engines;query result;web search engines;query log;hit rate
web search;search engine;search engines;query logs;query results;web search engines
online social media;collaborative filtering;users interact with;information retrieval;update streams;latent factor model;relevant information;access information;recommender systems
large number of;social network;social relations;baseline methods;information overload;contextual information
synthetic data sets;keyword queries;spatial web objects;relevant objects;search space;retrieval methods;mobile applications;distance-based;data structure called;retrieval techniques;visual features
received increasing attention;large numbers of;social web;applications including;meta-data;information extracted from;additional information;geographic information
search results;appearance-based;large-scale;indexing approach;block-based;digital cameras;hit rate
location-based;free text;training data;social media
dynamic programming algorithm;local context;named entities;selection criteria;user-defined;random walk model;real-life;named entity;entity recognition;global context;ground truth;highly-ranked;automatically discover
classification problem;classification;news sources;competing methods;supervised machine learning;large datasets
instances;relational structures;support vector machines;kernel-based;question/answer;large-scale;structural relationships;ranking model;qa) systems;structural relationships;tree kernels;ranking tasks;feature spaces;supervised learning;question answering;learning curve
search systems;search results;high level;ground-truth;data collections;evaluation measure;relevance judgments;ranking models;data set;evaluation measures;ranking model
search results;models learned;real-world;improving search;ranking models;unified framework;learning framework;result quality
real-world;probabilistic framework;community question answering;community question answering;recommender systems
community question-answering;question-answering systems;learning framework;answer ranking;learning method
data-driven;auxiliary;document classification;semantically meaningful;classification accuracy;real data;classification accuracies;baseline methods;semantic information;user study
web search;web search engine;large-scale;community question answering;large number of;information seeking;web search;search behavior
image retrieval;feature space;domain adaptation;search performance;object retrieval;content-based retrieval;spoken document retrieval;instances;baseline methods;relevant data;search intent;content-based retrieval;test collection
result merging;result merging;training data;logistic regression;mixture model;information sources;federated search;semi-supervised learning;documents retrieved;retrieval algorithms;combination weights;probabilistic model;document scores;heterogeneous information sources
user queries;multiple sites;replication;user interests;search engines;processing queries
search results;real users;competitive performance;search result;random variable;base-line;multiple aspects
result diversification;result diversification;regularization;click logs;explicitly represented;anchor text;random walk;topic models;reformulated queries;click logs;web resources;model parameters;query logs;retrieved documents;latent topics;multi-faceted
search results;multiple times;user preferences;preference-based;document retrieval;preference judgments;user study
clustering;trec data;large-scale;high quality;growing importance;rates
query segmentation;web search;retrieval results;web search;segmentation algorithm
statistical significance;average precision;document collection;ir evaluation
trec test collections;retrieval effectiveness;ir systems;relevance judgments;optimization problem;information retrieval;mathematical formulation;test collections;explicitly models;baseline methods;search engine;aware query;mathematical framework;test collection;selection algorithm;computationally intractable
document expansion;temporal properties;short documents;large number of;short texts;information retrieval;frequency information;digital library;document expansion
query operators;retrieval effectiveness;ranking function
retrieval effectiveness;language model;information retrieval;retrieval performance;natural language processing;information retrieval;average precision
natural language queries;retrieval effectiveness;retrieval models;information retrieval;term dependencies;higher-order;retrieval framework;query terms
query intent;classification;large-scale;real-world;labeled training data;classification methods;individual objects;regularization framework;information retrieval;graph regularization
classification;ranking methods;semi-automated;text classification;ranking method;text classification;semi-automated;classification error;classifier
document streams;probability estimates;statistical properties;classification model;classification;text search;model offers;classification accuracy;data sets;classification models;rates;stream classification
sample selection;low-quality;web applications;high-quality;test data;classification;user-generated;quality assessment;real-world;prediction performance;user-defined;machine learning;wikipedia articles;classification problem;user-generated content;classification approaches;training corpus;classifier
knowledge-based;query-biased;sentiment analysis;sentiment classification;external sources;classifier;user-oriented
relevance feedback;search strategies;coarse grained;ranked list;search sessions;fine grained;information retrieval;1;exploratory search;3;2;problem solving;query logs;long-term;contextual information;complex tasks;information retrieval;search behavior;user input
ad hoc retrieval;retrieval strategies;web search
sentiment analysis;federated search;high quality;product review;text data;search techniques;search result;social networks;opinion mining
filtering algorithms;target task;labeled instances;knowledge transfer;source tasks;labelled data;special case;knowledge base;models learned;information filtering;transfer learning;text mining;information filtering;automatic methods;source nodes;knowledge representation;knowledge transfer;transfer learning;knowledge base;network analysis;high-quality;machine learning;auxiliary data;source task;learning method
boolean queries;data fusion;query formulation;data fusion;query generation;information retrieval;recall-oriented;common practice;primary goal;modeling methods
information diffusion;diffusion process;network applications;internet users;cascade model;social network;viral marketing;3;2;social networks;increasing attention;opinion mining;individual users
search results;readability;desired information;web search engine;topical relevance;considerable effort;linear combination;retrieval models;information retrieval;user's search;relevance criteria;portfolio theory;wide range;result set;disk space;readability
keyword-based search;tf-idf;social media;spatial data;language model;user experience;meta-data;language models;temporal dimension;diverse sources;ranking models;1;instance;3;2;5;4;ir systems;mixture models;emerging topics;hierarchical structures;real-world;data streams;topic models;social media sites;relevance scores;ad-hoc;rank documents;user generated;kullback-leibler divergence
social media;evaluation methodology;real-world;large-scale;event detection;ranking problem
multi-type;retrieval systems
training set;data analysis;maximum likelihood estimation;naive-bayes;smoothing techniques;data sparsity;hyper-parameters;naive bayes;prior knowledge;visualization tool;bayesian approach;text classifiers;supervised learning;model training
interaction data;web applications;user interactions;web application
retrieval model;search engine
amazon's mechanical turk;open source;ranking tasks;automatic generation of
pre-defined;document level;transitive closure;search engines;semantic information;keyphrase;automatically extracts;natural language
data set;1;public-domain;data management;information retrieval
ir systems;search services;search interface;search engine
features extracted from;recommendation systems;ranking function;database
interactive search;search tasks;information retrieval tasks
web search queries;search queries;web sites;web search;analysis tool
knowledge bases;ranked list;query relaxation;data sources;search engine;retrieve information;query results;search interface
image retrieval;sketch-based;user interface;similarity search;user-friendly;content-based image retrieval
classification;user feedback
web-based;video content
image retrieval;mobile devices;user-generated;ranked list;image annotations;text-based;current web;image collection;image search;user interface;search engines;image collections;sparse set of;image annotations;query terms
instance;selection problem;sponsored search;database;optimization problems;dual problem;data representation;document corpus;optimization problem;computational advertising;similarity search;query string;search engine;page ("content
natural language processing;answer questions;natural language
search quality;search engines;web scale;search engine
enterprise search;open source;end users;search applications;makes sense;information retrieval;search tools;user experiences;web search;search services
algorithm's performance;information derived from;entity extraction;sentiment classification
hybrid model;ad-hoc retrieval;ir) techniques;ir techniques;information retrieval;ad-hoc information retrieval
entity search;rdf graphs;rdf) data;retrieval model;query terms;generation process;semantic web
semantically related;large corpora;free-text;topic models;topic model;identify patterns
labeled data

readability
large-scale;language model;prediction quality;query performance;5;web collections;retrieved documents
target class;labeled data is;cluster-based;cluster-based;clustering model;information retrieval;text corpus;classification approaches;clustering algorithm;individual classifiers;classification problems;classifier
traditional collaborative filtering;collaborative filtering;recommender systems;user feedback;short term;long term;enabling users to
web search;temporal information;query types;search result;user study;page content
relation extraction;kernel-based;content extraction;kernel based;vector machine;social relations;tree kernels;automatically extract;linguistic knowledge
accurately identify;classification;query segmentation;extraction task;named entities;search queries;search engine results;named entities;search engine queries
pruning strategy;trec web track;learned model;pruning strategies;search engine
topical relevance
answer quality

heterogeneous information sources;classification approach;online news;classification;individual users
squared error;user study;recommender systems;recommender systems
user studies;users' preferences;recommender systems
medical records;search task;query term;information retrieval;medical records;query terms
ranking model
1;latent semantic indexing;visual information;similarity matrix;random-walk
learning scheme;semi-supervised learning;regularization;computational cost;regularizer
automatically discovering;baseline methods;graph analysis;real world;social networking;hits algorithm
search systems;specific task;highly relevant;search tasks
social networks;real-world;link structure;social network;latent factor model
search results;term weights;ranking function;information retrieval;document structure;term frequency
ambiguous queries;ranking quality
classification;data types;genre classification;instance;svm classifier;classifier
ranking algorithm;search-engine;search session;user interface;relevance ranking
search results;search result;result page;search engine
statistical methods;ir techniques;automatically identifying
ranking performance
local features;global information;bayesian logistic regression;rank documents;difficult queries;relevance feedback;global context;average precision;modeling techniques;negative feedback
collaborative filtering;majority voting;weighted voting;relevance judgments;relevance feedback;probabilistic matrix factorization
monte carlo simulation;ranked list;large number of;monte carlo simulation;performance predictors;score distribution;average precision;document rankings
ranking results;real-world;retrieval scores;machine learning;retrieval models;retrieval model
training data;web search;user intent;search result;temporal features;machine learning techniques;web search

statistical translation;contextual constraints;context-sensitive;context-free;cross-language information retrieval;term dependencies;black box;cross-language information retrieval;test collection
hand-held;document sets;mobile devices;document length;readability
query suggestion;query suggestion;search result;input query;query suggestions
web documents;training set;archived data;classification;hybrid approach;large scale;document retrieval;naïve bayes;information retrieval;data set;case study;user interests;document collections;parallel processing;similarity based
text retrieval;information network;information retrieval tasks
information retrieval;retrieval systems;relevance judgments;search engine
result set;combinatorial optimization
ad-hoc;search task;end user;relevance judgments;retrieval evaluation
data-set;sentence extraction;sentiment classification
ad-hoc;ir metrics;real users
sensitive data
gradient descent;iterative learning;training sets;parallel execution
data set;time series
strong correlation
search results;search task;retrieval effectiveness;user feedback;predictive model;retrieval performance;user activity
monte-carlo;large number of;relevance judgments;retrieval evaluation;retrieval systems;probability distribution;sampling technique
retrieval performance;probabilistic information;location information;retrieval model;information retrieval
pruning strategy;scheduling algorithm;query efficiency;information retrieval
search result;3
context information;active learning strategies;rule-based;supervised learning;conditional random fields;customer reviews
search results;term frequency;classification;classification accuracy;short text;information retrieval;maximum entropy;short texts;labeling process;short text;query words;classifier
social media
survival analysis;click logs;click logs;implicit feedback;search engines;ranking quality
search results;search systems;search-result;implicit feedback;relevance feedback;search result;search engines;users' search
data collected from;user study;search topic;relevance judgments
active-learning;active learning;string similarity;logistic regression;classifier;small-scale
query log;ir evaluation;information access
anchor text
training data
upper bounds;query results;web search engines;web search engines;query results
probability ranking principle;parameter estimates;markov-chain;search process;interactive information retrieval
preference judgments;evaluation cost;user preferences
federated search;results merging;similar documents
context-aware;markov chain;high-level;sharing information;category information
prediction problem;classification models;classification problem;extracted features
classification process;textual features;high accuracy;machine learning techniques
social media;retrieve information;natural language processing
search evaluation;labeled data;intelligent systems;human computation
user ratings;online reviews;digital camera;retrieval tasks;related information;information retrieval;numerical values;web resources;fundamental problem;online product reviews;opinion mining;decision-making
management systems;tf-idf;ir research;ir models
multi-modal;ir researchers;information retrieval;annotated corpus;instance;domain-specific;multi-lingual
search techniques;search applications;user requirements;information retrieval;instance;domain-specific
image retrieval;java-based;open-source;large scale;real-world;design principles;information retrieval;information extracted from;visual information;main features;machine learning techniques;semantic interpretation;video retrieval;information retrieval;content-based image retrieval
graph mining;large-scale;large scale;information retrieval applications;information retrieval;ranking problem
prediction methods;prediction quality;performance predictors;performance prediction;query performance prediction;query performance prediction
information seeking;ir systems;evaluation techniques;information retrieval
information retrieval;search engines;user models;session-based;evaluation measures;user interaction

digital libraries;automatic methods;unsupervised techniques
data mining;high-dimensional;life sciences;data collection;large-scale;data mining;emerging applications;international workshop on;knowledge discovery;biological information;data driven;databases;medical records;great promise;systems biology;biological datasets;biological data;data mining techniques
static data;decision support systems;high end;applications including;sensor data is;knowledge discovery from sensor data;problems require;data mining approaches;homeland security;mobile devices;raw data;international workshop on;massive volumes;sensor networks;data streams;long-term;high-priority;data fusion;distributed data;knowledge discovery
web site;learning framework;graphical model
detection problem;random walk;data source;web data;case study
large scale;real-world networks;ground-truth;information networks
probabilistic model;social influence;social networks
answer quality;community based;high quality;online users;community based;information seeking
probabilistic model;search efficiency;small sets of;higher level
learning process;linked data;domain-independent;instances;specific domains;instance;data sources;labeled data;classifier;data integration
fine-grained;coarse-grained;maximum flow;equivalence class
semantic annotation;web pages;structured data;structured data from;web sites;knowledge discovery;human efforts;structured information;learning framework
users' interests;user's interests;social network analysis;large social networks;topic model;social network;sentiment analysis;generative model;influential users
graph-based;text summarization;benchmark datasets;random walks
web pages;suffix tree;multi-type;real data;data records;higher accuracy;web document;automatic methods
service provider;classification;high quality;social networks;sina weibo;classifier
computational geometry;location-based;approximation algorithm;external-memory;spatial databases
query execution;query evaluation;location-aware;user query;nearest-neighbor;location-based services;arbitrarily complex;spatial queries;knn-join;join predicates;plans
lower bound;database;optimal algorithms;result page;dynamically generated;data acquisition;theoretical results;hidden databases;search engines;worst case;real datasets;issue queries;allowing users to;search interface
search results;algorithm finds;dynamic programming;real datasets;search problem;query processing;specific problem
approximation algorithm;queries efficiently;greedy algorithms;plans;np-hard
xpath queries;multiple views;algorithm runs in;xml queries;worst-case;answering queries;query equivalence;probabilistic xml;query answering;query results;single view
approximation algorithms;probabilistic databases;query evaluation;plans;efficient query evaluation;conjunctive query;probabilistic database;database;data model;probabilistic databases;evaluation strategy;databases;tuple independent;tuple-independent;tree-decomposition;markov logic;data analytics
data-driven;database
processing units;evaluation strategy;distributed computing;programming model;23;join processing;parallel databases;data volumes;join query;cost-effective;join queries;olap applications;join operations
search algorithm;optimizer;cost-based optimization;plan space;cost-based;large datasets;information needed
labeling scheme;reachability queries;efficient query evaluation;multiple views;workflow execution;data items;fine-grained;workflow views
database design;query optimization;functional dependencies;inference rules
data sets;data redundancy;query performance;select-project-join;databases;relational databases;query engine
data analysis;commercial systems;synthetic datasets;decision support applications;window functions;optimization opportunities
query optimization;optimizer;data flows;black box;general-purpose;operator;data processing;join-order;user-defined functions;data flow;relational dbms;data analytics
optimizer;programming model;big data;data flows;highly competitive;shared memory;iterative algorithms;machine learning algorithms
clustering;link analysis;fault tolerant;query workloads;social network;competing methods;olap queries;data analysis;iterative algorithms;data-centric;user-defined functions;programming model;ad hoc;ad hoc queries
existing indexes;reachability queries;directed graph;general problem;wide range;real datasets;small world;path queries
large graphs;network traffic;reachability queries;performance guarantees;parallel computation;regular expression;real-life;evaluating queries;performance guarantees;real world;node labels
keyword queries;natural language text;index size;text corpora;indexing scheme;efficient indexing;evaluating queries;wide range;coding scheme;search effectiveness;structural information;natural language
query evaluation;open-world;database theory;finite model;query answering;conjunctive queries;query containment;query languages
frequent itemset mining;frequent itemsets;transaction databases;basis set
query results;naïve;high accuracy;16;low rank;low-rank;sensitive data;differentially private;privacy guarantees;query processing;privacy-preserving;random noise;real data;lower bound;margins;query result;low-rank
logistic regression;sensitive information;regression analysis;differentially private;optimization problem;linear regression;theoretical analysis;case studies;main idea;produce accurate;regression models;objective function
social-networking;uncertain graphs;social-network;privacy concerns;finer-grained;desired level of;real-world networks;social graphs;graph data;data collected
information loss;sensitive-attribute;privacy threat;privacy guarantee
graph structures;online social networks;large scale;graph datasets;real-life;sampling strategies;rank correlation
construction cost;database systems;real datasets;theoretical analysis;instance;baseline methods;index size;operator;temporal data;quality guarantees
analytical queries;memory-consumption;database management systems;database;intermediate results;main memory;query performance;business intelligence;database systems;query processing;databases;online analytical processing;transactional data;working set;online transaction processing
memory footprint;database systems;data layout;data structures;highly interactive;column-oriented;speed-ups;large datasets;column-oriented;user queries;ad hoc queries
database instances;database;communication costs;fine-grained;main memory;skewed;workload-aware;instance;distributed transactions;modern hardware;database systems
protocol called;instance;concurrency control mechanism;replication;serializability;transaction management;concurrency control
database;database applications;database server;database-backed;database-backed;database application;stored procedures;application server
hybrid approach;real data sets;large numbers of;accuracy compared to;minimum number of;data cleaning;data sets;heuristic approach;human-machine;data integration;np-hard
selection problem;voting scheme;blog data;heuristic algorithm;error rate;error-rates;error rate;decision making
total number of;dna sequences;databases;mathematical analysis;filtering techniques;long queries;upper bound;protein sequences
high level;structural features;time series;application domains;structural characteristics;data set;salient features;higher accuracy;dynamic programming approach;local features;temporal data;feature based
past queries;data analysis;spatial data;graph model;scientific domains;build models;high accuracy;massive amounts of data;latent structure;range queries
database systems;high throughput;multi-core;image data;real-world data sets;cost-effective;spatial database;spatial databases
explicitly model;training examples;sql queries;statistical techniques;operator;resource usage;cardinality estimates;query optimization;estimation accuracy;cost models;data examples;robust estimation;plans;real-life;training instances;database query processing;resource-usage;resource consumption;database;large scale;microsoft sql server;statistical models;estimation errors
mining framework;brute-force approach;np-complete;real data;social tagging;helping users;wide range;result quality;analysis tasks;similar items;desired information;similar users
distance measure;distance functions;database;space overhead;index structure;time series;distance measures;generic framework;retrieval performance;databases;metric spaces
replication;query execution;hadoop mapreduce
personalized search;large-scale;social network;supervision;data set;baseline methods;content delivery;key features
multi-version;low-overhead;open source;flash-based;higher throughput;database;disk access;transaction throughput;large scale;database server;cache management;persistent database
data structure;basic operations;data locality;data structures;data sets;compact data structure;storage systems;databases;times faster than
genetic programming;real-world;data cleansing;distance measures;data sources;data transformations;aggregation functions;data integration
frequent itemset;mining frequent itemsets;uncertain databases;uncertain data;databases;random variable;mining frequent itemsets;benchmark data sets
error-prone;privacy-preserving;mining algorithms;location-based services;uncertain data;real datasets;time series;application domains;wide range;data aggregation;sensor measurements;time-series;temporal data;monitoring systems
data cleaning;real world
parallel database;data processing;big data;database;energy-efficient;large-scale data analysis;key parameters;energy efficiency;design space;parallel dbms;database cluster
web pages;data record;big data;valuable information;collect data;international workshop on;connected components;social networking;programming models
user browsing;multiple domains;recommendation systems;cross domain;text search;learning problems;multiple types of;history data;social network;learning tasks;social network;cross domain;knowledge discovery;data sets;social network data;product search;search engine;news search;international workshop on;knowledge transfer;training and test data
gaussian processes;gaussian processes;machine learning
inference method;mixture model;mixture model;dirichlet process;variational bayes;dirichlet processes;image categorization
extraction algorithm;unsupervised learning;level features;data set;heuristic method;feature analysis;dimensional data
computational efficiency;feature extraction;pattern recognition;feature extraction;information processing;machine learning;dimensional data;maximum margin;real-world applications;maximum margin;dimensionality reduction
auc;informative features;skewed;imbalanced data;imbalanced data;class distributions;negative examples;linear combination;imbalanced data sets;margin-based;margin;classification task;feature selection;feature selection;feature selection methods;small samples;high dimensionality;classification problems;margin;feature selection
ranking techniques;large numbers of;feature selection techniques;real-world;selected features;models built;classification performance;feature selection;feature selection technique;feature selection techniques
probabilistic model;feature matching
parameter values;factorization method;gradient descent;automatically discover;regularization
regularization term;regularization;labeling effort;labeled features;labeled features;regularizer;regularization;labeled documents;instance;unlabeled documents;feature selection;text classification;building classifiers
real-world datasets;classification models;feature selection techniques;classification;sampling techniques;class noise;feature subsets;feature selection;data mining;relevant attributes;high dimensionality;high dimensionality;feature selection
named entities;rule-base;machine learning
classifier fusion;feature vector;database;linear svm;classifier;fusion methods;handwritten digits;svm classifiers;feature vectors;recognition rate
years ago;computational model;pattern recognition;specific task;simulation results
model parameters;fast convergence;sufficient condition for;benchmark dataset;regularization parameter;training algorithm;algorithm converges;training algorithm
face recognition;common features;wavelet decomposition;input image;feature extraction;face recognition;image datasets;face image;multi-scale;chi-square;virtual worlds;binary pattern;recognition rates
extraction techniques;feature extraction methods;large database;database
noisy data
pattern classification;training set;data compression;image datasets;reconstruction error;pattern classification;classification accuracy;sparse representation;limited number of
multi-stage;domain specific;vector machine;algorithm called;multi-threaded;fault detection;data preprocessing;multi-threaded
context-sensitive;context-sensitive;context-free;incremental learning;rule sets;incremental learning;rule generation
high-dimensional;training data;large number of;training data;machine learning;large data sets;training data set;generalization ability;data set size;multi-class;machine learning
evolutionary algorithms;genetic algorithm;energy management;knowledge-based;plans
fuzzy classification;data point;labeling effort;unlabeled data;data points;vision applications;active learning;expression recognition;active learning;batch mode active learning;applications involving;instances;facial expression recognition;facial expressions;video sequences;classification problems;real world;unlabeled set;manual effort;classification model;vast amounts of
data points;machine learning applications;database;clustering process;computationally expensive;instance;simulated data;subspace clustering;clustering algorithm;dimensional data;perform poorly;subspace clustering
hybrid method;early stage;optimum solution;ant colony optimization;genetic algorithm
factors affecting;medical image;medical research;data-mining;classification;time-series;great potential;machine learning;deep-learning;training datasets;fully automatic;time-series;magnetic resonance imaging;image data
clustering algorithms;unstructured text;real-world;application domain;forensic analysis;clustering documents;automated methods;document clustering
evolutionary algorithms;hypothesis space;structural properties;real-world;learning algorithm;naive bayes;effective search;linear svm;text data sets;text classifiers;text classification;algorithm (called
nearest neighbor;latent variables;linear dimensionality reduction;nearest neighbor;reconstruction error;recognition tasks;dimensional data;kernel methods;low-dimensional;dimensionality reduction
density-based;clustering algorithms;data analysis;dynamic range;input data;output quality;data streams;parameter selection;streaming data;data stream;grid-based;data streams;clustering quality;clustering
density-based clustering;density-based clustering;clustering approaches;global models;very large datasets
clustering;fine-grained;robotic systems;clustering;analyzing data
image datasets;network models;search procedure;fitness
unseen data;classification;classification;svm) classifiers;vector machine;classifier;fuzzy inference system;neural network;artificial neural network
neural networks;gaussian distributions;monte carlo sampling;state estimation;posterior distribution;state estimation;neural network
gradient-based;10;simulated annealing;neural networks;optimization method;neural network;9;neural network;training algorithm
state space;dynamic programming;state spaces;control policy;reinforcement learning;standard benchmarks;learning method;locally weighted
transfer learning;auxiliary;document classification;classification accuracy;learning task;training examples;selected features;learning tasks;target task;auxiliary data;selection criterion;transfer learning;learning method;transfer-learning
numerical experiments;vector machine;vector machine;time series;real data;hidden markov model;machine learning techniques;high probability
feature space;underlying structure;input data;margin;variable-size;hidden markov models;machine learning;classification performance;hidden markov models;prior information;svm classifier;classification problems;structured data
quadratic programming;greedy algorithms;vector machine;semi-supervised;semi-supervised learning methods;optimization algorithms;semi-supervised learning;graph-based;semi-supervised;support vector machines;efficient approximate
meta-learning;instances;optimization methods;meta-learning;multilayer perceptron;instance;network model
kernel machines;clustering algorithms;computational complexity;multi-class;information-theoretic;cluster membership;data clustering;optimization problem;conjugate gradient;maximum margin;kernel methods;large number of
support vector machines;kernel parameters;hyper-parameters;benchmark data sets;model selection;computational cost;support vector machines;margin;separability;classification error
false positive;classification;classification;rates;classifier;detection task;detection rate
limited resources;typically performed;classification techniques;classification
line segment;databases;vector machine;large margin;large margin;classifier;training samples;svm) classifier;binary classifiers;large margin classifiers;margin;classifier;multi-class classification problems
signal processing;classification;cross-validation;multi-scale;machine learning techniques;seismic data;clustering;feature extraction technique;learning problem;machine learning techniques;data characteristics;data collected;makes sense;machine learning;classification rate;classification;database;machine learning;data set;incoming data;low-dimensional;information theoretic measure;classification algorithms
flow field;machine learning methodology;flow field;time series;error analysis;flow field
information extraction;classifier;extraction accuracy;classification;information extraction system
discriminant analysis;classification;classification;classification accuracy;comprehensive evaluation;micro array data;data sets;classification results;principal component analysis;rule based;low dimensional space
feature space;classification;fixed size;naive-bayes classifier;training samples;classifier
instance labels;inference techniques;machine learning algorithms;models built;machine learning;classification performance;data sets;statistical relational learning;similarity-based;relational learning;classification task;benchmark data sets;relational data
image features;classification;classification;vector machine;image sequence;classification method;classifier
sample set;highly complex;real-world;prior knowledge;sample set;training samples;low dimensional;learning problems;optimization process;real world applications;learning task
human supervision;intelligent agents;reinforcement learning;application domains;reinforcement learning;high-level;learning framework
state space;state spaces;learning problem;reinforcement learning;action space;continuous-state;relevant information;state spaces;state-action
collected data;signal processing;active learning;classification accuracy;reinforcement learning;reinforcement learning;uncertainty sampling;queries issued;uncertainty sampling;learning framework
hidden variable;hidden variables;penalty term;time series;hidden variables;prediction performance;neural network
data points;autonomous navigation;normal distribution;long-term;supervised learning algorithm;supervised learning;data distribution
real-world;particle filter;reinforcement learning;particle filter;simulation results;continuous state
learning process;semi-structured;data items;relational tables;classifier performance;supervised learning;machine learning algorithms
data source;classification;classification;data mining applications;compared with conventional;multi-classifier;statistical significance;data sources;classification results;class label;statistical significance;prediction model;multiple sources;classification results
learning process;training data set;unlabeled data;feature set;classification;breast cancer;learning task;semi-supervised learning;predictive modeling;learning paradigm;labeled data;feature sets;baseline methods;learning problems;single view;breast cancer;semi-supervised classification;class label
computationally expensive;pair wise
lighting conditions;accuracies;machine vision;image processing;large-scale
learning process;learning agent;large number of;reinforcement learning;learning speed;real world;simulation results
markov process;learning method;learning method;dynamic environment
learning process;target task;rule set;source task;reinforcement learning;reinforcement learning;transition probability;transition model;transfer learning;transition model;learning method
optimization problems;optimization method;dimensional space;simulation results;global optimization
domain adaptation;classification models;classification;probability distributions;target domain;source domain;domain adaptation;data set;instances;classification models;classification accuracy;machine learning algorithms;health monitoring
probability estimates;real-life problems;statistics based;online advertising;common assumption;wireless networks
human behavior;singular value decomposition;features extracted from;feature space;machine learning;correlation coefficient;feature-spaces;recommending items;support vector machines;radial basis function
ensemble learning;risk analysis;generalization performance;risk analysis;instance;margin;ensemble learning
network analysis;detection problem;face book;online social networks;demographic information;clustering algorithms;social network analysis;community detection;closely related;topological structure;existing graph;social networks;clustering;connected components;graph datasets;clustering algorithm;graph clustering;social networks
data matrix;sensory data;classification;time series;classification
growing rapidly;recommender systems;recommender systems;neural network
clustering;discriminant analysis;feature vectors;learning models
label quality;label quality;learning algorithms;noise level;learning strategy;active learning;real-world applications;learning algorithm;supervised learning;learning performance
large scale;step size;logistic regression;input data;regression analysis;optimization method;conjugate gradient;conjugate gradient;line search;times faster than;logistic regression;computational cost;classification problems;objective function;efficient optimization;posterior probabilities
data-driven;parameter estimation;parameter estimation;mathematical models;large number of;random sample;data sets;optimization method;nonlinear regression
concept discovery;scalability issues;multi-relational;search spaces;query processing;concept discovery;learning task
recommendation systems;game-theoretic
intrusion detection;classification techniques;evaluation methodology;classification accuracy;knowledge-based;knowledge based;statistical learning
high-dimensional space;latent variables;probabilistic model;bayesian approach;posterior distribution;approximate inference
kullback-leibler;target object;bayesian inference;prior distribution;state-space;moving objects;computationally tractable;posterior distribution;statistical model
clustering;data-mining approach;optimization strategies;decision-making;supervised learning algorithms;similar behavior;data collected;input variables;data-mining techniques
special characteristics;classification;internet users;multimedia information retrieval;multi-label classification;binary classifiers;classifier
image retrieval;higher precision;similarity metric;visual attention;retrieval methods;content based image retrieval;segmentation method;object based
nearest neighbor algorithm;document classification;classification;real-world;web forums;statistical techniques;machine learning;term frequency;support vector machines;kernel functions;classification algorithms
gaussian mixture model;local optima;mixture model;expectation maximization;minimum description length;gaussian mixture models;evolutionary approach;evolutionary algorithm;mixture models;model selection criterion
variational inference;bayesian inference;normal distributions;obtained by combining;hidden) variables;expectation propagation;bayesian learning
stereo camera;statistical properties;gaussian processes;point cloud;gaussian process regression
predictive models;classification;rule learning;aggregated data;machine learning;data sets;learning method
clustering;probabilistic framework;dimensional space;database;text documents;text-based;text clustering;clustering documents;clustering performance;common task;clustering algorithm;competitive performance;clustering algorithm;semantic information
data-driven;frequency information;statistical method;semantic categories;automatically learning
text mining;ranking techniques;database;machine-learning;databases
nearest neighbor;database;application domain;vector machine;euclidean distances;metric learning;svm) classifier;classifier
multiple databases;rates;music collections
feature extraction;hand-crafted;content-based music;machine learning;similarity estimation;estimation methods;similarity estimation
music information retrieval;acoustic features;classification accuracy;genre classification;background model;acoustic features;genre classification;classification task;visual features
test data;classification;profile based;higher classification accuracy;user profiles;user models;user groups;statistical models
positive effect
high-dimensional;automatically learns;acoustic features;semantic information;dimensionality reduction techniques;fine-grained;efficient retrieval of;large sample;metric space;high-level;low-dimensional
pattern recognition;naive bayes;vector machine;multilayer perceptron;manually annotated;automatic feature selection;long-term;svm) models
ensemble learning;multi-class;image database;classification systems;decision trees;computed tomography;classification approach;single classifier;semantic annotations
management systems;rule-based;supervised machine learning;rule-based
decision tree classifier;decision support system;patient data;learning strategies;classifier
classification;classification accuracy;time series;time series;counterpart;document frequency;term frequency;natural language processing
pair-wise;pattern-based;amino acid;nearest neighbor;sequence-alignment;amino acids;nearest neighbor classifier;amino acid;protein function;svm classifier;sequence classification;support vector machines
false positive;artificial neural network
machine learning methods;input features;classification methods;decision tree;vector machine;machine learning;machine learning approaches;cross validation;test results;classification task;artificial neural network
gene expression;data normalization;statistical technique;data normalization;cluster analysis;biological data
sequence data;nearest neighbor;support vector machines;classification methods;classification performances
training data;kernel functions;kernel method;kernel regression;regression model;motion estimation
bayesian formulation;tracking algorithm;real-time tracking;probability density function;computational cost
optimization algorithm;modeling technique;modeling technique;line search;rates;model parameters;planning systems
plans;image features;machine learning techniques;clinical practice;cross-validation;binary classifiers;machine-learning;machine learning techniques;decision making

prediction model;input features;time series;time series;regression problem;baseline methods;numerical features;multiple kernel learning;social networks
instance-based;web service;amazon's mechanical turk;machine learning;labeled data;machine learning techniques;machine learning;social networking;decision tree learner
corpus based;latent topics;latent dirichlet allocation;learning tasks;multi-domain;domain-specific;document collections;multiple domains;association analysis
clustering;hybrid approach;user profile;recommender systems;collaborative filtering;information-theoretic;high quality;information loss;data matrix;tabular data;data matrix;collaborative filtering;model called;clustering
regularization;graph regularization;functions defined;prior knowledge
maximum-likelihood estimation;probability distribution over;large number of;web search engines;markov random fields;search engine;ranking functions;information retrieval systems
web page;ant colony;budget constraints;instances;ant colony algorithm;optimization techniques
kullback-leibler divergence;distance measure;hierarchical clustering;optimal combination of;gaussian distributions
conventional methods;structural features;social network;network evolution;social networks;network evolution
flow field
information content;high-accuracy;power supply;prediction models
high availability;control parameters;control method;neural network;neural network
data mining;data analysis;classification techniques;business processes;machine learning techniques;machine learning;data obtained from;information systems;data mining;machine learning techniques;classification algorithms

nearest neighbor algorithm;decision support;nearest neighbor algorithm;decision support;machine learning;input parameters;operating parameters;mathematical model of
power factor;power factor;low-cost;linear regression;regression methods;fold cross validation
data processing;power consumption;energy efficiency;energy efficiency
highly dynamic;dynamic behavior
service quality;theoretical framework;service quality;customer service;web sites;customer support;information quality;customer support
real data sets;factor analysis;latent variables;lower dimensional space;feature extraction;cluster membership;factor analysis;unlabeled samples;prior knowledge;independent component analysis;original data;training samples;low dimensional;principal component analysis;semi-supervised;semi-supervised;latent variable models;dimensionality reduction;expectation-maximization
clustering;data structure;cluster structure;distance measure;local linear;relational clustering;relational data;data mining;clustering model;relational data
discriminant analysis;support vector machines;classification;decision rule;classification method;classification problems
fitness function;blurred image;digital cameras;ground truth;estimation method
induction algorithm;input data;genetic algorithm;finite-state machine;finite-state;randomly generated;finite-state machine;tree construction;graph construction
artificial neural networks;artificial neural networks;mobile devices;mobile devices
genetic algorithm;fitness function;reinforcement learning;optimization problem;reinforcement learning;target function;optimization process
network management;vector machine;machine learning algorithms;machine learning algorithms;decision tree
test data;natural language text;decision tree;large corpora;human generated;combination methods;linear regression;corpus-based;natural language
parameter space;parameter space;multi-agent system;data mining methods;data-mining;data mining tasks;meta learning;meta learning;multi-agent system;solving complex
vector machine;vector machine;classification
binary classification;discriminant analysis;computational biology;classification;discriminant analysis;machine learning;training and test data;feature vectors
data set;textual information;textual content;success rate
machine learning methods to;neighborhood structure;data representation;pattern classification;structured information;linear classifiers;multiple kernel learning;alzheimer's disease
approximation error;activity recognition;linear-chain;hidden states;error bounds for;sensor data;conditional random fields
manifold learning;human activity;activity recognition;manifold learning;locally linear embedding;input space;machine learning;nearest-neighbor;manifold learning;intrinsic structure;mapping function;dimensionality reduction
real-world;statistical significance;data partitions;data streams;evolving data streams;clustering data streams
high-dimensional;incremental update;bayesian information criterion;data sets;cluster models;data streams;cluster models;clustering
optimization method;multi-objective;multi-objective optimization;knapsack problem;reference point;evolutionary algorithm;multi-objective optimization;reference point
parameter space;hill-climbing;genetic algorithms;data mining methods;meta-learning;multi-agent systems;parameter-space;role-based;description logic;formal model;search space
multitask learning;pattern recognition;multi-task;neural network model;prediction errors;training samples;data stream;class labels;class labels;model called;training sample
window size;class label;early stage;classifier performance;window size;classifier
data-driven;gaussian mixture models;learning algorithm;incremental learning;incremental learning;labeled and unlabeled data;low complexity
semantic classes;domain-specific;natural language
protein function;automatic extraction of;amino acids;automatically generated;high confidence;supervision;data set;linguistic patterns;supervision;graph-based;amino acid
biological entities;classification;feature selection method;semantic features;extraction process;machine learning;feature selection algorithm;classification results;feature selection;automatically extract;optimal set of
clustering;clustering;string similarity;edit distance;natural language texts;life science;string similarity;similarity measures
ranking algorithm;free text;entity relationship;graph-based;information extraction;text document;graph based;theoretical basis;word sense disambiguation
search task;public data;22;tree nodes;nearest-neighbor;candidate set;4;times faster than
false positives;visual phrases;spatial structure
nearest;multi-class classification;face recognition;test sample;classification performance;regularization;1;sparse representation;training samples;computational cost;regularization parameter;multi-class;classifier
high-dimensional;growing number of;image feature;classification;image representation;embedding methods;linear svms;large sets of;pascal voc;high accuracy;labeled examples;convex optimization problem
feature space;object recognition;local features;graph-cut;optimization algorithm;spatial context;scene categorization;scene classification;spatial domain;objective function
classification scheme;learning process;object recognition;density-based;classification;active learning;active learning;reinforcement learning;active learning methods;object class recognition;prior information;labeling process;classification task;markov decision process;sampling strategy
signal processing;vision applications;depth map;spatial resolution;low-complexity;depth maps
human pose estimation;pose estimation;data sets;regression function;image data
theoretical analysis;shape retrieval;shape descriptor
pair-wise;databases;graph matching;graph matching;pair-wise constraints;assignment problem;np-hard
dynamic programming;large parts;target object;articulated objects;key points;shortest path;articulated object;local image
closed-form solution;high levels of;closed-form solution;real data;estimation error
linear program;precision-recall;crf model;higher-order;visual perception;low-level
optimization method;linear complexity;image segmentation;problem called;segmentation quality
structured output;training images;appearance models;local optima;visual cues;class label;optimization problem;weakly supervised;test images;supervised methods;training data;gaussian processes;segmentation algorithms;weakly supervised;model selection;semantic segmentation
feature space;low rank;higher-level;higher-level;low-rank;low-level features;object detection;salient regions;high-level;objective function;low-level;salient objects
scene structure;structural constraints
image pixels;linear subspace;feature detectors;invariant features;scale invariant;scale-invariant;sparse set of;low-dimensional;feature matching
multi-camera;motion information;ground truth data;motion tracking
gait recognition;real data;energy minimization;image sequence;input images;fitness
joint model;automatically extracted;data structure;optimization scheme;surveillance video;semi-supervised;multiple information sources
regularization;low rank matrix approximation;optimization problem;line search;wide range;operator;missing values
sign language;classifier;data set;multi-class;sequential pattern;markov model
clustering;euclidean) space;classification;high dimensional;large-scale;linear dynamical systems;time series;time-series;real data;action based;dynamic scenes
clustering;vision applications;dictionary learning;low resolution;image super-resolution;high resolution;mapping function
effectively exploit;feature vector;classification;classification methods;image databases;pattern classification;databases;image classification
max-margin;nearest neighbor algorithm;computational complexity;decision boundaries;nearest neighbor;classification;data points;large-scale;real-world;nearest neighbor;linear combination;linear svms;computationally intensive;data sets;euclidean distances;classification results;linear classifiers;discriminative learning;kernel methods;classifier
video frames;classification;class specific;machine learning;instances;broad applications;iterative algorithm;cost-effective;distance metrics;multi-instance learning;multi-instance
convex optimization;visual search;dictionary learning;ranking list;feature extraction;learning paradigm;map estimation;reference image;local descriptors
face images;attribute-based;ground truth;similarity searches;classification;multi-attribute;multi-attribute queries;data set;similarity search;multi-attribute;image description;search results;classifier
computational efficiency;image descriptors;patch-based;image content;image matching;focus primarily on;standard datasets
image retrieval;attribute-based;million images;large-scale;graphical model;retrieval model;multi-attribute;semi-supervised;classifier
object recognition;false positive;recognition performance;large number of;object class recognition;rates;computed tomography;recognition rates
neural networks;multi-column;human performance;machine learning;handwritten digits;image classification;artificial neural network
probability estimates;large numbers of;object classes;nearest neighbor;input features;classification accuracy;reference data;naive bayes;local neighborhood;image classification;classification algorithm
attribute-based;learning mechanism;neural networks;mobile communications;high accuracy;batch learning;learning method
clustering;tree structures;edge detection;shape model;iterative learning;production rules;databases;tree learning;tree model;shape variations
discriminative power;shape model;object detection;sample points;reference point;shape model
fourier transform;shape descriptors;standard benchmarks;shape context;shape analysis;shape matching;image domain
efficient computation;vision systems;globally optimal;algorithm runs in
ct images;regularization;synthetic data;transformation matrix;noise model;shape model
assignment problem;game-theoretic approach;shape matching;metric spaces;game theory;np-hard
tensor-based;video frame
feature space;fast algorithm;local color;natural image;closed-form solution;sampling strategies;higher quality;gradient method;benchmark datasets;multiple image;nearest neighbors
human perception;input image;user studies;optimization framework;calibration method
surface normals;conditional random field;additional constraints;training set;information derived from;intrinsic image;optimization framework
working principle;optimization method;information needed
frequency information;low resolution;view synthesis;frequency domain;image quality
statistical approaches;statistical properties;human behavior;eye-movements;human eye;statistical modeling;eye movements;structural information;visual saliency;statistical framework;natural images
classification;active learning;real-world;active learning;active learning methods;supervision;surveillance video;internet scale;batch processing;unified framework;multi-criteria;human supervision
16;low-rank;vision problems;tracking algorithm;subspace learning;low-dimensional
data point;pair-wise;regularization;real data sets;higher-dimensional;optimization algorithms;graph cut
matrix factorization;linear programming;low-rank
data matrix;pattern recognition;regularizer;linear model;image representation;data representation;matrix factorization;latent structure;real world applications
multi-class problems;learning tasks;multi-task;applications including;multiple features;learning approaches;social tagging;category-specific;feature representation;semantic information;metric learning;knowledge transfer;real world;learning framework;applications involving;learning method;information sharing
latent variable model;learning mechanism;prior knowledge;specific problem;computational burden;latent variable models;model parameters
training set;test set;background clutter;recommender systems;filtering techniques;action recognition;lighting conditions;classifier
unsupervised learning;face databases;speech data;visual recognition;noisy data;spatial structure;unsupervised fashion;image denoising
variational methods;natural scenes;pitman-yor;gaussian process;computational efficiency;covariance functions;learning algorithms;stochastic search;propagation algorithm;segmentation accuracy;low rank;benchmark datasets;natural images;local optima
specific applications;nearest neighbor classifier;similarity metrics;effective classification;multi-class;object classification
high-dimensional;distance function;data points;encoding schemes;large scale;hash function;image databases;binary code;high-dimensional spaces;vision problems;coding scheme;performance gains;optimization process;efficient similarity search;data representations;nearest neighbor search
feature representation;textual information;large number of;retrieval performance;training samples;search engines;visual information;computational cost;query-dependent;9, 15;query-specific;classifier
feature space;multi-dimensional;image search;relevance feedback;image search;ranking functions
image datasets;highly correlated;graph structure;sparse reconstruction;sparse reconstruction
real-world datasets;multi-dimensional;numerical results;image classification;feature vectors;learning method
data structure;memory bandwidth;times higher;performance degradation;graph cuts;instances;compact data structure;high resolution;performance gain;memory consumption;applications requiring
open source;temporal features;fusion strategies;user-generated;large number of;annotated data;event detection;1;low-level features;event detection;recognition tasks;high-level;event recognition;low-level
face detection;fully automatic;local descriptors
training data;kernel-based;category recognition;kernel function;articulated objects;nearest-neighbor
data point;pair-wise;regularization;real data sets;higher-dimensional;optimization algorithms;graph cut

multi-camera;shape models;dynamic scenes;prior knowledge
fast marching;fourier transform;shape analysis;line segments;point-sets;distance transform
local features;feature detection;level features;image pairs;image matching;local image
local geometry;global information;partial differential equations;operator;laplace-beltrami;nearest neighbors
linear models;linear subspace;light sources;principal component analysis;theoretical results;theoretical results;video database;image set;upper bound;lighting conditions;real-world scenes;human face;low-dimensional;detection task;face database
frequency domain;reflectance model;image analysis;low-frequency;simple models
ground truth;image segmentation;fusion strategies;weighted voting;image analysis;similarity-based;spatially varying
special properties;vision problems;object detection;brute-force;detection approach
ct images;sparse representation;patch-based;image sequence
computational efficiency;vision applications;detection algorithms;visual patterns;perspective projection;low computational cost;real world;natural images
explicitly model;multi-task;prediction performance;learning algorithm;regression models;prior knowledge;competing methods;multi-task learning;regression methods;real world;alzheimer's disease
data point;data points;video summarization;convex optimization;linear combination;low-rank;data matrix;large datasets;theoretical foundations;image classification;sparse codes
linear programming;objective function;globally optimal;prior knowledge
graphical models;21;16;body parts;18;estimation problem;large number of;inference algorithm;tree model;pose estimation;human pose;estimation accuracy;7;5;human pose estimation;graphical model;human pose estimation
partial differential equations;multi-scale;high degree of
convergence properties;markov random field;efficient inference;image segmentation;local minimum;large number of;energy minimization;dynamic programming;loopy belief propagation;image denoising;theoretical analysis;2;algorithm converges;labeling problems
optimization problems;metric learning methods;computationally intractable;data points;object instances;large scale;supervision;large scale;metric learning;distance metric;statistical inference;fully supervised;computationally expensive
search space;latent variables;dictionary learning;pascal voc;object localization;max-margin;sparse codes;visual saliency;inference algorithms;conditional random field
dual decomposition;parameter learning;structured prediction;vision problems;prediction problems;max-margin;learning parameters
clustering;image clustering;image segmentation;data points;graph-based;segmentation results;parameter tuning;theoretical analysis;obtained by combining;diffusion process;learning method
high-dimensional;classification;pascal voc;object detection;speed-ups;image classification;sparse kernel;special case;low-dimensional;extracting features from;efficient learning
clustering;classification;locally linear;low rank;semi-supervised learning;linear combination;low-rank;learning tasks;data structures;semi-supervised classification;sparse matrix
diffusion process;classification;similarity matrices;multi-view;complementary information;machine learning;wide range;metric learning;fundamental problem;benchmark datasets;similarity measures;combining multiple;style algorithms
search systems;search methods;data sets;local features;visual search;efficient search;visual features;mobile devices;image databases;large-scale;visual words;product search;distance based;databases;search performance;local feature
search results;ranking method;similarity measure;spatial information;fundamental problem;inverted files;post-processing;visual words;spatial constraints;object retrieval;object retrieval;query expansion;nearest neighbors
scale invariant;confidence scores;object categorization;event detection;rank minimization;low rank;performance gains;large variations;objective function;multiple models
fast approximate;data point;computational complexity;image retrieval;data points;nearest;real data;prohibitively expensive;efficiently identify;iterative algorithm;clustering algorithm;cluster boundaries;clustering quality;neighborhood information;clustering
parameter space;real-world;single image;spatially varying;single image
long-range;depth ordering;image restoration
probabilistic framework;estimation accuracy;optimization problem;belief propagation;higher accuracy;object shape
spatially varying;light source
image processing;shape reconstruction
markov random field;user intervention;ground truth;error metrics;intrinsic image;high-level
training set;pattern recognition;parameter estimation;database;left ventricle;data association;segmentation results;manually annotated;training samples;training sets;model parameters;fully automatic;training set size;deep belief;statistical pattern recognition
1, 13;image segmentation;fusion strategies;synthetic data;mri data;weighted voting;target image;prior knowledge;fusion methods;local similarity;19, 11, 8
clustering;multiple instance learning;image segmentation;classification;instance;pixel-level;image classification;learning method
segmentation methods;outlier detection;low-rank;image sequence;unsupervised algorithm;image quality
objective function;classification techniques;graph-based;image-based;reliable detection;graphical model;data sets;prior knowledge;linear programming
local optimization;high-quality;spatial relationships;multi-view;logical structure;search space;high complexity;real-world;object detectors;low-level
manifold learning;sparse representation;image super-resolution;geometrical structure;learning performance;learning method
image segmentation;auxiliary;special structure;linear algebra;sparse matrices;optimization scheme;appearance models;random walks;segmentation algorithm
numerical experiments;optimization methods;label assignment;markov random fields;multi-label;efficient optimization
real data sets;maximum likelihood estimation;vision applications;decomposition method;camera parameters;linear programs;mapping function
line segments;information gain
learning algorithm;vision systems;classification;large scale;accuracies;classification tasks;linear classifiers;linear svm;vision problems;linear classifiers;classifier;learning method;times faster than
data point;multiple classes;classification accuracy;feature selection algorithms;multi-label;multi-label;image annotation;image categorization
signal processing;learning algorithm;stochastic gradient descent;optimization problem;sparse representations;image super-resolution;feature spaces;applications involving
image retrieval;multiple instance learning;instance labels;large scale;dynamic environments;training process;instance-level;text-based;computationally expensive;object tracking;classifier
regression trees;energy function;14;decision tree;regression tree;predictive performance;training data;vector-valued;labeling problems;conditional random field
search results;markov random field;outlier detection;ranking scores;text-based;extra information;similar images;graph cuts
image retrieval;large-scale;retrieval accuracy;fixed-length;classifier learning;instance-level;retrieval results;metric learning;canonical correlation analysis;learning framework;dimensionality reduction
high-dimensional;low-dimensional space;search algorithms;nearest neighbor;data points;large-scale;real data;data structures;vision problems;theoretical analysis;search algorithm;low-dimensional
image retrieval;visual appearance;semantically meaningful;object detectors;rule-based;visual cues;pascal voc;event based;low-level;high-level;event recognition;visual features
generation algorithm;large number of;fast computation;document collections;image collections;image clustering
shape recovery;multiple images;prior art;single image
depth ordering;conventional methods;depth ordering;markov random field model;boundary detection;object segmentation
database;object retrieval;heuristic rules;graphical model;map) estimation;object reconstruction
appearance model;surface normals;surface normal;singular value decomposition;additional constraints;multi-camera;surface reconstruction;matching algorithm
regression problem;real-world;image noise;surface normals
point cloud
ct images;computational efficiency;markov random field;database;template matching;post-processing;matching method;prior shape;local image
handle arbitrary;local appearance;unlike earlier;gaussian processes;graph nodes
linear program;26
feature extraction;shape prior;statistical analysis;hand-labeled;structured learning;graph cut
parameter space;globally optimal;globally optimal;benchmark dataset;cost function;real world scenarios
global convergence;optimization method;interactive segmentation;stereo matching;vision problems;markov random fields;solid state;loopy belief propagation
tuning parameters;problem instances;graphical models;combinatorial optimization;convex optimization;specific problem;approximate inference;objective function
bounded treewidth;linear programming;stereo matching;machine learning;powerful tools for;markov random fields;optical flow
memory requirements;interior point methods;large number of;event detection;robust tracking;principal component analysis;fast algorithms;interior-point methods;problems arising
regularization;convex formulation;high-quality;optimization problem;vector-valued
data sets;convex optimization;façade;low-rank;sparse matrix
probability values;pattern recognition;mathematical models;data sets;generative models;building blocks for;hidden markov models;shape analysis;classification approaches;temporal data;local minima
training data;image patches;neural networks;image databases;image denoising;image denoising;multi layer
parameter optimization;numerical experiments;probabilistic generative model;unsupervised learning;unsupervised learning;learning algorithm;large number of;trajectory data;translation invariant
visual content;semantic labels;database;large scale;meta-data;large-scale;large number of;great potential;visual analysis;databases;computational models;visual analysis
test image;image patches;image collections
classification accuracy;pascal voc;approximation methods;training examples;principal component analysis
training set;object classes;lower computational cost;feature vector;linear models;large-scale;object categorization;linear svms;feature generation;database;classifier
image retrieval;regularization;nearest neighbor;image features;context modeling;regularizer;regularization;retrieval accuracy;semantic representations;text features;object recognition;text-based;vision problems;scene classification;semantic space;visual information;image labeling;class-specific
local features;plays an essential role in;spatial context;object localization;visual phrases;individual features
linear scan;vision applications;hamming space;image data;binary code;speed-ups;hash table;uniformly distributed;nearest neighbor search
energy function;conditional random field;takes into account;topological structure;pascal voc;object classes;object detector;object detectors;classifier;detection task;background clutter
prediction problem;plans;structured learning
optimization technique;optimization problem;single image;intrinsic image;real world;statistical models;style algorithms
hand-held;matching algorithms;stereo images;patch-based;image pairs
visual tracking;tracking methods;detection results;high precision
training data;gaussian process regression;probability distribution over;digital cameras;sensor measurements;inherent uncertainty
edge detection;depth maps;vision applications;scene geometry;spatially-varying
lighting conditions;video sequence;reference image
distance metric;hierarchical model
point cloud;database;image-based;large-scale;feature descriptors;low-power;invariant features
real data
stereo camera;kalman filter;scene reconstruction;lower bound;parameter estimation
context-aware;14
structure-preserving;image features;human visual system
ordering constraints;dynamic programming;globally optimal;vision applications;6, 7;labeling problem;global optimal;space complexity;labeling problems
objective function;local minima;scale-space;motion models;image alignment
cost functions;data association;trajectory data;motion model;higher-order;network-flow;assignment problem;motion models
optical flow;optical flow;computationally expensive;spatially-varying
learned metric;linear representations;sampling method;optimization problem;visual tracking;training samples;closed-form solution;metric learning;distance metric;sparse linear;computationally expensive
automatically determines;optimizer;optimization methods;graph-cut;depth ordering;gradient descent;image sequences;image motion;object boundaries;generative model;motion estimation;optical flow;optimization strategy;optical flow estimation;continuous optimization
low level vision;hand-crafted;graphical structure;exponential family;vision problems;markov random fields;low level;motion estimation;theoretical analysis;low level vision;principled framework;task-specific
conditional random fields;low-level vision;graphical models;decision-theoretic;large-scale;probabilistic model;graph cuts;graph cuts;loopy belief propagation;graph cut;multi-label;8;maximum likelihood;inference algorithms
synthetic data;accurate classifiers;object detection;feature extraction;scene recognition;boosting algorithms;boosting algorithms;informative features;uci datasets;classifier
classification;sparse representation;standard benchmarks;local linear;machine learning applications;sparse representation;nearest neighbors;pascal voc;dimensionality reduction
real-world datasets;training data;graph embedding;real-world applications;subspace learning;noisy data;graph embedding
probabilistic framework;efficient inference;mixture model;databases;parameter learning;greedy algorithm;labeled data
nearest;face detection;object class;sliding window;machine learning;instance;object detection;competitive performance;object detector;classifier;positive class
instance;explicitly model;training data;object detection
discriminative training;estimation accuracy;background clutter;global optimization;pose estimation;computational cost;tree-structured;contour-based
active learning;class label;active learning;data-sets;weakly supervised;supervised methods;upper-bound;fully supervised;learning method;semantic segmentation
training set;object classes;training examples;object class;object parts;supervision;view-based;intra-class;pose estimation;multi-view;relevant information;benchmark data sets
tree-based;geometric constraints
shape space;activity recognition;point sets;real data;shape analysis;statistical modeling
computational complexity;graph matching;image matching;real-world applications;graph matching
object boundary;background clutter;22;training examples;local constraints;object detection;global constraints;object shape

image patches;theoretical properties;domain model;real data;machine learning;controlled experiments;tracking problem;topological properties
classification scheme;specially designed;natural scenes;real-world scenarios;vision systems;benchmark datasets;compares favorably with;natural images
training data;database;decomposition method;mrf) model;quadratic programming;optimization problem;large scale;markov random fields;mrf model;np-hard
unlabeled images;reference image;image patches;general purpose;database;quality assessment;learning strategy;general-purpose;prior knowledge;image representations;structural similarity;quality measure;typically rely on;hand-crafted;image quality;computational model;reference image;learning framework;local descriptors;image-patches
data-driven;data points;large-scale;large scale;neighborhood graphs;neighborhood graph;dimensional data;graph construction;increasingly popular
kernel regression;weighted average;image super-resolution;multi-scale;low-resolution;produce high quality;high-resolution;multi-scale
involving multiple;multi-camera;vision-based;video tracking
motion field;sparse representation;optimization algorithm;regularization
exemplar-based;human action;pose estimation;action recognition;tag prediction;exemplar-based
optical flow;image search;similar) object;facial expressions;estimation methods
regularization;motion estimation
 ;low rank;convex optimization;basis set;visual tracking;image sequences;image set;image alignment
dynamically changing;human detection;partial occlusion;detection performance;person tracking;svm classifiers;classifier
edge-preserving;image-based;optimization scheme;cost function;lighting conditions;sensor fusion;dynamic scenes;light source
face images;image patch;12;data transformation;deep learning;13;29;logistic regression;deep belief;highly nonlinear;face alignment
face images;regularization;mechanical turk;target task;privacy concerns;training dataset;machine learning;1;recognition tasks;training datasets;classification problems;classification task;automatically learning;recognition problem
face recognition;vector space;discriminative learning;image set;classification;kernel function;learning algorithms;linear discriminant analysis;modeling technique;object categorization;linear subspace;euclidean space;positive definite;covariance matrix;unified framework;learning method;specific problem
face recognition;training data;appearance-based;large-scale;feature extraction;feature selection;informative features
25;training set;human detection;training samples;pose estimation;human pose estimation;real-world scenes;shape variations
efficient parallel;computational complexity;image patches;classification;face detection;multi-view;multiple view;face detection;input images;input image;image representations;similarity matching
image search;image database;manually labeled;database;web-scale;global features;16;search engines;object detection;window-based;algorithm exploits;visual analysis;web images;salient objects
object classes;million images;large-scale;database;probability distribution over;large-scale;manually annotated;hierarchical structure;object localization;wide range;knowledge transfer;object detectors;target class
basis functions;detection method;spatiotemporal data;foreground detection;2;detection methods;natural images
training data;learning algorithm;coordinate descent;object detection;human pose estimation;separability
edge-preserving;applications including;depth map;stereo matching;linear regression;regression problem;image denoising;filtering methods;piecewise linear
natural scenes;human performance;low-level features;visual features;visual saliency;low-level
real-world;blind source separation;color space
domain knowledge;input image;human vision;saliency detection;region based;benchmark dataset;visual saliency
12;boundary detection
prediction accuracy;sequential nature;modeling task;real-world;probabilistic learning;large margin;visual attention;bayesian approach;eye movements;global context;task-specific
simulated annealing;summarization task;large-scale;dictionary learning;image collection;image collection;sparse representation;multiple categories;image set;learning task
rates;user-friendly;binary pattern;numerous applications;recognition rates
structural information
quality assessment;large number of;sparse representation;sparse representation;training samples;databases;image processing
appearance model;partial information;similar object;algorithm performs;spatial information;sparse representation;reconstruction error;visual tracking;sparse representation;subspace learning;local patches;tracking method;image sequences;visual tracking
comprehensive evaluation;accuracies;sparse representation;regularization;minimization problem
appearance model;histogram-based;spatial information;tracking algorithm;generative model;object tracking;classifier
clustering;clustering algorithms;video sequence;video segmentation;segmentation methods;spectral embedding;context-aware;articulated objects;image regions;video segmentation;moving objects;object boundaries;motion cues;cluster boundaries;motion segmentation;image segmentation
tracking algorithms;appearance model;appearance model;information contained in;ranking svm;weakly supervised;robust tracking;object appearance;support vector;visual appearance
face images;face recognition;features extracted from;face databases;binary pattern;matching problem;learning method
face recognition systems;image descriptors;face verification;intensity values;database;hand-crafted;real-world;belief networks;global structure;feature representation;high-resolution images;unsupervised learning;deep learning;object class
feature space;image processing;linear discriminant analysis;operator;training samples;operator;continuous functions;image processing
clustering;metric learning methods;learned metric;image datasets;metric learning;image analysis;content based image retrieval;metric learning;learning algorithms;learning framework;visual analysis;spatial correlation
real-world datasets;face recognition;nearest neighbor;limited memory;classification;memory constraints;training classifiers;streaming data;computing resources
data-driven;object classes;unsupervised learning;single class;object class;saliency detection;special case;34;multiple instance learning;multi-class;object detectors
image retrieval;retrieval precision;object retrieval;local patches;mapping rules;visual words;visual vocabulary;high recall;visual vocabulary
increasing demand for;video data;cost model;object detection;user study;video annotation;object detector;learning performance;image annotation
regularization;high dimensional;redundant features;human detection;training samples;object detection;feature selection;high dimension;binary pattern;classifier;limited number of
specially designed;object detection;multiple layers;hidden variables
input image;natural scenes;image patch;saliency detection;color space
case study;training examples
variational methods;model generalizes;random fields;optimization algorithms;energy functional;finite element
operator;image understanding;edge detection;fundamental properties;visual attention
scale invariant feature transform;17;vision algorithms;i;large number of;4;vision applications;human visual system
image database;image-space;global optimization;detection algorithm;object segmentation;feature point;façade;pixel-level
conventional methods;2;multi-scale;computational cost;maximum likelihood;likelihood function
classification;scientific data;image analysis;digital cameras;image quality;image annotation;quantitative analysis
multiple kernel learning;image features;low level;classification;image acquisition
tracking framework;geo-spatial;reconstruction algorithm;spanning trees;temporal evolution;geo-referenced;intrinsic parameters
database;activity recognition;intra-class;fine-grained;fine grained;high-resolution;inter-class
parameter space;motion tracking;optimization method;optimization problem;image sequences;real world
motion capture;motion tracking;similarity measure;multi-view;spatio-temporal;high quality;video sequence;spatio-temporal
regularization;variational formulation;motion estimation;large number of;object boundaries;flow field;optical flow;multi-label;convex formulation;motion segmentation
particle filtering;state space;tracking framework;particle filter
structured output;object model;vision applications;matching techniques;basis functions;online learning;detection methods;video sequences;object tracking;learning framework;estimation algorithm;feature matching
optimization framework;activity recognition;hand tracking;quadratic programming;real-life;hand tracking;object detectors;multi-target tracking
age estimation;face images;databases
training examples;classification;learning approaches;classification tasks;large margin;correlation filters;large margin;facial expression recognition;support vector machines
sparse learning;multi-task;specific information;expression recognition;facial expression recognition;recognition tasks;face verification
facial images;redundant information;optimization problem;age estimation;discriminative features;feature selection;manifold structure of;rank correlation
feature points;image patches;feature detection;database;low quality;20
face recognition;data points;dictionary learning;learning algorithm;linear combination;sparse representation;low-rank;rank minimization;sparse representation;objective function
database;image noise;region-based;shape information;object localization;visual words;object detection;object categories;times faster than
context information;appearance-based;scene geometry;confidence scores;transfer learning;video surveillance;false positive;learning framework;objective function;detection rate
domain adaptation;target class;object class;spatio-temporal;object detectors;pascal voc;fully automatic;real-world;object detectors;motion segmentation
10;mobile devices;object localization;object detection;object categories;detection performance;object detector;object detectors;processing power
supervised learning methods;multiple instance learning;loss function;training examples;incremental learning;object detection;false alarms
evolutionary algorithms;data structure;parameter-free;multi-view;parameter selection;compact representation;depth maps
user input;single view;compare favorably with;globally optimal;single image
energy-minimization;multiple classes;object classes;multiple images;supervision;instances;cost function;standard datasets;multi-class;discriminative-clustering
sparse representations;motion segmentation;computational cost;subspace clustering;spatial constraints
training images;training set;energy function;energy functions;test image;pascal voc;visually similar;graph-cuts;training data;automatic segmentation
tree structures;ground truth;minimum spanning tree;integer programming;aerial images
quality metrics;graph-based;aggregation methods;image analysis;data sets;boundary detection
highly accurate;classification accuracies;action recognition;nearest neighbor query;parallel structure;supervision;efficient approximate;highly scalable;model selection;feature matching
appearance model;minimum cost;action recognition;network flow;video frames
social interactions;social interactions
simple linear;level features;activity recognition;classifier;activity recognition;semantic space;svm classifiers;high-level;image representation
temporal dependencies;discrete optimization;classification;spatio-temporal;pairwise constraints;graphical model;fine-grained;model parameters;benchmark datasets;action recognition
tracking algorithm;standard benchmarks;visual tracking;gradient descent;image information;objective function
data sets;appearance models;online learning;training samples;motion patterns;multi-target tracking;multiple instance;learning method;multi-target tracking
model fitting;discrete optimization;pre-computed;optimization problem;data association;data association;closed-form solution;standard datasets;continuous optimization;multi-target tracking
high-dimensional;density estimation;state spaces;dynamic bayesian networks;decomposition techniques;articulated objects;particle filtering;particle filter;video sequences;estimation errors
video sequences;deformable objects;tracking algorithm;probabilistic model
dual decomposition;error propagation;scalability issues;data association;object detection;multiple object tracking;object detector;objective function
distance metric;feature descriptors;facial images;metric learning;discriminative information
expression recognition;estimation problem;pose estimation;video surveillance;vision applications
appearance model;convergence rates;feature tracking;view-based
face images;regularization;training data;classification;face recognition algorithms;low-rank;face databases;robust face recognition;low-rank;image data;sparse representation;approximation algorithm;prior works
face recognition;image sets;competing methods;local models;databases;quadratic programming;local neighborhood;video-based;reference set;geometric structure;image set;images captured;video-based;image sets;matching cost
facial expressions;binary classifiers;facial expression recognition;multi-class;temporal evolution
shape features;shape information;pascal voc;feature representation;object detection;low level;image classification;local structure;object detectors;object categories
10;feature extraction method;feature extraction;detection results;histogram based;image understanding;data driven;7
pascal voc;detection accuracy;graph cut
human pose;finding similar;online shopping;auxiliary;sparse reconstruction
probabilistic generative model;latent variables;text documents;supervision;model parameters;modeling approach;quality measure
pair-wise;large scale;sampling process;huge number of
image pixels;efficient inference;image segmentation;spatial relationships;spatial information;object-based;conditional random field;belief propagation;inference methods;object boundaries;graph cuts;inference algorithm;fully connected;fully-connected
automatically learns;segmentation problem;image segmentation;segmentation techniques;interactive segmentation;target image;parameter tuning;image-specific;conditional random field;margin;user interactions
matrix factorization;pattern recognition;synthetic data;unsupervised learning;regularizer;multiple subspaces;feature extraction;computationally expensive;rank minimization;clustering performance;subspace clustering;unified framework;theoretical analysis;visual learning;subspace clustering
clustering;feature space;data analysis;graph” representation;real-world;object-based;global structure;image matching;complex data;graph representation;metric spaces;relational data;random walks
dynamic programming;latent variables;exact inference;16;accuracies;model trained;large number of;event detection;18;complex events;hidden markov model;detection task;recognition tasks;max-margin;temporal structure;automatically discover
gaussian process regression;camera motion;vector field;camera motions;moving objects;dynamic scenes;camera views
data-driven;graph structure;monte carlo;event detection;unified framework;conventional methods;markov chain
maximum-weight;main idea;search strategies;classifier
multi-modal;activity recognition;human motion;time series;motion capture data;linear complexity;feature weighting
intra-class;human action recognition;action recognition;depth maps;human motion
feature space;regularization;learned metric;minimum description length;computational costs;visual tracking;metric learning;benchmark datasets;visual appearance;classification error;tracking applications
visual similarity;appearance model;tracking framework;learning phase;target object;visual tracking;appearance models;hidden markov model;video sequences;appearance variations;hidden markov model
clustering;real-world datasets;optimization framework;visual cues;data-association;extraction methods;optimization problem;iterative algorithm;motion information;low-level;multi-target tracking
motion patterns;tree structure;spatio-temporal;accurate classification;classification
spatial correlations;graph structure;min-cost;multiple objects;multi-view;global optimization;combinatorial optimization problem;human pose;assignment problem;multi-target tracking
recognition performance;linear discriminant analysis;input image;low resolution;linear features;principal component analysis;feature vectors
verification problem;large number of;higher degree of;data sets;training samples;identification problem;learning framework;learning method;limited number of
markov random field;minimization problem;contextual constraints;face recognition;agglomerative clustering;temporal structure
low-dimensional space;data points;distance metrics;high dimensional;learning approaches;component analysis;pairwise constraints;input space;distance metric;dimensional data;pairwise similarity;face verification
shape space;image features;body parts;single image;shape models;human pose;single image;human pose estimation;sampling strategy
15;multi-layer;perspective camera;single image
image pairs;object detection;high resolution;optical flow;recognition systems;real world
object class;higher-level;individual objects;geometric information;fine-grained;pascal voc;recognition systems;scene understanding;object tracking;benchmark data sets
feature space;completeness;lower dimensional;image features;classification accuracy;structured sparsity;large number of;spatial regions;feature selection;image classification;classifier
classification;region-based;articulated objects;competitive performance;real world;semantic segmentation;object detectors;class-specific
object recognition;image space;image patches;contour-based;multi-scale
spectral clustering;higher order;motion model;motion models;moving objects;operator;higher order;motion segmentation
detection algorithms;image segmentation;human performance;image understanding;detection performance;high-level;low-level
human perception;feature space;segmentation methods;applications including;iterative process;prior knowledge;segmentation method;geometric structure;shape analysis;density function;biomedical applications;low-level
clustering;synthetic data sets;search space;globally optimal;globally optimal;real images;local minima
independent motion;motion estimation;saliency detection;salient regions;multi-body;statistical inference
multiple kernel learning;large scale;recognition systems;feature fusion;event detection;fusion methods;low-level features;visual information;low-level;automatic speech recognition;high-level;combining multiple;visual features;audio-visual
classification performance;natural scenes;single image;temporal dynamics;data set;image sequences;scene classification;performance gain;scene understanding;scene categories
mixture components;classification accuracy;recognition task;supervision;visual words;linear complexity;human activities;em algorithm
recognition rate;spatio-temporal;social behavior;temporal context;social behavior;action recognition;temporal features
spatial-temporal;graphical structure;probabilistic graphical model;state-space;action recognition;transition model;additional information
probabilistic framework;image registration;registration algorithm;random variables;feature-based;theoretical analysis;maximum likelihood;neighborhood information
face detection;common practice;fundamental properties;target detection;tracking results

fixed size;camera motions;semi-definite programming;low-rank;prior knowledge
additional constraints;template-based;single view;real data;closed-form’ solutions
hypothesis space;object recognition;verification problem
accuracies;scene text;conditional random field;energy function;scene text
database
semantic structure;high level;inference problem;input images;image recognition
classification scheme;small sample;large collections of;scene categorization;classification results;human visual system;semantic structure;scene categories;inter-class;training sets
scene understanding;multi-dimensional;sampling approach;object category;statistical model
large-scale;regularization;large-scale;learning algorithm;optimization problem;coordinate descent;compressed domain;image classification;multi-class;visual features;classification algorithms
high level;problem instance;depth images;prior knowledge;pose estimation;regression model;low computational cost;output variables;human pose estimation;object segmentation
object recognition;parameter estimation;quantitative analysis;single image;topological properties;margin;conditional random fields;object categories
learning approaches;feature points;dictionary learning;optimization algorithm;greedy algorithm;random walk on;sparse representation;dictionary learning;sparse codes;category recognition;objective function
local features;feature vector;classification;hierarchical clustering;pascal voc;object detection;databases;image classification;visual saliency;machine learning algorithms;weighted sum of
score function;classification;distance function;multiple hypotheses;automatically extract;distance measures;low computational cost;score functions;segmentation quality;classification algorithm
identification problem;information-theoretic;subspace clustering;potential functions;shape variations
maximum weight;maximum weight;share similar;algorithm to compute;object segmentation
ground truth data;object appearance;background clutter;static images;large number of;high-order;optimization problem;labeling problem;shape prior;scale invariant;color histogram;real images;global constraints
clustering;similar objects;hierarchical clustering;standard datasets;intra-class;image regions;segmentation accuracy;similar images;segmentation methods;real world;image clustering;multi-scale
data set;data points;spatio-temporal;graph cuts;graph-cuts;generic framework;depth maps;multi-label;object segmentation
real-world datasets;domain adaptation;decision function;support vector regression;vision applications;regularizer;optimization algorithm;large number of;source domains;performance gain;svm classifiers;event recognition;classifier;web images
high-level;region based;selection techniques;event detection;video summarization
activity recognition;high-level;low-level;hierarchical model;max-margin
spatial information;activity recognition;classifier trained on;temporal features;addressing this problem
12;20;labeled samples;database;video data;semi-supervised learning;action recognition;human action recognition;1;human action recognition;2;action recognition;video surveillance;human actions;video annotation;data distribution
data sets;discriminative features;camera motions;minimization problem;energy functions;crf model;appearance models;online learning;tracking problem;multi-target tracking
sparse learning;computational complexity;special case;15;multi-task;particle filter;visual tracking;tracking performance;learning problem;pose variations;object tracking;multi-task
hand-crafted;high-level;object detection;low-level;image features
ranking algorithm;statistical properties;query sensitive;large-scale;web images;query-sensitive;search problem;indexing structure;distance-based;higher accuracy;algorithm takes;information loss;query points;indexing methods;pca-based
domain adaptation;statistical properties;kernel-based;competing methods;target domain;feature representations;domain adaptation;source domains;real-world applications;cross-validation;source domain to;visual recognition;labeled data from;image quality;perform poorly;source domain;low-dimensional;standard datasets
markov random field;input image;graph-based;input query;object class;link analysis techniques;similarity graph;semantic segmentation
visual patterns;high-order;individual objects;scene categorization;object detection;scene understanding;object categories
category specific;latent dirichlet allocation;local regions;target image;scene recognition;topic model;bayesian inference;context aware;context aware
large-scale;database;scene recognition;fine-grained;scene understanding;high-level
30;large-scale
scene classification;linear features;natural scenes;generative model;classification
fusion method;statistical dependence;classification accuracy;classification approaches;classification tasks;feature fusion;shape features;fusion methods;regression model;low level;logistic regression;image classification;combining multiple
detection techniques;local context;large scale;matching methods;region-based;classification performance;image classification;keyword based
trade-offs;large scale;visual recognition;high accuracy;error rate;classifier;object categories;information gain
computational efficiency;object recognition;weighted sum of;large-scale;classification accuracy;depth images;visual words;object categories;svm classification;local descriptors
high throughput;classification;fine-grained;classification approaches;image representation;large number of;key points;fine-grained;share similar;matching process;randomly generated;image information;image categorization;classifier;classification algorithms
high-dimensional;data structure;search space;nearest neighbor search;efficient similarity search
pattern detection;image patches;image segmentation;higher-level;rich information;higher level
auxiliary;inference engine;object detection;high-order;prior knowledge;graph-cuts;scene classification;scene understanding;semantic segmentation
hypothesis generation;cluster model;graph cuts;instances;geometric model;model fitting;cluster models
sample selection;color information;sampling-based;candidate pairs;data set;objective function
high-dimensional;design choices;spatial distribution;linear complexity;image processing;result quality
human activity;action sequences;explicitly modeling;fine-grained;human actions;video sequences
theoretical analysis;real-world;patch-based;multi-view) stereo
motion capture;image sequences;long sequences;reconstruction error
nearest;computational complexity;aggregation methods;image pixels;stereo matching;matching cost;tree structure;data sets;image pair;locally-optimal;matching cost;shortest distance;stereo correspondence
regularization term;optimization algorithm;low-rank;data matrix;missing data;low-rank matrix approximation;objective function
semantically similar;kernel-based;large-scale;high quality;metric distance;hash functions;supervised methods;vision problems;prior works;model training;hash function
high-quality;multi-core;video data;online content;online content;computational cost;tracking method
detection results;sampling scheme;sampling method;detection performance;sampling strategy
causal relationships;generative process;topic model;model parameters;random variable;collapsed gibbs sampling
feature space;feature representations;kernel density;probabilistic models;6;background modeling;1;scale-invariant;heuristic method;4;7;spatial information;increasingly complex
latent variable model;em) algorithm;image region;scene recognition;model parameters;discriminative training;structural svm;expectation-maximization
image pixels;markov random field model;image segmentation;potential function;local constraints;markov random field;energy minimization;image regions;higher-order;efficient approximate;labeling problems
textual description;scene recognition;specific task;baseline methods;object detection;image description;human subjects
training set;22;distance metrics;highly-skewed;real-world scenes;classification error
specially designed;image based;free space;scene geometry;view based;geometric structure;single image;estimation methods;image based
pair-wise;search space;computationally intractable;ad-hoc;scene understanding;prediction task;high order;structured prediction
conditional random field;semantically meaningful;image datasets;closely related;fine-grained;visual concepts;relevant attributes;human interaction
regularization;large-scale;weighted average;stochastic gradient descent;objective functions;cross-validation;image classification;dimensional data
classification scheme;object recognition;dictionary learning;object recognition;visually similar;category-specific;inter-related;learning problem;object categories
classification approaches;fine grained;object categorization
latent variables;classification;discriminative power;classification tasks;intra class;image content;human action;fine grained;spatial distribution;image representation;scene classification;visual features;discriminative information;image classification;classification task;classifier;max margin
human perception;image content;build models;visual recognition;recognition systems;contextual factors
independent motion;appearance variations;local information;hierarchical algorithm;image pairs
high-quality;input images;spatially-varying
regularization;camera motion;image noise;higher quality;high frequency;blurred image
spatially varying;image deblurring;estimation algorithms
high-accuracy;correlation analysis
source code;ground truth;taking into account;special structure;optimization problems;real-world;stereo matching;labeling problem;data sets;depth maps;taking into account;variational framework
video sequence;video segmentation;conditional random field;automatically created;labeling problem;unlike conventional;random variable;consistency constraints;higher order;segmentation algorithm
segmentation methods;multiple images;similar object;database;object parts;multiple-image;intra-class;generative model;image information
illumination conditions;automatic segmentation;multi view;multiple images;graph cuts;data set
ground truth;user interactions;active learning;interactive segmentation;data sets;theoretical analysis;segmentation algorithms
clustering;spectral clustering;graph structure;spectral clustering;irrelevant features;feature fusion;optimal combination of;distance-metric;clustering performance;clustering methods
hand-held;ground truth;extended kalman filter;data association;prior knowledge;scene points;finite element;finite element method;sparse set of;boundary points;perspective camera
multi-camera;scene flow;confidence measure;motion estimation;real data sets
video sequences;camera motion;ground truth;competing methods
24;25;1;data set
depth map;ground-truth
objective function;optimization criteria;local minimum;14;image pairs
data-driven;probabilistic framework;training data;nearest neighbor;anomaly detection;distinguishing feature;spatio-temporal;data dimension;spatial region;video surveillance;decision rules;video segments
underlying structure;discriminative models;hidden variables;multiple views;multi-view;human action;action recognition;recognition tasks;multi-view;continuous data
regularization;multi-class boosting;object recognition;optimization problem;machine learning;classification performance;optimization technique;boosting algorithms;multi-class;multi-class classification problems
decomposition techniques;parameter learning;learning algorithm;35;training images;single-image;potential functions;max-margin;depth estimation;conditional random fields
feature space;classification;large-scale;discriminative power;human vision;classification tasks;margins;classification task;classifier
video frames;scene geometry;image space;labeling problem;information flow;scene reconstruction;scene understanding;semantic segmentation;segmentation quality;conditional random field
clustering;line segment;single view;constraints imposed by;line segments;maximum likelihood;standard datasets
static images;learning algorithm;appearance-based;object classification;image features
real-world;temporal structure;object models;camera views
instance;virtual view;linear transformation;instances;action recognition;competitive performance
structured output;human-robot;applications ranging from;event detection;facial expressions;maximum-margin;sequential data;human activities;max-margin
mining framework;classification;context modeling;regularizer;spatial context;error bounds;classification performance;efficient optimization;discriminative information;image classification;arbitrarily large;objective function;similar characteristics
pose estimation;classification
learning process;image pixels;input data;globally optimal;structured output;image data;learning problem;task-specific;structured learning
detection problem;false positives;exhaustive search;classification;character segmentation;computationally expensive;scene text
energy function;color information;image space;depth ordering;image pair;compares favorably with
data-driven;video frames;database;similarity metric;high quality;shortest path;temporal coherence
automatically detected;color space;face detector
spatially varying;image information;real images;main components;spatially-varying
noise level;scene structure;temporal coherence;edge preserving;noise reduction;single image
feature point;low resolution;motion estimation
segmentation methods;image segmentation;input image;linear constraints;soft clustering;visually similar;segmentation results;segmentation method;kullback-leibler divergence;kernel density estimation;natural images;convex optimization problem
graph partitioning;multi-layer;database;visual patterns;natural image;multi-scale;graph structure;segmentation algorithms
error-prone;training samples;discriminative features;multi-class classification;classification
single-image;estimation method;single image
global illumination;structured light;high-frequency;real world scenes;high quality;lower bounds;shape recovery;input images
video sequence;spatio-temporal;stereo matching;optimization model;depth maps;video sequences;motion models;stereo images
integration process;markov random field;salient regions;multi-scale;specifically designed to
video sequences;video sequence;real world
multi-view;reconstruction algorithm;depth maps;multi-view
simple models;surface reconstruction;point cloud;global optimization
vision problems
clustering;human input;clustering performance;image clustering;face images
face recognition;linear subspace;classification;latent space;multi-view;feature extraction;extraction techniques;generalized eigenvalue problem;quadratic program;desirable properties;feature spaces;image data;label information
domain adaptation;target domain;data distributions;recognition task;low-rank;source domain to;source domain
image patches;probabilistic model;inference algorithm;prior distribution;markov random fields;image denoising;posterior distribution;low-level vision
latent variables;linear transformations;local regions;variational inference;hyper-parameters;topic models;visual words;model parameters;linear classifiers;image categorization;local descriptors
agent-based;video surveillance;behavior patterns;mixture model;real data
graph structures;face detection;real-world;tree-structured;face detection;pose estimation;commercial systems
training images;training data;regression function;feature selection method;highly accurate;shape model;correlation-based;face alignment;accurate models;learning framework;face alignment
11;global constraints;local features;image features;classification;local regions;30;training process;false positive;multiple image;algorithm called;multi-scale;benchmark dataset;conventional methods;error rate;additional features;classifier
high quality;stereo images
large scale;object retrieval;28;29;image datasets;retrieval performance;storage requirements;benchmark datasets;query expansion
belief propagation;probabilistic model;pairwise constraints;articulated objects;object boundaries;prior probability;human body;low-dimensional
high-dimensional;tree based;large scale;multiclass classification;convex optimization;large datasets;image classification
large number of
visual search;classification;regularizer;dictionary learning;semantic similarity;weakly supervised;image search;image classification;local descriptors
classification rule;classification;classification process;generalization performance;classification tasks;black box;detection algorithm;classification rules;tool called;database;classifier;image regions
dependency analysis;markov random field;algorithm performs well;markov random fields;linear models;bayesian network learning;conditional independence;markov blanket;bayesian network;large sets of;dependency analysis;bayesian networks;learning bayesian networks
large graphs;data mining techniques;medical imaging;algorithm called;graph analysis;network topologies;diverse applications;arise naturally in
document summarization;selection method;summarization techniques;case studies;specific characteristics;real-world data sets
prediction accuracy;parameter-free;background information;implicit feedback;high probability;prediction models
data set;decision trees;classification algorithms
clustering;case-based reasoning;years ago;classification;application domain;indexing scheme;matrix factorization
data mining methods;decision support system;positive results;decision support systems;data preprocessing;decision making
clustering;real-world datasets;efficient storage;storage structure;main memory;hierarchical clustering algorithms;designed to support;real-world;statistical significance;clustering algorithm;hard disk;clustering algorithms;data storage
data mining application;decision-making;data mining approach;time series;problem domain
base classifiers;decision trees;signal processing;classification;data mining techniques;ensemble classifiers;wide range
sequence mining;mining algorithm;time series
attribute-based;graph mining;applications including;marketing strategies;behavior patterns;interesting patterns;data streams;complex structures

case-based reasoning;based reasoning
model order selection;case study;measurement noise;human operators
classification method;classification;database;labeled instances;data set;instances;associative classifier;rule extraction;class association rules;missing values;classifier;evolutionary process
process model;decision trees;classification;classifier performance;case study;hierarchy levels;data preparation;data mining;real world;knowledge-driven
frequent itemsets;constraint based;pattern mining;frequent patterns
survival analysis;high dimensional;independent variables;piecewise linear;subset selection;censored data;regression model;model parameters;dependent variable;separability;model selection;feature selection
clustering;data warehouse;user intervention;hierarchical clustering;clustering tasks;semi-supervised;semi-supervised;user input
clustering;synthetic data sets;density-based;data points;outlier detection;data mining;local outliers;large-scale datasets;data set;minimum spanning tree;efficiently identify;spanning tree;efficient construction
distance measure;sampling algorithm;sampling techniques;scalability issues;mining association rules;sampling algorithms;association rules mining;large datasets;sample data;association rules;data mining algorithms
decision rules;rule generation;classification rule;rule set;learning models
vector spaces;general concept;vector space
text mining;data analysis;textual data;human-centered;hidden markov models;case studies;data mining;human-centered
text mining;text mining;information retrieval research;clustering technique;information retrieval;ir research;formal concept analysis;ir) research;information retrieval
textual descriptions;spatial web objects;web queries are;web data management;data management techniques;location-based services;web queries;data management
uncertain data;data management techniques;data integration;data analytics
large scale;data bases;social networks
data stream;data streams
statistical methods;database systems;database;parallel dbms;machine learning;data mining;database engine;open-source
decision support;data analytics;big data;sql server
lessons learned;data rates;monitoring data;big data;data stores;enterprise application;data analytics;storage systems;monitoring systems;high-level;resource utilization;enterprise systems;distributed systems;open-source
map reduce;main memory;higher-level;sparse matrix
recommendation quality;database;query execution times;large quantities of;cost model;query performance;database administrators;databases;column-oriented;data management
database systems;database;long-running;buffer management;database architecture;buffer management
data analysis;user session;data analytics;large-scale
database;high availability;microsoft sql server;database
web scale;database;database
analytical processing;big data;large-scale;long-running;data access;originally designed;query processing;data processing;mapreduce-based
social media;big data;lessons learned;sensor data streams;high scalability;numerous applications
stream processing;user-defined;stream processing;data processing;continuous queries;data distribution
increasing number of;history data;moving object;location-based service;nearest-neighbor;clustering scheme;mobile applications;query processing;wide range
database;memory usage;lock;isolation level;serializable;intensive workloads;serializable
integrating information from;unstructured text;structured data sources;valuable information;data sources;information extracted from;integrating information from;data management;unstructured data;data management
multidimensional data;social network;web-scale
mapreduce-based;tool called;machine learning;large datasets;automatic generation of;parallel execution;load balancing
large data sets;data sources;map-reduce;mapreduce-based;data warehouse
clustering;interactive exploration;big data;data visualization;scientific applications;data reduction;batch processing;cluster analysis;clustering structure
data generation;big data;large-scale;pseudo-random;random access;parallel execution;data generation
low precision;data cleaning;aggregate queries;data provenance;query results;data analytics
declarative query language;parallel database;open source;data feed;semi-structured data;data" management;web-scale;long running;definition language;key features;parallel database systems;spatial queries;fault tolerance
data stored;sampling-based;large volumes of data;real-world;massively parallel;approximate query processing;error guarantees;fault-tolerant;wide range;aggregate) queries;content distribution
data processing;data transformation;parallel processing;massive amounts of;raw data;data quality;biological information;genomic data;control mechanisms
optimizer;open-source;streaming environment;window based;visual interface;network monitoring;scientific data management;intermediate results;web logs;data flows;query processing;window-based;query plans;execution engine;relational database;data analytics
nearest;keyword queries;real-life data sets;textual description;efficient retrieval of;spatial web objects;text retrieval;mobile devices;taking into account;location-aware;web services;spatial web objects;object retrieval;query keywords;query results
applications requiring;database;human/machine;game theory;data-centric
metric space;data analysis;open source;external memory;database;similarity join;numeric data;application domains;feature vectors;join algorithms;data types;similar images;data processing;real-world;operator;distance functions;similarity joins;metric spaces;similarity joins;standard database
data-flow;query execution;long running queries;query execution plans;database;parallel processing;multi-core;query execution plan;query performance;visual analysis of;visual representation;interactive visual;error prone;graph representation
dynamic programming;open source;minimum number of;matching methods;time series;database;music retrieval
user-defined;graphical interface
query specification;query language;data model
query execution;adaptive query processing;data analysis;database systems;database;data collections;raw data;user queries;traditional database systems;social networks;data files
user preferences;database;web application;application integration;query processing;location-based services;location-based;user groups;preference queries;spatial queries;standard sql
real data sets;web pages;plans;data redundancy;query performance;select-project-join;databases;relational databases;query engine
query optimization;query evaluation;optimization framework;database;major components;energy-efficient;cost estimation;large quantities of;plan generation;performance tradeoffs;evaluation model;data sets;cost-based;graphical user interface;query processing;cost-based query optimizer;query plans;aware query;database management system
sparql queries;dynamic analysis;main features;graphical interface
ad-hoc queries;support queries
network distance;user interfaces;query workload;large networks;shortest path;distributed environments;road network;query processing;road networks;keyword search;keyword search;keyword search on
communication overhead;data transformation;high degree of;tool called;entity search;diverse sources;domain-specific;search engines;intermediate results;search strategies;parallel processing;operator;relevant entities;high-level;query sets;web data
user specifies;application domain;real-world;index structure;time series;hierarchical model;error guarantees;main idea;historical queries;absolute error
pose queries;data formats;database;ad-hoc;open data;data warehouse;business intelligence;business analysts;data integration;open data;retrieval engine;local database;data analytics;standard sql
database;web data;semantic relationships;entity-relationship;advanced search;semantic annotations
pre-defined;interactive exploration;aggregate information
amazon mechanical turk;execution model;query semantics;data model;queries posed;relational data
user-friendly;usage scenarios;specification language;xml schema;regular expressions
computational complexity;resource consumption;communication overhead;database;information dissemination;instance;computing devices;persistent storage;databases;ad-hoc;decision making
human movement;optimization methods;data representation;shortest path;road network;moving objects;real world
real-world;data set;data sets;real-world entities;helps users;record-linkage
semantic web technologies;observation data;databases;database technologies;database
big data;hadoop mapreduce;parallel dbms;scientific applications;data management techniques;social networks;data processing;data organization;data volumes
data analysis;open-source;big data;machine learning;data mining;similarity joins
machine learning;information retrieval;natural language processing;databases
schema-free;storage devices;relational database systems;storage model;data consistency;storage systems;databases;high-level;range queries;database systems
data storage;information network analysis;database;semi-structured;multi-typed;inter-related;real world;data repository;heterogeneous information networks;information networks
large graphs;large networks;information dissemination;theoretical results;theoretical results
large scale;sensor devices;health information;clinical information
data services;sensitive data;data centric;query processing in;privacy-preserving;data security;multidimensional space;data management;data confidentiality
web-pages;large data sets;stream data;structured data;massive graphs
data-driven;decision-making;big data;big data
large scale;social networks;data management;social networks



case study
mathematical model;control strategies;optimization problem
optimal combination of
simulation analysis;simulation results

low power;simulation results;convergence speed;adaptive algorithm;long-term;kalman filter;fuzzy logic;high precision


major source of

auxiliary
decision rules;vector machine;detection method;rough sets;test results;training sample
ant colony algorithm;simulation results;neural network;ant colony;neural network
data fusion;data fusion;fault-tolerant;test results;radial basis function;neural network
fourier transform;empirical mode decomposition;nearest neighbors;classification
feature extraction;bp) neural network
moving objects;pattern recognition;object segmentation;object segmentation
basic concepts
removal efficiency;removal rate;process parameters
input-output
comparative analysis
control theory;closed-loop;matlab/simulink;closed-loop
low-cost;simulation results;control strategy;design parameters
feature vector;database;fourier transform;feature-based;retrieval performance;local feature;retrieval method
measurement errors
numerical simulation;numerical simulation
design method;6
finite element;simulation results
optimization algorithm;optimal design;dynamic performance;membership functions;fuzzy rules;simulation results;optimal design
data-driven;clustering;model parameters;fuzzy model;fuzzy rules;modeling method;simulation results;high precision
calibration method

numerical simulation;numerical simulation

software systems
design method;design method
automatically generate;development environment;development environment

complex network;supply chain;degree distribution;complex network;clustering coefficient;supply chain;shortest path;power-law distribution;supply chain
scheduling algorithm;scheduling strategy;scheduling problem;large-scale
production process;case study

finite element model;test results;simulation results
matlab/simulink;control method;neural network;fuzzy control;neural network

modeling method


test results;high-accuracy;measurement errors;calibration method;high accuracy
purity
high-speed;high-speed;finite element
high efficiency;processing requirements;video sequence;image processing;edge image
sufficient conditions
image features;classification;vector machine;simulation experiments;principal component analysis;feature vectors

optimization design;design method;mathematical model of;optimal design;objective function;optimal design
network bandwidth;road network
design method
power factor;comparative analysis;control strategy
biometric recognition
decimal floating;low cost;low-power;data processing;higher accuracy;decimal floating
edge detection;target image;high quality;detection method;topological structure;target image;extraction accuracy;detection method
clustering;manifold learning;feature extraction method;features extracted;manifold learning;fault detection;dimensionality reduction;low dimensional;principal component analysis;learning method;clustering quality
long-term;case study;product design;large number of

search results;search algorithm;real networks;breadth-first search;complex networks;search strategy;high degree;shortest path;complex networks;search algorithm;traffic load

simulation results
shortest distance;shortest distance;genetic algorithm

vector space;moving platform
simulation model;product development;simulation model;product development
air quality;air quality
test cases;depth first search;generation algorithm
mathematical model
finite element model
wind speed
large quantities of
load distribution
high-speed;high-speed;closed-loop;control strategy;sufficient condition for;fault-tolerant;control problem
data structure;storage systems

high-speed;internet traffic
modeling method;control method
image segmentation;point cloud;automatically extracting;feature extraction;geo-referenced;contour extraction

convergence rate;objective function;search algorithm for
finite element model;finite element method;genetic algorithm
great potential

learning process;network security
positive effect
higher precision;frequency domain;wavelet analysis;prediction method;data processing;measurement errors;measurement data
control strategy;matlab/simulink;simulation results;dynamic performance
statistical methods;data acquisition;related data
traffic information;database design;spatial information;information management;information management;traffic management;traffic information;geographic information systems



object tracking;auxiliary;information exchange;sensor networks;object tracking;human body;target tracking
image retrieval;image retrieval;feature descriptor;content-based image retrieval;feature descriptor
mathematical model;numerical simulation
multi-view;cloud services
computational efficiency;finite difference;simulation results;finite difference
high accuracy;light source
signal processing;high precision
high-speed;high-speed
high efficiency
theoretical basis for;control method;auxiliary;fault tolerance;dynamic model
high temperature;test results

control mechanism;air quality;air quality
optimization design;structure parameters;numerical simulation;wide range;geometric structure
potential function;numerical simulation
transformation matrix
school news information system;school news information system;mathematical models;smoothing method
indoor environment;image processing;moving object
optimization design;simulation result;web cache;high quality;web caching;web cache;load balancing;application server;load balancing

ant colony algorithm;mathematical model of

statistics based;statistical method;tree model

numerical analysis;numerical simulation;finite element model
selection criteria;mathematical model;database;database
input data
numerical simulation;finite element method
optimization algorithms;optimization algorithms;optimization problem
design parameters
structure parameters;multi-objective optimization;objective functions;sample points
optimization method;genetic algorithms
test results
test results;optimization design;finite element method
motion) model
including: self identification and other identification, relevant object adaptation, environment adaptation, dynamic connection and data sensitivity of primitive graphic(pg
data stream;high rate;data streams
great significance;sensor network;sensor network;data transmission

optimal control
multiple images;free-form;image pairs;partial occlusion;image processing;objective function;multiple images




control strategies;simulation results;mathematical model of
relative error;spatial location

geographic information;flash-based;information processing;flash storage;geographic information;high speed;data storage
information fusion;recognition rate;evidence theory;bp neural network;evidence theory;simulation results;information fusion;bp neural network;wide range;probability distribution function;neural network
adaptive filtering;filtering algorithm;filtering algorithm;estimation method
feature vector;kernel matrix;detection method;fault detection;kernel principal component analysis;fault detection;large sample;simulation results
markov chain;forecasting model;relative error;accurately reflect;markov model
genetic algorithm;bp algorithm;bp algorithm;fast convergence;high accuracy;fitness
mathematical model of
high-speed;high-speed;fuzzy neural network;control algorithm;decision-making;intelligent control;simulation results
numerical simulation;numerical simulation;flow field

intelligent control;large number of;neural network
closed-loop;closed-loop;power supply;high-frequency
database;simulation results;simulation experiments
classification;vector machine;classifier;training samples;spatial structure;classification rate
power supply
image registration;registration algorithm;high-precision;frequency domain;fourier transform;sampling approach;spatial domain;pixel-level
vector machine;simulation results;classification

high speed;high speed
power supply
basic concepts
analytic hierarchy process;bayesian network;neural network


motion control;low cost;human-machine;motion control
tracking algorithms;tracking algorithms;vision-based;machine vision
information fusion;multi-sensor;machine vision
high frequency;working principle;optimal parameters;design requirements;structure parameters

case study;energy efficiency
factors affecting;monte-carlo;seismic data;production data;success rate
forecasting model
numerical experiments;numerical simulation
control theory;control theory;dynamic environment
kalman filter;state estimation;kalman filter
real - time monitoring;application logic;image processing;data processing
stereo vision;relative error;stereo vision
numerical simulation;calibration method;basic concepts
ct images;redundant information;image registration;registration algorithm;statistical analysis;edge detection;image registration;automatic extraction of
machine vision;hypothesis testing;normal distribution;machine vision

small sample;monte-carlo
finite element model;test results
data-driven;process model;multiple models;control strategy;simulation results
optimal parameters;stock market;support vector regression
search algorithm for;optimization problems;np-complete;population-based;large scale;real world;search algorithm
data collected from;recognition algorithm;feature extraction method;classification
multi-core;multi-core
finite element method;dynamic analysis
optimal parameters;mathematical model;times higher
network traffic;forecasting model

supply chain;supply chain;supply chain
fast response;control algorithm;fuzzy logic;fuzzy controller;simulation results;control algorithm

state space;wind speed;mathematical model of;control theory;simulation results;optimal control;control algorithm
rfid technology;rfid technology;credit card
design method;closed-loop;sufficient condition for
simulation model
high correlation;finite element;correlation method;correlation method
simulation result

numerical simulation;simulation results;flow rate
control strategies;simulation results;control strategy
1;3;2
high-speed;high-speed;feedback controller;fault-tolerant;control law;control strategy;fault-tolerant;simulation results
share information;online social network;online social network;social network
character segmentation;character segmentation;segmentation algorithm
nearest neighbor;unlabeled data;semi-supervised clustering;nearest neighbors;clustering algorithm;semi-supervised;nearest neighbors
high frequency;numerical simulation
simulation analysis
numerical simulation;simulation result
genetic algorithms;ant colony optimization
school news information system;school news information system;database;physical design;school news information system
clustering;high temperature;production line

completeness;fuzzy neural network;input variables;algorithm takes;fuzzy rules;measurement data

working principle;power consumption;finite element analysis
numerical simulation;feedback controller;linear representations;local linear;fuzzy model;control law;fuzzy model



large scale;development process
wireless network;health monitoring;finite element analysis;health monitoring
traffic monitoring;traffic information

real-world;high risk;design parameters

dynamic model
von-mises;finite element
high-speed;high-speed;pseudo-random;pseudo-random
simulation results;control strategy
mathematical model
principal components analysis;single-object;regression analysis;evaluation criteria;evaluation model;comprehensive evaluation

modeling method;modeling method;high accuracy
evaluation model;high risk;wide range;evaluation model
social media;17;historical information;query workloads;machine learning models;prediction errors;data set;data sets;analytical model;data flows;ad hoc
parallel database;database systems;database;partition-based;database systems;database;large database;database systems
stream processing systems;large volumes of;virtual machines;highly variable;stream processing systems;network latency;stream processing;rates;adaptive approach;data-intensive applications;social networking;resource allocation
optimization problems;filter ordering;detection algorithm;data stream;dynamic environment
query evaluation;systems support;database management;data types;query evaluation;query plan;operator
individual queries;design decisions;database;query workloads;databases;query optimization techniques;database servers;physical design;query workload;databases;database administrators;data placement;data placement;database schema
singular value decomposition;large numbers of;traditional models;spatio-temporal;high-order;tensor based;tensor-based;sensor networks;clustering algorithm;real world;temporal data;dimension reduction;traffic data
spatial attributes;social network analysis;text-based;spatial attributes;location information;wide range
data analysis;spatial data;visualization tools;database;multiple representations;data access;massive amounts of data;visual analytics;visual analysis;data integration
span multiple;mobility patterns;instance;provide answers;keeping track of;digital libraries;digital libraries;mobility patterns
data center;multi threaded;sql server;data center;database access;2;database access;1
moving objects;xml schema;moving object
multi-dimensional;web queries;scientific data;query language;scientific computing;aggregate functions;semantic web;persistent storage;relational database;user-defined functions;data type;data management;programming language
data integrity;instance;access patterns;takes into account;data model;long-term;long-term;metadata management;metadata management
kernel density estimation;density estimation;ontology alignment;ontology alignment;ontology alignment
hybrid approach;linked data;database technology;database;database;query response times;data sources;database technologies;structured data;data management;small-scale
decision support system;development process;decision-making;multi-attribute
cost functions;query language;optimization problems;multi-stage;manufacturing process;utility functions;finite domain;multi-stage;search heuristics
mathematical formulation;decision-theoretic model;decision-theoretic model
information sources;agent-based;semantic search
knowledge management;data collection;business process
database;databases
1;simulation study;excellent performance
decision makers;event-based
decision-making;obtain information;social web;decision-making;decision-makers
ad-hoc;user preferences;intelligent agent
high level;data warehouse;event processing;data warehouse;data extraction;data model;lower level;aggregated data;data source;network traffic;load balancing;distributed systems;trend analysis
past experience;decision process;decision processes;simulation results;agent-based;decision process
processing cost;query language
span multiple;information systems;decision support;database;rule-based;decision support;human experts;data sources;data mining techniques;related data;problem solving;database administrators;knowledge representation;knowledge extraction
data services;large-scale;ad-hoc;application level;databases;data management applications;databases;application development
data center;database replication;virtual machine;data management;database replicas
open source;hadoop mapreduce;large-scale;frequent updates;computational resources;web-scale;update operations;web-scale;index size
attribute-based;access control;mobile access;data security;healthcare data;data storage
database;data owners;database;data stored in;databases;relational database
trade-offs;large corpora;query workload;structured documents;xml documents;query answering
management architecture;large-scale;stream data;data transfer;data access;data processing;data-intensive applications;cost-effective;computing platform;dna sequence;mapreduce-based
traditional databases;graph database;database;relational model;data structures;database technologies;graph queries;query languages;graph databases;graph database
object based;structural relationships;random walk;page rank;graph analysis;operator;relational databases;query languages;social sciences
data structure;database systems;graph databases;graph databases;data management systems;graph traversal
multi-valued;attribute-level;quantitative association rules;data set;finding clusters;databases;association rules;mining associations;classifier
large networks;network datasets;query node;node labels
hash-based;degree distribution;data locality;large number of;social relations;spatial network;counterpart;replication;data movement;fast retrieval;social networks;power-law;online social network
semantically related;large-scale;large-scale;data movement;current web;1;semantic web;2;data management techniques;structured information;semantic web technologies
data collection;designed specifically for;mobile devices;collected data;data collection;central server;data transmission;mobile users
security requirements;sensitive information;data types;data protection;data protection;user-centric;user study
data model
central server;real-world;wireless communication;anonymization algorithm;location aware;mobile devices;wide range;location-aware;spatial distributions;reconstruction algorithm
sensitive data
mobile devices;privacy risk;privacy risk
database;performance goals;database management system;multiple types of;data server;workload management
performance degradation;taking into account;database;database queries;instances;query plans;operator;load balancing;data distribution;load balancing
monitoring applications;weighted average;continuous queries;data stream management systems;continuous queries
clustering;probabilistic modeling;data manipulation;pre-processing;unsupervised learning;outlier detection;rare-class;conference on knowledge discovery and data;pattern mining;knowledge discovery;data mining;time-series;supervised learning;dimension reduction
formal framework for;relational algebra;xml data;annotated data;answering queries;theoretical foundations;years ago
temporal information;search engines;temporal dimension;web search engines;information retrieval
temporal features
flash-based;high throughput;durability;indexing structures;increasing number of;indexing structure;high-throughput;lock;main memory;modern hardware
web engineering;business intelligence;international workshop on;edbt/icdt;business intelligence;international workshop on
data mining;social network;real-life;data mining projects;data mining;data mining problem;data management
discovery algorithms;bayesian information criterion;maximum entropy model;probabilistic models;iterative process;real data;correctly identify;minimum description length;knowledge discovery;candidate set;maximum entropy;avoid redundant
clustering;approximation algorithms;sampling algorithms;complex networks;main memory;disk access;disk accesses;estimation algorithms;numerous applications
domain adaptation;classification;target domain;training set;source domains;probability distribution;generalization performance;marginal probability;weighting scheme;theoretical analysis;classification accuracy;multiple sources;multiple domains;transfer learning;conditional probability;data distribution
random projection;computational complexity;distance function;average error;separability;random projections;set-covering;2011;classifier
source code;related data;software systems;international workshop on;data mining;development process;software quality
nearest neighbor;rknn queries;data updates;processing algorithms;efficient algorithms to;decision support systems;location-based services;theoretical analysis;algorithm to compute;nearest neighbors
recommendation systems;applications including;instance;storage space;large graphs;citation networks;distributed settings;social networks;graph queries;compression scheme
information-theoretic;clustering quality;step procedure;structural information;semistructured data;similarity measures;semistructured documents;structural similarity
search space;search algorithm;fuzzy objects;distance function;threshold queries;refinement process;tree-based;image databases;spatial queries;optimization techniques;nearest neighbor queries;query processing;real datasets;pruning rules;query results;probability threshold;fuzzy objects
general case;designed to support
execution plans for;parallel execution;data analysis;optimizer;data services;data mining applications;execution plan;parallel execution;parallel databases;online services;fault tolerance;click streams;graph data;massive data sets;highly scalable;search logs;distributed computation
stream processing;complex queries;sampling methods;random variables;uncertain data;relational operators;data model;efficient computation;monitoring applications;evaluation techniques;data streams;query planning;uncertain data streams;stream systems;uncertain data streams
human interactions;world wide web;online social networks;data mining and machine learning;social activities;complex systems;wide range;social networks;acm international workshop on;social networks;information networks;information technology;communication networks
robot control;apprenticeship learning;inverse reinforcement learning;reinforcement learning;machine learning;apprenticeship learning;machine learning techniques;machine learning
itemset mining;high level;user specifies;data mining techniques;constraint satisfaction;machine learning and data mining;constraint programming;machine learning;optimization problems;modeling language;specific problem;data mining problem;data mining problems
music recommendation;signal processing;machine learning methods;real-world;ranking models;helping users;multi-class
data analysis;visual analytics;history data;large datasets;visual analytics;visual analysis
data analysis;observed data;social science;probabilistic models;data sets;social network;matrix factorization;social network data;social networks;latent variable models;basic concepts;network data;latent variable models
minimum description length;locally optimal;binary data
high quality;artificial datasets;sample size;frequent itemsets;performance guarantees;higher quality;data mining;multiple times;frequent itemsets;efficient discovery of;association rules;database applications;association rules
large scale;global models;local structure;categorical data;data distribution
source code;hill-climbing;hill-climbing;bayesian network;structure learning;data sizes;target variable;data sets;combines ideas from;constraint-based;network structure;bayesian network
classification performance;conditional probabilities;computational complexity;bayesian network classifiers;embedded systems
markov logic networks;training data;taking into account;probability distribution over;social network;markov logic;conditional probabilities;knowledge base
real data sets;graphical models;training data is;probability estimates;score-based;classification performance;inference methods;efficient (approximate;score-based
multiple classes;large-margin;multi-class;margin;classifier;large-margin classifiers
multi-modal;social media;classification;data points;multi-view;logistic regression;data point;information retrieval;cross-media;classification method;similarity measures;fusion strategies;classification algorithm
applications including;high dimensional;label-noise;label-noise;logistic regression;gene expression analysis;classifier
parameter space;large-scale;latent space;large-scale datasets;online reviews;sentiment classification;document-level;sentiment classification;pre-processing;classification task;classifier;feature selection
high-dimensional;linear classifier;classification;worst-case;learning algorithm;kernel functions;real world;support vector machines;feature vectors;high classification accuracy
fast algorithms;learning algorithms;monte carlo;large-scale;feature construction;ensemble methods;tree-based;feature generation;decision trees;wide range;feature generation;automatically constructing;learning problem;monte carlo search;search algorithm;decision making;standard datasets
feature selection algorithm;discriminant analysis;data analysis tasks;unlabeled data;feature set;feature selection method;semi-supervised feature selection;structural information;higher order;classification performance;labeled data;feature selection;individual features;feature selection methods;dimensional data;semi-supervised;subspace learning;data-sets;labeled and unlabeled data;relevant features;selecting features
feature space;metric learning methods;learned metric;learning algorithms;learning tasks;predictive performance;instances;metric learning methods;metric learning;provide evidence;learning problem
software development;distributed computing;business data;large scale;unsupervised feature selection;massively parallel;large-scale;feature selection algorithms;feature selection algorithm;large-scale;feature selection;parallel processing;computing environments
clustering;principal components analysis;optimization problems;data mining applications;spectral methods;matrix factorization;power consumption;wide range;statistical analysis;data mining;relevant information
classification;classification;feature extraction;great potential;typically requires;segmentation algorithms
clustering;nearest;face verification;classification;distance metric learning;uci datasets;optimization algorithm;25;gradient-based;distance metric;metric space;machine learning algorithms;convex optimization problem
gaussian kernel;kernel k-means;clustering tasks;classification problems;reproducing kernel hilbert space;euclidean distances;high dimensional spaces
database;real datasets;highly accurate;convergence rates;boosting algorithms;support vector machines;conditional probabilities;nearest neighbors
regularization;classification;theoretical properties;generalization performance;ensemble methods;individual classifiers;closely-related;ensemble pruning;ensemble pruning;learning framework;classifier;space complexity
ensemble framework;big data;tree-based;ensemble methods;distributed databases;instances;supervised learning;embedded systems
linear program;database;frequent subgraph;interior point methods;pattern mining;large networks;graph based;support measure;support measures
label information;level features;locality-sensitive hashing;vector-valued;graph kernels;labeled graphs;efficiently extract;real-world;complex data;label propagation;benchmark datasets;structured data;similarity measures
object tracking;graph mining;spatio-temporal;tracking objects;graph patterns;contextual information;graph mining;mining algorithm
auxiliary;special structure;semi-supervised learning;directed graph;classification performance;eigenvalue problem;clustering problems;cluster sizes;special properties
matching algorithms;mining frequent patterns;np-complete;subgraph isomorphism;large networks;social networks;real-world
graph model;real world;graph properties;graph data
large-scale;problem remains;real-world;information systems;applications including;multi-core;communication costs;distributed memory;connected components;large number of;image analysis;data sets;real data sets;social networks;data mining
high-dimensional;binary data;machine learning;random projections;databases;numerous applications in
parallel implementation;large corpora;sparse matrices;large collections of;topic models;topic model;algorithm converges
similarity information;multi class classification;classification;hierarchical organization;large number of;error correcting output codes;binary code;binary classifiers;nearest neighbor rule
very large datasets;approximation error;data mining applications;main memory;memory requirements;real world datasets;large datasets;data mining;tensor decomposition;theoretical guarantees
regularization;large scale;convergence rate;search strategy;coordinate descent;coordinate descent;convergence rates;gradient method;optimization techniques
high-dimensional datasets;training data;large-scale;optimization method;massive datasets;logistic regression;supervised learning;sampling technique
likelihood ratio;similarity measure;digital library;similarity measures
optimizer;random variables;long-range;local models;relational models;probabilistic inference;optimization method
synthetic data sets;related terms;unstructured information;database;open data;multi-relational;simple heuristics;low-rank;information extracted from;data sets;context models;feature selection;latent semantic;contextual information;textual documents;highly scalable;sparse matrix
classification problem;learning rules;multi-relational;instances;inductive logic programming;rule discovery;breast cancer;synthetic data
optimization framework;training examples;large-scale;application domain;3;standard benchmarks;machine learning;data sets;4;computational biology;learning framework;multi-task
regularization;global optimal solution;multi-task;regularizer;learning problems;von neumann;learning tasks;machine learning;multi-task learning;metric learning;learning problem;regularization framework
beam search;binary classification;classifier;theoretical justification
multi-task;decision trees;multiple tasks;multi-task;spam filtering;decision rules;information gain;decision tree learning;predictive performance;multi-task learning;tree construction;decision trees;weak classifiers;multi-task
multi-task;boosting methods;optimization procedure;labeled data;multi-task learning;base learners;theoretical analysis;multi-task learning;learning task;multi-task
lower bound;multi-task;gaussian processes;learning models;real data;baseline methods;multi-task learning;computational cost;gaussian processes;multi-task
structured domains;long-range;real-world;linear-chain;information extraction systems;information extraction;semi-structured documents;context-specific;information extraction from;error reduction;conditional random fields;classifier
training examples;large scale;semantic model;sufficiently large;supervised approach;supervised learning;semantic relatedness;training sample
particle filters;dynamic programming;sequential monte carlo;inference techniques;bayesian inference
specific features;world knowledge;weakly supervised;sentiment analysis;labeled data;semi-supervised;trend analysis
textual data;textual content;text categorization;online learning;topic classification;real world;question answering;classifier;classification algorithm
learning process;learning algorithms;kernel-based;large-scale datasets;online learning;online learning;learning method;support vectors;kernel-based
decision rules;decision rules;data mining;rule sets;change detection;prediction tasks;data streams;decision model;data streams
naïve;frequent items;real datasets;skewed;data streams;mining algorithm
metric learning methods;learning algorithm;large data sets;metric learning;learning problem;loss functions;feature selection
service provider;mining results;completeness;data-mining;outlier mining;outlier-mining;specific task;outsourced data;computational resources;distance-based;data owner
real world data sets;information dissemination;privacy concerns;differentially private;regression models;differentially private;membership functions;approximation algorithm;machine learning;low-dimensional;selecting features
prediction techniques;regularization;discriminative models;sensitive information;regularizer;machine learning;data mining technologies;logistic regression;classifier
wikipedia-based;historical data;user session;ctr;collaborative-filtering;recommendation methods;web site
recommendation quality;computational efficiency;context-aware;user behavior;real-world applications;explicit feedback;contextual information;benchmark datasets;implicit feedback;learning method
multi-class classification;close connection;theoretical properties;label ranking;probability estimation;probability estimation;real-world;multi-class;ranking model
state space;large scale;prior knowledge;markov decision processes;sparse representation;theoretical analysis;transition model;decision making
preference-based;preference learning;ranking queries;inverse reinforcement learning;reinforcement learning;prior knowledge;instance;policy search;reward function
data-driven;function approximation;large state spaces;real-world;decision-making;reinforcement learning;wide range;long-term;control mechanisms;margin;training phase
state space;mixture model;inverse reinforcement learning;real-world;inverse reinforcement learning;state-action;reward function;reward functions;markov decision process
state space;auxiliary;search tree;action selection;optimal values;markov decision processes;monte carlo tree search;internal nodes;fast convergence
error-correcting output codes;real-world scenarios;classification framework;real-world;learning algorithms;reinforcement learning;reinforcement learning;classifier;action sets
planning problem;energy management;machine learning;united states;large datasets;high cost;data-mining techniques
high-dimensional;state spaces;algorithm performs well;inverted pendulum;basis functions;standard benchmarks;algorithm learns;resource management;transition model;state-action;learning method
apprenticeship learning;graph-based;measurement noise;reward function;markov random fields
naive bayes classifier;parameter-free;classification;classification process;rule-based;rule mining;prior distribution;classification rules;data sets;bayesian approach;databases;classifier;locally optimal
data mining methods;rule set;rule mining;bayesian inference;rule mining;interesting patterns;evaluation function
depth-first search;mining algorithms;data representation;data stream mining;fp-growth
label information;semi-supervised learning;unlabeled samples;semi-supervised learning methods;multi-label classification;supervised methods;graph-based;text data sets;label propagation;learning task;multi-label;global solution
web documents;web pages;unlabeled examples;semi-supervised learning;training classifiers;information extraction;instances;labeled training examples;structured information;semi-supervised;naive bayes;labeled examples
weighted graph;graph-based;graph-based;data sets;iterative algorithm;similarity measure between;label propagation;multi-class;convex optimization problem
pagerank algorithm;random walk;unlabeled data;classification accuracy;semi-supervised learning;random walk;users' preferences;label propagation;labeled data;random walks
unlabeled data;labeled data is;multi-label learning;single label;coordinate descent;classifier;multi-label classification;supervised methods;subspace learning;multi-label;labeling process;semi-supervised;large-margin;semi-supervised;learning method
compact representation;signal processing;real-world;artificial data;time series;time series;minimum description length;algorithm produces;feature selection;physical systems
low bandwidth;local features;optimization scheme;communication cost;sensor networks;sensor measurements;upper bound;support vector machines;distributed data
prediction accuracy;labeled training data;gaussian process regression;canonical correlation analysis;unsupervised fashion
optimization techniques;kernel-based;classification;classification framework;level features;hidden markov models;protein sequence;kernel methods;artificial neural network;relevant features
mining closed;pruning techniques;synthetic data;equivalence relation;mined patterns;equivalence relations;databases;discovery problem
multi-level;synthetic data;event sequences;real data;rates;event sequences
real life;social networks
np-hard problem;linear combination;social network;communication cost;heuristic algorithms;social networks;cost function;approximation algorithm;real datasets;approximation ratio
social media;information diffusion;viral marketing;additional features;probabilistic models;large number of;real-world networks;network structure;diffusion process
real-world;monte-carlo;efficient algorithms to;social network;seed set;large-scale social networks;diffusion model;greedy algorithm;belief propagation;computation costs
spatial information;social network;social relations;location-based social networks;social graph;learning models;social networks
expected number of;viral marketing;real-world;seed set;social networks;influence propagation;influential nodes
parameter values;parameter learning;synthetic data;social networks;common practice;power-law;model selection
data set;viral marketing;propagation model;social network;1;budget constraints;data sets;viral marketing;social networks;problem setting;greedy algorithm;approximation algorithm;np-hard
logistic regression) model;social influence
behavioral patterns;interesting patterns;baseline methods;power-law distribution;social networks;data distribution
prediction accuracy;trajectory patterns;gaussian process regression;real-world;prior knowledge;movement data;increasing attention;real world scenarios
social media;regularization;data source;social media;real-world;spatio-temporal;theoretical properties;optimization algorithm;case study;incomplete data;estimation problem
10;data-mining;optimization problems;real datasets;social network;11,22;data-mining results;wide range
training data;web search;web search engine;test queries;considerable effort;ranking models;information retrieval;ranking performance;document length;discounted cumulative gain;ranking functions;active sampling;query results
outlier detection;real datasets;pattern mining;temporal behavior;community members;step procedure;patterns discovered;numerous applications;effective tools
data structures;time series;data structures
intra-class;training set;nearest neighbor;classification;instances;decision boundary;medical diagnosis;large number of;time series;machine learning;support vector;time-series;training instances;time-series;maximum margin;support vector machines
temporal dependencies;sparse learning;gene expression;meaningful clusters;clustering structure;real-world;time series;bayesian inference;clustering methods;gibbs sampling;real data;auto-regressive;model estimation;clustering quality
cold start;feature space;predictive models;training data;auxiliary;frequency distribution;target domain;transfer learning;learning paradigm;recommendation methods;source data;increasing attention;transfer learning
dimensional space;user interfaces;learning algorithm;context-free;statistical learning;temporal information;user interface;problem solving;real world domains
clustering;spectral clustering;clustering performance;auxiliary;clustering algorithms;data manifold;clustering task;learning tasks;algorithm called;clustering tasks;knowledge transfer;unlabeled data;transfer learning
sentiment analysis;semi-automatic
multi-valued;user interactions;association rule mining;pattern mining;association rule mining;search paradigm;user interface;search engine;relevance feedback
machine learning;content analysis;machine learning algorithms;machine learning methods;natural language processing
web browser;web browsers;easily extensible;mobile devices;knowledge discovery;service-oriented;web services;data mining;scientific workflow
mining results;user-friendly;interactive exploration;real data sets;spatio-temporal;pattern mining;moving object;moving objects;mobile communication;real world applications;temporal patterns
open-source software;classification;genetic programming;combinatorial optimization;knowledge discovery;knowledge discovery
mining task
data mining and machine learning;workflow engine;workflow management
search terms;latent dirichlet allocation;relevant documents;similar topics;topic models;retrieved documents;linear order;topic modeling;visualization techniques;document collections
pattern mining;open-source
combinatorial optimization;lower bounds;machine learning
matrix factorization;social media;optimization techniques
machine learning algorithms;machine learning

free text;clinical data;data mining techniques;data mining applications;data mining;patient data;databases
concise representations;data analysis;magnetic resonance imaging;great success;sparse representations;biological network;scientific discovery;biomedical data;theoretical guarantees
similarity assessment;distance metrics;similarity measure;learning approaches;decision support;decision support applications;retrieval results;metric learning;distance metric;similarity measures;similarity based;metric learning
medical imaging;user groups;machine learning;data mining techniques;medical images
medical records;surveillance systems;drug design;united states;short term;data mining;long term;data generated from
cross domain;hypothesis generation;applications including;cross domain;kdd community;negative transfer;matching problem
taking into account;parallel computing;business data;learning algorithms;learning systems;computing devices;streaming data;adaptive algorithms;business applications;realistic data;learning models;increasing amounts of;expert knowledge;model building

hypothesis generation;information retrieval
information retrieval;digital library;information retrieval
linked data;structured documents;focused retrieval;book search;relevance feedback;test collections;knowledge transfer;evaluation measures
search applications;test collection;information access;retrieval systems
semantic search;semantic search;entity-oriented;international workshop on;international workshop on;entity-oriented
open source;open source;sigir 2012 workshop on;information retrieval;future directions for;open source;information retrieval
sigir 2012workshop on;information access;information access;workshop brought together
world wide web
individual queries;query term;information retrieval;desirable properties;search engines;retrieval model;retrieval function;likelihood function;information retrieval research;language model;pseudo-relevance feedback;general problem;query language model;language models;language modeling approaches;search engine;statistical model;retrieval precision;pseudo-relevance feedback;theoretical framework;rank documents;retrieval results;retrieval models;scoring function;language modeling approach;query likelihood;query terms
predictive models;feature extraction;human knowledge;large-scale;web sources;real-world;news events;web content;1, 3, 5;2;web resources;4;artificial-intelligence;6;knowledge-intensive;user behaviors
query evaluation;encoding scheme;xpath queries;xml tree;twig queries;query processing in;encoding scheme;query plans;relational databases;optimizer
sequence matching;sequence database;efficiently identify;computational biology;databases
maximum number of;probabilistic semantics;frequent subgraph mining;measure called;approximation quality;frequent subgraphs;uncertain graph data;graph data;frequent subgraph;theoretical analysis;uncertain graphs;real numbers;mining algorithm;graph databases
shortest paths;end-users;finding an optimal;database applications;large graph;real graphs;large graphs;shortest distance;heuristic algorithms;answering queries;graph analysis;mining tasks;labeling scheme;query-specific;shortest distance;np-hard
answer-set programming;instances;xml trees;np-complete;access control
web documents;spatial proximity;taking into account;object retrieval;text retrieval;search space;location-aware;geo-referenced;text documents;excellent performance
fundamental properties;mapping language;query language;query language;conjunctive queries;operator;metadata management;schema mappings;tuple-generating dependencies
set similarity join;synthetic datasets;textual information;social network;common interests;batch processing;similarity joins;similarity join;distance join;base-line
result diversification;similar object;np-hard problem;index structure;user queries;query result
frequent itemset mining;frequent itemsets;itemset mining;differentially private;databases
description language;distributed computing;large scale;data-intensive;data intensive applications;end user;data intensive
exploratory analysis;input data;workflow execution;execution model;management systems;scientific workflow;workflow engine;parallel execution
handle complex;workflow engine
parallel computation;workflow management system;workflow management
data dependencies;lower level;programming models;data movement;highly scalable;distributed-memory;coarse-grained;computing systems
low-quality;synthetic data;data fusion;real-world;integrate data from;data integration
network routing;performance bottlenecks;hadoop mapreduce;computation cost;storage structures;cost-based;model called;optimization techniques;distributed systems;query processing
query rewriting;query answering;query containment;relational databases;regular path queries;data exchange;conjunctive queries;schema mappings;general case;upper bounds;data stored in;query processing;databases;graph databases;query answering;schema mappings;query processing
high-dimensional;user's preferences;pruning technique;sensitivity analysis;result tuples;dimensional data;query result
graph database;social interactions;large scale;social communities;social network;large scale;social graph;large scale social networks;social network analysis;visual analysis
low accuracy;deep web;data quality;fusion methods;data sets;web data
query optimization;query evaluation;count-based;selectivity estimation;database;amazon's mechanical turk;selectivity estimates;text processing
data-driven;virtual machine;instances;increasing number of;constraint programming;performance degradation;application code;integer programming
real-world datasets;auxiliary;query loads;pruning rules;real datasets;subgraph isomorphism;neighborhood information;code base;graph databases;np-hard
protocol called;database server;database systems;multi-core;lock;main-memory;main memory;concurrency control mechanism;databases;concurrency control;distributed database;performance bottlenecks
image retrieval;classification problem;classification;image features;classification accuracy;meta-data;automatic methods;feature descriptor;data set;medical images;image representation
classification;web news;user queries;knowledge extraction;social networks;data collected;web data
process mining;event data;process mining;data analysis;information systems;high quality;ad hoc
decision-making;prediction methods;multi-relational;underlying assumption;user interface;link prediction
feature types;classification;spatial relations;similarity measure;large scale;spatial relation;closely related;spatial relation;semantic representation;knowledge base;natural language
search results;relevant documents;traditional information retrieval;query reformulation;information retrieval;natural language processing;textual documents;geographic information retrieval
clustering;learning process;ensemble methods;unsupervised learning;real data sets;information contained in;learning algorithms;clustering method;data clustering;adaptive clustering;boosting algorithms;ensemble method;clustering algorithm;ensemble methods;supervised learning;classifier
special case;kernel methods;pattern recognition;kernel functions;machine learning;data mining;operator;gaussian kernel;kernel methods;gaussian kernel
clustering;mutual information;deng, s., he, z., xu, x.: g-anmi: a [mutual information;genetic algorithm;categorical data;clustering accuracy;algorithm named;clustering categorical data;clustering algorithm;low efficiency;knowledge-based systems
ranking algorithm;ranking problem;classification problem;classification;instance;uniform distribution;multi-label classification;instance;single-label;nearest-neighbors;instances;single-label
clustering;clustering;clustering results;nearest neighbor;arbitrary shape;classification;prior knowledge;clustering result;nearest-neighbor;clustering techniques;dimensional data;clustering algorithm
high-dimensional datasets;itemset mining;closed itemsets;low support;search process;frequent itemsets;tree data structure;search space
data analysis;gene expression data sets;multi-objective;gene-expression;gene regulatory network;gene-expression data
multi-class problems;synthetic data sets;face recognition;training data set;training phase;face databases;face recognition;data set;base learners;multi-class;multi-class boosting;learning performance;ensemble method
selection problem;np-hard problem;cost model;machine learning;cost models;machine learning techniques;optimal performance;data locality;automatically learn
spectral embedding;data fusion;embedding methods;embedding methods;canonical correlation analysis;low dimensional space;graph representation
clustering;matrix factorization;labeled samples;dimensional space;graph based;low-rank;unlabeled samples;matrix factorization;clustering performance;document datasets;document clustering;gradient method;graph based;discriminative information;update rule;semi-supervised;semi-supervised
mining results;frequent itemset mining;graph mining;graph data;classification;frequent itemset;learning approaches;large sets of;frequent itemset mining;predictive performance;graph data;computational cost;complex structures;prediction models
feature space;regularization;classification problem;graph-construction;random subspaces;random subspaces;structural properties;real-world;learning algorithms;graph-based;problem domains;unlabeled samples;semi-supervised learning algorithms;graph-based semi-supervised learning;task-specific;dimensionality reduction;semi-supervised setting
multi-strategy;instance selection;automatically determines;instances;text classification tasks;learning algorithm;cost-sensitive;instance;active learning approach;cost-sensitive;text classification;interface design;additional cost
evolutionary algorithms;evolutionary algorithms;local search;multi-objective;meta-learning;automatic selection;model selection;evolutionary algorithm;model selection
web pages;test set;classification;large-scale;large scale;learning algorithm;vector machine;incremental learning;topic classification;feature vectors;test sets
text mining;domain experts;sampling techniques;machine learning;text mining;automatically generate;machine learning
data source;classification;classification;feature extraction;spatial features;data set;independent component analysis;data sets;spatial structure
singular value decomposition;linear models;singular vectors;sliding window;time series;sequential data;temporal evolution;sliding windows
mutual information;gaussian process regression;variable selection;soft sensor;process control;soft sensor;sampling rate;model training
desired behavior;operator;discrete event systems;normal behavior;dynamical systems
collective classification;classification;instances;multi-label learning;graph-based;multi-label classification;collective classification;real world datasets;labeled data;classification framework;ranking method;complex structures;multi-label;real world applications;classifier;relational data
matrix factorization;regularization;classification;classification;dictionary learning;training set;machine learning;matrix factorization;sparse representation;training samples;classification approach;sample size;learning method
neural network
combination function
automated reasoning;ontology-based;partial information;decision support system;patient data;machine learning;medical datasets;information systems;missing data;machine learning techniques;machine learning;patient data;decision making
gradient-based;learning rate;learning algorithm;learning scheme;feature selection;decision process;modeling approach;statistical tests;input variables;neural network;feature selection
moving object detection;gaussian mixture model;motion model;moving object;moving objects;moving object detection;optical flow;moving object detection;motion information
multi-agent;multi-agent;multi-task
rbf neural network;short-term;artificial intelligence;short-term
event-driven;energy-efficient;energy efficient;simulation results;learning scheme;sensor networks;learning scheme;sensor networks;network lifetime;data transmission
prediction model;time series;short term;data source;arima model;neural network;prediction models;training algorithm
multiple labels;classification;semi-supervised learning;learning algorithm;training classifiers;naive bayes classifier;labeled examples;problem setting;single-view;single-view;semi-supervised;benchmark datasets;class label;training algorithm
clustering;clustering;partial matching;linear complexity;similarity metric;text clustering;information retrieval;finding clusters;feature sets;clustering algorithm;locality-sensitive hashing
background model;gaussian mixture model;learning algorithm;color space;foreground detection;statistical model;foreground detection;video frame;detection performance;em algorithm
ordinal data;classification;classification;ordinal data;real datasets;linear discriminant analysis;data replication;information retrieval;kernel discriminant analysis;generic framework;wide range;ordinal data;binary classifier
clustering problem;machine learning and data mining;test problems
face images;computational complexity;extraction algorithm;facial expression recognition;facial expression recognition;high accuracy;memory consumption;strong classifier
instance
historical data;frequently occurring;time series;time series;power grid;additional information
data collected from;closed-loop;artificial neural networks;artificial neural network
problem statement;genetic algorithm;genetic algorithm
software development;processing units;mathematical models;heuristic method;accuracies;network structure;neural network;artificial neural network
neural network;control method;control method;neural network;adaptive control
bayesian network;large datasets;correlation coefficient;learning algorithm;structure learning;pair wise;bayesian networks;correlation coefficients;fully connected
case-based reasoning;learned models;case-based reasoning;qa) systems;machine learning;relevance feedback;case base;answer ranking;provide evidence;answer quality;user feedback;question answering;similarity measures
action selection;branching factor;monte carlo tree search
intrusion detection;network intrusion;network intrusion detection;network intrusion detection;classifier;false positive;classifier
classification;bayesian network classifiers;text corpora;text classification;semantic information;dimensionality reduction
statistical methods;database;texture features;artificial neural network;artificial neural network;classification rate;large database
mixture model;anomaly detection;real data;detection methods;transactional data;transactional data
feature space;fault tolerant;sliding window;sensor measurements;condition monitoring;clustering algorithm
ranking model;real-world;topic models;fuzzy clustering;machine learning;category-specific;topic modeling;automatically generated;pre-processing;machine learning algorithms;automatically extract;multi-lingual
virtual machine;high availability;measurement noise;traditional models;fault detection;efficiently identify;high availability;fault tolerance
future events
low power;transition probabilities;unsupervised learning
power factor;nearest neighbor;distance metrics;data mining approach;distance metric;forecasting accuracy;classifier
web based;low cost;wireless communication;web based;power consumption;wireless network;web based
fusion method;classification model;state detection;data fusion;case study;instance;classification models;level fusion;sensor data;level fusion;algorithm produces;multiple sources;machine learning algorithms;nearest neighbors;state detection
skewed data;real-world datasets;class distributions;boosting methods;boosting algorithm;skewed;machine learning applications;noisy data;data quality;noise-handling
data collected by;case-based reasoning;multi-agent;sensor technology;activity recognition;case-based reasoning;constraint satisfaction;activity recognition;wireless communication
accurately detect;web server;web traffic;detection approach
seismic data;pattern recognition;cluster-based;frequency domain;feature extraction;data set;classification accuracies;seismic data;machine learning algorithms;classifier;classification algorithms
sparse representation;linear combination;structural features;input data
feature space;training data;semantic indexing;machine learning;efficiently identify;large database;membership functions;video retrieval;audio features;low-level
clustering;shape representation;graph theory;extraction algorithm;classification framework;major components;multi-dimensional;human brain;mr images;shape analysis;fundamental problem;distance metric;shape variations;object classification;shape analysis
correct classification;supervised learning;machine learning algorithms;machine learning methods;nearest neighbors
spatial clustering;clustering;clustering results;spatial data;geometrical constraints
gaussian mixture model;clinical practice;gaussian mixture model;multi-class;semi-automated;classifier
probabilistic latent semantic analysis;probabilistic latent semantic analysis;classification accuracies;machine learning;gaussian mixture models;human action;recognition tasks;real-valued
digital images;image segmentation;genetic algorithm;image datasets;genetic algorithm;28;segmentation results;learning problems;learning parameters;classifier;weak classifiers;semantic segmentation
service provider;complex structure;connected component;share information;collect data;service providers
image patches;classification;stochastic gradient descent;fuzzy clustering;neural network;online learning;membership functions;learning scheme;classifier;visual learning;image recognition
parameter space;meta-learning;multi-agent system;data-mining;genetic algorithm;meta-learning;parameter-space;search algorithms;learning method
data points;independent variables;regression models;multiple linear regression;regression model;dependent variable;data repository;artificial neural network
data set;simple models;feature extraction;raw data;data set;complex data;computational burden;pre-processing
association rules;textual documents;data analysis;classification
clustering;classification;classification;similarity measure;time series;time series;classifier;similarity measures
clustering;survival analysis;data partitioning;machine learning;instance;clustering techniques;feature relevance;classification problems;separability
neural networks;probability distributions;jensen-shannon divergence;jensen-shannon divergence;information-theoretic measure;upper bound;classification problems;simulation results;neural networks
predictive performance;markov random field;graphical models;classifier
data set;hybrid method;synthetic datasets;similarity measure;real-world;hybrid method;statistical tests;real world;cluster analysis;benchmark datasets
domain knowledge;network models;neural networks;vision applications;classification accuracy;face recognition;convolutional neural;classification;competitive performance;human faces;support vector machines;human face;neural network
support vector machines;classification techniques;neural networks;stochastic gradient descent;incremental learning;training samples;convolutional neural;multi-robot;multi-robot;visual learning
discriminant analysis;lower dimensional;classification accuracy;small sample sizes;data set;multi-scale;recognition tasks;principal component analysis;multidimensional data
virtual world;face recognition;matching algorithm;database;high degree of;eye detection;rates;virtual communities;virtual-world;virtual worlds
feature extraction method;database;template matching;sparse representation;facial expression recognition;facial features;facial expressions;human visual system;facial expression recognition;classifier
face recognition;pattern recognition;canonical correlation analysis;face recognition;sparse representation;machine learning;classification performance;sparse representation;canonical correlation analysis;discriminative information;margin based;sparse reconstruction
regularization;physics-based;image data;physics-based;image sequence;produce accurate;image data
real datasets;selected features;associative classifier;association rules;traditional classifiers;classifier
false positive;web services;meta data;logistic regression;real-world data sets;classifier
dynamic programming;planning problem;boosting algorithm;partial observability
classification scheme;web pages;information sources;genre classification;web pages;multi-label;multi-label;web page;centroid-based
clustering;social tags;classification;social tags;social tags;hierarchical clustering algorithm;semantic information
bayesian information criterion;mixture model;real-world;time series;time series;increasing attention;mixture models;model selection
clustering;temporal dependencies;temporal features;unsupervised learning;supervised learning methods;semi-supervised learning;feature extraction;time series;machine learning;traditional clustering algorithms;clustering algorithm;recurrent neural network
graph theory;graph theory
evolutionary algorithms;auxiliary;multi-objective;reinforcement learning;reinforcement learning;evolutionary computation;fitness;evolutionary algorithm
ensemble learning;ensemble learning;training data;large-scale;real dataset;data center;prediction task
case study;class values;binary class;real-world;data set;case study;ensemble classification;wide range;multi-class;instances
real world data mining applications;instances;misclassification costs;image data sets;vector machine;misclassification costs;instance;vector spaces;prediction performance;imbalanced data sets;skewed
base classifiers;decision trees;linear models;imbalanced data;imbalanced data;classifier ensembles;imbalanced datasets;model trees;ensemble method;classification problems
virtual world;face recognition;classification;face recognition;object recognition;statistical analysis;input images;natural images;object classification
human users;world wide web;facial images;human performance;learning algorithms;recognition task;visual features;artificial intelligence;visual features
predictive models;high accuracy;face recognition;face recognition;highly accurate;irrelevant features;ensemble feature selection;ensemble feature selection;feature selection methods;human faces;learning performance
classification learning;face recognition;learning algorithm;classification;feature extraction;data preprocessing;learning vector quantization;classification algorithm
virtual world;problem remains;face recognition;biological entities;image datasets;window based;texture features;operator;binary pattern
gene) selection;aggregation techniques;large-scale;feature ranking;ensemble selection;classification performance;designed specifically for;data sets;bioinformatics datasets;classification results;feature selection;data mining;rank aggregation;gene selection;gene selection
clinical data;svm-based;multi-stage;vector machine;clinical practice;svm) based;wide range;machine learning;breast cancer
feature ranking;computational power;microarray data;microarray datasets;ensemble methods;computational costs;classification performance;ensemble feature selection;classification results;feature selection;feature subset;feature) selection;feature selection techniques;gene selection
high rate;open source;multi-objective;multi-objective;traffic light;increasing number of;reinforcement learning;rates;traffic light;traffic light
classification;past experience;support-vector-machines;semantically meaningful;large sample;classifier;detect anomalies
fuzzy inference system;classification;sliding window;sliding window;recognition tasks;fuzzy rules
nonnegative matrix factorization;entropy-based;face recognition;recognition rate;similarity measure;recognition performance;data set;face recognition;data matrix;principal component analysis;objective function
theoretical foundation;learning algorithm;reinforcement learning;batch learning;reinforcement learning;operator;batch learning;decision making
high-dimensional;microarray data;classification;variable selection;microarray data;dimensional data;gene expression
real data sets;pattern recognition;large-scale;class distributions;learning algorithms;vector machine;machine learning;data sets;wide range;data mining;sampling algorithms;classifier
binary classification;binary classification;classification models;classification;single class;machine learning;build models;binary classifiers;binary classifiers
regression tasks;regression models;instances;imbalanced datasets;training instances;prediction tasks;prediction problems
training set;multi-class;target distribution;classification;sampling techniques;imbalanced data;classification algorithms;data set;sampling approach;sampling process;imbalanced data;sampling framework;multi-class;classifier;classification algorithm
image segmentation;loss function;segmentation techniques;level sets;level set function;magnetic resonance images;level set;cross-validation;energy functional;level set
feature selection method;statistical test;vector machine;selected features;data set;feature subsets;learning machines;statistical tests
clustering;clustering algorithms;data warehouses;dynamic nature of;large datasets;clustering algorithm;dimensional data;histogram-based
regulatory networks;classification models;gene expression;feature extraction;data set;data sets;predictive performance;feature selection;gene expression;models trained on
clustering;bayesian model;data analysis;synthetic data;gene expression;network analysis;real-world;gene expression;protein interaction networks
fixed point
search space;decision trees;decision tree;simulation studies;training data;selection criterion;decision trees;detection performance;gene interactions;input variables
ensemble learning;prediction accuracy;data mining and knowledge discovery;sampling techniques;learning algorithms;sampling method;3;adaptive sampling;data sets;real world;data repository;benchmark data sets;learning method
medical records;data arrives;multi-dimensional;medical data;taking into account;data mining methods;data structures;medical data;data set;tensor-based;behavior analysis;medical knowledge;tensor analysis
simulated annealing;protein structure;parallel algorithm;protein structures;local minimum;simulated annealing;statistical method;protein structures
correlation-based;input data;classification;classification tasks;typically requires;performance metric;correlation-based;correlation coefficients;primary goal;classifier;training algorithm
high-dimensional;high-dimensional;simulation studies;imbalanced data;accurately predict;classification methods;class membership;high-dimensionality;class prediction;class prediction
query results;data repository;secondary structure;database;query language;relational database systems;query language;data model;designed to support;protein structures;relational database;user-friendly;query results;relational dbms;persistent storage
lower bound;monte carlo;data describing;moving-average;monte carlo simulation;finding clusters;upper bound
classification;classification;probability distributions;margin;wavelet analysis;probability distribution;segmentation accuracy;margin
impact analysis;gene expression;impact analysis
original matrix;classification;data visualization;learning vector quantization;classifier;learning vector quantization;kernel-based
bayesian model;search space;optimal value function;prior information;partially observable markov decision process;reinforcement learning;reinforcement learning;prior distribution;monte carlo tree search;learning problem;principled framework;monte-carlo tree search
markov decision processes;pomdp model;inverse reinforcement learning;reinforcement learning algorithm;partially observable markov decision processes;margin;partially observable domains
evolutionary algorithms;evolutionary algorithms;auxiliary;fitness;reinforcement learning;optimization problem;reinforcement learning;fitness;auxiliary
domain knowledge;training data;specific features;kernel-based;specific properties;specific properties;radial basis function;tensor-product;real-life;training data set;support vector;correlation patterns;support vector machines;prior-knowledge;biological data
united states;statistical significance;supervised learning techniques;predictive power
face recognition;pattern recognition;face recognition;correlation filters;correlation filters;correlation filters;principal component analysis;binary pattern;image processing;classifier
block level;block level;feature vector;discrete cosine transform
neural networks;classification;cross validation;data clustering;fuzzy logic;input data;mathematical model;neural networks;success rate;fuzzy rules;neural network;inference rules
genetic algorithms;neural networks;genetic algorithm;prediction error;input variables;absolute error;neural network;limited number of
greedy strategy;classification problem;neural networks;classifier;tree structure;binary classifiers;binary classifier;classification problem;binary classifiers;multi-class;multi-class;neural network;biological data;bioinformatics research
order statistics;statistical measures;feature selection techniques;classification;statistics based;microarray datasets;dimensionality reduction techniques;irrelevant features;bioinformatics datasets;feature subsets;feature selection;classification results;high dimensionality;feature selection technique;feature selection
markov random field;monte carlo;microarray data;sampling-based;breast cancer;sampling-based;network structure;posterior distribution;bayesian framework;markov chain;gene expression;network topology;protein-protein interaction network;synthetic data
face images;success rate;image processing;decision support system;decision support;accurate classification;evaluation scheme;image data;facial features;principal component analysis;machine learning algorithms;expert knowledge
classification performance;structure learning;structure learning;machine learning;dimensionality reduction techniques;image classification
gradient-descent;function-approximation;reinforcement learning;resource management;action space;single-agent;network size
update rule;label information;classification;recognition performance;matrix factorization;clustering performance;geometric structure;document clustering;document datasets;clustering methods;semi-supervised;semantic space;gradient method;semi-supervised;latent semantic indexing
regression algorithm;fourier transform
gradient-based;image segmentation;image features;image analysis;region-based;level set;magnetic resonance;mr images;level set
structured light;structured-light;stereo matching;matching algorithms;stereo matching
regression method;latent variables;quantitative analysis;principal component;limited number of;high dimension;regression methods;quantitative analysis;objective function
pattern recognition;target task;knowledge transfer;classifier;change detection;incremental learning;multi-task learning;metric learning;knowledge transfer;source tasks;multi-task learning;neural network;model called;metric learning
operator;partial differential equations;local minima;partial differential equations
digital libraries;collaborative-filtering;real-world
sampling methods;state space;state spaces;monte carlo;sampling-based;state space;learning tasks;sampling method;sample set;decision process;learning problem;belief state;continuous domains;decision processes;decision making;decision processes
face images;appearance model;facial images;classification;classification;principle component analysis;vector machine;age estimation;facial images;svm) classifier;basis function
gene) selection;precision-recall;feature selection techniques;selection techniques;microarray datasets;models built;desired level of;classification performance;instances;original data;bioinformatics datasets;randomly-generated;high dimensionality;real world;gene selection
cross-validation;privacy concerns;local models;decision support system;decision support;large number of;decision-making;privacy preserving;accuracies;databases;multiple sources;data repository;prediction models;model building
feature selection techniques;models built;feature selection method;multiple datasets;ensemble feature selection;classification performance;ensemble feature selection;feature subsets;feature selection technique;feature subset;gene selection;sampled data;gene selection
intrusion detection;anomaly detection;anomaly-based;detecting anomalies;anomaly detection;ad hoc
unsupervised learning;data types;rule learning;machine learning;healthcare data;data types;healthcare data;semantic information
classification;classification;mri data;learning algorithms;magnetic resonance images;machine learning;data collected from;pair wise;magnetic resonance imaging;alzheimer's disease;key features;alzheimer's disease
real data sets;classification techniques;training phase;classification accuracy;optimization algorithm;cross-validation;large datasets;support vector machines;classification tasks;classification algorithm
classification;classification;decision tree;training process;pruning strategies;time series;algorithm called;time series;time series
1, 2;data samples;input features;misclassification costs;vector machine;classification methods;cost-sensitive;machine learning;cost sensitive;data sets;2-10;dimensional data
learning mechanism;instance-based;classification;target domain;learning approaches;target object;feature-based;training samples;object classification;manual labeling;learning mechanism;machine learning;transfer learning;strong classifier;object classification
game theoretic;stochastic games;reinforcement learning;multiagent systems;multiagent systems;game theory
predictive accuracy;training classifiers;time series;local patterns;high resolution;high-resolution
hybrid approach;feature selection techniques;irrelevant features;microarray datasets;aggregation function;ensemble feature selection;bioinformatics datasets;feature subsets;feature selection;high dimensionality;gene selection;gene selection
structured output;free text;topic classification;clinical data;classification;classification;decision-making;topic modeling;topic modeling;topic modeling;natural language processing;text classification;machine learning techniques;classification results;dimensionality reduction
spam detection;spam detection;spam pages;web spam;web spam;ranking algorithms;search engines;web page;classifier;detection rate
web based;web based;large number of;biometric recognition;artificial neural networks
social media;classification models;classification;classification;classification accuracy;context based;user profiling;social ties;temporal features
semi-structured;indoor environment;bayesian classifier;extracting features from
neural network;real-world;neural networks;neural networks
digital libraries;classification techniques;classification method;classification;real data
classification models;unlabeled data;active learning;active learning;active learning framework;active learning;training process;scene classification;selection methods;learning scheme;scene classification;support vector machines
high-dimension;optimization algorithm;high quality;search space;high dimensional
decision trees;machine learning methods;classification;web sites;web spam;multilayer perceptron;search engines;machine learning techniques;neural networks
image based;sensitive information
existing database
data points;multivariate data;classification;class label;training dataset;multivariate data;classification;interactive visual;classifier
singular value decomposition;computational complexity;data analysis;np-complete problem;human-centered;correlation coefficient;case studies;similarity-based;visualization techniques;multidimensional data
multi-dimensional;data analysis;visual analytics;case study;dimensionality reduction;trend analysis
low-resolution;image patches;dictionary learning;image space;scaling factors;image super-resolution;image super-resolution;test image;learning scheme;sparse codes;high-resolution;training error
highly skewed;state detection;decision tree;real world datasets;class distribution;class distributions;machine learning techniques;instances;imbalanced datasets;total number of;logistic regression;condition monitoring;machine learning algorithms
nearest-neighbors;missing values;data streams;low-rank;expectation maximization;low-rank;missing data;data streams;machine learning algorithms;missing values
data analysis;machine learning methods;stock market;time series;regression model;stock market;temporal smoothness
window queries;sliding window;sliding-window;algorithm requires;uncertain data streams;query result;uncertain data streams;space complexity
high-dimensional;simulation data;numerical simulation;product development;hierarchical approach;visual representations;higher-dimensional;grid-based;design space;interactive visualization;design decisions;simulation results
clustering;clustering;estimation method;object localization;vision based;high accuracy;cluster analysis
ensemble learning;hybrid approach;feature selection techniques;classification;classification performance;feature selection;high dimensionality;high dimensionality;high dimensionality
markov decision processes;active learning;formal model for;learning procedure;manual construction of;markov decision processes;input/output;markov decision process;automatically learning;formal model
software development;machine learning models;multiple linear regression;gradient boosting;regression models
object recognition;data analysis;hierarchical structures;structured data;data elements;fixed size;controlled experiments;visualization techniques;user study
clustering;multiple features;feature mapping;higher classification accuracy;genre classification;low-level features;main components;level fusion;membership functions;audio features;low-level;classifier
magnetic resonance;mr) images;manifold learning;image segmentation;selection methods
software development;ranking techniques;feature) selection;feature selection techniques;classification;class attribute;imbalanced data;feature ranking;training process;imbalanced datasets;feature selection;feature subset;classification problems;software quality
approximation techniques;low rank matrix approximation;relevant features;low rank;selected features;high-dimensional datasets;feature selection algorithm;noise-reduction;feature selection;noise reduction;feature selection methods;classification accuracy;data characteristics;increasing attention;learning performance;feature selection
learning process;multi-agent system;knowledge sharing;machine learning;concept learning;social networks
software development;feature selection techniques;classification;class attribute;feature subset;similar results;feature selection algorithms;instances;original data;selection techniques;feature selection technique;specifically designed to;feature selection methods;software quality;features selected
cross validation;data sets;machine learning methodology;machine learning methodology;machine learning
software development;risk management;existing knowledge;higher level
partial matching;classification;classification;problem domain
auc;classification model;feature selection techniques;decision tree;feature selection technique;features extracted;naive bayes;classification performance;feature selection;feature selection;logistic regression;condition monitoring;operating conditions;reliability analysis;information gain;machine learning algorithms;features selected;nearest neighbors;state detection
reinforcement learning algorithms;dynamic programming;large scale;real-time dynamic programming;reinforcement learning algorithms;markov decision processes
gradient-based;gradient-based;historical data;dynamical systems;search procedure;population-based;local search;real world
svm algorithm;randomized data;svm training;optimization problem;larger datasets;machine learning;support vector machines;data organization;training algorithm
regularization;robust face recognition;face recognition
synthetic data sets;data points;mixture model;learning framework;image categorization;dirichlet processes;real-world;dirichlet process mixture;nonparametric bayesian;online algorithms;clustering approach
labeled samples;target domain;facial expression recognition;cross-domain;facial expression recognition;facial expressions;matching process;discriminative information;image acquisition
clustering;social media;automatically identifying;social media;mobile devices;social networks;high potential;geo-referenced;clustering approach;decision making;natural language
pre-defined;matching scheme;kernel-based;classification;large scale;high resolution images;machine learning techniques;intra-class;scene classification;geo-referenced;image classification;giving rise to
computational biology;jensen-shannon divergence;jensen-shannon divergence;original formulation;ensemble approach;learning framework
labeled data;neural networks;error analysis;neural network
gradient-based;training set;training data;maximum likelihood estimation;optimization algorithm;hidden markov models;hidden markov model;classification error;joint probability;maximum likelihood estimation;error rate;learning method
real data sets;recommender systems;personalized recommendation;probabilistic model;recommendation accuracy;personalized recommendation;semantic web;resource selection;personal preferences
hybrid model;applications including;loss function;phase space;temporal patterns;baseline methods;hidden markov model;state estimation;phase space;classifier
clustering;high-dimensional;similar objects;conventional methods;unsupervised learning;input data;nearest;discriminative power;test sample;clustering methods;hierarchical clustering;reconstruction error;image collections;globally optimal solution;clustering problems;dimensional data;model fitting
clustering;clustering;common features;semantically-related;false positive;template matching;closely related;pair wise;input-output;large collections of;large datasets;locality-sensitive hashing;malware detection
clustering;markov chain;low cost;similar behavior;correlation based;clustering method;markov chains;transition probabilities;resource management;markov chain;correlation based;lower-cost;data storage
sample size;absolute error;minimal optimization;classification;classification;fixed-length;naive bayes;pair wise;logistic regression;svm classifiers;nearest neighbor;support vector machines;classifier
intelligent agents;social network;general-purpose;human experts;social networks;semantic space
classification approach;classification;sentiment analysis;emotion recognition;class hierarchy;classification approach;classification task
growing number of;social media;predictive models;internet users;machine learning;information embedded in;rates;machine learning algorithms;social media sites;linguistic analysis;social media sites
growing number of;ensemble learning;auc;feature selection techniques;classification;feature selection;data mining;high dimensionality
greedy search;search space;pattern language;interesting patterns;labeled data;numeric attributes;higher-quality;quality measure;relative accuracy;optimal set of
fine-grained;privacy preserving;weighted graph;graph data
nonnegative matrix factorization;nonnegative matrix factorization;optimization algorithm;data clustering;gaussian noise;data sets
document clustering;link based ranking;classification;generalization performance;topic models;topic model;document management;topic modeling;baseline models;topic modeling;topic modeling
real data;document clustering;objective function;data points;dense regions
singular value decomposition;individual nodes;binary matrix;social network;neighborhood information
low-dimensional space;learning algorithms;pattern recognition;manifold learning;learning algorithm;feature extraction;feature mapping;data set;global structure;low-dimensional representation;manifold learning;learning method
map reduce;higher-level;open-source;low-level
software tools;high-throughput;data-mining;raw data
viral marketing;real-world;cascade model;social network;viral marketing;greedy algorithm;model called
clustering;data source;classification;text corpora;test sample;lower bound;text classification;text classification;information overload;margin;model selection
1;domain knowledge;5;metric learning
high utility;interestingness measure;search space;candidate generation;data structure;tight bound;mining process;real data;frequent itemset mining;sparse data;high utility;statistical significance;candidate generation;huge number of;interestingness measures
spectral clustering;regularization;data analysis;data manifold;learning algorithms;1;subspace learning;graph construction;local information;learning task;benchmark data sets;matrix factorization
mining results;data mining results;highly correlated;vector data;network data;binary features;feature vectors;knowledge discovery process;data mining results
batch-learning;bayesian network;dynamic programming;window size;change detection;time series;data set;change detection;real data sets;multivariate data;model selection
clustering;real data sets;prediction model;optimization framework;document classification;input features;coordinate descent;multi-task learning;multi-task learning;hierarchical task
query results;multi-step;feature set;similarity measure;data mining applications;data mining tasks;similarity search;subspace clustering;multi-step;search algorithm;heuristic approach;feature selection
large amounts of;mining framework;control policy;learning process;performs poorly;real-world;feature subspace;reinforcement learning;feature selection algorithm;high-dimensional spaces;autonomous agents;data mining;feature relevance;autonomous agents;optimal control
high-dimensional;informative features;gene expression data;microarray data;margin;machine learning techniques;time series;margin based;fixed-point;gradient descent;feature selection;provide evidence;feature selection methods;real data;feature weights;temporal data;margin-based;objective function;nearest neighbors;data distribution
multi-modal;data points;graph-based;sparse representation;constraint propagation;sparse representation;pair wise constraints;semi-supervised learning;label propagation;graph construction;constraint propagation
classification problem;classification;classification;semantic properties;query types;category information;web users;classifier;latent dirichlet allocation
distance function;trajectory data;shape similarity;segmentation algorithm;density-based clustering algorithm;longest common subsequence;similarity measures
bayesian network;causal relationships;np-complete problem;high reliability;real-world;large databases;observational data;constraint-based
regularization;multi core;recommender systems;low rank;data mining tasks;stochastic gradient descent;matrix factorization;low rank;coordinate descent;upper bound;real world;real world data sets
importance sampling;unlabeled data;hash functions;distance preserving;real datasets;instances;large datasets;sampling strategy;sequential scan;classifier
training set;decision trees;confidence intervals;confidence intervals;instances;decision trees;auc;post-processing;predictive performance;classification process;decision trees;multi-class
generative process;text documents;real datasets;sampling-based;topic models;topic models;real-world entities;topic models;topic model;rich information
outlier detection;outlier detection;data set;feature space;instances
real-world datasets;multi-aspect;sentiment words;online reviews;content words;build models;prediction tasks;individual preferences;automatically learn;small-scale
black-box;recommendation systems;microarray datasets;meta-learning;data-mining process;similarity measures
dual decomposition;logic program;graphical models;dual decomposition;specialized algorithms;representation language;graphical model;decomposition approach;markov logic;tree-structured;markov logic;data-mining tasks
human mobility;mobile user;multi-dimensional;location-based services;prediction accuracy;multiple features;user behavior;predictive power;linear regression;fine grained;model trees;mobile applications;location-based services;learning models;individual features;temporal data;temporal characteristics of
clustering;boosting framework;constrained clustering;clustering performance;cluster ensemble;pair wise constraints;clustering methods;integrates multiple
large volume;completeness;synthetic data;finding an optimal;np-complete;approximation algorithm;service providers;discovery problem;monitoring systems;bipartite graph;completeness;monitoring systems
clustering;subspace clusters;clustering methods;mining temporal;clustering approaches;clustering method;time series;interesting patterns;multivariate data;complex data;real world;measurement errors;subspace clusters;synthetic data
factors affecting;social media;multi-relational;data generation;social media;large scale;multi-relational;inference algorithm;large-scale;face book;inference algorithms for;gibbs sampling;social-media data;multi-threaded;model called
large-scale;graph classification;convergence rate;feature space;learning framework;kernel matrix;data set;multi-resolution;graph classification;subtree patterns;chemical compounds;real-world;feature spaces;large-scale;low-dimensional
computational complexity;classification problem;large scale;real-world;regression models;linear regression;indexing structure;user interests;online advertising;user clicks
data collection;video surveillance;machine learning algorithms;data mining;data pre-processing;data set;data sets;applications involve;geo-referenced;real world;measurement errors;missing values;spatial data mining
clustering;biclustering algorithms;gene expression data sets;microarray data;real-world;projected clustering;gene expression;gene expression data;similarity measures
document corpus;spoken language;explicitly represented;low accuracy;topic models;topic models;statistical model;topic model;topic modeling;text-based;spoken language
text mining;matrix factorization;linear constraints;data representation;data clustering;matrix factorization
temporal dependencies;detection algorithms;graphical models;special characteristics;anomaly detection;anomaly detection;time series;large-scale;complex systems;time-series;detect anomalies
clustering;specifically designed for;machine learning problems;multi-view;multi-view learning;semi-supervised learning;real-world;input features;data sets;classifiers trained on;multi-view learning;candidate set;training samples;max-margin;multiple views;multi-instance learning;multiple kernel learning;classifier
clustering;graph-based;clustering tasks;clustering methods;clustering
viral marketing;performance guarantees;social networks;social networks;path based;greedy algorithm;np-hard
location traces;density-based;anomaly detection;stochastic model;context-aware;real-world;anomaly detection;indoor environment;location traces;movement patterns;detection method;stochastic model;sensor networks;iterative algorithm;location-aware;asset management;transition probabilities;anomaly based;high-level;context-aware;finite state
clustering;statistical approaches;uncertain graphs;uncertain graphs;uncertain graphs;clustering
training set;classification;label noise;land cover;high quality;automated methods;high-cost;classifier
document collection;classification model;general purpose;classification;numeric data;heterogeneous network;textual data;text categorization;naïve bayes;text categorization;text classifier;preprocessing phase;algorithm produces;document collections;heterogeneous network;model generation;high dimensionality
spectral clustering;real-world datasets;multiple subspaces;pair wise;sparse representation;document clustering;document clustering
domain adaptation;labeled samples;low variance;target domain;real-world;source domains;share similar;selecting relevant;classifier
training data;low-rank;target domain;source domain;databases;low-rank;machine learning;knowledge learned;finding an optimal;subspace learning;sufficient training data;knowledge transfer;source domain to;face databases;transfer learning
human behavior;gaussian process;process model;prediction accuracy;social network;social network;human behavior;baseline methods;process model
real data sets;spatial datasets;predictive accuracy;parameter estimation;outlier detection;outlier detection;spatial datasets;linear-order;predictive model for
real-world datasets;influential users;decision-making;propagation model;explicitly modeling;social network;social influence;social networks
map reduce;data mining;machine learning problems;support vector machines;data mining
real data sets;synthetic data sets;social network analysis;objective functions;social network;social networks;greedy algorithm;social networks
algorithm performs well;automatically discovering;machine learning;predictive performance;online video;specifically designed to;audio features
algorithm performs;regularization;uci datasets;unlabeled data;loss function;semi-supervised learning;boosting algorithm;semi-supervised learning;labeled data;boosting algorithms;semi-supervised learning;binary classification problems;semi-supervised;classification problems;margin;labeled and unlabeled data
clustering;clustering;genetic algorithms;multi-objective;genetic algorithm;high quality;trade-offs;optimization problem;cluster level;object level;operator;clustering quality
cost function;biometric recognition;pattern recognition;classifier;error rate
unique features;aggregated data
classification;classification;data mining and machine learning;gradient descent;cost-sensitive;theoretical bounds;online learning;cost-sensitive;classification algorithms;classification tasks;weighted sum of;cost-sensitive classification
spectral clustering;ground truth;loss function;instance;constraint satisfaction;constrained clustering;pairwise constraints;pair wise;real-world applications;instances;pair wise constraints;label propagation;label propagation;spectral clustering algorithms;style algorithms
factor matrices;data analysis tasks;synthetic data sets;real-world;binary matrix;matrix factorization;data base;real-world data sets
database;multiple views;outlier ranking;outlier mining;high quality;outlier ranking;multiple views;subspace analysis;subspace analysis;multiple subspaces;attribute sets
information diffusion;accurately predict;information diffusion;real-world scenarios;statistical model
clustering;clustering algorithms;life sciences;replication;collaborative filtering;collaborative filtering;prediction accuracy;collaborative filtering algorithm;multi-core;massive datasets;rates;high accuracy;massive amounts of data;theoretical analysis;load balancing;data analytics;high throughput
frequent episodes;episode mining;approximation guarantees;event streams;event sequences;data mining problem
prediction accuracy;collaborative filtering;collaborative filtering;item recommendation;opinion mining;case study;baseline methods;accurate predictions;opinion mining;multiple aspects
high quality;diffusion model;basic assumption;common interests;real-world data sets;supervised learning;online communities
mining results;process model;event logs;completeness;business process management;process mining;control-flow;business processes;data mining;completeness
multiple communities;ground-truth;graph model;community detection;detection method;large scale;real-world networks;implicit assumption;detection methods;information networks;overlapping communities;community structure
clustering algorithms;algorithm performs;input data;real-world datasets;real-world;data clustering;clustering approach;similarity matrix;uncertain data;clustering algorithm;ensemble clustering;ensemble clustering
synthetic data;collaborative filtering;minimization problem;social network;prediction errors;low-rank;learning problem;link prediction;real world applications
regularization term;regularization;informative features;loss functions;learning algorithms;computational complexity;memory space;predictive model;online learning;supervised learning;learning framework
computational efficiency;support vector machines;classification;closed-form solutions;competing methods;classification framework;clustering performance;multi-class;desirable property;classification;real-world data sets;multi-class;local minima;kernel-based
prediction accuracy;real-world;spatiotemporal data;data collected from;long-term;los angeles;real-world;time-series;short-term
data representation;latent) structure;classification;instance;smoothing method;parameter estimation;higher order;naive bayes;naïve bayes;higher order;bayesian framework;instances;higher-order;labeled data;graph based;smoothing method;text classification;latent semantic indexing;higher order;benchmark datasets
data-driven;singular value decomposition;sample size;singular vectors;linear dimensionality reduction;high-dimension;low-rank;hamming space;maximum variance;data matrix;linear dimensionality reduction;error bounds for;laplacian matrix;principal component analysis;dimensional data;uniform sampling;low-dimensional
estimation method;large-scale;multiple features;large number of;social network;social network;high precision and recall;learning method
target language;training set;cross-language;labeled data;source language;training datasets;training algorithm;opinion mining;conditional random fields;cross-language
relevance vector machine;relevance vector machine;data mining tasks;kernel parameters;model training;data distribution
mobile users;recommender systems;context-aware;real-world;data set;context dependent;context-aware;mobile users;application scenarios
clustering;clustering algorithms;data set;instances;data sets;prefix tree;real-world data sets;real world;pattern based clustering
feature space;user profile;component analysis;database;dimension reduction;feature subspace;heterogeneous data sources;social network;prior knowledge;vector-based;instance;data sources;dimensionality reduction;eigenvalue problem;term-based;databases;feature spaces;log data;learning performance;dimensionality reduction
stochastic processes;data streams;time series;streaming data;real data;gaussian processes;novelty detection;sampling process;identify patterns
large-scale;stochastic gradient descent;competing methods;map reduce;compute nodes;distributed algorithms;memory consumption
classification;accurately identify;clinical information;group structure;adaptive algorithm;breast cancer;simulation results;feature weights;array data
approximation algorithms;underlying structure;inference problem;temporal information;np-complete;cascade model;source nodes;optimization problems;decision problems;social networks;performance guarantees;inference algorithms;synthetic data
multi-view clustering;kernel-based;multi-view;multiple representations;clustering accuracy;clustering process;instances;iterative algorithms;clustering framework;kernel matrices
mutual information;dynamic bayesian networks;large scale;markov blanket;global optimization;minimum description length;bayesian networks;systems biology;mutual information;np-hard
probability estimates;probability estimates;imbalanced data;classification systems;instances;bayesian approach;imbalanced data;supervised learning
clustering;clustering algorithms;parallel implementation;network size;large-scale;efficient clustering;clustering result;power law;kernel k-means;large-scale social networks;higher quality;clustering algorithm;degree distribution;large-scale social networks;social networks
ensemble learning;computationally hard;prediction performance;generalization performance;computational costs;ensemble pruning;pruning algorithm;base learners;ensemble pruning;memory consumption;combinatorial optimization problem
kernel learning;image retrieval;multiple instance;instance;learning tasks;semi-supervised learning;application called;text categorization;input-output;text-based image retrieval;machine learning applications;coordinate descent;unlabeled documents;input-output;data mining;margin;benchmark datasets;multiple kernel learning;learning framework;classifier
data point;von mises;data points;principle component analysis;gaussian distribution;distributed data;efficient learning
parameter-free;ground-truth;real-world;clustering methods;detection method;ground-truth;real-world networks;social networks;million nodes;clustering algorithm;information networks
network analysis;multi-relational;supervised approach;unsupervised learning;classification;prediction performance;heterogeneous networks;multi-relational;temporal information;heterogeneous networks;social networks;influence propagation;multiple types of;link prediction;real world
large-scale;variable selection;learning procedure;quadratic programming;linear svms;linear svms;training process;standard svm;support vector machines;benchmark data sets;sparse linear;training algorithm
3 abstracts
topic-aware;high accuracy;real-world;topic-aware;explicitly modeling;topic modeling;influence propagation;social influence
matrix factorization;recommender systems;recommender systems;large-scale;optimization approach;coordinate descent;large-scale datasets;stochastic gradient descent;coordinate descent;web-scale;matrix factorization;coordinate descent;update rule;distributed systems;times faster than;missing values;multi-core;matrix factorization
clustering;clustering;clustering results;distance measure;time series;raw data;time series;local patterns;data mining;time series;diverse domains;real world
classification performance;training set;training data;unlabeled data;labeled instances;labeled training data;machine learning;classifiers trained on;instances;data mining problems;training algorithm
clustering;spectral clustering;multiple kernel learning;classification;clustering problem;multiple views;interior point methods;optimization problem;data sources;optimal kernel;weight vector;multiple kernel learning;objective function;multiple kernel learning
social network data;real life;social networks;social networks
online reviews;supervised methods;domain independent;sentiment classification
clustering;range queries over;synthetic datasets;private data;real-life datasets;differentially private;differentially private;real-life;compressed data
minimum description length;propagation model;high accuracy;large graph
evolving data;unlabeled data;labeled data is;mixture model;hierarchical dirichlet process;semi-supervised learning;generative models;unlabeled samples;real-world applications;data set;gaussian mixture models;semi-supervised learning
classification techniques;instances;concept-evolution;computational resources;data stream;prediction error;ensemble technique;mining data streams;stream classification
margin;selection method;data sets;feature selection;margins;feature weighting;margin;select features;selection algorithm
feature space;numerical experiments;rough set;rough set;classification accuracy;error-correcting output codes;feature selection;binary classifiers;problem domain;uci datasets;multi-class classification problems
clustering;multi-view;information contained in;instances;clustering methods;multi-view;single-view
regularization;multi-task learning;input data;prediction methods;generalization performance;classification tasks;database;large number of;classification performance;multi-task learning;databases;protein sequence;biological databases;protein sequences;classification approach;source databases;base-line
network analysis;clustering;information network analysis;information network;privacy-preserving;similarity measure;link-based;privacy-preserving;link prediction;data privacy;information network
feature space;training data;test set;test data;training data is;component analysis;machine learning;predictive model;probability distribution;data sets;closed-form solution;dimensionality reduction method;structural information
classification;classification;multi-label learning;multi-view;multiple view;feature mapping;maximum margin;classification framework;multi-label;maximum margin;combining multiple;classifier
detection algorithms;evaluation criterion;kernel density;data points;small sample;outlier detection;real datasets;graphics processing;feature selection algorithm;outlier detection;imbalanced datasets;feature selection;feature selection
high-dimensional;feature space;very large datasets;regularizer;generalization performance;learning algorithms;large-scale;complex objects;model parameters;structural svms;structural svms;support vector machines;benchmark datasets
case studies;computational framework;predictive modeling;finding optimal
monte carlo;frequency distribution;sampling method;uniform sampling;large networks;graph analysis;computationally expensive;markov chain
classification problem;multilabel classification;loss function;classification;large number of;tree-structured;instance;multiple labels;greedy algorithm;real-world data sets;multilabel classification;classifier;decision theory
data point;learning process;classification;feature extraction;data points;feature vector;manifold learning;learning algorithm;discriminative information;feature extraction;benchmark data sets;feature vectors;class information;data mining;graph construction;semi-supervised;dimension reduction;semi-supervised
learning process;active learner;active learning;probabilistic model;social tagging;high classification accuracy
matrix factorization;overlapping clusters;face book;private data
end-users;user ratings;recommendation systems;synthetic data;matrix factorization;multi-source;local information;multiple information sources;multiple sources;matrix factorization
active learning;generalization error;active learning;graph-based;optimization algorithm;active learning;active learning methods;global consistency;active learning approach;benchmark datasets;learning method
relevance vector machine;multi-kernel;mixture model;clustering problem;em) algorithm;mixture models;time series;mixture %model;incremental learning;multi-kernel;expectation maximization;em algorithm;benchmark datasets;kernel parameters;model parameters
statistical methods;human behavior;online social networks;local models;user behavior;social networks;model called;social networks
algorithm finds;pattern-based;dynamic programming algorithm;classification;classification;time series;time series;large data sets;discovery algorithm;time series;interpretability
optimization framework;data points;inverse problem;data set;optimization techniques;traffic data
approximate inference;temporal data;prediction model;latent variables;high dimensional;large scale;spatio-temporal;prediction quality;statistical model;stochastic processes;linear order;gaussian distribution;real-life data sets;spatio-temporal;expectation propagation;spatio-temporal
nonnegative matrix factorization;domain adaptation;text classification tasks;optimization procedure;cross-lingual;labeling process;text classification;cross-lingual;text classification problems;learning method
association rules;market-basket data;data cubes;data cubes;association rule
detection algorithm;outlier) detection;anomaly detection;high-dimensional datasets;data mining
social media;image features;information sources;features extracted from;image content;machine learning;weakly supervised;image understanding;social networks;multimedia data;information source
text mining;web scale;precision-recall;extraction patterns;automatically extracted;knowledge bases;knowledge base;confidence values;information extraction;confidence scores;logic-based;markov logic;probabilistic inference;markov logic;ad hoc;knowledge base;labeled data
memory usage;large networks;social networks;social network
clustering;clustering algorithms;computational complexity;data points;classification;low-dimensional space;large scale;singular vectors;data matrix;large data sets;clustering performance;distance based;clustering algorithm;clustering;real world data sets
world wide web;complete set of;applications including;higher precision;detecting anomalies;real life datasets;real life;social networks;iterative algorithm
link formation;social network analysis;heterogeneous networks;link prediction;link prediction;predictive performance;structure information;social networks;fundamental problem;link prediction;graph model;network evolution;social networks
unlabeled data;multiple tasks;multi-task;classification;high dimensional;semantic features;learning algorithms;text data;multi-task learning;dyadic data;iterative algorithm;multi-task learning;latent semantic;semi-supervised;nonnegative matrix tri-factorization;semi-supervised;unsupervised learning;multi-task
real-world datasets;step forward;classification;classification;data mining techniques;sensitive attributes;decision theory;classifier ensembles;sensitive attribute;classifier;decision theory;classification algorithm
high-dimensional;multi-modal;numerical experiments;similar objects;multiple representations;hidden nodes;multiple views;deep learning;multiple representations;image datasets;hamming space;instance;multi-view;single view;low-dimensional
clustering;real data sets;dimensionality reduction;data visualization;data points;reconstruction error;subspace clustering;low dimensional;linear subspaces;dimensional data;semi-supervised classification;subspace clustering algorithms
regularization term;global optimal solution;graph embedding;regression analysis;dimension reduction;classification tasks;linear discriminant analysis;multi-class;convex formulation;loss functions
higher precision;singular value decomposition;matrix decompositions;adjacency matrix;low-rank
feature space;training data;classification;classification;dictionary learning;sparse representation;sparse representation;sparse representation;knowledge representation;computationally expensive;learning framework
gradient-based;magnetic resonance;numerical experiments;optimization approach;factor matrices;data set;matrix factorization;data sets;simulated data;optimization algorithm;matrix factorization
base classifiers;minimal optimization;naïve bayes;vector machine;cost-sensitive;rates;cost-sensitive;data mining;data mining algorithms;high sensitivity;nearest neighbors;fold cross validation
clustering;training set;classification techniques;accuracies;training sets;classification rules;data mining approach;data mining approach;classification algorithms
classification;image features;structural properties;shape descriptors;edge information;spatial distribution;retrieval performance;retrieval task;feature descriptor
statistical significance;confidence intervals;model called;confidence intervals;adjacency matrix
sentiment classification;automatic extraction of;natural language text;sentiment analysis;domain dependent;sentiment classification
general purpose;sentiment analysis;domain-specific;sentiment analysis;domain-specific;domain independent
learning process;sequence labeling;discriminative learning;fine-grained;user-generated;product feature;opinion mining;product features;feature extraction;knowledge-based;counterpart;fine-grained;context-based;product features;information extraction techniques;opinion mining;conditional random fields;popular items;similarity measures
sentiment analysis;sentiment classification;sentiment analysis
domain knowledge;opinion mining;knowledge representation;semantic information;opinion mining;domain models
classification;low accuracy;classification;variable selection;naive bayes;facial expressions;selection methods;facial expressions;support vector machines;data mining algorithms;neural networks
simulation data;biological processes;gene expression analysis;prior knowledge;coordinate descent;training samples;feature selection;gene interactions;supervised learning algorithms;gene expression;objective function;feature selection
high-dimensional;medical data;pattern recognition;case study;instances;classification accuracy;feature ranking;ensemble selection;case study;feature selection;feature subset;ranking methods;biomedical data;feature ranking
transfer learning;models trained on;transfer learning;data model;vector machine
clustering;clustering;dna sequences;post-processing;distance measures;human genome
regularization;recommender systems;classification;matrix factorization;sentiment analysis;multiple sources
features extracted from;classifier trained on;fuzzy clustering;common sense;emotion recognition;semi-supervised;opinion mining;semi-supervised
instance selection;domain adaptation;training set size;cross-domain;classification;sentiment analysis;target domain;domain adaptation;cross-domain;instances;document-level;training set;semi-supervised;source domain
regression problems;classification;supervision;large number of;supervision;fine-grained;corpus-based;corpus-based;main idea
classification;partial matching;classification;data compression;data compression;user generated content;classification performance;sentiment analysis;knowledge based;compression based;machine learning techniques;support vector machines
online social network;online social networks;social behavior;social networks
data set;graph-based;spectral clustering;los angeles;community detection
visualization technique;medical data;temporal data;visualization techniques
nearest neighbor;evidence theory;classification accuracy;vector machine;linear discriminant analysis;classifier;evidence theory;higher classification accuracy;neural network
data-driven;mining framework;large-scale;subgraph mining;human brain;graph patterns;graph mining;alzheimer's disease;spatial patterns;alzheimer's disease
protein-protein interaction network;posterior probability
auc;data mining methods;database;independent variables;medical data;real data;predictive model for;feature selection methods;decision making
clustering;real-life;exploratory data analysis;parameter free
sample data;build models;highly correlated
benchmark data;density estimation;nearest neighbor;classification;classification;cost-sensitive learning;time series;performance goals;wide range;numerical features;instance-based learning;time-series;nearest neighbor;skewed;class distribution;posterior probability
real-world datasets;concept hierarchies;concept hierarchies;information sources;automatically extracting;spatial information;temporal patterns;interesting patterns;semi structured;knowledge base;data sources;specification language;complex patterns;mining patterns;temporal data;temporal patterns;knowledge bases
search algorithms;interactive exploration;data exploration;high quality;range query;time series;larger datasets;data structures;similarity search;data analysis;evaluation strategy;false dismissal;result set
event types;spatio-temporal;pattern mining;real-life;apriori-based;data sets;mining algorithm;spatio-temporal
clustering;search space;large scale;communication costs;clustering method;social networks;strongly correlated;social graph;bipartite graph;social networks
concept-based;worst case;formal concept analysis;concept-based
biclustering algorithms;data analysis;expression patterns;gene expression data;synthetic datasets;large number of;data set;high-throughput;clustering methods;gene expression data sets;fast algorithms;gene expression;gene expression data
biclustering algorithms;local search;causal relationships;constraint programming;minimum description length;noisy data;scoring function
classification;data points;misclassification costs;rule-based;cost-sensitive;classifier;accurate classifiers;rule based;expected cost;classification decisions
phase transitions;hybrid approach;phase transition;hierarchical classifier;linear regression;regression model;dynamic systems;temporal data
web documents;supervised learning techniques;spatial data;geographic information;spatial relations;web documents;case study;information embedded in;geographic information;classifier;spatial data mining
data mining;computational cost;case study;spatio-temporal;geo-referenced;large number of;apriori-based;time-series;geo-referenced;time-series;computational savings;result quality
data mining approaches;case study;association rule mining;data types;spatio-temporal;categorical data;association rule mining;association rules;spatio-temporal;large number of
rule based;large number of;user generated;online content
clustering;classification;candidate models;data tables;simulated data;gaussian distribution;continuous data;model selection
decision trees;decision tree;misclassification costs;cost-sensitive learning;cost-effective;data sets;cost-sensitive;decision tree learning;game theory;greedy algorithms;attribute selection;cost-sensitive;information theoretic
unlabeled data;active learning;multi-view;semi-supervised learning methods;multi-view;labeled data;semi-supervised learning;semi-supervised learning;theoretical results;instances
evaluation methodology;real world data mining applications;extreme values;regression tasks;real-life;regression models;target variable;regression model;classification problems
instance;unsupervised clustering;cost-sensitive;classifier;classification performance;instance;cost-sensitive classification;objective function
spatial proximity;interestingness measures;frequent patterns;spatial analysis;association rule mining;pattern mining;pattern based;pattern mining;spatial features;interesting patterns;spatial distribution;local) regions;feature distribution;prediction tasks;interestingness measures;spatial correlation
great significance;online social networks;textual information;valuable information;location information;social networks;location-based services;social networks
web scale;web applications;table extraction;semi-structured;web sites;schema matching;manual labeling;web servers;major components;knowledge extraction;fully automatic;semi-structured;structured data
search space;random walk;large number of;composite items;composite items;composite items
classification;information theory;cost-sensitive;classification results;mutual information between;benchmark data sets
sensor network;algorithm named;algorithm requires;sensor networks;sensor network;cost function
data structure;classification problem;discrimination power;complex networks;instances;edge weight;real world networks
network processing;graph structures;network datasets;collective classification;shortest path;social networks;influence propagation;network structures
anomaly detection;network data;temporal data;network monitoring;data quality
online social networks;monte carlo;metropolis-hastings;rates;small sets of;markov chain;online social networks
web corpus;higher accuracy;human knowledge;recommendation systems;free-text;related algorithms;semi-structured;learning algorithm;making decisions;decision making;graph models;query logs;user study;decision making;natural language
text mining;accurately identify;user generated;web content;data mining;benchmark dataset
data-driven;web-based;international conference on;user-generated;textual content;manually annotated;entity recognition;data mining;conditional random fields
test data;database;named entities;information retrieval;regular expression;string matching
web documents;international conference on;correctly identify;data mining;information retrieval;information retrieval;numerous applications
hybrid approach;manually annotated;conditional random field;textual content;user generated content
knowledge-base;knowledge-base
shortest paths;large graphs;applications including;large-scale;social network analysis;real-world;large-scale;shortest path;landmark-based;shortest path;adjacency matrix;shortest-path;error rate;web retrieval
human mobility;social interactions;large-scale;social network;spatial distribution;rates;social networks;temporal features;network structure;temporal structure;social networks
pattern recognition;anomaly detection;statistical techniques;vector representation;canonical correlation analysis;network structure;graph mining;network data;relational data
globally optimal;large-scale;social network analysis;influential nodes;social network;instances;social networks;information propagation
data mining;data mining
accurately identify;shared information;relevant content;large amounts of data;manual labeling;high-level;information networks
clustering;specifically designed for;meaningful patterns;mining framework;visualization techniques;data mining;mining framework;clustering techniques;subspace clustering;traditional clustering algorithms;databases;dimensional data;evaluation measures;subspace clustering
mobile applications;fourier transform;mobile applications;temporal profiles;temporal profiles
visual analytics;business intelligence;main features;formal concept analysis;complex data;visual analytics
end-user;topological patterns;post processing;detection method;instance;digital library;topological patterns;graph data;topological properties
web-search;web application;budget constraints;online advertising;cost-effective;real world
tree structure;online shopping;latent dirichlet allocation;product descriptions;topic hierarchy
computational complexity;decision trees;data mining;predictive performance;data mining;high dimensionality
data mining methods;hospital information;similarity-based;mining process;stored data;temporal evolution;temporal data
information source;conventional methods;query log;query logs
service quality;search algorithms;end-users;domain-driven;statistics-based;real-life;case-study
detection algorithms;synthetic data;accurately identify;detection algorithms;correctly identify;data collected from;online auction;online auction;real world;agent-based
remote sensing images;probabilistic framework;probabilistic framework;change detection;satellite images;high-resolution
trajectory patterns;spatial-temporal;large scale;spatio-temporal;location based;high resolution;real world;decision making
high-dimensional;predictive models;classification techniques;modeling framework;real-world;data mining;database;scalable distributed;data mining;operator;class labels;frequent pattern mining
data mining;data mining
data mining
united states;data mining algorithms;high resolution images;mining framework;national security
active learning;classification;extraction algorithm;rule extraction;linear models;active learning;rule-extraction;regression models;rule sets;comprehensibility;produce accurate;data mining;complex data;input data;rule extraction;application domains
high risk;data mining
graphical models;large number of;graphical model;tree-structured;tree-structured;tree structured;graphical model
extracting information from;classification model;information services;auc;predictive performance;financial services;logistic regression
mechanical turk;decision-support applications;data mining;data mining;user-centric;user study
privacy preserving data mining;anonymization techniques;data privacy;data anonymization;data mining;data anonymization
mining results;human behavior;large amounts of;data mining;data mining;pattern discovery;frequent pattern
chi square;feature space;document corpus;score based;classification accuracy;document frequency;mutual information;term selection;text classification;data sets;feature selection;feature selection;feature selection methods;text classification;relevant information;high dimensionality;information gain;classifier;feature selection
image segmentation;community detection;prior knowledge;complex systems;social networks;network structure;image data;obtain information
high computational complexity;record linkage;heterogeneous data sources;candidate pairs;privacy preserving;matching accuracy;privacy preserving;integrate data from;record linkage is
social media;data mining methods;huge amounts of data;social media;behavior patterns;time-series;markov chains
score based;dataset characteristics;meta learning;regression models;meta learning;data mining tools;meta model
classification;real world datasets;classification methods;sensitive data;data mining research;real world;decision making
prediction techniques;data mining techniques;data mining;data mining technologies;data mining;privacy-preserving data mining
training set;historical data;classification;classification;classification accuracy;data mining
case study;classification;data mining
classification models;parallel processing;massive data;map-reduce;detection method;accurately detect;error rate;massive data sets;classification algorithms
network analysis;real-world datasets;distributed stream-processing system;counting algorithm;dynamic networks;computationally expensive;web services;fundamental problem;network structure;computing platform
rule learning;mining algorithm;parameter tuning
prediction problem;takes into account;human genome;additional information
intrusion detection;feature selection algorithm;classification model;feature set;intrusion detection;false alarm;detection accuracy;genetic algorithm;data set;feature selection algorithm;effective classification;detection rate;svm classifier;support vector machines;feature selection
clustering;data-flow;component analysis;programming model;large scale;massive data sets;map-reduce;cloud computing environment;execution engine;distributed environments;incremental clustering;data-intensive applications;clustering algorithm;programming model;data mining algorithms;programming paradigm;distributed environment
classification;svm training;classification accuracy;vector machine;vector machine;large data sets;classification;svm classification
join algorithm;nearest neighbor;hash function;high dimensional;large scale;high dimensional datasets;distributed environments;multimedia content;locality sensitive hashing;knn-join;load balancing
mining closed;mining closed;itemset mining;mapreduce-based;frequent item set;real-world data mining applications;large scale;closed frequent;large scale datasets;mining algorithms;memory requirement;computational cost
regularization term;real world;convex optimization problem
human-generated;term weights;summaries generated;multi-document summarization;combinatorial optimization;human generated;pearson correlation;latent semantic analysis;multi-document summarization
case studies;neural networks;prediction method;sample data;prediction errors;neural network;time series;multivariate data;neural network
local optima;linear models
data point;cluster structure;overlapping clustering;clustering process;real data;overlapping clustering;clustering problems;overlapping clustering;clustering model
tuning parameters;feature space;takes into account;linear programming;quadratic programming;time series;machine learning;original data;support vector machines
multiple criteria;classification problem;unlabeled data;linear programming;classifier;data sets;semi-supervised classification;semi-supervised;semi-supervised classification
data point;data structure;semi-supervised setting;large scale datasets;real data sets;takes into account;random walk on;coding scheme;labeled data is;random walks
probabilistic model;bayesian model;learning algorithms;learning tasks;instance
nonnegative matrix factorization;nonnegative matrix factorization;loss function;feature space;data mining and machine learning;high dimension;kernel methods
decision tree;neural network;model building
prediction accuracy;generalization performance;transfer learning;vector machine;machine learning;data set;data sets;regression model;transfer learning;benchmark data sets;learning method;small-scale
privacy-aware;collaborative filtering;privacy aware;privacy concerns;social network;probabilistic matrix factorization;social trust
collaborative filtering;collaborative filtering;collaborative filtering;large number of;online services;privacy-preserving;locality sensitive hashing;similar users
online social networks;community detection;privacy issues;graph data;user study;semi-automatic
graph structure;structural properties;valuable information;social network;service providers;social networks;similarity measures
social media;viral marketing;social networks;large volume;large-scale;finer-grained;social networks;high accuracy;geographic information;personal data
log data;globally optimal;binary data;rule mining;real world;log data
clustering;classification;outlier detection;data mining applications;intrinsic dimensionality;categorical data;similarity search;data complexity;distance measures;data sets;vector spaces;distance metric;cosine similarity;metric spaces
sequential pattern mining;low-level;execution traces;multimedia applications;automatically discover
semantic concept;itemset mining;frequent item sets;frequent item set;item set;high quality;large number of;item sets;large datasets;market basket
public domain;data sets;real life data sets;data sequences;mined patterns;artificial data;sequential patterns;episode mining;data set;data sets;data mining task;real life;data characteristics;episode mining;knowledge discovery process
database;applications ranging from;imbalanced data;stock market;learning algorithms;evaluation measure;change detection;usage patterns;mobile phone;imbalanced data sets;real world;cosine similarity
induction algorithms;knowledge discovery;causal discovery;knowledge discovery;causal model;theoretical result;causal model
training set;data description;data description;support vector;rough set theory;support vector
classification;data analysis;logistic regression;classification;outlier detection;outlier detection;real datasets;predictive modeling;logistic regression;data mining research;knowledge discovery;regression analysis;logistic regression;databases;detection methods
prediction intervals;nearest neighbors;regression models;artificial datasets;model selection;prediction intervals;local similarity;learning framework;model selection;prediction error
long--term;ir research;performance gains
poster session
information retrieval
web search;data mining
semantic annotation;statistical methods;linked data;web crawl;knowledge intensive;mobile search;text search;keyword queries;information retrieval;web scale;information access;information extraction;semantic annotations
big data;information access
evaluation methodology;document collection;web search;user interactions;ad-hoc retrieval;domain model;query recommendation;specific domains;enterprise search;implicit feedback;search engines;query logs;digital libraries;taking into account;information retrieval;domain models
real-world;compression methods;information science;query loads;intra-query;linear programming;query logs;disk access;real-life;query processing in;text collections;search engines;query processing;search engine;web search engines;analytical model
end-user;trec test collections;classification;large-scale;related queries;user query;selection techniques;web search results;online news;end-users;web search engine;news-related;relevant content;user queries;user-generated content;user-study;news search;classification approach;web search engine;user-generated;news events;fully automatic;unique characteristics;machine learned;web search;vertical search;classification accuracy;search result;multiple streams;web search engines
rdf data;hybrid approach;linked data;web search;retrieval effectiveness;end users;language model;federated search;language modelling;semantic web;current web;search problem;traditional information retrieval;entity search;test collections;current web;keyword search;distributed environment
user query;retrieval performance;retrieval systems;error-prone;document retrieval;retrieval techniques;random sample;ad hoc;information retrieval;vector space model;xml document collections;xml retrieval;xml queries;relevant document;information retrieval;query sets;reusability;evaluation methodology;document level;supervision;retrieval results;xml elements;length normalization
result diversification;probabilistic framework;query generation;web search;web search engine;queries submitted to;search result diversification;multiple documents;keyword-based;query log;documents retrieved;search engine;retrieved documents;ranking models;query suggestions;machine learning approaches
ranking approaches;portfolio theory;document relevance;document ranking;ad-hoc retrieval;relevant documents;document retrieval;expected values;information retrieval;step forward;ranking tasks;retrieved documents;quantum theory;ranking documents;probability ranking principle;probability ranking principle;retrieval effectiveness;probability theory;document ranking
automatically learns;description language;declarative queries;plans;large-scale;scientific applications;training data;training samples;science applications;cost models;statistical learning