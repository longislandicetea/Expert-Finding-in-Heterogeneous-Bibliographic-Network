#index 246016
#* Towards on-line analytical mining in large databases
#@ 961
#t 1998
#c ACM SIGMOD Record
#! Great efforts have been paid in the Intelligent Database Systems Research Lab for the research and development of efficient data mining methods and construction of on-line analytical data mining systems.Our work has been focused on the integration of data mining and OLAP technologies and the development of scalable, integrated, and multiple data mining functions. A data mining system, DBMiner, has been developed for interactive mining of multiple-level knowledge in large relational databases and data warehouses. The system implements a wide spectrum of data mining functions, including characterization, comparison, association, classification, prediction, and clustering. It also builds up a user-friendly, interactive data mining environment and a set of knowledge visualization tools. In-depth research has been performed on the efficiency and scalability of data mining methods. Moreover, the research has been extended to spatial data mining, multimedia data mining, text mining, and Web mining with several new data mining system prototypes constructed or under construction, including GeoMiner, MultiMediaMiner, and WebLogMiner.This article summarizes our research and development activities in the last several years and shares our experiences and lessons with the readers.

#index 248785
#* Exploratory mining and pruning optimizations of constrained associations rules
#@ 1020 190 961 1021
#t 1998
#c SIGMOD '98 Proceedings of the 1998 ACM SIGMOD international conference on Management of data
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 216508
#% 227883
#% 227919
#% 227922
#% 227953
#% 236414
#% 236696
#% 443091
#% 461909
#% 464204
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 481954
#! From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcomings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraint-based, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of constraint constructs, including domain, class, and SQL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.In this paper, we mainly focus on the technical challenges in guaranteeing a level of performance that is commensurate with the selectivities of the constraints in an association query. To this end, we introduce and analyze two properties of constraints that are critical to pruning: anti-monotonicity and succinctness. We then develop characterizations of various constraints into four categories, according to these properties. Finally, we describe a mining algorithm called CAP, which achieves a maximized degree of pruning for all categories of constraints. Experimental results indicate that CAP can run much faster, in some cases as much as 80 times, than several basic algorithms. This demonstrates how important the succinctness and anti-monotonicity properties are, in delivering the performance guarantee.

#index 248865
#* MultiMediaMiner: a system prototype for multimedia data mining
#@ 1160 961 1161 1162 1163
#t 1998
#c SIGMOD '98 Proceedings of the 1998 ACM SIGMOD international conference on Management of data
#% 232102
#% 394418
#% 420056
#% 437405
#% 481438
#% 514728
#% 581572
#! Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.

#index 252362
#* Data mining techniques
#@ 961
#t 1996
#c SIGMOD '96 Proceedings of the 1996 ACM SIGMOD international conference on Management of data
#! Data mining, or knowledge discovery in databases, has been popularly recognized as an important research issue with broad applications. We provide a comprehensive survey, in database perspective, on the data mining techniques developed recently. Several major kinds of data mining methods, including generalization, characterization, classification, clustering, association, evolution, pattern matching, data visualization, and meta-rule guided mining, will be reviewed. Techniques for mining knowledge in different kinds of databases, including relational, transaction, object-oriented, spatial, and active databases, as well as global information systems, will be examined. Potential data mining applications and some research issues will also be discussed.

#index 273899
#* Optimization of constrained frequent set queries with 2-variable constraints
#@ 190 2178 961 1021
#t 1999
#c SIGMOD '99 Proceedings of the 1999 ACM SIGMOD international conference on Management of data
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 216508
#% 227919
#% 227953
#% 236414
#% 248784
#% 248785
#% 248791
#% 248813
#% 464204
#% 479482
#% 479484
#% 479627
#% 481290
#% 481588
#% 481758
#% 481779
#! Currently, there is tremendous interest in providing ad-hoc mining capabilities in database management systems. As a first step towards this goal, in [15] we proposed an architecture for supporting constraint-based, human-centered, exploratory mining of various kinds of rules including associations, introduced the notion of constrained frequent set queries (CFQs), and developed effective pruning optimizations for CFQs with 1-variable (1-var) constraints.While 1-var constraints are useful for constraining the antecedent and consequent separately, many natural examples of CFQs illustrate the need for constraining the antecedent and consequent jointly, for which 2-variable (2-var) constraints are indispensable. Developing pruning optimizations for CFQs with 2-var constraints is the subject of this paper. But this is a difficult problem because: (i) in 2-var constraints, both variables keep changing and, unlike 1-var constraints, there is no fixed target for pruning; (ii) as we show, “conventional” monotonicity-based optimization techniques do not apply effectively to 2-var constraints.The contributions are as follows. (1) We introduce a notion of quasi-succinctness, which allows a quasi-succinct 2-var constraint to be reduced to two succinct 1-var constraints for pruning. (2) We characterize the class of 2-var constraints that are quasi-succinct. (3) We develop heuristic techniques for non-quasi-succinct constraints. Experimental results show the effectiveness of all our techniques. (4) We propose a query optimizer for CFQs and show that for a large class of constraints, the computation strategy generated by the optimizer is ccc-optimal, i.e., minimizing the effort incurred w.r.t. constraint checking and support counting.

#index 274146
#* Exploratory mining via constrained frequent set queries
#@ 2178 190 961 2253
#t 1999
#c SIGMOD '99 Proceedings of the 1999 ACM SIGMOD international conference on Management of data
#% 152934
#% 216508
#% 227919
#% 236414
#% 248784
#% 248785
#% 248813
#! Although there have been many studies on data mining, to date there have been few research prototypes or commercial systems supporting comprehensive query-driven mining, which encourages interactive exploration of the data. Our thesis is that constraint constructs and the optimization they induce play a pivotal role in mining queries, thus substantially enhancing the usefulness and performance of the mining system. This is based on the analogy of declarative query languages like SQL and query optimization which have made relational databases so successful. To this end, our proposed demo is not yet another data mining system, but of a new paradigm in data mining - mining with constraints, as the important first step towards supporting ad-hoc mining in DBMS.In this demo, we will show a prototype exploratory mining system that implements constraint-based mining query optimization methods proposed in [5]. We will demonstrate how a user can interact with the system for exploratory data mining and how efficiently the system may execute optimized data mining queries. The prototype system will include all the constraint pushing techniques for mining association rules outlined in [5], and will include additional capabilities for mining other kinds of rules for which the computation of constrained frequent sets forms the core first step.

#index 280473
#* Breaking the barrier of transactions: mining inter-transaction association rules
#@ 2470 2471 961 2472
#t 1999
#c KDD '99 Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 152934
#% 172386
#% 201894
#% 210160
#% 248785
#% 273899
#% 420063
#% 463903
#% 481290
#% 481758
#% 481779
#% 481954

#index 300120
#* Mining frequent patterns without candidate generation
#@ 961 3214 3215
#t 2000
#c SIGMOD '00 Proceedings of the 2000 ACM SIGMOD international conference on Management of data
#% 172386
#% 201894
#% 227919
#% 248785
#% 248791
#% 248813
#% 280409
#% 329598
#% 420063
#% 461909
#% 463903
#% 479484
#% 481290
#% 481754
#% 631926
#! Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.

#index 310558
#* Can we push more constraints into frequent pattern mining?
#@ 3214 961
#t 2000
#c Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 248785
#% 273899
#% 300120
#% 481290
#% 632028

#index 310559
#* FreeSpan: frequent pattern-projected sequential pattern mining
#@ 961 3214 3599 3600 508 3601
#t 2000
#c Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 210160
#% 300120
#% 329598
#% 420063
#% 459006
#% 463903
#% 481290

#index 333925
#* Efficient computation of Iceberg cubes with complex measures
#@ 961 3214 171 1282
#t 2001
#c SIGMOD '01 Proceedings of the 2001 ACM SIGMOD international conference on Management of data
#% 210182
#% 223781
#% 227880
#% 248785
#% 273899
#% 273916
#% 300120
#% 310558
#% 420053
#% 479450
#% 479795
#% 481290
#% 481951
#! It is often too expensive to compute and materialize a complete high-dimensional data cube. Computing an iceberg cube, which contains only aggregates above certain thresholds, is an effective way to derive nontrivial multi-dimensional aggregations for OLAP and data mining.In this paper, we study efficient methods for computing iceberg cubes with some popularly used complex measures, such as average, and develop a methodology that adopts a weaker but anti-monotonic condition for testing and pruning search space. In particular, for efficient computation of iceberg cubes with the average measure, we propose a top-k average pruning method and extend two previously studied methods, Apriori and BUC, to Top-k Apriori and Top-k BUC. To further improve the performance, an interesting hypertree structure, called H-tree, is designed and a new iceberg cubing method, called Top-k H-Cubing, is developed. Our performance study shows that Top-k BUC and Top-k H-Cubing are two promising candidates for scalable computation, and Top-k H-Cubing has better performance in most cases.

#index 334041
#* DNA-miner: a system prototype for mining DNA sequences
#@ 961 3957 1980 3958 3959 3214
#t 2001
#c SIGMOD '01 Proceedings of the 2001 ACM SIGMOD international conference on Management of data

#index 338580
#* Mining frequent patterns by pattern-growth: methodology and implications
#@ 961 3214
#t 2000
#c ACM SIGKDD Explorations Newsletter - Special issue on “Scalable data mining algorithms”
#% 152934
#% 227919
#% 248785
#% 248791
#% 273899
#% 273916
#% 280409
#% 300120
#% 310558
#% 310559
#% 329598
#% 420063
#% 459006
#% 463903
#% 464839
#% 464989
#% 464996
#% 479482
#% 479484
#% 479795
#% 479971
#% 481290
#% 631926
#% 631970
#% 632028

#index 342625
#* Mining top-n local outliers in large databases
#@ 4232 4233 961
#t 2001
#c Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining
#% 210173
#% 248790
#% 300136
#% 300183
#% 479791
#% 481281
#% 481956
#! Outlier detection is an important task in data mining with numerous applications, including credit card fraud detection, video surveillance, etc. A recent work on outlier detection has introduced a novel notion of local outlier in which the degree to which an object is outlying is dependent on the density of its local neighborhood, and each object can be assigned a Local Outlier Factor (LOF) which represents the likelihood of that object being an outlier. Although the concept of local outliers is a useful one, the computation of LOF values for every data objects requires a large number of &kgr;-nearest neighbors searches and can be computationally expensive. Since most objects are usually not outliers, it is useful to provide users with the option of finding only n most outstanding local outliers, i.e., the top-n data objects which are most likely to be local outliers according to their LOFs. However, if the pruning is not done carefully, finding top-n outliers could result in the same amount of computation as finding LOF for all objects. In this paper, we propose a novel method to efficiently find the top-n local outliers in large databases. The concept of "micro-cluster" is introduced to compress the data. An efficient micro-cluster-based local outlier mining algorithm is designed based on this concept. As our algorithm can be adversely affected by the overlapping in the micro-clusters, we proposed a meaningful cut-plane solution for overlapping data. The formal analysis and experiments show that this method can achieve good performance in finding the most outstanding local outliers.

#index 342817
#* Scalable frequent-pattern mining methods: an overview
#@ 961 190 3214
#t 2001
#c Tutorial notes of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 227917
#% 227919
#% 227922
#% 227953
#% 248784
#% 248785
#% 248791
#% 248813
#% 259993
#% 273899
#% 273916
#% 280473
#% 300120
#% 310494
#% 310558
#% 310559
#% 318994
#% 329598
#% 333925
#% 420063
#% 420067
#% 438134
#% 459006
#% 461909
#% 463903
#% 464204
#% 464822
#% 464839
#% 464989
#% 464996
#% 479482
#% 479484
#% 479627
#% 479795
#% 479971
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 481954
#% 584891
#% 631926
#% 632028
#% 632037

#index 397383
#* Mining long sequential patterns in a noisy environment
#@ 3573 1204 850 961
#t 2002
#c Proceedings of the 2002 ACM SIGMOD international conference on Management of data
#% 18658
#% 172892
#% 248791
#% 300120
#% 310500
#% 310542
#% 310559
#% 329537
#% 338586
#% 342642
#% 420062
#% 420063
#% 459006
#% 459020
#% 463903
#% 464403
#% 464714
#% 465003
#% 479971
#% 481290
#% 481758
#% 481779
#% 565487
#% 614619
#! Pattern discovery in long sequences is of great importance in many applications including computational biology study, consumer behavior analysis, system performance analysis, etc. In a noisy environment, an observed sequence may not accurately reflect the underlying behavior. For example, in a protein sequence, the amino acid N is likely to mutate to D with little impact to the biological function of the protein. It would be desirable if the occurrence of D in the observation can be related to a possible mutation from N in an appropriate manner. Unfortunately, the support measure (i.e., the number of occurrences) of a pattern does not serve this purpose. In this paper, we introduce the concept of compatibility matrix as the means to provide a probabilistic connection from the observation to the underlying true value. A new metric match is also proposed to capture the "real support" of a pattern which would be expected if a noise-free environment is assumed. In addition, in the context we address, a pattern could be very long. The standard pruning technique developed for the market basket problem may not work efficiently. As a result, a novel algorithm that combines statistical sampling and a new technique (namely border collapsing) is devised to discover long patterns in a minimal number of scans of the sequence database with sufficiently high confidence. Empirical results demonstrate the robustness of the match model (with respect to the noise) and the efficiency of the probabilistic algorithm.

#index 397417
#* CubeExplorer: online exploration of data cubes
#@ 961 4625 171 3214 1282
#t 2002
#c Proceedings of the 2002 ACM SIGMOD international conference on Management of data
#% 333925
#% 480630
#! Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation [2], cube-based feature extraction, and gradient analysis [1], and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.

#index 399793
#* Constrained frequent pattern mining: a pattern-growth view
#@ 3214 961
#t 2002
#c ACM SIGKDD Explorations Newsletter
#% 172386
#% 201894
#% 227919
#% 248785
#% 248791
#% 248813
#% 273899
#% 273916
#% 280409
#% 300120
#% 310558
#% 310559
#% 338580
#% 420063
#% 461909
#% 463903
#% 464989
#% 464996
#% 465003
#% 466490
#% 479484
#% 481290
#% 481754
#% 631926
#% 631985
#% 632028
#! It has been well recognized that frequent pattern mining plays an essential role in many important data mining tasks. However, frequent pattern mining often generates a very large number of patterns and rules, which reduces not only the efficiency but also the effectiveness of mining. Recent work has highlighted the importance of the constraint-based mining paradigm in the context of mining frequent itemsets, associations, correlations, sequential patterns, and many other interesting patterns in large databases.Recently, we developed efficient pattern-growth methods for frequent pattern mining. Interestingly, pattern-growth methods are not only efficient but also effective in mining with various constraints. Many tough constraints which cannot be handled by previous methods can be pushed deep into the pattern-growth mining process. In this paper, we overview the principles of pattern-growth methods for constrained frequent pattern mining and sequential pattern mining. Moreover, we explore the power of pattern-growth methods towards mining with tough constraints and highlight some interesting open problems.

#index 458833
#* Profit Mining: From Patterns to Actions
#@ 1282 3571 961
#t 2002
#c EDBT '02 Proceedings of the 8th International Conference on Extending Database Technology: Advances in Database Technology
#% 136350
#% 152934
#% 220706
#% 232136
#% 280437
#% 280456
#% 420082
#% 443092
#% 465754
#% 481290
#% 481588
#% 481758
#! A major obstacle in data mining applications is the gap between the statistic-based pattern extraction and the value-based decision making. We present a profit mining approach to reduce this gap. In profit mining, we are given a set of past transactions and pre-selected target items, and we like to build a model for recommending target items and promotion strategies to new customers, with the goal of maximizing the net profit. We identify several issues in profit mining and propose solutions. We evaluate the effectiveness of this approach using data sets of a wide range of characteristics.

#index 461908
#* Data Mining: Where is it Heading? (Panel)
#@ 961
#t 1997
#c ICDE '97 Proceedings of the Thirteenth International Conference on Data Engineering
#! Data mining is a promising field in which research and development activities are flourishing. It is also a young field with vast, unexplored territories. How can we contribute significantly to this fast expanding, multi-disciplinary field? This panel will bring database researchers together to share different views and insights on the issues in the field.

#index 464204
#* Maintenance of Discovered Association Rules in Large Databases: An Incremental Updating Technique
#@ 4843 961 5468 5469
#t 1996
#c ICDE '96 Proceedings of the Twelfth International Conference on Data Engineering
#! An incremental updating technique is developed for maintenance of the association rules discovered by database mining. There have been many studies on efficient discovery of association rules in large databases. However, it is nontrivial to maintain such discovered rules in large databases because a database may allow frequent or occasional updates and such updates may not only invalidate some existing strong association rules but also turn some weak rules into strong ones. In this study, an incremental updating technique is proposed for efficient maintenance of discovered association rules when new transaction data are added to a transaction database.

#index 466483
#* CMAR: Accurate and Efficient Classification Based on Multiple Class-Association Rules
#@ 5904 961 3214
#t 2001
#c ICDM '01 Proceedings of the 2001 IEEE International Conference on Data Mining
#! Previous studies propose that associative classification has high classification accuracy and strong flexibility at handling unstructured data. However, it still suffers from the huge set of mined rules and sometimes biased classification or overfitting since the classificationis based on only single high-confidence rule. In this study, we propose new associative classification method, CMAR, i.e., Classification based on Multiple Association Rules. The method extends an efficient frequent pattern mining method, FP-growth ,constructs classdistribution-associated FP-tree, and mines large database efficiently. Moreover, it applies CR-tree structure to store and retrieve mined association rulesefficiently, and prunes rules effectively based on confidence, correlation and database coverage. The classification is performed based on weighted X2 analysis using multiple strong association rules. Our extensive experiments on 26 databases from UCI machine learning database repository show that CMAR is consistent, highly effective at classificationof various kinds of databases and has better average classificationaccuracy in comparison with CBA and C4.5.Moreover,our performancestudy shows that the method is highly efficient and scalable in comparison with other reported associative classification methods.

#index 466490
#* H-Mine: Hyper-Structure Mining of Frequent Patterns in Large Databases
#@ 3214 961 2471 5712 4612 4613
#t 2001
#c ICDM '01 Proceedings of the 2001 IEEE International Conference on Data Mining
#! Methods for efficient mining of frequent patterns have been studied extensively by many researchers. However, the previously proposed methods still encounter someperformance bottlenecks when mining databases with different data characteristics, such as dense vs. sparse, long vs. short patterns, memory-based vs. disk-based, etc.In this study, we propose a simple and novel hyper-linkeddata structure, H-struct , and a new mining algorithm, H-mine ,which takes advantage of this data structure anddynamically adjusts links in the mining process. A distinct feature of this method is that it has very limitedand precisely predictable space overhead and runs really fast in memory-based setting. Moreover, it ca be scaled up to very large databases by database partitioning, and whenthe data set becomes dense,(conditional)FP-trees can be constructed dynamically as part of the mining process. Our study shows that H-mine has high performance in various kinds of data, outperforms the previously developedalgorithms in different settings, and is highly scalable in mining large databases. This study also proposes a new datamining methodology, space-preserving mining ,which mayhave strong impact in the future development of efficient and scalable data mining methods.

#index 480154
#* Mining Frequent Itemsets Using Support Constraints
#@ 1282 3572 961
#t 2000
#c VLDB '00 Proceedings of the 26th International Conference on Very Large Data Bases
#% 152934
#% 201894
#% 227917
#% 227919
#% 248012
#% 280409
#% 280439
#% 280487
#% 300120
#% 479484
#% 479817
#% 481290
#% 481588
#% 481754
#% 481758
#% 632029

#index 501226
#* Selective Materialization: An Efficient Method for Spatial Data Cube Construction
#@ 961 676 675
#t 1998
#c PAKDD '98 Proceedings of the Second Pacific-Asia Conference on Research and Development in Knowledge Discovery and Data Mining

#index 502147
#* Top Down FP-Growth for Association Rule Mining
#@ 1282 7576 961 7577
#t 2002
#c PAKDD '02 Proceedings of the 6th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining
#% 152934
#% 280487
#% 300120
#% 463903
#% 480154
#% 481758
#% 481779
#! In this paper, we propose an efficient algorithm, called TD-FP-Growth (the shorthand for Top-Down FP-Growth), to mine frequent patterns. TD-FP-Growth searches the FP-tree in the top-down order, as opposed to the bottom-up order of previously proposed FP-Growth. The advantage of the topdown search is not generating conditional pattern bases and sub-FP-trees, thus, saving substantial amount of time and space. We extend TD-FP-Growth to mine association rules by applying two new pruning strategies: one is to push multiple minimum supports and the other is to push the minimum confidence. Experiments show that these algorithms and strategies are highly effective in reducing the search space.

#index 571255
#* Index nesting – an efficient approach to indexing in object-oriented databases
#@ 3197 961 2471 8068
#t 1996
#c The VLDB Journal — The International Journal on Very Large Data Bases
#% 57955
#% 86954
#% 116056
#% 286189
#% 317933
#% 427212
#% 442665
#% 479913
#% 479932
#% 481435
#% 481449
#! In object-oriented database systems where the concept of the superclass-subclass is supported, an instance of a subclass is also an instance of its superclass. Consequently, the access scope of a query against a class in general includes the access scope of all its subclasses, unless specified otherwise. An index to support superclass-subclass relationship efficiently must provide efficient associative retrievals of objects from a single class or from several classes in a class hierarchy. This paper presents an efficient index called the hierarchical tree (the H-tree). For each class, an H-tree is maintained, allowing efficient search on a single class. These H-trees are appropriately linked to capture the superclass-subclass relationships, thus allowing efficient retrievals of instances from a class hierarchy. Both experimental and analytical results indicate that the H-tree is an efficient indexing structure.

#index 572295
#* A template model for multidimensional inter-transactional association rules
#@ 2472 4839 2471 961
#t 2002
#c The VLDB Journal — The International Journal on Very Large Data Bases
#% 152934
#% 172386
#% 172892
#% 201894
#% 210160
#% 210162
#% 213963
#% 213977
#% 227919
#% 227922
#% 227953
#% 236696
#% 248784
#% 248785
#% 273899
#% 280473
#% 280487
#% 318994
#% 371671
#% 420063
#% 420087
#% 420110
#% 443085
#% 443091
#% 461909
#% 462231
#% 462234
#% 463903
#% 464196
#% 464204
#% 464839
#% 477479
#% 479484
#% 479490
#% 479627
#% 479785
#% 479795
#% 480154
#% 481290
#% 481588
#% 481609
#% 481754
#% 481758
#% 481779
#% 481954
#% 631914
#% 632029
#% 632090
#! Multidimensional inter-transactional association rules extend the traditional association rules to describe more general associations among items with multiple properties across transactions. “After McDonald and Burger King open branches, KFC will open a branch two months later and one mile away” is an example of such rules. Since the number of potential inter-transactional association rules tends to be extremely large, mining inter-transactional associations poses more challenges on efficient processing than mining traditional intra-transactional associations. In order to make such association rule mining truly practical and computationally tractable, in this study we present a template model to help users declare the interesting multidimensional inter-transactional associations to be mined. With the guidance of templates, several optimization techniques, i.e., joining, converging, and speeding, are devised to speed up the discovery of inter-transactional association rules. We show, through a series of experiments on both synthetic and real-life data sets, that these optimization techniques can yield significant performance benefits.

#index 577234
#* Mining frequent item sets by opportunistic projection
#@ 7577 4503 1282 961
#t 2002
#c Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 152934
#% 201894
#% 227917
#% 248791
#% 300120
#% 310507
#% 329598
#% 342643
#% 459020
#% 465003
#% 466490
#% 481290
#% 481588
#% 481754
#% 481779
#! In this paper, we present a novel algorithm Opportune Project for mining complete set of frequent item sets by projecting databases to grow a frequent item set tree. Our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structures, array-based or tree-based, to represent projected transaction subsets, and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to features of the subsets. More importantly, we propose novel methods to build tree-based pseudo projections and array-based unfiltered projections for projected transaction subsets, which makes our algorithm both CPU time efficient and memory saving. Basically, the algorithm grows the frequent item set tree by depth first search, whereas breadth first search is used to build the upper portion of the tree if necessary. We test our algorithm versus several other algorithms on real world datasets, such as BMS-POS, and on IBM artificial datasets. The empirical results show that our algorithm is not only the most efficient on both sparse and dense databases at all levels of support threshold, but also highly scalable to very large databases.

#index 577235
#* PEBL: positive example based learning for Web page classification using SVM
#@ 8193 961 4553
#t 2002
#c Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 197394
#% 266215
#% 280817
#% 309141
#% 309142
#% 310556
#% 311027
#% 316533
#% 458379
#% 466263
#% 615723
#% 722756
#% 722811
#% 722812
#! Web page classification is one of the essential techniques for Web mining. Specifically, classifying Web pages of a user-interesting class is the first step of mining interesting information from the Web. However, constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples. For instance, in order to construct a "homepage" classifier, one needs to collect a sample of homepages (positive examples) and a sample of non-homepages (negative examples). In particular, collecting negative training examples requires arduous work and special caution to avoid biasing them. We introduce in this paper the Positive Example Based Learning (PEBL) framework for Web page classification which eliminates the need for manually collecting negative training examples in pre-processing. We present an algorithm called Mapping-Convergence (M-C) that achieves classification accuracy (with positive and unlabeled data) as high as that of traditional SVM (with positive and negative data). Our experiments show that when the M-C algorithm uses the same amount of positive examples as that of traditional SVM, the M-C algorithm performs as well as traditional SVM.

#index 629606
#* On Computing Condensed Frequent Pattern Bases
#@ 3214 171 9304 961
#t 2002
#c ICDM '02 Proceedings of the 2002 IEEE International Conference on Data Mining
#! Frequent pattern mining has been studied extensively.However, the effectiveness and efficiency of this mining isoften limited, since the number of frequent patterns generatedis often too large. In many applications it is sufficientto generate and examine only frequent patterns with supportfrequency in close-enough approximation instead of in fullprecision. Such a compact but close-enough frequent patternbase is called a condensed frequent patterns-base.In this paper, we propose and examine several alternativesat the design, representation, and implementation ofsuch condensed frequent pattern-bases. A few algorithmsfor computing such pattern-bases are proposed. Their effectivenessat pattern compression and their efficient computationmethods are investigated. A systematic performancestudy is conducted on different kinds of databases,which demonstrates the effectiveness and efficiency of ourapproach at handling frequent pattern mining in largedatabases.

#index 629610
#* Heterogeneous Learner for Web Page Classification
#@ 8193 4553 961
#t 2002
#c ICDM '02 Proceedings of the 2002 IEEE International Conference on Data Mining
#! Classification of an interesting class of Web pages (e.g.,personal homepages, resume pages) has been an interestingproblem. Typical machine learning algorithms for thisproblem require two classes of data for training: positiveand negative training examples. However, in applicationto Web page classification, gathering an unbiased sampleof negative examples appears to be difficult. We proposea heterogeneous learning framework for classifying Webpages, which (1) eliminates the need for negative trainingdata, and (2) increases classification accuracy by using twoheterogeneous learners. Our framework uses two heterogeneouslearners - a decision list and a linear separatorwhich complement each other - to eliminate the need fornegative training data in the training phase and to increasethe accuracy in the testing phase. Our results show that ourheterogeneous framework achieves high accuracy withoutrequiring negative training data; it enhances the accuracyof linear separators by reducing the errors on "low-margindata". That is, it classifies more accurately while requiringless human efforts in training.

#index 629644
#* Mining Top.K Frequent Closed Patterns without Minimum Support
#@ 961 4625 1980 9352
#t 2002
#c ICDM '02 Proceedings of the 2002 IEEE International Conference on Data Mining
#! In this paper, we propose a new mining task: mining top-kfrequent closed patterns of length no less than min_l, wherek is the desired number of frequent closed patterns to bemined, and min _l is the minimal length of each pattern.An efficient algorithm, called TFP, is developed for mining such patterns without minimum support. Two methods, closed_node_count and descendant_sum are proposedto effiectively raise support threshold and prune FP-tree bothduring and after the construction of FP-tree. During themining process, a novel top-down and bottom-up combinedFP-tree mining strategy is developed to speed-up support-raising and closed frequent pattern discovering. In addition,a fast hash-based closed pattern verification scheme has beenemployed to check efficiently if a potential closed pattern isreally closed.Our performance study shows that in most cases, TFPoutperforms CLOSET and CHARM, two efficient frequentclosed pattern mining algorithms, even when both are running with the best tuned min_support. Furthermore, themethod can be extended to generate association rules andto incorporate user-specified constraints. Thus we concludethat for frequent pattern mining, mining top-k frequent closedpatterns without min support is more preferable than thetraditional min_support-based mining.

#index 629708
#* gSpan: Graph-Based Substructure Pattern Mining
#@ 9413 961
#t 2002
#c ICDM '02 Proceedings of the 2002 IEEE International Conference on Data Mining
#! We investigate new approaches for frequent graph-basedpattern mining in graph datasets and propose a novel algorithmcalled gSpan (graph-based Substructure pattern mining),which discovers frequent substructures without candidategeneration. gSpan builds a new lexicographic orderamong graphs, and maps each graph to a unique minimumDFS code as its canonical label. Based on this lexico-graphicorder, gSpan adopts the depth-first search strategyto mine frequent connected subgraphs efficiently. Our performancestudy shows that gSpan substantially outperformsprevious algorithms, sometimes by an order of magnitude.

#index 631001
#* Discovering geographic knowledge in data rich environments: a report on a specialist meeting
#@ 9461 961
#t 2000
#c ACM SIGKDD Explorations Newsletter
#! On 18--20 March 1999, a Specialist Meeting on "Discovering geographic knowledge in data-rich environments" was convened under the auspices of the Varenius Project of the National Center for Geographic Information and Analysis (NCGIA). This, workshop brought together a diverse group of researchers and practitioners with interests in developing and applying new techniques for exploring large and diverse geographic datasets. The interaction prior to, during and after the three-day workshop resulted in the identification of research priorities and directions for continued development of "geographic knowledge discovery" (GKD) theory techniques.

#index 727869
#* CoMine: Efficient Mining of Correlated Patterns
#@ 6579 10032 10033 961
#t 2003
#c ICDM '03 Proceedings of the Third IEEE International Conference on Data Mining
#% 227919
#% 299985
#% 300120
#% 452846
#% 466476
#% 481290
#! Association rule mining often generates a huge numberof rules, but a majority of them either are redundantor don not reflect the tue correlation relationship amongdata objects.In this paper, we re-examine this problemand show that two interesting measures, all_confidence(denoted as \alpha) and coherence (denoted as \gamma), both disclosegenuine correlation relationships and can be computedefficiently.Moreover, we propose two interestingalgorithms, CoMine(\alpha) and CoMine(\gamma), based onextensions of a pattern-growth methodology.Our performancestudy shows that the CoMine algorithms havehigh performance in comparison with their Apriori-basedcounterpart algorithms.

#index 727913
#* TSP: Mining Top-K Closed Sequential Patterns
#@ 9352 9413 961
#t 2003
#c ICDM '03 Proceedings of the Third IEEE International Conference on Data Mining
#% 329537
#% 459006
#% 463903
#% 464996
#% 577256
#% 629644
#% 631985
#% 729938
#! Sequential pattern mining has been studied extensivelyin data mining community.Most previous studies requirethe specification of a minimum support threshold to performthe mining.However, it is difficult for users to providean appropriate threshold in practice.To overcomethis difficulty, we propose an alternative task: mining top-kfrequent closed sequential patterns of length no less thanmin_l, where k is the desired number of closed sequentialpatterns to be mined, and min_l is the minimum length ofeach pattern.We mine closed patterns since they are compactrepresentations of frequent patterns.We developed an efficient algorithm, called TSP, whichmakes use of the length constraint and the properties of top-kclosed sequential patterns to perform dynamic support-raisingand projected database-pruning.Our extensive performancestudy shows that TSP outperforms the closed sequentialpattern mining algorithm even when the latter isrunning with the best tuned minimum support threshold.

#index 729932
#* Mining concept-drifting data streams using ensemble classifiers
#@ 4558 2485 850 961
#t 2003
#c Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 132583
#% 136350
#% 273900
#% 310500
#% 333931
#% 342600
#% 342639
#% 378388
#% 397380
#% 424997
#% 428155
#% 449529
#% 466401
#% 481945
#% 578678
#% 594012
#% 629625
#% 993958
#% 1279299
#! Recently, mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection, target marketing, network intrusion detection, etc. Conventional knowledge discovery tools are facing two challenges, the overwhelming volume of the streaming data, and the concept drifts. In this paper, we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers. We train an ensemble of classification models, such as C4.5, RIPPER, naive Beyesian, etc., from sequential chunks of the data stream. The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment. Thus, the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification. Our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy, and the ensemble framework is effective for a variety of classification models.

#index 729933
#* CLOSET+: searching for the best strategies for mining frequent closed itemsets
#@ 4625 961 3214
#t 2003
#c Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 152934
#% 201894
#% 227917
#% 248791
#% 300120
#% 310494
#% 342643
#% 443348
#% 464714
#% 465003
#% 466490
#% 481290
#% 481779
#% 577234
#% 629644
#! Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis. Extensive studies have proposed various strategies for efficient frequent closed itemset mining, such as depth-first search vs. breadthfirst search, vertical formats vs. horizontal formats, tree-structure vs. other data structures, top-down vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask "what are the pros and cons of the strategies?" and "what and how can we pick and integrate the best strategies to achieve higher performance in general cases?"In this study, we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability.

#index 729938
#* CloseGraph: mining closed frequent graph patterns
#@ 9413 961
#t 2003
#c Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 300120
#% 410276
#% 465003
#% 466644
#% 481290
#% 577218
#% 629603
#% 629630
#% 629646
#% 629708
#! Recent research on pattern discovery has progressed form mining frequent itemsets and sequences to mining structured patterns including trees, lattices, and graphs. As a general data structure, graph can model complicated relations among data with wide applications in bioinformatics, Web exploration, and etc. However, mining large graph patterns in challenging due to the presence of an exponential number of frequent subgraphs. Instead of mining all the subgraphs, we propose to mine closed frequent graph patterns. A graph g is closed in a database if there exists no proper supergraph of g that has the same support as g. A closed graph pattern mining algorithm, CloseGraph, is developed by exploring several interesting pruning methods. Our performance study shows that CloseGraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increases the efficiency of mining, especially in the presence of large graph patterns.

#index 729940
#* Classifying large data sets using SVMs with hierarchical clusters
#@ 8193 3573 961
#t 2003
#c Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 210173
#% 248790
#% 269217
#% 269218
#% 342598
#% 342625
#% 420077
#% 438137
#% 458379
#% 466419
#% 466887
#% 543892
#% 577228
#% 577235
#% 722757
#% 855583

#index 730039
#* Text classification from positive and unlabeled documents
#@ 8193 3494 961
#t 2003
#c CIKM '03 Proceedings of the twelfth international conference on Information and knowledge management
#% 127850
#% 280817
#% 311027
#% 340903
#% 340904
#% 344447
#% 397135
#% 458379
#% 464641
#% 466263
#% 495944
#% 577235
#% 722811
#% 722812
#% 855583
#% 872759
#% 1279295
#! Most existing studies of text classification assume that the training data are completely labeled. In reality, however, many information retrieval problems can be more accurately described as learning a binary classifier from a set of incompletely labeled examples, where we typically have a small number of labeled positive examples and a very large number of unlabeled examples. In this paper, we study such a problem of performing Text Classification WithOut labeled Negative data TC-WON). In this paper, we explore an efficient extension of the standard Support Vector Machine (SVM) approach, called SVMC (Support Vector Mapping Convergence) [17]for the TC-WON tasks. Our analyses show that when the positive training data is not too under-sampled, SVMC significantly outperforms other methods because SVMC basically exploits the natural "gap" between positive and negative documents in the feature space, which eventually corresponds to improving the generalization performance. In the text domain there are likely to exist many gaps in the feature space because a document is usually mapped to a sparse and high dimensional feature space. However, as the number of positive training data decreases, the boundary of SVMC starts overfitting at some point and end up generating very poor results.This is because when the positive training data is too few, the boundary over-iterates and trespasses the natural gaps between positive and negative class in the feature space and thus ends up fitting tightly around the few positive training data.

#index 745491
#* CrossMine: Efficient Classification Across Multiple Database Relations
#@ 10548 961 3573 850
#t 2004
#c ICDE '04 Proceedings of the 20th International Conference on Data Engineering
#% 99396
#% 136350
#% 376266
#% 393907
#% 396021
#% 398844
#% 420077
#% 458257
#% 464304
#% 466073
#% 479787
#% 629708
#% 1289267
#! Most of today's structured data is stored in relationaldatabases. Such a database consists of multiplerelations which are linked together conceptually viaentity-relationship links in the design of relational databaseschemas. Multi-relational classification can be widelyused in many disciplines, such as financial decision making,medical research, and geographical applications.However, most classification approaches only work on single"flat" data relations. It is usually difficult to convertmultiple relations into a single flat relation without eitherintroducing huge, undesirable "universal relation" orlosing essential information. Previous works using InductiveLogic Programming approaches (recently also knownas Relational Mining) have proven effective with high accuracyin multi-relational classification. Unfortunately,they suffer from poor scalability w.r.t. the number of relationsand the number of attributes in databases.In this paper we propose CrossMine, an efficientand scalable approach for multi-relational classification.Several novel methods are developed in CrossMine,including (1) tuple ID propagation, which performssemantics-preserving virtual join to achieve high efficiencyon databases with complex schemas, and (2) a selectivesampling method, which makes it highly scalablew.r.t. the number of tuples in the databases. Both theoreticalbackgrounds and implementation techniques ofCrossMine are introduced. Our comprehensive experimentson both real and synthetic databases demonstratethe high scalability and accuracy of CrossMine.

#index 745515
#* BIDE: Efficient Mining of Frequent Closed Sequences
#@ 4625 961
#t 2004
#c ICDE '04 Proceedings of the 20th International Conference on Data Engineering
#% 310559
#% 329537
#% 338609
#% 397383
#% 413550
#% 459006
#% 463903
#% 464839
#% 464873
#% 477791
#% 479971
#% 481290
#% 577256
#% 629623
#% 629644
#% 631926
#% 660658
#% 729933
#% 729938
#! Previous studies have presented convincing argumentsthat a frequent pattern mining algorithm should not mineall frequent patterns but only the closed ones because thelatter leads to not only more compact yet complete resultset but also better efficiency. However, most of the previouslydeveloped closed pattern mining algorithms work underthe candidate maintenance-and-test paradigm which isinherently costly in both runtime and space usage when thesupport threshold is low or the patterns become long.In this paper, we present, BIDE, an efficient algorithmfor mining frequent closed sequences without candidatemaintenance. It adopts a novel sequence closure checkingscheme called BI-Directional Extension, and prunes thesearch space more deeply compared to the previous algorithmsby using the BackScan pruning method and the Scan-Skipoptimization technique. A thorough performance studywith both sparse and dense real-life data sets has demonstratedthat BIDE significantly outperforms the previous algorithms:it consumes order(s) of magnitude less memoryand can be more than an order of magnitude faster. It isalso linearly scalable in terms of database size.

#index 765429
#* Graph indexing: a frequent structure-based approach
#@ 9413 850 961
#t 2004
#c SIGMOD '04 Proceedings of the 2004 ACM SIGMOD international conference on Management of data
#% 344549
#% 378391
#% 397359
#% 435373
#% 443133
#% 466644
#% 479465
#% 480656
#% 629603
#% 629646
#% 629708
#% 654452
#% 660000
#% 729938
#% 729942
#% 731608
#% 1015336
#! Graph has become increasingly important in modelling complicated structures and schemaless data such as proteins, chemical compounds, and XML documents. Given a graph query, it is desirable to retrieve graphs quickly from a large database via graph-based indices. In this paper, we investigate the issues of indexing graphs and propose a novel solution by applying a graph mining technique. Different from the existing path-based methods, our approach, called gIndex, makes use of frequent substructure as the basic indexing feature. Frequent substructures are ideal candidates since they explore the intrinsic characteristics of the data and are relatively stable to database updates. To reduce the size of index structure, two techniques, size-increasing support constraint and discriminative fragments, are introduced. Our performance study shows that gIndex has 10 times smaller index size, but achieves 3--10 times better performance in comparison with a typical path-based method, GraphGrep. The gIndex approach not only provides and elegant solution to the graph indexing problem, but also demonstrates how database indexing and query processing can benefit form data mining, especially frequent pattern mining. Furthermore, the concepts developed here can be applied to indexing sequences, trees, and other complicated structures as well.

#index 765494
#* MAIDS: mining alarming incidents from data streams
#@ 10033 10772 10773 961 10774 10775
#t 2004
#c SIGMOD '04 Proceedings of the 2004 ACM SIGMOD international conference on Management of data
#% 210173
#% 248790
#% 300120
#% 333925
#% 376266
#% 378388
#% 993958
#% 1015261

#index 766198
#* Mining complex matchings across Web query interfaces
#@ 9693 4553 961
#t 2004
#c Proceedings of the 9th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery
#% 22948
#% 152934
#% 227919
#% 333932
#% 333988
#% 333990
#% 452846
#% 480645
#% 572314
#% 577214
#% 654459
#% 654467
#% 727869
#% 765410
#% 1712590
#! To enable information integration, schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sourcess. As a new attempt, this paper studies such matching as a data mining problem. Specifically, while complex matchings are common, because of their far more complex search space, most existing techniques focus on simple 1:1 matchings. To tackle this challenge, this paper takes a conceptually novel approach by viewing schema matching as correlation mining, for our task of matching Web query interfaces to integrate the myriad databases on the Internet. On this "deep Web," query interfaces generally form complex matchings between attribute groups (e.g., {author} corresponds to {first name, last name} in the Books domain). We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes (e.g., {first name, last name}) tend to be co-present in query interfaces and thus positively correlated. In contrast, synonym attributes are negatively correlated because they rarely co-occur. This insight enables us to discover complex matchings by a correlation mining approach, which consists of dual mining of positive and negative correlations. We evaluate our approach on deep Web sources in several object domains (e.g., Books and Airfares) and the results show that the correlation mining approach does discover semantically meaningful matchings among attributes.

#index 769890
#* Discovering complex matchings across web query interfaces: a correlation mining approach
#@ 9693 4553 961
#t 2004
#c Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 22948
#% 152934
#% 227919
#% 333932
#% 333990
#% 452846
#% 480645
#% 572314
#% 577214
#% 654459
#% 654467
#% 727869
#% 765410
#% 1712590
#! To enable information integration, schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources. While complex matchings are common, because of their far more complex search space, most existing techniques focus on simple 1:1 matchings. To tackle this challenge, this paper takes a conceptually novel approach by viewing schema matching as correlation mining, for our task of matching Web query interfaces to integrate the myriad databases on the Internet. On this "deep Web," query interfaces generally form complex matchings between attribute groups (e.g., [author] corresponds to [first name, last name] in the Books domain). We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes (e.g., [first name, last name]) tend to be co-present in query interfaces and thus positively correlated. In contrast, synonym attributes are negatively correlated because they rarely co-occur. This insight enables us to discover complex matchings by a correlation mining approach. In particular, we develop the DCM framework, which consists of data preparation, dual mining of positive and negative correlations, and finally matching selection. Unlike previous correlation mining algorithms, which mainly focus on finding strong positive correlations, our algorithm cares both positive and negative correlations, especially the subtlety of negative correlations, due to its special importance in schema matching. This leads to the introduction of a new correlation measure, $H$-measure, distinct from those proposed in previous work. We evaluate our approach extensively and the results show good accuracy for discovering complex matchings.

#index 769927
#* On demand classification of data streams
#@ 968 961 4625 850
#t 2004
#c Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 210173
#% 310500
#% 342600
#% 378388
#% 654489
#% 729437
#% 1015261
#! Current models of the classification problem do not effectively handle bursts of particular classes coming in at different times. In fact, the current model of the classification problem simply concentrates on methods for one-pass classification modeling of very large data sets. Our model for data stream classification views the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing streams are used for dynamic classification of data sets. This model reflects real life situations effectively, since it is desirable to classify test streams in real time over an evolving training and test stream. The aim here is to create a classification system in which the training model can adapt quickly to the changes of the underlying data stream. In order to achieve this goal, we propose an on-demand classification process which can dynamically select the appropriate window of past training data to build the classifier. The empirical results indicate that the system maintains a high classification accuracy in an evolving data stream, while providing an efficient solution to the classification task.

#index 769931
#* IncSpan: incremental mining of sequential patterns in large database
#@ 9360 9413 961
#t 2004
#c Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 287242
#% 425006
#% 459006
#% 463903
#% 464204
#% 464996
#% 479971
#% 481754
#% 502121
#% 577256
#% 646296
#% 729938
#% 745515
#! Many real life sequence databases grow incrementally. It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow, or when some new sequences are added into the database. Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates. However, it is nontrivial to mine sequential patterns incrementally, especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones. In this study, we develop an efficient algorithm, IncSpan, for incremental mining of sequential patterns, by exploring some interesting properties. Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin.

#index 769946
#* Clustering moving objects
#@ 11061 961 3573
#t 2004
#c Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 114667
#% 210173
#% 236700
#% 248790
#% 248792
#% 273890
#% 282141
#% 295512
#% 296738
#% 300174
#% 397377
#% 438137
#% 450723
#% 481281
#% 566128
#% 656803
#% 729917
#% 1015297
#! Due to the advances in positioning technologies, the real time information of moving objects becomes increasingly available, which has posed new challenges to the database research. As a long-standing technique to identify overall distribution patterns in data, clustering has achieved brilliant successes in analyzing static datasets. In this paper, we study the problem of clustering moving objects, which could catch interesting pattern changes during the motion process and provide better insight into the essence of the mobile data points. In order to catch the spatial-temporal regularities of moving objects and handle large amounts of data, micro-clustering [20] is employed. Efficient techniques are proposed to keep the moving micro-clusters geographically small. Important events such as the collisions among moving micro-clusters are also identified. In this way, high quality moving micro-clusters are dynamically maintained, which leads to fast and competitive clustering result at any given time instance. We validate our approaches with a through experimental evaluation, where orders of magnitude improvement on running time is observed over normal K-Means clustering method [14].

#index 769963
#* Mining scale-free networks using geodesic clustering
#@ 11081 11082 961
#t 2004
#c Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 210173
#% 268079
#% 290830
#% 342596
#% 577217
#% 629708
#% 720338
#% 729936
#% 731608
#! Many real-world graphs have been shown to be scale-free---vertex degrees follow power law distributions, vertices tend to cluster, and the average length of all shortest paths is small. We present a new model for understanding scale-free networks based on multilevel geodesic approximation, using a new data structure called a multilevel mesh.Using this multilevel framework, we propose a new kind of graph clustering for data reduction of very large graph systems such as social, biological, or electronic networks. Finally, we apply our algorithms to real-world social networks and protein interaction graphs to show that they can reveal knowledge embedded in underlying graph structures. We also demonstrate how our data structures can be used to quickly answer approximate distance and shortest path queries on scale-free networks.

#index 785433
#* Scalable Construction of Topic Directory with Nonparametric Closed Termset Mining
#@ 8193 11878 11879 961
#t 2004
#c ICDM '04 Proceedings of the Fourth IEEE International Conference on Data Mining
#! A topic directory, e.g., Yahoo directory, provides a view of a document set at different levelsof abstraction and is ideal for the interactive exploration and visualization of the document set. We present a method that dynamically generates a topic directory from a document set usinga frequent closed termset mining algorithm. Our method shows experimental results of equal quality to recent document clustering methods and has additional benefits such as automatic generation of topic labels and determination of a clustering parameter.

#index 794534
#* The third SIGKDD workshop on mining temporal and sequential data (KDD/TDM 2004)
#@ 11945 2810 961
#t 2004
#c ACM SIGKDD Explorations Newsletter
#! In this short report, we provide a summary of the results, issues, and research directions on mining temporal and sequential data, discussed in TDM-2004, held in conjunction with the 10-th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004) on August 22, 2004 in Seattle, Washington, U.S.A.

#index 799759
#* Mining thick skylines over large databases
#@ 4232 961 2497
#t 2004
#c PKDD '04 Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases
#! People recently are interested in a new operator, called skyline [3], which returns the objects that are not dominated by any other objects with regard to certain measures in a multi-dimensional space. Recent work on the skyline operator [3,15,8,13,2] focuses on efficient computation of skylines in large databases. However, such work gives users only thin skylines, i.e., single objects, which may not be desirable in some real applications. In this paper, we propose a novel concept, called thick skyline, which recommends not only skyline objects but also their nearby neighbors within -distance. Efficient computation methods are developed including (1) two efficient algorithms, Sampling-and-Pruning and Indexing-and-Estimating, to find such thick skyline with the help of statistics or indexes in large databases, and (2) a highly efficient Microcluster-based algorithm for mining thick skyline. The Microcluster-based method not only leads to substantial savings in computation but also provides a cocise representation of the thick skyline in the case of high cardinalities. Our experimental performance study shows that the proposed methods are both efficient and effective.

#index 800533
#* Mining Closed Relational Graphs with Connectivity Constraints
#@ 9413 12120 961
#t 2005
#c ICDE '05 Proceedings of the 21st International Conference on Data Engineering
#% 313959
#% 443723
#% 729938
#% 731608

#index 800566
#* Mining Evolving Customer-Product Relationships in Multi-dimensional Space
#@ 11879 961 10548 12167
#t 2005
#c ICDE '05 Proceedings of the 21st International Conference on Data Engineering
#% 342600
#% 420053
#% 463903
#% 481290
#% 481609
#% 993958
#! Previous work on mining transactional database has focused primarily on mining frequent itemsets, association rules, and sequential patterns. However, interesting relationships between customers and items, especially their evolution with time, have not been studied thoroughly. In this paper, we propose a Gaussian transformation-based regression model that captures time-variant relationships between customers and products. Moreover, since it is interesting to discover such relationships in a multi-dimensional space, an efficient method has been developed to compute multi-dimensional aggregates of such curves in a data cube environment. Our experimental results have demonstrated the promise of the approach.

#index 810072
#* Substructure similarity search in graph databases
#@ 9413 850 961
#t 2005
#c Proceedings of the 2005 ACM SIGMOD international conference on Management of data
#% 25470
#% 121278
#% 217812
#% 251403
#% 256685
#% 260974
#% 333679
#% 344549
#% 378391
#% 408396
#% 442886
#% 443133
#% 466644
#% 765429
#% 1015336
#! Advanced database systems face a great challenge raised by the emergence of massive, complex structural data in bioinformatics, chem-informatics, and many other applications. The most fundamental support needed in these applications is the efficient search of complex structured data. Since exact matching is often too restrictive, similarity search of complex structures becomes a vital operation that must be supported efficiently.In this paper, we investigate the issues of substructure similarity search using indexed features in graph databases. By transforming the edge relaxation ratio of a query graph into the maximum allowed missing features, our structural filtering algorithm, called Grafil, can filter many graphs without performing pairwise similarity computations. It is further shown that using either too few or too many features can result in poor filtering performance. Thus the challenge is to design an effective feature set selection strategy for filtering. By examining the effect of different feature selection mechanisms, we develop a multi-filter composition strategy, where each filter uses a distinct and complementary subset of the features. We identify the criteria to form effective feature sets for filtering, and demonstrate that combining features with similar size and selectivity can improve the filtering and search performance significantly. Moreover, the concept presented in Grafil can be applied to searching approximate non-consecutive sequences, trees, and other complicated structures as well.

#index 810094
#* GraphMiner: a structural pattern-mining system for large disk-based graph databases and its applications
#@ 1204 11027 11028 11029 3214 9413 961
#t 2005
#c Proceedings of the 2005 ACM SIGMOD international conference on Management of data
#% 466644
#% 629603
#% 629646
#% 629708
#% 729938
#% 769907
#! Mining frequent structural patterns from graph databases is an important research problem with broad applications. Recently, we developed an effective index structure, ADI, and efficient algorithms for mining frequent patterns from large, disk-based graph databases [5], as well as constraint-based mining techniques. The techniques have been integrated into a research prototype system--- GraphMiner. In this paper, we describe a demo of GraphMiner which showcases the technical details of the index structure and the mining algorithms including their efficient implementation, the mining performance and the comparison with some state-of-the-art methods, the constraint-based graph-pattern mining techniques and the procedure of constrained graph mining, as well as mining real data sets in novel applications.

#index 823356
#* Summarizing itemset patterns: a profile-based approach
#@ 9413 9360 961 12167
#t 2005
#c Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining
#% 152934
#% 231941
#% 237200
#% 248791
#% 262059
#% 287285
#% 338609
#% 342610
#% 452846
#% 463903
#% 478770
#% 577214
#% 629606
#% 629644
#% 722934
#% 727667
#% 727896
#% 742493
#% 765429
#% 769876
#% 769905
#! Frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets, sequences, and graphs. However, the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability, due to the huge number of patterns generated by the mining process.In this paper, we examine how to summarize a collection of itemset patterns using only K representatives, a small number of patterns that a user can handle easily. The K representatives should not only cover most of the frequent patterns but also approximate their supports. A generative model is built to extract and profile these representatives, under which the supports of the patterns can be easily recovered without consulting the original dataset. Based on the restoration error, we propose a quality measure function to determine the optimal value of parameter K. Polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement. Empirical studies indicate that we can obtain compact summarization in real datasets.

#index 823357
#* Mining closed relational graphs with connectivity constraints
#@ 9413 12120 961
#t 2005
#c Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining
#% 237380
#% 282226
#% 310514
#% 313959
#% 443723
#% 465003
#% 466644
#% 629603
#% 629646
#% 629708
#% 729933
#% 729938
#% 729942
#% 729984
#% 731608
#% 742493
#% 765429
#! Relational graphs are widely used in modeling large scale networks such as biological networks and social networks. In this kind of graph, connectivity becomes critical in identifying highly associated groups and clusters. In this paper, we investigate the issues of mining closed frequent graphs with connectivity constraints in massive relational graphs where each graph has around 10K nodes and 1M edges. We adopt the concept of edge connectivity and apply the results from graph theory, to speed up the mining process. Two approaches are developed to handle different mining requests: CloseCut, a pattern-growth approach, and splat, a pattern-reduction approach. We have applied these methods in biological datasets and found the discovered patterns interesting.

#index 823359
#* Cross-relational clustering with user's guidance
#@ 10548 961 850
#t 2005
#c Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining
#% 273891
#% 280419
#% 300131
#% 316483
#% 345824
#% 376266
#% 464291
#% 466410
#% 481281
#% 550556
#% 722929
#% 745491
#% 771842
#% 771944
#% 993987
#! Clustering is an essential data mining task with numerous applications. However, data in most real-life applications are high-dimensional in nature, and the related information often spreads across multiple relations. To ensure effective and efficient high-dimensional, cross-relational clustering, we propose a new approach, called CrossClus, which performs cross-relational clustering with user's guidance. We believe that user's guidance, even likely in very simple forms, could be essential for effective high-dimensional clustering since a user knows well the application requirements and data semantics. CrossClus is carried out as follows: A user specifies a clustering task and selects one or a small set of features pertinent to the task. CrossClus extracts the set of highly relevant features in multiple relations connected via linkages defined in the database schema, evaluates their effectiveness based on user's guidance, and identifies interesting clusters that fit user's needs. This method takes care of both quality in feature extraction and efficiency in clustering. Our comprehensive experiments demonstrate the effectiveness and scalability of this approach.

#index 823384
#* Parallel mining of closed sequential patterns
#@ 13881 961 13882
#t 2005
#c Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining
#% 310559
#% 329537
#% 329600
#% 338580
#% 459006
#% 463903
#% 464996
#% 479971
#% 729938
#% 745515
#% 769620
#! Discovery of sequential patterns is an essential data mining task with broad applications. Among several variations of sequential patterns, closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it. Unfortunately, there is no parallel closed sequential pattern mining method proposed yet. In this paper we develop an algorithm, called Par-CSP (Parallel Closed Sequential Pattern mining), to conduct parallel mining of closed sequential patterns on a distributed memory system. Par-CSP partitions the work among the processors by exploiting the divide-and-conquer property so that the overhead of interprocessor communication is minimized. Par-CSP applies dynamic scheduling to avoid processor idling. Moreover, it employs a technique, called selective sampling to address the load imbalance problem. We implement Par-CSP using MPI on a 64-node Linux cluster. Our experimental results show that Par-CSP attains good parallelization efficiencies on various input datasets.

#index 824710
#* Mining compressed frequent-pattern sets
#@ 12167 961 9413 9360
#t 2005
#c VLDB '05 Proceedings of the 31st international conference on Very large data bases
#% 152934
#% 227919
#% 248791
#% 280409
#% 300120
#% 410276
#% 420063
#% 461909
#% 463903
#% 466664
#% 479484
#% 481290
#% 729933
#% 769876
#! A major challenge in frequent-pattern mining is the sheer size of its mining results. In many cases, a high min_sup threshold may discover only commonsense patterns but a low one may generate an explosive number of output patterns, which severely restricts its usage.In this paper, we study the problem of compressing frequent-pattern sets. Typically, frequent patterns can be clustered with a tightness measure δ (called δ-cluster), and a representative pattern can be selected for each cluster. Unfortunately, finding a minimum set of representative patterns is NP-Hard. We develop two greedy methods, RPglobal and RPlocal. The former has the guaranteed compression bound but higher computational complexity. The latter sacrifices the theoretical bounds but is far more efficient. Our performance study shows that the compression quality using RPlocal is very close to RPglobal, and both can reduce the number of closed frequent patterns by almost two orders of magnitude. Furthermore, RPlocal mines even faster than FPClose[11], a very fast closed frequent-pattern mining method. We also show that RPglobal and RPlocal can be combined together to balance the quality and efficiency.

#index 824793
#* A tribute to Professor Hongjun Lu
#@ 98 961
#t 2005
#c ACM SIGMOD Record
#! Dr. Hongjun Lu, Professor of Computer Science, Hong Kong University of Science and Technology, lost his brave fight against cancer and left us in the evening of March 3, 2005. The world lost a dedicated and brilliant computer scientist. The database research community lost a respected and prolific researcher, an effortless organizer and promoter of database research in the world, a friend cherished by many colleagues and researchers, and a wonderful teacher who deeply affected and was revered by his students.

#index 850729
#* Graph indexing based on discriminative frequent structure analysis
#@ 9413 850 961
#t 2005
#c ACM Transactions on Database Systems (TODS) - Special Issue: SIGMOD/PODS 2004
#% 344549
#% 378391
#% 397359
#% 435374
#% 443133
#% 466644
#% 479465
#% 480656
#% 481290
#% 481754
#% 481779
#% 601159
#% 629603
#% 629646
#% 629708
#% 654452
#% 729938
#% 729942
#% 731608
#% 1015336
#! Graphs have become increasingly important in modelling complicated structures and schemaless data such as chemical compounds, proteins, and XML documents. Given a graph query, it is desirable to retrieve graphs quickly from a large database via indices. In this article, we investigate the issues of indexing graphs and propose a novel indexing model based on discriminative frequent structures that are identified through a graph mining process. We show that the compact index built under this model can achieve better performance in processing graph queries. Since discriminative frequent structures capture the intrinsic characteristics of the data, they are relatively stable to database updates, thus facilitating sampling-based feature extraction and incremental index maintenance. Our approach not only provides an elegant solution to the graph indexing problem, but also demonstrates how database indexing and query processing can benefit from data mining, especially frequent pattern mining. Furthermore, the concepts developed here can be generalized and applied to indexing sequences, trees, and other complicated structures as well.

#index 864391
#* C-Cubing: Efficient Computation of Closed Cubes by Aggregation-Based Checking
#@ 12167 15094 961 6736
#t 2006
#c ICDE '06 Proceedings of the 22nd International Conference on Data Engineering
#! It is well recognized that data cubing often produces huge outputs. Two popular efforts devoted to this problem are (1) iceberg cube, where only significant cells are kept, and (2) closed cube, where a group of cells which preserve roll-up/drill-down semantics are losslessly compressed to one cell. Due to its usability and importance, efficient computation of closed cubes still warrants a thorough study. In this paper, we propose a new measure, called closedness, for efficient closed data cubing. We show that closedness is an algebraic measure and can be computed efficiently and incrementally. Based on closedness measure, we develop an an aggregation-based approach, called C-Cubing (i.e., Closed-Cubing), and integrate it into two successful iceberg cubing algorithms: MM-Cubing and Star-Cubing. Our performance study shows that C-Cubing runs almost one order of magnitude faster than the previous approaches. We further study how the performance of the alternative algorithms of C-Cubing varies w.r.t the properties of the data sets.

#index 864470
#* Warehousing and Analyzing Massive RFID Data Sets
#@ 15163 961 11879 15164
#t 2006
#c ICDE '06 Proceedings of the 22nd International Conference on Data Engineering
#! Radio Frequency Identification (RFID) applications are set to play an essential role in object tracking and supply chain management systems. In the near future, it is expected that every major retailer will use RFID systems to track the movement of products from suppliers to warehouses, store backrooms and eventually to points of sale. The volume of information generated by such systems can be enormous as each individual item (a pallet, a case, or an SKU) will leave a trail of data as it moves through different locations. As a departure from the traditional data cube, we propose a new warehousing model that preserves object transitions while providing significant compression and path-dependent aggregates, based on the following observations: (1) items usually move together in large groups through early stages in the system (e.g., distribution centers) and only in later stages (e.g., stores) do they move in smaller groups, and (2) although RFID data is registered at the primitive level, data analysis usually takes place at a higher abstraction level. Techniques for summarizing and indexing data, and methods for processing a variety of queries based on this framework are developed in this study. Our experiments demonstrate the utility and feasibility of our design, data structure, and algorithms.

#index 864475
#* Searching Substructures with Superimposed Distance
#@ 9413 15170 961 850
#t 2006
#c ICDE '06 Proceedings of the 22nd International Conference on Data Engineering
#! Efficient indexing techniques have been developed for the exact and approximate substructure search in large scale graph databases. Unfortunately, the retrieval problem of structures with categorical or geometric distance constraints is not solved yet. In this paper, we develop a method called PIS (Partition-based Graph Index and Search) to support similarity search on substructures with superimposed distance constraints. PIS selects discriminative fragments in a query graph and uses an index to prune the graphs that violate the distance constraints. We identify a criterion to distinguish the selectivity of fragments in multiple graphs and develop a partition method to obtain a set of highly selective fragments, which is able to improve the pruning performance. Experimental results show that PIS is effective in processing real graph queries.

#index 864493
#* Mining, Indexing, and Similarity Search in Graphs and Complex Structures
#@ 961 9413 850
#t 2006
#c ICDE '06 Proceedings of the 22nd International Conference on Data Engineering
#! Scalable methods for mining, indexing, and similarity search in graphs and other complex structures, such as trees, lattices, and networks, have become increasingly important in data mining and database management. This is because a large set of emerging applications need to handle new kinds of objects with complex structures, such as trees (e.g., XML data), graphs (e.g., Web, chemical structures and biological graphs) and networks (e.g., social and biological networks). Such complicated data structures pose many new challenging research problems related to data mining, data management, and similarity search that do not exist in the traditional database and data mining studies.

#index 864498
#* On the Inverse Classification Problem and its Applications
#@ 968 15196 961
#t 2006
#c ICDE '06 Proceedings of the 22nd International Conference on Data Engineering
#! In this paper, we discuss the inverse classification problem, in which we desire to define the features of an incomplete record in such a way that will result in a desired class label. Such an approach is useful in applications in which it is an objective to determine a set of actions to be taken in order to guide the data mining application towards a desired solution. This system can be used for a variety of decision support applications which have pre-determined task criteria.

#index 864501
#* Top-Down Mining of Interesting Patterns from Very High Dimensional Data
#@ 6736 961 12167 15094
#t 2006
#c ICDE '06 Proceedings of the 22nd International Conference on Data Engineering
#! Many real world applications deal with transactional data, characterized by a huge number of transactions (tuples) with a small number of dimensions (attributes). However, there are some other applications that involve rather high dimensional data with a small number of tuples. Examples of such applications include bioinformatics, survey-based statistical analysis, text processing, and so on. High dimensional data pose great challenges to most existing data mining algorithms. Although there are numerous algorithms dealing with transactional data sets, there are few algorithms oriented to very high dimensional data sets with a relatively small number of tuples.

#index 875001
#* Ranking objects based on relationships
#@ 2164 2136 961 12167
#t 2006
#c Proceedings of the 2006 ACM SIGMOD international conference on Management of data
#% 169784
#% 278831
#% 387427
#% 479469
#% 479816
#% 480330
#% 631988
#% 643566
#% 659990
#% 660011
#% 805896
#% 993987
#% 1015317
#% 1016176
#! In many document collections, documents are related to objects such as document authors, products described in the document, or persons referred to in the document. In many applications, the goal is to find these objects that best match a set of keywords. However, the keywords may not necessarily occur in the target objects; they occur only in the documents. For example, in a product review database, a user might search for names of products (say, laptops) using keywords like "lightweight" and "business use" that occur only in the reviews but not in the names of laptops. In order to answer these queries, we need to exploit relationships between documents containing the keywords and the target objects related to those documents. Current keyword query paradigms do not exploit these relationships effectively and hence are inefficient for these queries.In this paper, we consider a class of queries called the "object finder" queries. Our main intuition is to exploit the relationships between searchable documents and related objects and further "aggregate" the document scores from these relationships in order to find the best ranking target objects. Building upon existing keyword search engines such as full text search, we design efficient algorithms that exploit the requirement of only the best k target objects to terminate early. The main challenge here is to push early termination through blocking operators such as group by and aggregation. Our experiments with real datasets and workloads demonstrate the effectiveness of our techniques. Although we present our techniques in the context of keyword search, our techniques apply to other types of ranked searches (e.g., multimedia search) as well.

#index 879653
#* Tensor space model for document analysis
#@ 8278 10859 961
#t 2006
#c SIGIR '06 Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval
#% 321635
#% 846431
#! Vector Space Model (VSM) has been at the core of information retrieval for the past decades. VSM considers the documents as vectors in high dimensional space.In such a vector space, techniques like Latent Semantic Indexing (LSI), Support Vector Machines (SVM), Naive Bayes, etc., can be then applied for indexing and classification. However, in some cases, the dimensionality of the document space might be extremely large, which makes these techniques infeasible due to the curse of dimensionality. In this paper, we propose a novel Tensor Space Model for document analysis. We represent documents as the second order tensors, or matrices. Correspondingly, a novel indexing algorithm called Tensor Latent Semantic Indexing (TensorLSI) is developed in the tensor space. Our theoretical analysis shows that TensorLSI is much more computationally efficient than the conventional Latent Semantic Indexing, which makes it applicable for extremely large scale data set. Several experimental results on standard document data sets demonstrate the efficiency and effectiveness of our algorithm.

#index 881489
#* Generating semantic annotations for frequent patterns with context analysis
#@ 13840 12167 9360 961 3494
#t 2006
#c Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 144034
#% 152934
#% 227919
#% 248791
#% 287285
#% 321635
#% 342629
#% 420481
#% 463903
#% 629644
#% 629708
#% 727896
#% 729418
#% 769876
#% 823356
#% 824710
#! As a fundamental data mining task, frequent pattern mining has widespread applications in many different domains. Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important nextstep - interpreting the discovered frequent patterns. Although some recent work has studied the compression and summarization of frequent patterns, the proposed techniques can only annotate a frequent pattern with non-semantical information (e.g. support), which provides only limited help for a user to understand the patterns.In this paper, we propose the novel problem of generating semantic annotations for frequent patterns. The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. We propose a general approach to generate such anannotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has potentially many applications such as generating a dictionary-like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on different datasets show that our approach is effective in generating semantic pattern annotations.

#index 881500
#* Extracting redundancy-aware top-k patterns
#@ 12167 9360 9413 961
#t 2006
#c Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 36672
#% 71901
#% 158687
#% 262112
#% 281656
#% 443092
#% 478770
#% 479816
#% 577214
#% 629644
#% 769876
#% 769893
#% 818209
#% 823344
#% 823356
#% 824710
#% 835872
#% 1845364
#! Observed in many applications, there is a potential need of extracting a small set of frequent patterns having not only high significance but also low redundancy. The significance is usually defined by the context of applications. Previous studies have been concentrating on how to compute top-k significant patterns or how to remove redundancy among patterns separately. There is limited work on finding those top-k patterns which demonstrate high-significance and low-redundancy simultaneously.In this paper, we study the problem of extracting redundancy-aware top-k patterns from a large collection of frequent patterns. We first examine the evaluation functions for measuring the combined significance of a pattern set and propose the MMS (Maximal Marginal Significance) as the problem formulation. The problem is known as NP-hard. We further present a greedy algorithm which approximates the optimal solution with performance bound O(log k) (with conditions on redundancy), where k is the number of reported patterns. The direct usage of redundancy-aware top-k patterns is illustrated through two real applications: disk block prefetch and document theme extraction. Our method can also be applied to processing redundancy-aware top-k queries in traditional database.

#index 881549
#* Discovering interesting patterns through user's interactive feedback
#@ 12167 9571 13840 961
#t 2006
#c Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 36672
#% 309208
#% 443092
#% 577224
#% 801616
#% 818209
#% 823336
#% 823360
#% 824710
#% 1272396
#! In this paper, we study the problem of discovering interesting patterns through user's interactive feedback. We assume a set of candidate patterns (ie, frequent patterns) has already been mined. Our goal is to help a particular user effectively discover interesting patterns according to his specific interest. Without requiring a user to explicitly construct a prior knowledge to measure the interestingness of patterns, we learn the user's prior knowledge from his interactive feedback. We propose two models to represent a user's prior: the log linear model and biased belief model. The former is designed for item-set patterns, whereas the latter is also applicable to sequential and structural patterns. To learn these models, we present a two-stage approach, progressive shrinking and clustering, to select sample patterns for feedback. The experimental results on real and synthetic data sets demonstrate the effectiveness of our approach.

#index 881567
#* GPLAG: detection of software plagiarism by program dependence graph analysis
#@ 16155 15196 961 850
#t 2006
#c Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 19622
#% 288990
#% 408396
#% 448355
#% 600046
#% 622212
#% 622419
#% 625932
#% 632029
#% 637298
#% 654447
#% 729938
#% 823218
#! Along with the blossom of open source projects comes the convenience for software plagiarism. A company, if less self-disciplined, may be tempted to plagiarize some open source projects for its own products. Although current plagiarism detection tools appear sufficient for academic use, they are nevertheless short for fighting against serious plagiarists. For example, disguises like statement reordering and code insertion can effectively confuse these tools. In this paper, we develop a new plagiarism detection tool, called GPLAG, which detects plagiarism by mining program dependence graphs (PDGs). A PDG is a graphic representation of the data and control dependencies within a procedure. Because PDGs are nearly invariant during plagiarism, GPLAG is more effective than state-of-the-art tools for plagiarism detection. In order to make GPLAG scalable to large programs, a statistical lossy filter is proposed to prune the plagiarism search space. Experiment study shows that GPLAG is both effective and efficient: It detects plagiarism that easily slips over existing tools, and it usually takes a few seconds to find (simulated) plagiarism in programs having thousands of lines of code.

#index 893108
#* Towards robust indexing for ranked queries
#@ 12167 15196 961
#t 2006
#c VLDB '06 Proceedings of the 32nd international conference on Very large data bases
#% 213981
#% 237205
#% 248010
#% 248028
#% 288976
#% 300180
#% 333854
#% 333951
#% 341704
#% 379902
#% 465167
#% 479816
#! Top-k query asks for k tuples ordered according to a specific ranking function that combines the values from multiple participating attributes. The combined score function is usually linear. To efficiently answer top-k queries, preprocessing and indexing the data have been used to speed up the run time performance. Many indexing methods allow the online query algorithms progressively retrieve the data and stop at a certain point. However, in many cases, the number of data accesses is sensitive to the query parameters (i.e., linear weights in the score functions).In this paper, we study the sequentially layered indexing problem where tuples are put into multiple consecutive layers and any top-k query can be answered by at most k layers of tuples. We propose a new criterion for building the layered index. A layered index is robust if for any k, the number of tuples in the top k layers is minimal in comparison with all the other alternatives. The robust index guarantees the worst case performance for arbitrary query parameters. We derive a necessary and sufficient condition for robust index. The problem is shown solvable within O(ndlog n) (where d is the number of dimensions, and n is the number of tuples). To reduce the high complexity of the exact solution, we develop an approximate approach, which has time complexity O(2d n(log n)r(d)-1), where r(d) = ⌈d/2⌉ + ⌊d/2⌋ ⌈d/2⌉. Our experimental results show that our proposed method outperforms the best known previous methods.

#index 893124
#* LinkClus: efficient clustering via heterogeneous semantic links
#@ 10548 961 850
#t 2006
#c VLDB '06 Proceedings of the 32nd international conference on Very large data bases
#% 152934
#% 210173
#% 248790
#% 273891
#% 283833
#% 469422
#% 481281
#% 550556
#% 577273
#% 629644
#% 643009
#% 729918
#% 729933
#% 769883
#% 805904
#% 823359
#% 835018
#% 840840
#! Data objects in a relational database are cross-linked with each other via multi-typed links. Links contain rich semantic information that may indicate important relationships among objects. Most current clustering methods rely only on the properties that belong to the objects per se. However, the similarities between objects are often indicated by the links, and desirable clusters cannot be generated using only the properties of objects.In this paper we explore linkage-based clustering, in which the similarity between two objects is measured based on the similarities between the objects linked with them. In comparison with a previous study (SimRank) that computes links recursively on all pairs of objects, we take advantage of the power law distribution of links, and develop a hierarchical structure called SimTree to represent similarities in multi-granularity manner. This method avoids the high cost of computing and storing pairwise similarities but still thoroughly explore relationships among objects. An efficient algorithm is proposed to compute similarities between objects by avoiding pairwise similarity computations through merging computations that go through the same branches in the SimTree. Experiments show the proposed approach achieves high efficiency, scalability, and accuracy in clustering multi-typed linked objects.

#index 893127
#* Answering top-k queries with multi-dimensional selections: the ranking cube approach
#@ 12167 961 9360 11879
#t 2006
#c VLDB '06 Proceedings of the 32nd international conference on Very large data bases
#% 43163
#% 223781
#% 227861
#% 227894
#% 237205
#% 248010
#% 248028
#% 248814
#% 300180
#% 333854
#% 333951
#% 397378
#% 399762
#% 427219
#% 480329
#% 765418
#% 810018
#% 824734
#% 993958
#% 1016173
#! Observed in many real applications, a top-k query often consists of two components to reflect a user's preference: a selection condition and a ranking function. A user may not only propose ad hoc ranking functions, but also use different interesting subsets of the data. In many cases, a user may want to have a thorough study of the data by initiating a multi-dimensional analysis of the top-k query results. Previous work on top-k query processing mainly focuses on optimizing data access according to the ranking function only. The problem of efficient answering top-k queries with multi-dimensional selections has not been well addressed yet.This paper proposes a new computational model, called ranking cube, for efficient answering top-k queries with multi-dimensional selections. We define a rank-aware measure for the cube, capturing our goal of responding to multi-dimensional ranking analysis. Based on the ranking cube, an efficient query algorithm is developed which progressively retrieves data blocks until the top-k results are found. The curse of dimensionality is a well-known challenge for the data cube and we cope with this difficulty by introducing a new technique of ranking fragments. Our experiments on Microsoft's SQL Server 2005 show that our proposed approaches have significant improvement over the previous methods.

#index 893157
#* Flowcube: constructing RFID flowcubes for multi-dimensional analysis of commodity flows
#@ 15163 961 11879
#t 2006
#c VLDB '06 Proceedings of the 32nd international conference on Very large data bases
#% 210182
#% 273916
#% 333925
#% 342666
#% 459021
#% 466593
#% 466716
#% 479646
#% 481588
#% 481758
#% 481951
#% 501226
#% 644230
#% 749034
#% 864470
#% 1015294
#% 1016228
#! With the advent of RFID (Radio Frequency Identication) technology, manufacturers, distributors, and retailers will be able to track the movement of individual objects throughout the supply chain. The volume of data generated by a typical RFID application will be enormous as each item will generate a complete history of all the individual locations that it occupied at every point in time, possibly from a specific production line at a given factory, passing through multiple warehouses, and all the way to a particular checkout counter in a store. The movement trails of such RFID data form gigantic commodity flowgraph representing the locations and durations of the path stages traversed by each item. This commodity flow contains rich multi-dimensional information on the characteristics, trends, changes and outliers of commodity movements.In this paper, we propose a method to construct a warehouse of commodity flows, called flowcube. As in standard OLAP, the model will be composed of cuboids that aggregate item flows at a given abstraction level. The flowcube differs from the traditional data cube in two major ways. First, the measure of each cell will not be a scalar aggregate but a commodity flowgraph that captures the major movement trends and significant deviations of the items aggregated in the cell. Second, each flowgraph itself can be viewed at multiple levels by changing the level of abstraction of path stages. In this paper, we motivate the importance of the model, and present an efficient method to compute it by (1) performing simultaneous aggregation of paths to all interesting abstraction levels, (2) pruning low support path segments along the item and path stage abstraction lattices, and (3) compressing the cube by removing rarely occurring cells, and cells whose commodity flows can be inferred from higher level cells.

#index 907502
#* Mining compressed commodity workflows from massive RFID data sets
#@ 15163 961 11879
#t 2006
#c CIKM '06 Proceedings of the 15th ACM international conference on Information and knowledge management
#% 459021
#% 466593
#% 466716
#% 487251
#% 560492
#% 629644
#% 729933
#% 749034
#% 864470
#% 864527
#% 893157
#% 903341
#! Radio Frequency Identification (RFID) technology is fast becoming a prevalent tool in tracking commodities in supply chain management applications. The movement of commodities through the supply chain forms a gigantic workflow that can be mined for the discovery of trends, flow correlations and outlier paths, that in turn can be valuable in understanding and optimizing business processes.In this paper, we propose a method to construct compressed probabilistic workflows that capture the movement trends and significant exceptions of the overall data sets, but with a size that is substantially smaller than that of the complete RFID workflow. Compression is achieved based on the following observations: (1) only a relatively small minority of items deviate from the general trend, (2)only truly non-redundant deviations, ie, those that substantially deviate from the previously recorded ones, are interesting, and (3) although RFID data is registered at the primitive level, data analysis usually takes place at a higher abstraction level. Techniques for workflow compression based on non-redundant transition and emission probabilities are derived; and an algorithm for computing approximate path probabilities is developed. Our experiments demonstrate the utility and feasibility of our design, data structure, and algorithms.

#index 915216
#* AC-Close: Efficiently Mining Approximate Closed Itemsets by Core Pattern Recovery
#@ 9360 850 961
#t 2006
#c ICDM '06 Proceedings of the Sixth International Conference on Data Mining
#! Recent studies have proposed methods to discover approximate frequent itemsets in the presence of random noise. By relaxing the rigid requirement of exact frequent pattern mining, some interesting patterns, which would previously be fragmented by exact pattern mining methods due to the random noise or measurement error, are successfully recovered. Unfortunately, a large number of "uninteresting" candidates are explored as well during the mining process, as a result of the relaxed pattern mining methodology. This severely slows down the mining process. Even worse, it is hard for an end user to distinguish the recovered interesting patterns from these uninteresting ones. In this paper, we propose an efficient algorithm AC-Close to recover the approximate closed itemsets from "core patterns". By focusing on the so-called core patterns, integrated with a top-down mining and several effective pruning strategies, the algorithm narrows down the search space to those potentially interesting ones. Experimental results show that AC-Close substantially outperforms the previously proposed method in terms of efficiency, while delivers a similar set of interesting recovered patterns.

#index 915296
#* How Bayesians Debug
#@ 16155 17625 961
#t 2006
#c ICDM '06 Proceedings of the Sixth International Conference on Data Mining
#! Manual debugging is expensive. And the high cost has motivated extensive research on automated fault lo- calization in both software engineering and data mining communities. Fault localization aims at automatically locating likely fault locations, and hence assists manual debugging. A number of fault localization algorithms have been developed in recent years, which prove effec- tive when multiple failing and passing cases are avail- able. However, we notice what is more commonly en- countered in practice is the two-sample debugging prob- lem, where only one failing and one passing cases are available. This problem has been either overlooked or insufficiently tackled in previous studies. In this paper, we develop a new fault localization al- gorithm, named BayesDebug, which simulates some manual debugging principles through a Bayesian ap- proach. Different from existing approaches that base fault analysis on multiple passing and failing cases, BayesDebug only requires one passing and one failing cases. We reason about why BayesDebug fits the two- sample debugging problem and why other approaches do not. Finally, an experiment with a real-world program grep-2.2 is conducted, which exemplifies the effective- ness of BayesDebug.

#index 960242
#* Progressive and selective merge: computing top-k with ad-hoc ranking functions
#@ 12167 961 17858
#t 2007
#c Proceedings of the 2007 ACM SIGMOD international conference on Management of data
#% 18614
#% 152937
#% 223781
#% 248010
#% 248804
#% 273916
#% 322884
#% 333854
#% 393907
#% 397608
#% 465167
#% 727671
#% 796217
#% 806212
#% 824704
#% 875000
#% 875001
#% 893120
#% 893127
#% 893128
#! The family of threshold algorithm (ie, TA) has been widely studied for efficiently computing top-k queries. TA uses a sort-merge framework that assumes data lists are pre-sorted, and the ranking functions are monotone. However, in many database applications, attribute values are indexed by tree-structured indices (eg, B-tree, R-tree), and the ranking functions are not necessarily monotone. To answer top-k queries with ad-hoc ranking functions, this paper studies anindex-merge paradigm that performs progressive search over the space of joint states composed by multiple index nodes. We address two challenges for efficient query processing. First, to minimize the search complexity, we present a double-heap algorithm which supports not only progressive state search but also progressive state generation. Second, to avoid unnecessary disk access, we characterize a type of "empty-state" that does not contribute to the final results, and propose a new materialization model, join-signature, to prune empty-states. Our performance study shows that the proposed method achieves one order of magnitude speed-up over baseline solutions.

#index 960283
#* Trajectory clustering: a partition-and-group framework
#@ 12241 961 3879
#t 2007
#c Proceedings of the 2007 ACM SIGMOD international conference on Management of data
#% 210173
#% 273890
#% 280416
#% 427199
#% 566128
#% 659971
#% 732531
#% 769883
#% 769896
#% 810049
#% 818916
#% 881456
#% 993965
#! Existing trajectory clustering algorithms group similar trajectories as a whole, thus discovering common trajectories. Our key observation is that clustering trajectories as a whole could miss common sub-trajectories. Discovering common sub-trajectories is very useful in many applications, especially if we have regions of special interest for analysis. In this paper, we propose a new partition-and-group framework for clustering trajectories, which partitions a trajectory into a set of line segments, and then, groups similar line segments together into a cluster. The primary advantage of this framework is to discover common sub-trajectories from a trajectory database. Based on this partition-and-group framework, we develop a trajectory clustering algorithm TRACLUS. Our algorithm consists of two phases: partitioning and grouping. For the first phase, we present a formal trajectory partitioning algorithm using the minimum description length(MDL) principle. For the second phase, we present a density-based line-segment clustering algorithm. Experimental results demonstrate that TRACLUS correctly discovers common sub-trajectories from real trajectory data.

#index 989682
#* Truth discovery with multiple conflicting information providers on the web
#@ 10548 961 850
#t 2007
#c Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 282905
#% 799636
#! The world-wide web has become the most important information source for most of us. Unfortunately, there is no guarantee for the correctness of information on the web. Moreover, different web sites often provide conflicting information on a subject, such as different specifications for the same product. In this paper we propose a new problem called Veracity, i.e., conformity to truth, which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites. We design a general framework for the Veracity problem, and invent an algorithm called TruthFinder, which utilizes the relationships between web sites and their information, i.e., a web site is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy web sites. Our experiments show that TruthFinder successfully finds true facts among conflicting information, and identifies trustworthy web sites better than the popular search engines.

#index 993958
#* Multi-dimensional regression analysis of time-series data streams
#@ 13317 171 961 2966 4625
#t 2002
#c VLDB '02 Proceedings of the 28th international conference on Very Large Data Bases
#% 172949
#% 210182
#% 223781
#% 227880
#% 273916
#% 333925
#% 333926
#% 333931
#% 342600
#% 420053
#% 428155
#% 459025
#% 463903
#% 464851
#% 464994
#% 477968
#% 480156
#% 480628
#% 480630
#% 480820
#% 481609
#% 481611
#% 481951
#% 594012
#! Real-time production systems and other dynamic environments often generate tremendous (potentially infinite) amount of stream data; the volume of data is too huge to be stored on disks or scanned multiple times. Can we perform on-line, multi-dimensional analysis and data mining of such data to alert people about dramatic changes of situations and to initiate timely, high-quality responses? This is a challenging task. In this paper, we investigate methods for on-line, multi-dimensional regression analysis of time-series stream data, with the following contributions: (1) our analysis shows that only a small number of compressed regression measures instead of the complete stream of data need to be registered for multi-dimensional linear regression analysis, (2) to facilitate on-line stream data analysis, a partially materialized data cube model, with regression as measure, and a tilt time frame as its time dimension, is proposed to minimize the amount of data to be retained in memory or stored on disks, and (3) an exception-guided drilling approach is developed for on-line, multi-dimensional exception-based regression analysis. Based on this design, algorithms are proposed for efficient analysis of time-series data streams. Our performance study compares the proposed algorithms and identifies the most memory- and time- efficient one for multi-dimensional stream data analysis.

#index 993996
#* Quotient cube: how to summarize the semantics of a data cube
#@ 190 3214 961
#t 2002
#c VLDB '02 Proceedings of the 28th international conference on Very Large Data Bases
#% 210182
#% 227880
#% 236410
#% 237202
#% 259995
#% 273916
#% 280448
#% 397388
#% 464215
#% 479450
#% 480820
#% 481290
#% 481951
#! Partitioning a data cube into sets of cells with "similar behavior" often better exposes the semantics in the cube. E.g., if we find that average boots sales in the West 10th store of Walmart was the same for winter as for the whole year, it signifies something interesting about the trend of boots sales in that location in that year. In this paper, we are interested in finding succinct summaries of the data cube, exploiting regularities present in the cube, with a clear basis. We would like the summary: (i) to be as concise as possible, (ii) to itself form a lattice preserving the rollup/drilldown semantics of the cube, and (iii) to allow the original cube to be fully recovered. We illustrate the utility of solving this problem and discuss the inherent challenges. We develop techniques for partitioning cube cells for obtaining succinct summaries, and introduce the quotient cube. We give efficient algorithms for computing it from a base table. For monotone aggregate functions (e.g., COUNT, MIN, MAX, SUM on non-negative measures, etc.), our solution is optimal (i.e., quotient cube of the least size). For nonmonotone functions (e.g., AVG), we obtain a locally optimal solution. We experimentally demonstrate the efficacy of our ideas and techniques and the scalability of our algorithms.

#index 1001362
#* Semantic annotation of frequent patterns
#@ 13840 12167 9360 961 3494
#t 2007
#c ACM Transactions on Knowledge Discovery from Data (TKDD)
#% 78171
#% 115608
#% 144034
#% 152934
#% 227919
#% 248791
#% 287285
#% 288885
#% 290830
#% 321635
#% 342597
#% 342629
#% 420481
#% 463903
#% 577224
#% 577285
#% 629644
#% 629708
#% 719598
#% 727896
#% 729418
#% 766412
#% 769876
#% 823356
#% 824710
#% 840846
#% 881472
#% 976826
#! Using frequent patterns to analyze data has been one of the fundamental approaches in many data mining applications. Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important next step—interpreting the discovered frequent patterns. Although the compression and summarization of frequent patterns has been studied in some recent work, the proposed techniques there can only annotate a frequent pattern with nonsemantical information (e.g., support), which provides only limited help for a user to understand the patterns. In this article, we study the novel problem of generating semantic annotations for frequent patterns. The goal is to discover the hidden meanings of a frequent pattern by annotating it with in-depth, concise, and structured information. We propose a general approach to generate such an annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach can well incorporate the user's prior knowledge, and has potentially many applications, such as generating a dictionary-like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on different datasets show that our approach is effective in generating semantic pattern annotations.

#index 1015261
#* A framework for clustering evolving data streams
#@ 968 961 4625 850
#t 2003
#c VLDB '03 Proceedings of the 29th international conference on Very large data bases - Volume 29
#% 36672
#% 210173
#% 248790
#% 273890
#% 310488
#% 310500
#% 320942
#% 378388
#% 481281
#% 594012
#% 654489
#% 659972
#! The clustering problem is a difficult problem for the data stream domain. This is because the large volumes of data arriving in a stream renders most traditional algorithms too inefficient. In recent years, a few one-pass clustering algorithms have been developed for the data stream problem. Although such methods address the scalability issues of the clustering problem, they are generally blind to the evolution of the data and do not address the following issues: (1) The quality of the clusters is poor when the data evolves considerably over time. (2) A data stream clustering algorithm requires much greater functionality in discovering and exploring clusters over different portions of the stream. The widely used practice of viewing data stream clustering algorithms as a class of one-pass clustering algorithms is not very useful from an application point of view. For example, a simple one-pass clustering algorithm over an entire data stream of a few years is dominated by the outdated history of the stream. The exploration of the stream over different time windows can provide the users with a much deeper understanding of the evolving behavior of the clusters. At the same time, it is not possible to simultaneously perform dynamic clustering over all possible time horizons for a data stream of even moderately large volume. This paper discusses a fundamentally different philosophy for data stream clustering which is guided by application-centered requirements. The idea is divide the clustering process into an online component which periodically stores detailed summary statistics and an offine component which uses only this summary statistics. The offine component is utilized by the analyst who can use a wide variety of inputs (such as time horizon or number of clusters) in order to provide a quick understanding of the broad clusters in the data stream. The problems of efficient choice, storage, and use of this statistical data for a fast data stream turns out to be quite tricky. For this purpose, we use the concepts of a pyramidal time frame in conjunction with a microclustering approach. Our performance experiments over a number of real and synthetic data sets illustrate the effectiveness, efficiency, and insights provided by our approach.

#index 1015294
#* Star-cubing: computing iceberg cubes by top-down and bottom-up integration
#@ 12167 961 11879 2966
#t 2003
#c VLDB '03 Proceedings of the 29th international conference on Very large data bases - Volume 29
#% 210182
#% 227880
#% 236410
#% 248785
#% 259995
#% 273916
#% 280448
#% 300120
#% 333925
#% 397388
#% 420053
#% 420141
#% 459025
#% 462204
#% 464706
#% 479450
#% 479476
#% 479646
#% 481290
#% 654446
#% 993958
#% 993996
#! Data cube computation is one of the most essential but expensive operations in data warehousing. Previous studies have developed two major approaches, top-down vs. bottom-up. The former, represented by the Multi-Way Array Cube (called MultiWay) algorithm [25], aggregates simultaneously on multiple dimensions; however, it cannot take advantage of Apriori pruning [2] when computing iceberg cubes (cubes that contain only aggregate cells whose measure value satisfies a threshold, called iceberg condition). The latter, represented by two algorithms: BUC [6] and H-Cubing[11], computes the iceberg cube bottom-up and facilitates Apriori pruning. BUC explores fast sorting and partitioning techniques; whereas H-Cubing explores a data structure, H-Tree, for shared computation. However, none of them fully explores multi-dimensional simultaneous aggregation. In this paper, we present a new method, Star-Cubing, that integrates the strengths of the previous three algorithms and performs aggregations on multiple dimensions simultaneously. It utilizes a star-tree structure, extends the simultaneous aggregation methods, and enables the pruning of the group-by's that do not satisfy the iceberg condition. Our performance study shows that Star-Cubing is highly efficient and outperforms all the previous methods in almost all kinds of data distributions.

#index 1016173
#* High-dimensional OLAP: a minimal cubing approach
#@ 11879 961 15163
#t 2004
#c VLDB '04 Proceedings of the Thirtieth international conference on Very large data bases - Volume 30
#% 210182
#% 223781
#% 227861
#% 227880
#% 236410
#% 248814
#% 273916
#% 280448
#% 290703
#% 333925
#% 387427
#% 397388
#% 420053
#% 462217
#% 480329
#% 481290
#% 481951
#% 654446
#% 993996
#% 1015294
#! Data cube has been playing an essential role in fast OLAP (online analytical processing) in many multi-dimensional data warehouses. However, there exist data sets in applications like bioinformatics, statistics, and text processing that are characterized by high dimensionality, e.g., over 100 dimensions, and moderate size, e.g., around 106 tuples. No feasible data cube can be constructed with such data sets. In this paper we will address the problem of developing an efficient algorithm to perform OLAP on such data sets. Experience tells us that although data analysis tasks may involve a high dimensional space, most OLAP operations are performed only on a small number of dimensions at a time. Based on this observation, we propose a novel method that computes a thin layer of the data cube together with associated value-list indices. This layer, while being manageable in size, will be capable of supporting flexible and fast OLAP operations in the original high dimensional space. Through experiments we will show that the method has I/O costs that scale nicely with dimensionality. Furthermore, the costs are comparable to that of accessing an existing data cube when full materialization is possible.

#index 1016200
#* A framework for projected clustering of high dimensional data streams
#@ 968 961 4625 850
#t 2004
#c VLDB '04 Proceedings of the Thirtieth international conference on Very large data bases - Volume 30
#% 36672
#% 210173
#% 248790
#% 248792
#% 273890
#% 273891
#% 302724
#% 310488
#% 310500
#% 320942
#% 378388
#% 481281
#% 594012
#% 654489
#% 659943
#% 659972
#% 740767
#% 1015261
#! The data stream problem has been studied extensively in recent years, because of the great ease in collection of stream data. The nature of stream data makes it essential to use algorithms which require only one pass over the data. Recently, single-scan, stream analysis methods have been proposed in this context. However, a lot of stream data is high-dimensional in nature. High-dimensional data is inherently more complex in clustering, classification, and similarity search. Recent research discusses methods for projected clustering over high-dimensional data sets. This method is however difficult to generalize to data streams because of the complexity of the method and the large volume of the data streams. In this paper, we propose a new, high-dimensional, projected data stream clustering method, called HPStream. The method incorporates a fading cluster structure, and the projection based clustering methodology. It is incrementally updatable and is highly scalable on both the number of dimensions and the size of the data streams, and it achieves better clustering quality in comparison with the previous stream clustering methods. Our performance study with both real and synthetic data sets demonstrates the efficiency and effectiveness of our proposed framework and implementation methods.

#index 1022239
#* Mining approximate top-k subspace anomalies in multi-dimensional time-series data
#@ 11879 961
#t 2007
#c VLDB '07 Proceedings of the 33rd international conference on Very large data bases
#% 34077
#% 248792
#% 273916
#% 280501
#% 300131
#% 300183
#% 301165
#% 333929
#% 420141
#% 577221
#% 664842
#% 765518
#% 800496
#% 809264
#% 810065
#% 824705
#% 824710
#% 844310
#% 893120
#% 993958
#% 1016173
#! Market analysis is a representative data analysis process with many applications. In such an analysis, critical numerical measures, such as profit and sales, fluctuate over time and form time-series data. Moreover, the time series data correspond to market segments, which are described by a set of attributes, such as age, gender, education, income level, and product-category, that form a multi-dimensional structure. To better understand market dynamics and predict future trends, it is crucial to study the dynamics of time-series in multi-dimensional market segments. This is a topic that has been largely ignored in time series and data cube research. In this study, we examine the issues of anomaly detection in multi-dimensional time-series data. We propose time-series data cube to capture the multi-dimensional space formed by the attribute structure. This facilitates the detection of anomalies based on expected values derived from higher level, "more general" time-series. Anomaly detection in a time-series data cube poses computational challenges, especially for high-dimensional, large data sets. To this end, we also propose an efficient search algorithm to iteratively select subspaces in the original high-dimensional space and detect anomalies within each one. Our experiments with both synthetic and real-world data demonstrate the effectiveness and efficiency of the proposed solution.

#index 1022268
#* Adaptive fastest path computation on a road network: a traffic mining approach
#@ 15163 961 11879 19173 19174
#t 2007
#c VLDB '07 Proceedings of the 33rd international conference on Very large data bases
#% 214769
#% 267476
#% 298270
#% 327432
#% 338580
#% 375388
#% 449588
#% 463903
#% 464223
#% 464996
#% 479787
#% 572923
#% 593917
#% 677994
#% 749474
#% 864397
#% 864470
#% 875476
#% 893157
#% 1272280
#% 1676469
#! Efficient fastest path computation in the presence of varying speed conditions on a large scale road network is an essential problem in modern navigation systems. Factors affecting road speed, such as weather, time of day, and vehicle type, need to be considered in order to select fast routes that match current driving conditions. Most existing systems compute fastest paths based on road Euclidean distance and a small set of predefined road speeds. However, "History is often the best teacher". Historical traffic data or driving patterns are often more useful than the simple Euclidean distance-based computation because people must have good reasons to choose these routes, e.g., they may want to avoid those that pass through high crime areas at night or that likely encounter accidents, road construction, or traffic jams. In this paper, we present an adaptive fastest path algorithm capable of efficiently accounting for important driving and speed patterns mined from a large set of traffic data. The algorithm is based on the following observations: (1) The hierarchy of roads can be used to partition the road network into areas, and different path pre-computation strategies can be used at the area level, (2) we can limit our route search strategy to edges and path segments that are actually frequently traveled in the data, and (3) drivers usually traverse the road network through the largest roads available given the distance of the trip, except if there are small roads with a significant speed advantage over the large ones. Through an extensive experimental evaluation on real road networks we show that our algorithm provides desirable (short and well-supported) routes, and that it is significantly faster than competing methods.

#index 1022279
#* Towards graph containment search and indexing
#@ 15196 9413 850 961 11559 19182
#t 2007
#c VLDB '07 Proceedings of the 33rd international conference on Very large data bases
#% 10419
#% 217812
#% 223567
#% 280409
#% 321327
#% 350323
#% 378391
#% 466644
#% 479465
#% 654452
#% 729938
#% 765429
#% 769951
#% 779470
#% 780860
#% 805893
#% 810072
#% 864425
#% 905193
#% 937108
#% 993958
#% 1717545
#! Given a set of model graphs D and a query graph q, containment search aims to find all model graphs g ε D such that q contains g (q ⊇ g). Due to the wide adoption of graph models, fast containment search of graph data finds many applications in various domains. In comparison to traditional graph search that retrieves all the graphs containing q (q ⊆ g), containment search has its own indexing characteristics that have not yet been examined. In this paper, we perform a systematic study on these characteristics and propose a contrast subgraph-based indexing model, called cIndex. Contrast subgraphs capture the structure differences between model graphs and query graphs, and are thus perfect for indexing due to their high selectivity. Using a redundancy-aware feature selection process, cIndex can sort out a set of significant and distinctive contrast subgraphs and maximize its indexing capability. We show that it is NP-complete to choose the best set of indexing features, and our greedy algorithm can approximate the one-level optimal index within a ratio of 1-- 1/e. Taking this solution as a base indexing model, we further extend it to accommodate hierarchical indexing methodologies and apply data space clustering and sampling techniques to reduce the index construction time. The proposed methodology provides a general solution to containment search and indexing, not only for graphs, but also for any data with transitive relations as well. Experimental results on real test data show that cIndex achieves near-optimal pruning power on various containment search workloads, and confirms its obvious advantage over indices built for traditional graph search in this new scenario.

#index 1022314
#* DataScope: viewing database contents in Google Maps' way
#@ 19229 11879 12167 961 19230 19231
#t 2007
#c VLDB '07 Proceedings of the 33rd international conference on Very large data bases
#% 577222
#% 893124
#% 893127
#% 1015294
#% 1016173
#! People have been relying on Google Maps, MapQuest, or other similar services to find desired locations on maps, browse surrounding businesses, get driving directions, etc.. Navigation by clicking and dragging the mouse to browse maps at multiple levels of resolution is one of the most attractive features in Web-based map exploration. Most database systems, though with some graphical user interfaces, are still lack of data-content browsing-based interfaces. Motivated by Google Maps, we develop DataScope, a Web-based data content visualization system, for people to view the desired data easily, interactively, and at multi-resolution.

#index 1063475
#* ARCube: supporting ranking aggregate queries in partially materialized data cubes
#@ 19229 12167 961
#t 2008
#c Proceedings of the 2008 ACM SIGMOD international conference on Management of data
#% 210182
#% 223781
#% 227880
#% 227894
#% 248806
#% 252304
#% 273916
#% 290703
#% 333854
#% 333925
#% 397378
#% 397388
#% 420053
#% 463760
#% 479646
#% 479795
#% 571045
#% 659993
#% 765418
#% 810018
#% 874975
#% 875001
#% 875002
#% 893126
#% 893127
#% 893128
#% 960243
#% 993996
#% 1015294
#% 1016173
#% 1022276
#% 1698966
#! Supporting ranking queries in database systems has been a popular research topic recently. However, there is a lack of study on supporting ranking queries in data warehouses where ranking is on multidimensional aggregates instead of on measures of base facts. To address this problem, we propose a query execution model to answer different types of ranking aggregate queries based on a unified, partial cube structure, ARCube. The query execution model follows a candidate generation and verification framework, where the most promising candidate cells are generated using a set of high-level guiding cells. We also identify a bounding principle for effective pruning: once a guiding cell is pruned, all of its children candidate cells can be pruned. We further address the problem of efficient online candidate aggregation and verification by developing a chunk-based execution model to verify a bulk of candidates within a bounded memory buffer. Our extensive performance study shows that the new framework not only leads to an order of magnitude performance improvements over the state-of-the-art method, but also is much more flexible in terms of the types of ranking aggregate queries supported.

#index 1063502
#* Mining significant graph patterns by leap search
#@ 9413 9360 961 850
#t 2008
#c Proceedings of the 2008 ACM SIGMOD international conference on Management of data
#% 152934
#% 280409
#% 299985
#% 342604
#% 420126
#% 466644
#% 577214
#% 629708
#% 722920
#% 729938
#% 765429
#% 813990
#% 840863
#% 915228
#% 915350
#% 960305
#% 976826
#% 1117006
#% 1272179
#% 1558464
#% 1673557
#! With ever-increasing amounts of graph data from disparate sources, there has been a strong need for exploiting significant graph patterns with user-specified objective functions. Most objective functions are not antimonotonic, which could fail all of frequency-centric graph mining algorithms. In this paper, we give the first comprehensive study on general mining method aiming to find most significant patterns directly. Our new mining framework, called LEAP (Descending Leap Mine), is developed to exploit the correlation between structural similarity and significance similarity in a way that the most significant pattern could be identified quickly by searching dissimilar graph patterns. Two novel concepts, structural leap search and frequency descending mining, are proposed to support leap search in graph pattern space. Our new mining method revealed that the widely adopted branch-and-bound search in data mining literature is indeed not the best, thus sketching a new picture on scalable graph pattern discovery. Empirical results show that LEAP achieves orders of magnitude speedup in comparison with the state-of-the-art method. Furthermore, graph classifiers built on mined patterns outperform the up-to-date graph kernel method in terms of efficiency and accuracy, demonstrating the high promise of such patterns.

#index 1063528
#* Sampling cube: a framework for statistical olap over sampling data
#@ 11879 961 19944 12241 19945
#t 2008
#c Proceedings of the 2008 ACM SIGMOD international conference on Management of data
#% 210182
#% 273916
#% 333929
#% 376266
#% 464215
#% 480499
#% 641976
#% 654446
#% 654467
#% 722929
#% 765518
#% 824733
#% 824734
#% 893167
#% 926881
#% 987193
#% 993996
#% 1015294
#% 1016173
#% 1016174
#% 1022205
#! Sampling is a popular method of data collection when it is impossible or too costly to reach the entire population. For example, television show ratings in the United States are gathered from a sample of roughly 5,000 households. To use the results effectively, the samples are further partitioned in a multidimensional space based on multiple attribute values. This naturally leads to the desirability of OLAP (Online Analytical Processing) over sampling data. However, unlike traditional data, sampling data is inherently uncertain, i.e., not representing the full data in the population. Thus, it is desirable to return not only query results but also the confidence intervals indicating the reliability of the results. Moreover, a certain segment in a multidimensional space may contain none or too few samples. This requires some additional analysis to return trustable results. In this paper we propose a Sampling Cube framework, which efficiently calculates confidence intervals for any multidimensional query and uses the OLAP structure to group similar segments to increase sampling size when needed. Further, to handle high dimensional data, a Sampling Cube Shell method is proposed to effectively reduce the storage requirement while still preserving query result quality.

#index 1063592
#* BibNetMiner: mining bibliographic information networks
#@ 19945 19229 19944 9360 961 10548 18038
#t 2008
#c Proceedings of the 2008 ACM SIGMOD international conference on Management of data
#% 823342
#% 893124
#% 893127
#% 989682
#% 1022314
#! Online bibliographic databases, such as DBLP in computer science and PubMed in medical sciences, contain abundant information about research publications in different fields. Each such database forms a gigantic information network (hence called BibNet), connecting in complex ways research papers, authors, conferences/journals, and possibly citation information as well, and provides a fertile land for information network analysis. Our BibNetMiner is designed for sophisticated information network mining on such bibliographic databases. In this demo, we will take the DBLP database as an example, demonstrate several attractive functions of BibNetMiner, including clustering, ranking and profiling of conferences and authors based on the research subfields. A user-friendly, visualization-enhanced interface will be provided to facilitate interactive exploration of a bibliographic database. This project will serve as an example to demonstrate the power of links in information network mining. Since the dataset is large and the network is heterogeneous, such a study will benefit the research on the analysis of massive heterogeneous information networks.

#index 1083649
#* Direct mining of discriminative and essential frequent patterns via model-based search tree
#@ 2485 14886 9360 17596 9413 961 15637 17636
#t 2008
#c Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 300120
#% 342604
#% 463903
#% 464996
#% 466483
#% 466644
#% 481290
#% 729938
#% 769891
#% 810064
#% 813990
#% 823384
#% 824699
#% 840863
#% 915350
#% 937794
#% 1063502
#% 1206650
#! Frequent patterns provide solutions to datasets that do not have well-structured feature vectors. However, frequent pattern mining is non-trivial since the number of unique patterns is exponential but many are non-discriminative and correlated. Currently, frequent pattern mining is performed in two sequential steps: enumerating a set of frequent patterns, followed by feature selection. Although many methods have been proposed in the past few years on how to perform each separate step efficiently, there is still limited success in eventually finding highly compact and discriminative patterns. The culprit is due to the inherent nature of this widely adopted two-step approach. This paper discusses these problems and proposes a new and different method. It builds a decision tree that partitions the data onto different nodes. Then at each node, it directly discovers a discriminative pattern to further divide its examples into purer subsets. Since the number of examples towards leaf level is relatively small, the new approach is able to examine patterns with extremely low global support that could not be enumerated on the whole dataset by the two-step method. The discovered feature vectors are more accurate on some of the most difficult graph as well as frequent itemset problems than most recently proposed algorithms but the total size is typically 50% or more smaller. Importantly, the minimum support of some discriminative patterns can be extremely low (e.g. 0.03%). In order to enumerate these low support patterns, state-of-the-art frequent pattern algorithm either cannot finish due to huge memory consumption or have to enumerate 101 to 103 times more patterns before they can even be found. Software and datasets are available by contacting the author.

#index 1083655
#* Knowledge transfer via multiple model local structure mapping
#@ 17596 2485 9716 961
#t 2008
#c Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 229931
#% 236497
#% 269217
#% 424997
#% 729932
#% 769888
#% 786633
#% 983814
#% 983828
#% 989592
#% 1042787
#% 1117009
#% 1272110
#% 1275171
#! The effectiveness of knowledge transfer using classification algorithms depends on the difference between the distribution that generates the training examples and the one from which test examples are to be drawn. The task can be especially difficult when the training examples are from one or several domains different from the test domain. In this paper, we propose a locally weighted ensemble framework to combine multiple models for transfer learning, where the weights are dynamically assigned according to a model's predictive power on each test example. It can integrate the advantages of various learning algorithms and the labeled information from multiple training domains into one unified classification model, which can then be applied on a different domain. Importantly, different from many previously proposed methods, none of the base learning method is required to be specifically designed for transfer learning. We show the optimality of a locally weighted ensemble framework as a general approach to combine multiple models for domain transfer. We then propose an implementation of the local weight assignments by mapping the structures of a model onto the structures of the test domain, and then weighting each model locally according to its consistency with the neighborhood structure around the test example. Experimental results on text classification, spam filtering and intrusion detection data sets demonstrate significant improvements in classification accuracy gained by the framework. On a transfer learning task of newsgroup message categorization, the proposed locally weighted ensemble framework achieves 97% accuracy when the best single model predicts correctly only on 73% of the test examples. In summary, the improvement in accuracy is over 10% and up to 30% across different problems.

#index 1083667
#* Mining preferences from superior and inferior examples
#@ 19119 3214 10511 11015 961
#t 2008
#c Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 288976
#% 300170
#% 331835
#% 376266
#% 408396
#% 420117
#% 458873
#% 465167
#% 566111
#% 577224
#% 629667
#% 729437
#% 813974
#% 992635
#% 993957
#% 994017
#% 1026877
#% 1272396
#! Mining user preferences plays a critical role in many important applications such as customer relationship management (CRM), product and service recommendation, and marketing campaigns. In this paper, we identify an interesting and practical problem of mining user preferences: in a multidimensional space where the user preferences on some categorical attributes are unknown, from some superior and inferior examples provided by a user, can we learn about the user's preferences on those categorical attributes? We model the problem systematically and show that mining user preferences from superior and inferior examples is challenging. Although the problem has great potential in practice, to the best of our knowledge, it has not been explored systematically before. As the first attempt to tackle the problem, we propose a greedy method and show that our method is practical using real data sets and synthetic data sets.

#index 1100188
#* Association Mining in Large Databases: A Re-examination of Its Measures
#@ 19229 12930 961
#t 2007
#c PKDD 2007 Proceedings of the 11th European conference on Principles and Practice of Knowledge Discovery in Databases
#% 227917
#% 227919
#% 392618
#% 452846
#% 464822
#% 577214
#% 632028
#% 727869
#% 769890
#% 769909
#! In the literature of data mining and statistics, numerous interestingness measures have been proposed to disclose succinct object relationships of association patterns. However, it is still not clear when a measure is truly effective in large data sets. Recent studies have identified a critical property, null-(transaction)invariance, for measuring event associations in large data sets, but many existing measures do not have this property. We thus re-examine the null-invariant measures and find interestingly that they can be expressed as a generalized mathematical mean, and there exists a total ordering of them. This ordering provides insights into the underlying philosophy of the measures and helps us understand and select the proper measure for different applications.

#index 1116998
#* Efficient Discovery of Frequent Approximate Sequential Patterns
#@ 15170 9413 961 850
#t 2007
#c ICDM '07 Proceedings of the 2007 Seventh IEEE International Conference on Data Mining
#! We propose an efficient algorithm for mining frequent approximate sequential patterns under the Hamming distance model. Our algorithm gains its efficiency by adopting a "break-down-and-build-up" methodology. The "breakdown" is based on the observation that all occurrences of a frequent pattern can be classified into groups, which we call strands. We developed efficient algorithms to quickly mine out all strands by iterative growth. In the "build-up" stage, these strands are grouped up to form the support sets from which all approximate patterns would be identified. A salient feature of our algorithm is its ability to grow the frequent patterns by iteratively assembling building blocks of significant sizes in a local search fashion. By avoiding incremental growth and global search, we achieve greater efficiency without losing the completeness of the mining result. Our experimental studies demonstrate that our algorithm is efficient in mining globally repeating approximate sequential patterns that would have been missed by existing methods.

#index 1117001
#* Spectral Regression: A Unified Approach for Sparse Subspace Learning
#@ 8278 10859 961
#t 2007
#c ICDM '07 Proceedings of the 2007 Seventh IEEE International Conference on Data Mining
#! Recently the problem of dimensionality reduction (or, subspace learning) has received a lot of interests in many fields of information processing, including data mining, information retrieval, and pattern recognition. Some popular methods include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA) and Locality Preserving Projection (LPP). However, a disadvantage of all these approaches is that the learned projective functions are linear combinations of all the original features, thus it is often difficult to interpret the results. In this paper, we propose a novel dimensionality reduction framework, called Unified Sparse Subspace Learning (USSL), for learning sparse projections. USSL casts the problem of learning the projective functions into a regression framework, which facilitates the use of different kinds of regularizers. By using a L1-norm regularizer (lasso), the sparse projections can be efficiently computed. Experimental results on real world classification and clustering problems demonstrate the effectiveness of our method.

#index 1117009
#* On Appropriate Assumptions to Mine Data Streams: Analysis and Practice
#@ 17596 2485 961
#t 2007
#c ICDM '07 Proceedings of the 2007 Seventh IEEE International Conference on Data Mining
#! Recent years have witnessed an increasing number of studies in stream mining, which aim at building an accurate model for continuously arriving data. Somehow most existing work makes the implicit assumption that the training data and the yet-to-come testing data are always sampled from the "same distribution, and yet this "same distribution evolves over time. We demonstrate that this may not be true, and one actually may never know either "how or "when the distribution changes. Thus, a model that fits well on the observed distribution can have unsatisfactory accuracy on the incoming data. Practically, one can just assume the bare minimum that learning from observed data is better than both random guessing and always predicting exactly the same class label. Importantly, we formally and experimentally demonstrate the robustness of a model averaging and simple voting-based framework for data streams, particularly when incoming data "continuously follows significantly different distributions. On a real streaming data, this framework reduces the expected error of baseline models by 60%, and remains the most accurate compared to those baseline models.

#index 1117038
#* Efficient Kernel Discriminant Analysis via Spectral Regression
#@ 8278 10859 961
#t 2007
#c ICDM '07 Proceedings of the 2007 Seventh IEEE International Conference on Data Mining
#! Linear Discriminant Analysis (LDA) has been a popular method for extracting features which preserve class separability. The projection vectors are commonly obtained by maximizing the between class covariance and simultaneously minimizing the within class covariance. LDA can be performed either in the original input space or in the reproducing kernel Hilbert space (RKHS) into which data points are mapped, which leads to Kernel Discriminant Analysis (KDA). When the data are highly nonlinear distributed, KDA can achieve better performance than LDA. However, computing the projective functions in KDA involves eigen-decomposition of kernel matrix, which is very expensive when a large number of training samples exist. In this paper, we present a new algorithm for kernel discriminant analysis, called Spectral Regression Kernel Discriminant Analysis (SRKDA). By using spectral graph analysis, SRKDA casts discriminant analysis into a regression framework which facilitates both efficient computation and the use of regularization techniques. Specifically, SRKDA only needs to solve a set of regularized regression problems and there is no eigenvector computation involved, which is a huge save of computational cost. Our computational analysis shows that SRKDA is 27 times faster than the ordinary KDA. Moreover, the new formulation makes it very easy to develop incremental version of the algorithm which can fully utilize the computational results of the existing training samples. Experiments on face recognition demonstrate the effectiveness and efficiency of the proposed algorithm.

#index 1117041
#* gApprox: Mining Frequent Approximate Patterns from a Massive Network
#@ 15196 9413 15170 961
#t 2007
#c ICDM '07 Proceedings of the 2007 Seventh IEEE International Conference on Data Mining
#! Recently, there arise a large number of graphs with massive sizes and complex structures in many new applications, such as biological networks, social networks, and the Web, demanding powerful data mining methods. Due to inherent noise or data diversity, it is crucial to address the issue of approximation, if one wants to mine patterns that are potentially interesting with tolerable variations. In this paper, we investigate the problem of mining frequent approximate patterns from a massive network and propose a method called gApprox. gApprox not only finds approximate network patterns, which is the key for many knowledge discovery applications on structural data, but also enriches the library of graph mining methodologies by introducing several novel techniques such as: (1) a complete and redundancy-free strategy to explore the new pattern space faced by gApprox; and (2) transform "frequent in an approximate sense" into an anti-monotonic constraint so that it can be pushed deep into the mining process. Systematic empirical studies on both real and synthetic data sets show that frequent approximate patterns mined from the worm protein-protein interaction network are biologically interesting and gApprox is both effective and efficient.

#index 1127437
#* TraClass: trajectory classification using hierarchical region-based and trajectory-based clustering
#@ 12241 961 11879 15163
#t 2008
#c Proceedings of the VLDB Endowment
#% 190581
#% 479817
#% 621551
#% 818916
#% 876074
#% 881545
#% 960283
#% 1206639
#% 1558464
#% 1856449
#! Trajectory classification, i.e., model construction for predicting the class labels of moving objects based on their trajectories and other features, has many important, real-world applications. A number of methods have been reported in the literature, but due to using the shapes of whole trajectories for classification, they have limited classification capability when discriminative features appear at parts of trajectories or are not relevant to the shapes of trajectories. These situations are often observed in long trajectories spreading over large geographic areas. Since an essential task for effective classification is generating discriminative features, a feature generation framework TraClass for trajectory data is proposed in this paper, which generates a hierarchy of features by partitioning trajectories and exploring two types of clustering: (1) region-based and (2) trajectory-based. The former captures the higher-level region-based features without using movement patterns, whereas the latter captures the lower-level trajectory-based features using movement patterns. The proposed framework overcomes the limitations of the previous studies because trajectory partitioning makes discriminative parts of trajectories identifiable, and the two types of clustering collaborate to find features of both regions and sub-trajectories. Experimental results demonstrate that TraClass generates high-quality features and achieves high classification accuracy from real trajectory data.

#index 1176865
#* Non-negative Matrix Factorization on Manifold
#@ 8278 10859 10834 961
#t 2008
#c ICDM '08 Proceedings of the 2008 Eighth IEEE International Conference on Data Mining
#! Recently Non-negative Matrix Factorization (NMF) has received a lot of attentions in information retrieval, computer vision and pattern recognition. NMF aims to find two non-negative matrices whose product can well approximate the original matrix. The sizes of these two matrices are usually smaller than the original matrix. This results in a compressed version of the original data matrix. The solution of NMF yields a natural parts-based representation for the data. When NMF is applied for data representation, a major disadvantage is that it fails to consider the geometric structure in the data. In this paper, we develop a graph based approach for parts-based data representation in order to overcome this limitation. We construct an affinity graph to encode the geometrical information and seek a matrix factorization which respects the graph structure. We demonstrate the success of this novel algorithm by applying it on real world problems.

#index 1176876
#* Graph OLAP: Towards Online Analytical Processing on Graphs
#@ 15196 9413 15170 961 850
#t 2008
#c ICDM '08 Proceedings of the 2008 Eighth IEEE International Conference on Data Mining
#! OLAP (On-Line Analytical Processing) is an important notion in data analysis. Recently, more and more graph or networked data sources come into being. There exists a similar need to deploy graph analysis from different perspectives and with multiple granularities. However, traditional OLAP technology cannot handle such demands because it does not consider the links among individual data tuples. In this paper, we develop a novel graph OLAP framework, which presents a multi-dimensional and multi-level view over graphs. The contributions of this work are two-fold. First, starting from basic definitions, i.e., what are dimensions and measures in the graph OLAP scenario, we develop a conceptual framework for data cubes on graphs. We also look into different semantics of OLAP operations, and classify the framework into two major subcases: informational OLAP and topological OLAP. Then, with more emphasis on informational OLAP (topological OLAP will be covered in a future study due to the lack of space), we show how a graph cube can be materialized by calculating a special kind of measure called aggregated graph and how to implement it efficiently. This includes both full materialization and partial materialization where constraints are enforced to obtain an iceberg cube. We can see that the aggregated graphs, which depend on the graph properties of underlying networks, are much harder to compute than their traditional OLAP counterparts, due to the increased structural complexity of data. Empirical studies show insightful results on real datasets and demonstrate the efficiency of our proposed optimizations.

#index 1176884
#* Text Cube: Computing IR Measures for Multidimensional Text Database Analysis
#@ 22155 17321 961 15170 22156
#t 2008
#c ICDM '08 Proceedings of the 2008 Eighth IEEE International Conference on Data Mining
#! Since Jim Gray introduced the concept of ”data cube” in 1997, data cube, associated with online analytical processing (OLAP), has become a driving engine in data warehouse industry. Because the boom of Internet has given rise to an ever increasing amount of text data associated with other multidimensional information, it is natural to propose a data cube model that integrates the power of traditional OLAP and IR techniques for text. In this paper, we propose a Text-Cube model on multidimensional text database and study effective OLAP over such data. Two kinds of hierarchies are distinguishable inside: dimensional hierarchy and term hierarchy. By incorporating these hierarchies, we conduct systematic studies on efficient text-cube implementation, OLAP execution and query processing. Our performance study shows the high promise of our methods.

#index 1176894
#* A Practical Approach to Classify Evolving Data Streams: Training with Limited Amount of Labeled Data
#@ 22167 17596 10403 961 3712
#t 2008
#c ICDM '08 Proceedings of the 2008 Eighth IEEE International Conference on Data Mining
#! Recent approaches in classifying evolving data streams are based on supervised learning algorithms, which can be trained with labeled data only. Manual labeling of data is both costly and time consuming. Therefore, in a real streaming environment, where huge volumes of data appear at a high speed, labeled data may be very scarce. Thus, only a limited amount of training data may be available for building the classification models, leading to poorly trained classifiers. We apply a novel technique to overcome this problem by building a classification model from a training set having both unlabeled and a small amount of labeled instances. This model is built as micro-clusters using semisupervised clustering technique and classification is performed with κ-nearest neighbor algorithm. An ensemble of these models is used to classify the unlabeled data. Empirical evaluation on both synthetic data and real botnet traffic reveals that our approach, using only a small amount of labeled data for training, outperforms state-of-the-art stream classification algorithms that use twenty times more labeled data than our approach.

#index 1176896
#* Stream Sequential Pattern Mining with Precise Error Bounds
#@ 22172 17321 961
#t 2008
#c ICDM '08 Proceedings of the 2008 Eighth IEEE International Conference on Data Mining
#! Sequential pattern mining is an interesting data mining problem with many real-world applications. This problem has been studied extensively in static databases. However, in recent years, emerging applications have introduced a new form of data called data stream. In a data stream, new elements are generated continuously. This poses additional constraints on the methods used for mining such data: memory usage is restricted, the infinitely flowing original dataset cannot be scanned multiple times, and current results should be available on demand.This paper introduces two effective methods for mining sequential patterns from data streams: the SS-BE method and the SS-MB method. The proposed methods break the stream into batches and only process each batch once. The two methods use different pruning strategies that restrict the memory usage but can still guarantee that all true sequential patterns are output at the end of any batch. Both algorithms scale linearly in execution time as the number of sequences grows, making them effective methods for sequential pattern mining in data streams. The experimental results also show that our methods are very accurate in that only a small fraction of the patterns that are output are false positives. Even for these false positives, SS-BE guarantees that their true support is above a pre-defined threshold.

#index 1195978
#* A Multi-partition Multi-chunk Ensemble Technique to Classify Concept-Drifting Data Streams
#@ 22167 17596 10403 961 3712
#t 2009
#c PAKDD '09 Proceedings of the 13th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining
#% 273900
#% 310500
#% 342600
#% 449529
#% 729932
#% 769888
#% 840891
#% 1020400
#% 1117009
#! We propose a multi-partition, multi-chunk ensemble classifier based data mining technique to classify concept-drifting data streams. Existing ensemble techniques in classifying concept-drifting data streams follow a single-partition, single-chunk approach, in which a single data chunk is used to train one classifier. In our approach, we train a collection of v classifiers from r consecutive data chunks using v -fold partitioning of the data, and build an ensemble of such classifiers. By introducing this multi-partition, multi-chunk ensemble technique, we significantly reduce classification error compared to the single-partition, single-chunk ensemble approaches. We have theoretically justified the usefulness of our algorithm, and empirically proved its effectiveness over other state-of-the-art stream classification techniques on synthetic data and real botnet traffic.

#index 1206639
#* Trajectory Outlier Detection: A Partition-and-Detect Framework
#@ 12241 961 11879
#t 2008
#c ICDE '08 Proceedings of the 2008 IEEE 24th International Conference on Data Engineering
#! Outlier detection has been a popular data mining task. However, there is a lack of serious study on outlier detection for trajectory data. Even worse, an existing trajectory outlier detection algorithm has limited capability to detect outlying sub-trajectories. In this paper, we propose a novel partition-and-detect framework for trajectory outlier detection, which partitions a trajectory into a set of line segments, and then, detects outlying line segments for trajectory outliers. The primary advantage of this framework is to detect outlying sub-trajectories from a trajectory database. Based on this partition-and-detect framework, we develop a trajectory outlier detection algorithm TRAOD. Our algorithm consists of two phases: partitioning and detection. For the first phase, we propose a two-level trajectory partitioning strategy that ensures both high quality and high efficiency. For the second phase, we present a hybrid of the distance-based and density-based approaches. Experimental results demonstrate that TRAOD correctly detects outlying sub-trajectories from real trajectory data.

#index 1206650
#* Direct Discriminative Pattern Mining for Effective Classification
#@ 9360 9413 961 850
#t 2008
#c ICDE '08 Proceedings of the 2008 IEEE 24th International Conference on Data Engineering
#! The application of frequent patterns in classification has demonstrated its power in recent studies. It often adopts a two-step approach: frequent pattern (or classification rule) mining followed by feature selection (or rule ranking). However, this two-step process could be computationally expensive, especially when the problem scale is large or the minimum support is low. It was observed that frequent pattern mining usually produces a huge number of "patterns" that could not only slow down the mining process but also make feature selection hard to complete. In this paper, we propose a direct discriminative pattern mining approach, DDPMine, to tackle the efficiency issue arising from the two-step approach. DDPMine performs a branch-and-bound search for directly mining discriminative patterns without generating the complete pattern set. Instead of selecting best patterns in a batch, we introduce a "feature-centered" mining approach that generates discriminative patterns sequentially on a progressively shrinking FP-tree by incrementally eliminating training instances. The instance elimination effectively reduces the problem size iteratively and expedites the mining process. Empirical results show that DDPMine achieves orders of magnitude speedup without any downgrade of classification accuracy. It outperforms the state-of-the-art associative classification methods in terms of both accuracy and efficiency.

#index 1206661
#* Training Linear Discriminant Analysis in Linear Time
#@ 8278 10859 961
#t 2008
#c ICDE '08 Proceedings of the 2008 IEEE 24th International Conference on Data Engineering
#! Linear Discriminant Analysis (LDA) has been a popular method for extracting features which preserve class separability. It has been widely used in many fields of information processing, such as machine learning, data mining, information retrieval, and pattern recognition. However, the computation of LDA involves dense matrices eigen-decomposition which can be computationally expensive both in time and memory. Specifically, LDA has O(mnt + t3) time complexity and requires O(mn + mt + nt) memory, where m is the number of samples, n is the number of features and t = min (m,n). When both m and n are large, it is infeasible to apply LDA. In this paper, we propose a novel algorithm for discriminant analysis, called Spectral Regression Discriminant Analysis (SRDA). By using spectral graph analysis, SRDA casts discriminant analysis into a regression framework which facilitates both efficient computation and the use of regularization techniques. Our theoretical analysis shows that SRDA can be computed with O(ms) time and O(ms) memory, where s(les n) is the average number of non-zero features in each sample. Extensive experimental results on four real world data sets demonstrate the effectiveness and efficiency of our algorithm.

#index 1206752
#* P-Cube: Answering Preference Queries in Multi-Dimensional Space
#@ 12167 961
#t 2008
#c ICDE '08 Proceedings of the 2008 IEEE 24th International Conference on Data Engineering
#! Many new applications that involve decision making need online (i.e., OLAP-styled) preference analysis with multi-dimensional boolean selections. Typical preference queries includes top-k queries and skyline queries. An analytical query often comes with a set of boolean predicates that constrain a target subset of data, which, may also vary incrementally by drilling/rolling operators. To efficiently support preference queries with multiple boolean predicates, neither boolean-then-preference nor preference-then-boolean approach is satisfactory. To integrate boolean pruning and preference pruning in a unified framework, we propose signature, a new materialization measure for multi-dimensional group-bys. Based on this, we propose P-Cube (i.e., data cube for preference queries) and study its complete life cycle, including signature generation, compression, decomposition, incremental maintenance and usage for efficient on-line analytical query processing. We present a signature-based progressive algorithm that is able to simultaneously push boolean and preference constraints deep into the database search. Our performance study shows that the proposed method achieves at least one order of magnitude speed-up over existing approaches.

#index 1206864
#* Efficient Mining of Closed Repetitive Gapped Subsequences from a Sequence Database
#@ 17321 18641 961 18642
#t 2009
#c ICDE '09 Proceedings of the 2009 IEEE International Conference on Data Engineering
#! There is a huge wealth of sequence data available, for example, customer purchase histories, program execution traces, DNA, and protein sequences. Analyzing this wealth of data to mine important knowledge is certainly a worthwhile goal.In this paper, as a step forward to analyzing patterns in sequences, we introduce the problem of mining closed repetitive gapped subsequences and propose efficient solutions. Given a database of sequences where each sequence is an ordered list of events, the pattern we would like to mine is called repetitive gapped subsequence, which is a subsequence (possibly with gaps between two successive events within it) of some sequences in the database. We introduce the concept of repetitive support to measure how frequently a pattern repeats in the database. Different from the sequential pattern mining problem, repetitive support captures not only repetitions of a pattern in different sequences but also the repetitions within a sequence. Given a userspecified support threshold min_sup, we study finding the set of all patterns with repetitive support no less than min_sup. To obtain a compact yet complete result set and improve the efficiency, we also study finding closed patterns. Efficient mining algorithms to find the complete set of desired patterns are proposed based on the idea of instance growth. Our performance study on various datasets shows the efficiency of our approach. A case study is also performed to show the utility of our approach.

#index 1207011
#* Temporal Outlier Detection in Vehicle Traffic Data
#@ 11879 23406 961 12241
#t 2009
#c ICDE '09 Proceedings of the 2009 IEEE International Conference on Data Engineering
#! Outlier detection in vehicle traffic data is a practical problem that has gained traction lately due to an increasing capability to track moving vehicles in city roads. In contrast to other applications, this particular domain includes a very dynamic dimension: time. Many existing algorithms have studied the problem of outlier detection at a single instant in time. This study proposes a method for detecting temporal outliers with an emphasis on historical similarity trends between data points. Outliers are calculated from drastic changes in the trends. Experiments with real world traffic data show that this approach is effective and efficient.

#index 1207076
#* Integrating OLAP and Ranking: The Ranking-Cube Methodology
#@ 12167 961
#t 2007
#c ICDEW '07 Proceedings of the 2007 IEEE 23rd International Conference on Data Engineering Workshop
#! OLAP (On-Line Analytical Processing) and Ranking are currently separate technologies in the database systems. OLAP emphasizes on efficient multi-dimensional data analysis and ranking is good for effective data exploration in massive data. In this paper, we discuss the problem of integrating OLAP and ranking, such that ranking serves as a function block for data analysis and exploration in the OLAP environment. Towards this goal, we present the ranking cube: a semi off-line materialization and semi online computation model. This paper discusses the framework, the implementation issues and the possible extensions of the ranking cube.

#index 1210517
#* Mining Massive RFID, Trajectory, and Traffic Data Sets
#@ 961 12241 15163 11879
#t 2008
#c Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining

#index 1214655
#* Heterogeneous source consensus learning via decision propagation and negotiation
#@ 17596 2485 19945 961
#t 2009
#c Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 252011
#% 420495
#% 424997
#% 722902
#% 728025
#% 770836
#% 837616
#% 844317
#% 866325
#% 937551
#% 961268
#% 961278
#% 987253
#% 1047784
#% 1074074
#% 1083655
#% 1130817
#% 1133031
#! Nowadays, enormous amounts of data are continuously generated not only in massive scale, but also from different, sometimes conflicting, views. Therefore, it is important to consolidate different concepts for intelligent decision making. For example, to predict the research areas of some people, the best results are usually achieved by combining and consolidating predictions obtained from the publication network, co-authorship network and the textual content of their publications. Multiple supervised and unsupervised hypotheses can be drawn from these information sources, and negotiating their differences and consolidating decisions usually yields a much more accurate model due to the diversity and heterogeneity of these models. In this paper, we address the problem of "consensus learning" among competing hypotheses, which either rely on outside knowledge (supervised learning) or internal structure (unsupervised clustering). We argue that consensus learning is an NP-hard problem and thus propose to solve it by an efficient heuristic method. We construct a belief graph to first propagate predictions from supervised models to the unsupervised, and then negotiate and reach consensus among them. Their final decision is further consolidated by calculating each model's weight based on its degree of consistency with other models. Experiments are conducted on 20 Newsgroups data, Cora research papers, DBLP author-conference network, and Yahoo! Movies datasets, and the results show that the proposed method improves the classification accuracy and the clustering quality measure (NMI) over the best base model by up to 10%. Furthermore, it runs in time proportional to the number of instances, which is very efficient for large scale data sets.

#index 1214677
#* Classification of software behaviors for failure detection: a discriminative pattern mining approach
#@ 18641 9360 961 18642 24001
#t 2009
#c Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 231941
#% 339045
#% 343052
#% 420063
#% 463903
#% 481290
#% 643965
#% 729437
#% 765387
#% 765521
#% 813990
#% 902158
#% 906081
#% 935763
#% 989617
#% 1039055
#% 1063502
#% 1707664
#! Software is a ubiquitous component of our daily life. We often depend on the correct working of software systems. Due to the difficulty and complexity of software systems, bugs and anomalies are prevalent. Bugs have caused billions of dollars loss, in addition to privacy and security threats. In this work, we address software reliability issues by proposing a novel method to classify software behaviors based on past history or runs. With the technique, it is possible to generalize past known errors and mistakes to capture failures and anomalies. Our technique first mines a set of discriminative features capturing repetitive series of events from program execution traces. It then performs feature selection to select the best features for classification. These features are then used to train a classifier to detect failures. Experiments and case studies on traces of several benchmark software systems and a real-life concurrency bug from MySQL server show the utility of the technique in capturing failures and anomalies. On average, our pattern-based classification technique outperforms the baseline approach by 24.68% in accuracy.

#index 1214701
#* Ranking-based clustering of heterogeneous information networks with star network schema
#@ 19945 24022 961
#t 2009
#c Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 268079
#% 283833
#% 290830
#% 466675
#% 577273
#% 592143
#% 750863
#% 769906
#% 769967
#% 805896
#% 840840
#% 876018
#% 989654
#% 1063503
#% 1063512
#% 1074127
#% 1181261
#% 1650298
#! A heterogeneous information network is an information network composed of multiple types of objects. Clustering on such a network may lead to better understanding of both hidden structures of the network and the individual role played by every object in each cluster. However, although clustering on homogeneous networks has been studied over decades, clustering on heterogeneous networks has not been addressed until recently. A recent study proposed a new algorithm, RankClus, for clustering on bi-typed heterogeneous networks. However, a real-world network may consist of more than two types, and the interactions among multi-typed objects play a key role at disclosing the rich semantics that a network carries. In this paper, we study clustering of multi-typed heterogeneous networks with a star network schema and propose a novel algorithm, NetClus, that utilizes links across multityped objects to generate high-quality net-clusters. An iterative enhancement method is developed that leads to effective ranking-based clustering in such heterogeneous networks. Our experiments on DBLP data show that NetClus generates more accurate clustering results than the baseline topic model algorithm PLSA and the recently proposed algorithm, RankClus. Further, NetClus generates informative clusters, presenting good ranking and cluster membership information for each attribute object in each net-cluster.

#index 1214717
#* Exploring social tagging graph for web object classification
#@ 19944 14303 13840 961
#t 2009
#c Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 248810
#% 451617
#% 458379
#% 466922
#% 785366
#% 869504
#% 869525
#% 869527
#% 956544
#% 1006582
#% 1035588
#% 1055739
#% 1055743
#% 1074116
#% 1074129
#% 1131829
#% 1269755
#! This paper studies web object classification problem with the novel exploration of social tags. Automatically classifying web objects into manageable semantic categories has long been a fundamental preprocess for indexing, browsing, searching, and mining these objects. The explosive growth of heterogeneous web objects, especially non-textual objects such as products, pictures, and videos, has made the problem of web classification increasingly challenging. Such objects often suffer from a lack of easy-extractable features with semantic information, interconnections between each other, as well as training examples with category labels. In this paper, we explore the social tagging data to bridge this gap. We cast web object classification problem as an optimization problem on a graph of objects and tags. We then propose an efficient algorithm which not only utilizes social tags as enriched semantic features for the objects, but also infers the categories of unlabeled objects from both homogeneous and heterogeneous labeled objects, through the implicit connection of social tags. Experiment results show that the exploration of social tags effectively boosts web object classification. Our algorithm significantly outperforms the state-of-the-art of general classification methods.

#index 1267760
#* Integrating Novel Class Detection with Classification for Concept-Drifting Data Streams
#@ 22167 17596 10403 961 3712
#t 2009
#c ECML PKDD '09 Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II
#% 342600
#% 577297
#% 729932
#% 731721
#% 823408
#% 840891
#% 957742
#% 1176894
#% 1206700
#! In a typical data stream classification task, it is assumed that the total number of classes are fixed. This assumption may not be valid in a real streaming environment, where new classes may evolve. Traditional data stream classification techniques are not capable of recognizing novel class instances until the appearance of the novel class is manually identified, and labeled instances of that class are presented to the learning algorithm for training. The problem becomes more challenging in the presence of concept-drift, when the underlying data distribution changes over time. We propose a novel and efficient technique that can automatically detect the emergence of a novel class in the presence of concept-drift by quantifying cohesion among unlabeled test instances, and separation of the test instances from training instances. Our approach is non-parametric, meaning, it does not assume any underlying distributions of data. Comparison with the state-of-the-art stream classification techniques prove the superiority of our approach.

#index 1269753
#* Isometric projection
#@ 8278 10859 961
#t 2007
#c AAAI'07 Proceedings of the 22nd national conference on Artificial intelligence - Volume 1
#% 70370
#% 235342
#% 317525
#% 341596
#% 729437
#% 791402
#% 800190
#% 835741
#% 836827
#! Recently the problem of dimensionality reduction has received a lot of interests in many fields of information processing. We consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional Euclidean space. The most popular manifold learning algorithms include Locally Linear Embedding, ISOMAP, and Laplacian Eigenmap. However, these algorithms are nonlinear and only provide the embedding results of training samples. In this paper, we propose a novel linear dimensionality reduction algorithm, called Isometric Projection. Isometric Projection constructs a weighted data graph where the weights are discrete approximations of the geodesic distances on the data manifold. A linear subspace is then obtained by preserving the pairwise distances. In this way, Isometric Projection can be defined everywhere. Comparing to Principal Component Analysis (PCA) which is widely used in data processing, our algorithm is more capable of discovering the intrinsic geometrical structure. Specially, PCA is optimal only when the data space is linear, while our algorithm has no such assumption and therefore can handle more complex data space. Experimental results on two real life data sets illustrate the effectiveness of the proposed method.

#index 1270185
#* Sparse projections over graph
#@ 8278 10859 961
#t 2008
#c AAAI'08 Proceedings of the 23rd national conference on Artificial intelligence - Volume 2
#% 80995
#% 643008
#% 729437
#% 791402
#% 835741
#% 837604
#% 876025
#! Recent study has shown that canonical algorithms such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) can be obtained from graph based dimensionality reduction framework. However, these algorithms yield projective maps which are linear combination of all the original features. The results are difficult to be interpreted psychologically and physiologically. This paper presents a novel technique for learning a sparse projection over graphs. The data in the reduced subspace is represented as a linear combination of a subset of the most relevant features. Comparing to PCA and LDA, the results obtained by sparse projection are often easier to be interpreted. Our algorithm is based on a graph embedding model, which encodes the discriminating and geometrical structure in terms of the data affinity. Once the embedding results are obtained, we then apply regularized regression for learning a set of sparse basis functions. Specifically, by using L1-norm regularizer (e.g. lasso), the sparse projections can be efficiently computed. Experimental results on two document databases demonstrate the effectiveness of our method.

#index 1274866
#* Locality sensitive discriminant analysis
#@ 8278 10859 18360 961 27113
#t 2007
#c IJCAI'07 Proceedings of the 20th international joint conference on Artifical intelligence
#% 235342
#% 305004
#% 443605
#% 729437
#% 812578
#% 812580
#% 1022958
#! Linear Discriminant Analysis (LDA) is a popular data-analytic tool for studying the class relationship between data points. A major disadvantage of LDA is that it fails to discover the local geometrical structure of the data manifold. In this paper, we introduce a novel linear algorithm for discriminant analysis, called Locality Sensitive Discriminant Analysis (LSDA). When there is no sufficient training samples, local structure is generally more important than global structure for discriminant analysis. By discovering the local manifold structure, LSDA finds a projection which maximizes the margin between data points from different classes at each local area. Specifically, the data points are mapped into a subspace in which the nearby points with the same label are close to each other while the nearby points with different labels are far apart. Experiments carried out on several standard face databases show a clear improvement over the results of LDA-based recognition.

#index 1305450
#* Locality preserving nonnegative matrix factorization
#@ 8278 10859 13801 27113 961
#t 2009
#c IJCAI'09 Proceedings of the 21st international jont conference on Artifical intelligence
#% 313959
#% 643008
#% 729437
#% 837604
#% 876918
#% 961218
#% 1176865
#% 1176946
#! Matrix factorization techniques have been frequently applied in information processing tasks. Among them, Non-negative Matrix Factorization (NMF) have received considerable attentions due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts-based in human brain. On the other hand, from geometric perspective the data is usually sampled from a low dimensional manifold embedded in high dimensional ambient space. One hopes then to find a compact representation which uncovers the hidden topics and simultaneously respects the intrinsic geometric structure. In this paper, we propose a novel algorithm, called Locality Preserving Non-negative Matrix Factorization (LPNMF), for this purpose. For two data points, we use KL-divergence to evaluate their similarity on the hidden topics. The optimal maps are obtained such that the feature values on hidden topics are restricted to be non-negative and vary smoothly along the geodesics of the data manifold. Our empirical study shows the encouraging results of the proposed algorithm in comparisons to the state-of-the-art algorithms on two large high-dimensional databases.

#index 1314743
#* Topic modeling for OLAP on multidimensional text databases: topic cube and its applications
#@ 20185 3494 961 28568 28569
#t 2009
#c Statistical Analysis and Data Mining - Best of SDM'09
#! As the amount of textual information grows explosively in various kinds of business systems, it becomes more and more desirable to analyze both structured data records and unstructured text data simultaneously. Although online analytical processing (OLAP) techniques have been proven very useful for analyzing and mining structured data, they face challenges in handling text data. On the other hand, probabilistic topic models are among the most effective approaches to latent topic analysis and mining on text data. In this paper, we study a new data model called topic cube to combine OLAP with probabilistic topic modeling and enable OLAP on the dimension of text data in a multidimensional text database. Topic cube extends the traditional data cube to cope with a topic hierarchy and stores probabilistic content measures of text documents learned through a probabilistic topic model. To materialize topic cubes efficiently, we propose two heuristic aggregations to speed up the iterative Expectation-Maximization (EM) algorithm for estimating topic models by leveraging the models learned on component data cells to choose a good starting point for iteration. Experimental results show that these heuristic aggregations are much faster than the baseline method of computing each topic cube from scratch. We also discuss some potential uses of topic cube and show sample experimental results. Copyright © 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 2: 378-395, 2009

#index 1318605
#* Parallel PathFinder Algorithms for Mining Structures from Graphs
#@ 28604 3494 961
#t 2009
#c ICDM '09 Proceedings of the 2009 Ninth IEEE International Conference on Data Mining
#! PathFinder networks are increasingly used in Data Mining for different purposes, like network visualization or knowledge extraction. This novel way of representing graphical data has been proven to give better results than other link reduction algorithms, like minimum spanning networks However, this increase in quality comes with a high computation cost, typically of the order of n^3 or higher, where n is the number of nodes in the graph. While this problem has previously been tackled by using mathematical properties to speed up the algorithm, in this paper, we propose two new algorithms to speed up PathFinder computation based on parallelization techniques to take advantage of the increasingly available multi-core hardware platform. Experiments show that both new algorithms are more efficient than the state of the art algorithms; one of them can achieve speed-ups of up to x127 with an average of x23 on recent hardware (2007).

#index 1318691
#* iTopicModel: Information Network-Integrated Topic Modeling
#@ 19945 961 17596 24022
#t 2009
#c ICDM '09 Proceedings of the 2009 Ninth IEEE International Conference on Data Mining
#! Document networks, i.e., networks associated with text information, are becoming increasingly popular due to the ubiquity of Web documents, blogs, and various kinds of online data. In this paper, we propose a novel topic modeling framework for document networks, which builds a unified generative topic model that is able to consider both text and structure information for documents. A graphical model is proposed to describe the generative model. On the top layer of this graphical model, we define a novel multivariate Markov Random Field for topic distribution random variables for each document, to model the dependency relationships among documents over the network structure. On the bottom layer, we follow the traditional topic model to model the generation of text for each document. A joint distribution function for both the text and structure of the documents is thus provided. A solution to estimate this topic model is given, by maximizing the log-likelihood of the joint probability. Some important practical issues in real applications are also discussed, including how to decide the topic number and how to choose a good network structure. We apply the model on two real datasets, DBLP and Cora, and the experiments show that this model is more effective in comparison with the state-of-the-art topic modeling algorithms.

#index 1318704
#* Filtering and Refinement: A Two-Stage Approach for Efficient and Effective Anomaly Detection
#@ 28701 28702 961
#t 2009
#c ICDM '09 Proceedings of the 2009 Ninth IEEE International Conference on Data Mining
#! Anomaly detection is an important data mining task. Most existing methods treat anomalies as inconsistencies and spend the majority amount of time on modeling normal instances. A recently proposed, sampling-based approach may substantially boost the efficiency in anomaly detection but may also lead to weaker accuracy and robustness. In this study, we propose a two-stage approach to find anomalies in complex datasets with high accuracy as well as low time complexity and space cost. Instead of analyzing normal instances, our algorithm first employs an efficient deterministic space partition algorithm to eliminate obvious normal instances and generates a small set of anomaly candidates with a single scan of the dataset. It then checks each candidate with density-based multiple criteria to determine the final results. This two-stage framework also detects anomalies of different notions. Our experiments show that this new approach finds anomalies successfully in different conditions and ensures a good balance of efficiency, accuracy, and robustness.

#index 1318825
#* Mining Personal Image Collection for Social Group Suggestion
#@ 11636 11010 961 11564
#t 2009
#c ICDMW '09 Proceedings of the 2009 IEEE International Conference on Data Mining Workshops
#! Popular photo-sharing sites have attracted millions of people and helped construct massive social networks in cyberspace. Different from traditional social relationship, users actively interact within groups where common interests are shared on certain types of events or topics captured by photos and videos. Contributing images to a group would greatly promote the interactions between users and expand their social networks. In this work, we intend to produce accurate predictions of suitable photo-sharing groups from a user's images by mining images both on the Web and in the user’s personal collection. To this end, we designed a new approach to cluster popular groups into categories by analyzing the similarity of groups via SimRank. Both visual content and its annotations are integrated to understand the events or topics depicted in the images. Experiments on real user images demonstrate the feasibility of the proposed approach.

#index 1328094
#* iNextCube: information network-enhanced text cube
#@ 24022 29008 19945 15196 961 29009 19229 3494 20185 22156
#t 2009
#c Proceedings of the VLDB Endowment
#% 210182
#% 280819
#% 340146
#% 893143
#% 1019106
#% 1176884
#% 1181261
#% 1214701
#% 1314743
#% 1715634
#! Nowadays, most business, administration, and/or scientific databases contain both structured attributes and text attributes. We call a database that consists of both multidimensional structured data and narrative text data as multidimensional text database. Searching, OLAP, and mining such databases pose many research challenges. To enhance the power of data analysis, interesting entities and relationships can be extracted from such databases to derive heterogeneous information networks, which in turn will substantially increase the power and flexibility of data exploration in such databases. Based on our previous studies on TextCube [1], TopicCube [2], and information network analysis, such as RankClus [3] and NetClus [4], we construct iNextCube, an information-Network-enhanced text Cube. In this demo, we show the power of iNextCube in the search and analysis of two multidimensional text databases: (i) a DBLP-based CS bibliographic database, and (ii) an online news database.

#index 1328118
#* Promotion analysis in multi-dimensional space
#@ 19229 12167 13840 961
#t 2009
#c Proceedings of the VLDB Endowment
#% 273902
#% 273916
#% 333854
#% 420053
#% 420082
#% 765155
#% 875003
#% 875025
#% 956547
#% 1016203
#% 1063474
#% 1063475
#% 1075132
#% 1206698
#! Promotion is one of the key ingredients in marketing. It is often desirable to find merit in an object (e.g., product, person, organization, or service) and promote it in an appropriate community. In this paper, we propose a novel functionality, called promotion analysis through ranking, for promoting a given object by leveraging highly ranked results. Since the object may not be highly ranked in the global space, our goal is to discover promotive subspaces in which the object becomes prominent. To achieve this goal, the notion of promotiveness is formulated. We show that this functionality is practical and useful in a wide variety of applications such as business intelligence. However, computing promotive subspaces is challenging due to the explosion of search space and high aggregation cost. For efficient computation, we propose a PromoRank framework, and develop three efficient optimization techniques, namely subspace pruning, object pruning, and promotion cube, which are seamlessly integrated into the framework. Our empirical evaluation on two real data sets confirms the effectiveness of promotion analysis, and that our proposed algorithms significantly outperform baseline solutions.

#index 1328161
#* A particle-and-density based evolutionary clustering method for dynamic networks
#@ 12242 961
#t 2009
#c Proceedings of the VLDB Endowment
#% 479658
#% 810066
#% 823342
#% 823344
#% 881460
#% 881514
#% 881523
#% 960283
#% 989586
#% 989640
#% 989643
#% 989654
#% 989663
#% 1022269
#% 1030883
#% 1055740
#% 1083675
#% 1083699
#% 1914479
#! Recently, dynamic networks are attracting increasing interest due to their high potential in capturing natural and social phenomena over time. Discovery of evolutionary communities in dynamic networks has become a critical task. The previous evolutionary clustering methods usually adopt the temporal smoothness framework, which has a desirable feature of controlling the balance between temporal noise and true concept drift of communities. They, however, have some major drawbacks: (1) assuming only a fixed number of communities over time; and (2) not allowing arbitrary start/stop of community over time. The forming of new communities and dissolving of existing communities are very common phenomena in real dynamic networks. In this paper, we propose a new particle-and-density based evolutionary clustering method that efficiently discovers a variable number of communities of arbitrary forming and dissolving. We first model a dynamic network as a collection of lots of particles called nano-communities, and a community as a densely connected subset of particles, called a quasi l-clique-by-clique (shortly, l-KK). Each particle contains a small amount of information about the evolution of data or patterns, and the quasi l-KKs inherent in a given dynamic network provide us with guidance on how to find a variable number of communities of arbitrary forming and dissolving. We propose a density-based clustering method that efficiently finds temporally smoothed local clusters of high quality by using a cost embedding technique and optimal modularity. We also propose a mapping method based on information theory that makes sequences of smoothed local clusters as close as possible to data-inherent quasi l-KKs. The result of the mapping method allows us to easily identify the stage of each community among the three stages: evolving, forming, and dissolving. Experimental studies, by using various data sets, demonstrate that our method improves the clustering accuracy, and at the same time, the time performance by an order of magnitude compared with the current state-of-the art method.

#index 1328171
#* Mining graph patterns efficiently via randomized summaries
#@ 15196 29008 29048 29049 9413 961
#t 2009
#c Proceedings of the VLDB Endowment
#% 300120
#% 342604
#% 431105
#% 466644
#% 480810
#% 481290
#% 481779
#% 629708
#% 745477
#% 765429
#% 769940
#% 813990
#% 823342
#% 823347
#% 841960
#% 867050
#% 869492
#% 881466
#% 894441
#% 994157
#% 1063501
#% 1063502
#% 1063512
#% 1117006
#% 1117010
#% 1127358
#% 1176876
#! Graphs are prevalent in many domains such as Bioinformatics, social networks, Web and cyber-security. Graph pattern mining has become an important tool in the management and analysis of complexly structured data, where example applications include indexing, clustering and classification. Existing graph mining algorithms have achieved great success by exploiting various properties in the pattern space. Unfortunately, due to the fundamental role subgraph isomorphism plays in these methods, they may all enter into a pitfall when the cost to enumerate a huge set of isomorphic embeddings blows up, especially in large graphs. The solution we propose for this problem resorts to reduction on the data space. For each graph, we build a summary of it and mine this shrunk graph instead. Compared to other data reduction techniques that either reduce the number of transactions or compress between transactions, this new framework, called Summarize-Mine, suggests a third path by compressing within transactions. Summarize-Mine is effective in cutting down the size of graphs, thus decreasing the embedding enumeration cost. However, compression might lose patterns at the same time. We address this issue by generating randomized summaries and repeating the process for multiple rounds, where the main idea is that true patterns are unlikely to miss from all rounds. We provide strict probabilistic guarantees on pattern loss likelihood. Experiments on real malware trace data show that Summarize-Mine is very efficient, which can find interesting malware fingerprints that were not revealed previously.

#index 1393131
#* Research frontiers in advanced data mining technologies and applications
#@ 961
#t 2007
#c PAKDD'07 Proceedings of the 11th Pacific-Asia conference on Advances in knowledge discovery and data mining
#% 818916
#! Research in data mining has two general directions: theoretical foundations and advanced technologies and applications. In this talk, we will focus on the research issues for advanced technologies and applications in data mining and discuss some recent progress in this direction, including (1) pattern mining, usage, and understanding, (2) information network analysis, (3) stream data mining, (4) mining moving object data, RFID data, and data from sensor networks, (5) spatiotemporal and multimedia data mining, (6) biological data mining, (7) text and Web mining, (8) data mining for software engineering and computer system analysis, and (9) data cube-oriented multidimensional online analytical processing.

#index 1393168
#* gPrune: a constraint pushing framework for graph pattern mining
#@ 15170 9413 961 850
#t 2007
#c PAKDD'07 Proceedings of the 11th Pacific-Asia conference on Advances in knowledge discovery and data mining
#% 248785
#% 300120
#% 310494
#% 466644
#% 481290
#% 580588
#% 629603
#% 629623
#% 629646
#% 629708
#% 674497
#% 727845
#% 727896
#% 729938
#% 765429
#% 766666
#% 769889
#% 769940
#% 769951
#% 813034
#% 813990
#% 823357
#% 1656291
#% 1707794
#! In graph mining applications, there has been an increasingly strong urge for imposing user-specified constraints on the mining results. However, unlike most traditional itemset constraints, structural constraints, such as density and diameter of a graph, are very hard to be pushed deep into the mining process. In this paper, we give the first comprehensive study on the pruning properties of both traditional and structural constraints aiming to reduce not only the pattern search space but the data search space as well. A new general framework, called gPrune, is proposed to incorporate all the constraints in such a way that they recursively reinforce each other through the entire mining process. A new concept, Pattern-inseparable Data-antimonotonicity, is proposed to handle the structural constraints unique in the context of graph, which, combined with known pruning properties, provides a comprehensive and unified classification framework for structural constraints. The exploration of these antimonotonicities in the context of graph pattern mining is a significant extension to the known classification of constraints, and deepens our understanding of the pruning properties of structural graph constraints.

#index 1426623
#* MoveMine: mining moving object databases
#@ 23406 28410 12241 21437 24022 961 31093
#t 2010
#c Proceedings of the 2010 ACM SIGMOD International Conference on Management of data
#% 907380
#% 960283
#% 1206639
#% 1206688
#! With the maturity of GPS, wireless, and Web technologies, increasing amounts of movement data collected from various moving objects, such as animals, vehicles, mobile devices, and climate radars, have become widely available. Analyzing such data has broad applications, e.g., in ecological study, vehicle control, mobile communication management, and climatological forecast. However, few data mining tools are available for flexible and scalable analysis of massive-scale moving object data. Our system, MoveMine, is designed for sophisticated moving object data mining by integrating several attractive functions including moving object pattern mining and trajectory mining. We explore the state-of-the-art and novel techniques at implementation of the selected functions. A user-friendly interface is provided to facilitate interactive exploration of mining results and flexible tuning of the underlying methods. Since MoveMine is tested on multiple kinds of real data sets, it will benefit users to carry out versatile analysis on these kinds of data. At the same time, it will benefit researchers to realize the importance and limitations of current techniques as well as the potential future studies in moving object data mining.

#index 1426635
#* Mining knowledge from databases: an information network analysis approach
#@ 961 19945 9413 850
#t 2010
#c Proceedings of the 2010 ACM SIGMOD International Conference on Management of data
#% 268079
#% 577273
#% 865734
#% 989654
#% 1002279
#% 1081580
#% 1181261
#% 1214701
#% 1214717
#% 1394202
#! Most people consider a database is merely a data repository that supports data storage and retrieval. Actually, a database contains rich, inter-related, multi-typed data and information, forming one or a set of gigantic, interconnected, heterogeneous information networks. Much knowledge can be derived from such information networks if we systematically develop an effective and scalable database-oriented information network analysis technology. In this tutorial, we introduce database-oriented information network analysis methods and demonstrate how information networks can be used to improve data quality and consistency, facilitate data integration, and generate interesting knowledge. This tutorial presents an organized picture on how to turn a database into one or a set of organized heterogeneous information networks, how information networks can be used for data cleaning, data consolidation, and data qualify improvement, how to discover various kinds of knowledge from information networks, how to perform OLAP in information networks, and how to transform database data into knowledge by information network analysis. Moreover, we present interesting case studies on real datasets, including DBLP and Flickr, and show how interesting and organized knowledge can be generated from database-oriented information networks.

#index 1434110
#* Mining useful patterns: my evolutionary view
#@ 961
#t 2010
#c Proceedings of the ACM SIGKDD Workshop on Useful Patterns
#! Pattern mining has been studied in the data mining community for over 15 years, with lots of interesting results and methods reported. One critical issue in pattern mining is the usefulness of patterns, i.e., what patterns are likely useful for what applications, instead of yet another efficient pattern mining algorithm. In my talk, I will discuss my evolutionary view on the usefulness of patterns and present a set of examples on what patterns are considered to be useful in certain practice. This may give some insight on pattern analysis, based on my own study, and point out a few open research problems and possible exploration of broad applications of pattern mining.

#index 1434118
#* Authorship classification: a syntactic tree mining approach
#@ 31175 31176 31177 961
#t 2010
#c Proceedings of the ACM SIGKDD Workshop on Useful Patterns
#% 27842
#% 428405
#% 464996
#% 577218
#% 578558
#% 729938
#% 729941
#% 729957
#% 745515
#% 789011
#% 844421
#% 852013
#% 916421
#% 939736
#% 1019151
#% 1035127
#% 1063502
#% 1190357
#% 1206650
#% 1214677
#% 1227600
#% 1261581
#% 1665151
#% 1673557
#% 1676557
#% 1682064
#! In the past, there have been dozens of studies on automatic authorship classification, and many of these studies concluded that the writing style is one of the best indicators of original authorship. From among the hundreds of features which were developed, syntactic features were best able to reflect an author's writing style. However, due to the high computational complexity of extracting and computing syntactic features, only simple variations of basic syntactic features of function words and part-of-speech tags were considered. In this paper, we propose a novel approach to mining discriminative k-embedded-edge subtree patterns from a given set of syntactic trees that reduces the computational burden of using complex syntactic structures as a feature set. This method is shown to increase the classification accuracy. We also design a new kernel based on these features. Comprehensive experiments on real datasets of news articles and movie reviews demonstrate that our approach is reliable and more accurate than previous studies.

#index 1451159
#* Mining advisor-advisee relationships from research publication networks
#@ 24023 961 31407 12695 20185 24022 31408
#t 2010
#c Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 261205
#% 268079
#% 387427
#% 727824
#% 801324
#% 891559
#% 1000502
#% 1176886
#% 1214701
#% 1214703
#% 1269756
#% 1465175
#% 1810385
#! Information network contains abundant knowledge about relationships among people or entities. Unfortunately, such kind of knowledge is often hidden in a network where different kinds of relationships are not explicitly categorized. For example, in a research publication network, the advisor-advisee relationships among researchers are hidden in the coauthor network. Discovery of those relationships can benefit many interesting applications such as expert finding and research community analysis. In this paper, we take a computer science bibliographic network as an example, to analyze the roles of authors and to discover the likely advisor-advisee relationships. In particular, we propose a time-constrained probabilistic factor graph model (TPFG), which takes a research publication network as input and models the advisor-advisee relationship mining problem using a jointly likelihood objective function. We further design an efficient learning algorithm to optimize the objective function. Based on that our model suggests and ranks probable advisors for every author. Experimental results show that the proposed approach infer advisor-advisee relationships efficiently and achieves a state-of-the-art accuracy (80-90%). We also apply the discovered advisor-advisee relationships to bole search, a specific expert finding task and empirical study shows that the search performance can be effectively improved (+4.09% by NDCG@5).

#index 1451221
#* On community outliers and their efficient detection in information networks
#@ 17596 13929 2485 24023 19945 961
#t 2010
#c Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 191603
#% 274612
#% 290830
#% 300136
#% 313959
#% 342638
#% 466745
#% 570886
#% 577273
#% 729983
#% 731721
#% 769881
#% 785358
#% 812382
#% 844334
#% 916785
#% 975040
#% 989618
#% 989654
#% 995140
#% 1000502
#% 1063629
#% 1130929
#% 1214714
#% 1318671
#! Linked or networked data are ubiquitous in many applications. Examples include web data or hypertext documents connected via hyperlinks, social networks or user profiles connected via friend links, co-authorship and citation information, blog data, movie reviews and so on. In these datasets (called "information networks"), closely related objects that share the same properties or interests form a community. For example, a community in blogsphere could be users mostly interested in cell phone reviews and news. Outlier detection in information networks can reveal important anomalous and interesting behaviors that are not obvious if community information is ignored. An example could be a low-income person being friends with many rich people even though his income is not anomalously low when considered over the entire population. This paper first introduces the concept of community outliers (interesting points or rising stars for a more positive sense), and then shows that well-known baseline approaches without considering links or community information cannot find these community outliers. We propose an efficient solution by modeling networked data as a mixture model composed of multiple normal communities and a set of randomly generated outliers. The probabilistic model characterizes both data and links simultaneously by defining their joint distribution based on hidden Markov random fields (HMRF). Maximizing the data likelihood and the posterior of the model gives the solution to the outlier inference problem. We apply the model on both synthetic data and DBLP data sets, and the results demonstrate importance of this concept, as well as the effectiveness and efficiency of the proposed approach.

#index 1451233
#* PET: a statistical model for popular events tracking in social communities
#@ 22155 22156 13840 961
#t 2010
#c Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 338741
#% 342707
#% 577220
#% 722904
#% 729943
#% 754107
#% 769967
#% 823344
#% 824666
#% 868469
#% 881460
#% 881476
#% 907511
#% 989650
#% 1055681
#% 1083732
#% 1214669
#% 1269909
#% 1292518
#% 1318691
#% 1399993
#% 1650298
#% 1655304
#! User generated information in online communities has been characterized with the mixture of a text stream and a network structure both changing over time. A good example is a web-blogging community with the daily blog posts and a social network of bloggers. An important task of analyzing an online community is to observe and track the popular events, or topics that evolve over time in the community. Existing approaches usually focus on either the burstiness of topics or the evolution of networks, but ignoring the interplay between textual topics and network structures. In this paper, we formally define the problem of popular event tracking in online communities (PET), focusing on the interplay between texts and networks. We propose a novel statistical method that models the the popularity of events over time, taking into consideration the burstiness of user interest, information diffusion on the network structure, and the evolution of textual topics. Specifically, a Gibbs Random Field is defined to model the influence of historic status and the dependency relationships in the graph; thereafter a topic model generates the words in text content of the event, regularized by the Gibbs Random Field. We prove that two classic models in information diffusion and text burstiness are special cases of our model under certain situations. Empirical experiments with two different communities and datasets (i.e., Twitter and DBLP) show that our approach is effective and outperforms existing approaches.

#index 1451250
#* Mining periodic behaviors for moving objects
#@ 23406 17321 961 31093 31492
#t 2010
#c Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 310542
#% 342642
#% 464986
#% 480156
#% 565487
#% 629677
#% 769899
#% 810060
#% 813978
#% 823356
#% 844299
#% 881542
#% 975028
#% 1206625
#! Periodicity is a frequently happening phenomenon for moving objects. Finding periodic behaviors is essential to understanding object movements. However, periodic behaviors could be complicated, involving multiple interleaving periods, partial time span, and spatiotemporal noises and outliers. In this paper, we address the problem of mining periodic behaviors for moving objects. It involves two sub-problems: how to detect the periods in complex movement, and how to mine periodic movement behaviors. Our main assumption is that the observed movement is generated from multiple interleaved periodic behaviors associated with certain reference locations. Based on this assumption, we propose a two-stage algorithm, Periodica, to solve the problem. At the first stage, the notion of observation spot is proposed to capture the reference locations. Through observation spots, multiple periods in the movement can be retrieved using a method that combines Fourier transform and autocorrelation. At the second stage, a probabilistic model is proposed to characterize the periodic behaviors. For a specific period, periodic behaviors are statistically generalized from partial movement sequences through hierarchical clustering. Empirical studies on both synthetic and real data sets demonstrate the effectiveness of our method.

#index 1477784
#* Mining heterogeneous information networks
#@ 961
#t 2010
#c Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining

#index 1482198
#* Mining topic-level influence in heterogeneous networks
#@ 31936 12695 961 31937 31938
#t 2010
#c CIKM '10 Proceedings of the 19th ACM international conference on Information and knowledge management
#% 27724
#% 342596
#% 348173
#% 577217
#% 729923
#% 754098
#% 754107
#% 956540
#% 983833
#% 1055737
#% 1083624
#% 1083641
#% 1083672
#% 1083684
#% 1083738
#% 1181261
#% 1214638
#% 1214696
#% 1214701
#% 1214702
#% 1214703
#% 1214714
#% 1214722
#% 1355040
#% 1399993
#% 1400031
#% 1451243
#! Influence is a complex and subtle force that governs the dynamics of social networks as well as the behaviors of involved users. Understanding influence can benefit various applications such as viral marketing, recommendation, and information retrieval. However, most existing works on social influence analysis have focused on verifying the existence of social influence. Few works systematically investigate how to mine the strength of direct and indirect influence between nodes in heterogeneous networks. To address the problem, we propose a generative graphical model which utilizes the heterogeneous link information and the textual content associated with each node in the network to mine topic-level direct influence. Based on the learned direct influence, a topic-level influence propagation and aggregation algorithm is proposed to derive the indirect influence between nodes. We further study how the discovered topic-level influence can help the prediction of user behaviors. We validate the approach on three different genres of data sets: Twitter, Digg, and citation networks. Qualitatively, our approach can discover interesting influence patterns in heterogeneous networks. Quantitatively, the learned topic-level influence can greatly improve the accuracy of user behavior prediction.

#index 1482200
#* SHRINK: a structural clustering algorithm for detecting hierarchical communities in networks
#@ 31941 31942 961 22160 19945 31943
#t 2010
#c CIKM '10 Proceedings of the 19th ACM international conference on Information and knowledge management
#% 273890
#% 282905
#% 313959
#% 342596
#% 989654
#% 991977
#% 1914479
#! Community detection is an important task for mining the structure and function of complex networks. Generally, there are several different kinds of nodes in a network which are cluster nodes densely connected within communities, as well as some special nodes like hubs bridging multiple communities and outliers marginally connected with a community. In addition, it has been shown that there is a hierarchical structure in complex networks with communities embedded within other communities. Therefore, a good algorithm is desirable to be able to not only detect hierarchical communities, but also identify hubs and outliers. In this paper, we propose a parameter-free hierarchical network clustering algorithm SHRINK by combining the advantages of density-based clustering and modularity optimization methods. Based on the structural connectivity information, the proposed algorithm can effectively reveal the embedded hierarchical community structure with multiresolution in large-scale weighted undirected networks, and identify hubs and outliers as well. Moreover, it overcomes the sensitive threshold problem of density-based clustering algorithms and the resolution limit possessed by other modularity-based methods. To illustrate our methodology, we conduct experiments with both real-world and synthetic datasets for community detection, and compare with many other baseline methods. Experimental results demonstrate that SHRINK achieves the best performance with consistent improvements.

#index 1482263
#* Visual cube and on-line analytical processing of images
#@ 11010 961 32008 11564 17321 22155
#t 2010
#c CIKM '10 Proceedings of the 19th ACM international conference on Information and knowledge management
#% 227880
#% 248865
#% 420053
#% 818916
#% 823345
#% 837616
#% 915249
#% 954969
#% 1016131
#% 1019162
#% 1040539
#% 1090318
#% 1176884
#! On-Line Analytical Processing (OLAP) has shown great success in many industry applications, including sales, marketing, management, financial data analysis, etc. In this paper, we propose Visual Cube and multi-dimensional OLAP of image collections, such as web images indexed in search engines (e.g., Google and Bing), product images (e.g. Amazon) and photos shared on social networks (e.g., Facebook and Flickr). It provides online responses to user requests with summarized statistics of image information and handles rich semantics related to image visual features. A clustering structure measure is proposed to help users freely navigate and explore images. Efficient algorithms are developed to construct Visual Cube. In addition, we introduce the new issue of Cell Overlapping in data cube and present efficient solutions for Visual Cube computation and OLAP operations. Extensive experiments are conducted and the results show good performance of our algorithms.

#index 1482398
#* Mapping web pages to database records via link paths
#@ 31177 21685 961 6955
#t 2010
#c CIKM '10 Proceedings of the 19th ACM international conference on Information and knowledge management
#% 268079
#% 340928
#% 503216
#! In this paper we propose a new knowledge management task which aims to map Web pages to their corresponding records in a structured database. For example, the DBLP database contains records for many computer scientists, and most of these persons have public Web pages; if we can map the database record with the appropriate Web page then the new information could be used to further describe the person's database record. To accomplish this goal we employ link paths which contain anchor texts from multiple paths through the Web ending at the Web page in question. We hypothesize that the information from these link paths can be used to generate an accurate Web page to database record mapping. Experiments on two large, real world data sets, DBLP and IMDB for the structured data and computer science faculty members' Web pages and official movie homepages for the Web page data, show that our method does provide an accurate mapping. Finally, we conclude by issuing a call for further research on this promising new task.

#index 1490608
#* Survey on social tagging techniques
#@ 23141 14303 19944 961
#t 2010
#c ACM SIGKDD Explorations Newsletter
#% 722904
#% 869482
#% 869525
#% 869548
#% 874542
#% 881054
#% 905319
#% 955010
#% 956515
#% 956544
#% 956589
#% 987205
#% 1025695
#% 1035588
#% 1055704
#% 1055739
#% 1055743
#% 1065400
#% 1065405
#% 1074070
#% 1074115
#% 1074116
#% 1083745
#% 1089475
#% 1098457
#% 1155630
#% 1166510
#% 1166511
#% 1190090
#% 1190091
#% 1214717
#% 1292651
#% 1667787
#% 1732610
#! Social tagging on online portals has become a trend now. It has emerged as one of the best ways of associating metadata with web objects. With the increase in the kinds of web objects becoming available, collaborative tagging of such objects is also developing along new dimensions. This popularity has led to a vast literature on social tagging. In this survey paper, we would like to summarize different techniques employed to study various aspects of tagging. Broadly, we would discuss about properties of tag streams, tagging models, tag semantics, generating recommendations using tags, visualizations of tags, applications of tags and problems associated with tagging usage. We would discuss topics like why people tag, what influences the choice of tags, how to model the tagging process, kinds of tags, different power laws observed in tagging domain, how tags are created, how to choose the right tags for recommendation, etc. We conclude with thoughts on future work in the area.

#index 1495539
#* Structure is informative: on mining structured information networks
#@ 961
#t 2010
#c ECML PKDD'10 Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases: Part I

#index 1495579
#* Graph regularized transductive classification on heterogeneous information networks
#@ 28410 19945 32500 961 17596
#t 2010
#c ECML PKDD'10 Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases: Part I
#% 248810
#% 727834
#% 729982
#% 840840
#% 876018
#% 881557
#% 961218
#% 961268
#% 961278
#% 1214701
#% 1214717
#% 1273915
#% 1289267
#% 1650403
#! A heterogeneous information network is a network composed of multiple types of objects and links. Recently, it has been recognized that strongly-typed heterogeneous information networks are prevalent in the real world. Sometimes, label information is available for some objects. Learning from such labeled and unlabeled data via transductive classification can lead to good knowledge extraction of the hidden network structure. However, although classification on homogeneous networks has been studied for decades, classification on heterogeneous networks has not been explored until recently. In this paper, we consider the transductive classification problem on heterogeneous networked data which share a common topic. Only some objects in the given network are labeled, and we aim to predict labels for all types of the remaining objects. A novel graph-based regularization framework, GNetMine, is proposed to model the link structure in information networks with arbitrary network schema and arbitrary number of object/link types. Specifically, we explicitly respect the type differences by preserving consistency over each relation graph corresponding to each type of links separately. Efficient computational schemes are then introduced to solve the corresponding optimization problem. Experiments on the DBLP data set show that our algorithm significantly improves the classification accuracy over existing state-of-the-art methods.

#index 1495585
#* NDPMine: efficiently mining discriminative numerical features for pattern-based classification
#@ 31176 31175 31177 961 32503
#t 2010
#c ECML PKDD'10 Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases: Part II
#% 235377
#% 345823
#% 425033
#% 466483
#% 483552
#% 729941
#% 789011
#% 1083649
#% 1083688
#% 1183448
#% 1206650
#% 1214677
#% 1214716
#% 1558464
#! Pattern-based classification has demonstrated its power in recent studies, but because the cost of mining discriminative patterns as features in classification is very expensive, several efficient algorithms have been proposed to rectify this problem. These algorithms assume that feature values of the mined patterns are binary, i.e., a pattern either exists or not. In some problems, however, the number of times a pattern appears is more informative than whether a pattern appears or not. To resolve these deficiencies, we propose a mathematical programming method that directly mines discriminative patterns as numerical features for classification. We also propose a novel search space shrinking technique which addresses the inefficiencies in iterative pattern mining algorithms. Finally, we show that our method is an order of magnitude faster, significantly more memory efficient and more accurate than current approaches.

#index 1495604
#* Classification and novel class detection of data streams in a dynamic feature space
#@ 22167 32521 17596 10403 961 3712
#t 2010
#c ECML PKDD'10 Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases: Part II
#% 342600
#% 729932
#% 769888
#% 823408
#% 840891
#% 915314
#% 1052684
#% 1176894
#% 1206700
#% 1267760
#! Data stream classification poses many challenges, most of which are not addressed by the state-of-the-art. We present DXMiner, which addresses four major challenges to data stream classification, namely, infinite length, concept-drift, concept-evolution, and feature-evolution. Data streams are assumed to be infinite in length, which necessitates single-pass incremental learning techniques. Concept-drift occurs in a data stream when the underlying concept changes over time. Most existing data stream classification techniques address only the infinite length and concept-drift problems. However, concept-evolution and feature- evolution are also major challenges, and these are ignored by most of the existing approaches. Concept-evolution occurs in the stream when novel classes arrive, and feature-evolution occurs when new features emerge in the stream. Our previous work addresses the concept-evolution problem in addition to addressing the infinite length and concept-drift problems. Most of the existing data stream classification techniques, including our previous work, assume that the feature space of the data points in the stream is static. This assumption may be impractical for some type of data, for example text data. DXMiner considers the dynamic nature of the feature space and provides an elegant solution for classification and novel class detection when the feature space is dynamic. We show that our approach outperforms state-of-the-art stream classification techniques in classifying and detecting novel classes in real data streams.

#index 1523825
#* On graph query optimization in large networks
#@ 18038 961
#t 2010
#c Proceedings of the VLDB Endowment
#% 288990
#% 378391
#% 404719
#% 410276
#% 641398
#% 660011
#% 723439
#% 765429
#% 772884
#% 798044
#% 824693
#% 864425
#% 937108
#% 960259
#% 960304
#% 960305
#% 1022280
#% 1063500
#% 1063514
#% 1108856
#% 1127380
#% 1181229
#% 1217208
#% 1328183
#% 1333435
#! The dramatic proliferation of sophisticated networks has resulted in a growing need for supporting effective querying and mining methods over such large-scale graph-structured data. At the core of many advanced network operations lies a common and critical graph query primitive: how to search graph structures efficiently within a large network? Unfortunately, the graph query is hard due to the NP-complete nature of subgraph isomorphism. It becomes even challenging when the network examined is large and diverse. In this paper, we present a high performance graph indexing mechanism, SPath, to address the graph query problem on large networks. SPath leverages decomposed shortest paths around vertex neighborhood as basic indexing units, which prove to be both effective in graph search space pruning and highly scalable in index construction and deployment. Via SPath, a graph query is processed and optimized beyond the traditional vertex-at-a-time fashion to a more efficient path-at-a-time way: the query is first decomposed to a set of shortest paths, among which a subset of candidates with good selectivity is picked by a query plan optimizer; Candidate paths are further joined together to help recover the query graph to finalize the graph query processing. We evaluate SPath with the state-of-the-art GraphQL on both real and synthetic data sets. Our experimental studies demonstrate the effectiveness and scalability of SPath, which proves to be a more practical and efficient indexing method in addressing graph queries on large networks.

#index 1523860
#* Swarm: mining relaxed temporal moving object clusters
#@ 23406 17321 961 31093
#t 2010
#c Proceedings of the VLDB Endowment
#% 462231
#% 481290
#% 518854
#% 659971
#% 729418
#% 729933
#% 784297
#% 810049
#% 875505
#% 907380
#% 957731
#% 960283
#% 1016195
#% 1127436
#% 1176982
#% 1206688
#% 1426623
#% 1720762
#! Recent improvements in positioning technology make massive moving object data widely available. One important analysis is to find the moving objects that travel together. Existing methods put a strong constraint in defining moving object cluster, that they require the moving objects to stick together for consecutive timestamps. Our key observation is that the moving objects in a cluster may actually diverge temporarily and congregate at certain timestamps. Motivated by this, we propose the concept of swarm which captures the moving objects that move within arbitrary shape of clusters for certain timestamps that are possibly non-consecutive. The goal of our paper is to find all discriminative swarms, namely closed swarm. While the search space for closed swarms is prohibitively huge, we design a method, ObjectGrowth, to efficiently retrieve the answer. In ObjectGrowth, two effective pruning strategies are proposed to greatly reduce the search space and a novel closure checking rule is developed to report closed swarms on-the-fly. Empirical studies on the real data as well as large synthetic data demonstrate the effectiveness and efficiency of our methods.

#index 1535425
#* Addressing Concept-Evolution in Concept-Drifting Data Streams
#@ 22167 32521 10403 23259 17596 961 3712
#t 2010
#c ICDM '10 Proceedings of the 2010 IEEE International Conference on Data Mining
#! The problem of data stream classification is challenging because of many practical aspects associated with efficient processing and temporal behavior of the stream. Two such well studied aspects are infinite length and concept-drift. Since a data stream may be considered a continuous process, which is theoretically infinite in length, it is impractical to store and use all the historical data for training. Data streams also frequently experience concept-drift as a result of changes in the underlying concepts. However, another important characteristic of data streams, namely, concept-evolution is rarely addressed in the literature. Concept-evolution occurs as a result of new classes evolving in the stream. This paper addresses concept-evolution in addition to the existing challenges of infinite-length and concept-drift. In this paper, the concept-evolution phenomenon is studied, and the insights are used to construct superior novel class detection techniques. First, we propose an adaptive threshold for outlier detection, which is a vital part of novel class detection. Second, we propose a probabilistic approach for novel class detection using discrete Gini Coefficient, and prove its effectiveness both theoretically and empirically. Finally, we address the issue of simultaneous multiple novel class occurrence, and provide an elegant solution to detect more than one novel classes at the same time. We also consider feature-evolution in text data streams, which occurs because new features (i.e., words) evolve in the stream. Comparison with state-of-the-art data stream classification techniques establishes the effectiveness of the proposed approach.

#index 1535465
#* gSkeletonClu: Density-Based Network Clustering via Structure-Connected Tree Division or Agglomeration
#@ 31942 31941 961 22160 18038 34772
#t 2010
#c ICDM '10 Proceedings of the 2010 IEEE International Conference on Data Mining
#! Community detection is an important task for mining the structure and function of complex networks. Many pervious approaches are difficult to detect communities with arbitrary size and shape, and are unable to identify hubs and outliers. A recently proposed network clustering algorithm, SCAN, is effective and can overcome this difficulty. However, it depends on a sensitive parameter: minimum similarity threshold $\varepsilon$, but provides no automated way to find it. In this paper, we propose a novel density-based network clustering algorithm, called gSkeletonClu (graph-skeleton based clustering). By projecting a network to its Core-Connected Maximal Spanning Tree (CCMST), the network clustering problem is converted to finding core-connected components in the CCMST. We discover that all possible values of the parameter $\varepsilon$ lie in the edge weights of the corresponding CCMST. By means of tree divisive or agglomerative clustering, our algorithm can find the optimal parameter $\varepsilon$ and detect communities, hubs and outliers in large-scale undirected networks automatically without any user interaction. Extensive experiments on both real-world and synthetic networks demonstrate the superior performance of gSkeletonClu over the baseline methods.

#index 1535468
#* Tru-Alarm: Trustworthiness Analysis of Sensor Networks in Cyber-Physical Systems
#@ 21437 28701 31175 961 34774 7839
#t 2010
#c ICDM '10 Proceedings of the 2010 IEEE International Conference on Data Mining
#! A Cyber-Physical System (CPS) integrates physical devices (e.g., sensors, cameras) with cyber (or informational)components to form a situation-integrated analytical system that responds intelligently to dynamic changes of the real-world scenarios. One key issue in CPS research is trustworthiness analysis of the observed data: Due to technology limitations and environmental influences, the CPS data are inherently noisy that may trigger many false alarms. It is highly desirable to sift meaningful information from a large volume of noisy data. In this paper, we propose a method called Tru-Alarm which finds out trustworthy alarms and increases the feasibility of CPS. Tru-Alarm estimates the locations of objects causing alarms, constructs an object-alarm graph and carries out trustworthiness inferences based on linked information in the graph. Extensive experiments show that Tru-Alarm filters out noises and false information efficiently and guarantees not missing any meaningful alarms.

#index 1538420
#* Speed up kernel discriminant analysis
#@ 8278 10859 961
#t 2011
#c The VLDB Journal — The International Journal on Very Large Data Bases
#% 80995
#% 105567
#% 252304
#% 341596
#% 420077
#% 443790
#% 480307
#% 726624
#% 810069
#% 819917
#% 857439
#% 876025
#% 946436
#% 997140
#% 1013661
#% 1019136
#% 1034713
#% 1117001
#% 1117038
#% 1164191
#% 1272365
#% 1656362
#% 1775495
#! Linear discriminant analysis (LDA) has been a popular method for dimensionality reduction, which preserves class separability. The projection vectors are commonly obtained by maximizing the between-class covariance and simultaneously minimizing the within-class covariance. LDA can be performed either in the original input space or in the reproducing kernel Hilbert space (RKHS) into which data points are mapped, which leads to kernel discriminant analysis (KDA). When the data are highly nonlinear distributed, KDA can achieve better performance than LDA. However, computing the projective functions in KDA involves eigen-decomposition of kernel matrix, which is very expensive when a large number of training samples exist. In this paper, we present a new algorithm for kernel discriminant analysis, called Spectral Regression Kernel Discriminant Analysis (SRKDA). By using spectral graph analysis, SRKDA casts discriminant analysis into a regression framework, which facilitates both efficient computation and the use of regularization techniques. Specifically, SRKDA only needs to solve a set of regularized regression problems, and there is no eigenvector computation involved, which is a huge save of computational cost. The new formulation makes it very easy to develop incremental version of the algorithm, which can fully utilize the computational results of the existing training samples. Moreover, it is easy to produce sparse projections (Sparse KDA) with a L 1-norm regularizer. Extensive experiments on spoken letter, handwritten digit image and face image data demonstrate the effectiveness and efficiency of the proposed algorithm.

#index 1561591
#* Unexpected results in automatic list extraction on the web
#@ 31177 21685 35117 961 6955
#t 2011
#c ACM SIGKDD Explorations Newsletter
#% 397415
#% 729978
#% 956500
#% 1044500
#% 1117028
#% 1127393
#% 1190153
#% 1328133
#% 1394469
#! The discovery and extraction of general lists on the Web continues to be an important problem facing theWeb mining community. There have been numerous studies that claim to automatically extract structured data (i.e. lists, record sets, tables, etc.) from the Web for various purposes. Our own recent experiences have shown that the list-finding methods used as part of these larger frameworks do not generalize well and therefore ought to be reevaluated. This paper briefly describes some of the current approaches, and tests them on various list-pages. Based on our findings, we conclude that analyzing aWeb page's DOM-structure is not sufficient for the general list finding task.

#index 1581864
#* Differentially private data cubes: optimizing noise sources and consistency
#@ 17321 247 961 23406
#t 2011
#c Proceedings of the 2011 ACM SIGMOD International Conference on Management of data
#% 67453
#% 248030
#% 757953
#% 810028
#% 864412
#% 963241
#% 977011
#% 1016173
#% 1022247
#% 1061644
#% 1083653
#% 1141473
#% 1190072
#% 1193149
#% 1198224
#% 1198225
#% 1206678
#% 1214684
#% 1217148
#% 1217156
#% 1224602
#% 1381029
#% 1414540
#% 1426454
#% 1426563
#% 1451189
#% 1451190
#% 1523886
#% 1740518
#! Data cubes play an essential role in data analysis and decision support. In a data cube, data from a fact table is aggregated on subsets of the table's dimensions, forming a collection of smaller tables called cuboids. When the fact table includes sensitive data such as salary or diagnosis, publishing even a subset of its cuboids may compromise individuals' privacy. In this paper, we address this problem using differential privacy (DP), which provides provable privacy guarantees for individuals by adding noise to query answers. We choose an initial subset of cuboids to compute directly from the fact table, injecting DP noise as usual; and then compute the remaining cuboids from the initial set. Given a fixed privacy guarantee, we show that it is NP-hard to choose the initial set of cuboids so that the maximal noise over all published cuboids is minimized, or so that the number of cuboids with noise below a given threshold (precise cuboids) is maximized. We provide an efficient procedure with running time polynomial in the number of cuboids to select the initial set of cuboids, such that the maximal noise in all published cuboids will be within a factor (ln|L| + 1)^2 of the optimal, where |L| is the number of cuboids to be published, or the number of precise cuboids will be within a factor (1 - 1/e) of the optimal. We also show how to enforce consistency in the published cuboids while simultaneously improving their utility (reducing error). In an empirical evaluation on real and synthetic data, we report the amounts of error of different publishing algorithms, and show that our approaches outperform baselines significantly.

#index 1581917
#* Graph cube: on warehousing and OLAP multidimensional networks
#@ 18038 11879 12167 961
#t 2011
#c Proceedings of the 2011 ACM SIGMOD International Conference on Management of data
#% 210182
#% 223781
#% 273697
#% 420053
#% 481749
#% 824711
#% 846209
#% 860097
#% 881526
#% 994154
#% 1016173
#% 1022205
#% 1063501
#% 1063512
#% 1063518
#% 1063535
#% 1176876
#% 1176884
#% 1328169
#% 1372657
#% 1428692
#% 1441395
#! We consider extending decision support facilities toward large sophisticated networks, upon which multidimensional attributes are associated with network entities, thereby forming the so-called multidimensional networks. Data warehouses and OLAP (Online Analytical Processing) technology have proven to be effective tools for decision support on relational data. However, they are not well-equipped to handle the new yet important multidimensional networks. In this paper, we introduce Graph Cube, a new data warehousing model that supports OLAP queries effectively on large multidimensional networks. By taking account of both attribute aggregation and structure summarization of the networks, Graph Cube goes beyond the traditional data cube model involved solely with numeric value based group-by's, thus resulting in a more insightful and structure-enriched aggregate network within every possible multidimensional space. Besides traditional cuboid queries, a new class of OLAP queries, crossboid, is introduced that is uniquely useful in multidimensional networks and has not been studied before. We implement Graph Cube by combining special characteristics of multidimensional networks with the existing well-studied data cube techniques. We perform extensive experimental studies on a series of real world data sets and Graph Cube is shown to be a powerful and efficient tool for decision support on large multidimensional networks.

#index 1581965
#* WINACS: construction and analysis of web-based computer science information networks
#@ 31177 32500 21685 35416 961 35417 35418 31176 35419 35420 19945 35421 24023 28701
#t 2011
#c Proceedings of the 2011 ACM SIGMOD International Conference on Management of data
#% 1176884
#% 1214701
#% 1328118
#% 1426635
#% 1451159
#% 1482398
#% 1560218
#% 1561591
#% 1610177
#! WINACS (Web-based Information Network Analysis for Computer Science) is a project that incorporates many recent, exciting developments in data sciences to construct a Web-based computer science information network and to discover, retrieve, rank, cluster, and analyze such an information network. With the rapid development of the Web, huge amounts of information are available in the form of Web documents, structures, and links. It has been a dream of the database and Web communities to harvest such information and reconcile the unstructured nature of the Web with the neat, semi-structured schemas of the database paradigm. Taking computer science as a dedicated domain, WINACS first discovers related Web entity structures, and then constructs a heterogeneous computer science information network in order to rank, cluster and analyze this network and support intelligent and analytical queries.

#index 1594610
#* Bidirectional mining of non-redundant recurrent rules from a sequence database
#@ 18641 17321 35590 961
#t 2011
#c ICDE '11 Proceedings of the 2011 IEEE 27th International Conference on Data Engineering
#! We are interested in scalable mining of a non-redundant set of significant recurrent rules from a sequence database. Recurrent rules have the form "whenever a series of precedent events occurs, eventually a series of consequent events occurs". They are intuitive and characterize behaviors in many domains. An example is the domain of software specification, in which the rules capture a family of properties beneficial to program verification and bug detection. We enhance a past work on mining recurrent rules by Lo, Khoo, and Liu to perform mining more scalably. We propose a new set of pruning properties embedded in a new mining algorithm. Performance and case studies on benchmark synthetic and real datasets show that our approach is much more efficient and outperforms the state-of-the-art approach in mining recurrent rules by up to two orders of magnitude.

#index 1598339
#* Learning search tasks in queries and web pages via graph regularization
#@ 28410 11083 35687 961 10859 18538 4152
#t 2011
#c Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval
#% 248810
#% 590523
#% 754059
#% 785366
#% 805878
#% 869527
#% 956503
#% 961218
#% 1043040
#% 1074093
#% 1131829
#% 1190102
#% 1214717
#% 1227610
#% 1268061
#% 1292540
#% 1400033
#% 1495579
#! As the Internet grows explosively, search engines play a more and more important role for users in effectively accessing online information. Recently, it has been recognized that a query is often triggered by a search task that the user wants to accomplish. Similarly, many web pages are specifically designed to help accomplish a certain task. Therefore, learning hidden tasks behind queries and web pages can help search engines return the most useful web pages to users by task matching. For instance, the search task that triggers query "thinkpad T410 broken" is to maintain a computer, and it is desirable for a search engine to return the Lenovo troubleshooting page on the top of the list. However, existing search engine technologies mainly focus on topic detection or relevance ranking, which are not able to predict the task that triggers a query and the task a web page can accomplish. In this paper, we propose to simultaneously classify queries and web pages into the popular search tasks by exploiting their content together with click-through logs. Specifically, we construct a taskoriented heterogeneous graph among queries and web pages. Each pair of objects in the graph are linked together as long as they potentially share similar search tasks. A novel graph-based regularization algorithm is designed for search task prediction by leveraging the graph. Extensive experiments in real search log data demonstrate the effectiveness of our method over state-of-the-art classifiers, and the search performance can be significantly improved by using the task prediction results as additional information.

#index 1598377
#* Learning online discussion structures by conditional random fields
#@ 22252 24023 3494 961
#t 2011
#c Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval
#% 46803
#% 464434
#% 577224
#% 840966
#% 879569
#% 879602
#% 956516
#% 1019165
#% 1074109
#% 1214699
#% 1227593
#% 1227598
#% 1292733
#% 1417383
#% 1470659
#% 1587368
#% 1815753
#! Online forum discussions are emerging as valuable information repository, where knowledge is accumulated by the interaction among users, leading to multiple threads with structures. Such replying structure in each thread conveys important information about the discussion content. Unfortunately, not all the online forum sites would explicitly record such replying relationship, making it hard to for both users and computers to digest the information buried in a thread discussion. In this paper, we propose a probabilistic model in the Conditional Random Fields framework to predict the replying structure for a threaded online discussion. Different from previous thread reconstruction methods, most of which fail to consider dependency between the posts, we cast the problem as a supervised structure learning problem to incorporate the features describing the structural dependency among the discussion content and learn their relationship. Experiment results on three different online forums show that the proposed method can well capture the replying structures in online discussion threads, and multiple tasks such as forum search and question answering can benefit from the reconstructed replying structures.

#index 1598379
#* Authorship classification: a discriminative syntactic tree mining approach
#@ 31175 31176 31177 961 35724
#t 2011
#c Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval
#% 27842
#% 184486
#% 428405
#% 464996
#% 577218
#% 578558
#% 729941
#% 729957
#% 844421
#% 852013
#% 879670
#% 916421
#% 939736
#% 984061
#% 1063502
#% 1083649
#% 1190357
#% 1194641
#% 1206650
#% 1214677
#% 1214686
#% 1251650
#% 1261581
#% 1392491
#% 1495585
#% 1676557
#% 1682064
#! In the past, there have been dozens of studies on automatic authorship classification, and many of these studies concluded that the writing style is one of the best indicators for original authorship. From among the hundreds of features which were developed, syntactic features were best able to reflect an author's writing style. However, due to the high computational complexity for extracting and computing syntactic features, only simple variations of basic syntactic features such as function words, POS(Part of Speech) tags, and rewrite rules were considered. In this paper, we propose a new feature set of k-embedded-edge subtree patterns that holds more syntactic information than previous feature sets. We also propose a novel approach to directly mining them from a given set of syntactic trees. We show that this approach reduces the computational burden of using complex syntactic structures as the feature set. Comprehensive experiments on real-world datasets demonstrate that our approach is reliable and more accurate than previous studies.

#index 1598455
#* Collective topic modeling for heterogeneous networks
#@ 22160 22156 961
#t 2011
#c Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval
#% 280819
#% 722904
#% 769906
#% 1083734
#% 1214701
#! In this paper, we propose a joint probabilistic topic model for simultaneously modeling the contents of multi-typed objects of a heterogeneous information network. The intuition behind our model is that different objects of the heterogeneous network share a common set of latent topics so as to adjust the multinomial distributions over topics for different objects collectively. Experimental results demonstrate the effectiveness of our approach for the tasks of topic modeling and object clustering.

#index 1606000
#* LikeMiner: a system for mining the power of 'like' in social media networks
#@ 11010 24023 11564 28701 961
#t 2011
#c Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 328355
#% 589927
#% 592155
#% 635689
#% 664855
#% 722904
#% 726267
#% 729923
#% 1040539
#% 1041734
#% 1214702
#% 1400136
#% 1451243
#% 1550601
#% 1560408
#! Social media is becoming increasingly ubiquitous and popular on the Internet. Due to the huge popularity of social media websites, such as Facebook, Twitter, YouTube and Flickr, many companies or public figures are now active in maintaining pages on those websites to interact with online users, attracting a large number of fans/followers by posting interesting objects, e.g., (product) photos/videos and text messages. 'Like' has now become a very popular social function by allowing users to express their like of certain objects. It provides an accurate way of estimating user interests and an effective way of sharing/promoting information in social media. In this demo, we propose a system called LikeMiner to mine the power of 'like' in social media networks. We introduce a heterogeneous network model for social media with 'likes', and propose 'like' mining algorithms to estimate representativeness and influence of objects. The implemented prototype system demonstrates the effectiveness of the proposed approach using the large scale Facebook data.

#index 1606070
#* Probabilistic topic models with biased propagation on heterogeneous information networks
#@ 22160 961 22156 24022 22155
#t 2011
#c Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 132779
#% 268079
#% 277483
#% 280819
#% 290830
#% 643008
#% 722904
#% 769906
#% 799636
#% 818266
#% 875959
#% 879587
#% 1055681
#% 1055743
#% 1083684
#% 1083734
#% 1130899
#% 1166526
#% 1211703
#% 1214645
#% 1214701
#% 1318691
#% 1451205
#% 1560379
#% 1598455
#! With the development of Web applications, textual documents are not only getting richer, but also ubiquitously interconnected with users and other objects in various ways, which brings about text-rich heterogeneous information networks. Topic models have been proposed and shown to be useful for document analysis, and the interactions among multi-typed objects play a key role at disclosing the rich semantics of the network. However, most of topic models only consider the textual information while ignore the network structures or can merely integrate with homogeneous networks. None of them can handle heterogeneous information network well. In this paper, we propose a novel topic model with biased propagation (TMBP) algorithm to directly incorporate heterogeneous information network with topic modeling in a unified way. The underlying intuition is that multi-typed objects should be treated differently along with their inherent textual information and the rich semantics of the heterogeneous information network. A simple and unbiased topic propagation across such a heterogeneous network does not make much sense. Consequently, we investigate and develop two biased propagation frameworks, the biased random walk framework and the biased regularization framework, for the TMBP algorithm from different perspectives, which can discover latent topics and identify clusters of multi-typed objects simultaneously. We extensively evaluate the proposed approach and compare to the state-of-the-art techniques on several datasets. Experimental results demonstrate that the improvement in our proposed approach is consistent and promising.

#index 1606073
#* Ranking-based classification of heterogeneous information networks
#@ 28410 961 32500
#t 2011
#c Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 118771
#% 235377
#% 268079
#% 281186
#% 290830
#% 805896
#% 818266
#% 876018
#% 961268
#% 961278
#% 1083698
#% 1214701
#% 1214714
#% 1292715
#% 1305507
#% 1495579
#! It has been recently recognized that heterogeneous information networks composed of multiple types of nodes and links are prevalent in the real world. Both classification and ranking of the nodes (or data objects) in such networks are essential for network analysis. However, so far these approaches have generally been performed separately. In this paper, we combine ranking and classification in order to perform more accurate analysis of a heterogeneous information network. Our intuition is that highly ranked objects within a class should play more important roles in classification. On the other hand, class membership information is important for determining a quality ranking over a dataset. We believe it is therefore beneficial to integrate classification and ranking in a simultaneous, mutually enhancing process, and to this end, propose a novel ranking-based iterative classification framework, called RankClass. Specifically, we build a graph-based ranking model to iteratively compute the ranking distribution of the objects within each class. At each iteration, according to the current ranking results, the graph structure used in the ranking algorithm is adjusted so that the sub-network corresponding to the specific class is emphasized, while the rest of the network is weakened. As our experiments show, integrating ranking with classification not only generates more accurate classes than the state-of-art classification methods on networked data, but also provides meaningful ranking of objects within each class, serving as a more informative view of the data than traditional classification.

#index 1614889
#* Heterogeneous network-based trust analysis: a survey
#@ 23141 961
#t 2011
#c ACM SIGKDD Explorations Newsletter
#% 182966
#% 268079
#% 272764
#% 290830
#% 340146
#% 577367
#% 636340
#% 825661
#% 956520
#% 1016177
#% 1020895
#% 1036075
#% 1081580
#% 1103050
#% 1106426
#% 1168662
#% 1229283
#% 1292466
#% 1300623
#% 1328103
#% 1328155
#% 1328156
#% 1334653
#% 1355029
#% 1400032
#% 1400041
#% 1472315
#% 1484339
#% 1491640
#% 1492069
#% 1512628
#% 1523915
#% 1523954
#% 1560172
#% 1560195
#% 1560235
#% 1560376
#% 1560377
#% 1560405
#% 1560422
#! Different information sources publish information with different degrees of correctness and originality. False information can often result in considerable damage. Hence, trustworthinessof information is an important issue in this datadriven world economy. Reputation of different agents in a network has been studied earlier in a variety of domains like e-commerce, social sciences, sensor networks, and P2P networks. Recently there has been work in the data mining community on performing trust analysis based on the data provided by multiple information providers for different objects, and such agents and their provided information about data objects form a multi-typed heterogeneous network. The trust analysis under such a framework is considered as heterogeneous network-based trust analysis. This paper will survey heterogeneous network-based trust analysis models and their applications. We would conclude with a summary and some thoughts on future research in the area.

#index 1617291
#* Linear discriminant dimensionality reduction
#@ 23981 23406 961
#t 2011
#c ECML PKDD'11 Proceedings of the 2011 European conference on Machine learning and knowledge discovery in databases - Volume Part I
#% 80995
#% 224113
#% 235342
#% 722929
#% 729437
#% 757953
#% 803567
#% 829010
#% 836827
#% 876025
#% 876058
#% 913838
#% 983907
#% 983940
#% 1117001
#% 1128929
#% 1211747
#% 1270195
#% 1379069
#% 1417091
#% 1535440
#! Fisher criterion has achieved great success in dimensionality reduction. Two representative methods based on Fisher criterion are Fisher Score and Linear Discriminant Analysis (LDA). The former is developed for feature selection while the latter is designed for subspace learning. In the past decade, these two approaches are often studied independently. In this paper, based on the observation that Fisher score and LDA are complementary, we propose to integrate Fisher score and LDA in a unified framework, namely Linear Discriminant Dimensionality Reduction (LDDR). We aim at finding a subset of features, based on which the learnt linear transformation via LDA maximizes the Fisher criterion. LDDR inherits the advantages of Fisher score and LDA and is able to do feature selection and subspace learning simultaneously. Both Fisher score and LDA can be seen as the special cases of the proposed method. The resultant optimization problem is a mixed integer programming, which is difficult to solve. It is relaxed into a L2,1-norm constrained least square problem and solved by accelerated proximal gradient descent algorithm. Experiments on benchmark face recognition data sets illustrate that the proposed method outperforms the state of the art methods arguably.

#index 1617309
#* Efficient mining of top correlated patterns based on null-invariant measures
#@ 31175 31932 961
#t 2011
#c ECML PKDD'11 Proceedings of the 2011 European conference on Machine learning and knowledge discovery in databases - Volume Part II
#% 152934
#% 227919
#% 452846
#% 481758
#% 577214
#% 727869
#% 818916
#% 824710
#% 835018
#% 1318601
#% 1318691
#% 1465175
#% 1503467
#! Mining strong correlations from transactional databases often leads to more meaningful results than mining association rules. In such mining, null (transaction)-invariance is an important property of the correlation measures. Unfortunately, some useful null-invariant measures such as Kulczynski and Cosine, which can discover correlations even for the very unbalanced cases, lack the (anti)-monotonicity property. Thus, they could only be applied to frequent itemsets as the post-evaluation step. For large datasets and for low supports, this approach is computationally prohibitive. This paper presents new properties for all known null-invariant measures. Based on these properties, we develop efficient pruning techniques and design the Apriori-like algorithm NICOMINER for mining strongly correlated patterns directly. We develop both the threshold-bounded and the top-k variations of the algorithm, where top-k is used when the optimal correlation threshold is not known in advance and to give user control over the output size. We test NICOMINER on real-life datasets from different application domains, using Cosine as an example of the null-invariant correlation measure. We show that NICOMINER outperforms support-based approach more than an order of magnitude, and that it is very useful for discovering top correlations in itemsets with low support.

#index 1668641
#* Mining flipping correlations from large datasets with taxonomies
#@ 31932 31175 31177 961
#t 2011
#c Proceedings of the VLDB Endowment
#% 152934
#% 227919
#% 280433
#% 300120
#% 300477
#% 392618
#% 464822
#% 478298
#% 481588
#% 481758
#% 577214
#% 767654
#% 799740
#% 835018
#% 1465175
#! In this paper we introduce a new type of pattern -- a flipping correlation pattern. The flipping patterns are obtained from contrasting the correlations between items at different levels of abstraction. They represent surprising correlations, both positive and negative, which are specific for a given abstraction level, and which "flip" from positive to negative and vice versa when items are generalized to a higher level of abstraction. We design an efficient algorithm for finding flipping correlations, the Flipper algorithm, which outperforms naïve pattern mining methods by several orders of magnitude. We apply Flipper to real-life datasets and show that the discovered patterns are non-redundant, surprising and actionable. Flipper finds strong contrasting correlations in itemsets with low-to-medium support, while existing techniques cannot handle the pattern discovery in this frequency range.

#index 1669937
#* Ranking outliers using symmetric neighborhood relationship
#@ 4232 4233 961 1204
#t 2006
#c PAKDD'06 Proceedings of the 10th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining
#% 201876
#% 210173
#% 248790
#% 300136
#% 300163
#% 300183
#% 333929
#% 342625
#% 342638
#% 479791
#% 479986
#% 481281
#% 501988
#% 566132
#% 578689
#% 765134
#% 765438
#% 775363
#% 799747
#% 813973
#% 818916
#! Mining outliers in database is to find exceptional objects that deviate from the rest of the data set. Besides classical outlier analysis algorithms, recent studies have focused on mining local outliers, i.e., the outliers that have density distribution significantly different from their neighborhood. The estimation of density distribution at the location of an object has so far been based on the density distribution of its k-nearest neighbors [2,11]. However, when outliers are in the location where the density distributions in the neighborhood are significantly different, for example, in the case of objects from a sparse cluster close to a denser cluster, this may result in wrong estimation. To avoid this problem, here we propose a simple but effective measure on local outliers based on a symmetric neighborhood relationship. The proposed measure considers both neighbors and reverse neighbors of an object when estimating its density distribution. As a result, outliers so discovered are more meaningful. To compute such local outliers efficiently, several mining algorithms are developed that detects top-n outliers based on our definition. A comprehensive performance evaluation and analysis shows that our methods are not only efficient in the computation but also more effective in ranking outliers.

#index 1673587
#* Efficient classification from multiple heterogeneous databases
#@ 10548 961
#t 2005
#c PKDD'05 Proceedings of the 9th European conference on Principles and Practice of Knowledge Discovery in Databases
#% 91872
#% 397369
#% 443085
#% 458178
#% 458257
#% 512307
#% 572314
#% 577289
#% 729930
#% 745491
#% 765433
#% 772829
#% 1290272
#! With the fast expansion of computer networks, it is inevitable to study data mining on heterogeneous databases. In this paper we propose MDBM, an accurate and efficient approach for classification on multiple heterogeneous databases. We propose a regression-based method for predicting the usefulness of inter-database links that serve as bridges for information transfer, because such links are automatically detected and may or may not be useful or even valid. Because of the high cost of inter-database communication, MDBM employs a new strategy for cross-database classification, which finds and performs actions with high benefit-to-cost ratios. The experiments show that MDBM achieves high accuracy in cross-database classification, with much higher efficiency than previous approaches.

#index 1673591
#* Community mining from multi-relational networks
#@ 8278 15094 10859 9413 961
#t 2005
#c PKDD'05 Proceedings of the 9th European conference on Principles and Practice of Knowledge Discovery in Databases
#% 146494
#% 342596
#% 1499466
#! Social network analysis has attracted much attention in recent years. Community mining is one of the major directions in social network analysis. Most of the existing methods on community mining assume that there is only one kind of relation in the network, and moreover, the mining results are independent of the users’ needs or preferences. However, in reality, there exist multiple, heterogeneous social networks, each representing a particular kind of relationship, and each kind of relationship may play a distinct role in a particular task. In this paper, we systematically analyze the problem of mining hidden communities on heterogeneous social networks. Based on the observation that different relations have different importance with respect to a certain query, we propose a new method for learning an optimal linear combination of these relations which can best meet the user’s expectation. With the obtained relation, better performance can be achieved for community mining.

#index 1673601
#* Efficient processing of ranked queries with sweeping selection
#@ 4232 2497 961
#t 2005
#c PKDD'05 Proceedings of the 9th European conference on Principles and Practice of Knowledge Discovery in Databases
#% 86950
#% 201876
#% 210173
#% 248010
#% 248796
#% 300180
#% 333854
#% 333951
#% 410276
#% 427199
#% 464195
#% 465167
#% 479816
#% 591565
#% 654480
#% 733373
#% 799759
#% 1016182
#! Existing methods for top-k ranked query employ techniques including sorting, updating thresholds and materializing views. In this paper, we propose two novel index-based techniques for top-k ranked query: (1) indexing the layered skyline, and (2) indexing microclusters of objects into a grid structure. We also develop efficient algorithms for ranked query by locating the answer points during the sweeping of the line/hyperplane of the score function over the indexed objects. Both methods can be easily plugged into typical multi-dimensional database indexes. The comprehensive experiments not only demonstrate that our methods outperform the existing ones, but also illustrate that the application of data mining technique (microclustering) is a useful and effective solution for database query processing.

#index 1688451
#* LPTA: A Probabilistic Model for Latent Periodic Topic Analysis
#@ 19944 32008 961 3494 27092
#t 2011
#c ICDM '11 Proceedings of the 2011 IEEE 11th International Conference on Data Mining
#! This paper studies the problem of latent periodic topic analysis from time stamped documents. The examples of time stamped documents include news articles, sales records, financial reports, TV programs, and more recently, posts from social media websites such as Flickr, Twitter, and Face book. Different from detecting periodic patterns in traditional time series database, we discover the topics of coherent semantics and periodic characteristics where a topic is represented by a distribution of words. We propose a model called LPTA (Latent Periodic Topic Analysis) that exploits the periodicity of the terms as well as term co-occurrences. To show the effectiveness of our model, we collect several representative datasets including Seminar, DBLP and Flickr. The results show that our model can discover the latent periodic topics effectively and leverage the information from both text and time well.

#index 1688485
#* Patent Maintenance Recommendation with Patent Information Network Model
#@ 11010 4272 12163 18499 28656 10893 28759 961
#t 2011
#c ICDM '11 Proceedings of the 2011 IEEE 11th International Conference on Data Mining
#! Patents are of crucial importance for businesses, because they provide legal protection for the invented techniques, processes or products. A patent can be held for up to 20 years. However, large maintenance fees need to be paid to keep it enforceable. If the patent is deemed not valuable, the owner may decide to abandon it by stopping paying the maintenance fees to reduce the cost. For large companies or organizations, making such decisions is difficult because too many patents need to be investigated. In this paper, we introduce the new patent mining problem of automatic patent maintenance prediction, and propose a systematic solution to analyze patents for recommending patent maintenance decision. We model the patents as a heterogeneous time-evolving information network and propose new patent features to build model for a ranked prediction on whether to maintain or abandon a patent. In addition, a network-based refinement approach is proposed to further improve the performance. We have conducted experiments on the large scale United States Patent and Trademark Office (USPTO) database which contains over four million granted patents. The results show that our technique can achieve high performance.

#index 1688489
#* A Spectral Framework for Detecting Inconsistency across Multi-source Object Relationships
#@ 17596 2485 24039 4208 961
#t 2011
#c ICDM '11 Proceedings of the 2011 IEEE 11th International Conference on Data Mining
#! In this paper, we propose to conduct anomaly detection across multiple sources to identify objects that have inconsistent behavior across these sources. We assume that a set of objects can be described from various perspectives (multiple information sources). The underlying clustering structure of normal objects is usually shared by multiple sources. However, anomalous objects belong to different clusters when considering different aspects. For example, there exist movies that are expected to be liked by kids by genre, but are liked by grown-ups based on user viewing history. To identify such objects, we propose to compute the distance between different eigen decomposition results of the same object with respect to different sources as its anomalous score. We also give interpretations from the perspectives of constrained spectral clustering and random walks over graph. Experimental results on several UCI as well as DBLP and Movie Lens datasets demonstrate the effectiveness of the proposed approach.

#index 1688495
#* Signature Pattern Covering via Local Greedy Algorithm and Pattern Shrink
#@ 31176 36108 32503 961 38006 38007
#t 2011
#c ICDM '11 Proceedings of the 2011 IEEE 11th International Conference on Data Mining
#! Pattern mining is a fundamental problem that has a wide range of applications. In this paper, we study the problem of finding a minimum set of signature patterns that explain all data. In the problem, we are given objects where each object has an item set and a label. A pattern is called a signature pattern if all objects with the pattern have the same label. This problem has many interesting applications such as assertion mining in hardware design and identifying failure causes from various log data. We show that the previous pattern mining methods are not suitable for mining signature patterns and identify the problems. Then we propose a novel pattern enumeration method which we call Pattern Shrink. Our method is strongly coupled with another novel method that is very similar to finding a local optimum with a negligible loss in performance. Our proposed methods show a speedup of more than ten times over the previous methods. Our methods are flexible enough to be extended to mining high confidence patterns, instead of signature patterns.

#index 1688506
#* The Joint Inference of Topic Diffusion and Evolution in Social Communities
#@ 22155 13840 961 24222 32500
#t 2011
#c ICDM '11 Proceedings of the 2011 IEEE 11th International Conference on Data Mining
#! The prevalence of Web 2.0 techniques has led to the boom of various online communities, where topics spread ubiquitously among user-generated documents. Working together with this diffusion process is the evolution of topic content, where novel contents are introduced by documents which adopt the topic. Unlike explicit user behavior (e.g., buying a DVD), both the diffusion paths and the evolutionary process of a topic are implicit, making their discovery challenging. In this paper, we track the evolution of an arbitrary topic and reveal the latent diffusion paths of that topic in a social community. A novel and principled probabilistic model is proposed which casts our task as an joint inference problem, which considers textual documents, social influences, and topic evolution in a unified way. Specifically, a mixture model is introduced to model the generation of text according to the diffusion and the evolution of the topic, while the whole diffusion process is regularized with user-level social influences through a Gaussian Markov Random Field. Experiments on both synthetic data and real world data show that the discovery of topic diffusion and evolution benefits from this joint inference, and the probabilistic model we propose performs significantly better than existing methods.

#index 1688531
#* Detecting Recurring and Novel Classes in Concept-Drifting Data Streams
#@ 22167 38056 10403 23259 17596 961 3712
#t 2011
#c ICDM '11 Proceedings of the 2011 IEEE 11th International Conference on Data Mining
#! Concept-evolution is one of the major challenges in data stream classification, which occurs when a new class evolves in the stream. This problem remains unaddressed by most state-of-the-art techniques. A recurring class is a special case of concept-evolution. This special case takes place when a class appears in the stream, then disappears for a long time, and again appears. Existing data stream classification techniques that address the concept-evolution problem, wrongly detect the recurring classes as novel class. This creates two main problems. First, much resource is wasted in detecting a recurring class as novel class, because novel class detection is much more computationally- and memory-intensive, as compared to simply recognizing an existing class. Second, when a novel class is identified, human experts are involved in collecting and labeling the instances of that class for future modeling. If a recurrent class is reported as novel class, it will be only a waste of human effort to find out whether it is really a novel class. In this paper, we address the recurring issue, and propose a more realistic novel class detection technique, which remembers a class and identifies it as "not novel" when it reappears after a long disappearance. Our approach has shown significant reduction in classification error over state-of-the-art stream classification techniques on several benchmark data streams.

#index 1697428
#* Aggregation of multiple judgments for evaluating ordered lists
#@ 35724 3494 961
#t 2010
#c ECIR'2010 Proceedings of the 32nd European conference on Advances in Information Retrieval
#% 329537
#% 459006
#% 660658
#% 786539
#% 815142
#% 817487
#% 879582
#% 939549
#% 939756
#% 950043
#% 958441
#% 1213427
#% 1262954
#% 1262955
#% 1272344
#% 1274973
#% 1712170
#! Many tasks (e.g., search and summarization) result in an ordered list of items. In order to evaluate such an ordered list of items, we need to compare it with an ideal ordered list created by a human expert for the same set of items. To reduce any bias, multiple human experts are often used to create multiple ideal ordered lists. An interesting challenge in such an evaluation method is thus how to aggregate these different ideal lists to compute a single score for an ordered list to be evaluated. In this paper, we propose three new methods for aggregating multiple order judgments to evaluate ordered lists: weighted correlation aggregation, rank-based aggregation, and frequent sequential pattern-based aggregation. Experiment results on ordering sentences for text summarization show that all the three new methods outperform the state of the art average correlation methods in terms of discriminativeness and robustness against noise. Among the three proposed methods, the frequent sequential pattern-based method performs the best due to the flexible modeling of agreements and disagreements among human experts at various levels of granularity.

#index 1707456
#* Relation strength-aware clustering of heterogeneous information networks with incomplete attributes
#@ 19945 968 961
#t 2012
#c Proceedings of the VLDB Endowment
#% 36672
#% 722902
#% 769881
#% 881460
#% 881514
#% 989586
#% 989618
#% 989636
#% 995140
#% 1055681
#% 1055741
#% 1117695
#% 1186295
#% 1214655
#% 1214701
#% 1214714
#% 1289267
#% 1318691
#% 1328161
#% 1328169
#% 1650298
#! With the rapid development of online social media, online shopping sites and cyber-physical systems, heterogeneous information networks have become increasingly popular and content-rich over time. In many cases, such networks contain multiple types of objects and links, as well as different kinds of attributes. The clustering of these objects can provide useful insights in many applications. However, the clustering of such networks can be challenging since (a) the attribute values of objects are often incomplete, which implies that an object may carry only partial attributes or even no attributes to correctly label itself; and (b) the links of different types may carry different kinds of semantic meanings, and it is a difficult task to determine the nature of their relative importance in helping the clustering for a given purpose. In this paper, we address these challenges by proposing a model-based clustering algorithm. We design a probabilistic model which clusters the objects of different types into a common hidden space, by using a user-specified set of attributes, as well as the links from different relations. The strengths of different types of links are automatically learned, and are determined by the given purpose of clustering. An iterative algorithm is designed for solving the clustering problem, in which the strengths of different types of links and the quality of clustering results mutually enhance each other. Our experimental results on real and synthetic data sets demonstrate the effectiveness and efficiency of the algorithm.

#index 1710575
#* Hierarchical web-page clustering via in-page and cross-page link structures
#@ 22155 24022 961 2438
#t 2010
#c PAKDD'10 Proceedings of the 14th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining - Volume Part II
#% 262045
#% 279755
#% 282905
#% 413608
#% 481281
#% 607793
#% 827127
#% 907509
#% 989654
#% 1090774
#% 1100188
#% 1275120
#% 1410607
#! Despite of the wide diversity of web-pages, web-pages residing in a particular organization, in most cases, are organized with semantically hierarchic structures For example, the website of a computer science department contains pages about its people, courses and research, among which pages of people are categorized into faculty, staff and students, and pages of research diversify into different areas Uncovering such hierarchic structures could supply users a convenient way of comprehensive navigation and accelerate other web mining tasks In this study, we extract a similarity matrix among pages via in-page and crosspage link structures, based on which a density-based clustering algorithm is developed, which hierarchically groups densely linked webpages into semantic clusters Our experiments show that this method is efficient and effective, and sheds light on mining and exploring web structures.

#index 1710584
#* Classification and novel class detection in data streams with active mining
#@ 22167 17596 10403 961 3712
#t 2010
#c PAKDD'10 Proceedings of the 14th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining - Volume Part II
#% 342600
#% 729932
#% 823408
#% 840891
#% 1052684
#% 1116999
#% 1176894
#% 1206700
#% 1267760
#! We present ActMiner, which addresses four major challenges to data stream classification, namely, infinite length, concept-drift, concept-evolution, and limited labeled data Most of the existing data stream classification techniques address only the infinite length and concept-drift problems Our previous work, MineClass, addresses the concept-evolution problem in addition to addressing the infinite length and concept-drift problems Concept-evolution occurs in the stream when novel classes arrive However, most of the existing data stream classification techniques, including MineClass, require that all the instances in a data stream be labeled by human experts and become available for training This assumption is impractical, since data labeling is both time consuming and costly Therefore, it is impossible to label a majority of the data points in a high-speed data stream This scarcity of labeled data naturally leads to poorly trained classifiers ActMiner actively selects only those data points for labeling for which the expected classification error is high Therefore, ActMiner extends MineClass, and addresses the limited labeled data problem in addition to addressing the other three problems It outperforms the state-of-the-art data stream classification techniques that use ten times or more labeled data than ActMiner.

#index 1730734
#* A Bayesian approach to discovering truth from conflicting sources for data integration
#@ 22156 10633 10845 961
#t 2012
#c Proceedings of the VLDB Endowment
#% 273687
#% 290830
#% 482108
#% 989682
#% 1313373
#% 1328155
#% 1328156
#% 1355029
#% 1484339
#% 1491640
#% 1536551
#% 1560376
#% 1560377
#% 1826432
#! In practical data integration systems, it is common for the data sources being integrated to provide conflicting information about the same entity. Consequently, a major challenge for data integration is to derive the most complete and accurate integrated records from diverse and sometimes conflicting sources. We term this challenge the truth finding problem. We observe that some sources are generally more reliable than others, and therefore a good model of source quality is the key to solving the truth finding problem. In this work, we propose a probabilistic graphical model that can automatically infer true records and source quality without any supervision. In contrast to previous methods, our principled approach leverages a generative process of two types of errors (false positive and false negative) by modeling two different aspects of source quality. In so doing, ours is also the first approach designed to merge multi-valued attribute types. Our method is scalable, due to an efficient sampling-based inference algorithm that needs very few iterations in practice and enjoys linear time complexity, with an even faster incremental variant. Experiments on two real world datasets show that our new method outperforms existing state-of-the-art approaches to the truth finding problem.

#index 1735398
#* Data mining for diagnostic debugging in sensor networks: preliminary evidence and lessons learned
#@ 32503 39402 39403 39404 961
#t 2008
#c Sensor-KDD'08 Proceedings of the Second international conference on Knowledge Discovery from Sensor Data
#% 252533
#% 300120
#% 329537
#% 341700
#% 438133
#% 449869
#% 459006
#% 481290
#% 644182
#% 778732
#% 787823
#% 818916
#% 835018
#% 926881
#% 985041
#% 1060231
#% 1060244
#% 1102900
#% 1132720
#% 1396875
#! Sensor networks and pervasive computing systems intimately combine computation, communication and interactions with the physical world, thus increasing the complexity of the development effort, violating communication protocol layering, and making traditional network diagnostics and debugging less effective at catching problems. Tighter coupling between communication, computation, and interaction with the physical world is likely to be an increasing trend in emerging edge networks and pervasive systems. This paper reviews recent tools developed by the authors to understand the root causes of complex interaction bugs in edge network systems that combine computation, communication and sensing. We concern ourselves with automated failure diagnosis in the face of non-reproducible behavior, high interactive complexity, and resource constraints. Several examples are given to finding bugs in real sensor network code using the tools developed, demonstrating the efficacy of the approach.

#index 1764757
#* Survey on web spam detection: principles and algorithms
#@ 40480 961
#t 2012
#c ACM SIGKDD Explorations Newsletter
#% 255137
#% 258598
#% 262061
#% 290830
#% 296646
#% 309145
#% 321635
#% 330707
#% 340147
#% 348173
#% 348174
#% 451536
#% 510723
#% 577273
#% 577329
#% 590524
#% 679843
#% 754088
#% 754098
#% 769885
#% 769925
#% 772018
#% 783474
#% 799632
#% 805668
#% 807297
#% 818221
#% 818223
#% 824694
#% 869470
#% 869471
#% 869549
#% 874266
#% 879603
#% 893125
#% 912202
#% 956523
#% 956613
#% 957992
#% 957993
#% 957994
#% 957995
#% 958003
#% 958004
#% 987245
#% 989505
#% 1006352
#% 1016177
#% 1020410
#% 1055784
#% 1055824
#% 1074107
#% 1077150
#% 1125903
#% 1130814
#% 1130841
#% 1130921
#% 1174537
#% 1186295
#% 1190227
#% 1211088
#% 1250370
#% 1263886
#% 1289272
#% 1431624
#% 1472970
#% 1536557
#% 1560965
#% 1565812
#% 1603006
#% 1699583
#! Search engines became a de facto place to start information acquisition on the Web. Though due to web spam phenomenon, search results are not always as good as desired. Moreover, spam evolves that makes the problem of providing high quality search even more challenging. Over the last decade research on adversarial information retrieval has gained a lot of interest both from academia and industry. In this paper we present a systematic review of web spam detection techniques with the focus on algorithms and underlying principles. We categorize all existing algorithms into three categories based on the type of information they use: content-based methods, link-based methods, and methods based on non-traditional data such as user behaviour, clicks, HTTP sessions. In turn, we perform a subcategorization of link-based category into five groups based on ideas and principles used: labels propagation, link pruning and reweighting, labels refinement, graph regularization, and featurebased. We also define the concept of web spam numerically and provide a brief survey on various spam forms. Finally, we summarize the observations and underlying principles applied for web spam detection.

#index 1770360
#* Optimizing index for taxonomy keyword search
#@ 17321 4558 10198 961 40557
#t 2012
#c SIGMOD '12 Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data
#% 198055
#% 273697
#% 576114
#% 756964
#% 869501
#% 869535
#% 931292
#% 939601
#% 956564
#% 1074101
#% 1083721
#% 1130854
#% 1166527
#% 1181293
#% 1269899
#% 1392432
#% 1538767
#% 1567485
#% 1641950
#% 1702425
#% 1770359
#% 1826433
#% 1845813
#! Query substitution is an important problem in information retrieval. Much work focuses on how to find substitutes for any given query. In this paper, we study how to efficiently process a keyword query whose substitutes are defined by a given taxonomy. This problem is challenging because each term in a query can have a large number of substitutes, and the original query can be rewritten into any of their combinations. We propose to build an additional index (besides inverted index) to efficiently process queries. For a query workload, we formulate an optimization problem which chooses the additional index structure, aiming at minimizing the query evaluation cost, under given index space constraints. We show the NP-hardness of the problem, and propose a pseudo-polynomial time algorithm using dynamic programming, as well as an 1 over 4(1-1/e)-approximation algorithm to solve the problem. Experimental results show that, with only 10% additional index space, our approach can greatly reduce the query evaluation cost.

#index 1826268
#* On trivial solution and scale transfer problems in graph regularized NMF
#@ 23981 4434 961
#t 2011
#c IJCAI'11 Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Volume Two
#% 313959
#% 643008
#% 729918
#% 757953
#% 837604
#% 881468
#% 891559
#% 915294
#% 961218
#% 1176865
#% 1214657
#% 1327693
#! Combining graph regularization with nonnegative matrix (tri-)factorization (NMF) has shown great performance improvement compared with traditional nonnegative matrix (tri-)factorization models due to its ability to utilize the geometric structure of the documents and words. In this paper, we show that these models are not well-defined and suffering from trivial solution and scale transfer problems. In order to solve these common problems, we propose two models for graph regularized non-negative matrix (tri-)factorization, which can be applied for document clustering and co-clustering respectively. In the proposed models, a Normalized Cut-like constraint is imposed on the cluster assignment matrix to make the optimization problem well-defined. We derive a multiplicative updating algorithm for the proposed models, and prove its convergence. Experiments of clustering and coclustering on benchmark text data sets demonstrate that the proposed models outperform the original models as well as many other state-of-the-art clustering methods.

#index 1826269
#* Joint feature selection and subspace learning
#@ 23981 23406 961
#t 2011
#c IJCAI'11 Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Volume Two
#% 224113
#% 235342
#% 722929
#% 732522
#% 757953
#% 836827
#% 876025
#% 913838
#% 1117001
#% 1128929
#% 1379069
#! Dimensionality reduction is a very important topic in machine learning. It can be generally classified into two categories: feature selection and subspace learning. In the past decades, many methods have been proposed for dimensionality reduction. However, most of these works study feature selection and subspace learning independently. In this paper, we present a framework for joint feature selection and subspace learning. We reformulate the subspace learning problem and use L2,1-norm on the projection matrix to achieve row-sparsity, which leads to selecting relevant features and learning transformation simultaneously. We discuss two situations of the proposed framework, and present their optimization algorithms. Experiments on benchmark face recognition data sets illustrate that the proposed framework outperforms the state of the art methods overwhelmingly.

#index 1846700
#* On Discovery of Traveling Companions from Streaming Trajectories
#@ 21437 30965 36067 961 41444 34774 7839
#t 2012
#c ICDE '12 Proceedings of the 2012 IEEE 28th International Conference on Data Engineering
#! The advance of object tracking technologies leads to huge volumes of spatio-temporal data collected in the form of trajectory data stream. In this study, we investigate the problem of discovering object groups that travel together (i.e., traveling companions) from trajectory stream. Such technique has broad applications in the areas of scientific study, transportation management and military surveillance. To discover traveling companions, the monitoring system should cluster the objects of each snapshot and intersect the clustering results to retrieve moving-together objects. Since both clustering and intersection steps involve high computational overhead, the key issue of companion discovery is to improve the algorithm's efficiency. We propose the models of closed companion candidates and smart intersection to accelerate data processing. A new data structure termed traveling buddy is designed to facilitate scalable and flexible companion discovery on trajectory stream. The traveling buddies are micro-groups of objects that are tightly bound together. By only storing the object relationships rather than their spatial coordinates, the buddies can be dynamically maintained along trajectory stream with low cost. Based on traveling buddies, the system can discover companions without accessing the object details. The proposed methods are evaluated with extensive experiments on both real and synthetic datasets. The buddy-based method is an order of magnitude faster than existing methods. It also outperforms other competitors with higher precision and recall in companion discovery.

#index 1846724
#* Mining Knowledge from Data: An Information Network Analysis Approach
#@ 961 19945 9413 850
#t 2012
#c ICDE '12 Proceedings of the 2012 IEEE 28th International Conference on Data Engineering
#! Most objects and data in the real world are interconnected, forming complex, heterogeneous but often semistructured information networks. However, many database researchers consider a database merely as a data repository that supports storage and retrieval rather than an information-rich, inter-related and multi-typed information network that supports comprehensive data analysis, whereas many network researchers focus on homogeneous networks. Departing from both, we view interconnected, semi-structured datasets as heterogeneous, information-rich networks and study how to uncover hidden knowledge in such networks. For example, a university database can be viewed as a heterogeneous information network, where objects of multiple types, such as students, professors, courses, departments, and multiple typed relationships, such as teach and advise are intertwined together, providing abundant information. In this tutorial, we present an organized picture on mining heterogeneous information networks and introduce a set of interesting, effective and scalable network mining methods. The topics to be covered include (i) database as an information network, (ii) mining information networks: clustering, classification, ranking, similarity search, and meta path-guided analysis, (iii) construction of quality, informative networks by data mining, (iv) trend and evolution analysis in heterogeneous information networks, and (v) research frontiers. We show that heterogeneous information networks are informative, and link analysis on such networks is powerful at uncovering critical knowledge hidden in large semi-structured datasets. Finally, we also present a few promising research directions.

#index 1846833
#* Multidimensional Analysis of Atypical Events in Cyber-Physical Data
#@ 21437 28701 31175 961 7839 19945 15163 41575
#t 2012
#c ICDE '12 Proceedings of the 2012 IEEE 28th International Conference on Data Engineering
#! A Cyber-Physical System (CPS) integrates physical devices (e.g., sensors, cameras) with cyber (or informational) components to form a situation-integrated analytical system that may respond intelligently to dynamic changes of the real-world situations. CPS claims many promising applications, such as traffic observation, battlefield surveillance and sensor-network based monitoring. One important research topic in CPS is about the atypical event analysis, i.e., retrieving the events from large amount of data and analyzing them with spatial, temporal and other multi-dimensional information. Many traditional approaches are not feasible for such analysis since they use numeric measures and cannot describe the complex atypical events. In this study, we propose a new model of atypical cluster to effectively represent those events and efficiently retrieve them from massive data. The micro-cluster is designed to summarize individual events, and the macro-cluster is used to integrate the information from multiple event. To facilitate scalable, flexible and online analysis, the concept of significant cluster is defined and a guided clustering algorithm is proposed to retrieve significant clusters in an efficient manner. We conduct experiments on real datasets with the size of more than 50 GB, the results show that the proposed method can provide more accurate information with only 15% to 20% time cost of the baselines.

#index 1872283
#* Mining event periodicity from incomplete observations
#@ 23406 41719 961
#t 2012
#c Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 310542
#% 464986
#% 480156
#% 813978
#% 844299
#% 905945
#% 1089789
#% 1451250
#! Advanced technology in GPS and sensors enables us to track physical events, such as human movements and facility usage. Periodicity analysis from the recorded data is an important data mining task which provides useful insights into the physical events and enables us to report outliers and predict future behaviors. To mine periodicity in an event, we have to face real-world challenges of inherently complicated periodic behaviors and imperfect data collection problem. Specifically, the hidden temporal periodic behaviors could be oscillating and noisy, and the observations of the event could be incomplete. In this paper, we propose a novel probabilistic measure for periodicity and design a practical method to detect periods. Our method has thoroughly considered the uncertainties and noises in periodic behaviors and is provably robust to incomplete observations. Comprehensive experiments on both synthetic and real datasets demonstrate the effectiveness of our method.

#index 1872316
#* Parallel field ranking
#@ 28410 41754 10859 8278 961
#t 2012
#c Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 8153
#% 290830
#% 341269
#% 442110
#% 732522
#% 780688
#% 875948
#% 905203
#% 1077150
#% 1164191
#% 1190090
#% 1227644
#% 1275220
#% 1558464
#% 1598386
#% 1605923
#% 1606026
#% 1775495
#! Recently, ranking data with respect to the intrinsic geometric structure (manifold ranking) has received considerable attentions, with encouraging performance in many applications in pattern recognition, information retrieval and recommendation systems. Most of the existing manifold ranking methods focus on learning a ranking function that varies smoothly along the data manifold. However, beyond smoothness, a desirable ranking function should vary monotonically along the geodesics of the data manifold, such that the ranking order along the geodesics is preserved. In this paper, we aim to learn a ranking function that varies linearly and therefore monotonically along the geodesics of the data manifold. Recent theoretical work shows that the gradient field of a linear function on the manifold has to be a parallel vector field. Therefore, we propose a novel ranking algorithm on the data manifolds, called Parallel Field Ranking. Specifically, we try to learn a ranking function and a vector field simultaneously. We require the vector field to be close to the gradient field of the ranking function, and the vector field to be as parallel as possible. Moreover, we require the value of the ranking function at the query point to be the highest, and then decrease linearly along the manifold. Experimental results on both synthetic data and real data demonstrate the effectiveness of our proposed algorithm.

#index 1872334
#* Integrating community matching and outlier detection for mining evolutionary community outliers
#@ 23141 17596 19945 961
#t 2012
#c Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 300136
#% 300183
#% 333929
#% 443853
#% 466745
#% 479791
#% 494396
#% 570886
#% 577263
#% 781774
#% 785389
#% 844317
#% 1041180
#% 1202160
#% 1206639
#% 1232040
#% 1292669
#% 1318691
#% 1451221
#% 1482422
#% 1562549
#% 1594652
#! Temporal datasets, in which data evolves continuously, exist in a wide variety of applications, and identifying anomalous or outlying objects from temporal datasets is an important and challenging task. Different from traditional outlier detection, which detects objects that have quite different behavior compared with the other objects, temporal outlier detection tries to identify objects that have different evolutionary behavior compared with other objects. Usually objects form multiple communities, and most of the objects belonging to the same community follow similar patterns of evolution. However, there are some objects which evolve in a very different way relative to other community members, and we define such objects as evolutionary community outliers. This definition represents a novel type of outliers considering both temporal dimension and community patterns. We investigate the problem of identifying evolutionary community outliers given the discovered communities from two snapshots of an evolving dataset. To tackle the challenges of community evolution and outlier detection, we propose an integrated optimization framework which conducts outlier-aware community matching across snapshots and identification of evolutionary outliers in a tightly coupled way. A coordinate descent algorithm is proposed to improve community matching and outlier detection performance iteratively. Experimental results on both synthetic and real datasets show that the proposed approach is highly effective in discovering interesting evolutionary community outliers.

#index 1872355
#* Event-based social networks: linking the online and offline social worlds
#@ 31976 12631 14198 3937 30173 961
#t 2012
#c Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 36672
#% 313959
#% 330687
#% 769935
#% 823328
#% 956578
#% 1002007
#% 1198232
#% 1300556
#% 1425621
#% 1451163
#% 1606045
#% 1606049
#% 1633202
#! Newly emerged event-based online social services, such as Meetup and Plancast, have experienced increased popularity and rapid growth. From these services, we observed a new type of social network - event-based social network (EBSN). An EBSN does not only contain online social interactions as in other conventional online social networks, but also includes valuable offline social interactions captured in offline activities. By analyzing real data collected from Meetup, we investigated EBSN properties and discovered many unique and interesting characteristics, such as heavy-tailed degree distributions and strong locality of social interactions. We subsequently studied the heterogeneous nature (co-existence of both online and offline social interactions) of EBSNs on two challenging problems: community detection and information flow. We found that communities detected in EBSNs are more cohesive than those in other types of social networks (e.g. location-based social networks). In the context of information flow, we studied the event recommendation problem. By experimenting various information diffusion patterns, we found that a community-based diffusion model that takes into account of both online and offline interactions provides the best prediction power. This paper is the first research to study EBSNs at scale and paves the way for future studies on this new type of social network. A sample dataset of this study can be downloaded from http://www.largenetwork.org/ebsn.

#index 1872391
#* Integrating meta-path selection with user-guided object clustering in heterogeneous information networks
#@ 19945 41828 961 9413 850 28701
#t 2012
#c Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 280819
#% 313959
#% 464631
#% 466574
#% 722902
#% 722929
#% 722934
#% 769881
#% 770782
#% 829025
#% 840892
#% 876018
#% 916785
#% 989618
#% 989654
#% 995140
#% 1002279
#% 1063503
#% 1117695
#% 1125382
#% 1181261
#% 1214701
#% 1474171
#% 1565432
#! Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks. A major challenge for link-based clustering in such networks is its potential to generate many different results, carrying rather diverse semantic meanings. In order to generate desired clustering, we propose to use meta-path, a path that connects object types via a sequence of relations, to control clustering with distinct semantics. Nevertheless, it is easier for a user to provide a few examples ("seeds") than a weighted combination of sophisticated meta-paths to specify her clustering preference. Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks, where a user first provides a small set of object seeds for each cluster as guidance. Then the system learns the weights for each meta-path that are consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths. A probabilistic approach is proposed to solve the problem, and an effective and efficient iterative algorithm, PathSelClus, is proposed to learn the model, where the clustering quality and the meta-path weights are mutually enhancing each other. Our experiments with several clustering tasks in two real networks demonstrate the power of the algorithm in comparison with the baselines.

#index 1872412
#* Query-driven discovery of semantically similar substructures in heterogeneous networks
#@ 28701 19945 18038 961
#t 2012
#c Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 288990
#% 1022280
#% 1214701
#% 1451228
#% 1523825
#% 1581921
#% 1606073
#% 1635098
#% 1635140
#! Heterogeneous information networks that contain multiple types of objects and links are ubiquitous in the real world, such as bibliographic networks, cyber-physical networks, and social media networks. Although researchers have studied various data mining tasks in information networks, interactive query-based network exploration techniques have not been addressed systematically, which, in fact, are highly desirable for exploring large-scale information networks. In this demo, we introduce and demonstrate our recent research project on query-driven discovery of semantically similar substructures in heterogeneous networks. Given a subgraph query, our system searches a given large information network and finds efficiently a list of subgraphs that are structurally identical and semantically similar. Since data mining methods are used to obtain semantically similar entities (nodes), we use discovery as a term to describe this process. In order to achieve high efficiency and scalability, we design and implement a filter-and verification search framework, which can first generate promising subgraph candidates using off line indices built by data mining results, and then verify candidates with a recursive pruning matching process. The proposed system demonstrates the effectiveness of our query-driven semantic similarity search framework and the efficiency of the proposed methodology on multiple real-world heterogeneous information networks.

#index 1880397
#* Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics
#@ 10394 961 12695 850
#t 2012
#c The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining

#index 1895106
#* Mining knowledge from interconnected data: a heterogeneous information network analysis approach
#@ 19945 961 9413 850
#t 2012
#c Proceedings of the VLDB Endowment
#% 1063512
#% 1081580
#% 1176876
#% 1181261
#% 1214701
#% 1451159
#% 1495579
#% 1581917
#% 1606073
#% 1635098
#% 1693927
#% 1707456
#% 1730734
#% 1872391
#! Most objects and data in the real world are interconnected, forming complex, heterogeneous but often semi-structured information networks. However, most people consider a database merely as a data repository that supports data storage and retrieval rather than one or a set of heterogeneous information networks that contain rich, inter-related, multi-typed data and information. Most network science researchers only study homogeneous networks, without distinguishing the different types of objects and links in the networks. In this tutorial, we view database and other interconnected data as heterogeneous information networks, and study how to leverage the rich semantic meaning of types of objects and links in the networks. We systematically introduce the technologies that can effectively and efficiently mine useful knowledge from such information networks.

#index 1904111
#* Panel on Mining the Big Data
#@ 4087 578 11941 961 3989 43624
#t 2012
#c Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining

#index 1925642
#* Community trend outlier detection using soft temporal pattern mining
#@ 23141 17596 19945 961
#t 2012
#c ECML PKDD'12 Proceedings of the 2012 European conference on Machine Learning and Knowledge Discovery in Databases - Volume Part II
#% 333929
#% 466425
#% 481290
#% 781774
#% 784297
#% 785389
#% 1019143
#% 1202160
#% 1206639
#% 1214633
#% 1446962
#% 1451221
#% 1482422
#% 1523860
#% 1562549
#% 1594652
#% 1607952
#% 1872334
#! Numerous applications, such as bank transactions, road traffic, and news feeds, generate temporal datasets, in which data evolves continuously. To understand the temporal behavior and characteristics of the dataset and its elements, we need effective tools that can capture evolution of the objects. In this paper, we propose a novel and important problem in evolution behavior discovery. Given a series of snapshots of a temporal dataset, each of which consists of evolving communities, our goal is to find objects which evolve in a dramatically different way compared with the other community members. We define such objects as community trend outliers. It is a challenging problem as evolutionary patterns are hidden deeply in noisy evolving datasets and thus it is difficult to distinguish anomalous objects from normal ones. We propose an effective two-step procedure to detect community trend outliers. We first model the normal evolutionary behavior of communities across time using soft patterns discovered from the dataset. In the second step, we propose effective measures to evaluate chances of an object deviating from the normal evolutionary patterns. Experimental results on both synthetic and real datasets show that the proposed approach is highly effective in discovering interesting community trend outliers.

#index 1978715
#* ETM: Entity Topic Models for Mining Documents Associated with Entities
#@ 31176 19945 44282 961
#t 2012
#c ICDM '12 Proceedings of the 2012 IEEE 12th International Conference on Data Mining
#! Topic models, which factor each document into different topics and represent each topic as a distribution of terms, have been widely and successfully used to better understand collections of text documents. However, documents are also associated with further information, such as the set of real-world entities mentioned in them. For example, news articles are usually related to several people, organizations, countries or locations. Since those associated entities carry rich information, it is highly desirable to build more expressive, entity-based topic models, which can capture the term distributions for each topic, each entity, as well as each topic-entity pair. In this paper, we therefore introduce a novel Entity Topic Model (ETM) for documents that are associated with a set of entities. ETM not only models the generative process of a term given its topic and entity information, but also models the correlation of entity term distributions and topic term distributions. A Gibbs sampling-based algorithm is proposed to learn the model. Experiments on real datasets demonstrate the effectiveness of our approach over several state-of-the-art baselines.

#index 1978801
#* Stream Classification with Recurring and Novel Class Detection Using Class-Based Ensemble
#@ 23700 22167 10403 23259 961 3712
#t 2012
#c ICDM '12 Proceedings of the 2012 IEEE 12th International Conference on Data Mining
#! Concept-evolution has recently received a lot of attention in the context of mining data streams. Concept-evolution occurs when a new class evolves in the stream. Although many recent studies address this issue, most of them do not consider the scenario of recurring classes in the stream. A class is called recurring if it appears in the stream, disappears for a while, and then reappears again. Existing data stream classification techniques either misclassify the recurring class instances as another class, or falsely identify the recurring classes as novel. This increases the prediction error of the classifiers, and in some cases causes unnecessary waste in memory and computational resources. In this paper we address the recurring class issue by proposing a novel "class-based" ensemble technique, which substitutes the traditional "chunk-based" ensemble approaches and correctly distinguishes between a recurring class and a novel one. We analytically and experimentally confirm the superiority of our method over state-of-the-art techniques.

#index 1978818
#* Towards Active Learning on Graphs: An Error Bound Minimization Approach
#@ 23981 961
#t 2012
#c ICDM '12 Proceedings of the 2012 IEEE 12th International Conference on Data Mining
#! Active learning on graphs has received increasing interest in the past years. In this paper, we propose a \textit{nonadaptive} active learning approach on graphs, based on generalization error bound minimization. In particular, we present a data-dependent error bound for a graph-based learning method, namely learning with local and global consistency (LLGC). We show that the empirical transductive Rademacher complexity of the function class for LLGC provides a natural criterion for active learning. The resulting active learning approach is to select a subset of nodes on a graph such that the empirical transductive Rademacher complexity of LLGC is minimized. We propose a simple yet effective sequential optimization algorithm to solve it. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art active learning methods on graphs.

