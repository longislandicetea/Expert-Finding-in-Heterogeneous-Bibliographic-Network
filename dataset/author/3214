#index 300120
#* Mining frequent patterns without candidate generation
#@ 961 3214 3215
#t 2000
#c SIGMOD '00 Proceedings of the 2000 ACM SIGMOD international conference on Management of data
#% 172386
#% 201894
#% 227919
#% 248785
#% 248791
#% 248813
#% 280409
#% 329598
#% 420063
#% 461909
#% 463903
#% 479484
#% 481290
#% 481754
#% 631926
#! Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.

#index 301173
#* Towards data mining benchmarking: a test bed for performance study of frequent pattern mining
#@ 3214 3344 3345 3346
#t 2000
#c SIGMOD '00 Proceedings of the 2000 ACM SIGMOD international conference on Management of data
#! Performance benchmarking has played an important role in the research and development in relational DBMS, object-relational DBMS, data warehouse systems, etc. We believe that benchmarking data mining algorithms is a long overdue task, and it will play an important role in the research and development of data mining systems as well.Frequent pattern mining forms a core component in mining associations, correlations, sequential patterns, partial periodicity, etc., which are of great potential value in applications. There have been a lot of methods proposed and developed for efficient frequent pattern mining in various kinds of databases, including transaction databases, time-series databases, etc. However, so far there is no serious performance benchmarking study of different frequent pattern mining methods.To facilitate an analytical comparison of different frequent mining methods, we have constructed an open test bed for performance study of a set of recently developed, popularly used methods for mining frequent patterns in transaction databases and mining sequential patterns in sequence databases, with different data characteristics. The testbed consists of the following components.A synthetic data generator, which can generate large sets of synthetic data in various kinds of data distributions. A few large data sets from real world applications will also be provided.A good set of typical frequent pattern mining methods, ranging from classical algorithms to recent studies. The method are grouped into three classes: frequent pattern mining, max-pattern mining, and sequential pattern mining. For frequent pattern mining, we will demonstrate Apriori, hashing, partitioning, sampling, TreeProjection, and FP-growth. For maximal pattern mining, we will demonstrate MaxMiner, TreeProjection, and FP-growth-max. For sequential pattern mining, we will demonstrate GSP and FreeSpan.A set of performance curves. These algorithms their running speeds, scalabilities, bottlenecks, and performance on different data distributions, will be compared and demonstrated upon request. Some performance curves from our pre-conference experimental evaluations will also be shown.An open testbed. Our goal is to construct an extensible test bed which integrates the above components and supports an open-ended testing service. Researchers can upload the object codes of their mining algorithms, and run them in the test bed using these data sets. The architecture is shown in Figure 1.This testbed is our first step towards benchmarking data mining algorithms. By doing so, performance of different algorithms can be reported consistently, on the same platform, and in the same environment. After the demo, we plan to make the testbed available on the WWW so that it may, hopefully, benefit further research and development of efficient data mining methods.

#index 310558
#* Can we push more constraints into frequent pattern mining?
#@ 3214 961
#t 2000
#c Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 248785
#% 273899
#% 300120
#% 481290
#% 632028

#index 310559
#* FreeSpan: frequent pattern-projected sequential pattern mining
#@ 961 3214 3599 3600 508 3601
#t 2000
#c Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 210160
#% 300120
#% 329598
#% 420063
#% 459006
#% 463903
#% 481290

#index 333925
#* Efficient computation of Iceberg cubes with complex measures
#@ 961 3214 171 1282
#t 2001
#c SIGMOD '01 Proceedings of the 2001 ACM SIGMOD international conference on Management of data
#% 210182
#% 223781
#% 227880
#% 248785
#% 273899
#% 273916
#% 300120
#% 310558
#% 420053
#% 479450
#% 479795
#% 481290
#% 481951
#! It is often too expensive to compute and materialize a complete high-dimensional data cube. Computing an iceberg cube, which contains only aggregates above certain thresholds, is an effective way to derive nontrivial multi-dimensional aggregations for OLAP and data mining.In this paper, we study efficient methods for computing iceberg cubes with some popularly used complex measures, such as average, and develop a methodology that adopts a weaker but anti-monotonic condition for testing and pruning search space. In particular, for efficient computation of iceberg cubes with the average measure, we propose a top-k average pruning method and extend two previously studied methods, Apriori and BUC, to Top-k Apriori and Top-k BUC. To further improve the performance, an interesting hypertree structure, called H-tree, is designed and a new iceberg cubing method, called Top-k H-Cubing, is developed. Our performance study shows that Top-k BUC and Top-k H-Cubing are two promising candidates for scalable computation, and Top-k H-Cubing has better performance in most cases.

#index 334041
#* DNA-miner: a system prototype for mining DNA sequences
#@ 961 3957 1980 3958 3959 3214
#t 2001
#c SIGMOD '01 Proceedings of the 2001 ACM SIGMOD international conference on Management of data

#index 338580
#* Mining frequent patterns by pattern-growth: methodology and implications
#@ 961 3214
#t 2000
#c ACM SIGKDD Explorations Newsletter - Special issue on “Scalable data mining algorithms”
#% 152934
#% 227919
#% 248785
#% 248791
#% 273899
#% 273916
#% 280409
#% 300120
#% 310558
#% 310559
#% 329598
#% 420063
#% 459006
#% 463903
#% 464839
#% 464989
#% 464996
#% 479482
#% 479484
#% 479795
#% 479971
#% 481290
#% 631926
#% 631970
#% 632028

#index 342817
#* Scalable frequent-pattern mining methods: an overview
#@ 961 190 3214
#t 2001
#c Tutorial notes of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 227917
#% 227919
#% 227922
#% 227953
#% 248784
#% 248785
#% 248791
#% 248813
#% 259993
#% 273899
#% 273916
#% 280473
#% 300120
#% 310494
#% 310558
#% 310559
#% 318994
#% 329598
#% 333925
#% 420063
#% 420067
#% 438134
#% 459006
#% 461909
#% 463903
#% 464204
#% 464822
#% 464839
#% 464989
#% 464996
#% 479482
#% 479484
#% 479627
#% 479795
#% 479971
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 481954
#% 584891
#% 631926
#% 632028
#% 632037

#index 397411
#* COMMIX: towards effective web information extraction, integration and query answering
#@ 4611 4612 4613 4614 4615 3214
#t 2002
#c Proceedings of the 2002 ACM SIGMOD international conference on Management of data
#! As WWW becomes more and more popular and powerful, how to search information on the web in database way becomes an important research topic. COMMIX, which is developed in the DB group in Peking University (China), is a system towards building very large database using data from the Web for information extraction, integration and query answering. COMMIX has some innovative features, such as ontology-based wrapper generation, XML-based information integration, view-based query answering, and QBE-style XML query interface.

#index 397417
#* CubeExplorer: online exploration of data cubes
#@ 961 4625 171 3214 1282
#t 2002
#c Proceedings of the 2002 ACM SIGMOD international conference on Management of data
#% 333925
#% 480630
#! Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation [2], cube-based feature extraction, and gradient analysis [1], and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.

#index 399793
#* Constrained frequent pattern mining: a pattern-growth view
#@ 3214 961
#t 2002
#c ACM SIGKDD Explorations Newsletter
#% 172386
#% 201894
#% 227919
#% 248785
#% 248791
#% 248813
#% 273899
#% 273916
#% 280409
#% 300120
#% 310558
#% 310559
#% 338580
#% 420063
#% 461909
#% 463903
#% 464989
#% 464996
#% 465003
#% 466490
#% 479484
#% 481290
#% 481754
#% 631926
#% 631985
#% 632028
#! It has been well recognized that frequent pattern mining plays an essential role in many important data mining tasks. However, frequent pattern mining often generates a very large number of patterns and rules, which reduces not only the efficiency but also the effectiveness of mining. Recent work has highlighted the importance of the constraint-based mining paradigm in the context of mining frequent itemsets, associations, correlations, sequential patterns, and many other interesting patterns in large databases.Recently, we developed efficient pattern-growth methods for frequent pattern mining. Interestingly, pattern-growth methods are not only efficient but also effective in mining with various constraints. Many tough constraints which cannot be handled by previous methods can be pushed deep into the pattern-growth mining process. In this paper, we overview the principles of pattern-growth methods for constrained frequent pattern mining and sequential pattern mining. Moreover, we explore the power of pattern-growth methods towards mining with tough constraints and highlight some interesting open problems.

#index 466483
#* CMAR: Accurate and Efficient Classification Based on Multiple Class-Association Rules
#@ 5904 961 3214
#t 2001
#c ICDM '01 Proceedings of the 2001 IEEE International Conference on Data Mining
#! Previous studies propose that associative classification has high classification accuracy and strong flexibility at handling unstructured data. However, it still suffers from the huge set of mined rules and sometimes biased classification or overfitting since the classificationis based on only single high-confidence rule. In this study, we propose new associative classification method, CMAR, i.e., Classification based on Multiple Association Rules. The method extends an efficient frequent pattern mining method, FP-growth ,constructs classdistribution-associated FP-tree, and mines large database efficiently. Moreover, it applies CR-tree structure to store and retrieve mined association rulesefficiently, and prunes rules effectively based on confidence, correlation and database coverage. The classification is performed based on weighted X2 analysis using multiple strong association rules. Our extensive experiments on 26 databases from UCI machine learning database repository show that CMAR is consistent, highly effective at classificationof various kinds of databases and has better average classificationaccuracy in comparison with CBA and C4.5.Moreover,our performancestudy shows that the method is highly efficient and scalable in comparison with other reported associative classification methods.

#index 466490
#* H-Mine: Hyper-Structure Mining of Frequent Patterns in Large Databases
#@ 3214 961 2471 5712 4612 4613
#t 2001
#c ICDM '01 Proceedings of the 2001 IEEE International Conference on Data Mining
#! Methods for efficient mining of frequent patterns have been studied extensively by many researchers. However, the previously proposed methods still encounter someperformance bottlenecks when mining databases with different data characteristics, such as dense vs. sparse, long vs. short patterns, memory-based vs. disk-based, etc.In this study, we propose a simple and novel hyper-linkeddata structure, H-struct , and a new mining algorithm, H-mine ,which takes advantage of this data structure anddynamically adjusts links in the mining process. A distinct feature of this method is that it has very limitedand precisely predictable space overhead and runs really fast in memory-based setting. Moreover, it ca be scaled up to very large databases by database partitioning, and whenthe data set becomes dense,(conditional)FP-trees can be constructed dynamically as part of the mining process. Our study shows that H-mine has high performance in various kinds of data, outperforms the previously developedalgorithms in different settings, and is highly scalable in mining large databases. This study also proposes a new datamining methodology, space-preserving mining ,which mayhave strong impact in the future development of efficient and scalable data mining methods.

#index 629606
#* On Computing Condensed Frequent Pattern Bases
#@ 3214 171 9304 961
#t 2002
#c ICDM '02 Proceedings of the 2002 IEEE International Conference on Data Mining
#! Frequent pattern mining has been studied extensively.However, the effectiveness and efficiency of this mining isoften limited, since the number of frequent patterns generatedis often too large. In many applications it is sufficientto generate and examine only frequent patterns with supportfrequency in close-enough approximation instead of in fullprecision. Such a compact but close-enough frequent patternbase is called a condensed frequent patterns-base.In this paper, we propose and examine several alternativesat the design, representation, and implementation ofsuch condensed frequent pattern-bases. A few algorithmsfor computing such pattern-bases are proposed. Their effectivenessat pattern compression and their efficient computationmethods are investigated. A systematic performancestudy is conducted on different kinds of databases,which demonstrates the effectiveness and efficiency of ourapproach at handling frequent pattern mining in largedatabases.

#index 654446
#* QC-trees: an efficient summary structure for semantic OLAP
#@ 190 3214 9681
#t 2003
#c Proceedings of the 2003 ACM SIGMOD international conference on Management of data
#% 152928
#% 198465
#% 210182
#% 227868
#% 227869
#% 227880
#% 227944
#% 236410
#% 259995
#% 273696
#% 273916
#% 280448
#% 287047
#% 340301
#% 397388
#% 459026
#% 464215
#% 479450
#% 480460
#% 480470
#% 480820
#% 481951
#% 993996
#! Recently, a technique called quotient cube was proposed as a summary structure for a data cube that preserves its semantics, with applications for online exploration and visualization. The authors showed that a quotient cube can be constructed very efficiently and it leads to a significant reduction in the cube size. While it is an interesting proposal, that paper leaves many issues unaddressed. Firstly, a direct representation of a quotient cube is not as compact as possible and thus still wastes space. Secondly, while a quotient cube can in principle be used for answering queries, no specific algorithms were given in the paper. Thirdly, maintaining any summary structure incrementally against updates is an important task, a topic not addressed there. In this paper, we propose an efficient data structure called QC-tree and an efficient algorithm for directly constructing it from a base table, solving the first problem. We give efficient algorithms that address the remaining questions. We report results from an extensive performance study that illustrate the space and time savings achieved by our algorithms over previous ones (wherever they exist).

#index 654500
#* SOCQET: semantic OLAP with compressed cube and summarization
#@ 190 3214 9681
#t 2003
#c Proceedings of the 2003 ACM SIGMOD international conference on Management of data
#% 654446
#% 993996

#index 727908
#* MaPle: A Fast Algorithm for Maximal Pattern-based Clustering
#@ 3214 10078 10079 4558 850
#t 2003
#c ICDM '03 Proceedings of the Third IEEE International Conference on Data Mining
#% 152934
#% 248792
#% 273891
#% 280417
#% 300120
#% 300131
#% 397382
#% 464888
#% 469422
#% 480124
#% 481290
#! Pattern-based clustering is important in many applications,such as DNA micro-array data analysis, automaticrecommendation systems and target marketing systems.However, pattern-based clustering in large databasesis challenging. On the one hand, there can be a huge numberof clusters and many of them can be redundant and thusmake the pattern-based clustering ineffective. On the otherhand, the previous proposed methods may not be efficient orscalable in mining large databases.In this paper, we study the problem of maximal pattern-basedclustering. Redundant clusters are avoided completelyby mining only the maximal pattern-based clusters.MaPle, an efficient and scalable mining algorithm is developed.It conducts a depth-first, divide-and-conquer searchand prunes unnecessary branches smartly. Our extensiveperformance study on both synthetic data sets and real datasets shows that maximal pattern-based clustering is effective.It reduces the number of clusters substantially. Moreover,MaPle is more efficient and scalable than the previouslyproposed pattern-based clustering methods in mininglarge databases.

#index 729933
#* CLOSET+: searching for the best strategies for mining frequent closed itemsets
#@ 4625 961 3214
#t 2003
#c Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 152934
#% 201894
#% 227917
#% 248791
#% 300120
#% 310494
#% 342643
#% 443348
#% 464714
#% 465003
#% 466490
#% 481290
#% 481779
#% 577234
#% 629644
#! Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis. Extensive studies have proposed various strategies for efficient frequent closed itemset mining, such as depth-first search vs. breadthfirst search, vertical formats vs. horizontal formats, tree-structure vs. other data structures, top-down vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask "what are the pros and cons of the strategies?" and "what and how can we pick and integrate the best strategies to achieve higher performance in general cases?"In this study, we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability.

#index 729972
#* Interactive exploration of coherent patterns in time-series gene expression data
#@ 10197 3214 5691
#t 2003
#c Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 273890
#% 397382
#% 469425
#! Discovering coherent gene expression patterns in time-series gene expression data is an important task in bioinformatics research and biomedical applications. In this paper, we propose an interactive exploration framework for mining coherent expression patterns in time-series gene expression data. We develop a novel tool, coherent pattern index graph, to give users highly confident indications of the existences of coherent patterns. To derive a coherent pattern index graph, we devise an attraction tree structure to record the genes in the data set and summarize the information needed for the interactive exploration. We present fast and scalable algorithms to construct attraction trees and coherent pattern index graphs from gene expression data sets. We conduct an extensive performance study on some real data sets to verify our design. The experimental results strongly show that our approach is more effective than the state-of-the-art methods in mining real gene expression data, and is scalable in mining large data sets.

#index 729987
#* Mining phenotypes and informative genes from gene expression data
#@ 10215 5691 3214
#t 2003
#c Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 248792
#% 469422
#% 659967
#! Mining microarray gene expression data is an important research topic in bioinformatics with broad applications. While most of the previous studies focus on clustering either genes or samples, it is interesting to ask whether we can partition the complete set of samples into exclusive groups (called phenotypes) and find a set of informative genes that can manifest the phenotype structure. In this paper, we propose a new problem of simultaneously mining phenotypes and informative genes from gene expression data. Some statistics-based metrics are proposed to measure the quality of the mining results. Two interesting algorithms are developed: the heuristic search and the mutual reinforcing adjustment method. We present an extensive performance study on both real-world data sets and synthetic data sets. The mining results from the two proposed methods are clearly better than those from the previous methods. They are ready for the real-world applications. Between the two methods, the mutual reinforcing adjustment method is in general more scalable, more effective and with better quality of the mining results.

#index 745457
#* Data Mining for Intrusion Detection: Techniques, Applications and Systems
#@ 3214 10505 10506 10507
#t 2004
#c ICDE '04 Proceedings of the 20th International Conference on Data Engineering

#index 748010
#* Towards interactive exploration of gene expression patterns
#@ 10197 3214 5691
#t 2003
#c ACM SIGKDD Explorations Newsletter
#% 60576
#% 273890
#% 324431
#% 397382
#% 397631
#% 438617
#% 469422
#% 469425
#% 589434
#% 659967
#% 727908
#% 729972
#! Analyzing coherent gene expression patterns is an important task in bioinformatics research and biomedical applications. Recently, various clustering methods have been adapted or proposed to identify clusters of co-expressed genes and recognize coherent expression patterns as the centroids of the clusters. However, the interpretation of co-expressed genes and coherent patterns mainly depends on the domain knowledge, which presents several challenges for coherent pattern mining and cannot be solved by most existing clustering approaches.In this paper, we introduce an interactive exploration system GeneX (Gene eXplorer) for mining coherent expression patterns. We develop a novel coherent pattern index graph to provide highly confident indications of the existence of coherent patterns. Typical exploration operations are supported based on the index graph. We also provide a bunch of graphical views as the user interface to visualize the data set and facilitate the interactive operations. To help users to interpret and validate the mining results, we design the gene annotation panel that connects the genes with some public annotation databases. The experimental results show that our approach is more effective than the state-of-the-art methods in mining real gene expression data sets.

#index 769907
#* Scalable mining of large disk-based graph databases
#@ 11027 1204 3214 11028 11029
#t 2004
#c Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 410276
#% 466644
#% 577218
#% 629603
#% 629646
#% 629708
#% 729938
#% 765429
#! Mining frequent structural patterns from graph databases is an interesting problem with broad applications. Most of the previous studies focus on pruning unfruitful search subspaces effectively, but few of them address the mining on large, disk-based databases. As many graph databases in applications cannot be held into main memory, scalable mining of large, disk-based graph databases remains a challenging problem. In this paper, we develop an effective index structure, ADI (for adjacency index), to support mining various graph patterns over large databases that cannot be held into main memory. The index is simple and efficient to build. Moreover, the new index structure can be easily adopted in various existing graph pattern mining algorithms. As an example, we adapt the well-known gSpan algorithm by using the ADI structure. The experimental results show that the new index structure enables the scalable graph pattern mining over large databases. In one set of the experiments, the new disk-based method can mine graph databases with one million graphs, while the original gSpan algorithm can only handle databases of up to 300 thousand graphs. Moreover, our new method is faster than gSpan when both can run in main memory.

#index 769917
#* A rank sum test method for informative gene discovery
#@ 11036 3214 10881 5491
#t 2004
#c Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 269217
#% 297684
#% 328374
#% 397641
#% 425048
#% 451139
#% 717415
#% 729437
#% 769919
#! Finding informative genes from microarray data is an important research problem in bioinformatics research and applications. Most of the existing methods rank features according to their discriminative capability and then find a subset of discriminative genes (usually top k genes). In particular, t-statistic criterion and its variants have been adopted extensively. This kind of methods rely on the statistics principle of t-test, which requires that the data follows a normal distribution. However, according to our investigation, the normality condition often cannot be met in real data sets.To avoid the assumption of the normality condition, in this paper, we propose a rank sum test method for informative gene discovery. The method uses a rank-sum statistic as the ranking criterion. Moreover, we propose using the significance level threshold, instead of the number of informative genes, as the parameter. The significance level threshold as a parameter carries the quality specification in statistics. We follow the Pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t-statistic method in theory.To verify the effectiveness of the rank sum method, we use support vector machine (SVM) to construct classifiers based on the identified informative genes on two well known data sets, namely colon data and leukemia data. The prediction accuracy reaches 96.2% on the colon data and 100% on the leukemia data. The results are clearly better than those from the previous feature ranking methods. By experiments, we also verify that using significance level threshold is more effective than directly specifying an arbitrary k.

#index 769919
#* Mining coherent gene clusters from gene-sample-time microarray data
#@ 10197 3214 11038 10215 5691
#t 2004
#c Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#% 328317
#% 397382
#% 469422
#% 727908
#% 729972
#% 729987
#! Extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications. In this paper, we explore a novel type of gene-sample-time microarray data sets, which records the expression levels of various genes under a set of samples during a series of time points. In particular, we propose the mining of coherent gene clusters from such data sets. Each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series. The coherent gene clusters may identify the samples corresponding to some phenotypes (e.g., diseases), and suggest the candidate genes correlated to the phenotypes. We present two efficient algorithms, namely the Sample-Gene Search and the Gene-Sample Search, to mine the complete set of coherent gene clusters. We empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets. The test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters.

#index 800531
#* Mining Cross-Graph Quasi-Cliques in Gene Expression and Protein Interaction Data
#@ 3214 10197 5691
#t 2005
#c ICDE '05 Proceedings of the 21st International Conference on Data Engineering
#% 397382
#% 469422
#% 731609

#index 800634
#* Online Mining of Data Streams: Applications, Techniques and Progress
#@ 4558 3214 850
#t 2005
#c ICDE '05 Proceedings of the 21st International Conference on Data Engineering

#index 810094
#* GraphMiner: a structural pattern-mining system for large disk-based graph databases and its applications
#@ 1204 11027 11028 11029 3214 9413 961
#t 2005
#c Proceedings of the 2005 ACM SIGMOD international conference on Management of data
#% 466644
#% 629603
#% 629646
#% 629708
#% 729938
#% 769907
#! Mining frequent structural patterns from graph databases is an important research problem with broad applications. Recently, we developed an effective index structure, ADI, and efficient algorithms for mining frequent patterns from large, disk-based graph databases [5], as well as constraint-based mining techniques. The techniques have been integrated into a research prototype system--- GraphMiner. In this paper, we describe a demo of GraphMiner which showcases the technical details of the index structure and the mining algorithms including their efficient implementation, the mining performance and the comparison with some state-of-the-art methods, the constraint-based graph-pattern mining techniques and the procedure of constrained graph mining, as well as mining real data sets in novel applications.

#index 823347
#* On mining cross-graph quasi-cliques
#@ 3214 10197 5691
#t 2005
#c Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining
#% 268040
#% 324431
#% 397382
#% 408396
#% 466644
#% 466675
#% 469422
#% 469425
#% 498852
#% 577219
#% 629708
#% 729938
#% 731604
#% 765429
#% 769887
#% 769894
#% 769907
#! Joint mining of multiple data sets can often discover interesting, novel, and reliable patterns which cannot be obtained solely from any single source. For example, in cross-market customer segmentation, a group of customers who behave similarly in multiple markets should be considered as a more coherent and more reliable cluster than clusters found in a single market. As another example, in bioinformatics, by joint mining of gene expression data and protein interaction data, we can find clusters of genes which show coherent expression patterns and also produce interacting proteins. Such clusters may be potential pathways.In this paper, we investigate a novel data mining problem, mining cross-graph quasi-cliques, which is generalized from several interesting applications such as cross-market customer segmentation and joint mining of gene expression data and protein interaction data. We build a general model for mining cross-graph quasi-cliques, show why the complete set of cross-graph quasi-cliques cannot be found by previous data mining methods, and study the complexity of the problem. While the problem is difficult, we develop an efficient algorithm, Crochet, which exploits several interesting and effective techniques and heuristics to efficaciously mine cross-graph quasi-cliques. A systematic performance study is reported on both synthetic and real data sets. We demonstrate some interesting and meaningful cross-graph quasi-cliques in bioinformatics. The experimental results also show that algorithm Crochet is efficient and scalable.

#index 823424
#* Pattern-based similarity search for microarray data
#@ 4558 3214 850
#t 2005
#c Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining
#% 289010
#% 397382
#% 468476
#% 469422
#% 593863
#! One fundamental task in near-neighbor search as well as other similarity matching efforts is to find a distance function that can efficiently quantify the similarity between two objects in a meaningful way. In DNA microarray analysis, the expression levels of two closely related genes may rise and fall synchronously in response to a set of experimental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very similar. Unfortunately, none of the conventional distance metrics such as the Lp norm can model this similarity effectively. In this paper, we study the near-neighbor search problem based on this new type of similarity. We propose to measure the distance between two genes by subspace pattern similarity, i.e., whether they exhibit a synchronous pattern of rise and fall on a subset of dimensions. We then present an efficient algorithm for subspace near-neighbor search based on pattern similarity distance, and we perform tests on various data sets to show its effectiveness.

#index 824672
#* Catching the best views of skyline: a semantic approach based on decisive subspaces
#@ 3214 4232 2497 4552
#t 2005
#c VLDB '05 Proceedings of the 31st international conference on Very large data bases
#% 100803
#% 288976
#% 333854
#% 384416
#% 465167
#% 479986
#% 480671
#% 654480
#% 800555
#% 824671
#% 993954
#% 993996
#! The skyline operator is important for multi-criteria decision making applications. Although many recent studies developed efficient methods to compute skyline objects in a specific space, the fundamental problem on the semantics of skylines remains open: Why and in which subspaces is (or is not) an object in the skyline? Practically, users may also be interested in the skylines in any subspaces. Then, what is the relationship between the skylines in the subspaces and those in the super-spaces? How can we effectively analyze the subspace skylines? Can we efficiently compute skylines in various subspaces?In this paper, we investigate the semantics of skylines, propose the subspace skyline analysis, and extend the full-space skyline computation to subspace skyline computation. We introduce a novel notion of skyline group which essentially is a group of objects that are coincidentally in the skylines of some subspaces. We identify the decisive subspaces that qualify skyline groups in the subspace skylines. The new notions concisely capture the semantics and the structures of skylines in various subspaces. Multidimensional roll-up and drilldown analysis is introduced. We also develop an efficient algorithm, Skyey, to compute the set of skyline groups and, for each subspace, the set of objects that are in the subspace skyline. A performance study is reported to evaluate our approach.

#index 844401
#* Efficiently Mining Frequent Closed Partial Orders
#@ 3214 14869 4558 1282 850 4625
#t 2005
#c ICDM '05 Proceedings of the Fifth IEEE International Conference on Data Mining
#% 310515
#% 397632
#% 420063
#% 463903
#% 464996
#% 729922
#! Mining ordering information from sequence data is an important data mining task. Sequential pattern mining [1] can be regarded as mining frequent segments of total orders from sequence data. However, sequential patterns are often insufficient to concisely capture the general ordering information.

#index 864452
#* SUBSKY: Efficient Computation of Skylines in Subspaces
#@ 4552 14012 3214
#t 2006
#c ICDE '06 Proceedings of the 22nd International Conference on Data Engineering
#! Given a set of multi-dimensional points, the skyline contains the best points according to any preference function that is monotone on all axes. In practice, applications that require skyline analysis usually provide numerous candidate attributes, and various users depending on their interests may issue queries regarding different (small) subsets of the dimensions. Formally, given a relation with a large number (e.g.,ge 10) of attributes, a query aims at finding the skyline in an arbitrary subspace with a low dimensionality (e.g., 2). The existing algorithms do not support subspace skyline retrieval efficiently because they (i) require scanning the entire database at least once, or (ii) are optimized for one particular subspace but incur significant overhead for other subspaces. In this paper, we propose a technique SUBSKY which settles the problem using a single B-tree, and can be implemented in any relational database. The core of SUBSKY is a transformation that converts multi-dimensional data to 1D values, and enables several effective pruning heuristics. Extensive experiments with real data confirm that SUBSKY outperforms alternative approaches significantly in both efficiency and scalability.

#index 881507
#* On privacy preservation against adversarial data mining
#@ 968 3214 7430
#t 2006
#c Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 1868
#% 300184
#% 333876
#% 342614
#% 577233
#% 577289
#% 740764
#% 769885
#% 800515
#% 993988
#! Privacy preserving data processing has become an important topic recently because of advances in hardware technology which have lead to widespread proliferation of demographic and sensitive data. A rudimentary way to preserve privacy is to simply hide the information in some of the sensitive fields picked by a user. However, such a method is far from satisfactory in its ability to prevent adversarial data mining. Real data records are not randomly distributed. As a result, some fields in the records may be correlated with one another. If the correlation is sufficiently high, it may be possible for an adversary to predict some of the sensitive fields using other fields.In this paper, we study the problem of privacy preservation against adversarial data mining, which is to hide a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved. In other words, even by data mining, an adversary still cannot accurately recover the hidden data entries. We model the problem concisely and develop an efficient heuristic algorithm which can find good solutions in practice. An extensive performance study is conducted on both synthetic and real data sets to examine the effectiveness of our approach.

#index 881543
#* Suppressing model overfitting in mining concept-drifting data streams
#@ 4558 7463 3214 850 4839
#t 2006
#c Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 95730
#% 273900
#% 310500
#% 333931
#% 342600
#% 342639
#% 378388
#% 397380
#% 424997
#% 594012
#% 729932
#% 785339
#% 823333
#% 823408
#% 844341
#% 993958
#! Mining data streams of changing class distributions is important for real-time business decision support. The stream classifier must evolve to reflect the current class distribution. This poses a serious challenge. On the one hand, relying on historical data may increase the chances of learning obsolete models. On the other hand, learning only from the latest data may lead to biased classifiers, as the latest data is often an unrepresentative sample of the current class distribution. The problem is particularly acute in classifying rare events, when, for example, instances of the rare class do not even show up in the most recent training data. In this paper, we use a stochastic model to describe the concept shifting patterns and formulate this problem as an optimization one: from the historical and the current training data that we have observed, find the most-likely current distribution, and learn a classifier based on the most-likely distribution. We derive an analytic solution and approximate this solution with an efficient algorithm, which calibrates the influence of historical data carefully to create an accurate classifier. We evaluate our algorithm with both synthetic and real-world datasets. Our results show that our algorithm produces accurate and efficient classification.

#index 881551
#* Utility-based anonymization using local recoding
#@ 10589 1204 3214 16138 11029 1241
#t 2006
#c Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 248030
#% 443463
#% 576761
#% 576762
#% 577239
#% 800515
#% 801690
#% 810011
#% 824726
#% 864406
#% 1700134
#! Privacy becomes a more and more serious concern in applications involving microdata. Recently, efficient anonymization has attracted much research work. Most of the previous methods use global recoding, which maps the domains of the quasi-identifier attributes to generalized or changed values. However, global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data. Moreover, anonymized data is often for analysis. As well accepted in many analytical applications, different attributes in a data set may have different utility in the analysis. The utility of attributes has not been considered in the previous methods.In this paper, we study the problem of utility-based anonymization. First, we propose a simple framework to specify utility of attributes. The framework covers both numeric and categorical data. Second, we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization. Our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. Furthermore, our utility-based method can boost the quality of analysis using the anonymized data.

#index 893204
#* Using high dimensional indexes to support relevance feedback based interactive images retrieval
#@ 17355 17356 1204 11029 3214
#t 2006
#c VLDB '06 Proceedings of the 32nd international conference on Very large data bases
#% 318785
#% 341267
#% 411758
#% 427199
#% 443889
#% 479462
#% 479649
#% 480632
#% 592183
#% 810069
#% 814646
#% 1775156
#! Image retrieval has found more and more applications. Due to the well recognized semantic gap problem, the accuracy and the recall of image similarity search are often still low. As an effective method to improve the quality of image retrieval, the relevance feedback approach actively applies users' feedback to refine the search. As searching a large image database is often costly, to improve the efficiency, high dimensional indexes may help. However, many existing database indexes are not adaptive to updates of distance measures caused by users' feedback. In this paper, we propose a demo to illustrate the relevance feedback based interactive images retrieval procedure, and examine the effectiveness and the efficiency of various indexes. Particularly, audience can interactively investigate the effect of updated distance measures on the data space where the images are supposed to be indexed, and on the distributions of the similar images in the indexes. We also introduce our new B+-tree-like index method based on cluster splitting and iDistance.

#index 907499
#* Classification spanning correlated data streams
#@ 9314 1282 1241 10164 3214
#t 2006
#c CIKM '06 Proceedings of the 15th ACM international conference on Information and knowledge management
#% 246831
#% 273682
#% 273908
#% 310500
#% 378388
#% 397354
#% 479787
#% 594012
#% 635215
#% 654444
#% 729932
#% 729965
#% 765494
#% 809257
#% 844359
#% 881938
#% 993949
#% 993961
#% 1015296
#% 1016156
#% 1250553
#! In many applications, classifiers need to be built based on multiple related data streams. For example, stock streams and news streams are related, where the classification patterns may involve features from both streams. Thus instead of mining on a single isolated stream, we need to examine multiple related data streams in order to find such patterns and build an accurate classifier. Other examples of related streams include traffic reports and car accidents, sensor readings of different types or at different locations, etc. In this paper, we consider the classification problem defined over sliding-window join of several input data streams. As the data streams arrive in fast pace and the many-to-many join relationship blows up the data arrival rate even more, it is impractical to compute the join and then build the classifier each time the window slides forward. We present an efficient algorithm to build a Naïve Bayesian classifier in such context. Our method does not need to perform the join operations but is still able to build exactly the same classifier as if built on the joined result. It only examines each input tuple twice, independent of the number of tuples it joins in other streams, therefore, is able to keep pace with the fast arriving data streams in the presence of many-to-many join relationships. The experiments confirmed that our classification algorithm is more efficient than conventional methods while maintaining good classification accuracy.

#index 915273
#* Improving Grouped-Entity Resolution Using Quasi-Cliques
#@ 17602 14105 2021 672 3214
#t 2006
#c ICDM '06 Proceedings of the Sixth International Conference on Data Mining
#! The entity resolution (ER) problem, which identifies duplicate entities that refer to the same real world entity, is essential in many applications. In this paper, in particular, we focus on resolving entities that contain a group of related elements in them (e.g., an author entity with a list of citations, a singer entity with song list, or an intermediate result by GROUP BY SQL query). Such entities, named as grouped-entities, frequently occur in many applications. The previous approaches toward grouped-entity resolution often rely on textual similarity, and produce a large number of false positives. As a complementing technique, in this paper, we present our experience of applying a recently proposed graph mining technique, Quasi-Clique, atop conventional ER solutions. Our approach exploits contextual information mined from the group of elements per entity in addition to syntactic similarity. Extensive experiments verify that our proposal improves precision and recall up to 83% when used together with a variety of existing ER solutions, but never worsens them.

#index 949572
#* Utility-based anonymization for privacy preservation with less information loss
#@ 10589 1204 3214 16138 11029 1241
#t 2006
#c ACM SIGKDD Explorations Newsletter
#% 248030
#% 321455
#% 443463
#% 576761
#% 576762
#% 577239
#% 785363
#% 800514
#% 800515
#% 801690
#% 810011
#% 824726
#% 864406
#% 1700134
#! Privacy becomes a more and more serious concern in applications involving microdata. Recently, efficient anonymization has attracted much research work. Most of the previous methods use global recoding, which maps the domains of the quasi-identifier attributes to generalized or changed values. However, global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data. Moreover, anonymized data is often used for analysis. As well accepted in many analytical applications, different attributes in a data set may have different utility in the analysis. The utility of attributes has not been considered in the previous methods. In this paper, we study the problem of utility-based anonymization. First, we propose a simple framework to specify utility of attributes. The framework covers both numeric and categorical data. Second, we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization. Our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. Furthermore, our utility-based method can boost the quality of analysis using the anonymized data.

#index 989652
#* Mining favorable facets
#@ 10073 3214 1241 1282
#t 2007
#c Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 287414
#% 289148
#% 427199
#% 465167
#% 480671
#% 800512
#% 806212
#% 810024
#% 824671
#% 824672
#% 864451
#% 875011
#% 915803
#% 992635
#% 993954
#% 1408811
#% 1712421
#! The importance of dominance and skyline analysis has been well recognized in multi-criteria decision making applications. Most previous studies assume a fixed order on the attributes. In practice, different customers may have different preferences on nominal attributes. In this paper, we identify an interesting data mining problem, finding favorable facets, which has not been studied before. Given a set of points in a multidimensional space, for a specific target point p we want to discover with respect to which combinations of orders (e.g., customer preferences) on the nominal attributes p is not dominated by any other points. Such combinations are called the favorable facets of p. We consider both the effectiveness and the efficiency of the mining. A given point may have many favorable facets. We propose the notion of minimal disqualifying condition (MDC) which is effective in summarizing favorable facets. We develop efficient algorithms for favorable facet mining for different application scenarios. The first method computes favorable facets on the fly. The second method pre-computes all minimal disqualifying conditions so that the favorable facets can be looked up in constant time. An extensive performance study using both synthetic and real data sets is reported to verify their effectiveness and efficiency.

#index 989667
#* Cleaning disguised missing data: a heuristic approach
#@ 18696 3214
#t 2007
#c Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 17144
#% 798813
#% 878941
#% 1272290
#! In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely, such as causing significant biases and misleading results in hypothesis tests, correlation analysis and regressions. The very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection. They highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers. To tackle the problem of cleaning disguised missing data, in this paper, we first model the distribution of disguised missing data, and propose the embedded unbiased sample heuristic. Then, we develop an effective and efficient method to identify the frequently used disguise values which capture the major body of the disguised missing data. Our method does not require any domain background knowledge to find the suspicious disguise values. We report an empirical evaluation using real data sets, which shows that our method is effective - the frequently used disguise values found by our method match the values identified by the domain experts nicely. Our method is also efficient and scalable for processing large data sets.

#index 993996
#* Quotient cube: how to summarize the semantics of a data cube
#@ 190 3214 961
#t 2002
#c VLDB '02 Proceedings of the 28th international conference on Very Large Data Bases
#% 210182
#% 227880
#% 236410
#% 237202
#% 259995
#% 273916
#% 280448
#% 397388
#% 464215
#% 479450
#% 480820
#% 481290
#% 481951
#! Partitioning a data cube into sets of cells with "similar behavior" often better exposes the semantics in the cube. E.g., if we find that average boots sales in the West 10th store of Walmart was the same for winter as for the whole year, it signifies something interesting about the trend of boots sales in that location in that year. In this paper, we are interested in finding succinct summaries of the data cube, exploiting regularities present in the cube, with a clear basis. We would like the summary: (i) to be as concise as possible, (ii) to itself form a lattice preserving the rollup/drilldown semantics of the cube, and (iii) to allow the original cube to be fully recovered. We illustrate the utility of solving this problem and discuss the inherent challenges. We develop techniques for partitioning cube cells for obtaining succinct summaries, and introduce the quotient cube. We give efficient algorithms for computing it from a base table. For monotone aggregate functions (e.g., COUNT, MIN, MAX, SUM on non-negative measures, etc.), our solution is optimal (i.e., quotient cube of the least size). For nonmonotone functions (e.g., AVG), we obtain a locally optimal solution. We experimentally demonstrate the efficacy of our ideas and techniques and the scalability of our algorithms.

#index 998861
#* Introduction to the special issue on data mining for health informatics
#@ 1020 3214
#t 2007
#c ACM SIGKDD Explorations Newsletter - Special issue on data mining for health informatics
#! One of the holy grails of medical research in the next decade is what is often called "personalized" medicine. The goal is to individualize therapy and treatment options, and even prevention strategies. This movement is fueled by the rapid advances made in high-throughput biotechnologies. Prime examples include SNP (Single Nucleotide Polymorphisms) chips and CGH (Comparative Genomic Hybridization) arrays for DNA profiling, oligonucleotide arrays for mRNA expression, and advanced mass spectrometry for peptide/protein and metabolite quantitation.

#index 1015366
#* Efficacious data cube exploration by semantic summarization and compression
#@ 190 3214 9681
#t 2003
#c VLDB '03 Proceedings of the 29th international conference on Very large data bases - Volume 29
#% 207552
#% 654446
#% 993996
#! Data cube is the core operator in data warehousing and OLAP. Its efficient computation, maintenance, and utilization for query answering and advanced analysis have been the subjects of numerous studies. However, for many applications, the huge size of the data cube limits its applicability as a means for semantic exploration by the user. Recently, we have developed a systematic approach to achieve efficacious data cube construction and exploration by semantic summarization and compression. Our approach is pivoted on a notion of quotient cube that groups together structurally related data cube cells with common (aggregate) measure values into equivalence classes. The equivalence relation used to partition the cube lattice preserves the roll-up/drill-down semantics of the data cube, in that the same kind of explorations can be conducted in the quotient cube as in the original cube, between classes instead of between cells. We have also developed compact data structures for representing a quotient cube and efficient algorithms for answering queries using a quotient cube for its incremental maintenance against updates. We have implemented SOCQET, a prototype data warehousing system making use of our results on quotient cube. In this demo, we will demonstrate (1) the critical techniques of building a quotient cube; (2) use of a quotient cube to answer various queries and to support advanced OLAP; (3) an empirical study on the effectiveness and efficiency of quotient cube-based data warehouses and OLAP; (4) a user interface for visual and interactive OLAP; and (5) SOCQET, a research prototype data warehousing system integrating all the techniques. The demo reflects our latest research results and may stimulate some interesting future studies.

#index 1016243
#* GPX: interactive mining of gene expression data
#@ 10197 3214 5691
#t 2004
#c VLDB '04 Proceedings of the Thirtieth international conference on Very large data bases - Volume 30
#% 729972
#% 748010
#% 769919
#! Discovering co-expressed genes and coherent expression patterns in gene expression data is an important data analysis task in bioinformatics research and biomedical applications. Although various clustering methods have been proposed, two tough challenges still remain on how to integrate the users' domain knowledge and how to handle the high connectivity in the data. Recently, we have systematically studied the problem and proposed an effective approach [3]. In this paper, we describe a demonstration of GPX (for Gene Pattern eXplorer), an integrated environment for interactive exploration of coherent expression patterns and co-expressed genes in gene expression data. GPX integrates several novel techniques, including the coherent pattern index graph, a gene annotation panel, and a graphical interface, to adopt users' domain knowledge and support explorative operations in the clustering procedure. The GPX system as well as its techniques will be showcased, and the progress of GPX will be exemplified using several real-world gene expression data sets.

#index 1022203
#* Probabilistic skylines on uncertain data
#@ 3214 19119 10511 12151
#t 2007
#c VLDB '07 Proceedings of the 33rd international conference on Very large data bases
#% 82346
#% 288976
#% 289148
#% 321455
#% 480671
#% 654480
#% 654487
#% 800555
#% 810024
#% 824670
#% 824671
#% 824672
#% 824728
#% 849816
#% 864394
#% 864452
#% 864453
#% 875011
#% 875012
#% 875025
#% 893150
#% 993954
#% 1016201
#% 1016202
#% 1669490
#% 1688273
#% 1700131
#% 1720764
#! Uncertain data are inherent in some important applications. Although a considerable amount of research has been dedicated to modeling uncertain data and answering some types of queries on uncertain data, how to conduct advanced analysis on uncertain data remains an open problem at large. In this paper, we tackle the problem of skyline analysis on uncertain data. We propose a novel probabilistic skyline model where an uncertain object may take a probability to be in the skyline, and a p-skyline contains all the objects whose skyline probabilities are at least p. Computing probabilistic skylines on large uncertain data sets is challenging. We develop two efficient algorithms. The bottom-up algorithm computes the skyline probabilities of some selected instances of uncertain objects, and uses those instances to prune other instances and uncertain objects effectively. The top-down algorithm recursively partitions the instances of uncertain objects into subsets, and prunes subsets and objects aggressively. Our experimental results on both the real NBA player data set and the benchmark synthetic data sets show that probabilistic skylines are interesting and useful, and our two algorithms are efficient on large data sets, and complementary to each other in performance.

#index 1022247
#* Minimality attack in privacy preserving data publishing
#@ 10073 1241 1282 3214
#t 2007
#c VLDB '07 Proceedings of the 33rd international conference on Very large data bases
#% 300184
#% 576761
#% 576762
#% 577233
#% 785363
#% 800514
#% 801690
#% 810011
#% 864412
#% 874988
#% 874989
#% 881497
#% 881546
#% 881551
#% 893100
#% 951837
#% 960291
#% 963759
#% 1700134
#! Data publishing generates much concern over the protection of individual privacy. Recent studies consider cases where the adversary may possess different kinds of knowledge about the data. In this paper, we show that knowledge of the mechanism or algorithm of anonymization for data publication can also lead to extra information that assists the adversary and jeopardizes individual privacy. In particular, all known mechanisms try to minimize information loss and such an attempt provides a loophole for attacks. We call such an attack a minimality attack. In this paper, we introduce a model called m-confidentiality which deals with minimality attacks, and propose a feasible solution. Our experiments show that minimality attacks are practical concerns on real datasets and that our algorithm can prevent such attacks with very little overhead and information loss.

#index 1022276
#* Efficiently answering top-k typicality queries on large databases
#@ 18696 3214 19181 10511 1530
#t 2007
#c VLDB '07 Proceedings of the 33rd international conference on Very large data bases
#% 22982
#% 271236
#% 281750
#% 333854
#% 450489
#% 599545
#% 743388
#% 765460
#% 803119
#% 841716
#% 875023
#% 881500
#% 890349
#% 893126
#% 893127
#% 942353
#% 1728321
#! Finding typical instances is an effective approach to understand and analyze large data sets. In this paper, we apply the idea of typicality analysis from psychology and cognition science to database query answering, and study the novel problem of answering top-k typicality queries. We model typicality in large data sets systematically. To answer questions like "Who are the top-k most typical NBA players?", the measure of simple typicality is developed. To answer questions like "Who are the top-k most typical guards distinguishing guards from other players?", the notion of discriminative typicality is proposed. Computing the exact answer to a top-k typicality query requires quadratic time which is often too costly for online query answering on large databases. We develop a series of approximation methods for various situations. (1) The randomized tournament algorithm has linear complexity though it does not provide a theoretical guarantee on the quality of the answers. (2) The direct local typicality approximation using VP-trees provides an approximation quality guarantee. (3) A VP-tree can be exploited to index a large set of objects. Then, typicality queries can be answered efficiently with quality guarantees by a tournament method based on a Local Typicality Tree data structure. An extensive performance study using two real data sets and a series of synthetic data sets clearly show that top-k typicality queries are meaningful and our methods are practical.

#index 1044457
#* Anonymity for continuous data publishing
#@ 16095 1282 1241 3214
#t 2008
#c EDBT '08 Proceedings of the 11th international conference on Extending database technology: Advances in database technology
#% 576761
#% 800514
#% 801690
#% 810011
#% 844340
#% 864406
#% 864412
#% 881497
#% 881546
#% 960291
#% 975045
#% 982549
#% 1725659
#! k-anonymization is an important privacy protection mechanism in data publishing. While there has been a great deal of work in recent years, almost all considered a single static release. Such mechanisms only protect the data up to the first release or first recipient. In practical applications, data is published continuously as new data arrive; the same data may be anonymized differently for a different purpose or a different recipient. In such scenarios, even when all releases are properly k-anonymized, the anonymity of an individual may be unintentionally compromised if recipient cross-examines all the releases received or colludes with other recipients. Preventing such attacks, called correspondence attacks, faces major challenges. In this paper, we systematically characterize the correspondence attacks and propose an efficient anonymization algorithm to thwart the attacks in the model of continuous data publishing.

#index 1044491
#* OrthoCluster: a new tool for mining synteny blocks and applications in comparative genomics
#@ 19819 19820 3214 1282 19821 19822
#t 2008
#c EDBT '08 Proceedings of the 11th international conference on Extending database technology: Advances in database technology
#% 832949
#% 1796826
#! By comparing genomes among both closely and distally related species, comparative genomics analysis characterizes structures and functions of different genomes in both conserved and divergent regions. Synteny blocks, which are conserved blocks of genes on chromosomes of related species, play important roles in comparative genomics analysis. Although a few tools have been designed to identify synteny blocks, most of them cannot handle some challenging application requirements, particularly the strandedness of genes, gene inversions, gene duplications, and comparison of more than two genomes. We developed a data mining tool, Ortho-Cluster, which can handle all those challenges. It is publicly available at http://genome.sfu.ca/projects/orthocluster. OrthoCluster takes the annotated gene sets of candidate genomes and pairwise orthologous relationships as input and efficiently identifies the complete set of synteny blocks. In addition, OrthoCluster identifies four types of genome rearrangement events namely inversion, transposition, insertion/deletion, and reciprocal translocation. To be fleexible in various application scenarios, OrthoCluster comes with a systematic set of parameters such as the synteny block size, number of mismatches allowed, whether the strandedness is enforced, whether gene ordering is preserved. Furthermore, OrthoCluster can be used to identify segmental duplication in a genome. In this paper, we introduce the major technical ideas, and present some interesting findings using OrthoCluster.

#index 1063520
#* Ranking queries on uncertain data: a probabilistic threshold approach
#@ 18696 3214 19934 10511
#t 2008
#c Proceedings of the 2008 ACM SIGMOD international conference on Management of data
#% 663
#% 32879
#% 190611
#% 209725
#% 333854
#% 463114
#% 599545
#% 654487
#% 824728
#% 864394
#% 864455
#% 893167
#% 976984
#% 977013
#% 1016201
#% 1016202
#% 1022203
#% 1044478
#% 1206645
#% 1206646
#! Uncertain data is inherent in a few important applications such as environmental surveillance and mobile object tracking. Top-k queries (also known as ranking queries) are often natural and useful in analyzing uncertain data in those applications. In this paper, we study the problem of answering probabilistic threshold top-k queries on uncertain data, which computes uncertain records taking a probability of at least p to be in the top-k list where p is a user specified probability threshold. We present an efficient exact algorithm, a fast sampling algorithm, and a Poisson approximation based algorithm. An empirical study using real and synthetic data sets verifies the effectiveness of probabilistic threshold top-k queries and the efficiency of our methods.

#index 1063574
#* DiMaC: a system for cleaning disguised missing data
#@ 18696 3214
#t 2008
#c Proceedings of the 2008 ACM SIGMOD international conference on Management of data
#% 17144
#% 185079
#% 878941
#% 989667
#% 1272290
#! In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely. The very limited previous studies on cleaning disguised missing data highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers. Recently, we have studied the problem of cleaning disguised missing data systematically, and proposed an effective heuristic approach [2]. In this paper, we describe a demonstration of DiMaC, a Disguised Missing Data Cleaning system which can find the frequently used disguise values in data sets without requiring any domain background knowledge. In this demo, we will show (1) the critical techniques of finding suspicious disguise values; (2) the architecture and user interface of DiMaC system; (3) an empirical case study on both real and synthetic data sets, which verifies the effectiveness and the efficiency of the techniques; (4) some challenges arising from real applications and several direction for future work.

#index 1063595
#* Query answering techniques on uncertain and probabilistic data: tutorial summary
#@ 3214 18696 4552 10511
#t 2008
#c Proceedings of the 2008 ACM SIGMOD international conference on Management of data
#% 663
#% 32879
#% 190611
#% 209725
#% 442830
#% 480102
#% 599545
#% 654487
#% 785098
#% 824718
#% 824728
#% 824733
#% 864394
#% 864455
#% 893121
#% 893167
#% 893189
#% 907562
#% 976984
#% 977013
#% 977014
#% 983259
#% 1016201
#% 1016202
#% 1022203
#% 1022205
#% 1022206
#% 1022259
#% 1044478
#% 1063520
#% 1206645
#% 1206646
#% 1207234
#% 1720764
#! Uncertain data are inherent in some important applications, such as environmental surveillance, market analysis, and quantitative economics research. Due to the importance of those applications and the rapidly increasing amount of uncertain data collected and accumulated, analyzing large collections of uncertain data has become an important task and has attracted more and more interest from the database community. Recently, uncertain data management has become an emerging hot area in database research and development. In this tutorial, we systematically review some representative studies on answering various queries on uncertain and probabilistic data.

#index 1077041
#* Advances in information and knowledge management
#@ 20322 3214
#t 2008
#c ACM SIGIR Forum
#% 1016295
#% 1019054
#% 1019073
#% 1019102
#% 1019123
#% 1019147
#! Several research areas today overlap between the tracks of databases, information retrieval and knowledge management, such as natural language processing, semantic web, digital libraries, visualization, information quality and data mining. Inter-disciplinary research across these tracks encourages advances in the development of databases, the extraction of information and the discovery of knowledge. This is precisely the focus of our article. We explain the research issues addressed in a Ph.D. workshop recently held at the ACM Conference on Information and Knowledge Management. This workshop had presentations on novel ideas addressing challenges in information and knowledge management. It covered a broad range of topics such as XML architectures, sensor data streams, personal information managers and text pre-processing. In this article, we provide an overview of the research problems and solutions discussed in the Ph.D. workshop. Our article thus describes the latest technological developments in information and knowledge management as seen by academia. This cutting edge technology also finds practical applications in the corporate world.

#index 1083667
#* Mining preferences from superior and inferior examples
#@ 19119 3214 10511 11015 961
#t 2008
#c Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 288976
#% 300170
#% 331835
#% 376266
#% 408396
#% 420117
#% 458873
#% 465167
#% 566111
#% 577224
#% 629667
#% 729437
#% 813974
#% 992635
#% 993957
#% 994017
#% 1026877
#% 1272396
#! Mining user preferences plays a critical role in many important applications such as customer relationship management (CRM), product and service recommendation, and marketing campaigns. In this paper, we identify an interesting and practical problem of mining user preferences: in a multidimensional space where the user preferences on some categorical attributes are unknown, from some superior and inferior examples provided by a user, can we learn about the user's preferences on those categorical attributes? We model the problem systematically and show that mining user preferences from superior and inferior examples is challenging. Although the problem has great potential in practice, to the best of our knowledge, it has not been explored systematically before. As the first attempt to tackle the problem, we propose a greedy method and show that our method is practical using real data sets and synthetic data sets.

#index 1083721
#* Context-aware query suggestion by mining click-through and session data
#@ 20798 10197 3214 12631 20799 17480 4267
#t 2008
#c Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 210173
#% 232719
#% 310567
#% 330617
#% 348155
#% 459006
#% 464996
#% 479962
#% 591792
#% 754125
#% 766440
#% 783475
#% 838531
#% 869500
#% 869501
#% 963669
#% 987193
#% 987212
#% 989578
#% 1712595
#! Query suggestion plays an important role in improving the usability of search engines. Although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs, none of them are context-aware - they do not take into account the immediately preceding queries as context in query suggestion. In this paper, we propose a novel context-aware query suggestion approach which is in two steps. In the offine model-learning step, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. Then, from session data a concept sequence suffix tree is constructed as the query suggestion model. In the online query suggestion step, a user's search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence sufix tree, our approach suggests queries to the user in a context-aware manner. We test our approach on a large-scale search log of a commercial search engine containing 1:8 billion search queries, 2:6 billion clicks, and 840 million query sessions. The experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions.

#index 1083747
#* DiMaC: a disguised missing data cleaning tool
#@ 18696 3214
#t 2008
#c Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 17144
#% 185079
#% 878941
#% 989667
#% 1272290
#! In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely. The very limited previous studies on cleaning disguised missing data highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers. Recently, we have studied the problem of cleaning disguised missing data systematically, and proposed an effective heuristic approach [2]. In this paper, we present a demonstration of DiMaC, a Disguised Missing Data Cleaning tool which can find the frequently used disguise values in data sets without any domain background knowledge. In this demo, we will show (1) the critical techniques of finding suspicious disguise values; (2) the architecture and user interface of DiMaC system; (3) an empirical case study on both real and synthetic data sets, which verifies the effectiveness and the efficiency of the techniques; and (4) some challenges arising from real applications and several direction for future work.

#index 1127433
#* Efficient skyline querying with variable user preferences on nominal attributes
#@ 10073 1241 3214 21439 21440 21441
#t 2008
#c Proceedings of the VLDB Endowment
#% 287414
#% 288976
#% 289148
#% 465167
#% 480671
#% 654480
#% 800512
#% 800555
#% 806212
#% 810024
#% 824670
#% 824671
#% 824672
#% 864451
#% 875011
#% 912241
#% 915803
#% 989652
#% 992635
#% 993954
#% 1022224
#% 1022226
#% 1408811
#% 1712421
#! Current skyline evaluation techniques assume a fixed ordering on the attributes. However, dynamic preferences on nominal attributes are more realistic in known applications. In order to generate online response for any such preference issued by a user, one obvious solution is to enumerate all possible preferences and materialize all results of these preferences. However, the pre-processing and storage requirements of a full materialization are typically prohibitive. Instead, we propose a semi-materialization method called the IPO-tree Search which stores partial useful results only. With these partial results, the result of each possible preference can be returned efficiently. We have also conducted experiments to show the efficiency of our proposed algorithm.

#index 1133030
#* Mining frequent cross-graph quasi-cliques
#@ 10197 3214
#t 2009
#c ACM Transactions on Knowledge Discovery from Data (TKDD)
#% 268040
#% 324431
#% 408396
#% 466644
#% 466675
#% 469425
#% 498852
#% 577219
#% 629708
#% 729938
#% 731604
#% 731609
#% 765429
#% 769887
#% 769894
#% 769907
#% 823347
#! Joint mining of multiple datasets can often discover interesting, novel, and reliable patterns which cannot be obtained solely from any single source. For example, in bioinformatics, jointly mining multiple gene expression datasets obtained by different labs or during various biological processes may overcome the heavy noise in the data. Moreover, by joint mining of gene expression data and protein-protein interaction data, we may discover clusters of genes which show coherent expression patterns and also produce interacting proteins. Such clusters may be potential pathways. In this article, we investigate a novel data mining problem, mining frequent cross-graph quasi-cliques, which is generalized from several interesting applications in bioinformatics, cross-market customer segmentation, social network analysis, and Web mining. In a graph, a set of vertices S is a γ-quasi-clique (0 v in S directly connects to at least γ ⋅ (&verbar;S&verbar; − 1) other vertices in S. Given a set of graphs G1, …, Gn and parameter min_sup (0 min_sup ≤ 1), a set of vertices S is a frequent cross-graph quasi-clique if S is a γ-quasi-clique in at least min_sup ⋅ n graphs, and there does not exist a proper superset of S having the property. We build a general model, show why the complete set of frequent cross-graph quasi-cliques cannot be found by previous data mining methods, and study the complexity of the problem. While the problem is difficult, we develop practical algorithms which exploit several interesting and effective techniques and heuristics to efficaciously mine frequent cross-graph quasi-cliques. A systematic performance study is reported on both synthetic and real data sets. We demonstrate some interesting and meaningful frequent cross-graph quasi-cliques in bioinformatics. The experimental results also show that our algorithms are efficient and scalable.

#index 1176943
#* Publishing Sensitive Transactions for Itemset Utility
#@ 9314 12103 1282 22225 3214
#t 2008
#c ICDM '08 Proceedings of the 2008 Eighth IEEE International Conference on Data Mining
#! We consider the problem of publishing sensitive transaction data with privacy preservation. High dimensionality of transaction data poses unique challenges on data privacy and data utility. On one hand, re-identification attacks tend to use a subset of items that infrequently occur in transactions, called moles. On the other hand, data mining applications typically depend on subsets of items that frequently occur in transactions, called nuggets. Thus the problem is how to eliminate all moles while retaining nuggets as much as possible. A challenge is that moles and nuggets are multi-dimensional with exponential growth and are tangled together by shared items. We present a novel and scalable solution to this problem. The novelty lies in a compact border data structure that eliminates the need of generating all moles and nuggets.

#index 1200862
#* A brief survey on anonymization techniques for privacy preserving publishing of social network data
#@ 23020 3214 23021
#t 2008
#c ACM SIGKDD Explorations Newsletter
#% 44876
#% 215062
#% 248030
#% 271120
#% 576111
#% 576214
#% 576761
#% 629708
#% 810028
#% 864406
#% 864412
#% 874989
#% 881551
#% 893100
#% 904307
#% 956511
#% 975045
#% 983866
#% 989570
#% 1063476
#% 1127417
#% 1206582
#% 1206763
#% 1415851
#! Nowadays, partly driven by many Web 2.0 applications, more and more social network data has been made publicly available and analyzed in one way or another. Privacy preserving publishing of social network data becomes a more and more important concern. In this paper, we present a brief yet systematic review of the existing anonymization techniques for privacy preserving publishing of social network data. We identify the new challenges in privacy preserving publishing of social network data comparing to the extensively studied relational case, and examine the possible problem formulation in three important dimensions: privacy, background knowledge, and data utility. We survey the existing anonymization methods for privacy preservation in two categories: clustering-based approaches and graph modification approaches.

#index 1206645
#* Efficiently Answering Probabilistic Threshold Top-k Queries on Uncertain Data
#@ 18696 3214 19934 10511
#t 2008
#c ICDE '08 Proceedings of the 2008 IEEE 24th International Conference on Data Engineering
#! In this paper, we propose a novel type of probabilistic threshold top-k queries on uncertain data, and give an exact algorithm. More details can be found in [4].

#index 1206763
#* Preserving Privacy in Social Networks Against Neighborhood Attacks
#@ 23020 3214
#t 2008
#c ICDE '08 Proceedings of the 2008 IEEE 24th International Conference on Data Engineering
#! Recently, as more and more social network data has been published in one way or another, preserving privacy in publishing social network data becomes an important concern. With some local knowledge about individuals in a social network, an adversary may attack the privacy of some victims easily. Unfortunately, most of the previous studies on privacy preservation can deal with relational data only, and cannot be applied to social network data. In this paper, we take an initiative towards preserving privacy in social network data. We identify an essential type of privacy attacks: neighborhood attacks. If an adversary has some knowledge about the neighbors of a target victim and the relationship among the neighbors, the victim may be re-identified from a social network even if the victim's identity is preserved using the conventional anonymization techniques. We show that the problem is challenging, and present a practical solution to battle neighborhood attacks. The empirical study indicates that anonymized social networks generated by our method can still be used to answer aggregate network queries with high accuracy.

#index 1206819
#* Distance-Based Representative Skyline
#@ 4552 23239 10511 3214
#t 2009
#c ICDE '09 Proceedings of the 2009 IEEE International Conference on Data Engineering
#! Given an integer $k$, a {\em representative skyline} contains the $k$ skyline points that best describe the tradeoffs among different dimensions offered by the full skyline. Although this topic has been previously studied, the existing solution may sometimes produce $k$ points that appear in an arbitrarily tiny cluster, and therefore, fail to be representative. Motivated by this, we propose a new definition of representative skyline that minimizes the distance between a non-representative skyline point and its nearest representative. We also study algorithms for computing distance-based representative skylines. In 2D space, there is a dynamic programming algorithm that guarantees the optimal solution. For dimensionality at least 3, we prove that the problem is NP-hard, and give a 2-approximate polynomial time algorithm. Using a multidimensional access method, our algorithm can directly report the representative skyline, without retrieving the full skyline. We show that our representative skyline not only better captures the contour of the entire skyline than the previous method, but also can be computed much faster.

#index 1206852
#* Online Interval Skyline Queries on Time Series
#@ 19119 3214
#t 2009
#c ICDE '09 Proceedings of the 2009 IEEE International Conference on Data Engineering
#! In many applications, we need to analyze a large number of time series. Segments of time series demonstrating dominating advantages over others are often of particular interest. In this paper, we advocate interval skyline queries, a novel type of time series analysis queries. For a set of time series and a given time interval [i : j], an interval skyline query returns the time series which are not dominated by any other time series in the interval. We illustrate the usefulness of interval skyline queries in applications. Moreover, we develop an on-the-fly method and a view-materialization method to online answer interval skyline queries on time series. The on-the-fly method keeps the minimum and the maximum values of the time series using radix priority search trees and sketches, and computes the skyline at the query time. The view-materialization method maintains the skylines over all intervals in a compact data structure. Through theoretical analysis and extensive experiments, we show that both methods only require linear space and are efficient in query answering as well as incremental maintenance.

#index 1206882
#* Privacy Preserving Publishing on Multiple Quasi-identifiers
#@ 3214 4552 19919 14012
#t 2009
#c ICDE '09 Proceedings of the 2009 IEEE International Conference on Data Engineering
#! In some applications of privacy preserving data publishing, a practical demand is to publish a data set on multiple quasi-identifiers for multiple users simultaneously, which poses several challenges. Can we generate one anonymized version of the data so that the privacy preservation requirement like $k$-anonymity is satisfied for all users and the information loss is reduced as much as possible? In this paper, we identify and tackle the novel problem by an elegant solution.The full paper is available at http://www.cs.sfu.ca/~jpei/publications/butterfly-tr.pdf

#index 1210519
#* Mining Uncertain and Probabilistic Data: problems, Challenges, Methods, and Applications
#@ 3214 18696
#t 2008
#c Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining

#index 1211088
#* Link spam target detection using page farms
#@ 23020 3214
#t 2009
#c ACM Transactions on Knowledge Discovery from Data (TKDD)
#% 290830
#% 309749
#% 410276
#% 772018
#% 799632
#% 807297
#% 818223
#% 824694
#% 869469
#% 869471
#% 879603
#% 882035
#% 893125
#% 912202
#% 957996
#% 958004
#% 987245
#% 1016177
#% 1279489
#! Currently, most popular Web search engines adopt some link-based ranking methods such as PageRank. Driven by the huge potential benefit of improving rankings of Web pages, many tricks have been attempted to boost page rankings. The most common way, which is known as link spam, is to make up some artificially designed link structures. Detecting link spam effectively is a big challenge. In this article, we develop novel and effective detection methods for link spam target pages using page farms. The essential idea is intuitive: whether a page is the beneficiary of link spam is reflected by how it collects its PageRank score. Technically, how a target page collects its PageRank score is modeled by a page farm, which consists of pages contributing a major portion of the PageRank score of the target page. We propose two spamicity measures based on page farms. They can be used as an effective measure to check whether the pages are link spam target pages. An empirical study using a newly available real dataset strongly suggests that our method is effective. It outperforms the state-of-the-art methods like SpamRank and SpamMass in both precision and recall.

#index 1211648
#* Top-k typicality queries and efficient query answering methods on large databases
#@ 18696 3214 23718 10511 1530
#t 2009
#c The VLDB Journal — The International Journal on Very Large Data Bases
#% 22982
#% 210173
#% 248790
#% 248792
#% 271130
#% 271236
#% 273890
#% 281750
#% 294634
#% 333854
#% 438137
#% 443531
#% 450489
#% 566128
#% 599545
#% 631988
#% 743388
#% 765460
#% 781774
#% 803119
#% 841716
#% 875023
#% 881500
#% 893126
#% 893127
#% 942353
#% 1022276
#% 1375422
#% 1861495
#! Finding typical instances is an effective approach to understand and analyze large data sets. In this paper, we apply the idea of typicality analysis from psychology and cognitive science to database query answering, and study the novel problem of answering top-k typicality queries. We model typicality in large data sets systematically. Three types of top-k typicality queries are formulated. To answer questions like "Who are the top-k most typical NBA players?", the measure of simple typicality is developed. To answer questions like "Who are the top-k most typical guards distinguishing guards from other players?", the notion of discriminative typicality is proposed. Moreover, to answer questions like "Who are the best k typical guards in whole representing different types of guards?", the notion of representative typicality is used. Computing the exact answer to a top-k typicality query requires quadratic time which is often too costly for online query answering on large databases. We develop a series of approximation methods for various situations: (1) the randomized tournament algorithm has linear complexity though it does not provide a theoretical guarantee on the quality of the answers; (2) the direct local typicality approximation using VP-trees provides an approximation quality guarantee; (3) a local typicality tree data structure can be exploited to index a large set of objects. Then, typicality queries can be answered efficiently with quality guarantees by a tournament method based on a Local Typicality Tree. An extensive performance study using two real data sets and a series of synthetic data sets clearly shows that top-k typicality queries are meaningful and our methods are practical.

#index 1214756
#* Can we learn a template-independent wrapper for news article extraction from a single training site?
#@ 20223 17347 20222 3214 18434 20221 18538
#t 2009
#c Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 271065
#% 275915
#% 312860
#% 334313
#% 397605
#% 480824
#% 577321
#% 654469
#% 754078
#% 754108
#% 779889
#% 805845
#% 805846
#% 818233
#% 869518
#% 938578
#% 956642
#% 989660
#% 1269910
#! Automatic news extraction from news pages is important in many Web applications such as news aggregation. However, the existing news extraction methods based on template-level wrapper induction have three serious limitations. First, the existing methods cannot correctly extract pages belonging to an unseen template. Second, it is costly to maintain up-to-date wrappers for a large amount of news websites, because any change of a template may invalidate the corresponding wrapper. Last, the existing methods can merely extract unformatted plain texts, and thus are not user friendly. In this paper, we tackle the problem of template-independent Web news extraction in a user-friendly way. We formalize Web news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site. Novel features dedicated to news titles and bodies are developed. Correlations between news titles and news bodies are exploited. Our template-independent wrapper can extract news pages from different sites regardless of templates. Moreover, our approach can extract not only texts, but also images and animates within the news bodies and the extracted news articles are in the same visual style as in the original pages. In our experiments, a wrapper learned from 40 pages from a single news site achieved an accuracy of 98.1% on 3,973 news pages from 12 news sites.

#index 1214761
#* OLAP on search logs: an infrastructure supporting data-driven applications in search engines
#@ 23020 10197 3214 4267
#t 2009
#c Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 235941
#% 348155
#% 459006
#% 577224
#% 591792
#% 818226
#% 838531
#% 869501
#% 869536
#% 879565
#% 879567
#% 939629
#% 956546
#% 956552
#% 960303
#% 963669
#% 987203
#% 987212
#% 1055677
#% 1063518
#% 1074092
#% 1083721
#! Search logs, which contain rich and up-to-date information about users' needs and preferences, have become a critical data source for search engines. Recently, more and more data-driven applications are being developed in search engines based on search logs, such as query suggestion, keyword bidding, and dissatisfactory query analysis. In this paper, by observing that many data-driven applications in search engines highly rely on online mining of search logs, we develop an OLAP system on search logs which serves as an infrastructure supporting various data-driven applications. An empirical study using real data of over two billion query sessions demonstrates the usefulness and feasibility of our design.

#index 1217254
#* MobileMiner: a real world case study of data mining in mobile communication
#@ 4611 24029 4614 4613 4612 24220 24221 3214
#t 2009
#c Proceedings of the 2009 ACM SIGMOD International Conference on Management of data
#% 469422
#% 823347
#% 1023265
#% 1176867
#! Mobile communication data analysis has been often used as a background application to motivate many data mining problems. However, very few data mining researchers have a chance to see a working data mining system on real mobile communication data. In this demo, we showcase our new system MobileMiner on a real mobile communication data set, which presents a case study of business solutions using state-of-the-art data mining techniques. MobileMiner adaptively profiles users' behavior from their calling and moving record streams. Customer segmentation and social community analysis can be conducted based on user profiles. We show how data mining techniques can help in mobile communication data analysis. Moreover, we also show some interesting observations which still cannot be mined by the current techniques, and thus may motivate new research and development.

#index 1250571
#* Minimum description length principle: generators are preferable to closed patterns
#@ 2420 12902 66 3214 171
#t 2006
#c AAAI'06 Proceedings of the 21st national conference on Artificial intelligence - Volume 1
#% 152934
#% 234979
#% 300120
#% 314836
#% 338594
#% 431033
#% 487998
#% 722493
#% 729933
#% 729984
#% 798763
#% 824931
#! The generators and the unique closed pattern of an equivalence class of itemsets share a common set of transactions. The generators are the minimal ones among the equivalent itemsets, while the closed pattern is the maximum one. As a generator is usually smaller than the closed pattern in cardinality, by the Minimum Description Length Principle, the generator is preferable to the closed pattern in inductive inference and classification. To efficiently discover frequent generators from a large dataset, we develop a depth-first algorithm called Gr-growth. The idea is novel in contrast to traditional breadth-first bottom-up generator-mining algorithms. Our extensive performance study shows that Gr-growth is significantly faster (an order or even two orders of magnitudes when the support thresholds are low) than the existing generator mining algorithms. It can be also faster than the state-of-the-art frequent closed itemset mining algorithms such as FPclose and CLOSET+.

#index 1261978
#* Proceedings of the 1st ACM SIGKDD Workshop on Knowledge Discovery from Uncertain Data
#@ 3214 3892 12140
#t 2009
#c The 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
#! The importance of uncertain data is growing quickly in many essential applications such as environmental surveillance, mobile object tracking and data integration. Recently, storing, collecting, processing, and analyzing uncertain data has attracted increasing attention from both academia and industry. Analyzing and mining uncertain data needs collaboration and joint effort from multiple research communities including reasoning under uncertainty, uncertain databases and mining uncertain data. For example, statistics and probabilistic reasoning can provide support with models for representing uncertainty. The uncertain database community can provide methods for storing and managing uncertain data, while research in mining uncertain data can provide data analysis tasks and methods. It is important to build connections among those communities to tackle the overall problem of analyzing and mining uncertain data. There are many common challenges among the communities. One is to understand the different modeling assumptions made, and how they impact the methods, both in terms of accuracy and efficiency. Different researchers hold different assumptions and this is one of the major obstacles in the research of mining uncertain data. Another is the scalability of proposed management and analysis methods. Finally, to make analysis and mining useful and practical, we need real data sets for testing. Unfortunately, uncertain data sets are often hard to get. The goal of the First ACM SIGKDD Workshop on Knowledge Discovery from Uncertain Data (U'09) is to discuss in depth the challenges, opportunities and techniques on the topic of analyzing and mining uncertain data. The theme of this workshop is to make connections among the research areas of uncertain databases, probabilistic reasoning, and data mining, as well as to build bridges among the aspects of models, data, applications, novel mining tasks and effective solutions. By making connections among different communities, we aim at understanding each other in terms of scientific foundation as well as commonality and differences in research methodology. The workshop program is very stimulating and exciting. We are pleased to feature two invited talks by pioneers in mining uncertain data. Christopher Jermaine will give an invited talk titled "Managing and Mining Uncertain Data: What Might We Do Better?" Matthias Renz will address the topic "Querying and Mining Uncertain Data: Methods, Applications, and Challenges". Moreover, 8 accepted papers in 4 full presentations and 4 concise presentations will cover a bunch of interesting topics and on-going research projects about uncertain data mining.

#index 1267796
#* Debt Detection in Social Security by Sequence Classification Using Both Positive and Negative Patterns
#@ 19752 22935 25486 3214 19272 5606 22936
#t 2009
#c ECML PKDD '09 Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II
#% 280441
#% 280488
#% 280515
#% 310559
#% 329537
#% 459006
#% 463903
#% 464996
#% 466483
#% 471264
#% 577249
#% 577256
#% 844035
#% 844344
#% 844814
#% 1040761
#% 1081949
#% 1117080
#% 1155719
#% 1196009
#% 1390144
#! Debt detection is important for improving payment accuracy in social security. Since debt detection from customer transactional data can be generally modelled as a fraud detection problem, a straightforward solution is to extract features from transaction sequences and build a sequence classifier for debts. The existing sequence classification methods based on sequential patterns consider only positive patterns. However, according to our experience in a large social security application, negative patterns are very useful in accurate debt detection. In this paper, we present a successful case study of debt detection in a large social security application. The central technique is building sequence classification using both positive and negative sequential patterns.

#index 1305497
#* Early prediction on time series: a nearest neighbor approach
#@ 18273 3214 850
#t 2009
#c IJCAI'09 Proceedings of the 21st international jont conference on Artifical intelligence
#% 280488
#% 577221
#% 737337
#% 876074
#% 881545
#% 1673559
#! In this paper, we formulate the problem of early classification of time series data, which is important in some time-sensitive applications such as health-informatics. We introduce a novel concept of MPL (Minimum Prediction Length) and develop ECTS (Early Classification on Time Series), an effective 1-nearest neighbor classification method. ECTS makes early predictions and at the same time retains the accuracy comparable to that of a 1NN classifier using the full-length time series. Our empirical study using benchmark time series data sets shows that ECTS works well on the real data sets where 1NN classification is effective.

#index 1400780
#* Threshold-based probabilistic top-k dominating queries
#@ 19934 10511 10874 3214 1204
#t 2010
#c The VLDB Journal — The International Journal on Very Large Data Bases
#% 663
#% 4430
#% 32879
#% 152937
#% 235023
#% 273685
#% 410276
#% 427199
#% 442830
#% 480671
#% 527189
#% 643566
#% 654487
#% 806212
#% 823402
#% 824728
#% 864394
#% 875012
#% 893189
#% 915307
#% 976984
#% 1016201
#% 1016202
#% 1022203
#% 1022242
#% 1063520
#% 1127377
#% 1147662
#% 1206716
#% 1224602
#% 1408794
#% 1669490
#! Recently, due to intrinsic characteristics in many underlying data sets, a number of probabilistic queries on uncertain data have been investigated. Top-k dominating queries are very important in many applications including decision making in a multidimensional space. In this paper, we study the problem of efficiently computing top-k dominating queries on uncertain data. We first formally define the problem. Then, we develop an efficient, threshold-based algorithm to compute the exact solution. To overcome some inherent computational deficiency in an exact computation, we develop an efficient randomized algorithm with an accuracy guarantee. Our extensive experiments demonstrate that both algorithms are quite efficient, while the randomized algorithm is quite scalable against data set sizes, object areas, k values, etc. The randomized algorithm is also highly accurate in practice.

#index 1426555
#* Logging every footstep: quantile summaries for the entire history
#@ 4552 10711 24147 3214 10483
#t 2010
#c Proceedings of the 2010 ACM SIGMOD International Conference on Management of data
#% 56081
#% 58371
#% 248820
#% 273907
#% 287070
#% 333931
#% 453493
#% 479473
#% 480817
#% 482104
#% 571296
#% 763997
#% 800494
#% 801696
#% 816392
#% 993969
#% 1054486
#% 1373450
#! Quantiles are a crucial type of order statistics in databases. Extensive research has been focused on maintaining a space-efficient structure for approximate quantile computation as the underlying dataset is updated. The existing solutions, however, are designed to support only the current, most-updated, snapshot of the dataset. Queries on the past versions of the data cannot be answered. This paper studies the problem of historical quantile search. The objective is to enable ε-approximate quantile retrieval on any snapshot of the dataset in history. The problem is very important in analyzing the evolution of a distribution, monitoring the quality of services, query optimization in temporal databases, and so on. We present the first formal results in the literature. First, we prove a novel theoretical lower bound on the space cost of supporting ε-approximate historical quantile queries. The bound reveals the fundamental difference between answering quantile queries about the past and those about the present time. Second, we propose a structure for finding ε-approximate historical quantiles, and show that it consumes more space than the lower bound by only a square-logarithmic factor. Extensive experiments demonstrate that in practice our technique performs much better than predicted by theory. In particular, the quantiles it returns are remarkably more accurate than the theoretical precision guarantee.

#index 1428417
#* Summary of the first ACM SIGKDD workshop on knowledge discovery from uncertain data (U'09)
#@ 3214 3892 12140
#t 2010
#c ACM SIGKDD Explorations Newsletter

#index 1450885
#* Context-aware ranking in web search
#@ 31231 10197 3214 31232 17480 4267
#t 2010
#c Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval
#% 284796
#% 577224
#% 818207
#% 818221
#% 818259
#% 853543
#% 869501
#% 869536
#% 881540
#% 956552
#% 987211
#% 987212
#% 1083721
#% 1166517
#% 1190074
#% 1227577
#! The context of a search query often provides a search engine meaningful hints for answering the current query better. Previous studies on context-aware search were either focused on the development of context models or limited to a relatively small scale investigation under a controlled laboratory setting. Particularly, about context-aware ranking for Web search, the following two critical problems are largely remained unsolved. First, how can we take advantage of different types of contexts in ranking? Second, how can we integrate context information into a ranking model? In this paper, we tackle the above two essential problems analytically and empirically. We develop different ranking principles for different types of contexts. Moreover, we adopt a learning-to-rank approach and integrate the ranking principles into a state-of-the-art ranking model by encoding the context information as features of the model. We empirically test our approach using a large search log data set obtained from a major commercial search engine. Our evaluation uses both human judgments and implicit user click data. The experimental results clearly show that our context-aware ranking approach improves the ranking of a commercial search engine which ignores context information. Furthermore, our method outperforms a baseline method which considers context information in ranking.

#index 1451032
#* Search and browse log mining for web information retrieval: challenges, methods, and applications
#@ 10197 3214 4267
#t 2010
#c Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval
#! Huge amounts of search log data have been accumulated in various search engines. Currently, a commercial search engine receives billions of queries and collects tera-bytes of log data on any single day. Other than search log data, browse logs can be collected by client-side browser plug-ins, which record the browse information if users' permissions are granted. Such massive amounts of search/browse log data, on the one hand, provide great opportunities to mine the wisdom of crowds and improve search results as well as online advertisement. On the other hand, designing effective and efficient methods to clean, model, and process large scale log data also presents great challenges. In this tutorial, we focus on mining search and browse log data for Web information retrieval. We consider a Web information retrieval system consisting of four components, namely, query understanding, document understanding, query-document matching, and user understanding. Accordingly, we organize the tutorial materials along these four aspects. For each aspect, we will survey the major tasks, challenges, fundamental principles, and state-of-the-art methods. The goal of this tutorial is to provide a systematic survey on large-scale search/browse log mining to the IR community. It will help IR researchers to get familiar with the core challenges and promising directions in log mining. At the same time, this tutorial may also serve the developers of Web information retrieval systems as a comprehensive and in-depth reference to the advanced log mining techniques.

#index 1451193
#* Neighbor query friendly compression of social networks
#@ 31441 3214
#t 2010
#c Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 311808
#% 399764
#% 410276
#% 656242
#% 656281
#% 656282
#% 740507
#% 754117
#% 1035579
#% 1214643
#! Compressing social networks can substantially facilitate mining and advanced analysis of large social networks. Preferably, social networks should be compressed in a way that they still can be queried efficiently without decompression. Arguably, neighbor queries, which search for all neighbors of a query vertex, are the most essential operations on social networks. Can we compress social networks effectively in a neighbor query friendly manner, that is, neighbor queries still can be answered in sublinear time using the compression? In this paper, we develop an effective social network compression approach achieved by a novel Eulerian data structure using multi-position linearizations of directed graphs. Our method comes with a nontrivial theoretical bound on the compression rate. To the best of our knowledge, our approach is the first that can answer both out-neighbor and in-neighbor queries in sublinear time. An extensive empirical study on more than a dozen benchmark real data sets verifies our design.

#index 1490606
#* A brief survey on sequence classification
#@ 18273 3214 3861
#t 2010
#c ACM SIGKDD Explorations Newsletter
#% 235377
#% 266284
#% 280488
#% 289519
#% 310545
#% 311027
#% 344447
#% 420132
#% 451055
#% 458369
#% 464434
#% 466501
#% 501994
#% 577221
#% 577227
#% 722803
#% 729953
#% 788670
#% 793247
#% 799394
#% 818236
#% 832727
#% 840941
#% 844035
#% 844344
#% 864131
#% 875991
#% 876074
#% 881545
#% 881823
#% 902457
#% 908995
#% 951838
#% 992857
#% 998747
#% 1044404
#% 1103307
#% 1126334
#% 1127609
#% 1173693
#% 1214716
#% 1305497
#% 1440242
#% 1661589
#% 1717571
#! Sequence classification has a broad range of applications such as genomic analysis, information retrieval, health informatics, finance, and abnormal detection. Different from the classification task on feature vectors, sequences do not have explicit features. Even with sophisticated feature selection techniques, the dimensionality of potential features may still be very high and the sequential nature of features is difficult to capture. This makes sequence classification a more challenging task than classification on feature vectors. In this paper, we present a brief review of the existing work on sequence classification. We summarize the sequence classification in terms of methodologies and application domains. We also provide a review on several extensions of the sequence classification problem, such as early classification on sequences and semi-supervised learning on sequences.

#index 1523870
#* Computing closed skycubes
#@ 21165 3214 32937
#t 2010
#c Proceedings of the VLDB Endowment
#% 288976
#% 289148
#% 384416
#% 464873
#% 465167
#% 480671
#% 729933
#% 824671
#% 824672
#% 864452
#% 875011
#% 912241
#! In this paper, we tackle the problem of efficient skycube computation. We introduce a novel approach significantly reducing domination tests for a given subspace and the number of subspaces searched. Technically, we identify two types of skyline points that can be directly derived without using any domination tests. Moreover, based on formal concept analysis, we introduce two closure operators that enable a concise representation of skyline cubes. We show that this concise representation is easy to compute and develop an efficient algorithm, which only needs to search a small portion of the huge search space. We show with empirical results the merits of our approach.

#index 1535481
#* Probabilistic Inference Protection on Anonymized Data
#@ 10073 1241 1282 9314 3214 850
#t 2010
#c ICDM '10 Proceedings of the 2010 IEEE International Conference on Data Mining
#! Background knowledge is an important factor in privacy preserving data publishing. Probabilistic distribution-based background knowledge is a powerful kind of background knowledge which is easily accessible to adversaries. However, to the best of our knowledge, there is no existing work that can provide a privacy guarantee under adversary attack with such background knowledge. The difficulty of the problem lies in the high complexity of the probability computation and the non-monotone nature of the privacy condition. The only solution known to us relies on approximate algorithms with no known error bound. In this paper, we propose a new bounding condition that overcomes the difficulties of the problem and gives a privacy guarantee. This condition is based on probability deviations in the anonymized data groups, which is much easier to compute and which is a monotone function on the grouping sizes.

#index 1538418
#* Ranking queries on uncertain data
#@ 18696 3214 10511
#t 2011
#c The VLDB Journal — The International Journal on Very Large Data Bases
#% 663
#% 32879
#% 190611
#% 209725
#% 333854
#% 453464
#% 463114
#% 599545
#% 654487
#% 810098
#% 824728
#% 864394
#% 864455
#% 875023
#% 893126
#% 893167
#% 976984
#% 977013
#% 983259
#% 1016201
#% 1016202
#% 1022203
#% 1044478
#% 1063520
#% 1147662
#% 1206645
#% 1206646
#% 1206717
#% 1206893
#% 1217141
#% 1217174
#% 1229788
#! Uncertain data is inherent in a few important applications. It is far from trivial to extend ranking queries (also known as top-k queries), a popular type of queries on certain data, to uncertain data. In this paper, we cast ranking queries on uncertain data using three parameters: rank threshold k, probability threshold p, and answer set size threshold l. Systematically, we identify four types of ranking queries on uncertain data. First, a probability threshold top-k query computes the uncertain records taking a probability of at least p to be in the top-k list. Second, a top-(k, l) query returns the top-l uncertain records whose probabilities of being ranked among top-k are the largest. Third, the p-rank of an uncertain record is the smallest number k such that the record takes a probability of at least p to be ranked in the top-k list. A rank threshold top-k query retrieves the records whose p-ranks are at most k. Last, a top-(p, l) query returns the top-l uncertain records with the smallest p-ranks. To answer such ranking queries, we present an efficient exact algorithm, a fast sampling algorithm, and a Poisson approximation-based algorithm. To answer top-(k, l) queries and top-(p, l) queries, we propose PRist+, a compact index. An efficient index construction algorithm and efficacious query answering methods are developed for PRist+. An empirical study using real and synthetic data sets verifies the effectiveness of the probabilistic ranking queries and the efficiency of our methods.

#index 1581881
#* On k-skip shortest paths
#@ 4552 24147 3214
#t 2011
#c Proceedings of the 2011 ACM SIGMOD International Conference on Management of data
#% 31486
#% 324484
#% 410276
#% 442858
#% 443208
#% 443533
#% 450545
#% 813718
#% 839701
#% 841649
#% 985951
#% 1063472
#% 1181255
#% 1211643
#% 1292553
#% 1328210
#% 1412885
#% 1426510
#% 1482228
#% 1484129
#% 1523971
#% 1676469
#! Given two vertices s, t in a graph, let P be the shortest path (SP) from s to t, and P* a subset of the vertices in P. P* is a k-skip shortest path from s to t, if it includes at least a vertex out of every k consecutive vertices in P. In general, P* succinctly describes P by sampling the vertices in P with a rate of at least 1/k. This makes P* a natural substitute in scenarios where reporting every single vertex of P is unnecessary or even undesired. This paper studies k-skip SP computation in the context of spatial network databases (SNDB). Our technique has two properties crucial for real-time query processing in SNDB. First, our solution is able to answer k-skip queries significantly faster than finding the original SPs in their entirety. Second, the previous objective is achieved with a structure that occupies less space than storing the underlying road network. The proposed algorithms are the outcome of a careful theoretical analysis that reveals valuable insight into the characteristics of the k-skip SP problem. Their efficiency has been confirmed by extensive experiments with real data.

#index 1581951
#* Privacy-aware data management in information networks
#@ 10208 11746 4346 3214 15612
#t 2011
#c Proceedings of the 2011 ACM SIGMOD International Conference on Management of data
#% 956511
#% 963241
#% 1063476
#% 1127417
#% 1190107
#% 1190108
#% 1190207
#% 1195950
#% 1200862
#% 1206763
#% 1217125
#% 1259854
#% 1318624
#% 1318653
#% 1328188
#% 1366214
#% 1399968
#% 1400043
#% 1415851
#% 1426540
#% 1475160
#% 1478165
#% 1523886
#% 1524388
#% 1535288
#% 1581409
#% 1581862
#% 1740518
#! The proliferation of information networks, as a means of sharing information, has raised privacy concerns for enterprises who manage such networks and for individual users that participate in such networks. For enterprises, the main challenge is to satisfy two competing goals: releasing network data for useful data analysis and also preserving the identities or sensitive relationships of the individuals participating in the network. Individual users, on the other hand, require personalized methods that increase their awareness of the visibility of their private information. This tutorial provides a systematic survey of the problems and state-of-the-art methods related to both enterprise and personalized privacy in information networks. The tutorial discusses privacy threats, privacy attacks, and privacy-preserving mechanisms tailored specifically to network data.

#index 1584349
#* Can the Utility of Anonymized Data be Used for Privacy Breaches?
#@ 10073 1241 1282 850 3214
#t 2011
#c ACM Transactions on Knowledge Discovery from Data (TKDD)
#% 663
#% 227919
#% 481779
#% 576761
#% 785363
#% 800514
#% 810011
#% 824733
#% 864412
#% 864665
#% 874988
#% 881507
#% 881546
#% 893100
#% 960291
#% 1022205
#% 1022247
#% 1083631
#% 1083653
#% 1206714
#% 1206716
#% 1206745
#% 1206814
#% 1206815
#% 1217156
#% 1535481
#% 1700134
#! Group based anonymization is the most widely studied approach for privacy-preserving data publishing. Privacy models/definitions using group based anonymization includes k-anonymity, l-diversity, and t-closeness, to name a few. The goal of this article is to raise a fundamental issue regarding the privacy exposure of the approaches using group based anonymization. This has been overlooked in the past. The group based anonymization approach by bucketization basically hides each individual record behind a group to preserve data privacy. If not properly anonymized, patterns can actually be derived from the published data and be used by an adversary to breach individual privacy. For example, from the medical records released, if patterns such as that people from certain countries rarely suffer from some disease can be derived, then the information can be used to imply linkage of other people in an anonymized group with this disease with higher likelihood. We call the derived patterns from the published data the foreground knowledge. This is in contrast to the background knowledge that the adversary may obtain from other channels, as studied in some previous work. Finally, our experimental results show such an attack is realistic in the privacy benchmark dataset under the traditional group based anonymization approach.

#index 1594654
#* Outlier detection on uncertain data: Objects, instances, and inferences
#@ 19119 3214
#t 2011
#c ICDE '11 Proceedings of the 2011 IEEE 27th International Conference on Data Engineering
#! This paper studies the problem of outlier detection on uncertain data. We start with a comprehensive model considering both uncertain objects and their instances. An uncertain object has some inherent attributes and consists of a set of instances which are modeled by a probability density distribution. We detect outliers at both the instance level and the object level. To detect outlier instances, it is a prerequisite to know normal instances. By assuming that uncertain objects with similar properties tend to have similar instances, we learn the normal instances for each uncertain object using the instances of objects with similar properties. Consequently, outlier instances can be detected by comparing against normal ones. Furthermore, we can detect outlier objects most of whose instances are outliers. Technically, we use a Bayesian inference algorithm to solve the problem, and develop an approximation algorithm and a filtering algorithm to speed up the computation. An extensive empirical study on both real data and synthetic data verifies the effectiveness and efficiency of our algorithms.

#index 1598548
#* Enhancing web search by mining search and browse logs
#@ 10197 3214 4267
#t 2011
#c Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval
#! Huge amounts of search log data have been accumulated in various search engines. Currently, a commercial search engine receives billions of queries and collects tera-bytes of log data on any single day. Other than search log data, browse logs can be collected by client-side browser plug-ins, which record the browse information if users' permissions are granted. Such massive amounts of search/browse log data, on the one hand, provide great opportunities to mine the wisdom of crowds and improve web search results. On the other hand, designing effective and efficient methods to clean, model, and process large scale log data also presents great challenges. In this tutorial, we will focus on mining search and browse log data for search engines. We will start with an introduction of search and browse log data and an overview of frequently-used data summarization in log mining. We will then elaborate how log mining applications enhance the five major components of a search engine, namely, query understanding, document understanding, query-document matching, user understanding, and monitoring and feedbacks. For each aspect, we will survey the major tasks, fundamental principles, and state-of-the-art methods. Finally, we will discuss the challenges and future trends of log data mining. The goal of this tutorial is to provide a systematic survey on large-scale search/browse log mining to the IR community. It may help IR researchers to get familiar with the core challenges and promising directions in log mining. At the same time, this tutorial may also serve the developers of web information retrieval systems as a comprehensive and in-depth reference to the advanced log mining techniques.

#index 1606082
#* Towards bounding sequential patterns
#@ 21165 3214
#t 2011
#c Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining
#% 152934
#% 329537
#% 459006
#% 463903
#% 464996
#% 466487
#% 477791
#% 576118
#% 577256
#% 589384
#% 763551
#% 789589
#% 805094
#% 839173
#% 864470
#% 925598
#% 949146
#% 1035590
#% 1063518
#% 1172640
#% 1214761
#% 1440242
#% 1738861
#! Given a sequence database, can we have a non-trivial upper bound on the number of sequential patterns? The problem of bounding sequential patterns is very challenging in theory due to the combinatorial complexity of sequences, even given some inspiring results on bounding itemsets in frequent itemset mining. Moreover, the problem is highly meaningful in practice, since the upper bound can be used in many applications such as space allocation in building sequence data warehouses. In this paper, we tackle the problem of bounding sequential patterns by presenting, for the first time in the field of sequential pattern mining, strong combinatorial results on computing the number of possible sequential patterns that can be generated at a given length k. We introduce, as a case study, two novel techniques to estimate the number of candidate sequences. An extensive empirical study on both real data and synthetic data verifies the effectiveness of our methods.

#index 1606342
#* On pruning for top-k ranking in uncertain databases
#@ 36205 26883 26069 8144 3214
#t 2011
#c Proceedings of the VLDB Endowment
#% 663
#% 480418
#% 810020
#% 864455
#% 1022203
#% 1063520
#% 1147662
#% 1206893
#% 1217141
#% 1565405
#! Top-k ranking for an uncertain database is to rank tuples in it so that the best k of them can be determined. The problem has been formalized under the unified approach based on parameterized ranking functions (PRFs) and the possible world semantics. Given a PRF, one can always compute the ranking function values of all the tuples to determine the top-k tuples, which is a formidable task for large databases. In this paper, we present a general approach to pruning for the framework based on PRFs. We show a mathematical manipulation of possible worlds which reveals key insights in the part of computation that may be pruned and how to achieve it in a systematic fashion. This leads to concrete pruning methods for a wide range of ranking functions. We show experimentally the effectiveness of our approach.

#index 1673620
#* A random method for quantifying changing distributions in data streams
#@ 4558 3214
#t 2005
#c PKDD'05 Proceedings of the 9th European conference on Principles and Practice of Knowledge Discovery in Databases
#% 115608
#% 342600
#% 400847
#% 424997
#% 443616
#% 481460
#% 654489
#% 729932
#% 729980
#! In applications such as fraud and intrusion detection, it is of great interest to measure the evolving trends in the data. We consider the problem of quantifying changes between two datasets with class labels. Traditionally, changes are often measured by first estimating the probability distributions of the given data, and then computing the distance, for instance, the K-L divergence, between the estimated distributions. However, this approach is computationally infeasible for large, high dimensional datasets. The problem becomes more challenging in the streaming data environment, as the high speed makes it difficult for the learning process to keep up with the concept drifts in the data. To tackle this problem, we propose a method to quantify concept drifts using a universal model that incurs minimal learning cost. In addition, our model also provides the ability of performing classification.

#index 1846803
#* Random Error Reduction in Similarity Search on Time Series: A Statistical Approach
#@ 41552 11809 3214
#t 2012
#c ICDE '12 Proceedings of the 2012 IEEE 28th International Conference on Data Engineering
#! Errors in measurement can be categorized into two types: systematic errors that are predictable, and random errors that are inherently unpredictable and have null expected value. Random error is always present in a measurement. More often than not, readings in time series may contain inherent random errors due to causes like dynamic error, drift, noise, hysteresis, digitalization error and limited sampling frequency. Random errors may affect the quality of time series analysis substantially. Unfortunately, most of the existing time series mining and analysis methods, such as similarity search, clustering, and classification tasks, do not address random errors, possibly because random error in a time series, which can be modeled as a random variable of unknown distribution, is hard to handle. In this paper, we tackle this challenging problem. Taking similarity search as an example, which is an essential task in time series analysis, we develop MISQ, a statistical approach for random error reduction in time series analysis. The major intuition in our method is to use only the readings at different time instants in a time series to reduce random errors. We achieve a highly desirable property in MISQ: it can ensure that the recall is above a user-specified threshold. An extensive empirical study on 20 benchmark real data sets clearly shows that our method can lead to better performance than the baseline method without random error reduction in real applications such as classification. Moreover, MISQ achieves good quality in similarity search.

#index 1872224
#* Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ 4278 3996 3214
#t 2012
#c The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
#! The KDD conference has seen remarkable growth since its origins as an IJCAI workshop in Detroit in 1989, evolving into a full-fledged research conference in 1995, underscoring the important role data mining as a field has played in extracting knowledge and actionable insights from vast troves of data that is being generated in the digital world around us. This year we received a record 755 submissions to the research program, from which 133 papers were accepted, for an aggregate acceptance rate of 17.6% (quite similar to recent years). Among the academic conferences, the KDD conference has typically more of an emphasis on research motivated by real-world applications. It is important to keep in mind that it is this synergy of research in areas like algorithms, computational geometry, database, graph theory, machine learning, natural language processing, statistics, visualization and many others when applied to problems arising in diverse fields such as web, medicine, climatology, marketing that drives our field forward, makes it vibrant and fun - who would know that ideas in computational geometry can be adapted to construct fast algorithms to improve online advertising and movie recommendations? The breadth of topics covered in this year's research program is truly comprehensive, including social networks, privacy, text mining, predictive modeling, time-series forecasting, spatial data analysis, geometry, and more. We are very fortunate to have 4 world-class keynote speakers this year spanning industry and academia, providing inspirational talks on cutting-edge techniques and issues in web mining, information networks, statistical inference for big data, and social computing. The process of whittling down the initial 734 submissions to the final set of 133 accepted papers required the coordination and time of a large number of willing volunteers. The program committee (PC) consisted of over 350 reviewers (PC members) and 50 senior PC members. In the first phase each submitted paper was automatically assigned to 3 reviewers (after a bidding process). Once the reviews from each of the 3 reviewers were completed, the program chairs rejected papers that did not receive much support from any of the reviewers. We rejected 259 papers at this stage. Special care was taken to minimize the error of rejecting a potentially good paper at this stage. The papers that survived the first phase were assigned to the senior PC members based on their bids, they had the option of initiating a discussion for any of their papers, e.g., if there was significant divergence in scores among reviewers, or if a paper was on the borderline of being accepted. Following the discussion phase, the senior PC members provided a recommendation score and a detailed meta-review for each paper. In the final phase, we (the program chairs) analyzed all of this information, starting with the obvious accept and reject decisions, and then gradually focusing in more detail on the papers near the borderline, seeking additional reviews and input from the PC and senior PC members where appropriate. We also initiated a shepherding phase with 15 papers having the opportunity of fixing mild issues we thought would be possible to address before they can be accepted. 13 of them were accepted after thorough revisions. Finally, it is quite likely that in hindsight some worthy papers may have been rejected as part of this process - these errors are an unfortunate reality of modern computer science conferences, and hard to avoid when a very large number of decisions have to be made over a short time span based on a subjective reviewing process. Nevertheless, we, the PC chairs, are responsible for those unfortunate errors and welcome suggestions on the matter.

#index 1879023
#* Mining query subtopics from search log data
#@ 13697 35541 4267 10197 3214 35540
#t 2012
#c SIGIR '12 Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval
#% 297550
#% 310567
#% 330617
#% 401405
#% 577224
#% 590523
#% 766433
#% 817846
#% 838469
#% 853542
#% 879567
#% 879639
#% 987203
#% 987221
#% 987222
#% 987223
#% 987326
#% 1034802
#% 1083721
#% 1130878
#% 1190055
#% 1202162
#% 1213625
#% 1227619
#% 1400099
#% 1536529
#% 1537503
#! Most queries in web search are ambiguous and multifaceted. Identifying the major senses and facets of queries from search log data, referred to as query subtopic mining in this paper, is a very important issue in web search. Through search log analysis, we show that there are two interesting phenomena of user behavior that can be leveraged to identify query subtopics, referred to as `one subtopic per search' and `subtopic clarification by keyword'. One subtopic per search means that if a user clicks multiple URLs in one query, then the clicked URLs tend to represent the same sense or facet. Subtopic clarification by keyword means that users often add an additional keyword or keywords to expand the query in order to clarify their search intent. Thus, the keywords tend to be indicative of the sense or facet. We propose a clustering algorithm that can effectively leverage the two phenomena to automatically mine the major subtopics of queries, where each subtopic is represented by a cluster containing a number of URLs and keywords. The mined subtopics of queries can be used in multiple tasks in web search and we evaluate them in aspects of the search result presentation such as clustering and re-ranking. We demonstrate that our clustering algorithm can effectively mine query subtopics with an F1 measure in the range of 0.896-0.956. Our experimental results show that the use of the subtopics mined by our approach can significantly improve the state-of-the-art methods used for search result clustering. Experimental results based on click data also show that the re-ranking of search result based on our method can significantly improve the efficiency of users' ability to find information.

#index 1978746
#* Community Preserving Lossy Compression of Social Networks
#@ 31441 3214
#t 2012
#c ICDM '12 Proceedings of the 2012 IEEE 12th International Conference on Data Mining
#! Compression plays an important role in social network analysis from both practical and theoretical points of view. Although there are a few pioneering studies on social network compression, they mainly focus on lossless approaches. In this paper, we tackle the novel problem of community preserving lossy compression of social networks. The trade-off between space and information preserved in a lossy compression presents an interesting angle for social network analysis, and, at the same time, makes the problem very challenging. We propose a sequence graph compression approach, discuss the design of objective functions towards community preservation, and present an interesting and practically effective greedy algorithm. Our experimental results on both real data sets and synthetic data sets demonstrate the promise of our method.

